{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n# Importing the libraries\nimport pandas as pd\nimport seaborn as sns\n\n# Importing the training dataset\ntrain = pd.read_csv(\"../input/train.csv\")\n\n# Viewing the number of rows and columns in the training dataset\ntrain_shape = train.shape\nprint(train_shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"**Topic: Machine Learning From Disaster**\n\n**Challenge:** To Completethe analysis of what sort of people were likely to survive. \n\nApply machine learining and its tools to predict which passengers survived the tragedy.\nSkills Required: Python, R | Binary Classification\n\n**Basic Information: **\nThe data has been split into two groups:\n\ntraining set (train.csv)\ntest set (test.csv)\n\n**Training Set**: \nIt is used here to build your machine learning models. \nGround truth is provided for each passenger.\nModel is based on “features” like passengers’ gender and class.\n\n**Test set**\nTo check the performance of model on unseen data.\nGround truth is not provided for any passenger.\nModel is used to  model to predict whether or not they survived the sinking of the Titanic.\n\nStep 1: We have dataset of different information about passengers on the board - Titanic. This information will be used to predict whether those people survived or not."},{"metadata":{"_uuid":"f8d3724c90a5ce107cb95f0ea5b85acbf869ba18"},"cell_type":"markdown","source":"Step 2: To Get the Information of all the data stored in the Table"},{"metadata":{"trusted":true,"_uuid":"50b6358cf437eb8ebdd55a8aa406375f438aa1ed"},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fb09c5dccb429054e7cef4dd488b266660225e66"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"396ed4ec2c3e05ae70ba9b8bebadc7b82464dd23"},"cell_type":"code","source":"# We can use DataFrame.pivot_table() to easily do this\n# Importing the library for plotting\nimport matplotlib.pyplot as plt\n# Calling the pivot_table() function for Sex\nsex_pivot = train.pivot_table(index = \"Sex\", values = \"Survived\")\nsex_pivot.plot.bar()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"430b37b39416800a67601fb40a038deff850bb2f"},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Pie chart, where the slices will be ordered and plotted counter-clockwise:\nlabels = 'Female', 'Male'\nsizes = [70, 20]\nexplode = (0, 0.1)  # only \"explode\" the 2nd slice (i.e. 'Male')\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=90)\nax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"25bb577d3c6f5f756cffa71d6d0ba76786be1edf"},"cell_type":"code","source":"# drop unnecessary columns, these columns won't be useful in analysis and prediction\ntrain = train.drop(['PassengerId','Name','Ticket'], axis=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ae83f2cd1fcc56ef95c2b005beb822ac20f961f4"},"cell_type":"code","source":"# Embarked\n\n# only in titanic_df, fill the two missing values with the most occurred value, which is \"S\".\ntrain[\"Embarked\"] = train[\"Embarked\"].fillna(\"S\")\n\n# plot\nsns.catplot('Embarked','Survived', data=train,height=4,aspect=3)\n\nfig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(15,5))\n\n# sns.factorplot('Embarked',data=titanic_df,kind='count',order=['S','C','Q'],ax=axis1)\n# sns.factorplot('Survived',hue=\"Embarked\",data=titanic_df,kind='count',order=[1,0],ax=axis2)\nsns.countplot(x='Embarked', data=train, ax=axis1)\nsns.countplot(x='Survived', hue=\"Embarked\", data=train, order=[1,0], ax=axis2)\n\n# group by embarked, and get the mean for survived passengers for each value in Embarked\nembark_perc = train[[\"Embarked\", \"Survived\"]].groupby(['Embarked'],as_index=False).mean()\nsns.barplot(x='Embarked', y='Survived', data=embark_perc,order=['S','C','Q'],ax=axis3)\n\n# Either to consider Embarked column in predictions,\n# and remove \"S\" dummy variable, \n# and leave \"C\" & \"Q\", since they seem to have a good rate for Survival.\n\n# OR, don't create dummy variables for Embarked column, just drop it, \n# because logically, Embarked doesn't seem to be useful in prediction.\n\nembark_dummies_titanic  = pd.get_dummies(train['Embarked'])\nembark_dummies_titanic.drop(['S'], axis=1, inplace=True)\n\n#embark_dummies_test  = pd.get_dummies(test_df['Embarked'])\n#embark_dummies_test.drop(['S'], axis=1, inplace=True)\n\ntrain = train.join(embark_dummies_titanic)\n#test_df    = test_df.join(embark_dummies_test)\n\ntrain.drop(['Embarked'], axis=1,inplace=True)\n#test_df.drop(['Embarked'], axis=1,inplace=True) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0b40dfc6783b30961a4ae0bcc57dcd5b4c9f818d","scrolled":true},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ac0b012edc339f2e38eb090ba5265b41e64d08cf"},"cell_type":"markdown","source":"**Function for Correlation:** \n\n**Correlation** is a measure of how strongly one variable depends on another. Consider a hypothetical dataset containing information about professionals in the software industry. ... Correlation can be an important tool for feature engineering in building machine learning models."},{"metadata":{"trusted":true,"_uuid":"50bca32a574fc4f27278ee179c0117049267fff4"},"cell_type":"code","source":"def plot_correlation_map( df ):\n    corr = train.corr()\n    _ , ax = plt.subplots( figsize =( 12 , 10 ) )\n    cmap = sns.diverging_palette( 220 , 10 , as_cmap = True )\n    _ = sns.heatmap(\n        corr, \n        cmap = cmap,\n        square=True, \n        cbar_kws={ 'shrink' : .9 }, \n        ax=ax, \n        annot = True, \n        annot_kws = { 'fontsize' : 12 }\n    )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fdc624e1072dc5d0246c50aad2d820cb7f9aa8d7"},"cell_type":"code","source":"plot_correlation_map( train )\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c5c1c511ebeed7ef7231e9d13243215064837351"},"cell_type":"markdown","source":"**Exploratory Data Analysis**\nLet's begin some exploratory data analysis! We'll start by checking out missing data!\n\nExploratory Data Analysis refers to the critical process of performing initial investigations on data so as to discover patterns,to spot anomalies,to test hypothesis and to check assumptions with the help of summary statistics and graphical representations.\n\n**Missing Data**\nWe can use seaborn to create a simple heatmap to see where we are missing data!"},{"metadata":{"trusted":true,"_uuid":"b345998024e0ef36fc0a56d79bd3214afc69f9d2"},"cell_type":"code","source":"sns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='viridis')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aaf8098a7a6f72ce863977e7194c67a73c07c972"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0a1c9c05057d5bdc0e6671bc5a12b14b8b3e4663"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}