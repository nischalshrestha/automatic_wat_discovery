{"metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "_is_fork": false, "_change_revision": 0, "language_info": {"version": "3.6.1", "pygments_lexer": "ipython3", "codemirror_mode": {"version": 3, "name": "ipython"}, "mimetype": "text/x-python", "file_extension": ".py", "nbconvert_exporter": "python", "name": "python"}}, "nbformat_minor": 0, "cells": [{"cell_type": "markdown", "metadata": {"_uuid": "d27f935104fb57c2ef96a480a52912a7e188cac6", "_cell_guid": "51d0f21c-14b0-eb3d-4f91-d4192d876141"}, "outputs": [], "source": "# &#x1F4D1; &nbsp; P0: Titanic Survival Exploration (the third version)\nthe first version: https://olgabelitskaya.github.io/Data_Analyst_ND_Project2.html\n\nthe second version: https://olgabelitskaya.github.io/MLE_ND_P0_V0.html", "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "3fff980a4ea9b98760c8722adf8ef66d31693580", "_cell_guid": "baf02ed9-4120-333b-81d9-0457929f5ed6"}, "outputs": [], "source": "## 1.  References\n### Dataset\nIn 1912, the ship RMS Titanic struck an iceberg on its maiden voyage and sank, resulting in the deaths of most of its passengers and crew. In this project, we will explore a subset of the RMS Titanic passenger manifest to determine which features best predict whether someone survived or did not survive. \n\nThe data contains demographics and voyage information from 891 of the 2224 passengers and crew on board the ship.\n\nThis link allows reading the description of this dataset on the Kaggle website, where the data was obtained. https://www.kaggle.com/c/titanic/data\n\n### Resources :\nIntro to Data Science. Udacity: https://www.udacity.com/course/intro-to-data-science--ud359\n\nStatistics in Python. Scipy Lecture Notes: http://www.scipy-lectures.org/packages/statistics/index.html\n\nA Visual Introduction to Machine Learning: http://www.r2d3.us/visual-intro-to-machine-learning-part-1/\n\nThe scikit-learn metrics: http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics", "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "bbe9b151feba70a5fad0ee7053457f565f70311b", "_cell_guid": "96bfd4b8-9711-cdcb-f731-fed94a78a391"}, "outputs": [], "source": "## 2. Code Tools", "execution_count": null}, {"cell_type": "code", "metadata": {"_uuid": "eefa16d8b93d029fc3909831199e4a24e8640f75", "_cell_guid": "97619f45-1aa9-87ad-84a1-120050eb3944", "trusted": false, "_execution_state": "idle"}, "outputs": [], "source": "#########################\n### IMPORT  LIBRARIES ###\n######################### \nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom time import time\nimport matplotlib.pyplot as plt\n\nfrom IPython.display import display \n  \n# Pretty display for notebooks\nimport warnings\nwarnings.filterwarnings(\"ignore\", category = UserWarning, module = \"matplotlib\")\nfrom matplotlib import style\n%matplotlib inline\n\n########################\n### EXTRA  LIBRARIES ###\n########################\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\nfrom sklearn.metrics import f1_score, make_scorer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\nfrom sklearn.ensemble import AdaBoostClassifier \nfrom sklearn.ensemble import GradientBoostingClassifier \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV, train_test_split\n\nimport keras as ks\nfrom keras.models import Sequential, load_model, Model\nfrom keras.preprocessing import sequence\nfrom keras.optimizers import SGD, RMSprop\nfrom keras.layers import Dense, Dropout, LSTM\nfrom keras.layers import Activation, Flatten, Input, BatchNormalization\nfrom keras.layers import Convolution1D, MaxPooling1D, Conv2D, MaxPooling2D\nfrom keras.layers.embeddings import Embedding\nfrom keras.wrappers.scikit_learn import KerasRegressor", "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "8dc8ee36b1d9fb5a2134c7dea18d2bffa374617b", "_cell_guid": "242ef5ab-d80c-b1ad-5841-4915e4ed0af1"}, "outputs": [], "source": "##  3. Statistical Analysis  and   Data   Exploration\n### 3.1 Dataset\nLet's extract the data from the .csv file, create a  pandas DataFrame and look at the available indicators:\n\n- ***Survived***: Outcome of survival (0 = No; 1 = Yes)\n- ***Pclass***: Socio-economic class (1 = Upper class; 2 = Middle class; 3 = Lower class)\n- ***Name***: Name of passenger\n- ***Sex***: Sex of the passenger\n- ***Age***: Age of the passenger (Some entries contain NaN)\n- ***SibSp***: Number of siblings and spouses of the passenger aboard\n- ***Parch***: Number of parents and children of the passenger aboard\n- ***Ticket***: Ticket number of the passenger\n- ***Fare***: Fare paid by the passenger\n- ***Cabin***: Cabin number of the passenger (Some entries contain NaN)\n- ***Embarked***: Port of embarkation of the passenger (C = Cherbourg; Q = Queenstown; S = Southampton)", "execution_count": null}, {"cell_type": "code", "metadata": {"_uuid": "b9d6dfbd86df2a218daf2b41037f8fa1e049559f", "_cell_guid": "79ec3e2a-fed3-d15d-b0f6-569f13ff3d5c", "trusted": false, "_execution_state": "idle"}, "outputs": [], "source": "# Load the dataset\ntrain_file = '../input/train.csv'\ntest_file = '../input/test.csv'\ntrain_data = pd.read_csv(train_file)\ntest_data = pd.read_csv(test_file)\nfull_data = train_data.append(test_data, ignore_index=True)\n# Success\nprint (\"The Titanic dataset has {} data points with {} variables.\".format(*full_data.shape))", "execution_count": null}, {"cell_type": "code", "metadata": {"_uuid": "220c31a5be03d01ea07cb71720d5e6d40588b340", "_cell_guid": "e7f841b0-0533-946c-1467-912088ce061f", "trusted": false, "_execution_state": "idle"}, "outputs": [], "source": "print (\"Examples of Data Points\")\ndisplay(test_data[:3].T)", "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "6137e9eef3023ae366ae48bf56be5fbaaaa497df", "_cell_guid": "3b4c3a8c-fbb1-2f1f-67db-83b36a86dd65"}, "outputs": [], "source": "### 3.2 Survived\nThe next step is removing the Survived feature from the data and storing it separately as our prediction targets.\n\nThe passenger data and the outcomes of survival are now paired. That means for any passenger **all_features.loc[i]**, they have the survival **outcomes[i]**.", "execution_count": null}, {"cell_type": "code", "metadata": {"_uuid": "d9ade16d64aea1bd663e5126307d963a7ee91937", "_cell_guid": "105b185b-d5d4-b3b9-071e-a84bc40c0747", "trusted": false, "_execution_state": "idle"}, "outputs": [], "source": "# Store the 'Survived' feature in a new variable and remove it from the dataset\noutcomes = train_data['Survived']\nall_features = train_data.drop('Survived', axis = 1)\n \nprint (\"The percentage of survived passengers: {:.2f}%\".format(100*outcomes.mean()))", "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "11e2bdf29014837782b54da75413d9cd65577ee1", "_cell_guid": "7dcc51b2-96d3-3fee-22bb-17f6b232f344"}, "outputs": [], "source": "### 3.3 Accuracy Score\nTo measure the performance of our predictions, we need metrics to score our predictions against the true outcomes of survival.\n\n- 1) The built function **accuracy_score_0()** calculates the proportion of passengers where our prediction of their survival is correct.\n- 2) Functions *sklearn.metrics*: \n- **recall_score**, \n- **accuracy_score**, \n- **precision_score**, \n- **f1_score**,\n- **make_scorer**.", "execution_count": null}, {"cell_type": "code", "metadata": {"_uuid": "4d75ec336307fc645ba1f8b96f519ef54eefbddd", "_cell_guid": "88e652d0-83ab-009e-0c81-cc0bcc93868e", "trusted": false, "_execution_state": "idle"}, "outputs": [], "source": "def accuracy_score_0(truth, pred):         \n    # Ensure that the number of predictions matches number of outcomes\n    if len(truth) == len(pred):         \n        # Calculate and return the accuracy as a percent\n        return (\"Predictions have an accuracy of {:.2f}%.\".format((truth == pred).mean()*100))    \n    else:\n        return (\"Number of predictions does not match number of outcomes!\")", "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "001274c7652d02d341254bfc7103a1ab8ada0604", "_cell_guid": "d8ee719d-d1c5-2495-1470-3a4f5d469e4b"}, "outputs": [], "source": "## 4. Making   Prediction\n### 4.1 Intuitive Predictions\nIf we were asked to make a prediction about any passenger aboard the RMS Titanic whom we knew nothing about, then the best prediction we could make would be that they did not survive. This is because we can assume that a majority of the passengers (more than 50%) did not survive the ship sinking.\n\nThe **predictions_0()** function below will always predict that a passenger did not survive. Let's check its accuracy.", "execution_count": null}, {"cell_type": "code", "metadata": {"_uuid": "22c9f7a5dbd0e7a5e417f1ca7760c66ee0de3c5c", "_cell_guid": "bf26c030-1889-0304-77e6-a5cd2da34c0b", "trusted": false, "_execution_state": "idle"}, "outputs": [], "source": "def predictions_0(data):\n    \"\"\" Model with no features. Always predicts a passenger did not survive. \"\"\"\n    predictions = []\n    for _, passenger in data.iterrows():        \n        # Predict the survival of 'passenger'\n        predictions.append(0)    \n    # Return our predictions\n    return pd.Series(predictions)\n# Make the predictions that a passenger did not survive\npredictions = predictions_0(all_features) \n# Check accuracy\naccuracy_score_0(outcomes, predictions)", "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "7ee2d3d5d0395f52bffb63ad7a76157fbc5ae9d6", "_cell_guid": "729a8a6f-c5f5-1d02-46a3-4b67fdf18512"}, "outputs": [], "source": "Now let us check the feature \"**Sex**\", the outcomes \"**Survived**\" and their possible dependence. \n\nHere is a special database for these categories including the indicators in percentages:\n\n- \"**Survived by sex in percentages I**\" determines the percentage of survived passengers of this sex in relation to the total number of survivors; \n- \"**Survived by sex in percentages II**\" - the percentage of survived passengers of this sex in relation to the total number of passengers of the same sex.", "execution_count": null}, {"cell_type": "code", "metadata": {"_uuid": "7401478e93aa60aa4f1f62736c8473816d66f350", "_cell_guid": "ecf43b44-f52d-bef7-02dc-3a4d7887d4fc", "trusted": false, "_execution_state": "idle"}, "outputs": [], "source": "# Groups by sex\nnumber_by_sex = pd.Series(train_data.groupby('Sex').count()['PassengerId'])\nsurvived_by_sex = pd.Series(train_data.groupby('Sex').sum()['Survived'])\n\ndef percent_xy(x,y):\n    return round(100.0*x/y, 2)\n\n# Count percentages\nnumber_by_sex_in_per = pd.Series(percent_xy(number_by_sex, len(train_data)))\nsurvived_by_sex_in_per1 = pd.Series(percent_xy(survived_by_sex, outcomes.sum()))\nsurvived_by_sex_in_per2 = pd.Series(percent_xy(survived_by_sex, number_by_sex))\nsex_data = pd.DataFrame({'Number by sex': number_by_sex,\n                         'Number by sex in percentages':number_by_sex_in_per,\n                         'Survived by sex':survived_by_sex,\n                         'Survived by sex in percentages I':survived_by_sex_in_per1,\n                         'Survived by sex in percentages II':survived_by_sex_in_per2})\nsex_data", "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "d7fc0c19b1c7b48f61708870d2241813abf83c79", "_cell_guid": "b4c636b5-d2d2-f4ce-c330-8a5c6444eab3"}, "outputs": [], "source": "Combining the data in one histogram clearly shows the tendency: female passengers were more likely to survive in this instance.", "execution_count": null}, {"cell_type": "code", "metadata": {"_uuid": "8a76cc86923db2c492c02309441446508335aba3", "_cell_guid": "f511a686-be0a-c881-385b-79c28de1b663", "trusted": false, "_execution_state": "idle"}, "outputs": [], "source": "# plot parameters\nplt.rcParams['figure.figsize'] = (13, 6)\nstyle.use('seaborn-bright')\nfig = plt.figure()\nax = fig.add_subplot(111)\n\nind = np.array([1,2])           # the x locations for the indicator\nwidth = 0.2                     # the width of the bars\n\n## Bars\nrects1 = ax.bar(ind, number_by_sex_in_per, width, alpha = 0.7)\nrects2 = ax.bar(ind+width, survived_by_sex_in_per1, width, alpha = 0.7)\nrects3 = ax.bar(ind+2*width, survived_by_sex_in_per2, width, alpha = 0.7)\n\n# Axes and labels\nax.set_xlim(-width+1,len(ind)+width+0.6)\nax.set_ylim(0,90)\nax.set_ylabel('Values by sex in percentages')\nax.set_title('Passengers, Statistics by Sex in Percentages', fontsize=20)\nxTickMarks = ['female','male']\nax.set_xticks(ind+width)\nxtickNames = ax.set_xticklabels(xTickMarks)\nplt.setp(xtickNames, rotation=0, fontsize=30)\n\n## Legend\nax.legend((rects1[0], rects2[0], rects3[0]), \n          ('Number by sex in percentages', \n           'Survived by sex in percentages I', \n           'Survived by sex in percentages II'))", "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "427837e0626af162ccdc8fa9176b2341e613c6cb", "_cell_guid": "70a77af1-67cc-4f0a-47a3-e50926107951"}, "outputs": [], "source": "Let's build on our previous prediction: If a passenger was female, then we will predict that they survived. Otherwise, we will predict the passenger did not survive.", "execution_count": null}, {"cell_type": "code", "metadata": {"_uuid": "61a500be9f7628b1e68a0a61f10cf361b023fd75", "_cell_guid": "f51d2d5a-45cf-8201-a45c-10bf22f4a3fa", "trusted": false, "_execution_state": "idle"}, "outputs": [], "source": "def predictions_1(data):\n    \"\"\" Model with one feature: \n            - Predict a passenger survived if they are female. \"\"\"\n    \n    predictions = []\n    for _, passenger in data.iterrows():\n        if passenger['Sex'] == 'female':\n            predictions.append(1)\n        else:\n            predictions.append(0)\n    \n    # Return our predictions\n    return pd.Series(predictions)\n\n# Make the predictions \npredictions1 = predictions_1(all_features) \n# Check accuracy\naccuracy_score_0(outcomes, predictions1)", "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "d9c1ecffeb3fde1a44a7959192b854ed5351f9ad", "_cell_guid": "66100172-9f34-0611-31d1-cbdda6d0a7de"}, "outputs": [], "source": "Using just the Sex feature for each passenger, we are able to increase the accuracy of our predictions by a significant margin.\nNow, let's consider using an additional feature to see if we can further improve our predictions. We will start by looking at the Age.", "execution_count": null}, {"cell_type": "code", "metadata": {"_uuid": "4958489fdfd3e0d2c4cef7e7b7a6bff5214d10b2", "_cell_guid": "e515cde0-84d1-f229-ce9a-26604517ccf4", "trusted": false, "_execution_state": "idle"}, "outputs": [], "source": "# Data for male passengers\nmale = train_data[train_data.Sex=='male']\nmale.Age.hist(alpha=0.7)\nmale[male.Survived==1].Age.hist(alpha=0.7)\n\n# Labels, title, legend\nplt.xlabel(\"Age\")\nplt.ylabel(\"Number by age\")\nplt.title(\"Male Passengers, Statistics by Age\", fontsize=20)\nplt.legend([\"Number of Male Passengers\", \n           \"Male Survived Passengers by Age\"])", "execution_count": null}, {"cell_type": "code", "metadata": {"_uuid": "401e34e413adc21251f012056f1eeb7a77646577", "_cell_guid": "e33aea61-85ed-12df-c3be-21bd0c981afe", "trusted": false, "_execution_state": "idle"}, "outputs": [], "source": "def predictions_2(data):\n    \"\"\" Model with two features: \n            - Predict a passenger survived if they are female.\n            - Predict a passenger survived if they are male and younger than 10. \"\"\"\n    \n    predictions = []\n    for _, passenger in data.iterrows():\n        if passenger['Sex'] == 'female':\n            predictions.append(1)\n        elif passenger['Sex'] == 'male' and passenger['Age'] < 10:\n            predictions.append(1)\n        else:\n            predictions.append(0)\n    \n    # Return our predictions\n    return pd.Series(predictions)\n# Make the predictions \npredictions2 = predictions_2(all_features) \n# Check accuracy\naccuracy_score_0(outcomes, predictions2)", "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "89c8e58389d5496173841acff2a8bfccae8177fc", "_cell_guid": "d5f9d510-a494-097b-088b-8b81b466d0c6"}, "outputs": [], "source": "Adding the feature **Age** as a condition in conjunction with **Sex** improves the accuracy by a small margin more than with simply using the feature **Sex** alone. Now we can try to find a series of features and conditions to split the data on to obtain an outcome prediction accuracy of at least 80%. This may require multiple features and multiple levels of conditional statements to succeed. We can use the same feature multiple times with different conditions.\n\nThere are some experiments and the function **prediction_final()** as a result:", "execution_count": null}, {"cell_type": "code", "metadata": {"_uuid": "6001b23c84693b179c6483d97745c6210e3a8997", "_cell_guid": "25c35af4-2d27-ccc2-4aa7-5762902420f2", "trusted": false, "_execution_state": "idle"}, "outputs": [], "source": "# Data for male and young passengers\nyoung_male = male[male.Age < 15]\nsurvived_young_male = young_male[young_male.Survived==1]\n\n# Create histogram\nplt.hist((young_male.Pclass, survived_young_male.Pclass), \n          bins=range(1,5), rwidth=0.7, alpha=0.7, align='left')\n\n# Ticks, labels, title, legend\nplt.xticks([1,2,3])\nplt.xlabel(\"Plass\")\nplt.ylabel(\"Number by Pclass\")\nplt.title(\"Male Young Passengers, Statistics by Pclass\", fontsize=20)\nplt.legend([\"Male Young Passengers\", \"Survived Male Young Passengers\"])", "execution_count": null}, {"cell_type": "code", "metadata": {"_uuid": "d60a3b90d2ff05aee49495ce809d0711a5b6c4fd", "_cell_guid": "15702878-a67a-5362-8ca0-7ed567b528c5", "trusted": false, "_execution_state": "idle"}, "outputs": [], "source": "# Create histogram\nplt.hist((young_male.SibSp, survived_young_male.SibSp), \n         bins=range(0,7), rwidth=0.5, align='left', alpha=0.7)\n\n# Labels, title, legend\nplt.xlabel(\"SibSb\")\nplt.ylabel(\"Number by SibSp\")\nplt.title(\"Male Young Passengers, Statistics by SibSp\", fontsize=20)\nplt.legend([\"Male Young Passengers\", \"Survived Young Male Passengers\"])", "execution_count": null}, {"cell_type": "code", "metadata": {"_uuid": "a534d2590f53e96c4a5d8b91eae406fafa761b2d", "_cell_guid": "bb67f805-4bb3-0252-99a9-58b9e427e3c6", "trusted": false, "_execution_state": "idle"}, "outputs": [], "source": "def predictions_final(data):\n    \"\"\" Model with multiple features. Makes a prediction with an accuracy of at least 80%. \"\"\"\n    \n    predictions = []\n    for _, passenger in data.iterrows():\n        if (passenger['Sex'] == 'female'):\n            predictions.append(1)\n                \n        elif passenger['Pclass'] in [1,2] and (passenger['Age'] < 16 or passenger['Age'] > 75):\n            predictions.append(1)\n            \n        elif passenger['Age'] < 15 and passenger['SibSp'] < 3:\n            predictions.append(1)\n            \n        else:\n            predictions.append(0)\n    \n    # Return our predictions\n    return pd.Series(predictions)\n# Make the predictions \npredictions_final = predictions_final(all_features) \n# Check accuracy\naccuracy_score_0(outcomes, predictions_final)", "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "03153792d25c3b3dc97d8bb7b4fbe8628b97a870", "_cell_guid": "4879563e-5c42-0b8d-a67e-fa1df17f9f8e"}, "outputs": [], "source": "The final set of features **Sex**, **Age**, **SibSp** and **Pclass** are the most informative on my opinion.\n\nAs we noted the percentage of survivors of passengers is much higher among women than among men, and it was used in our predictions.\n\nNext, I proceed from the assumption that because of humanitarian reasons people rescue children and elders at first. Unfortunately, this was only valid for the passengers of the first and second classes in this dataset.\n\nAnd the latest clarification, which overcomes the border of 80% in prediction accuracy: if a family has more than three children, absolutely all the family may not be survived in catastrophic situations and in an atmosphere of panic.", "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "459e1cd486cbbc7e24371a40e5bb7e003964d3cc", "_cell_guid": "3dc475ee-4507-2fc4-8c21-012348bd77f4"}, "outputs": [], "source": "Let's evaluate the quality of the prediction.", "execution_count": null}, {"cell_type": "code", "metadata": {"_uuid": "9dfee961e35d3b5aedc7e8a2e1a86c9a9382ecd7", "_cell_guid": "5b97519b-5561-d502-d117-b344e8a09647", "trusted": false, "_execution_state": "idle"}, "outputs": [], "source": "print (\"Predictions have an accuracy of {:.2f}%.\".format(accuracy_score(outcomes, predictions_final)*100))\nprint (\"Predictions have an recall score equal to {:.2f}%.\".format(recall_score(outcomes, predictions_final)*100))\nprint (\"Predictions have an precision score equal to {:.2f}%.\".format(precision_score(outcomes,predictions_final)*100))", "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "49bbb8042a9c74593454a3b314eed975a9aabe91", "_cell_guid": "8ccc3161-4e7c-efb2-2fe6-187e103a3959"}, "outputs": [], "source": "The evaluation terminology:\n\n- accuracy = number of  people that  are  correctly  predicted as survived or non-survived / number  of all  people  in  the dataset \n- recall = number of people that are predicted as survived and they are actually survived / number of  people are actually survived\n- precision =  number of people that  are predicted as survived  and they are actually survived / number of people that are predicted as survived\n\nIn this part of the project, I have used a manual implementation of a simple machine learning model, the decision tree which splits a set of data into smaller and smaller groups (called nodes), by one feature at a time. The predictions become more accurate if each of the resulting subsets is more homogeneous (contain similar labels) than before.\n\nA decision tree is just one of many models that come from supervised learning, i.e. learning a model from labeled training data to make predictions about unseen or future data in a set of samples the desired outputs are already known.", "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "931b56baa4d204aa729db4ada50fde51540d5b52", "_cell_guid": "46ecac7c-785a-d373-32b1-c612a0b91ee8"}, "outputs": [], "source": "### 4.2 Classifiers\nI think this machine learning problem is in the classification field. It needs to predict the labels for the passengers: 'yes' or 'no' for the feature 'Survived'.\n\nFor simplicity, the border between regression and classification can be described in this way:\n\n- classification: predict the values of discrete or categorical targets;\n- regression: predict the values of continuous targets.\n\nLet's display some important information about the data training set.", "execution_count": null}, {"cell_type": "code", "metadata": {"_uuid": "dcac98159f0067fe1bd189eaa05b479a11d804ba", "_cell_guid": "af777b47-0250-59c2-bf34-64afe4adb103", "trusted": false, "_execution_state": "idle"}, "outputs": [], "source": "# Calculate the number of passengers\nn_passengers = len(train_data)\n\n# Calculate the number of features\nn_features = len(list(train_data.T.index))\n\n# Number of survived passengers\nn_survived = len(train_data[train_data['Survived'] == 1])\n\n# Number of non-survived passengers\nn_nonsurvived = len(train_data[train_data['Survived'] == 0])\n\n# Print the results\nprint (\"Total number of passengers: {}\".format(n_passengers))\nprint (\"Number of features: {}\".format(n_features))\nprint (\"Number of passengers who survived: {}\".format(n_survived))\nprint (\"Number of passengers who did not survive: {}\".format(n_nonsurvived))\nprint (\"Persentage of survived passengers: {:.2f}%\".format(100*n_survived/n_passengers))\n\n# Show the list of columns\nprint (\"\\nFeature columns:\\n{}\".format(list(all_features.columns)))\nprint (\"\\nTarget column: 'Survived'\")", "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "754bd4f6b3439923b714dbad125d78f7685c42c7", "_cell_guid": "64daf4e3-b116-3a6e-56e4-5f50b47370b8"}, "outputs": [], "source": "Let's exclude the features that cannot have an influence on the target and the feature **Cabin** that has too many **NaN** values.", "execution_count": null}, {"cell_type": "code", "metadata": {"_uuid": "231f0e596161e788cae8faadffec3db94d50c38d", "_cell_guid": "f59b433b-2160-434e-b18c-ef9989583664", "trusted": false, "_execution_state": "idle"}, "outputs": [], "source": "# Count nan values for the feature 'Cabin'\nn_nan_cabin = pd.isnull(all_features['Cabin']).sum()\nprint (\" Persentage of NaN values in 'Cabin': {:.2f}%\".format(100.0*n_nan_cabin/n_passengers))\n# Setup a new feature list\nfeature_list2 = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\npre_features = all_features[feature_list2]\ntest_features = test_data[feature_list2]", "execution_count": null}, {"cell_type": "code", "metadata": {"_uuid": "1b9698d1fca3131d47bb502de9a0993f26bb2316", "_cell_guid": "ed960f3d-a940-bca3-05f1-b30c5959add9", "trusted": false, "_execution_state": "idle"}, "outputs": [], "source": "pre_features.head(11).T", "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "85f6139884b2813f92151cd7b4321720c7f96e00", "_cell_guid": "6a33912c-998f-2d98-83d5-50ad0562a857"}, "outputs": [], "source": "Now I should count missing values for the remained variables.", "execution_count": null}, {"cell_type": "code", "metadata": {"_uuid": "d130cf3a56d4b7a62c6329d8c2252ae4b874670c", "_cell_guid": "fa60394c-b248-cb02-4c67-0586355705b0", "trusted": false, "_execution_state": "idle"}, "outputs": [], "source": "print (\"Number of missing values in the train set\\n\")\nfor feature in feature_list2:\n    print(\"{}:{}\".format(feature, pd.isnull(pre_features[feature]).sum()))\n\nprint('')\n    \nprint (\"Number of missing values in the test set\\n\")\nfor feature in feature_list2:\n    print(\"{}:{}\".format(feature, pd.isnull(test_features[feature]).sum()))", "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "671f6e8958d2569ffc293f3372e75f93fbd87673", "_cell_guid": "2010abe8-5ec1-cb5f-0294-aa0777115484"}, "outputs": [], "source": "As we can see, there are several non-numeric columns that need to be converted. Some of them have simply two values (male/female, 1/0, etc.). These features can be reasonably converted into binary values.\n\nOther columns, like **Embarked**, have more than two values and are known as categorical variables. The recommended way to handle such a column is to create as many columns as possible values and assign a 1 to one of them and 0 to all others.\n\nThese generated columns are sometimes called dummy variables, and I will use the **pandas.get_dummies()** function to perform this transformation.\n\nIn the preprocessing function, I will also replace found missing values in the features **Age** and **Fare** by the mean and in the feature **Embarked** by the most common value", "execution_count": null}, {"cell_type": "code", "metadata": {"_uuid": "18e54e904e918bb11ef9288804808727cce03f5a", "_cell_guid": "a9d6a908-2278-6b04-c3fa-353381e0472f", "trusted": false, "_execution_state": "idle"}, "outputs": [], "source": "pre_features2 = pre_features.copy()\ntest_features2 = test_features.copy()\n\npre_features2['Sex'].replace({'male' : 1, 'female': 0}, inplace=True)\ntest_features2['Sex'].replace({'male' : 1, 'female': 0}, inplace=True)\n\nage_mean = pd.concat([pre_features2.Age, test_features2.Age], ignore_index=True).mean()\npre_features2['Age'] = pre_features2['Age'].fillna(age_mean)\ntest_features2['Age'] = test_features2['Age'].fillna(age_mean)\n\nfare_mean = pd.concat([pre_features2.Fare, test_features2.Fare], ignore_index=True).mean()\ntest_features2['Fare'] = test_features2['Fare'].fillna(fare_mean)\n\nmost_common = pd.get_dummies(pre_features2['Embarked']).sum().sort_values(ascending=False).index[0]\npre_features2['Embarked'] = pre_features2['Embarked'].fillna(most_common)", "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "143a49f060e1dd21e771e7b3abd1438754899974", "_cell_guid": "b2fafb5b-deba-fc1b-038b-42d41dd4fc35"}, "outputs": [], "source": "Here is the result of applying this function.", "execution_count": null}, {"cell_type": "code", "metadata": {"_uuid": "0c29c0420201c34b4a885e7f2431d117c0e88e50", "_cell_guid": "e2626a16-283c-b40f-9e8a-d7f6b29ac995", "trusted": false, "_execution_state": "idle"}, "outputs": [], "source": "pre_features2.head(8).T", "execution_count": null}, {"cell_type": "code", "metadata": {"_uuid": "eff1d90f3fa89cf8170062127d8dfa56d253a2a1", "collapsed": false, "_cell_guid": "5372981e-5852-43be-b807-cda647dd314b", "trusted": false, "_execution_state": "idle"}, "outputs": [], "source": "pre_features2 = pd.get_dummies(pre_features2, prefix=['Embarked'])\ntest_features2 = pd.get_dummies(test_features2, prefix=['Embarked'], columns=['Embarked'])", "execution_count": null}, {"cell_type": "code", "metadata": {"_uuid": "fa47122776beff32489974de6df105317b535cff", "_cell_guid": "8c6ec7af-1ae4-aa60-75d2-350feb3e7f11", "trusted": false, "_execution_state": "idle"}, "outputs": [], "source": "pd.isnull(pre_features2).sum()", "execution_count": null}, {"cell_type": "code", "metadata": {"_uuid": "d368bee477ebfb0cdc7572b42f32b3957245da07", "_cell_guid": "cd5249f0-3267-a583-5c53-6d4bfbef1e89", "trusted": false, "_execution_state": "idle"}, "outputs": [], "source": "pd.isnull(test_features2).sum()", "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "c06124c4918ead26d85051aaa88a1af18d144d28", "_cell_guid": "9f77f3ff-07d3-deba-633f-b2414c8e77d4"}, "outputs": [], "source": "For predictions I have chosen the following models:\n\n- GradientBoostingClassifier();\n- RandomForestClassifier();\n- AdaBoostClassifier().\n\nLet's have a look at their applications and characteristics:\n\n1) **GradientBoostingClassifier**.\n\n- Applications: in the field of learning to rank (for example, web-seach), in ecology.\n  - Web-Search Ranking with Initialized Gradient Boosted Regression Trees: http://www.jmlr.org/proceedings/papers/v14/mohan11a/mohan11a.pdf\n  - Gradient boosting machines, a tutorial: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3885826/\n- Strengths: natural handling of data of mixed type (= heterogeneous features), predictive power, robustness to outliers in output space (via robust loss functions).\n- Weaknesses: scalability, due to the sequential nature of boosting it can hardly be parallelized.\n\n2) **RandomForestClassifier**.\n\n- Applications: in ecology, bioinformatics.\n  - RANDOM FORESTS FOR CLASSIFICATION IN ECOLOGY: http://onlinelibrary.wiley.com/doi/10.1890/07-0539.1/abstract;jsessionid=AB1864A895F3244AF0699EB0317F2C99.f02t02\n  - Random Forest for Bioinformatics: http://www.cs.cmu.edu/~qyj/papersA08/11-rfbook.pdf\n- Strengths: runs efficiently on large data bases; gives estimates of what variables are important in the classification; maintains accuracy when a large proportion of the data are missing; high prediction accuracy.\n- Weaknesses: difficult to interpret, can be slow to evaluate.\n\n3) **AdaBoostClassifier**.\n\n- Applications: the problem of face detection, text classification, etc.\n  - AdaBoost-based face detection for embedded systems: http://www.sciencedirect.com/science/article/pii/S1077314210000871\n  - Text Classification by Boosting Weak Learners based on Terms and Concepts: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.443.8019&rep=rep1&type=pdf\n- Strengths: can be used with data that is textual, numeric, discrete, etc.; can be combined with any other learning algorithm, not prone to overfitting; simple to implement.\n- Weaknesses: can be sensitive to noisy data and outliers; the performance depends on data and weak learner (can fail if weak classifiers too complex).\n\nAll these classifiers will produce enough good predictions in this case. We should produce the result with the variant of ranking and it's a well-known fact that classification tends to be a better paradigm for ranking than regression.\n\nLet's initialize three helper functions which we can use for training and testing sets the three supervised learning models we've chosen above.\n\nThe functions are as follows:\n\n- **train_classifier** - takes as input a classifier and training data and fits the classifier to the data;\n- **predict_labels** - takes as input a fit classifier, features, and a target labeling and makes predictions using the F1 score;\n- **train_predict** - takes as input a classifier, and the training data, and performs train_clasifier and predict_labels.\n\nThis function will report the **F1 score** for the training data.", "execution_count": null}, {"cell_type": "code", "metadata": {"_uuid": "5df79a19c1c6f51768da850c8c9071813bf97952", "_cell_guid": "e401f8e6-ccbb-af49-18d5-f79e4eed451f", "trusted": false, "_execution_state": "idle"}, "outputs": [], "source": "def train_classifier(clf, X_train, y_train):\n    ''' Fits a classifier to the training data. '''\n    \n    # Start the clock, train the classifier, then stop the clock\n    start = time()\n    clf.fit(X_train, y_train)\n    end = time()\n    \n    # Print the results\n    print (\"Trained model in {:.4f} seconds\".format(end - start))\n    \ndef predict_labels(clf, features, target):\n    ''' Makes predictions using a fit classifier based on F1 score. '''\n    \n    # Start the clock, make predictions, then stop the clock\n    start = time()\n    y_pred = clf.predict(features)\n    end = time()\n    \n    # Print and return results\n    print (\"Made predictions in {:.4f} seconds.\".format(end - start))\n    return f1_score(target.values, y_pred, pos_label=1)\n\n\ndef train_predict(clf, X_train, y_train):\n    ''' Train and predict using a classifer based on F1 score. '''\n    \n    # Indicate the classifier and the training set size\n    print (\"Training a {} using a training set size of {}. . .\".\\\n           format(clf.__class__.__name__, len(X_train)))\n    \n    # Train the classifier\n    train_classifier(clf, X_train, y_train)\n    \n    # Print the results of prediction for both training and testing\n    print (\"F1 score for training set: {:.4f}.\".format(predict_labels(clf, X_train, y_train)))", "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "e5ec4fad5e502337633743fbccdcd74978c3fb37", "_cell_guid": "b9b5df1b-c16c-8fd5-4507-a46c052e5d63"}, "outputs": [], "source": "With the predefined functions above, I will now import the three supervised learning models and run the **train_predict** function for each one. It needs to train and predict on each classifier for three different training set sizes: 200, 400, and all data points. \n\nHence,  nine different outputs will be displayed below \u2014 three for each model using the varying training set sizes.", "execution_count": null}, {"cell_type": "code", "metadata": {"_uuid": "aa5a0fc4137e7fe61437fa39721b7a283c2cf8c6", "_cell_guid": "88f1905d-2fce-81dd-87fd-1e3f3ca695eb", "trusted": false, "_execution_state": "idle"}, "outputs": [], "source": "# Initialize the three models\nclf_A = AdaBoostClassifier()\nclf_B = GradientBoostingClassifier()\nclf_C = RandomForestClassifier()\n\n# Set up the training set sizes\nX_train_200, y_train_200 = pre_features2[:200], outcomes[:200]\nX_train_400, y_train_400 = pre_features2[:400], outcomes[:400]\nX_train, y_train = pre_features2, outcomes\nX_test = test_features2\n\n# Execute the 'train_predict' function for each classifier and each training set size\nfor clf in [clf_A, clf_B, clf_C]:\n    for (X_train, y_train) in [(X_train_200, y_train_200), \n                               (X_train_400, y_train_400), \n                               (X_train, y_train)]:\n        train_predict(clf, X_train, y_train)", "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "fd71710712d772b9501c06c469658508d92eaf73", "_cell_guid": "9ffa3665-4f15-ff02-a451-6bc854367343"}, "outputs": [], "source": "The best result has the **RandomForestClassifier** and I will try to apply it for test predictions.", "execution_count": null}, {"cell_type": "code", "metadata": {"_uuid": "c8691380d0bfaefb36f25add6a69949df353a336", "_cell_guid": "30c0aedf-1f43-26f3-d5f6-3649b3f25489", "trusted": false, "_execution_state": "idle"}, "outputs": [], "source": "clf = RandomForestClassifier()\nclf.fit(X_train, y_train)\ny_pred = clf.predict(test_features2)\ny_pred", "execution_count": null}, {"cell_type": "code", "metadata": {"_uuid": "b8e8d78f352d81bf0dacbf7570d6732288c79f51", "_cell_guid": "b45f5550-b6dd-9fcd-405d-eb162398817d", "trusted": false, "_execution_state": "idle"}, "outputs": [], "source": "submission = pd.DataFrame({\"PassengerId\": test_data[\"PassengerId\"], \"Survived\": y_pred})", "execution_count": null}, {"cell_type": "code", "metadata": {"_uuid": "cd7c55f0aaa8d6af0ac29e3d22a127f12fb70e48", "_cell_guid": "f77feb0f-21db-85ab-60fa-b58afe8ec05b", "trusted": false, "_execution_state": "idle"}, "outputs": [], "source": "# submission.to_csv('C:/users/OlgaBelitskaya/submission.csv', index=False)", "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "862bc8eed03cc9722970b4f60f994fabbd9e3016", "collapsed": false, "_cell_guid": "14adfce9-bae4-4eab-bf95-b6bb7b853037", "_execution_state": "idle"}, "outputs": [], "source": "## 5. MLP Neural Networks", "execution_count": null}, {"cell_type": "code", "metadata": {"_uuid": "fb507f41bc622e57ae301b98ba1a807b858fe0fe", "collapsed": false, "_cell_guid": "42865f81-2d08-422d-a50d-64274a76333e", "trusted": false, "_execution_state": "idle"}, "outputs": [], "source": "pre_features_array = pre_features2.as_matrix()\noutcomes_array = outcomes.as_matrix()\ntest_features_array = test_features2.as_matrix()\n\nX_train2, X_test2, y_train2, y_test2 = train_test_split(pre_features_array, outcomes_array, \n                                                        test_size=0.2, random_state=1)\nX_train2.shape, X_test2.shape                                                       ", "execution_count": null}, {"cell_type": "code", "metadata": {"_uuid": "f8c7240d3b69a3281081ed24d21d1696e48ccafc", "collapsed": false, "_cell_guid": "23583048-cd79-421c-a003-e23eb36360d9", "trusted": false, "_execution_state": "idle"}, "outputs": [], "source": "def mlp_model():\n    model = Sequential()\n    \n    model.add(Dense(18, input_dim=9, kernel_initializer='uniform', activation='relu'))\n    model.add(Dense(9, kernel_initializer='uniform', activation='relu'))\n    model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    return model", "execution_count": null}, {"cell_type": "code", "metadata": {"_uuid": "f51fe97567112366d674ffa7acb30bdeb2d98729", "collapsed": false, "_cell_guid": "41861189-d1c3-493d-8c36-5b95e90b1081", "trusted": false, "_execution_state": "idle"}, "outputs": [], "source": "mlp_model = mlp_model()\n\nmlp_history = mlp_model.fit(X_train2, y_train2, validation_data=(X_test2, y_test2),\n                            epochs=100, batch_size=8, verbose=0)", "execution_count": null}, {"cell_type": "code", "metadata": {"_uuid": "a1b14b857f9921943db75e17864f48760fad57e0", "collapsed": false, "_cell_guid": "8075180b-60d5-4b51-ae9a-308a242eb157", "trusted": false, "_execution_state": "idle"}, "outputs": [], "source": "def loss_plot(fit_history):\n    plt.figure(figsize=(14, 6))\n    plt.plot(fit_history.history['loss'], label = 'train')\n    plt.plot(fit_history.history['val_loss'], label = 'test')\n    plt.legend()\n    plt.title('Loss Function');  \n    \ndef acc_plot(fit_history):\n    plt.figure(figsize=(14, 6))\n    plt.plot(fit_history.history['acc'], label = 'train')\n    plt.plot(fit_history.history['val_acc'], label = 'test')\n    plt.legend()\n    plt.title('Accuracy'); ", "execution_count": null}, {"cell_type": "code", "metadata": {"_uuid": "980258a5df1bd60b4914be0c03ab4bdf0423c189", "collapsed": false, "_cell_guid": "b2726e4f-dc0f-46af-ad98-165cdfa42fa1", "trusted": false, "_execution_state": "idle"}, "outputs": [], "source": "loss_plot(mlp_history)\nacc_plot(mlp_history)", "execution_count": null}, {"cell_type": "code", "metadata": {"_uuid": "a30679af52af64b0268668c69aa1bd836e97ab5d", "collapsed": false, "_cell_guid": "64974296-ceb8-486e-aa4f-38c9de67618c", "trusted": false, "_execution_state": "busy"}, "outputs": [], "source": "mlp_scores = mlp_model.evaluate(X_test2, y_test2, verbose=0)\nprint(\"Accuracy: %.2f%%\" % (mlp_scores[1]*100))", "execution_count": null}, {"cell_type": "code", "metadata": {"_uuid": "c439d5d2fbe6445c4523344dc6ba12389f130a2e", "collapsed": false, "_cell_guid": "66b3785c-c97d-493b-890b-e26d4ae74eb0", "trusted": false, "_execution_state": "busy"}, "outputs": [], "source": "y_pred_mlp = np.round(mlp_model.predict(test_features_array)).astype(int)\ny_pred_mlp[:,0]", "execution_count": null}, {"cell_type": "code", "metadata": {"_uuid": "c5db7be13a8f0daf85626b9812ee9194bc96e1ad", "collapsed": false, "_cell_guid": "af699560-5339-4aee-9554-3a85ee152216", "trusted": false, "_execution_state": "busy"}, "outputs": [], "source": "submission_mlp = pd.DataFrame({\"PassengerId\": test_data[\"PassengerId\"], \"Survived\": y_pred_mlp[:,0]})", "execution_count": null}, {"cell_type": "code", "metadata": {"_uuid": "cbb509b4538b0d78a9e174836c85395ef96f1c5c", "collapsed": false, "_cell_guid": "e2654b73-10d6-4bed-9b8e-83c593d15ec7", "trusted": false, "_execution_state": "busy"}, "outputs": [], "source": "# submission_mlp.to_csv('C:/users/OlgaBelitskaya/submission.csv', index=False)", "execution_count": null}, {"cell_type": "code", "metadata": {"_uuid": "82c0b351277d7004bbe8bc574641185c0bdd451e", "collapsed": false, "_cell_guid": "39872287-6998-46c6-8fce-ab809ad260c7", "trusted": false, "_execution_state": "busy"}, "outputs": [], "source": "len(submission_mlp[\"Survived\"])", "execution_count": null}, {"cell_type": "code", "metadata": {"_uuid": "7c7ac0d944af04ee0855b395f9fa8c9bb206ab58", "collapsed": false, "_cell_guid": "fd2f7c77-b127-4af3-887e-d8bf1bc39159", "trusted": false, "_execution_state": "busy"}, "outputs": [], "source": "sum(submission_mlp[\"Survived\"]==submission[\"Survived\"])", "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "320f81564025ef10349e6dbbe9f9ad20169580e6", "_cell_guid": "3a7fa44a-f34a-62ff-86f4-d495e0816de4"}, "outputs": [], "source": "## 6. Conclusion", "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "f7494dd405be9b89efb49a7c1e3762b74c8b2001", "_cell_guid": "3f3b14ee-fbb0-2606-4129-5225319bb00c"}, "outputs": [], "source": "Overvaluation of meaning and application of machine learning is unlikely to succeed. And the particular supervised method has a special importance because of the possibility of a permanent correlation of the predictions with the result of real actions.\n\nThere are several natural ideas for applying the supervised learning.\n\nI. For every catastrophic situation, find out the exact sequence of steps and technical facilities which maximally decrease the damage. On the basis of the identified trends, it is possible to develop and check in practice clear guidelines to save lives and restore economic activities (for example, during and after the floods). Applying the scientific methods, in this case, means thousands of lives and quick recovering the economics. The useful features can evaluate disasters (areas, time period), damage (human lives, economic indicators) and recovery process (speed, effectiveness).\n\nII. The same techniques could be useful in the process of creating self-learning facilities of virtual reality in order to bring the process of their development to the real counterparts, predict it and make corrections in time. Here the set of concrete features is very individual and depends on the object. For example, it can be growth, flowering, etc. for the plant and its imitation.", "execution_count": null}, {"cell_type": "code", "metadata": {"_uuid": "551065f5f4633ae643878107e3890efed070b53a", "collapsed": true, "_cell_guid": "9627d162-2527-a66c-b7fd-d12c5ce2e089", "trusted": false, "_execution_state": "busy"}, "outputs": [], "source": "", "execution_count": null}], "nbformat": 4}