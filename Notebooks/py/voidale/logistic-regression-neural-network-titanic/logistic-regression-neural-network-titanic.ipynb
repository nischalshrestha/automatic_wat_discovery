{"cells":[{"metadata":{"_uuid":"efe1c016c31743856bbb713b11df15a2d74390f0","_cell_guid":"945ded65-a599-4daf-90da-d238fc056b2b","trusted":false,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebrxa\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.\n\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import model_selection, ensemble\nfrom scipy.stats import skew","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"755ecacb3770e6ea0ccba1dfbad1ff8fbb4fe845","_cell_guid":"b75acc57-454c-4a2b-be55-ca08b26259f6","trusted":false,"collapsed":true},"cell_type":"code","source":"train_data = pd.read_csv('../input/train.csv')\ntest_data = pd.read_csv('../input/test.csv')\ncombined = [train_data, test_data]\n\n# show column headers\nprint(train_data.columns.values, test_data.columns.values)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"52cc45b4c2aa00ecc6c9f54e54bc34cd8602d2d1","_cell_guid":"50bd95c9-2e32-48f6-9b2f-ac62393026a8","trusted":false,"collapsed":true},"cell_type":"code","source":"# print example\ntrain_data.head()\ntest_data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c600663361bf1761c0eb3823edbc09ea4a916e1c","scrolled":true,"_cell_guid":"d95d238d-96e2-43c8-887e-12f76387e7b5","trusted":false,"collapsed":true},"cell_type":"code","source":"# stats about the data\ntrain_data.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a42ece45235cbcad4081be70d9ac656905481945","_cell_guid":"1ff8c844-1c2b-43d4-8e9e-7679d4b26705"},"cell_type":"markdown","source":"## Test what might be meanful features to train our model on"},{"metadata":{"_uuid":"8662838c0c4280a1e7c25d880debcf7be96e5dd0","_cell_guid":"5251a7a5-0321-4d2e-84a3-bd0a78bad1fc","trusted":false,"collapsed":true},"cell_type":"code","source":"train_data[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"02996c92feeb37725e5e41c92d3bb358a21ae972","_cell_guid":"d9acd50b-1e75-4e2f-9b89-ea67f5b61ae8","trusted":false,"collapsed":true},"cell_type":"code","source":"train_data[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5095433e7e088b1eae99027dd08c8225d35f7860","_cell_guid":"bcb61c86-8f19-44b7-b928-22f4da19ccca","trusted":false,"collapsed":true},"cell_type":"code","source":"train_data[['Sex', 'Survived']].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"67062c1fdb7a4794a715adece378cf97fd17a09d","_cell_guid":"dcbdc6f4-6d1a-4707-9010-32dc64d6acb7","trusted":false,"collapsed":true},"cell_type":"code","source":"train_data[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"08a49975d08da22e39468b97c3f601897ce9c7bc","_cell_guid":"490eb869-6adc-46d3-87c4-ba31cf397ff6","trusted":false,"collapsed":true},"cell_type":"code","source":"train_data[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1fa5c8ea6928719b0053c2520f47f5e9b7562248","_cell_guid":"7a3c760f-bf5c-4347-804e-f80c96731960"},"cell_type":"markdown","source":"# Clean data"},{"metadata":{"_uuid":"6db44817a99d1ba671862fe0cc3a40d36ff7a495","_cell_guid":"450f5283-8dd3-4f1c-a801-127b0c561e31","trusted":false,"collapsed":true},"cell_type":"code","source":"# drop unneeded features\ntrain_data = train_data.drop(['PassengerId', 'Ticket', 'Cabin'], axis=1)\ntest_data = test_data.drop(['Ticket', 'Cabin'], axis=1)\n\n# train data skwed\nnumeric_feats = train_data.dtypes[train_data.dtypes != \"object\"].index\n\nskewed_feats = train_data[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\nskewed_feats = skewed_feats[skewed_feats > 0.75]\nskewed_feats = skewed_feats.index\n\ntrain_data[skewed_feats] = np.log1p(train_data[skewed_feats])\n\n# test data skwed\nnumeric_feats = test_data.dtypes[test_data.dtypes != \"object\"].index\n\nskewed_feats = test_data[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\nskewed_feats = skewed_feats[skewed_feats > 0.75]\nskewed_feats = skewed_feats.index\n\ntest_data[skewed_feats] = np.log1p(test_data[skewed_feats])\n\ncombine = [train_data, test_data]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b97630f2d413a92d75fb295514cf528bbfa856ed","_cell_guid":"f7d5103d-58ad-4453-a097-c690c88b1186","trusted":false,"collapsed":true},"cell_type":"code","source":"# Complete data\nfreq_port = train_data.Embarked.dropna().mode()[0]\n\nfor dataset in combine:\n    # complete fare with median\n    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)\n    \n    # use popular port for empty\n    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)\n    \n    # complete age with median\n    dataset['Age'].fillna(dataset['Age'].median(), inplace = True)\n\nprint (train_data.shape, test_data.shape)\nprint (train_data.isnull().sum(), test_data.isnull().sum())\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bcf8bf8cf5a65691c8192ca08703dbae94509e86","_cell_guid":"e1a27066-1954-4cf8-98cc-3b9e1b5d58bd","trusted":false,"collapsed":true},"cell_type":"code","source":"## Create more data\nfor dataset in combine:\n    dataset['FamilySize'] = dataset ['SibSp'] + dataset['Parch'] + 1\n    dataset['IsAlone'] = 1 #initialize to yes/1 is alone\n    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0 # now update to no/0 if family size is greater than 1\n    dataset['Title'] = dataset['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n    dataset['FareBin'] = pd.qcut(dataset['Fare'], 4)\n    #Age Bins/Buckets using cut or value bins: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html\n    dataset['AgeBin'] = pd.cut(dataset['Age'].astype(int), 5)\n    \n#cleanup rare title names\n#print(data1['Title'].value_counts())\nstat_min = 10 #while small is arbitrary, we'll use the common minimum in statistics: http://nicholasjjackson.com/2012/03/08/sample-size-is-10-a-magic-number/\ntitle_names = (train_data['Title'].value_counts() < stat_min) #this will create a true false series with title name as index\n\n#apply and lambda functions are quick and dirty code to find and replace with fewer lines of code: https://community.modeanalytics.com/python/tutorial/pandas-groupby-and-python-lambda-functions/\ntrain_data['Title'] = train_data['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)\n# print(train_data['Title'].value_counts())\n\n\n#preview data again\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9dc82308d6f79e72b805a805bbb97af5dced8744","_cell_guid":"1ccbff15-24f2-475a-9afa-31c9e060aafc","trusted":false,"collapsed":true},"cell_type":"code","source":"# convert using label encoder labels to numbers\n\n#code categorical data\nlabel = LabelEncoder()\nfor dataset in combine:    \n    dataset['Sex_Code'] = label.fit_transform(dataset['Sex'])\n    dataset['Embarked_Code'] = label.fit_transform(dataset['Embarked'])\n    dataset['Title_Code'] = label.fit_transform(dataset['Title'])\n    dataset['AgeBin_Code'] = label.fit_transform(dataset['AgeBin'])\n    dataset['FareBin_Code'] = label.fit_transform(dataset['FareBin'])\n    \n#define y variable aka target/outcome\nTarget = ['Survived']\n\n#define x variables for original features aka feature selection\ndata1_x = ['Sex','Pclass', 'Embarked', 'Title','SibSp', 'Parch', 'Age', 'Fare', 'FamilySize', 'IsAlone'] #pretty name/values for charts\ndata1_x_calc = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code','SibSp', 'Parch', 'Age', 'Fare'] #coded for algorithm calculation\ndata1_xy =  Target + data1_x\nprint('Original X Y: ', data1_xy, '\\n')\n\n#define x variables for original w/bin features to remove continuous variables\ndata1_x_bin = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code', 'FamilySize', 'AgeBin_Code', 'FareBin_Code']\ndata1_xy_bin = Target + data1_x_bin\nprint('Bin X Y: ', data1_xy_bin, '\\n')\n\n#define x and y variables for dummy features original\ndata1_dummy = pd.get_dummies(train_data[data1_x])\ndata1_x_dummy = data1_dummy.columns.tolist()\ndata1_xy_dummy = Target + data1_x_dummy\nprint('Dummy X Y: ', data1_xy_dummy, '\\n')\n\n\ndata1_dummy.head()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b5c83b36028aba4338790627ed797d7b8cd0afa2","_cell_guid":"e8ac119e-eec4-4355-b276-6eca2834a965","trusted":false,"collapsed":true},"cell_type":"code","source":"# double check for nulls\nprint (train_data.isnull().sum(), test_data.isnull().sum())\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9b9de46425470e245b7d8b349d4b018dabdd6ea9","_cell_guid":"c81800f0-3fab-4766-b3e2-0fea5e409e4e","trusted":false,"collapsed":true},"cell_type":"code","source":"#split train and test data with function defaults\ntrain1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(train_data[data1_x_calc], train_data[Target], random_state = 0, test_size = 0)\ntrain1_x_bin, test1_x_bin, train1_y_bin, test1_y_bin = model_selection.train_test_split(train_data[data1_x_bin], train_data[Target], random_state = 0, test_size = 0)\ntrain1_x_dummy, test1_x_dummy, train1_y_dummy, test1_y_dummy = model_selection.train_test_split(data1_dummy[data1_x_dummy], train_data[Target], random_state = 0, test_size = 0)\n\n\nprint(\"Data Shape: {}\".format(train_data.shape))\nprint(\"Train1 Shape: {}\".format(train1_x.shape))\nprint(\"Test1 Shape: {}\".format(test1_x.shape))\nprint(\"Train1_y Shape: {}\".format(train1_y.shape))\nprint(\"Test1_y Shape: {}\".format(test1_y.shape))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ce6dcc7a98d2bf076f084ae1c7bba10a7dc7bade","collapsed":true,"_cell_guid":"b95e99ea-c681-4ebc-9554-e4b77b9462b5","trusted":false},"cell_type":"code","source":"# alg = ensemble.RandomForestClassifier(n_estimators = 100)\n# # alg = LogisticRegression()\n# alg.fit()\n# cv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) \n# cv_results = model_selection.cross_validate(alg, train_data[data1_x_bin], train_data[Target], cv  = cv_split)\n# print(cv_results['fit_time'].mean(), cv_results['train_score'].mean(), cv_results['test_score'].mean(), cv_results['test_score'].min())\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e6802bf8261407f577834ac3e6f9b154e21a2002","_cell_guid":"6f16386f-7fe4-4778-96b7-e1147ea61707","trusted":false,"collapsed":true},"cell_type":"code","source":"# Logistic Regression\nX_train = train_data[data1_x_calc]\nY_train = train_data[Target]\nX_test  = test_data[data1_x_calc]\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\n\nraf = ensemble.RandomForestClassifier(n_estimators = 100)\nraf.fit(X_train, Y_train)\nY_pred2 = raf.predict(X_test)\nacc_log2 = round(raf.score(X_train, Y_train) * 100, 2)\n\nprint ('Logistic Regression: ', acc_log, 'Random Forest: ', acc_log2)\n\n# random_forest = RandomForestClassifier(n_estimators=100)\n# random_forest.fit(X_train, y)\n# random_forest_preds = random_forest.predict(X_test)\n# random_forest.score(X_train, y)\n# accuracy = round(random_forest.score(X_train, y) * 100, 2)\n# print(accuracy)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"13529858f235b432de5c25a9ffd193d8748dcd63","_cell_guid":"a3c9ec5f-75bf-4d2c-80a0-fd264f847221","trusted":false,"collapsed":true},"cell_type":"code","source":"submission = pd.DataFrame({\n        \"PassengerId\": test_data[\"PassengerId\"],\n        \"Survived\": Y_pred2\n    })\nsubmission.to_csv('random_forest_y_pred_2_f.csv', index=False)\nprint('done')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9ae4099943e89cab8b553f879265967dd069c7d1","collapsed":true,"_cell_guid":"c0b1aa67-b31d-4427-a42d-d959186fc1b8","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9e853a7653310645ef17661f5233af7589bd305e","collapsed":true,"_cell_guid":"d10a7644-44b9-469f-8b7d-44c26c51e1a8","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py","version":"3.6.4","codemirror_mode":{"version":3,"name":"ipython"},"mimetype":"text/x-python","name":"python"}},"nbformat":4,"nbformat_minor":1}