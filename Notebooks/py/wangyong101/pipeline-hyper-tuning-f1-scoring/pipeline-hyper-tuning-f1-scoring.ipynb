{"nbformat": 4, "cells": [{"cell_type": "markdown", "source": ["-"], "metadata": {"_uuid": "112b77596efbb3d51a04adc2df5efdb986142d38", "_cell_guid": "4e69a7fc-6e60-4c03-851f-bb79ebc611d2"}}, {"cell_type": "markdown", "source": ["# Introduction\n", "\n", "Preprocess the titantic data for next feature selection,union,standardize and etc"], "metadata": {"_uuid": "a315ca97d32b4e43d3728900da815063928be636", "_cell_guid": "963ae8cb-62e5-4129-b797-f6f885ab752e"}}, {"outputs": [], "cell_type": "code", "source": ["# load pandas, numpy, matplotlib, seaborn, re module\n", "import pandas as pd\n", "import numpy as np\n", "\n", "# Visualisation\n", "import re\n", "import seaborn as sns\n", "import matplotlib.pyplot as plt\n", "import matplotlib as mpl\n", "import matplotlib.pyplot as plt\n", "import matplotlib.pylab as pylab\n", "import seaborn as sns\n", "\n", "# Configure visualisations\n", "%matplotlib inline\n", "sns.set_style( 'white' )\n", "sns.set_palette(\"muted\")"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "799847db84a4605f438c818a0b5388cbfd7e15ee", "_cell_guid": "fe02474e-f76a-445d-a154-5701b2adc4bc"}}, {"cell_type": "markdown", "source": ["# load original data"], "metadata": {"_uuid": "868be6a25f7f1c84ef33a42fe8c2062b4564d2e4", "_cell_guid": "8fc2dc66-961a-4a33-a911-dd62aa60722e"}}, {"outputs": [], "cell_type": "code", "source": ["train=pd.read_csv(\"../input/train.csv\")\n", "test = pd.read_csv(\"../input/test.csv\")\n", "combined = train.append(test)\n", "train.shape, test.shape, combined.shape"], "execution_count": null, "metadata": {"_uuid": "150a78d9513f0b496b59eb6b6db7cef61b8c0bc6", "_cell_guid": "2685be00-c7b4-4f49-bdf3-1a5e3a062c02"}}, {"cell_type": "markdown", "source": ["## Quickly gothrough the target & feature info\n", "## Target(Survived)"], "metadata": {"_uuid": "9bb106c7db3dfa6688044aee60258a8af962ae24", "_cell_guid": "24fc8967-7c76-4001-9b81-2020a2273d5e"}}, {"outputs": [], "cell_type": "code", "source": ["Target = train.Survived\n", "fig, ax = plt.subplots(1,2,figsize=(10,5),)\n", "sns.countplot(x=Target,ax=ax[0])\n", "train.Survived.value_counts().plot(kind=\"pie\")\n", "# In training set, the more passenger were dead."], "execution_count": null, "metadata": {"_uuid": "30a4a23e941db553a0d60a4445c6c625bbe6cd69", "_cell_guid": "2a3a0456-abc7-4bf8-b195-9fc6b25180ce"}}, {"cell_type": "markdown", "source": ["## Train Features"], "metadata": {"_uuid": "404928d29773a5500f4b64372458348777b4262d", "_cell_guid": "7f5f0593-b607-44ef-985e-58dc5e4c93d4"}}, {"outputs": [], "cell_type": "code", "source": ["train.info()"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "9970f0171c06f28f7b8edaeda061afad6907fc35", "_cell_guid": "d2dcabb0-35f4-46d2-bbd3-0de0cae21f0c"}}, {"cell_type": "markdown", "source": ["## Missing Data identifing"], "metadata": {"_uuid": "f1a51f5a058f6755239a459b5d863b627dcc5057", "_cell_guid": "791e1d09-9fa8-46e8-815d-44b91511af85"}}, {"outputs": [], "cell_type": "code", "source": ["Missing = combined.isnull().sum()\n", "Existed = combined.notnull().sum()\n", "Missing = pd.concat([Missing,Missing+Existed],keys=[\"Missing\",\"Total\"],axis=1).sort_values(by=\"Missing\",ascending=False)\n", "Missing[Missing.Missing>0]"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "824ae692d9a71ca5e5c8868013aa13a45a4caa94", "_cell_guid": "d640d570-aeea-4c4d-bdcb-052c1af5f67a"}}, {"cell_type": "markdown", "source": ["- Fare    (missing value), should be handled later\n", "- Embarked(missing value), should be hanledd later\n", "- Age     (missing value), should be handled later\n", "- Cabin   (missing value), should be handled later. \n", "\n", "**Survived** should be predicted via classification. It is the ultimate target\n"], "metadata": {"_uuid": "d90b8018183d3960241422c8caf8bac61f3cc304", "_cell_guid": "4af0fdf7-315a-4dec-a221-55be5df5836d"}}, {"cell_type": "markdown", "source": ["# Exploring Data Analysis\n", "\n", "## initial a key features collector(factors)"], "metadata": {"_uuid": "049693077435f76e5d92dc6460c55429d86cc9d3", "_cell_guid": "07771a86-d49a-4143-abc3-487171dcc58c"}}, {"outputs": [], "cell_type": "code", "source": ["# factors used to collect key features which has contribution to target.\n", "factors={}"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "262fe68c34c5425258084404acf6567e1244d5e9", "_cell_guid": "a9c4bd5d-f040-4555-aebb-9e6f779b2dac"}}, {"outputs": [], "cell_type": "code", "source": ["factors[\"Survived\"]=1\n", "factors[\"PassengerId\"]=1\n", "#append feature to factors one by one\n", "# two must be selected: Survived, PassengerID. They are submssion required feature."], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "44a6bcbda660cbcaa7acd602f88ad45d280a0e14", "_cell_guid": "0ab200b1-a963-4d5c-a956-4a4d17a2e474"}}, {"cell_type": "markdown", "source": ["## Numberical Data Exploring"], "metadata": {"_uuid": "076cdc3d7114c81a0029a88a1d5c1f8c0a5d2ebd", "_cell_guid": "6264de27-2371-419c-917b-d346388f5279"}}, {"outputs": [], "cell_type": "code", "source": ["train.select_dtypes(include=[\"number\"]).describe()"], "execution_count": null, "metadata": {"collapsed": true, "scrolled": true, "_uuid": "c9538070a5f7e2676239f8edc0d25efb50585510", "_cell_guid": "93c80a82-78bf-41ff-831d-a177467205a5"}}, {"cell_type": "markdown", "source": ["### ploting\n", "Visualize ploting to see the relation with target(Survived)"], "metadata": {"_uuid": "3e58970a92e2ac973fd03ce38c9079dd9d1285b0", "_cell_guid": "12cbe14b-52af-4fbc-9d23-9de1fc877bec"}}, {"outputs": [], "cell_type": "code", "source": ["columns = train.select_dtypes(include=[\"number\"]).columns\n", "fig,ax =plt.subplots(2,len(columns),figsize=(21,10))\n", "for i in range(len(columns)):\n", "    col=columns[i]\n", "    data = train[col]\n", "    data = data[data.notnull()]\n", "    #data.hist(ax=ax[0,i])\n", "    #print(data.name)\n", "    sns.distplot(data,ax=ax[0,i],kde=False,label=None)\n", "    ax[0,i].set_xlabel(\"\")\n", "    ax[0,i].set_xticklabels([])\n", "    #sns.regplot(x=train[col], y=train.Survived,ax=ax[1,i]);\n", "    if col!=\"Survived\":\n", "        sns.regplot(x=train[col], y=train.Survived,ax=ax[1,i],logistic=True,marker=\"+\");\n", "    else:\n", "        sns.regplot(x=train[col], y=train.Survived,ax=ax[1,i],marker=\"+\");\n", "#plt.subplots_adjust(hspace=0,wspace=2)"], "execution_count": null, "metadata": {"collapsed": true, "scrolled": false, "_uuid": "8513deb77a13ffa02519c94337bb4accb2142be3", "_cell_guid": "9f556160-bf12-4ed3-bff2-3021cb8b4d4f"}}, {"cell_type": "markdown", "source": ["### collecting factors(Pclass, Age, SibSp, Parch, Fare)"], "metadata": {"_uuid": "6472f0b81487dee82fd819a256bf4ca9638daf60", "_cell_guid": "4f8d0eee-0b84-46cf-b560-074c550e40d5"}}, {"outputs": [], "cell_type": "code", "source": ["# following columns identified as key features\n", "# Pasenger ID was excluded as it looks like average distrubition. It hardly has any trends with the Survived. \n", "# Fare looks a bit of strange when higher than 200. It will be further explored later.\n", "\n", "columns = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare'] \n", "\n", "for i in columns:\n", "    factors[i]=1\n", "factors"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "7ee31ab512498b7c17a4ac09862fced799c454c0", "_cell_guid": "3e22df15-c4af-48b1-a9e8-09cd8220e27d"}}, {"cell_type": "markdown", "source": ["## Categories Data Exploring"], "metadata": {"_uuid": "3efd6cbb27596ac6e267c4caecafcc5b6694c1e3", "_cell_guid": "1c26bb3f-5ed3-45a2-89e0-53c9aac14c4d"}}, {"cell_type": "markdown", "source": ["### Categories describe\n", "\n", "by describe() the categories( or \"Object\") types unique date to see if the data is able to use directly or need further extraction.\n", "For example, the unique value less than 10 can be directly use. More than 10, it might need further handling."], "metadata": {"_uuid": "f7b5802f33268b3bff575062ae657e7190049126", "_cell_guid": "4c4b25d7-9ce1-4360-8c2c-56428612915a"}}, {"outputs": [], "cell_type": "code", "source": ["train.select_dtypes(include=[\"O\"]).describe()"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "f3cf7a2237022358ef9acee5bbca4f77a73d143f", "_cell_guid": "27c63ef2-fe0d-4e9d-bd30-b8c435f99b8f"}}, {"cell_type": "markdown", "source": ["- less categories: It could be used directly\n", "    - Sex(2), \n", "    - Embarked(3)\n", "    \n", "- more categories: Need further handling. Otherwise it might be noise to true signal. The entropy of these 3 feature is far higher than normal feature.\n", "        I used to use scipy entropy function to compare feature importance but not use in the notebook\n", "    - Name(891)\n", "    - Ticket(681)\n", "    - Cabin(147)"], "metadata": {"_uuid": "e29133ea62a8950a974f888bf667aad9eb643c47", "_cell_guid": "39c3a224-d169-4411-937e-55edcf3775a2"}}, {"cell_type": "markdown", "source": ["### Sex & Embarked related with Survial rate"], "metadata": {"_uuid": "e3c02c60ffbc9db4b4115953a371aad2d528cc05", "_cell_guid": "b0aa8225-bf21-43dc-b82d-e9f14f3e3750"}}, {"outputs": [], "cell_type": "code", "source": ["columns = [\"Survived\",\"Sex\",\"Embarked\"]\n", "\n", "fig,ax =plt.subplots(2,len(columns),figsize=(10,5))\n", "\n", "for i in range(len(columns)):\n", "    col=columns[i]\n", "    data = train[col]\n", "    data = data[data.notnull()]\n", "    \n", "    sns.countplot(x=col, data=train,ax=ax[0,i])\n", "    ax[0,i].set_xlabel(\"\")\n", "    ax[0,i].set_xticklabels([])\n", "    if col!=\"Survived\":\n", "        sns.pointplot(x=train[col],y=train.Survived,ax=ax[1,i]);\n", "    else:\n", "        pass\n", "plt.subplots_adjust(hspace=0.5,wspace=0.5)"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "bb1f58cf832bd80d557e37cacaf3c17f89bdeec0", "_cell_guid": "2bfc79f2-3649-4958-8049-2f6538880709"}}, {"cell_type": "markdown", "source": ["**Brief**:\n", "\n", "- Sex: (key factor)\n", "    - Around 600 man but around 20% survived. \n", "    - Around 300 women, around 80% survived\n", "\n", "- Embarked(key factor)\n", "    - around 600 passengers embarked at \"S\", around 35% survived\n", "    - around 200 passengers embarked at \"C\", aournd 55% survived\n", "    - around 100 passengers embarked at \"Q\", aournd 40% survived"], "metadata": {"collapsed": true, "_uuid": "9f42faaf2fcfb3d4e0a5ad68e015106f8ad16636", "_cell_guid": "b2bbe522-5f0d-4af9-850a-7fb7e140d61b"}}, {"cell_type": "markdown", "source": ["### Collecting factors(Sex, Embarked)"], "metadata": {"_uuid": "710429f1b23d135d51f2ccca5fce56b7f55e1455", "_cell_guid": "a8af44a1-c384-4646-8b34-fd786b81d9bb"}}, {"outputs": [], "cell_type": "code", "source": ["# following columns identified as key factors\n", "columns = [\"Sex\",\"Embarked\"] \n", "\n", "for i in columns:\n", "    factors[i]=1\n", "len(factors) ,factors"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "a04f064403c0ee8ec4c68378e5a705ff28236fb3", "_cell_guid": "9364b288-dae8-4326-b3d3-011e6106d412"}}, {"cell_type": "markdown", "source": ["# Feature abstracting(extracting)"], "metadata": {"collapsed": true, "_uuid": "61252ad594a4054dfad13396ab1cee46c52ec19f", "_cell_guid": "963547a4-b852-48dd-8b5a-aedf659a47b5"}}, {"cell_type": "markdown", "source": ["## Name(891)"], "metadata": {"_uuid": "00ac65d2cc125f0aef80a2920a162390f67f0ecf", "_cell_guid": "320a9872-5693-4115-849f-90b350b07930"}}, {"outputs": [], "cell_type": "code", "source": ["# Thanks for kaggle turorial \n", "# Titanic Data Science Solutions Python Notebook and other online resources. I saved time on this pieces.\n", "\n", "combined[\"t_titles\"] = combined.Name.str.extract(\"([A-Za-z]+)\\.\",expand = False)\n", "\n", "# a map of more aggregated titles\n", "title_map = { \n", "            \"Capt\":       \"Officer\",\n", "            \"Col\":        \"Officer\",\n", "            \"Major\":      \"Officer\",\n", "            \"Jonkheer\":   \"Royalty\",\n", "            \"Don\":        \"Royalty\",\n", "            \"Sir\" :       \"Royalty\",\n", "            \"Dr\":         \"Officer\",\n", "            \"Rev\":        \"Officer\",\n", "            \"the Countess\":\"Royalty\",\n", "            \"Dona\":       \"Royalty\",\n", "            \"Mme\":        \"Mrs\",\n", "            \"Mlle\":       \"Miss\",\n", "            \"Ms\":         \"Mrs\",\n", "            \"Mr\" :        \"Mr\",\n", "            \"Mrs\" :       \"Mrs\",\n", "            \"Miss\" :      \"Miss\",\n", "            \"Master\" :    \"Master\",\n", "            \"Lady\" :      \"Royalty\",\n", "            \"Countess\":   \"Royalty\"\n", "            }\n", "combined[\"t_titles\"] =combined[\"t_titles\"].map(title_map)\n", "combined[\"t_titles\"].value_counts()"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "484a65b94a34eb0ca181487b9e231f1b0db78e17", "_cell_guid": "9c4cf141-669a-4535-a838-149b8e3db657"}}, {"cell_type": "markdown", "source": ["### Added to factors if no missing"], "metadata": {"_uuid": "00e741babf537bc1723807bee7c4d72a9df70cb5", "_cell_guid": "6fa62fcb-1365-4a41-8b38-8d533abb8da2"}}, {"outputs": [], "cell_type": "code", "source": ["if combined.t_titles.isnull().sum()==0:\n", "    factors[\"t_titles\"]=1\n", "    print(len(factors),\"\\n\",factors)\n", "else:\n", "    print(\"Missing data.\")"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "4aec8929eb410d693f8fe718a2ac3d3512ef9c0d", "_cell_guid": "c52f85b3-52cd-4497-81d5-f72d9feeddd3"}}, {"cell_type": "markdown", "source": ["## Ticket(681)"], "metadata": {"_uuid": "c49f34643c1fb5e425f85be2fc777b8f827c83b8", "_cell_guid": "f3b56c83-9476-4d56-b078-f72f63e0c266"}}, {"outputs": [], "cell_type": "code", "source": ["#print(combined.Ticket.head())\n", "t_pre =combined.Ticket.str.extract(\"(?P<Pre>[A-Za-z/.]+[0-9]?)\",expand=False)\n", "t_pre = t_pre.str.replace(\"[/.]\",\"\")\n", "sns.pointplot(x=t_pre,y=combined.Survived) "], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "cad1ebde8f27caf285f3d4de32c9ffbaf95abfad", "_cell_guid": "0371a30c-454c-4c96-b03b-63b9aa1628f1"}}, {"cell_type": "markdown", "source": ["### added ticket_pre information to factors after check missing "], "metadata": {"_uuid": "e4d7fe8ee989a50403469a97eff54ca1d1b351bb", "_cell_guid": "a8a4d27c-bbb3-48c0-88c5-392ae786d03f"}}, {"outputs": [], "cell_type": "code", "source": ["t_pre[t_pre.isnull()]=\"NA\"  #fill the pre with \"NA\" for whose ticket no pre information.\n", "\n", "if t_pre.isnull().sum()==0:\n", "    combined[\"t_pre\"] = t_pre\n", "    factors[\"t_pre\"]=1\n", "    print(len(factors),\"\\n\",factors)\n", "else:\n", "    print(\"missing data\")\n", "    print(combined[combined[\"t_pre\"].isnull()].head(3))"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "52a7a38b369957c34b33216d42ef82aa5fcbd6a9", "_cell_guid": "0c28e76a-9ff6-41b4-a2a6-e41000d2d200"}}, {"cell_type": "markdown", "source": ["### added ticket_num info if missing handled"], "metadata": {"_uuid": "0c89cf79fb06ce63dccd449aa2c9184e32051916", "_cell_guid": "3f8d676a-bc22-43fc-8aca-6696fbeb635e"}}, {"outputs": [], "cell_type": "code", "source": ["t_num =combined.Ticket.str.extract(\"(?P<Num>[0-9]{3,10})\",expand=False)\n", "t_num =t_num.fillna(\"9999\") #there is no ticket number 9999, so use it as special number for not ticket number passenger.\n", "fig,ax =plt.subplots(1,1,figsize=(5,5))\n", "sns.regplot(t_num.astype(int),combined.Survived)\n", "t_num.astype(int).sort_values(ascending=False).head()"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "b07b6143faeacc729c87531ac20f62a67769a211", "_cell_guid": "1a1d1dc1-ab49-4bc1-a874-55a2ae5391af"}}, {"outputs": [], "cell_type": "code", "source": ["if t_num.isnull().sum()==0:\n", "    combined[\"t_num\"]=t_num.astype(int)\n", "    factors[\"t_num\"]=1\n", "    print(len(factors),\"\\n\",factors)\n", "else:\n", "    print(\"Missing Data\")\n", "    print(combined[combined[\"t_num\"].isnull().head(3)])"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "5e5be89e1ab376dda307724464695187f34fbc77", "_cell_guid": "f1d82b88-2b6f-48db-8b52-290ee2fea755"}}, {"cell_type": "markdown", "source": ["### 2nd round of ticket feature extracting(length, first number)"], "metadata": {"_uuid": "c02cbd1d71a97bc62131e430b6c23f412ec061d1", "_cell_guid": "7c3151c8-d8da-49df-b171-813c5fed1662"}}, {"outputs": [], "cell_type": "code", "source": ["#two differenct ways to extract the number length & value info. I choiced both feature. \n", "#In later featuring importance, the num_start show high important. one might be removed if further polish needed.\n", "\n", "t_num_log10 = np.log10(combined[\"t_num\"]).astype(\"int\") # the length calculated by log10 with higher confidence  \n", "#t_num_len = t_num.str.len()  \n", "t_num_start=t_num.str.get(0)\n", "#t_num_start2=t_num.str.slice(0,1)\n", "\n", "#cols = [t_num_log,t_num_len,t_num_start,t_num_start2]\n", "cols = [t_num_log10,t_num_start]\n", "\n", "fig,ax=plt.subplots(2,len(cols),figsize=(10,5))\n", "Y = combined.Survived\n", "for i in range(len(cols)):\n", "    col=cols[i]\n", "    sns.regplot(col.astype(\"int\"),Y,ax=ax[0,i])    \n", "    sns.pointplot(col,Y,ax=ax[1,i])\n"], "execution_count": null, "metadata": {"collapsed": true, "scrolled": true, "_uuid": "e3f0bc8c7da9fa94655a6b18432562f11e2fc629", "_cell_guid": "eb190afe-3513-4c30-b367-6767eb9e5d3b"}}, {"cell_type": "markdown", "source": ["#### added ticket_number_length, ticket_number_log feature if missing data is handled."], "metadata": {"_uuid": "73c2a22b3b7c61a9c40f018925c15de0e54920bf", "_cell_guid": "4f8583a5-334b-4e4e-b4e9-cfd4f952426c"}}, {"outputs": [], "cell_type": "code", "source": ["mask1 = t_num_start.isnull()\n", "mask2 = t_num_log10.isnull()\n", "if mask1.sum()==0 and mask2.sum()==0 :\n", "    combined[\"t_num_start\"]=t_num_start\n", "    combined[\"t_num_log10\"]=t_num_log10\n", "    factors[\"t_num_start\"]=1\n", "    factors[\"t_num_log10\"]=1\n", "    print(len(factors),\"\\n\",factors)\n", "else:\n", "    print(\"Missing Data\")\n", "    print(combined.loc[mask1])\n", "    print(combined.loc[mask2])\n", "    \n", "    "], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "e3dfecd63caaff42bea1da4f7f01bec58c8c0749", "_cell_guid": "9c042f36-b39d-4e3f-b1c1-341c17f43e37"}}, {"cell_type": "markdown", "source": ["### 3rd round ticket extraction - shareticket\n", "\n", "- the feature was found when fill the fare ticket\n", "- the 3rd class ticket fare was unreasonable high when multiple people share ticket. \n", "    - reason A: the parch or Sibsp could help to answer for family trip with one ticket\n", "    - reason B: the people shared same ticket id but without Parch or Sibsp. Is it group trip?\n", "\n", "- extract ** t_nshare feature ** to help understanding the reason)\n", "    - later the fare ticket will be extracted and f_Single(unit ticket price) will be collected\n"], "metadata": {"_uuid": "f6c8da48ca8b20f61c475f089392044ddf031d78", "_cell_guid": "956814c2-2cbc-4b30-94c8-28ccc6af97e3"}}, {"outputs": [], "cell_type": "code", "source": ["t_nShare = combined.Ticket.value_counts()\n", "t_nShare=combined.Ticket.replace(t_nShare.index.values,t_nShare.tolist())\n", "fig,ax = plt.subplots(1,2,figsize = (10,5))\n", "sns.pointplot(x=t_nShare,y=combined.Survived,ax=ax[0])\n", "sns.regplot(x=t_nShare,y=combined.Survived,ax=ax[1])"], "execution_count": null, "metadata": {"collapsed": true, "scrolled": false, "_uuid": "cd93f695c068d4188d9c81966b0be1fc4b959bee", "_cell_guid": "8d8c1788-7dab-4156-a819-0e2c532a2295"}}, {"outputs": [], "cell_type": "code", "source": ["mask = t_nShare.isnull()\n", "\n", "if mask.sum()==0:\n", "    combined[\"t_nShare\"]=t_nShare\n", "    factors[\"t_nShare\"]=1\n", "    print(len(factors),\"\\n\",factors)\n", "else:\n", "    print(\"Missing Data\")\n", "    print(combined.loc[mask])\n", "    \n", "    "], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "b1dc774ef942be2c64c644b014aea8b21df61061", "_cell_guid": "6fe17394-8fdc-4959-b939-70c96b63e66f"}}, {"cell_type": "markdown", "source": ["## Cabin(147)\n", "\n", "- ** to much of missing( more than 1014 in total)**\n", "- handle it after missing handling\n"], "metadata": {"_uuid": "93d291f77865227238e85747a562cc1169a64912", "_cell_guid": "bac8b4af-f55f-4517-8967-c40cf84fecf9"}}, {"cell_type": "markdown", "source": ["## Family Size"], "metadata": {"_uuid": "5ddda02d901a2dce343ec45ed4e800dd92dd9d2e", "_cell_guid": "4b13a4ad-a9d0-49ff-9cfc-f3f4c60e425e"}}, {"outputs": [], "cell_type": "code", "source": ["FamilySize = combined[\"SibSp\"] + combined[\"Parch\"]\n", "\n", "fig,ax = plt.subplots(1,2,figsize = (10,5))\n", "sns.pointplot(x=FamilySize,y=combined.Survived,ax=ax[0])\n", "sns.regplot(x=FamilySize,y=combined.Survived,ax=ax[1])"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "b5d66df4166ad23ab015dde375a59650a9a8cf2f", "_cell_guid": "cfd24a74-4410-4655-a7d3-8eb2ca1203e2"}}, {"outputs": [], "cell_type": "code", "source": ["mask = FamilySize.isnull()\n", "if mask.sum()==0:\n", "    combined[\"FamilySize\"]=FamilySize\n", "    factors[\"FamilySize\"]=1\n", "    print(len(factors),\"\\n\",factors)\n", "else:\n", "    print(\"Missing Data\")\n", "    print(combined.loc[mask])  \n"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "882bad91a6310da7bee72198e86ff48488897f71", "_cell_guid": "748cc16e-e8e3-4c39-af44-b150ee53146a"}}, {"cell_type": "markdown", "source": ["# Missing Handling(from min missing columns to highest)"], "metadata": {"_uuid": "c39ecf31f2e0acb10f6f8b422423c3914650c11c", "_cell_guid": "fb138d24-e7b7-4564-a0c0-ddddb2847b6a"}}, {"cell_type": "markdown", "source": ["## Fare (1 missing)"], "metadata": {"_uuid": "0ede69a65d3cabc82a73e0e71bb7f91c54429efd", "_cell_guid": "219ddd2f-a67b-4f42-aeef-fbc74f89d9be"}}, {"outputs": [], "cell_type": "code", "source": ["mask = combined.Fare.isnull()\n", "combined[mask]\n"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "147a387a99ab4a1422ae6fedfd9980f1eacbcb73", "_cell_guid": "035e0c52-e00c-440c-a614-253aad0a7d51"}}, {"outputs": [], "cell_type": "code", "source": ["# use R2 coefficient to determine to feature contribution to Fare\n", "(combined.corr()**2).sort_values(by=\"Fare\",ascending=False).Fare"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "7cd3ce4bc95def98a8a8a527860940e2940ba321", "_cell_guid": "21287300-8e76-4ad9-a32b-3827d9d9640b"}}, {"outputs": [], "cell_type": "code", "source": ["\n", "pd.pivot_table(combined[combined.t_nShare==1],columns=\"Pclass\",values=\"Fare\",index=[\"t_nShare\",\"Parch\"],aggfunc=\"median\")"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "2a1692c408d3f70f82f50e72a517661e02cbde93", "_cell_guid": "34d8d0c4-c791-4b62-b327-b05ca82a883a"}}, {"outputs": [], "cell_type": "code", "source": ["mask1 = combined.t_titles == \"Mr\" \n", "mask2 = combined.Pclass ==3\n", "mask3 = combined.Parch == 0\n", "mask4 = combined.t_nShare ==1\n", "mask = mask1 & mask2 & mask3 & mask4\n", "\n", "fill_fare =combined[mask].Fare.median()\n", "fill_fare"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "ef96e55eea6d77090bdecec9e19a98ca5f75f89e", "_cell_guid": "c56215f1-e3e8-4d30-b470-b2abd678bb6b"}}, {"cell_type": "markdown", "source": ["### added Fare to factors if missing handled"], "metadata": {"_uuid": "cd773f5d1f0ff7cc48d583ffbf4ae7d2f98af3c3", "_cell_guid": "c2142620-09b1-41c7-a1b8-b8146e7ee758"}}, {"outputs": [], "cell_type": "code", "source": ["Fare=combined[\"Fare\"].fillna(fill_fare)\n", "\n", "mask = Fare.isnull()\n", "if mask.sum()==0:\n", "    combined[\"Fare\"]=Fare\n", "    factors[\"Fare\"]=1\n", "    print(len(factors),\"\\n\",factors)\n", "else:\n", "    print(\"Missing Data\")\n", "    print(combined.loc[mask,\"Fare\"])\n", "    "], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "412f9a0c5135326430ff1f1b22ab02e425988664", "_cell_guid": "0c9c58e2-e70e-4ed9-8e76-afc8cb5f5672"}}, {"cell_type": "markdown", "source": ["### f_Single feature \n", "- assume the fare price is the unite price * shared ticket id people number"], "metadata": {"_uuid": "495d045b4368e8621e0351e3222d42ccd0e1f62f", "_cell_guid": "5c73752f-7b31-4679-949a-d1760345acea"}}, {"outputs": [], "cell_type": "code", "source": ["combined[combined.Fare.isnull()]"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "b7db183835393de50d6f89c7f3284b8642671ed3", "_cell_guid": "8448ff36-5f9d-49c2-91cb-8f90b9a87341"}}, {"outputs": [], "cell_type": "code", "source": ["f_Single=combined.Fare/combined.t_nShare\n", "\n", "fig,ax = plt.subplots(1,2,figsize = (10,5))\n", "sns.distplot(f_Single,ax=ax[0],kde=False)\n", "sns.regplot(x=f_Single,y=combined.Survived,ax=ax[1],logistic=True)"], "execution_count": null, "metadata": {"collapsed": true, "scrolled": true, "_uuid": "f48572dcfaab7388b910e1282b072acadf89c631", "_cell_guid": "9730cd0d-0ddf-4479-9066-01dca0bcbaa7"}}, {"outputs": [], "cell_type": "code", "source": ["mask = f_Single.isnull()\n", "\n", "if mask.sum()==0:\n", "    combined[\"f_Single\"]=f_Single\n", "    factors[\"f_Single\"]=1\n", "    print(len(factors),\"\\n\",factors)\n", "else:\n", "    print(\"Missing Data\")\n", "    print(combined.loc[mask])\n", "    "], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "6ff7af9c8edbc2915cdce6f8c85dc644804ffca6", "_cell_guid": "de77ed96-7a46-433f-b833-700b93bb12e2"}}, {"cell_type": "markdown", "source": ["## Cabin (1014 missing)"], "metadata": {"_uuid": "d2c9fb470a90bf272a9a0b0b0d0b4249ac2cebb2", "_cell_guid": "2b387a85-4d2e-46c2-bc7a-edbbcbbaa1d3"}}, {"outputs": [], "cell_type": "code", "source": ["# too much of missing, set all missing as \"NA\"\n", "c_cabin = combined.Cabin.fillna(\"NA\")\n", "c_pre =c_cabin.str.extract(\"(?P<Pre>[A-Za-z/.])\",expand=False)"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "1a6f52f993854f4c49544b8f744942722a541715", "_cell_guid": "c698b204-784e-408f-9884-72de3d5d4d7a"}}, {"outputs": [], "cell_type": "code", "source": ["fig,ax=plt.subplots(1,2,figsize=(10,5))\n", "sns.pointplot(x=c_pre,y=combined.Survived,ax=ax[0])\n", "sns.barplot(x=c_pre,y=combined.Survived,ax=ax[1])"], "execution_count": null, "metadata": {"collapsed": true, "scrolled": false, "_uuid": "9e6230592477a41f629b2a31ba9036f50f87e844", "_cell_guid": "28fc09a7-8822-49aa-b4fe-8120e71002dd"}}, {"outputs": [], "cell_type": "code", "source": ["mask = c_pre.isnull()\n", "\n", "if mask.sum()==0:\n", "    combined[\"c_pre\"]=c_pre\n", "    factors[\"c_pre\"]=1\n", "    print(len(factors),\"\\n\",factors)\n", "else:\n", "    print(\"Missing Data\")\n", "    print(combined.loc[mask])"], "execution_count": null, "metadata": {"collapsed": true, "scrolled": true, "_uuid": "fb13d21bbecf9e96b56b283e34479e99ed7e59f3", "_cell_guid": "bef6d0a0-3f0d-4df1-94aa-b09ce51d539c"}}, {"cell_type": "markdown", "source": ["## Embarked(2 missing)"], "metadata": {"_uuid": "3b07c2386ef95099745cc28328bcef23599b6369", "_cell_guid": "18bf9608-7fe8-4f65-b577-4023d6d87d75"}}, {"outputs": [], "cell_type": "code", "source": ["print(combined[combined.Embarked.isnull()])\n", "mask1 =combined.Embarked.notnull()\n", "mask2 = combined.f_Single >=40\n", "#mask3 =combined.c_pre==\"B\" # too much of missing value, do not use it.\n", "mask4 = combined.t_num_start ==\"1\"\n", "mask5 = combined.Pclass ==1\n", "mask6 = combined.t_pre ==\"NA\"\n", "mask = mask1&mask2&mask4 & mask5 & mask6\n", "\n", "sns.violinplot(x=\"t_num_log10\",y=\"Embarked\",data=combined[mask],logistic=False)"], "execution_count": null, "metadata": {"collapsed": true, "scrolled": false, "_uuid": "dbd1015c09181d5109c9b7887514c43d3d53a905", "_cell_guid": "9d01177c-e9d3-4a7e-bf74-8b2022f65081"}}, {"cell_type": "markdown", "source": ["** Set the Embarked missing value as \"NA\". It could not be handled from above analysis **"], "metadata": {"_uuid": "bfec927a8590ebc3a7b927a792afcd2a110f9a7f", "_cell_guid": "db788b9e-0316-4856-bf7e-8328cf089eb7"}}, {"outputs": [], "cell_type": "code", "source": ["Embarked=combined[\"Embarked\"].fillna(\"NA\")\n", "\n", "mask = Embarked.isnull()\n", "\n", "if mask.sum()==0:\n", "    combined[\"Embarked\"]=Embarked\n", "    factors[\"Embarked\"]=1\n", "    print(len(factors),\"\\n\",factors)\n", "else:\n", "    print(\"Missing Data\")\n", "    print(combined.loc[mask])\n"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "cfcb95052911d101f3e372d3eea3fa8391eb11c6", "_cell_guid": "2b31b019-80c4-4ee4-abc4-fdaa0e0e8c1f"}}, {"cell_type": "markdown", "source": ["## Age (263 Missing)\n", "\n", "** Age could be better predict by other features. but it too complicated. Deal it in next version"], "metadata": {"_uuid": "37e8f3df5a4b0bbba3d835b6e719b29fc2d77cb1", "_cell_guid": "d952a707-f05d-4c79-866d-42c2bdbb8a33"}}, {"outputs": [], "cell_type": "code", "source": ["AgeR2 = combined.corr().Age **2  # R2 = square of corr. R2 is the deterination coefficent\n", "print(AgeR2.sort_values(ascending=False))"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "f27063b91c7c07c921602b2e17fab7b0ae806374", "_cell_guid": "01fb57a4-b808-45d3-a135-c4996880f0c0"}}, {"outputs": [], "cell_type": "code", "source": ["pd.pivot_table(data=combined, columns=[\"t_titles\"],index=[\"Pclass\",\"SibSp\"],values=\"Age\")"], "execution_count": null, "metadata": {"collapsed": true, "scrolled": false, "_uuid": "b514184d78cb0893d2ed04077ca5256ba9f6cb63", "_cell_guid": "7f1edab4-806e-42e2-955a-0ff7bcf3fa78"}}, {"outputs": [], "cell_type": "code", "source": ["g=sns.FacetGrid(data=combined,col=\"t_titles\",row=\"Pclass\")\n", "g.map(sns.distplot, \"Age\")"], "execution_count": null, "metadata": {"collapsed": true, "scrolled": true, "_uuid": "b764c3527800e78526e4c23bb0c2809a423d5511", "_cell_guid": "39617764-f626-402a-8537-297066dbd0ec"}}, {"cell_type": "markdown", "source": ["Too complicated, Need handle by regression model. Deal it later.\n", "The main factors could be used for fill Age missing values are followed:\n", "\n", "- t_titles\n", "- Pclass\n", "- f_Single\n", "- SibSp\n", "- Parch\n"], "metadata": {"_uuid": "8db0970d159f6c1c33435c463985aa82d5caf8e5", "_cell_guid": "1d81f1ff-9089-41e1-89c1-7518722a3b45"}}, {"cell_type": "markdown", "source": ["### fill age missing values"], "metadata": {"collapsed": true, "_uuid": "17f2e2d85c1356638e164df30776ac0c5ec820e0", "_cell_guid": "d60531c6-7ae8-459d-b5c6-23c82d2610c7"}}, {"outputs": [], "cell_type": "code", "source": ["group_key = [\"Pclass\",\"Embarked\",\"t_titles\"] # Remove f_single as more missing value returned.\n", "fill_Age_mean = lambda g: g.fillna(g.mean())\n", "fill_Age_result=combined.groupby(group_key).Age.transform(fill_Age_mean)\n"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "061956135e906ca31ddc2adf8aad162d4d8dd9ec", "_cell_guid": "ce190c7d-95f2-45dd-afe4-960b304c9e75"}}, {"outputs": [], "cell_type": "code", "source": ["mask = fill_Age_result.isnull()\n", "\n", "if mask.sum()==0:\n", "    combined[\"Age\"]=fill_Age_result\n", "    factors[\"Age\"]=1\n", "    print(len(factors),\"\\n\",factors)\n", "else:\n", "    print(\"Missing Data\")\n", "    print(combined.loc[mask])"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "848b56859e0bba5817474713a968530394f07d8e", "_cell_guid": "e685f872-cc19-4cb4-8798-3f2269fb216c"}}, {"outputs": [], "cell_type": "code", "source": ["np.log10(50)"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "0dc7b1a52320e93f0e2de1db60acf7493c611da7", "_cell_guid": "9608cee6-eac1-4640-9934-d66d072c8f84"}}, {"cell_type": "markdown", "source": ["# Encoding categorical features\n", "Transform the string categories features to numerical data. SVM, GradientBoost and lineregression related estimators like number instead of string"], "metadata": {"_uuid": "5ebf7743fcc4497e30e9176e45a5c225380e2f53", "_cell_guid": "81a7be9b-e787-4006-a13d-9a99452827b8"}}, {"outputs": [], "cell_type": "code", "source": ["from sklearn import preprocessing\n", "le = preprocessing.LabelEncoder()"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "affeb1748acd6e6103efb8ab6ae9e3ff5453a2f4", "_cell_guid": "ebd9f4f7-5682-4509-8476-944c94df9774"}}, {"outputs": [], "cell_type": "code", "source": ["tmp = combined[list(factors.keys())]\n", "cols = tmp.select_dtypes(include=[\"O\"]).columns"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "5e4265ac9874a80c0fe874f29aed3e1ed3d9b63a", "_cell_guid": "180cc387-3037-4c1b-8de9-d137d8e62155"}}, {"outputs": [], "cell_type": "code", "source": ["for i in cols[:]:\n", "    print(i)\n", "    le.fit(tmp[i])\n", "    Encoded=le.transform(tmp[i])\n", "    combined[i]=Encoded\n", "combined.head()"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "85098576d029e36a60e9885f4d7f48c3c9dddd4a", "_cell_guid": "54cddf55-8da2-4b40-a1c7-26f9ba591267"}}, {"cell_type": "markdown", "source": ["# Write the preprocess data into database"], "metadata": {"_uuid": "8eeb565a50b100551f897ff54fca02ea18a74616", "_cell_guid": "33036366-41dd-420b-b5b0-94a2413f7b78"}}, {"outputs": [], "cell_type": "code", "source": ["combined[list(factors.keys())].info()"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "9245aaaf6f70575e2817b972daa4cd6671a101b0", "_cell_guid": "bc375c81-5583-47de-836f-f05640c3bf45"}}, {"outputs": [], "cell_type": "code", "source": ["day =\"2017_9_21\"\n", "version=\"2\"\n", "fname = \"PreProcess\"+ day +\"_v\"+version+\".h5\"\n", "combined_pre = combined[list(factors.keys())]\n", "combined_pre.to_hdf(fname,\"pre\")\n", "combined_pre.shape, pd.read_hdf(fname,\"pre\").shape"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "a7b7e497abba1aa54a1fb67535044e0bd1a22542", "_cell_guid": "b5063d15-faa4-471e-abb4-aea80e301458"}}, {"cell_type": "markdown", "source": ["# Summary\n", "- Features(15) collected in to factors\n", "    - Feature selected(4)\n", "        - Pclass\n", "        - SibSp\n", "        - Parch\n", "        - Sex      \n", "    - Feature missing valued filled(3)\n", "        - Age\n", "        - Fare\n", "        - Embarked\n", "        - Cabin  (fill & added c_pre feature)\n", "    - new Feature Added(8)\n", "        - t_title\n", "        - t_pre\n", "        - t_num\n", "        - t_num_start\n", "        _ t_num_log10\n", "        - t_num_nShare\n", "        - f_Single\n", "        - c_pre\n"], "metadata": {"collapsed": true, "_uuid": "aa4c7945f5ff5e27fef132b8163ee69bbc874aa6", "_cell_guid": "2c3be3ab-f3c4-4de1-92f8-78b04e61035d"}}, {"cell_type": "markdown", "source": ["# Stage 2 - Prediction\n"], "metadata": {"collapsed": true, "_uuid": "649d0cd259576cf0fba343e57a94ed33cbf8f2e7", "_cell_guid": "fb0366af-5fce-425e-b6c7-235348a6ee39"}}, {"outputs": [], "cell_type": "code", "source": ["import pandas as pd\n", "import numpy as np\n", "\n", "import scipy.stats as stats\n", "import seaborn as sns\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline\n", "\n", "# Going to use these 5 base models for the stacking\n", "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n", "from sklearn.svm import SVC\n", "from sklearn.cross_validation import KFold;\n", "from sklearn.model_selection import cross_val_score\n", "\n", "import plotly.offline as py\n", "py.init_notebook_mode(connected=True)\n", "import plotly.graph_objs as go\n", "import plotly.tools as tls\n", "import re\n", "import sklearn\n", "import xgboost as xgb\n", "import warnings\n", "warnings.filterwarnings('ignore')\n"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "9af869b6180f9e5fc1ceccbe41b4b8a999b88acb", "_cell_guid": "313148b4-292e-450e-b8b8-9e9995381bd4"}}, {"outputs": [], "cell_type": "code", "source": ["combined =pd.read_hdf(\"PreProcess2017_9_21_v2.h5\",\"pre\")\n", "train=combined[:891]\n", "test=combined[891:]\n", "\n", "drop_col=[\"Survived\",\"PassengerId\"]\n", "X_train=combined.drop(drop_col,axis=1)[:891]\n", "X_test =combined.drop(drop_col,axis=1)[891:]\n", "y_train =combined[\"Survived\"][:891]\n", "PassengerId = combined[\"PassengerId\"][891:]\n", "x_train_cols=X_train.columns\n", "print(combined.shape,X_train.shape,X_test.shape,y_train.shape,PassengerId.shape)\n", "print(x_train_cols)"], "execution_count": null, "metadata": {"collapsed": true, "scrolled": true, "_uuid": "03f16fba93ff1ae021aebc1425941415dc106fc2", "_cell_guid": "13d2d279-571b-4b50-af3c-3ee0502d25a2"}}, {"cell_type": "markdown", "source": ["# Change 1 - \n", "## feature selections\n", "** Univariate feature selection**\n", "\n", "Univariate feature selection works by selecting the best features based on univariate statistical tests. It can be seen as a preprocessing step to an estimator. Scikit-learn exposes feature selection routines as objects that implement the transform method:\n", "\n", "- SelectKBest removes all but the k highest scoring features\n", "\n", "- Remove the code below\n", "\n", "\n", "** Update select the KBest after tuning the parameter. Not now **"], "metadata": {"_uuid": "afbd5808ebe67e7a460ee02b34f00100ba7303b3", "_cell_guid": "8c2c33b7-d6ae-4014-9122-156ff8f34694"}}, {"outputs": [], "cell_type": "code", "source": ["from sklearn.feature_selection import SelectKBest\n", "from sklearn.feature_selection import chi2, f_classif,mutual_info_classif\n", "#x_train, x_test, y_train\n", "sel = SelectKBest(mutual_info_classif,k=14)\n", "# k less than 10, lead to mean socre descrease\n", "# k = all, the standard dieviation is a bit of high. some feature might be noise\n", "x_train = sel.fit_transform(X_train, y_train)\n", "mask =sel.get_support()\n", "x_test=X_test.loc[:,mask]\n", "print(x_train.shape,x_test.shape)"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "98bee5f018669326f5149ee18a8e2d7dd988f0c4", "_cell_guid": "e74b78fd-4df5-4a66-bc3b-7ac583eabdd7"}}, {"outputs": [], "cell_type": "code", "source": ["print(\"select features:\",x_train_cols[mask])\n", "print(\"Dropped Features:\",x_train_cols[~mask])\n", "#improve in future"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "03474b8299dd37e3dcc76588966fd43511a0a141", "_cell_guid": "d3022bb8-4d5c-4b9e-b96a-901b557e667d"}}, {"cell_type": "markdown", "source": ["## Feature standardization\n", "\n", "Standardization of datasets is a common requirement for many machine learning estimators implemented in scikit-learn; they might behave badly if the individual features do not more or less look like standard normally distributed data: Gaussian with zero mean and unit variance.\n", "In practice we often ignore the shape of the distribution and just transform the data to center it by removing the mean value of each feature, then scale it by dividing non-constant features by their standard deviation.\n", "\n", "For instance, many elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the l1 and l2 regularizers of linear models) assume that all features are centered around zero and have variance in the same order. If a feature has a variance that is orders of magnitude larger than others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.\n", "\n", "The function scale provides a quick and easy way to perform this operation on a single array-like dataset:"], "metadata": {"_uuid": "3bf81488ebef30ac6952c1701149282fe9517833", "_cell_guid": "495debec-697d-46ec-838c-a181b88fca97"}}, {"outputs": [], "cell_type": "code", "source": ["#Standardization, or mean removal and variance scaling\n", "from sklearn import preprocessing\n", "\n", "x_train = preprocessing.scale(x_train)\n", "x_test =preprocessing.scale(x_test)\n", "x_train.shape,x_test.shape"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "4925a46d9eed5ba084b6faaa9fa60c9d8e6ec229", "_cell_guid": "492daa10-36b5-4fee-930d-e35bb76a5c80"}}, {"cell_type": "markdown", "source": ["# Ensembling & Stacking models\n", "Finally after that brief whirlwind detour with regards to feature engineering and formatting, we finally arrive at the meat and gist of the this notebook.\n", "Creating a Stacking ensemble\n", "\n", "## Helpers via Python Classes\n", "Here we invoke the use of Python's classes to help make it more convenient for us. For any newcomers to programming, one normally hears Classes being used in conjunction with Object-Oriented Programming (OOP).   \n", "In short, a class helps to extend some code/program for creating objects (variables for old-school peeps) as well as to implement functions and methods specific to that class.  \n", "\n", "In the section of code below, we essentially write a class SklearnHelper that allows one to extend the inbuilt methods (such as train, predict and fit) common to all the Sklearn classifiers.  \n", "Therefore this cuts out redundancy as won't need to write the same methods five times if we wanted to invoke five different classifiers."], "metadata": {"_uuid": "064e9b5a0cf28484a0e02ed1360d9366ccad35bb", "_cell_guid": "304c1256-228a-4215-bfb3-ae74b73e6a77"}}, {"outputs": [], "cell_type": "code", "source": ["# Some useful parameters which will come in handy later on\n", "ntrain = train.shape[0]\n", "ntest = test.shape[0]\n", "SEED = 42 # for reproducibility\n", "NFOLDS = 5 # set folds for out-of-fold prediction\n", "kf = KFold(ntrain, n_folds= NFOLDS)\n", "\n", "# Class to extend the Sklearn classifier\n", "class SklearnHelper(object):\n", "    def __init__(self, clf, seed=0, params=None):\n", "        params['random_state'] = seed\n", "        self.clf = clf(**params)\n", "\n", "    def train(self, x_train, y_train):\n", "        self.clf.fit(x_train, y_train)\n", "\n", "    def predict(self, x):\n", "        return self.clf.predict(x)\n", "    \n", "    def fit(self,x,y):\n", "        return self.clf.fit(x,y)\n", "    \n", "    def feature_importances(self,x,y):\n", "        return self.clf.fit(x,y).feature_importances_\n", "    \n", "# Class to extend XGboost classifer"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "fd7ad3632846398111d5bd25e9ca0447a1fcd892", "_cell_guid": "227424aa-01d1-42ca-ad01-db101778f8ff"}}, {"outputs": [], "cell_type": "code", "source": ["def get_oof(clf, x_train, y_train, x_test):\n", "    oof_train = np.zeros((ntrain,))\n", "    oof_test = np.zeros((ntest,))\n", "    oof_test_skf = np.empty((NFOLDS, ntest))\n", "\n", "    for i, (train_index, test_index) in enumerate(kf):\n", "        x_tr = x_train[train_index]\n", "        y_tr = y_train[train_index]\n", "        x_te = x_train[test_index]\n", "\n", "        clf.train(x_tr, y_tr)\n", "\n", "        oof_train[test_index] = clf.predict(x_te)\n", "        oof_test_skf[i, :] = clf.predict(x_test)\n", "\n", "    oof_test[:] = oof_test_skf.mean(axis=0)\n", "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)\n", "\n", "from sklearn.model_selection import learning_curve\n", "\n", "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n", "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5), scoring='accuracy'):\n", "    plt.figure(figsize=(5,5))\n", "    plt.title(title)\n", "    if ylim is not None:\n", "        plt.ylim(*ylim)\n", "    plt.xlabel(\"Training examples\")\n", "    plt.ylabel(scoring)\n", "    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, scoring=scoring,\n", "                                                            n_jobs=n_jobs, train_sizes=train_sizes)\n", "    train_scores_mean = np.mean(train_scores, axis=1)\n", "    train_scores_std = np.std(train_scores, axis=1)\n", "    test_scores_mean = np.mean(test_scores, axis=1)\n", "    test_scores_std = np.std(test_scores, axis=1)\n", "    plt.grid()\n", "\n", "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n", "                     train_scores_mean + train_scores_std, alpha=0.1,\n", "                     color=\"r\")\n", "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n", "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n", "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n", "             label=\"Training score\")\n", "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n", "             label=\"Cross-validation score\")\n", "\n", "    plt.legend(loc=\"best\")\n", "    return plt"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "c3dc52e116b380b247b541ebab88f10967ec8349", "_cell_guid": "de3cf23e-c049-4744-8596-82eb9c869242"}}, {"cell_type": "markdown", "source": ["## Generating our Base First-Level Models\n", "So now let us prepare five learning models as our first level classification. These models can all be conveniently invoked via the Sklearn library and are listed as follows:\n", "1. Random Forest classifier\n", "2. Extra Trees classifier\n", "3. AdaBoost classifer\n", "4. Gradient Boosting classifer\n", "5. Support Vector Machine\n"], "metadata": {"_uuid": "b13851790cf12c3f8351013e818af017aeabfd81", "_cell_guid": "50be94ad-4446-4791-b8c9-0de874f900a7"}}, {"outputs": [], "cell_type": "code", "source": ["# Put in our parameters for said classifiers\n", "# Random Forest parameters\n", "rf_params = {}\n", "# Extra Trees Parameters\n", "et_params = {}\n", "# AdaBoost parameters\n", "ada_params = {}\n", "# Gradient Boosting parameters\n", "#Gradient Boosting {'loss': 'exponential', 'max_depth': 5, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'n_estimators': 100} \n", "gb_params = {}\n", "# Support Vector Classifier parameters \n", "svc_params = {}"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "fe36f9a2292bca7e3a7619a7afe193923d5a1f03", "_cell_guid": "58372634-b64b-4d05-9124-adeacf5fc70b"}}, {"cell_type": "markdown", "source": ["## parameter tuning( Very Slow, more than 10 minutes)"], "metadata": {"_uuid": "165a211e00465c18a58da6dbbcf2fcf95d84aa68", "_cell_guid": "e1fab9fd-24e6-4619-b850-a8909ebcdecf"}}, {"outputs": [], "cell_type": "code", "source": ["from sklearn.model_selection import train_test_split\n", "from sklearn.model_selection import GridSearchCV\n", "from sklearn.metrics import classification_report\n", "\n", "RF  = RandomForestClassifier()\n", "Ada = AdaBoostClassifier()\n", "GB  = GradientBoostingClassifier()\n", "ET  = ExtraTreesClassifier()\n", "SVM = SVC()\n", "XGB = xgb.XGBClassifier()\n", "\n", "ada_tuned_parameters =[{\"n_estimators\":[500],\"algorithm\":[\"SAMME\", \"SAMME.R\"],\"learning_rate\":[1]}]\n", "gb_tuned_parameters =[{\"loss\":[\"deviance\", \"exponential\"],'n_estimators': [100,500], 'max_features': [\"sqrt\",\"log2\"],\n", "                       'max_depth': [5,7],'min_samples_leaf':[1]}]\n", "et_tuned_parameters = [{'n_estimators': [100,500],\"criterion\":[\"gini\",\"entropy\"],\"max_features\":[\"auto\",\"log2\",None],\"max_depth\":[5,15,None]}]\n", "rf_tuned_parameters = [{'n_estimators': [100,500],\"criterion\":[\"gini\",\"entropy\"],\"max_features\":[\"auto\",\"log2\",None],\"max_depth\":[5,15,None]}]\n", "svm_tuned_parameters = [{'kernel': ['rbf'],'C': [1,10],'gamma': [\"auto\", 1e-2, 1e-3],\"class_weight\":[\"balanced\",None]}]\n", "xgb_tuned_parameters = [{'n_estimators': [500],\"max_depth\":[5],\"learning_rate\":[0.025],\"gamma\":[0.01,0.1],\"min_child_weight\":[7,9]}]\n", "\n", "scores = ['f1']\n", "\n", "estimators={\"Ada Boost\":Ada, \n", "            \"Gradient Boosting\":GB, \n", "            \"Extra Trees\":ET,\n", "            \"Support VectorMachine\":SVM,\n", "            \"XGBooting\":XGB}\n", "\n", "px_train, px_test, py_train, py_test = train_test_split(x_train, y_train, test_size=0.25, random_state=0)\n", "\n", "for score in scores:\n", "    print(\"# Tuning hyper-parameters for %s\" % score)  \n", "    \n", "    clf0 = GridSearchCV(Ada, ada_tuned_parameters, cv=5,scoring='%s_macro' % score)\n", "    clf0.fit(x_train, y_train)\n", "    print(\"Ada\\n\",clf0.best_params_,\"\\n\\n\")\n", "    ada_params=clf0.best_params_\n", "    \n", "    clf1 = GridSearchCV(SVM, svm_tuned_parameters, cv=5,scoring='%s_macro' % score)\n", "    clf1.fit(x_train, y_train)\n", "    print(\"SVM\\n\",clf1.best_params_,\"\\n\\n\")\n", "    svc_params=clf1.best_params_\n", "    \n", "    clf2 = GridSearchCV(GB, gb_tuned_parameters, cv=5,scoring='%s_macro' % score)\n", "    clf2.fit(x_train, y_train)\n", "    print(\"Gradient Boosting\",clf2.best_params_,\"\\n\\n\")\n", "    \n", "    gb_params = clf2.best_params_\n", "    \n", "    clf3 = GridSearchCV(ET, et_tuned_parameters, cv=5,scoring='%s_macro' % score)\n", "    clf3.fit(x_train, y_train)\n", "    print(\"Extree\",clf3.best_params_,\"\\n\\n\")\n", "    et_params = clf3.best_params_\n", "    \n", "    clf4 = GridSearchCV(XGB, xgb_tuned_parameters, cv=5,scoring='%s_macro' % score)\n", "    clf4.fit(x_train, y_train)\n", "    print(\"XGB\",clf4.best_params_,\"\\n\\n\")\n", "    xgb_params = clf4.best_params_\n", "\n", "   \n", "    clf5 = GridSearchCV(RF, rf_tuned_parameters, cv=5,scoring='%s_macro' % score)\n", "    clf5.fit(x_train, y_train)\n", "    print(\"Random Forest\",clf5.best_params_,\"\\n\\n\")\n", "    rf_params = clf5.best_params_"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "162a1273950d429aa29c0ac61c5426e5ab78b458", "_cell_guid": "161f5fb3-8b9c-4a71-836b-796d9550d140"}}, {"cell_type": "markdown", "source": ["## Possible tuned parameters\n", "- Ada  {'algorithm': 'SAMME', 'learning_rate': 1, 'n_estimators': 500} \n", "- SVM  {'C': 10, 'class_weight': None, 'gamma': 0.01, 'kernel': 'rbf'} \n", "- Gradient Boosting {'loss': 'deviance', 'max_depth': 5, 'max_features': ['log2','sqrt'],'min_samples_leaf': 1, 'n_estimators': 100} \n", "- Extree {'criterion': ['entropy','gini'] 'max_depth': 15, 'max_features': 'auto', 'n_estimators': 100} \n", "- Random Forest {'criterion': ['entropy','gini'] 'max_depth': 15, 'max_features': 'log2', 'n_estimators': 500} \n", "- XGB  {'gamma': 0.1, 'learning_rate': 0.025, 'max_depth': 5, 'min_child_weight': 7, 'n_estimators': 500} \n"], "metadata": {"_uuid": "b61144f1a8effd1767e497d1376f920f9442043e", "_cell_guid": "428af8ad-f107-42e3-a098-9152bdfaa3f5"}}, {"outputs": [], "cell_type": "code", "source": ["# Create 5 objects that represent our 4 models\n", "rf = SklearnHelper(clf=RandomForestClassifier, seed=SEED, params=rf_params)\n", "et = SklearnHelper(clf=ExtraTreesClassifier, seed=SEED, params=et_params)\n", "ada = SklearnHelper(clf=AdaBoostClassifier, seed=SEED, params=ada_params)\n", "gb = SklearnHelper(clf=GradientBoostingClassifier, seed=SEED, params=gb_params)\n", "svc = SklearnHelper(clf=SVC, seed=SEED, params=svc_params)"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "fc1302c1eae96649ace97c60ca7bfc2ec9f979f2", "_cell_guid": "2674f6d8-f573-43df-9b68-59abbfed6250"}}, {"outputs": [], "cell_type": "code", "source": ["\n"], "execution_count": null, "metadata": {"collapsed": true, "scrolled": true, "_uuid": "90966fb1d65d41d96fe9b8c810a3726f9acb2da1", "_cell_guid": "1196a0cc-923f-42b1-8822-ed4fe1d443ac"}}, {"cell_type": "markdown", "source": ["## Output of the First level Predictions\n", "We now feed the training and test data into our 5 base classifiers and use the Out-of-Fold prediction function we defined earlier to generate our first level predictions. \n", "\n", "Allow a handful of minutes for the chunk of code below to run."], "metadata": {"_uuid": "fe1453c1698fe841fcc75d76d5303a25e8a47964", "_cell_guid": "ba7e32b5-a071-416c-8a56-ae29e15386a7"}}, {"outputs": [], "cell_type": "code", "source": ["\n", "# Create our OOF train and test predictions. These base results will be used as new features\n", "et_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test) # Extra Trees\n", "rf_oof_train, rf_oof_test = get_oof(rf,x_train, y_train, x_test) # Random Forest\n", "ada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) # AdaBoost \n", "svc_oof_train, svc_oof_test = get_oof(svc,x_train, y_train, x_test) # Support Vector Classifier\n", "gb_oof_train, gb_oof_test = get_oof(gb,x_train, y_train, x_test) # Gradient Boost\n", "\n", "print(\"Training is complete\")"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "7a2f7e7125792ce8a3a4ff9ab73d222a93cb5420", "_cell_guid": "5481a964-426a-4082-8f0e-4ba58ed4b530"}}, {"outputs": [], "cell_type": "code", "source": ["\n", "predictions_ada=ada.clf.predict(x_test)\n", "predictions_svc=svc.clf.predict(x_test)\n", "predictions_et=et.clf.predict(x_test)\n", "predictions_rf=rf.clf.predict(x_test)\n", "predictions_gb=gb.clf.predict(x_test)\n", "print(\"Prediction is completed\")"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "7e18937301d32a13745f9cc34d4f2ec0332ff054", "_cell_guid": "93f48c09-8ab2-49a0-9870-d9b4aebe312c"}}, {"cell_type": "markdown", "source": ["\n", "## learning model via XGBoost\n"], "metadata": {"_uuid": "bb87169019589bc08923073eb65afd15ad42cef9", "_cell_guid": "2feac13b-c877-44ab-afa0-9ba07e27688e"}}, {"outputs": [], "cell_type": "code", "source": ["from sklearn.metrics import classification_report\n", "\n", "gbm=XGB.set_params(**xgb_params).fit(x_train, y_train)\n", "predictions_xgb = gbm.predict(x_test)\n", "\n", "y_train_pred_xgb = gbm.predict(x_train)\n", "\n", "target =[\"Dead\",\"Survived\"]\n", "result = classification_report(y_train, y_train_pred_xgb, target_names=target)\n", "\n", "print(result)"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "ab7e9728e70cd68ddb15a0b99a556d6ac2dc6d34", "_cell_guid": "5cb5eb16-580e-4f95-a507-9fac18cae130"}}, {"cell_type": "markdown", "source": ["## Performance comparing"], "metadata": {"_uuid": "0d4dc6dee39fb0c1c4edbc1cf66b34080ad776d5", "_cell_guid": "8858dce2-3cae-4139-a21a-4dee30c4ef3e"}}, {"outputs": [], "cell_type": "code", "source": ["et_score=et.clf.score(x_train,y_train)\n", "rf_score=rf.clf.score(x_train,y_train)\n", "ada_score=ada.clf.score(x_train,y_train)\n", "gb_score=gb.clf.score(x_train,y_train)\n", "svc_score=svc.clf.score(x_train,y_train)\n", "xgb_score = gbm.score(x_train,y_train)\n", "\n", "score_df = pd.DataFrame( [{\n", "     'Random Forest score': rf_score,\n", "     'Extra Trees score': et_score,\n", "      'AdaBoost score': ada_score,\n", "    'Gradient Boost score': gb_score,\n", "    \"Support Vector Machine\":svc_score,\n", "    \"XGBoost\":xgb_score\n", "    }])\n", "score_df"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "96542122f1cad2773c2047828e0ab4d690869ed0", "_cell_guid": "e2f5a3b2-bbe6-4cb5-925b-7731ef88b138"}}, {"cell_type": "markdown", "source": ["# Second-Level Predictions from the First-level Output\u00b6\n", "## First-level output as new features\n", "Having now obtained our first-level predictions, one can think of it as essentially building a new set of features to be used as training data for the next classifier. \n", "\n", "As per the code below, we are therefore having as our new columns the first-level predictions from our earlier classifiers and we train the next classifier on this."], "metadata": {"_uuid": "81cfa8d2e69faab8cb95d1249ae61febd2a7c4c2", "_cell_guid": "58c0ecc3-f8e3-456f-a2ba-d271753f8125"}}, {"outputs": [], "cell_type": "code", "source": ["base_predictions_train = pd.DataFrame( {'RandomForest': rf_oof_train.ravel(),\n", "     'ExtraTrees': et_oof_train.ravel(),\n", "     'AdaBoost': ada_oof_train.ravel(),\n", "      'GradientBoost': gb_oof_train.ravel(),   \n", "     \"XGB\":y_train_pred_xgb\n", "    })\n", "base_predictions_train.head(10)"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "84bf665574f05451a2772910250bf2b434f265c6", "_cell_guid": "dd641da7-0033-483e-a2df-8692bc5daa06"}}, {"outputs": [], "cell_type": "code", "source": ["#this code could be removed in future after ....xgb/vote class handled\n", "\n", "vc_x_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\n", "vc_x_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)\n", "vc_x_train.shape,vc_x_test.shape"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "49d384059ba06928cbfa903f70289b9fe72e7762", "_cell_guid": "3d4f68ac-85c1-47e0-89b9-7ce1da4ea619"}}, {"cell_type": "markdown", "source": ["# Ensemble"], "metadata": {"_uuid": "80d51d3c4a954f0dea3d84dfbf58aa37579ea9ff", "_cell_guid": "6a35b462-9e39-40bf-8185-b02acee0770d"}}, {"outputs": [], "cell_type": "code", "source": ["from sklearn.ensemble import VotingClassifier\n", "\n", "clf_vc = VotingClassifier(estimators=[('ada', ada.clf), ('RF',rf.clf),\n", "                                      ('Gradient Boost', gb.clf),('Support Vector Machine', svc.clf), ('XGBoost', gbm)], \n", "                          voting='hard')\n", "clf_vc = clf_vc.fit(vc_x_train, y_train)\n", "predictions_vc =clf_vc.predict(vc_x_test)"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "f8f04c70812d4b20db7350819af5a6255a49eb3f", "_cell_guid": "d9df7cf0-c6e1-4139-8180-edc00e93a740"}}, {"outputs": [], "cell_type": "code", "source": ["from sklearn.metrics import classification_report\n", "#local = pd.read_csv(\"Stacking20170915-1.csv\").Survived.values\n", "#local.shape, predictions.shape\n", "vc_score = clf_vc.score(vc_x_train, y_train)"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "46944115710b86a157b1593e59760be2c55091d3", "_cell_guid": "59ca1cf7-947e-476c-a393-cf5d5b8be951"}}, {"outputs": [], "cell_type": "code", "source": ["base_predictions_train = pd.DataFrame( {\n", "    'RandomForest': rf_oof_train.ravel(),\n", "     'ExtraTrees': et_oof_train.ravel(),\n", "     'AdaBoost': ada_oof_train.ravel(),\n", "      'GradientBoost': gb_oof_train.ravel(),\n", "      \"Voting\"  :clf_vc.predict(vc_x_train),\n", "      \"XGB\":y_train_pred_xgb\n", "                                        \n", "    })\n", "base_predictions_train.head(10)"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "727175ff5c3cb544a9ac4c3baafb9f4d6101615f", "_cell_guid": "d81f7daf-3ee7-4b04-b1c7-a5badd1e1321"}}, {"cell_type": "markdown", "source": ["## Learning Curing( learning or guessing)"], "metadata": {"_uuid": "38a6cba1c208cf10b4e623816073ee7292dc6ac9", "_cell_guid": "1b526e9f-21d4-42e3-a923-e905b9e48b52"}}, {"outputs": [], "cell_type": "code", "source": ["\n", "# Ada Boost\n", "plot_learning_curve(ada.clf, \"Ada Boost\", x_train, y_train, cv=5)\n", "\n", "# Extra Tree\n", "plot_learning_curve(et.clf, \"Extra Tree\", x_train, y_train, cv=5)\n", "\n", "# Gradient Boost\n", "plot_learning_curve(gb.clf, \"Gradient Boost\", x_train, y_train, cv=5)\n", "\n", "# Random Forest score\n", "plot_learning_curve(rf.clf, \"Random Forest\", x_train, y_train, cv=5)\n", "\n", "# Support Vector Machine\n", "plot_learning_curve(svc.clf, \"Support Vector Machine\", x_train, y_train, cv=5)\n", "\n", "# XGBosst\n", "plot_learning_curve(gbm, \"XGBoost\", x_train, y_train, cv=5)\n", "\n", "#Ensemble\n", "plot_learning_curve(clf_vc, \"VotingClassifier\", x_train, y_train, cv=5)"], "execution_count": null, "metadata": {"collapsed": true, "scrolled": false, "_uuid": "02bfff33602dac20090570890e90221a8ee611cb", "_cell_guid": "5c22493a-f2b1-4174-8b34-29e00b3b64d7"}}, {"cell_type": "markdown", "source": ["# Evaluation the performance"], "metadata": {"_uuid": "4a3f7008dbaaeb50bbac3adb788c6632fb442ed5", "_cell_guid": "af652452-63cf-498a-8efa-335ed80cd520"}}, {"outputs": [], "cell_type": "code", "source": ["score_df = pd.DataFrame( [{\n", "     'Random Forest score': rf_score,\n", "     'Extra Trees score': et_score,\n", "      'AdaBoost score': ada_score,\n", "    'Gradient Boost score': gb_score,\n", "    \"Support Vector Machine\":svc_score,\n", "      \"XGBoost\":xgb_score,\n", "    \"Voting\":vc_score\n", "    }])\n", "score_df.sort_values(by=0,axis=1)"], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "294300662d91d4f7c8c37f8d9035eadd0994110d", "_cell_guid": "9eab2f31-5758-4a4e-a710-9401c8ca02ed"}}, {"cell_type": "markdown", "source": ["## score of f1 (mean, std, score+/-2 std)"], "metadata": {"_uuid": "2f3870a6a3c65a3dc5e1f5698c027dd1ea0b9234", "_cell_guid": "af45b25d-f055-4f71-ba0c-3e91ea044ebf"}}, {"outputs": [], "cell_type": "code", "source": ["tmp_clfs={'Random Forest': rf.clf,\n", "     'Extra Trees': et.clf,\n", "      'AdaBoost': ada.clf,\n", "    'Gradient Boost': gb.clf,\n", "    \"Support Vector Machine\":svc.clf,\n", "    \"XGB\": gbm,\n", "    \"Voting\":clf_vc\n", "    }\n", "cvs_all=[]\n", "for i,j in tmp_clfs.items():\n", "    score=cross_val_score(j, x_train,y_train, cv=5,scoring=\"f1\")\n", "    cvs={}\n", "    cvs[\"Estimator\"]=i\n", "    cvs[\"Score_mean\"]=score.mean()\n", "    cvs[\"Score_std\"]=score.std()\n", "    cvs[\"Score_low2z\"]=score.mean()-score.std()*3\n", "    cvs[\"Score_high2z\"]=score.mean()+score.std()*3\n", "    cvs_all.append(cvs)\n", "pd.DataFrame(cvs_all).sort_values(by=\"Score_low2z\",ascending=False)"], "execution_count": null, "metadata": {"collapsed": true, "scrolled": false, "_uuid": "f80dbb5da1a8284b56e2007ee712a4d552d660f4", "_cell_guid": "1a65e89e-47c4-484c-9a28-4d56faa1753c"}}, {"cell_type": "markdown", "source": ["## score of accuracy"], "metadata": {"_uuid": "d1a800d68cc483dc5e94310008b41b853af55c52", "_cell_guid": "44884de8-9bf3-4904-894d-f3f11f5a4f89"}}, {"cell_type": "markdown", "source": ["#  Producing the Submission file\n", "Finally having trained and fit all our first-level and second-level models, we can now output the predictions into the proper format for submission to the Titanic competition as follows:"], "metadata": {"_uuid": "16f14a5ed925e157628284a55a44b1472bc50441", "_cell_guid": "dbd58c87-88ca-43e6-ba57-6c84ac06dea1"}}, {"outputs": [], "cell_type": "code", "source": ["# Generate Submission File \n", "StackingSubmission = pd.DataFrame({ 'PassengerId': PassengerId,\n", "                            'Survived': predictions_vc.astype(int) })\n", "StackingSubmission.to_csv(\"../input/parameter_tune_vc.csv\", index=False)"], "execution_count": null, "metadata": {"_uuid": "0a702bc6b545c197f382288da4f6240d9b6f9cf1", "_cell_guid": "26b42ca3-de9c-40d5-829a-7eb569545214"}}, {"outputs": [], "cell_type": "code", "source": [], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "094c705efa330c102b8f3bc67c50a81a7a56e29f", "_cell_guid": "cf600c1e-bf20-49b8-9921-bf381a765fa9"}}, {"outputs": [], "cell_type": "code", "source": [], "execution_count": null, "metadata": {"collapsed": true, "_uuid": "035c65172ff7f64ea96387a65e2cdd9caf4083a3", "_cell_guid": "a58886fa-4572-4290-ba0f-f63688d1fa8c"}}], "nbformat_minor": 1, "metadata": {"language_info": {"pygments_lexer": "ipython3", "codemirror_mode": {"name": "ipython", "version": 3}, "mimetype": "text/x-python", "file_extension": ".py", "nbconvert_exporter": "python", "name": "python", "version": "3.6.1"}, "toc": {"navigate_menu": true, "widenNotebook": false, "toc_cell": false, "moveMenuLeft": true, "toc_section_display": "block", "toc_window_display": true, "number_sections": true, "threshold": 4, "colors": {"navigate_text": "#333333", "running_highlight": "#FF0000", "selected_highlight": "#FFD700", "navigate_num": "#000000", "wrapper_background": "#FFFFFF", "hover_highlight": "#DAA520", "sidebar_border": "#EEEEEE"}, "toc_position": {"width": "169px", "height": "765px", "left": "0px", "right": "855px", "top": "106px"}, "nav_menu": {"width": "252px", "height": "245px"}, "sideBar": true}, "kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}, "varInspector": {"window_display": false, "types_to_exclude": ["module", "function", "builtin_function_or_method", "instance", "_Feature"], "cols": {"lenType": 16, "lenName": 16, "lenVar": 40}, "kernels_config": {"python": {"library": "var_list.py", "delete_cmd_postfix": "", "varRefreshCmd": "print(var_dic_list())", "delete_cmd_prefix": "del "}, "r": {"library": "var_list.r", "delete_cmd_postfix": ") ", "varRefreshCmd": "cat(var_dic_list()) ", "delete_cmd_prefix": "rm("}}}}}