{"cells":[{"metadata":{"_cell_guid":"ab88d904-9700-4689-9460-c3f40eb83bb4","_uuid":"cd578834ffbe195a6376732c736a535de96420a8"},"cell_type":"markdown","source":"# Titanic: Simplified Solution for Beginners Using Dense Neural Networks\n**[Renan Gomes Barreto](https://www.kaggle.com/renangomes)**  -  *May, 2018*\n \n![Titanic](https://media.giphy.com/media/Uj3SeuVfg2oCs/giphy.gif)\n\n## Introduction\n\nThis notebook contains a brief introduction on how to create a multi-layered Neural Network and solve the Titanic problem using a simple model in Keras. Density models, also called Multi-layer perceptrons (MLP), can be used as a basis and, from them, more complex models can be constructed.\n\nIn this notebook, we will create a binary classifier using the data of passengers of the Titanic. With this dataset, we want to predict whether or not the passenger survived the shipwreck.\n****\nIn order to solve problems with Neural Networks, just like any machine learning problem, it is first necessary to understand the problem and, above all, to understand the data before elaborating an architecture. This process is extensive and often comprises most of the work. In this way, this notebook aims to introduce some important points and is organized as follows:\n\n* [Introduction](#Introduction)\n* [The Problem](#The-Problem)\n* [Loading the dataset](#Loading-the-dataset)\n* [Preprocessing](#Preprocessing)\n* [Implementing the Neural Network](#Implementing-the-Neural-Network)\n* [Results](#Results)\n* [Conclusion](#Conclusion)"},{"metadata":{"_cell_guid":"d96c38b4-dcb0-4b5d-afec-da78f793edea","_uuid":"c70d422ef47ea53cd8ee3c92f8fe465b9b15f42e"},"cell_type":"markdown","source":"## The Problem\n\nIn this problem, information from the Titanic passengers will be used as a database to identify which passengers have survived. On the Titanic, one of the reasons that caused the wreck was that there were not enough lifeboats for the passengers and the crew. Among the passengers, some groups of people were more likely to survive than others, such as women, children, and the upper class. In this way, the problem is to use a Neural Network to identify which people could survive."},{"metadata":{"_cell_guid":"a40e23de-99f3-489a-9e99-4dc27e7aea3b","_uuid":"17b7983dc7f2f77bf76e4d663f590e7da00d2ab2"},"cell_type":"markdown","source":"## Loading the Dataset"},{"metadata":{"_cell_guid":"b72720f9-ac83-46c8-8204-f88eab2ccd80","_uuid":"dc0311226b830b7482fe58c4b2c105a7dfccb6c7"},"cell_type":"markdown","source":"### Reading the dataset files\n\nTo start, you must analyze the dataset's input attributes, their types, and the target attribute. This can be done through Pandas, an important Python library for parsing and preprocessing data."},{"metadata":{"_cell_guid":"4299bf6e-fb96-4ee4-a1a0-4922030a4438","_uuid":"886470fab8c75de23da7c3f722330e2a3b4671d6","trusted":false,"collapsed":true},"cell_type":"code","source":"import numpy as np\nnp.random.seed(10)\n\nimport pandas as pd \n\ntrain = pd.DataFrame(pd.read_csv(\"../input/train.csv\", index_col=[0], header=0))\ntest  = pd.DataFrame(pd.read_csv(\"../input/test.csv\", index_col=[0], header=0))\ndisplay(train.head())","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"abecdda5-27f3-409b-adf9-106dc6890cb8","_uuid":"94cbf76367bbc96c683c44c13024d1a78c48b981"},"cell_type":"markdown","source":"## Preprocessing\n\nThe Name, Ticket and Cabin columns appear to be unique to the passenger, so we will discard them. On closer examination, we could use these columns to improve data or even deduce missing data."},{"metadata":{"_cell_guid":"c476386f-4c28-4f27-af74-9333dc6a41f0","_uuid":"b39e8f9e76e3ab247430ecd94bc031408757e33d","trusted":false,"collapsed":true},"cell_type":"code","source":"train.drop(columns=['Name', 'Ticket', 'Cabin'], inplace=True)\ntest.drop(columns=['Name', 'Ticket', 'Cabin'], inplace=True)\ndisplay(train.head())","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1adabffb-f879-4359-bbae-f8ea4ece308c","_uuid":"7832f7a70e2df765a1770f88c4b0877419ba0126"},"cell_type":"markdown","source":"### Handling missing Data\n\nMissing data is a serious problem in machine learning. Somehow we should handle them. The easiest way to solve this issue is simply by deleting all the rows in the dataset that have this data or by replacing them with a fixed value. In our case, for the Age and Fare,  numeric columns, we replace the missing data by the mean. The SibSp and Parch columns had their data replaced with -1."},{"metadata":{"_cell_guid":"a312382d-f2ee-41f9-960f-f18aa3d55b93","_uuid":"b38dc26cf1f5f67e0f3e3f2197d3e4f331bb0701","collapsed":true,"trusted":false},"cell_type":"code","source":"train['Age'].fillna(train['Age'].mean(), inplace=True)\ntrain['Fare'].fillna(train['Fare'].mean(), inplace=True)\ntrain['SibSp'].fillna(-1, inplace=True)\ntrain['Parch'].fillna(-1, inplace=True)\n\ntest['Age'].fillna(train['Age'].mean(), inplace=True)\ntest['Fare'].fillna(train['Fare'].mean(), inplace=True)\ntest['SibSp'].fillna(-1, inplace=True)\ntest['Parch'].fillna(-1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d0b6b66d-a0d9-4560-8813-38e84a9bb185","_uuid":"9d74497d8455d3fbd2eba39c7cc4696da2a0190d"},"cell_type":"markdown","source":"### Encoding categorical columns\n\nThe Pclass, Sex, and Embarked columns appear to be categorical. These columns should be mapped to numbers, and each possible value should preferably become a new binary column. This can be done easily by using the pandas get_dummies function."},{"metadata":{"_cell_guid":"682deee3-9270-47b2-aa44-7208f1c33c39","_uuid":"adef72feeefeeb7cb0714718b21430237a3d9059","trusted":false,"collapsed":true},"cell_type":"code","source":"train = pd.get_dummies(train, dummy_na=True, columns=['Pclass', 'Sex', 'Embarked']).astype(float)\ntest = pd.get_dummies(test, dummy_na=True, columns=['Pclass', 'Sex', 'Embarked']).astype(float)\n\ndisplay(train.head())\ndisplay(test.head())","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1a2cc0d9-4a85-4f8f-9b92-7f82b7e364cf","_uuid":"55070c4c73c431cf562625225fad847afc16920b"},"cell_type":"markdown","source":"## Implementing the Neural Network"},{"metadata":{"_cell_guid":"441473cf-3bf2-4c14-81fb-440f54908b9b","_uuid":"53aef0dd7fe5a773f008a671adbf48291098ea21"},"cell_type":"markdown","source":"### Separating attributes from output\n\nThe output attributes will be separated. In addition, we separated the original training dataset into two (training and validation)."},{"metadata":{"_cell_guid":"578c8c14-815e-4e6b-8b47-66f2e3925f5e","_uuid":"51a5f4011bc1cba826bf18ae9f4234bd6e74b852","trusted":false,"collapsed":true},"cell_type":"code","source":"X_train = train.drop(columns=[\"Survived\"])[:-120]\ny_train = train[\"Survived\"][:-120]\n\nX_val = train.drop(columns=[\"Survived\"])[-120:]\ny_val = train[\"Survived\"][-120:]\n\nX_test = test\n\nprint(\"X_train: \", X_train.shape)\nprint(\"y_train: \", y_train.shape)\nprint(\"X_val: \",   X_val.shape)\nprint(\"y_val: \",   y_val.shape)\nprint(\"X_test: \",   X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"aa77dac8-fd2c-44bb-b680-558163e7170f","_uuid":"3edcb0e351c985bb2f028324322f9f1c475aa370"},"cell_type":"markdown","source":"### Model definition\n\nWe will create a simple model using Keras. Feel free to change the number of neurons, layers, activation functions, etc."},{"metadata":{"_cell_guid":"21babc43-f5dc-461d-aafa-149716dbed01","_uuid":"dc4ec50fc89fe9e6b647ba9779c289cf2ad27466","trusted":false,"collapsed":true},"cell_type":"code","source":"from keras.optimizers import Adam\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\n\nmodel = Sequential()\nmodel.add(Dense(32, input_dim=X_train.shape[1], activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0001), metrics=['accuracy'])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8fdb28a7-b64d-4378-b8d4-0fe16029ffe5","_uuid":"a9b4441e65518f207e9d41f77e9d6cc997543d6b"},"cell_type":"markdown","source":"### Training\n\nWe will train the network for 1 epochs with a batch size of 32. If you want to see statistics during training, activate the verbose parameter."},{"metadata":{"_cell_guid":"965d01c0-30b4-4a60-a572-44d3d63c54b3","_uuid":"c84d109436be19a78f1ca4d416e3b8a97879f76b","trusted":false,"collapsed":true},"cell_type":"code","source":"import time\n\nepochs = 1750\nstart_time = time.time()\n\nhistory = model.fit(X_train.as_matrix(), y_train.as_matrix(), epochs=epochs, batch_size=32, \n                    validation_data=(X_val.as_matrix(), y_val.as_matrix()), verbose=0, shuffle=True)\n\nprint(\"Time spent: %d seconds\" % (time.time() - start_time), \"\\r\\nEpochs: %d\" % (epochs))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5567f322-ce19-4f94-8773-9af3dd37f5b4","_uuid":"b6403b8e3694f55528f34668fdb0cb5b84246579"},"cell_type":"markdown","source":"### Training Charts"},{"metadata":{"_cell_guid":"253de9cf-fe44-4143-8a68-f958dfc6c3dc","_uuid":"b3b8d3c5d3aa220177d02ce1408d1494240966e7","trusted":false,"collapsed":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(history.history['acc'], color=\"r\")\nplt.plot(history.history['val_acc'], color=\"g\")\nplt.title('Training Chart')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Training', 'Validation'], loc='lower right')\nplt.show()\n\nplt.plot(history.history['loss'], color=\"r\")\nplt.plot(history.history['val_loss'], color=\"g\")\nplt.title('Training Chart')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Training', 'Validation'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"03f47d50-72a9-4526-9fc0-f70ca1660b75","_uuid":"9071b538f6eb645d11f0fd6ed1891333f615c516"},"cell_type":"markdown","source":"## Results"},{"metadata":{"_cell_guid":"e1786bed-1a66-494f-8983-79f56fb2d70a","_uuid":"ac427ab8cf5a406c2da7bdc622f2a66996866a41"},"cell_type":"markdown","source":"### Confusion Matrix - Training and Validation\n\nIn order to understand the training result, we will use the accuracy_score and confusion_matrix functions from the sklearn library.\n\nRemember that the variables y_train and X_train are Pandas dataframe, so we will usually have to use the as_matrix() function before using them."},{"metadata":{"_uuid":"db85f0b08e13325e832b4a64842a3329fd9a3655","_cell_guid":"afae4d90-d754-40c8-ae3f-66f5368e94ac","scrolled":true,"collapsed":true,"trusted":false},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nimport seaborn as sns\nimport numpy as np\n\nprint(\"Accuracy (Training dataset):\", accuracy_score(y_train.as_matrix(), np.round(model.predict(X_train.as_matrix()))), \"\\r\\n\")\n\nconfusionMatrixDF = pd.DataFrame( confusion_matrix(y_train.as_matrix(), np.round(model.predict(X_train.as_matrix()))),\n                                 index=('Survivor', 'Victim'), columns=('Survivor', 'Victim'))\n\nheatmap = sns.heatmap(confusionMatrixDF, annot=True, fmt=\"d\", cmap=\"Blues\",  vmin=0)\nheatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=14)\nheatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=14)\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()\n\nprint(\"Accuracy (Validation dataset):\", accuracy_score(y_val.as_matrix(), np.round(model.predict(X_val.as_matrix()))), \"\\r\\n\")\n\nconfusionMatrixDF = pd.DataFrame( confusion_matrix(y_val.as_matrix(), np.round(model.predict(X_val.as_matrix()))),\n                                 index=('Survivor', 'Victim'), columns=('Survivor', 'Victim'))\n\nheatmap = sns.heatmap(confusionMatrixDF, annot=True, fmt=\"d\", cmap=\"Blues\",  vmin=0)\nheatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=14)\nheatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=14)\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9813c467-d896-4b21-b2bd-e8daaac73b02","_uuid":"85a133ed39c6f79e5f8da292e324d23c49a60ef7"},"cell_type":"markdown","source":"### Preparing the file for submission"},{"metadata":{"_cell_guid":"98809b2e-3f9f-4a88-a55e-4a83edf5922e","_uuid":"95f3d9450da03a0d10ed71d762512759c9a4880b","collapsed":true,"trusted":false},"cell_type":"code","source":"y_test_pred = model.predict(X_test.as_matrix())\n\nX_test_submission = X_test.copy()\nX_test_submission['Survived'] = np.round(y_test_pred).astype(int)\nX_test_submission['Survived'].to_csv('submission.csv', header=True)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d6ce80dc-b39d-4b4f-b96a-87dab4bd7b7e","_uuid":"4c600e3d4b3171267b5205dc53248e94babc794a"},"cell_type":"markdown","source":"## Conclusion\n\nIn this notebook, we show a simple solution to the Titanic Dataset problem.\n\nIt was implemented a Dense Neural Network with two layers that obtained a satisfactory accuracy in the validation dataset."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"file_extension":".py","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"version":3,"name":"ipython"},"name":"python","pygments_lexer":"ipython3","nbconvert_exporter":"python"}},"nbformat":4,"nbformat_minor":1}