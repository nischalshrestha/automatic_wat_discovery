{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#Classifiers\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import tree\n\nimport os\nprint(os.listdir(\"../input\"))\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"**Step 1**: **Import the dataset.**\n\nwe first have to import the data we are going to be using. Dataset is diveded into two parts which are train, test."},{"metadata":{"trusted":true,"_uuid":"3ec99365f5724d53873c60917d4c1e50773bc5a7"},"cell_type":"code","source":"train = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\nY_test = pd.read_csv(\"../input/gender_submission.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"06c211d7cc9b3b4b4b15acc6db0aba71c3982def"},"cell_type":"markdown","source":"**Step 2:** **visualize the dataset**\n\nBefore we start writing the whole code we first have to know what our data is actually about because working on a machine learning algorithm without knowing what we are dealing with is the height of stupidity and we'll get to see some colourful graphs so it's a win-win ;)"},{"metadata":{"trusted":true,"_uuid":"0f510d11d02e78204f3c9c21eccab9044f40f5b1"},"cell_type":"code","source":"train.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"50630e2b8055c25eed14ff4c755d084bcc6a367e"},"cell_type":"markdown","source":"In the output above we can see that there are 12 features and 891 training examples. But there is some data missing in 'Age', 'Cabin' which we'll clean later."},{"metadata":{"trusted":true,"_uuid":"823baa97984b3836716845dea13e6765509c17e8"},"cell_type":"code","source":"def visualize_data():    \n    fig = plt.figure(figsize=(18, 6))\n    \n    plt.subplot2grid((2, 3), (0, 0))                #This plot shows the % of how many people died and how many survived.\n    train.Survived.value_counts(normalize='true').plot(kind='bar')\n    plt.title('Survivors')\n    plt.xticks(np.arange(2), ('Deceased', \"Survived\"))\n\n    plt.subplot2grid((2, 3), (0, 1), colspan=2)    #This plot shows the relation btw the age and class of the passanger.\n    for x in [1, 2, 3]:\n        train.Age[train.Pclass == x].plot(kind='kde')\n    plt.legend(('1st', '2nd', '3rd'))\n    plt.title('Age wrt class')\n\n    plt.subplot2grid((2, 3), (1, 0))               #This plot shows the % of how many male passangers died and how many of them survived.\n    train.Survived[train.Sex == \"male\"].value_counts(normalize='True').plot(kind='bar', color='b', alpha=0.5)\n    plt.title('Male survivors')\n    plt.xticks(np.arange(2), ('Deceased', 'Survived'))\n\n    plt.subplot2grid((2, 3), (1, 1))               #This plot shows the % of how many female passangers died and how many of them survived.\n    train.Survived[train.Sex == \"female\"].value_counts(normalize='True').plot(kind='bar', color='r', alpha=0.5)\n    plt.title('Female survivors')\n    plt.xticks(np.arange(2), ('Survived', 'Deceased'))\n\n    plt.subplot2grid((2, 3), (1, 2))               #This plot shows the people survived on the basis of gender\n    train.Sex[train.Survived == 1].value_counts(normalize='True').plot(kind='bar', color=['r', 'b'], alpha=0.5)\n    plt.title('Sex of survivors')\n    plt.xticks(np.arange(2), ('Female', \"Male\"))\n    \n    plt.show()\nvisualize_data()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6e26f72d232c5dbe70db7905411103d1dd4f67f6"},"cell_type":"markdown","source":"**Step 4: clean the dataset**\n\nOur dataset has some holes in it as some values are missing and we have string values for some features. So in this function we'll clean our data and get rid of these anomalies."},{"metadata":{"trusted":true,"_uuid":"c55e58094564d687f10cbb7677891089aadd4f65"},"cell_type":"code","source":"train['Age'] = train['Age'].fillna(train['Age'].dropna().median()) #replace the NaN values in Age column with median.\ntest['Age'] = test['Age'].fillna(test['Age'].dropna().median())\n\ntrain['Sex'] = train.Sex.apply(lambda x: 0 if x == \"female\" else 1)\ntest['Sex'] = test.Sex.apply(lambda x: 0 if x == \"female\" else 1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"136fa16f2c5739c23d98428472aa57c2682bd3a4"},"cell_type":"markdown","source":"Now we are going' to convert our data into features(X) and target(Y) variables."},{"metadata":{"trusted":true,"_uuid":"09cc60ebba00f84f69ed700079876f4b672ad1e6"},"cell_type":"code","source":"X_train = np.array(train[['Age', 'Sex', 'Pclass', 'SibSp', 'Parch', 'PassengerId']])        #Training set\nY_train = np.reshape(np.array(train['Survived']), (X_train.shape[0], 1))     #Target set\n\nX_test = np.array(test[['Age', 'Sex', 'Pclass', 'SibSp', 'Parch', 'PassengerId']])   #Test set\n\nprint('Shape of X train:' + str(X_train.shape))\nprint('Shape of Y train:' + str(Y_train.shape))\nprint('Shape of X test:' + str(X_test.shape))\nprint('Shape of Y test:' + str(Y_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dcfe53b63fa1d9c9c37ec1a4a45a76a2ed21b171"},"cell_type":"markdown","source":"**Step 5: Train the model**\n\nWe are going to try some predefined scikit learn classifiers and then we'll choose which classifier works best for our data."},{"metadata":{"_uuid":"5f9b8a64418d25e69dd47da810a3f05d12fa01b9"},"cell_type":"markdown","source":"**Model 1: KNeighborsClassifier**"},{"metadata":{"trusted":true,"_uuid":"1f0cf22124502f356e977b5056340ac2302b2e59"},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors = 3)\n\nknn.fit(X_train, Y_train)\n\nY_pred = knn.predict(X_test)\n\nknn_acc = round(knn.score(X_test, Y_test) * 100, 2)\nprint(knn_acc)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e66f64b576170f30f69c8753d5242b7c596da17e"},"cell_type":"markdown","source":"**Model 2: Random forest classifier**"},{"metadata":{"trusted":true,"_uuid":"e6628b6fd4fbfdcb96f7c81e7a518f4559e41fea"},"cell_type":"code","source":"random_forest = RandomForestClassifier(n_estimators=100)\n\nrandom_forest.fit(X_train, Y_train)\n\nY_pred = random_forest.predict(X_test)\n\nrandom_forest.score(X_train, Y_train)\nrfc_acc = round(random_forest.score(X_test, Y_test) * 100, 2)\nprint(rfc_acc)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"39f14ab3e302dc4d259dbc19e02f429cad42e30b"},"cell_type":"markdown","source":"**Model 3: Decision tree classifier**"},{"metadata":{"trusted":true,"_uuid":"b5cb279b8c779e96037d43fbb6d06e04b8278129"},"cell_type":"code","source":"decision_tree = tree.DecisionTreeClassifier(random_state=1, max_depth=7, min_samples_split=2)\n\ndecision_tree.fit(X_train, Y_train)\n\nY_pred = decision_tree.predict(X_test)\n\ndt_acc = round(decision_tree.score(X_test, Y_test) * 100, 2)\nprint(dt_acc)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b97d70c23e1a80a68f40243e737c50aef5b05369"},"cell_type":"markdown","source":"**Model 4: Support vector machine**"},{"metadata":{"trusted":true,"_uuid":"fade1f1e92aa8a2f41fa01b529546aa65a4275fe"},"cell_type":"code","source":"svc = SVC()\n\nsvc.fit(X_train, Y_train)\n\nY_pred = svc.predict(X_test)\n\nsvm_acc = round(svc.score(X_test, Y_test) * 100, 2)\nprint(svm_acc)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"68c57a4c959e59563c12a35c5c3647b08e826fa2"},"cell_type":"markdown","source":"**Model evaluation**\n\nWe can now rank our evaluation of all the models to choose the best one for our problem. While both Decision Tree and Random Forest score the same, we choose to use Random Forest as they correct for decision trees' habit of overfitting to their training set."},{"metadata":{"trusted":true,"_uuid":"ec466083a037972d192ffaf9e812153fca86e1b0"},"cell_type":"code","source":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Random Forest', 'Decision Tree'],\n    'Score': [svm_acc, knn_acc, rfc_acc, dt_acc]})\nmodels.sort_values(by='Score', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c641edddde756d838f8095bbe20b8c4f1bc2f854"},"cell_type":"code","source":"submission = pd.DataFrame({\n        \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"953cfc48294aa35f8e609c529db5af5a7bf27510"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}