{"cells":[{"metadata":{"_uuid":"c6fe38531b2772eaeb8689165738f2a62bb9bd63"},"cell_type":"markdown","source":"# Titanic\n\nThis is an introductory Kernel for the Titanic challenge in Kaggle, it is intended to serve as a tutorial and introduction to the applied techniques while I explore them. \n\nI have tried my best to explain the actions and rationale on each section and have tried to include the relevant links and references that I have found to be useful while addressing that particular step, some of the approaches I apply have been adapted from other kernels on the challenge (like those from Chris Deotte and Konstantin).\n\nI will start by setting up the required libraries and utilities that we will be using through this exercise:\n\n# 1-) Environment setup\n\nWe start by loading the required libraries and setting some conditions for the environment\n"},{"metadata":{"trusted":true,"_uuid":"33465377d380e74280999c30e48ff816d24e79f2"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import clear_output\nsns.set(style='darkgrid', context='notebook', palette='coolwarm',font_scale=1.5)\n% matplotlib inline\n\nfrom IPython.core.debugger import set_trace\n\n# Gathering the libraries to pre-process the data\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\n# Gathering the required libraries for the models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nimport tensorflow as tf\n\n# Getting the libraries to optimize and assess performance\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import cross_val_score \nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.model_selection import StratifiedKFold, learning_curve\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_selection import RFECV\n\n# Turning off warnings\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0eb1af0a7da0afbab5dcc05d4431930e1c6f95f3"},"cell_type":"markdown","source":"#### After we have the right libraries on board let's get the data set"},{"metadata":{"trusted":true,"_uuid":"d47db829e04409708e74b90a18e1fb1f31847f0b"},"cell_type":"code","source":"# The provided data sets\ntrain_data = pd.read_csv('../input/train.csv')\ntest_data = pd.read_csv('../input/test.csv')\n\n# A join sand box to assess what we got and to play around without disturbing the originals\njoin_data = pd.concat([train_data,test_data])\njoin_data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"058291ff21fea6d7a26ddf52dc915bc0314ddab6"},"cell_type":"markdown","source":"# 2-)  Exploratory Data Analysis (EDA)\n\nDuring this phase we are going to focus on understanding the data and defining our approach, not modifying the data in any way\n\n    - Data set profiles (test + train)\n    - Quick glance and the feature relevance for survival\n    - Correlations and significant relations\n    - Potential imputation and grouping strategies\n\n#### Let's start by getting a good sense of the data we are receiving"},{"metadata":{"trusted":true,"_uuid":"a2667f7112336322f80eca83c3b6966f3cb84311"},"cell_type":"code","source":"fig = plt.figure(figsize=(20,10))\nsns.heatmap(join_data.isnull(),yticklabels=False,cbar=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0ae32dc1e3941187a95cfd46813f0e31df385311"},"cell_type":"code","source":"print('\\n==== Training Data =====')\nprint(train_data.info())\nprint('\\n==== Testing Data =====')\nprint(test_data.info())\nprint('\\n==== Join Data =====')\nprint(join_data.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d8f9fe2efc4178c01b049855c931f8fd0844a899"},"cell_type":"code","source":"print('\\n==== Training Data =====')\nprint(train_data.describe())\nprint(train_data.describe(include=['O']))\nprint('\\n==== Testing Data =====')\nprint(test_data.describe())\nprint(test_data.describe(include=['O']))\nprint('\\n==== Join Data =====')\nprint(join_data.describe())\nprint(join_data.describe(include=['O']))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2577bf62a5366cd7f0e8d744decacc00a668e659"},"cell_type":"markdown","source":"#### =============================================================================================\n#### At the first glance we can see we are getting a total of 1309 record, 891 on the training set and 418 on the testing set. 12 features, 7 numeric and 5 are strings\n\nQuick observations:\n    - Most of the features are fairly complete, Age have about 20% of values missing, Cabin is only available for about 1/5 of the records\n    - There are a handful records where the port of embarkation and the fare are missing\n    - only 38% of the listed passengers survived\n    - There are only 3 ports of embarkation\n    - The passenger ID seems to be just a key to the list but not really providing any information\n    \n#### Let's start going by each feature and see their relevance for survival, starting with:\n\n## === Age ===\n"},{"metadata":{"trusted":true,"_uuid":"547aef75fe1a9e702ca4aca6cad41907f3738c52"},"cell_type":"code","source":"# Plotting the survival rate given the age\ng = sns.FacetGrid(train_data,hue=\"Survived\",palette='magma',height=6,aspect=2).add_legend()\ng = g.map(plt.hist,'Age',bins=40,alpha=0.7)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b5596fcedff0e392210b1efe7a35a514123724f7"},"cell_type":"markdown","source":"As it might be expected, younger passengers survived at a higher rate than older passengers, also, exploring a little bit more the age and sex relationships with the gender"},{"metadata":{"trusted":true,"_uuid":"269b879cf421fb923ebfa5d7479fb513818efccd"},"cell_type":"code","source":"# Making the plot much larger than the default for better visualization\nfig = plt.figure(figsize=(20,10))\nsns.boxplot(x='Pclass',y='Age',hue='Sex',data=train_data,palette='magma')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e8e6fd7d840ad0294b59597587e1fc93f2903df5"},"cell_type":"markdown","source":"We can see that there are significant diferences between the average ages per class and per gender, we can use the mean values to impute the missing value according to what class and gender the record shows."},{"metadata":{"trusted":true,"_uuid":"a4c4e54d3524a03ac548ca675d5cd1596dcfdba6"},"cell_type":"code","source":"# Getting the average age for each group\njoin_data.pivot_table(values='Age',index='Pclass',columns='Sex')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"588caed048e5c5c93cd932cebc9a7f69d320a615"},"cell_type":"code","source":"# For the imputation strategy we are going to use the average ages we capture before per gender and class\n# to achieve this objective we define a short function\ndef AgeImp (Columns):\n    # Gattering the fields:\n    Age = Columns[0]\n    Pclass = Columns[1]\n    Sex = Columns [2]\n    # First, we only impute if the age value is missing\n    if pd.isnull(Age):\n        if Pclass == 3 :# Third class\n            if Sex == 'male':\n                return 26\n            else:\n                return 22.2\n        if Pclass == 2 :# Second class\n            if Sex == 'male':\n                return 30.9\n            else:\n                return 27.5\n        if Pclass == 1 :# First class\n            if Sex == 'male':\n                return 41\n            else:\n                return 37\n    else:\n        return(Age)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1dc0830009ba6ee75a2d68f4a006d91d034770ff"},"cell_type":"markdown","source":"Now, let's explore class a little bit more\n\n## === Class ==="},{"metadata":{"trusted":true,"_uuid":"4d675c9454c36538e44202df1812905fbb6e4ba6"},"cell_type":"code","source":"# Plotting some of the relevant metrics along with Class to see if there is further differentiation\n\ng = sns.FacetGrid(train_data, col='Survived',row='Sex',hue='Embarked',\n                  palette='magma',height=5, aspect=1.5,margin_titles=True)\ng = g.map(sns.countplot,'Pclass',order=[1,2,3]).add_legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"05d004956314fad56aada1526a7210d735ac0b40"},"cell_type":"code","source":"# Looking also at the numbers\ntrain_data.pivot_table(values='Survived',index=['Pclass','Embarked'],columns='Sex',margins=True,margins_name='Total')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"68b5b6c547aa3d55a00635eeddd251b1e8e2259e"},"cell_type":"code","source":"# Looking at the sizes of this groups on our train set to understand the probabilities above\ntrain_data.pivot_table(values='Ticket',index=['Pclass','Embarked'],columns='Sex',margins=True,margins_name='Total',\n                       aggfunc=(lambda x: x.count()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"04245bd5a6a17d4312a08969f4eec96db1b460f5"},"cell_type":"markdown","source":"#### This is quite a significant influence, we can see that 74% of Females (more than 92% if 1st or 2nd class) survived while only around 19% of the men did, also, 63% of first class survived while just 24% of the third class did, there seems to be a close relationship with the port of embarkation with the first class passengers, we will explore this later once we explore the correlations after the categorical variables have been transformed\n\nWith the current knowledge we could predict with 90%+ certainty that 80 passengers (Females from 1st and Second class) will survive, also, looking at the embarkation ports, only 13% of the third class males that embarked in 'S' survived (even less for 'Q') this give us another 119 passengers, we might hard code this rules (that already set a floor of 87% accuracy or higher) and only rely on our predictions for other groups.\n\nWe have looked already at Age, Class along with Sex in both cases, let's move on to:\n\n## === Cabin ===\n\nFor the particulars of the Cabin, during the initial exploration we saw it was available for only around 1/5 (295 in total) of the records, let's explore a little bit more. The first character of the 'cabin' seems to contain the 'Deck', this might be relevant for survival, also, might be highly correlated with the 'class', let's explore this a bit\n"},{"metadata":{"trusted":true,"_uuid":"2007150fc751d420658dc5c4fdbc309fc4640cb8"},"cell_type":"code","source":"# Capturing the first character, 'n' will be shown for the missing values (NaN)\njoin_data['Deck'] = join_data['Cabin'].apply(lambda x: str(x)[0])\nprint(join_data['Deck'].value_counts())\njoin_data.pivot_table(values='Survived',index='Deck',columns='Pclass',margins=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0b3b90025a1f99bbab281fdea7de824e4f0daf00"},"cell_type":"markdown","source":"#### The deck identifier (or absence of it) seem to be significant to the survival probability so we should extract and use this feature, however, a few decks have so few records (T, G) that we should reclassify those to 'n' (not available), let's create a quick dictionary from the list above to accomplish this task later"},{"metadata":{"trusted":true,"_uuid":"744c23b628dbecb834a1a52e8e3163e70f1e0417"},"cell_type":"code","source":"# We keep and ordinal relationship where A > B > C since there is an ordinal relationship of the decks\nDeckDict = {'n':10,\n            'A':0,\n            'B':1,\n            'C':2,\n            'D':3,\n            'E':4,\n            'F':5,\n            'T':6,\n            'G':7}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f0ac09e99679725354c6633f3b4797033338b16f"},"cell_type":"markdown","source":"## === Embarked ===="},{"metadata":{"trusted":true,"_uuid":"23ad055555c65dc12adcb66bc252f03c614f3d1c"},"cell_type":"code","source":"# Now looking at the port of embarkation\njoin_data['Embarked'].value_counts()\njoin_data.pivot_table(values='PassengerId',index='Embarked',columns='Pclass',\n                      aggfunc=(lambda x: x.count()),margins=True,margins_name='Total')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1b0d17d8fe2f9a4d2fae21d5a5fa111ed9e7d52e"},"cell_type":"code","source":"# looking at survival rates on the smae populations\ntrain_data.pivot_table(values='Survived',index='Embarked',columns='Pclass',\n                      margins=True,margins_name='Total')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e333382300853315621358b0134d17efbd7442be"},"cell_type":"markdown","source":"It seems that those that embarked on 'S' have a lower survival probabilty, however, when we look at the passenger counts it is clear that most of the 3rd class embarked there, therefore, this might be the real driver\n\n##### For simplicity, for the few records missing we can use the most common port 'S'\n\nNow let's move to the next feature\n\n## ==== Fare ====\n\nLet's start by the basic step of imputing the missing value with the average per class and gender"},{"metadata":{"trusted":true,"_uuid":"cea437a7803e5305704c4aac01f050cbc7ba4468"},"cell_type":"code","source":"join_data.pivot_table(values='Fare',index='Pclass',columns='Sex')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"896a2fe11eae53f64fbf46ac8ec107c67a6a9bc7"},"cell_type":"code","source":"# Creating a simple funtion to impute the Fare, the thresholds are according to the average Fares on the training data\ndef FareImp (Columns):\n    # Gattering the fields:\n    Fare = Columns[0]\n    Pclass = Columns[1]\n    Sex = Columns [2]\n    # First, we only impute if the Fare value is missing\n    if pd.isnull(Fare):\n        if Pclass == 3 :# Third class\n            if Sex == 'male':\n                return 12.41\n            else:\n                return 15.32\n        if Pclass == 2 :# Second class\n            if Sex == 'male':\n                return 19.90\n            else:\n                return 23.23\n        if Pclass == 1 :# First class\n            if Sex == 'male':\n                return 69.88\n            else:\n                return 109.41\n    else:\n        return(Fare)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7180bb8e54b7747e84dd28086e5ab8719ea5bc71"},"cell_type":"markdown","source":"Now, on the FARE feature there is a catch we need to get out of the way, first, if you explore the data you might have notice that a lot of people share cabins, to illustrate why this point matters let's pull one cabin:"},{"metadata":{"trusted":true,"_uuid":"580abddff35679fcbef867d8b3ca8629e0569198"},"cell_type":"code","source":"join_data[join_data['Cabin']=='F4']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7b76f7eff0be74541dd87b95b01d7581988c3deb"},"cell_type":"markdown","source":"We can see this is a family of 4, notice that they all paid the same fare, for every one passenger we could calculate their 'FamilySize' by adding the 'Parch' + 'SibSp' + 1 (Partners + Sibling/childs + themselves), let's add this variable and evaluate the fares accordingly"},{"metadata":{"trusted":true,"_uuid":"f6b669b76a8b4656de98c85674b6ad7dacf8b044"},"cell_type":"code","source":"join_data['FamilySize'] = join_data['Parch'] + join_data['SibSp'] + 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2737bfabbb409490a578d3825d73e98ec4769bf1"},"cell_type":"code","source":"# Looking also at the numbers\njoin_data.pivot_table(values='Fare',index='Pclass',columns='FamilySize',margins=True,margins_name='Average')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eaa8b7fd41a6e2149eb3e764279639057fcfc175"},"cell_type":"markdown","source":"We can see clearly that the Fares increase significantly (for the same class) with the family size, considering that most families would share a cabin this makes no sense, unless, we assume that the reflected Fare is the price paid by the cabin for all members sharing it, therefore the actual Fare per passenguer was most likely the reflected value divided by the Family size.\n\nBefore moving to the next section let's take a last look at the remaining features:\n\n## ==== Name, Ticket, Parch, SibSp ====\n\nAs you can see on the Fare analysis with the records for the family, those traveling together also share the same 'Ticket' number, beyond that, some tickets add accronyms preceding the ticket number to document the selling agency, none of this information seem to add over dimensions we are already capturing on the 'FamilySize' variable.\n\nRegarding the Parch and SibSp, we are summarizing those already, also on the Family size variable, therefore we should drop those features.\n\nFinally, let's take a short view at the name, there are probably a 1000 things that could be done\n"},{"metadata":{"trusted":true,"_uuid":"5070e24083732d2097221670b29d1bf61046ef22"},"cell_type":"code","source":"join_data['Name'].head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2281ff7774cb29995621ecef973009e6f2bda421"},"cell_type":"markdown","source":"From a first look, there is a 'Title' associated to each person, it is esencially whatever precedes the '.', we could capture that and see what is the dependancy to survival"},{"metadata":{"trusted":true,"_uuid":"66673b94e5fb2a1ec6e88c98b8f52d8e8b16c05f"},"cell_type":"code","source":"'''\nThe firts step in to getting the 'Title' out of a name is extract it out of the full string, we are going to create a\nshort function to do just that\n'''\ndef ExtractTitle (Name):\n    namewords = Name.split()\n    for word in namewords:\n        if '.' in word:\n            return (word)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dde6f6e8592868b0af9029ac2f9c760f9c18d15c"},"cell_type":"code","source":"# let's test our new function, let's first locate a random name:\ntestname = join_data['Name'].iloc[10]\ntesttitle = ExtractTitle(testname)\nprint('Name: {}, Title: {}'.format(testname,testtitle))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1ede6327916d1a4664cdb1db07d926052af2013a"},"cell_type":"code","source":"# Now let's create a field for the title within our data set\njoin_data['Title'] = join_data['Name'].apply(ExtractTitle)\njoin_data['Title'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"53b06746d38bac0a535eaad3d7108243ced0423b"},"cell_type":"markdown","source":"We can see that there are many titles with really low occurence, also, many others that have the same meaning, let's copy and paste the list above and create a dictionary to consolidate"},{"metadata":{"trusted":true,"_uuid":"d01fad98912a86295112479bff6a1ea80d79613e"},"cell_type":"code","source":"# Copying the list above and simplify\nTitleDict = {'Mr.':'Mr',\n             'Miss.':'Miss',\n             'Mrs.':'Miss',\n             'Master.':'Master',\n             'Rev.':'Mr',\n             'Dr.':'Mr',\n             'Col.':'Mr',\n             'Major.':'Mr',\n             'Ms.':'Miss',\n             'Mlle.':'Miss',\n             'Sir.':'Mr',\n             'Countess.':'Miss',\n             'Capt.':'Mr',\n             'Don.':'Mr',\n             'Lady.':'Miss',\n             'Jonkheer.':'Mr',\n             'Dona.':'Miss',\n             'Mme.':'Miss'}\n# Finally, let's apply our translation to the Title field and see how it relates to survival\njoin_data['Title'] = join_data['Title'].map(TitleDict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9c570a419839acd9ee98e1add68bd870bfeba1bf"},"cell_type":"code","source":"# After this steps we can merge the extraction and mapping in to a single function for future use\ndef ExtractAndMapTitle (Name):\n    namewords = Name.split()\n    for word in namewords:\n        if '.' in word:\n            return (TitleDict[word])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0c9f4e5bc72732a93ebc8c10c87d24b05651584d"},"cell_type":"code","source":"join_data.pivot_table(values='Survived',index='Title',columns=['Pclass','Sex'],\n                      margins=True,aggfunc=(lambda x: x.count()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b6da4fc1e866b98a105d7596e283fd178106122d"},"cell_type":"markdown","source":"#### We can see that we have a 'female' Mr, that doesn't make sense, after looking for it, it turns out to be a female Dr,  let's correct this by assigning all females as 'Miss'"},{"metadata":{"trusted":true,"_uuid":"c3631abe11431a95a387aa6e58574d19f8e970fd"},"cell_type":"code","source":"join_data['Title'][join_data['Sex'] == 'female'] = 'Miss'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"32e5271e07fe997b2b7ecd5b26c36569dc8283ff"},"cell_type":"code","source":"join_data[join_data['Title'] == 'Master']['Age'].plot.hist()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"72f9fe2d84f70a29411dccd6d725eb84c01d02de"},"cell_type":"markdown","source":"#### Also, the title master describes a male kid under 15, not sure how much extra information the title is offering over what is already available with Sex and Age, we will keep it and see if they are useful\n\nFinally, before moving on to implement all the actions we have identified there are some opportunities to tune our performance given the particularities of the data, by these I mean in relation to the conversion of the categorical values and the scaling of the data\n\nAt some point we will need to transform all our categories in to numerical values, we can do these two ways:\n    - Assigning a 'numerical' index to each relevant category (i.e Mr=0, Miss=1 etc)\n    - Creating a dummy variable flag that expands the categorical value in to n-1 features of true/false expressed as 0 or 1 (i.e from embarkation port 'S','C' and 'Q', to feature 'Embarked-Q' (as 0 or 1) and 'Embarked-S' (as 0 and 1), there is no need for a 'Embarked-C' since the scenario where the other two are 0 already conveys that information.\n    \nThe right approach will deepend mostly on the question if the 'order' of the categorical labels conveys information or not, for example for the title, if Mr=1 and Miss=2 or Mr=2 and Miss=1 makes no difference, however, for the Deck field, if B=2 and C=3 or B=0 and C=5 is significant since B is a deck above C with one degree of separation (illustrative example, I am no boat expert to know this for a fact) therefore the order and difference between them matter.\n\nalso, when we have multiple possible values, we could simple keep them all and scale the data or generalize them in to buckets or bin (i.e Age can be transformed in to 'child','kid','adult' and 'senior'. Because of these potential variances we are going to build a superset that incorporate them all and then create sub-sets than given the basic a most common practice data set (scale with dummy variables) we modified one at the time to validate what is the impact on the performance of our model.\n\n### Now looking at the ticket field\n"},{"metadata":{"trusted":true,"_uuid":"a00c62402c59471f3472415a40aa5da35081bf77"},"cell_type":"code","source":"# Let's start by isolating those passenguers that are connected through the ticket number\ntestDF = train_data.copy()\ntestDF['SharedTicket'] = testDF['Ticket'][testDF['Ticket'].duplicated(keep=False)]\nprint(testDF.describe(include=['O']))\n\ntestDF.pivot_table(values='SharedTicket',index='Survived',columns='Pclass',fill_value=0,\n                      aggfunc=(lambda x: x.count()),margins=True,margins_name='Total')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"76d35cd17b1efb66b91066abfbada7ac15bcc132"},"cell_type":"markdown","source":"#### We can see that there are 134 'groups' of people that share the same ticket numbers, half of them make it, half of them don't, we know that most women and children within the same groups shared the same destiny together, so we need to identify those groups.\n\n#### Also, from other Kernels we know that some relatives travelled together but stayed on others cabins with consecutive ticket numbers, so let's combine this criteria along with the last name to identify family groups and their survival status, there is a nuance here since that might be several different last names and ticket numbers, therefore this process will need to be recursive to ensure all relationships are captured"},{"metadata":{"trusted":true,"_uuid":"df3ff8101b10a8285bdff38da958d3ef91deb9fd"},"cell_type":"code","source":"# First we need to capture the last name of each passenguer a create a family group ID with the ticket #\ndf = join_data.copy()\ndf['LastName'] = df['Name'].apply(lambda x: x.split(',')[0])\ndf['TicketP'] = df['Ticket'].apply(lambda x: x.split()[-1][:-2]) # Removing the last two digits of the ticket\ndf['FamilyGroup'] = df['LastName']+df['TicketP']+'XX' # replacing last two digits for XX\n\n# Now we link together all the tickets and family names that are related\ndf2 = df.groupby(['Ticket'])\nknowtickets = []\nknowfamilies = []\nTicketList={}\n\n# filling up a dictionary that relates each ticket # to a unique family group linking all related passengers\nfor tick, tick_df in df2:\n    # first we ensure we have not mapped already this ticket number to another group\n    if tick not in knowtickets:\n        # We capture all the ticket/family list groupings associated with this ticket number\n        ticketlist = []\n        ticketlist.append(tick)\n        familylist = []\n        i = 0\n        while i < len(ticketlist):\n            # Capture all the families associated with this particular ticket list\n            families = list(df[df['Ticket']==ticketlist[i]]['FamilyGroup'].unique())\n            for fam in families:\n                if fam not in familylist:\n                    familylist.append(fam)\n                if fam not in knowfamilies:     \n                    # mark the family as 'know'\n                    knowfamilies.append(fam)\n                    # now we recursively capture all the tickets associated with this family and add them to the lookup\n                    tickets = list(df[df['FamilyGroup']== fam ]['Ticket'].unique())\n                    for tick2 in tickets:\n                        if tick2 not in ticketlist:\n                            ticketlist.append(tick2)\n            i = i+1\n        # After we have captured all the associated tickets and families that are linked, then add them on the\n        # dictionary and mark them as know\n        for tick3 in ticketlist:\n            # Mapping all the tickets to the first family name\n            TicketList[tick3] = familylist[0]\n            # adding the tickets to the know tickets\n            knowtickets.append(tick)\n\n# now, we create a field within the data set to document the associated group for each passenger\ndf['GroupID'] = df['Ticket']\ndf.replace({'GroupID':TicketList},  inplace=True)\n# now we capture the survival profile of each group\n\ndf3 = df.groupby(['GroupID'])\nFamilyList = []\nfor fam, fam_df in df3:\n    # if there are more than 1 on the group\n    if len(fam_df) != 1:\n        # Now we analyze what happen with that group\n        # count of members on the group\n        pcount = len(fam_df) \n        # Number of females and kids on the group\n        fkcount = len(fam_df[(fam_df['Age']<15)|(fam_df['Sex']=='female')]) \n        # Count of know survivers\n        KnowSurvivers = int(fam_df['Survived'].sum())\n        # Count of know female and kid survivers\n        KnowFemKidsSurv = int(fam_df['Survived'][(fam_df['Age']<15)|(fam_df['Sex']=='female')].sum())\n        # Count of know victims\n        KnowVictims = fam_df['PassengerId'][fam_df['Survived'] == 0].count() \n        # count of know male adult victims\n        KnowVictimsMA = fam_df['PassengerId'][(fam_df['Survived'] == 0)&(fam_df['Age'] >14)&(fam_df['Sex']=='male')].count()\n        # Generating an index between -1 and 1 that indicated if the group survived or not and to what degree\n        GroupSurvIndex = (KnowSurvivers-KnowVictims)/pcount\n        \n        # Finally, taking a page from other Kernels, creating a flag mapping the survival of the Female and kids\n        if KnowFemKidsSurv > 0:\n            GroupSurvFlag = 1\n        elif (KnowVictims - KnowVictimsMA) > 0: # if some female or kids are know victims\n            GroupSurvFlag = 0\n        else:\n            GroupSurvFlag = 0.5\n        \n        # Capturing all the dimensions\n        FamilyList.append([fam,pcount,KnowSurvivers,KnowVictims,\n                           KnowFemKidsSurv,KnowVictimsMA,GroupSurvFlag,GroupSurvIndex])\n\nFamily_Groups = pd.DataFrame(FamilyList,\n                             columns=['GroupID','PeopleCount','K_Survivers','K_Victims','K_FemKidS',\n                                      'K_MaleV','GroupSurvFlag','GroupSurvIndex'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4d8cd4f5be803771a43f25f4b3b7417fee3ce952"},"cell_type":"markdown","source":"#### Now, that we have an inventory of families and their survival status we use it along a funtion that receives the name and ticket and add two columns, one flag to identify if the person belonged to a family group and the second will be that family group survival status"},{"metadata":{"trusted":true,"_uuid":"12b7f21f8d36a94219a2c630ec02292630dd93d3"},"cell_type":"code","source":"# Creating a function maps passengers to their groups and their know survival\ndef FamilyGrouping (df):\n    df['GroupID'] = df['Ticket']\n    df.replace({'GroupID':TicketList}, inplace=True)\n    # now we add the fields to document the survival profile of the groups\n    df['GroupSurvIndex'] = 0\n    df['GroupSize'] = 1\n    df['GroupSurvFlag'] = 0.5\n\n    GroupInfo = []\n    GroupList = list(Family_Groups['GroupID'])\n    for i in range(len(df)):\n        familyID = df.iloc[i]['GroupID']\n        if familyID in GroupList:\n            SurvStatus = float(Family_Groups[Family_Groups['GroupID'] == familyID]['GroupSurvFlag'])\n            SurvProb = float(Family_Groups[Family_Groups['GroupID'] == familyID]['GroupSurvIndex'])\n            GroupSize = int(Family_Groups[Family_Groups['GroupID'] == familyID]['PeopleCount'])\n            GroupInfo.append([SurvStatus,GroupSize,SurvProb])\n        else:\n            GroupInfo.append([0.5,1,0]) # Passenger traveling alone (group size of 1) with neutral values\n\n    df[['GroupSurvFlag','GroupSize','GroupSurvIndex']] = GroupInfo\n    return(df.drop(['GroupID'],axis=1)) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"baf7f599cc4139824947601acf48d73d61318f8f"},"cell_type":"markdown","source":"### From the EDA we have determined that the following actions will help\n    \n    - Age:\n        - Impute the missing values considering the Class and Gender\n        - Create a new feature 'AgeTier' that groups passangers on Age ranges\n    - Cabin:\n        - Extract the 'Deck' feature\n        - Create a new 'DeckSum' feature that group low occurence decks along with unknow assigning a numerical index\n        - Transform the 'raw' 'Deck' feature in to a numerical index\n        - Drop the Cabin feature\n    - Fare (SibSp, Parch)\n        - Impute the missing values\n        - Create FamilySize variable\n        - Calculate RealFare ('FarePP') per passenger\n        - Create a new feature 'FareTier' that group fares by bins\n        - Tranform the 'FairTier' assigning a numerical index\n        - Drop the SibSp and Parch features since they have been absorved by the Family size feature\n    - Name:\n        - Extract whatever is before the '.' as the new 'Title' feature\n        - Use a dictionary to consolidate the 'Titles' in to a new 'TitleCat' feature\n        - Tranform from categorical to numerical using 'dummy' (aka on-hot)\n        - Extract Family Groups and identify if their survival is 'know'\n        - Drop the 'Name' feature\n    - Sex\n        - This categorical will need to be mapped as a numerical flag\n    - Embarqued\n        - Impute the missing value with the most common ('S')\n        - Tranform from categorical to numerical\n    - Finally, drop the ticket and PassengerID features\n\n#### Since we will be taking this steps over several data sets, and on an actual application repeating them over new batches of data, we will consolidate them in to a subfunction, also leveraging those functions we have already created during the EDA.\n\n## Age ->\n\nImplementing the function to impute the age and to group different age groups"},{"metadata":{"trusted":true,"_uuid":"582726fc037030dfe0214ead2434596a4ac05985"},"cell_type":"code","source":"# For the imputation strategy we are going to use the average ages we capture before per gender and class\n# to achieve this objective we define a short function\ndef AgeImp (Columns):\n    # Gattering the fields:\n    Age = Columns[0]\n    Pclass = Columns[1]\n    Sex = Columns [2]\n    # First, we only impute if the age value is missing\n    if pd.isnull(Age):\n        if Pclass == 3 :# Third class\n            if Sex == 'male':\n                return 26\n            else:\n                return 22.2\n        if Pclass == 2 :# Second class\n            if Sex == 'male':\n                return 30.9\n            else:\n                return 27.5\n        if Pclass == 1 :# First class\n            if Sex == 'male':\n                return 41\n            else:\n                return 37\n    else:\n        return(Age)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d59309d604dd86ce6c9b9fb5fe71eaf21a6c1441"},"cell_type":"code","source":"# the thresholds and somehow arbitrary, we could potential test the sensitivity of the results to one or more of them\ndef AgeMapping (Age):\n    if Age < 15: # Kid\n        return(0)\n    elif Age < 65: # Adult\n        return(0.5)\n    elif Age >= 65: # Senior\n        return(1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cb13d1def58b5fdca261cfe2d399c6d778e14625"},"cell_type":"markdown","source":"# 4-) Data pre-processing"},{"metadata":{"trusted":true,"_uuid":"90808a918700fdb4654acd652bba1fe50381a820"},"cell_type":"code","source":"'''\nData preprocessing function will receive a pandas dataframe containing titanic challenge features as an input, \n11 features are expected (dropping the target variable), this function implements the steps identified on the \nFeature engineering\n\n'''\ndef DataPreprocessing(FeatureDF):\n\n  \n    # == AGE ==\n    # Imputing the missing values on the Age\n    FeatureDF['Age'] = FeatureDF[['Age','Pclass','Sex']].apply(AgeImp,axis=1)\n    # Mapping the Age groups\n    FeatureDF['AgeTier'] = FeatureDF['Age'].apply(AgeMapping)\n    \n    # == CABIN ===\n    # Extracting the 'Deck' Feature\n    FeatureDF['Deck'] = FeatureDF['Cabin'].apply(lambda x: str(x)[0])\n    # Using the dictionary we created during the EDA to consolidate uncommon flags and map all to numeric indexes\n    FeatureDF.replace({'Deck':DeckDict},inplace=True)\n    \n    # == FARE ==\n    # Imputing the missing values\n    FeatureDF['Fare'] = FeatureDF[['Fare','Pclass','Sex']].apply(FareImp,axis=1)\n    # Creating the Family Size feature\n    FeatureDF['FamilySize'] = FeatureDF['Parch'] + FeatureDF['SibSp'] + 1\n    # Calculating the Real Fare per passenger\n    FeatureDF['RealFare'] = FeatureDF['Fare']/FeatureDF['FamilySize']\n    # Group Fare by tiers (using the thresholds of the quartiles)\n    # FeatureDF['FareTiers'] = pd.qcut(FeatureDF['RealFare'], q=4,labels=False)\n\n    # == NAME ==\n    # Capturing the Title and mapping it in to a subset of categories\n    FeatureDF['Title'] = FeatureDF['Name'].apply(ExtractAndMapTitle)\n    # Creating the Family groups survival flags\n    FeatureDF = FamilyGrouping(FeatureDF.copy())\n    # Correcting for female doctors (just 1 on the training data set)\n    FeatureDF['Title'][(FeatureDF['Sex'] == 'female') & (FeatureDF['Title'] == 'Mr')] = 'Miss'\n    # Transforming from categorical to numerical\n    FeatureDF = pd.get_dummies(FeatureDF,columns=['Title'],drop_first=True)\n    \n    # == SEX ===\n    # Transforming from categorical to numerical\n    sex_d = {'male':0,'female':1}\n    FeatureDF.replace({'Sex':sex_d}, inplace=True)\n    \n    # == Embarked ==\n    # Impute missing values with 'S'\n    # FeatureDF['Embarked'].fillna(FeatureDF['Embarked'].mode()[0], inplace = True)\n    # Transform from categorical to numerical\n    # FeatureDF = pd.get_dummies(FeatureDF,columns=['Embarked'],drop_first=True)\n\n    # Dropping all the unused unused features\n    FeatureDF.drop(['PassengerId','Ticket','Cabin','Name','Parch','SibSp','Embarked'],\n                   axis=1,inplace=True)\n    return (pd.DataFrame(scaler.fit_transform(FeatureDF),columns=FeatureDF.columns))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9f4dccaa3835c3f7e71540801a7b1ffeaa172fcf"},"cell_type":"markdown","source":"# 5-) Model selection and testing\n\nWe start by using our pre-processing utility to obtain a feature data set to train our models"},{"metadata":{"trusted":true,"_uuid":"78cb8efe3040cfddd4826c6bed5223906fc0962d"},"cell_type":"code","source":"TD_X = DataPreprocessing(train_data.drop(['Survived'],axis=1).copy())\nTD_y = train_data['Survived']\nK_X = DataPreprocessing(test_data.copy())\n\n# Getting the data splits for training and testing\nX_train, X_test, y_train, y_test = train_test_split(TD_X, TD_y, test_size=0.33,random_state=29)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5580f2599a9945341aba9d9ed7cc2777897fa22a"},"cell_type":"code","source":"fig = plt.figure(figsize=(20,10))\nsns.heatmap(TD_X.corr())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1ccf412c4e5918d2f1522f8d4122221c5296e529"},"cell_type":"markdown","source":"With a few exceptions, most of our features don't seem to be really highly correlated\n\n### Now we are ready to star testing models\n    \n## Trying some alternatives for the models ================================"},{"metadata":{"trusted":true,"_uuid":"652e284649340cd7c08991e4e60945ec7e9caa4c"},"cell_type":"code","source":"'''\nCreating a function to run a grid search over several alternatives\n'''\ndef testmodels(RunDes):\n    # Setting up the support parameters for all runs\n    num_folds = 5\n    seed = 42\n    lastRun = len(models) # We capture how many models we have tried already so we don't re-train them again\n\n    #$$$$$$$$$ Defining all the grid seaches for all approaches to be tried $$$$$$$$$$$$$$$\n\n    ################### For Random Forest\n    parameters_RD = {\"n_estimators\":list(range(80,401,20)),\"max_depth\":[2, 3, 4, 5],\n                     \"min_samples_split\":[2, 3, 10],\"min_samples_leaf\":[1,3,8]}\n    rfc = RandomForestClassifier()\n    models.append(['RandomForest',GridSearchCV(estimator=rfc, param_grid=parameters_RD,scoring='roc_auc',\n                                               cv=num_folds,n_jobs=-1)])\n    ################### For SVC\n    parameters_svm = {\"C\":[2.0, 2.5, 3.0],\"max_iter\":[250, 500, 1000]}\n    svm = SVC()\n    models.append(['SupportVectorM',GridSearchCV(estimator=svm, param_grid=parameters_svm,scoring='roc_auc',\n                                               cv=num_folds,n_jobs=-1)])              \n    ################### For XGBoost\n    XGpd = XGBClassifier()\n    parameters_xg = {'min_child_weight': [1, 5, 10],'gamma': [0.5, 1, 1.5, 2, 5],'subsample': [0.6, 0.8, 1.0],\n                     'colsample_bytree': [0.6, 0.8, 1.0],'max_depth': [3, 4, 5]}\n    models.append(['XG Boost',GridSearchCV(estimator=XGpd, param_grid=parameters_xg,scoring='roc_auc',\n                                               cv=num_folds,n_jobs=-1)])\n    ################### For KNN\n    parameters_knn = {\"n_neighbors\":[6,7,8,9,10,11,12,14,16,18,20,22]}\n    knn = KNeighborsClassifier()\n    models.append(['KNN',GridSearchCV(estimator=knn, param_grid=parameters_knn,scoring='roc_auc',\n                                      cv=num_folds,n_jobs=-1)])\n\n    ########################################################################################\n    ################## Running all the models and capturing the results ####################\n    ########################################################################################\n    i = 0\n    for model, grid in models:\n        # skip on the models already trained\n        if ( i < lastRun):\n            i = i+1\n        else:\n            # first we train our grid\n            grid.fit(X_train,y_train)\n            # Now we test on the holdout data\n            predTrainData = grid.predict(X_test)\n            # Capturing the trained model and its results\n            predTestData = grid.predict(K_X)       \n            ModelResults.append([model,grid.best_score_, precision_score(y_test,predTrainData),\n                                 accuracy_score(y_test,predTrainData),RunDes,predTestData,Field2Drop])\n            i = i+1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ac021abd7bd1325a3ecb750a929858bfc56fb670"},"cell_type":"code","source":"# Creating the overall results containers\nModelResults = []\nmodels = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c0e3e4a16e11f9ec526e493026c693d36c2dbe56"},"cell_type":"code","source":"# Running our first attemp with all the features\n\n# Creating a 'tag' for the run, in case some parameters or inputs are optimized to have the reference of performance\n# of the model after each change\nRunDes = 'Baseline - All Features'\nField2Drop = []\ntestmodels(RunDes)\n\n################## now we format and print our results for visualization\nResultsDF = pd.DataFrame(ModelResults,columns=['Model','Train Accu','Precision',\n                              'Accuracy','Run','Predictions','DroppedFields'])\nResultsDF.sort_values(by=['Accuracy'],ascending=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5982c8c6bddcf339b3b5050ad73d7e669cec97b3"},"cell_type":"code","source":"########################################################################################\n## Using Cross-validation to assess variability of the best model over unseen data  ####\n########################################################################################\nbestM = ResultsDF['Accuracy'].idxmax()\n\nall_accuracies = cross_val_score(estimator=models[bestM][1], X=X_test, y=y_test, cv=3) \nprint(all_accuracies)\nprint('---------------------')\nprint('Average accuracy = {}, deviation = {}'.format(all_accuracies.mean(),all_accuracies.std())) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"452b8fb57eabf572e88634a507460ae9a007c9b6"},"cell_type":"code","source":"# Create the Recurrent Feature Elimination (RFE) object and compute a cross-validated score.\nbestM = ResultsDF['Accuracy'].idxmax()\nrfecv = RFECV(estimator=models[0][1].best_estimator_, step=1, cv=4,scoring='accuracy')\nrfecv.fit(X_test, y_test)\nprint(\"Optimal number of features : %d\" % rfecv.n_features_)\n# Plot number of features VS. cross-validation scores\nplt.figure()\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score (nb of correct classifications)\")\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\nplt.show()\nprint(pd.DataFrame(rfecv.support_,index=TD_X.columns))\nprint(pd.DataFrame(rfecv.ranking_,index=TD_X.columns))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"57cca8881e288d963a60793d11290a48b153cf00"},"cell_type":"code","source":"# TDropping the features that the RFE identified as not adding to much value\nTD_X = DataPreprocessing(train_data.drop(['Survived'],axis=1).copy())\nTD_y = train_data['Survived']\nField2Drop = ['Pclass', 'Age', 'Fare', 'AgeTier', 'Deck', 'FamilySize', 'GroupSize']\nTD_X.drop( Field2Drop,axis=1,inplace=True)\n# Getting the data splits again for training and testing\nX_train, X_test, y_train, y_test = train_test_split(TD_X, TD_y, test_size=0.2,random_state=42)\n# PReparing the test data set too\nK_X = DataPreprocessing(test_data.copy())\nK_X.drop( Field2Drop,axis=1,inplace=True)\n\n# Creating a 'tag' for the run, in case some parameters or inputs are optimized to have the reference of performance\n# of the model after each change\nRunDes = 'Removing some features'\ntestmodels(RunDes)\n\n################## now we format and print our results for visualization\nResultsDF = pd.DataFrame(ModelResults,columns=['Model','Train Accu','Precision',\n                              'Accuracy','Run','Predictions','DroppedFields'])\nResultsDF.sort_values(by=['Accuracy'],ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f9d83a94690eae68af64c544927495f57155ac14"},"cell_type":"markdown","source":"### Trying a neural network"},{"metadata":{"trusted":false,"_uuid":"cd67dd2dc411aa7289a18928d08f3321154aa0be"},"cell_type":"code","source":"'''\nCreating a funtion that builds, trains and test a neural network according to a set of parameters\n'''\ndef NeuralNetwork (X_train, y_train, X_test, parameters):\n    # We start by capturing the pass parameters\n    batch = parameters[0]\n    epochs = parameters[1]\n    hiddenU = parameters[2]\n    \n    # Capturing the feautres in Tensors\n    features = []\n    for column in X_train.columns:\n        features.append(tf.feature_column.numeric_column(column))\n\n    # Defining the input function and training the classifier\n    input_func = tf.estimator.inputs.pandas_input_fn(x=X_train,y=y_train,batch_size=batch,num_epochs=epochs,shuffle=True)\n    classifier = tf.estimator.DNNClassifier(hidden_units= hiddenU,n_classes=2,feature_columns=features)\n    classifier.train(input_fn=input_func,steps=200)\n\n    # testing\n    prediction_func = tf.estimator.inputs.pandas_input_fn(x=X_test,batch_size=len(X_test),shuffle=False)\n    predictions = list(classifier.predict(input_fn=prediction_func))\n\n    # Capturing the results\n    ts_final = []\n    for pred in predictions:\n        ts_final.append(pred['class_ids'][0])\n    return(ts_final)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e50e4ec5f61b4a2f7797aaf27d9fd324c6dbec11"},"cell_type":"code","source":"# Trying cleaning up some features that don't seem to add too much value\nTD_X = DataPreprocessing(train_data.drop(['Survived'],axis=1).copy())\nTD_y = train_data['Survived']\nField2Drop = ['Pclass', 'Age', 'Fare', 'AgeTier', 'Deck', 'FamilySize', 'GroupSize']\nTD_X.drop( Field2Drop,axis=1,inplace=True)\n\n# Preparing the test data set too\nK_X = DataPreprocessing(test_data.copy())\nK_X.drop( Field2Drop,axis=1,inplace=True)\n# Getting the data splits again for training and testing\nX_train, X_test, y_train, y_test = train_test_split(TD_X, TD_y, test_size=0.33,random_state=42)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c13b2b42d4743fe76a4e396555ce1645dc410fb6"},"cell_type":"code","source":"# One time prediction with the best setting for the output to Kaggle\nTestD = DataPreprocessing(test_data.copy())\nNNparam = [40,6,[20,35,40]]\nNNTPred = NeuralNetwork(X_train,y_train,X_test,NNparam)\nNNRPred = NeuralNetwork(TD_X,TD_y,K_X,NNparam)\nNNTaccu = accuracy_score(y_test,NNTPred)\nNNTprec = precision_score(y_test,NNTPred)\nprint('##'*30)\nprint('Neural Network, precision: {}, Accuracy: {}'.format(NNTprec,NNTaccu))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"017cf886d87bc111e7de4dae6872228545ffd11f"},"cell_type":"markdown","source":"The introduction of a Neural Network seem to have added a little more precision we can try to summit both results and see what happens"},{"metadata":{"trusted":false,"_uuid":"5735b65337029781c28796f206699877ecaaaab9"},"cell_type":"code","source":"# writing the output file\nresult_df = pd.DataFrame(list(NNRPred),index=test_data['PassengerId'],columns=['Survived'])\nresult_df.to_csv('TitanicPredictions-NN.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8c8cbe89b69a870a389e6cccf92f6d2b382979e0"},"cell_type":"markdown","source":"# 6-) Output generation"},{"metadata":{"trusted":false,"_uuid":"721a180acd58e6094328b8fc22c7847b5a66c897"},"cell_type":"code","source":"########################################################################################\n###################### Generating the final results for Kaggle #########################\n########################################################################################\n# Using the best model we got\nbestM = ResultsDF['Accuracy'].idxmax()\nField2Drop = ModelResults[bestM][6]\n# Generating the prediction with the best possible model\nTest_X = DataPreprocessing(test_data.copy())\nTest_X.drop( Field2Drop,axis=1,inplace=True)\nFinalPred = models[bestM][1].predict(Test_X)\n\n# writing the output file\nresult_df = pd.DataFrame(list(FinalPred),index=test_data['PassengerId'],columns=['Survived'])\nresult_df.to_csv('TitanicPredictions-Final.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c33168589ba44629c9ee4164f98dee288dd4954c"},"cell_type":"markdown","source":"### This notebook contain just the tip of the iceberg, I personally attempted many other approaches and iterations through different paths that didn't provide better results, also, as mentioned at nauseum on every other kernels and forums, the test data for this particular challenge don't seem to be that consistent with the training set, therefore, as good as the approach could appear on cross-validation it didn't performed better than 0.81 on multiple attemps.\n\nIf you found this notebook helpful or you just liked it, some upvotes would be very much appreciated - That will keep me motivated :)"},{"metadata":{"trusted":false,"_uuid":"1f91d8d626b21cce3a724da6f85f0277a28a97ed"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}