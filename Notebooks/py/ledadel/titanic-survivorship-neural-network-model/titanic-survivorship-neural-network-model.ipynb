{"nbformat": 4, "cells": [{"source": ["This is my first Kaggle project, a Titanic survivorship model using a simple Neural Network built with tflearn.\n", "I tried to explain each step of the process, and I hope it will help someone who is starting out just like I was.\n", "\n", "Along the way I am reusing some code from the following kernels kindly published:\n", "https://www.kaggle.com/linxinzhe/tensorflow-deep-learning-to-solve-titanic/notebook\n", "https://www.kaggle.com/sinakhorami/titanic-best-working-classifier\n", "\n", "Features used for input (others were dropped):\n", "- Sex (one-hot encoded as 0s and 1s)\n", "- Age (raw values)\n", "- SibSp (raw values)\n", "- Parch (raw values)\n", "- Pclass (one hot encoded and broken into 3 new features one per class)\n", "- Embarked (one hot encoded and broken into 3 new features one per class)\n", "\n", "My Neural Network architecture (built using tflearn):\n", "- Two hidden layers with tanh activations (20 and 40 units) and two dropout layers (0.8)\n", "(though I got pretty good accuracy using a single layer as well)\n", "- Output layer with two units and softmax activation\n", "- Training done using learning rate of 0.01, batch size=32 and 300 epochs"], "metadata": {"_cell_guid": "ab202f5b-5d03-40b0-8616-67aae59304cc", "_uuid": "ecac6559aa49009baf5c1b90c526356e85a5bf16"}, "cell_type": "markdown"}, {"execution_count": null, "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "from IPython.display import display # Allows the use of display() for DataFrames\n", "\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "\n", "# Any results you write to the current directory are saved as output."], "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "d2dcd0ee-e6a6-4859-ba07-7266d7607f24", "_uuid": "7eff4e9deff914d4075b6abfe8b1ee9d47bf07ba"}}, {"execution_count": null, "source": ["#loading data\n", "train_data= pd.read_csv(\"../input/train.csv\")\n", "test_data=pd.read_csv(\"../input/test.csv\")\n", "\n", "display(train_data.head())\n", "display(test_data.head())\n", "print ('train data len',len(train_data))\n", "print ('test_data len',len(test_data))"], "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "2fc962f9-2d3f-46eb-885d-4ec9d77e90a0", "_uuid": "dadf07888a0ff70d3f5b04269e0d4b837b72f383"}}, {"execution_count": null, "source": ["#save PassengerId for evaluation\n", "test_passenger_id=test_data[\"PassengerId\"]"], "outputs": [], "cell_type": "code", "metadata": {"_uuid": "d2cfc550924c1ead0075c82a086ba7e81f1e187c", "collapsed": true, "_cell_guid": "8e91e7a7-278d-4034-89ee-c0fbb3197fbe"}}, {"execution_count": null, "source": ["# Store the 'Survived' feature in a new variable 'outcomes' and remove it from the dataset \n", "# test dataset doesnt have this feature since we will be using it to test our prediction model\n", "outcomes = train_data['Survived']\n", "train_data = train_data.drop('Survived', axis = 1)"], "outputs": [], "cell_type": "code", "metadata": {"_uuid": "52bd19da7639a93dff1ac3341921720127ccfde3", "collapsed": true, "_cell_guid": "1796c67b-0d5a-4b3e-9806-23a617310946"}}, {"execution_count": null, "source": ["#combine all data for preprocessing\n", "full_data = pd.concat([train_data, test_data])\n", "print('full data length is',len(full_data))\n", "print ('train data len is',len(train_data))\n", "print ('test data len is',len(test_data))\n", "display(full_data.head())\n", "display(full_data.tail())"], "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "5cc5e0fd-f6bd-4a84-836d-26666a62ac92", "_uuid": "738344182d7785f477e7c1a8b9d99e48b63ecf0b"}}, {"execution_count": null, "source": ["#Removing nonessential features that in my opinion do not have predictive power:\n", "#name,ticket, passengerId, Fare\n", "#also removing Cabin because too many NAs (1014 out of 1309), though I do think it is an interesting feature worth considering\n", "\n", "cabin_null_count = full_data['Cabin'].isnull().sum()\n", "print ('Cabin NA value count:',cabin_null_count)\n", "print ('out of total',len(full_data['Cabin']))\n", "trim_data = full_data.drop(['Name','Ticket','PassengerId','Cabin','Fare'],axis = 1)\n", "display(trim_data[:10])"], "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "05a0f356-ce23-4810-8997-b6253cc66763", "_uuid": "11339f1257e9fe5f99b0f19c7051479af6a55f72"}}, {"execution_count": null, "source": ["# Feature processing: sex, convert into binary\n", "trim_data['Sex'] = trim_data['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n", "display(trim_data.head())"], "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "34ea2c47-1a5c-4906-be67-ce183a8b5e3a", "_uuid": "ebb89fcb057d8845e9d900df06a9d3978dabdb79"}}, {"execution_count": null, "source": ["# Feature processing: Age\n", "#there are many NA values in this feature so first we will replace them with a random number\n", "#generated between (mean - std) and (mean + std) \n", "#approach borrowed from:\n", "#https://www.kaggle.com/sinakhorami/titanic-best-working-classifier\n", "\n", "age_avg   = trim_data['Age'].mean()\n", "print (age_avg)\n", "age_std    = trim_data['Age'].std()\n", "print (age_std)\n", "age_null_count = trim_data['Age'].isnull().sum()\n", "print (age_null_count)\n", "    \n", "age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n", "print (age_null_random_list)\n", "trim_data['Age'][np.isnan(trim_data['Age'])] = age_null_random_list\n", "print (trim_data[:20])"], "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "962dfde8-441c-4d52-9221-34750f8fa9db", "_uuid": "478f0dbfca5bf8a62b732d020c07dd4865650577"}}, {"execution_count": null, "source": ["#checking if there are any more NA values in other features: yes, Embarked has more NAs\n", "for feature in trim_data:\n", "    print ( feature,trim_data[feature].isnull().sum())"], "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "a15a50ff-965c-4c2e-8499-d4308a3027e2", "_uuid": "082f1f394deeeb5f7deaecc0af5e51fddd37503c"}}, {"execution_count": null, "source": ["# Feature processing: Embarked\n", "#Embarked feature has only two NA values. \n", "#we will fill those with the most occurred value ( 'S' ).\n", "#approach borrowed from:\n", "#https://www.kaggle.com/sinakhorami/titanic-best-working-classifier\n", "\n", "trim_data['Embarked'] = trim_data['Embarked'].fillna('S')"], "outputs": [], "cell_type": "code", "metadata": {"_uuid": "0bae2397ccb939eadbcaadbef38fe60bfbd2e02d", "collapsed": true, "_cell_guid": "19a08af8-8932-40a4-8d53-9c20f72866f2"}}, {"execution_count": null, "source": ["#Optional feature processing (only if you want to include 'Fare' in your model):\n", "# 'Fare' has one missing value, NA, \n", "#we can fill in with the median value\n", "#approach borrowed from:\n", "#https://www.kaggle.com/sinakhorami/titanic-best-working-classifier\n", "\n", "#trim_data['Fare'] = trim_data['Fare'].fillna(trim_data['Fare'].median())\n", "#print (trim_data[:10])"], "outputs": [], "cell_type": "code", "metadata": {"_uuid": "a4135fff2d78a823a903ba1a3e79e468fbb120da", "collapsed": true, "_cell_guid": "9fc9fcf5-d119-4c68-abf1-ad9630b5ed3c"}}, {"execution_count": null, "source": ["#checking to make sure no more missing values\n", "for feature in trim_data:\n", "    print (feature,trim_data[feature].isnull().sum())"], "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "7eb662ae-3473-4558-b58a-52dd410b2dde", "_uuid": "56f5e2371f434fb01ceceb42ab304b276417b350"}}, {"execution_count": null, "source": ["#Processing feature: Pclass and Embarked\n", "#splitting each feature into new binary features \n", "\n", "def dummy_data(data, columns):\n", "    for column in columns:\n", "        data = pd.concat([data, pd.get_dummies(data[column], prefix=column)], axis=1)\n", "        data = data.drop(column, axis=1)\n", "    return data\n", "\n", "\n", "dummy_columns = [\"Pclass\",'Embarked']\n", "trim_data=dummy_data(trim_data, dummy_columns)\n", "display(trim_data.head())"], "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "e28bf9da-119a-4d50-a23c-49e8194a0f83", "_uuid": "953aa024b8b0fa86d53536079c915ac4d2099468"}}, {"execution_count": null, "source": ["#Optional code (if you want to scale Age data to range[0,1])\n", "#approach borrowed from:\n", "#https://www.kaggle.com/linxinzhe/tensorflow-deep-learning-to-solve-titanic/notebook\n", "\n", "#from sklearn.preprocessing import MinMaxScaler\n", "#features=['Age']\n", "\n", "#def normalize(data,feature):\n", "    #scaler = MinMaxScaler()\n", "    #data[feature] = scaler.fit_transform(data[feature].values.reshape(-1,1))\n", "    #return data\n", "\n", "#for feature in features:\n", "    #trim_data=normalize(trim_data,feature)\n", "\n", "#display(trim_data.head())"], "outputs": [], "cell_type": "code", "metadata": {"_uuid": "21f3e998d2a1f396815312763bce641b633d0df1", "collapsed": true, "_cell_guid": "c8569b0d-7ab1-4afc-926d-fea7bb625e25"}}, {"execution_count": null, "source": ["#splitting data back to train and test set\n", "train_data=trim_data[:891]\n", "test_data=trim_data[891:]\n", "print ('train_data len',len(train_data))\n", "print ('test_data len',len(test_data))"], "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "6ef78ae6-6b70-4cd4-b583-e57be2caf432", "_uuid": "3aa2d586e7af4c276dfa7c7edcafa405899a613a"}}, {"execution_count": null, "source": ["#Converting 'outcomes' to shape (891,2)\n", "from sklearn.preprocessing import LabelBinarizer\n", "lb=LabelBinarizer()\n", "labels=lb.fit_transform(outcomes)\n", "labels=np.hstack((labels,1-labels))\n", "print (labels.shape)\n", "print (labels[:5])"], "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "8fb88db4-2a0b-4393-a3dc-e2c21e6d587f", "_uuid": "2d5492e3d386bc17b0a4a7c3f6fa02ca99109624"}}, {"execution_count": null, "source": ["print (train_data.shape[1])"], "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "f9b6cb90-e392-4c06-91b5-e3d43bce79ba", "_uuid": "448badd1088a14462586ca2e2ab8df7b4c403ff8"}}, {"execution_count": null, "source": ["#convering pd.dataframe into array (this is needed for NN code to work)\n", "print (type(train_data))\n", "train_data=np.array(train_data)\n", "print (type(train_data))\n", "#print (train_data[:5])"], "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "ab932c8a-8633-4732-9dda-54cbb8dc306e", "_uuid": "609518af8a900710e32d6bcab65b3df9e1e67492"}}, {"execution_count": null, "source": ["import tensorflow as tf\n", "import tflearn\n", "# Network building\n", "def build_model():\n", "    # This resets all parameters and variables\n", "    tf.reset_default_graph()\n", "    \n", "    num_hidden_1=20\n", "    num_hidden_2=40\n", "    num_output=2\n", "    \n", "    net=tflearn.input_data([None, train_data.shape[1]])              #input layer\n", "    net=tflearn.fully_connected(net,num_hidden_1,activation='tanh') #hidden layer 1\n", "    net=tflearn.dropout(net,0.8)\n", "    net=tflearn.fully_connected(net,num_hidden_2,activation='tanh') #hidden layer 2\n", "    net=tflearn.dropout(net,0.8)\n", "    net=tflearn.fully_connected(net,num_output,activation='softmax') #output layer\n", "    net=tflearn.regression(net, optimizer='sgd', learning_rate=0.01, loss='categorical_crossentropy')\n", "    model = tflearn.DNN(net)\n", "    return model"], "outputs": [], "cell_type": "code", "metadata": {"_uuid": "a7f859e3031eee1ec2287b094c143e7443b48816", "collapsed": true, "_cell_guid": "826e9d02-0dfc-4670-946c-8f5308fe02c8"}}, {"execution_count": null, "source": ["#Build the model and saving as a variable 'model'\n", "model = build_model()"], "outputs": [], "cell_type": "code", "metadata": {"_uuid": "e926292b3e2af527c37382ff6f8badbb14adf187", "collapsed": true, "_cell_guid": "58265237-f46b-4650-961d-7509f6f1e838"}}, {"execution_count": null, "source": ["# Training or fitting the model to the data. \n", "#validation_set=0.1 reserves 10% of the data set as the validation set. \n", "#You can also set the batch size and number of epochs with the batch_size and n_epoch keywords, respectively. .\n", "model.fit(train_data, labels, validation_set=0.1,show_metric=True,batch_size=32, n_epoch=300)"], "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "c5014f39-44a2-46b0-959b-f39b90644602", "_uuid": "7277f9afaec9288d060583ce819aaea0e1786652", "scrolled": true}}, {"execution_count": null, "source": ["model.predict(test_data)"], "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "90163e0c-a62f-4acf-a11b-5cbe98bd858c", "_uuid": "512668b05264e1b7e08cbd0f16ba20809ad7d462"}}, {"execution_count": null, "source": ["predictions = np.array(model.predict(test_data)).argmax(axis=1)\n", "print (predictions)"], "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "02e616b7-22f0-46c2-a04f-cb73e14976c7", "_uuid": "4b3e511a89ce3bfb5d5dc52837797a04dfd9296d"}}, {"execution_count": null, "source": ["#getting predictions out into the right format for submission:\n", "#https://www.kaggle.com/linxinzhe/tensorflow-deep-learning-to-solve-titanic/notebook\n", "passenger_id=test_passenger_id.copy()\n", "evaluation=passenger_id.to_frame()\n", "evaluation[\"Survived\"]=predictions\n", "evaluation[:10]"], "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "33017d5e-22a4-44f6-baa7-79f8c2bef5ba", "_uuid": "e4ee4841a95ffb03194161f279b7102fd6c8d226"}}, {"execution_count": null, "source": ["# Write the solution to file\n", "evaluation.to_csv(\"evaluation_submission_nn.csv\",index=False)"], "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "f1694400-28b0-4501-8cf0-84be7c66f892", "collapsed": true, "_kg_hide-input": false, "_uuid": "3c1b053444719b863f28698693c3d969bde93876", "_kg_hide-output": true}}, {"execution_count": null, "source": [], "outputs": [], "cell_type": "code", "metadata": {"_uuid": "adcfd7a576b0160e1214ab6ffdb35ea0b9066453", "collapsed": true, "_cell_guid": "250aa838-7cd1-4664-a3bb-ac572636cb98"}}], "nbformat_minor": 1, "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"version": 3, "name": "ipython"}, "pygments_lexer": "ipython3", "name": "python", "file_extension": ".py", "version": "3.6.1", "nbconvert_exporter": "python", "mimetype": "text/x-python"}}}