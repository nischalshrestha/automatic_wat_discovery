{"metadata": {"kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "version": "3.6.1", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "name": "python"}}, "cells": [{"metadata": {"_uuid": "e034945ad404e4ef852ab7af3f281af63f2c2198", "_execution_state": "idle", "_cell_guid": "82d86551-dece-4ecb-bf46-dc4ffba69b64", "collapsed": false}, "outputs": [], "cell_type": "markdown", "execution_count": null, "source": "My first Competition"}, {"metadata": {"_uuid": "0bed8211cdda45e587135ae0f22187ecacbb9693", "_execution_state": "idle", "trusted": false, "_cell_guid": "31f93d70-e00d-45b4-a3a3-955b993fc89f"}, "outputs": [], "cell_type": "code", "execution_count": null, "source": "#Author:Sandeep Ramesh\n\n# Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\n\n# Importing the dataset\ntitanic_train = pd.read_csv('../input/train.csv')\ntitanic_test= pd.read_csv('../input/test.csv')\ntitanic_train.info()\ntitanic_test.info()\ntitanic_train.describe()"}, {"metadata": {"_uuid": "6646b9982d44b5a5b807f343ed8de5c7967c09ac", "_execution_state": "idle", "collapsed": false}, "source": "#plotting the scatter matrix first\nimport matplotlib.pyplot as plt\nfrom pandas.tools.plotting import scatter_matrix\nscatter_matrix(titanic_train, figsize=(25,25))\nplt.show()", "execution_count": null, "outputs": [], "cell_type": "code"}, {"metadata": {"_uuid": "1eb38cd2c616a8feadd73784cbc91b4cfb21d76e", "_execution_state": "idle", "collapsed": false}, "source": "#dropping the columns which might not affect prediction\ntitanic_train=titanic_train.drop(['PassengerId','Ticket'],1)\ntitanic_test=titanic_test.drop(['Ticket'],1)\n\n#To convert Sex to category datatype\ntitanic_train['Sex']=titanic_train['Sex'].astype('category')\ntitanic_test['Sex']=titanic_test['Sex'].astype('category')\n\n#drop cabin because of too many NaN values\ntitanic_train= titanic_train.drop(['Cabin'],1)\ntitanic_test= titanic_test.drop(['Cabin'],1)\n\n#heatmap with correlation \nsns.heatmap(titanic_train.corr())", "execution_count": null, "outputs": [], "cell_type": "code"}, {"metadata": {"_uuid": "0ac2ecccd1aaba7e187928279f56dfff19d55ad9", "_execution_state": "idle", "collapsed": false}, "source": "#filling the Embarked with most frequent column after getting a count\n#sub=pd.DataFrame(titanic_train)\n#sub.to_csv('newtrain.csv')\nsns.factorplot('Embarked',kind='count',data=titanic_train)\ntitanic_train['Embarked'] = titanic_train['Embarked'].fillna(\"S\")\ntitanic_test['Embarked'] = titanic_test['Embarked'].fillna(\"S\")\n\n#converting Embarked to dummy variable and dropping the extra column 'S' for dummy trap since S has low chance\nsns.factorplot('Embarked','Survived',data=titanic_train)\ntitanic_train=titanic_train.join(pd.get_dummies(titanic_train.Embarked,prefix='Embarked'))\ntitanic_train=titanic_train.drop(['Embarked','Embarked_S'],1)\ntitanic_test=titanic_test.join(pd.get_dummies(titanic_test.Embarked,prefix='Embarked'))\ntitanic_test=titanic_test.drop(['Embarked','Embarked_S'],1)", "execution_count": null, "outputs": [], "cell_type": "code"}, {"metadata": {"_uuid": "9e85a9f2b7ffafc6c875863452edbd62de6608db", "_execution_state": "idle", "collapsed": false}, "source": "#missing values in Age and replace with median\nimputer = Imputer(missing_values='NaN',strategy='median',axis=0)\nimputer = imputer.fit(titanic_train[['Age']])\ntitanic_train[['Age']]=imputer.transform(titanic_train[['Age']])\nimputer = imputer.fit(titanic_test[['Age']])\ntitanic_test[['Age']]=imputer.transform(titanic_test[['Age']])\n\n\n\n#missing values in fare and replace with median\nimputer = Imputer(missing_values='NaN',strategy='median',axis=0)\nimputer = imputer.fit(titanic_train[['Fare']])\ntitanic_train[['Fare']]=imputer.transform(titanic_train[['Fare']])\nimputer = imputer.fit(titanic_test[['Fare']])\ntitanic_test[['Fare']]=imputer.transform(titanic_test[['Fare']])\n\n\n#To convert Sex from categorical to dummy variables and determine which gender to drop for dummy variable trap\nsns.factorplot('Sex','Survived',data=titanic_train)\ntitanic_train=titanic_train.join(pd.get_dummies(titanic_train.Sex,prefix='Sex'))\ntitanic_train=titanic_train.drop(['Sex','Sex_male'],1)\ntitanic_test=titanic_test.join(pd.get_dummies(titanic_test.Sex,prefix='Sex'))\ntitanic_test=titanic_test.drop(['Sex','Sex_male'],1)", "execution_count": null, "outputs": [], "cell_type": "code"}, {"metadata": {"_uuid": "5f273a5090e49f79b2a48e894ac4ea5a71df3680", "_execution_state": "idle", "collapsed": false}, "source": "#converting Pclass categorical to dummy variables and determine which class to drop for dummy variable trap\n#titanic_train['Pclass']=pd.get_dummies(titanic_train.Pclass)\nsns.factorplot('Pclass','Survived',data=titanic_train)\nplt.hist(titanic_train.Pclass)    #to determine class 2 or class 3 to drop\ntitanic_train=titanic_train.join(pd.get_dummies(titanic_train.Pclass,prefix='Pclass'))\ntitanic_train=titanic_train.drop(['Pclass','Pclass_3'],1)\ntitanic_test=titanic_test.join(pd.get_dummies(titanic_test.Pclass,prefix='Pclass'))\ntitanic_test=titanic_test.drop(['Pclass','Pclass_3'],1)", "execution_count": null, "outputs": [], "cell_type": "code"}, {"metadata": {"_uuid": "5ab2690b70eeb03b102c03f3ba9b656651ad0f6a", "_execution_state": "idle", "collapsed": false}, "source": "#creating a new feature Title and replace least used titles to commonly used titles\ntitanic_train['Title']=titanic_train.Name.map(lambda x: x.split(',')[1].split('.')[0].strip())\nunique=titanic_train.Title.unique().astype('str')\ntitanic_train['Title']=titanic_train.Title.replace(to_replace=['Don','Rev','Dr','Major','Sir','Col','Capt','Jonkheer'],value='Mr')\ntitanic_train['Title']=titanic_train.Title.replace(to_replace=['the Countess','Lady','Mlle'],value='Mrs')\ntitanic_train['Title']=titanic_train.Title.replace(to_replace=['Ms','Mme'],value='Miss')\ntitanic_train.Title.value_counts()\n\ntitanic_test['Title']=titanic_test.Name.map(lambda x: x.split(',')[1].split('.')[0].strip())\nunique1=titanic_test.Title.unique().astype('str')\ntitanic_test['Title']=titanic_test.Title.replace(to_replace=['Don','Rev','Dr','Major','Sir','Col','Capt','Jonkheer'],value='Mr')\ntitanic_test['Title']=titanic_test.Title.replace(to_replace=['the Countess','Lady','Mlle','Dona'],value='Mrs')\ntitanic_test['Title']=titanic_test.Title.replace(to_replace=['Ms','Mme'],value='Miss')\ntitanic_test.Title.value_counts()\n\n#drop the name feature column\ntitanic_train=titanic_train.drop(['Name'],axis=1)\ntitanic_test=titanic_test.drop(['Name'],axis=1)\n\n#dropping the title 'Mr' to avoid dummy variable trap\nsns.factorplot(x=\"Title\",y=\"Survived\",data=titanic_train)\ntitanic_train=titanic_train.join(pd.get_dummies(titanic_train.Title,prefix='Title'))\ntitanic_train=titanic_train.drop(['Title','Title_Mr'],1)\ntitanic_test=titanic_test.join(pd.get_dummies(titanic_test.Title,prefix='Title'))\ntitanic_test=titanic_test.drop(['Title','Title_Mr'],1)", "execution_count": null, "outputs": [], "cell_type": "code"}, {"metadata": {"_uuid": "a8c95beff4c909f75986d7338c6d9ea663322cb9", "_execution_state": "idle", "collapsed": false}, "source": "#Preparing the dataset for train and test\nx_train=titanic_train.drop(['Survived'],1)\ny_train=titanic_train['Survived']\nx_test=titanic_test.drop(['PassengerId'],1)\n\n\n#Fitting and predicting the various Machine Learning Algorithms\n\nlr=LogisticRegression(random_state=0)\nlr=lr.fit(x_train,y_train)\ny_pred_lr=lr.predict(x_test)\nscore=lr.score(x_train,y_train)\nprint(\"Logistic Regression Classifier score:{}\".format(score))\n\nrfc=RandomForestClassifier(n_estimators=100,criterion='entropy',random_state=0)\nrfc=rfc.fit(x_train,y_train)\ny_pred_rf=rfc.predict(x_test)\nscore1=rfc.score(x_train,y_train)\nprint(\"Random forest Classifier score:{}\".format(score1))\n\n\nknn = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\nknn=knn.fit(x_train, y_train)\ny_pred_knn=knn.predict(x_test)\nscore=knn.score(x_train,y_train)\nprint(\"K Nearest Neighbors Classifier score:{}\".format(score))\n\nnb = GaussianNB()\nnb=nb.fit(x_train,y_train)\ny_pred_nb=nb.predict(x_test)\nscore=nb.score(x_train,y_train)\nprint(\"Naive Bayes Classifier score:{}\".format(score))\n\nsvc = SVC(kernel = 'rbf', random_state = 0)\nsvc.fit(x_train, y_train)\ny_pred_svc = svc.predict(x_test)\nscore=svc.score(x_train,y_train)\nprint(\"Kernel SVM Classifier score:{}\".format(score))\n\n\nxg = XGBClassifier(max_depth=3,n_estimators=100)\nxg.fit(x_train, y_train)\ny_pred_xgb = xg.predict(x_test)\nscore=xg.score(x_train,y_train)\nprint(\"XG Boost Classifier score:{}\".format(score))\n\n#XGB Forest is the best classifier according to the prediction on test set\n\nresults=pd.DataFrame({\"PassengerId\":titanic_test['PassengerId'],\"Survived\":y_pred_xgb})\nresults.to_csv('Final_Output.csv',index=False)\n", "execution_count": null, "outputs": [], "cell_type": "code"}], "nbformat": 4, "nbformat_minor": 0}