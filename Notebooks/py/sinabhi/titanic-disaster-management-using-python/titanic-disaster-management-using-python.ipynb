{"nbformat": 4, "cells": [{"execution_count": null, "cell_type": "markdown", "outputs": [], "source": "Prediction for Titanic Survival. The steps taken for analyzing, wrangling and predicting are - \n\n1.  Importing the train and test data\n2. Feature-wise visualization to get a better idea about the importance of the particular feature and then transforming them into more usable features\n3. Using label encoder to encode the string features into numerical form\n4. Using grid search to find the best parameters for any one of the following algorithms - Random Forest, SVM, XGBoost\n5. Using Cross-Validation to verify the model\n6. Running Prediction on the actual test data", "metadata": {"_execution_state": "idle", "_uuid": "1599f2c080c0f0b5417e25cf39983a5c2d6ac49c", "_cell_guid": "aa6ee494-0df3-4cf1-9e64-3e98b428ff73", "collapsed": false}}, {"execution_count": null, "cell_type": "markdown", "outputs": [], "source": "## Importing the libraries and data ##", "metadata": {"_execution_state": "idle", "_uuid": "07185cf91ff3391f74c95d2560ec823112b773b7", "collapsed": false}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "# pandas\nimport pandas as pd\nfrom pandas import Series,DataFrame\n\n# numpy, matplotlib, seaborn\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\n%matplotlib inline\n\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nimport xgboost as xgb\n\nfrom sklearn.metrics import make_scorer, accuracy_score\nfrom sklearn.model_selection import GridSearchCV\n\ndata_train = pd.read_csv('../input/train.csv')\ndata_test = pd.read_csv('../input/test.csv')\n\n#Combining the train and test data so that feature transformation can be applied to both\ndata_combined = data_train.append(data_test)\n\n#Let's take a peek at the data\ndata_combined.sample(3)", "metadata": {"_execution_state": "idle", "_uuid": "dbd4359f1d7644b0d8a241a90106c41cfa96ea31", "_cell_guid": "38722e49-20db-4a0e-bb9f-d6e192362998", "collapsed": false, "trusted": false}}, {"execution_count": null, "cell_type": "markdown", "outputs": [], "source": "## Descibing the features ##\n\n - PassengerId - Unique Id to identify the passengers\n - Name - Passenger's Name\n - Age - Passenger's Age\n - Cabin - Cabin Number\n - Embarked - City of boarding the ship\n - Fare - Fare for the trip\n - Parch - Count of Parent and Children aboard\n - SibSp - Count of Siblings and Spouse aboard\n - Pclass - Class of travel\n - Sex - Passenger's Sex\n - Ticket  - Ticket Number\n - Survived - Whether the passenger survived the catastrophe or not\n\n----------\n\n**Type of features -** \n\nID Features - PassengerId, Name <br>\nCategorical Features - Cabin, Embarked, Pclass, Sex, Ticket <br>\nNumerical Features - Age, Fare, Parch, SibSp <br>\nLabel Column - Survived <br>\n\n", "metadata": {"_execution_state": "idle", "_uuid": "bb7770cc5a9e11033a8ad376ae7b5b92b436a924", "collapsed": false}}, {"execution_count": null, "cell_type": "markdown", "outputs": [], "source": "## Feature transformation ##", "metadata": {"_execution_state": "idle", "_uuid": "d5ea0270e2ff0f50df38587e37a1031e48eabb33", "_cell_guid": "68faf9e6-1206-4402-bcc9-a252927b8434", "collapsed": false}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "#Name\n\n#Taking out title from Names\ndata_combined['Title'] = data_combined['Name'].map(lambda name:name.split(',')[1].split('.')[0].strip())\ntitle_dict = {\n    \"Mr\": \"Mr\",\n    \"Miss\": \"Miss\",\n    \"Mrs\": \"Mrs\",\n    \"Master\": \"Master\",\n    \"Dr\": \"Rare\",\n    \"Rev\": \"Rare\",\n    \"Col\": \"Rare\",\n    \"Mlle\": \"Miss\",\n    \"Major\": \"Rare\",\n    \"Sir\": \"Rare\",\n    \"Ms\": \"Miss\",\n    \"the Countess\": \"Rare\",\n    \"Lady\": \"Rare\",\n    \"Jonkheer\": \"Rare\",\n    \"Mme\": \"Mrs\",\n    \"Don\": \"Rare\",\n    \"Capt\": \"Rare\",\n    \"Dona\": \"Rare\"\n}\n\ndata_combined['Title'] = data_combined['Title'].map(title_dict)\nprint(data_combined.groupby(['Sex','Title']).size())", "metadata": {"_execution_state": "idle", "_uuid": "161a6cfee1c6a2ef22d8dc04dfea0a837c8824fd", "_cell_guid": "fd7144e8-2678-451b-9a49-f4c01e29403e", "collapsed": false, "trusted": false}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "# Family\n\n# Instead of having two columns Parch & SibSp, \n#We can have the total family size for each individual\n# Meaning, if having any family member(whether parent, brother, ...etc) will increase chances of Survival or not.\ndata_combined['FamilySize'] =  data_combined[\"Parch\"] + data_combined[\"SibSp\"] + 1\ndata_combined['Singleton'] = data_combined['FamilySize'].map(lambda s: 1 if s == 1 else 0)\ndata_combined['SmallFamily'] = data_combined['FamilySize'].map(lambda s: 1 if 2<=s<=4 else 0)\ndata_combined['LargeFamily'] = data_combined['FamilySize'].map(lambda s: 1 if 5<=s else 0)\n\n# plot\nsns.factorplot('FamilySize', hue='Survived', data=data_combined,kind='count')", "metadata": {"_execution_state": "idle", "_uuid": "38f7a10a4ef28e1c7eb92f1f2ce247601058a935", "_cell_guid": "a75f00e4-4141-4ebb-87f0-0a98d8898b16", "collapsed": false, "trusted": false}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "# Embarked\n\n# only in titanic_df, fill the two missing values with the most occurred value, which is \"S\".\ndata_combined[\"Embarked\"] = data_combined[\"Embarked\"].fillna(\"S\")\n\n# plot\nsns.factorplot('Embarked','Survived', data=data_combined,size=4,aspect=3)\nfig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(15,5))\nsns.countplot(x='Embarked', data=data_combined, ax=axis1)\nsns.countplot(x='Survived', hue=\"Embarked\", data=data_combined, order=[1,0], ax=axis2)\n# group by embarked, and get the mean for survived passengers for each value in Embarked\nembark_perc = data_combined[[\"Embarked\", \"Survived\"]].groupby(['Embarked'],as_index=False).mean()\nsns.barplot(x='Embarked', y='Survived', data=embark_perc,order=['S','C','Q'],ax=axis3)", "metadata": {"_execution_state": "idle", "_uuid": "79ffabf72702d383c973738ff60a0350906bee4c", "_cell_guid": "bf88f652-0147-4523-bd74-d8e7e3a941f9", "collapsed": false, "trusted": false}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "# Age - We are passing age as a numerical column\n\nfig, (axis1,axis2) = plt.subplots(1,2,figsize=(15,4))\naxis1.set_title('Original Age values - Titanic')\naxis2.set_title('New Age values - Titanic')\n\n# get average, std, and number of NaN values in data_train\naverage_age_titanic   = data_combined[\"Age\"].mean()\nstd_age_titanic       = data_combined[\"Age\"].std()\ncount_nan_age_titanic = data_combined[\"Age\"].isnull().sum()\n\n# generate random numbers between (mean - std) & (mean + std)\nrand_1 = np.random.randint(average_age_titanic - std_age_titanic, average_age_titanic + std_age_titanic, size = count_nan_age_titanic)\n\n# plot original Age values\n# NOTE: drop all null values, and convert to int\ndata_combined['Age'].dropna().astype(int).hist(bins=70, ax=axis1)\n\n# fill NaN values in Age column with random values generated\ndata_combined[\"Age\"][np.isnan(data_combined[\"Age\"])] = rand_1\n\n# convert from float to int\ndata_combined['Age'] = data_combined['Age'].astype(int)\n        \n# plot new Age Values\ndata_combined['Age'].hist(bins=70, ax=axis2)", "metadata": {"_execution_state": "idle", "_uuid": "d1df0a0af4270d5edeccfb71986633cc2fe7000a", "_cell_guid": "b0267cb5-e136-4811-a5c2-8f2be435c44a", "collapsed": false, "trusted": false}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "#Binning Age into a new feature 'AgeClass'\n\ndef getAgeClass(age):\n    if (age<=16):\n        return 0\n    elif (age<=32):\n        return 1\n    elif (age<=48):\n        return 2\n    elif (age<=64):\n        return 3\n    else:\n        return 4\n\ndata_combined['AgeClass'] = data_combined['Age'].apply(getAgeClass)\n\ndata_combined['AgeClass'].value_counts()", "metadata": {"_execution_state": "idle", "_uuid": "509624562cccc5ea18f78aa70748e00bed0386eb", "collapsed": false}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "# Fare\n\n# only for data_test, since there is a missing \"Fare\" values\ndata_combined[\"Fare\"].fillna(data_combined[\"Fare\"].median(), inplace=True)\n\n# convert from float to int\ndata_combined['Fare'] = data_combined['Fare'].astype(int)\n\n# get fare for survived & didn't survive passengers \nfare_not_survived = data_combined[\"Fare\"][data_combined[\"Survived\"] == 0]\nfare_survived     = data_combined[\"Fare\"][data_combined[\"Survived\"] == 1]\n\n# get average and std for fare of survived/not survived passengers\navgerage_fare = DataFrame([fare_not_survived.mean(), fare_survived.mean()])\nstd_fare      = DataFrame([fare_not_survived.std(), fare_survived.std()])\n\n# plot\ndata_combined['Fare'].plot(kind='hist', figsize=(15,3),bins=100, xlim=(0,50))\n\navgerage_fare.index.names = std_fare.index.names = [\"Survived\"]\navgerage_fare.plot(yerr=std_fare,kind='bar',legend=False)", "metadata": {"_execution_state": "idle", "_uuid": "d33c159831abca6b79ef07abd9b43e56c47b19eb", "_cell_guid": "80426de9-b158-4b23-8fa8-d8c09de6c515", "collapsed": false, "trusted": false}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "# Cabin\n\ndata_combined['Cabin'].fillna('N',inplace=True)\ndata_combined['Cabin'].fillna('N',inplace=True)\n\ndata_combined['Cabin'] = data_combined.Cabin.apply(lambda c:c[0])", "metadata": {"_execution_state": "idle", "_uuid": "cced224b616f7bf87595af3b5c30d1b8b47a5fcc", "_cell_guid": "cea1a935-d1f4-48ed-ad00-966bfa32be6e", "collapsed": false, "trusted": false}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "# Child and Mother\n\ndef get_mother(details):\n    sex,parch,age,title = details\n    return 1 if (sex=='female' and parch>0 and age>18 and title!='Miss') else 0\n\ndata_combined['isChild'] = data_combined['Age'].apply(lambda age: 1 if age<18 else 0)\ndata_combined['isMother'] = data_combined[['Sex','Parch','Age','Title']].apply(get_mother, axis=1)", "metadata": {"_execution_state": "idle", "_uuid": "94bf93f5fef17d509e76fdc0bad77315757e580a", "_cell_guid": "82190a7f-af29-4c43-84af-10a129e9c888", "collapsed": false, "trusted": false}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "# Ticket\n\ndef cleanTicket(ticket):\n        ticket = ticket.replace('.','')\n        ticket = ticket.replace('/','')\n        ticket = ticket.split()\n        #ticket = map(lambda t : t.strip(), ticket)\n        #ticket = filter(lambda t : not t.isdigit(), ticket)\n        if len(ticket) > 1:\n            return ticket[0].strip()\n        else: \n            return 'XXX'\n        return ticket\n        \ndata_combined['Ticket'] = data_combined['Ticket'].apply(cleanTicket)", "metadata": {"_execution_state": "idle", "_uuid": "8fd354282a34f3ee86f2dfb17e8ffcc49c8b09f6", "_cell_guid": "51e01d36-29e2-4d55-9ec7-df06356dd9be", "collapsed": false, "trusted": false}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "# drop unnecessary columns, these columns won't be useful in analysis and prediction\ndata_combined = data_combined.drop(['Name', 'Parch', 'SibSp', 'isChild', 'isMother', 'Singleton'], axis=1)\n\ndata_train = data_combined.head(891).drop(['PassengerId'], axis=1)\ndata_test    = data_combined.tail(418).drop(['Survived'], axis=1)", "metadata": {"_execution_state": "idle", "_uuid": "8c0e3e576dc86c35a21256cfd5a6b679357e5b44", "_cell_guid": "7651f955-342c-4f1c-8898-6c43ecc9dace", "collapsed": false, "trusted": false}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "data_combined.head()", "metadata": {"_execution_state": "idle", "_uuid": "a83781bfcebae9381781e08fcfa5f8c833a8c494", "_cell_guid": "9e1ca520-8a18-402a-9575-50eae1e5e47d", "collapsed": false, "trusted": false}}, {"execution_count": null, "cell_type": "markdown", "outputs": [], "source": "## Final Encoding using label encoder ##", "metadata": {"_execution_state": "idle", "_uuid": "bc3fcff1886df067bb7b1bf292f6580088845c67", "_cell_guid": "2336c210-20f6-4b6b-89c8-55f83d3e6d1f", "collapsed": false}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "from sklearn import preprocessing\ndef encode_features(df_train, df_test):\n    features = ['Cabin','Embarked','Sex','Ticket','Title']\n    df_combined = pd.concat([df_train[features], df_test[features]])\n    \n    for feature in features:\n        le = preprocessing.LabelEncoder()\n        le = le.fit(df_combined[feature])\n        df_train[feature] = le.transform(df_train[feature])\n        df_test[feature] = le.transform(df_test[feature])\n    return df_train, df_test\n\ndata_train, data_test = encode_features(data_train, data_test)\ndata_train.head()", "metadata": {"_execution_state": "idle", "_uuid": "d8e7dcb6b5b6e63cb425541cc4e13e3630d45690", "_cell_guid": "b889b626-e588-46de-83da-edf4352597a0", "collapsed": false, "trusted": false}}, {"execution_count": null, "cell_type": "markdown", "outputs": [], "source": "## Splitting up the training data ##", "metadata": {"_execution_state": "idle", "_uuid": "33484d705a684f03b821db30fd1160bf6856a623", "_cell_guid": "da14d86e-71ab-4ebb-bc25-263205d3995f", "collapsed": false}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "from sklearn.model_selection import train_test_split\nX_all = data_train.drop('Survived', axis=1)\nY_all = data_train['Survived']\n\n#We are keeping the ratio between training and validation set to be 0.20\ntest_ratio = 0.20\nX_train,X_test,Y_train,Y_test = train_test_split(X_all, Y_all, test_size=test_ratio, random_state=23)", "metadata": {"_execution_state": "idle", "_uuid": "d35b41972f98c3253075da0d8ef7e60dd466691d", "_cell_guid": "9dde0925-d192-4d23-bfae-faa4f7a5933d", "collapsed": false, "trusted": false}}, {"execution_count": null, "cell_type": "markdown", "outputs": [], "source": "## Fitting and tuning the algorithm ##", "metadata": {"_execution_state": "idle", "_uuid": "578c372343e9a7c24f77950401b503ee705ab167", "_cell_guid": "4614e60b-65b1-4978-8b04-57fbfbcc624f", "collapsed": false}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "#Using XGBoost algorithm\n\nclf = xgb.XGBClassifier()\n\n#Choose some parameter combinations to try\nparameters = {'n_estimators': [500,1000,1500], \n              'learning_rate': [0.02, 0.03, 0.04], \n              'max_depth': [1, 2, 3, 5]\n              }\n\n# Type of scoring used to compare parameter combinations\nacc_scorer = make_scorer(accuracy_score)", "metadata": {"_execution_state": "idle", "_uuid": "b14c1694b430e52c476c744cdf4a75496daef24a", "collapsed": false}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "# Using SVM\n\nclf = SVC()\n\nparameters = {'kernel': ['rbf','linear'],\n              'C': [1, 10, 20]\n            }\n\n# Type of scoring used to compare parameter combinations\nacc_scorer = make_scorer(accuracy_score)", "metadata": {"_execution_state": "idle", "_uuid": "2d446152030b4c6d06c7c080110aa70c2580607a", "collapsed": false}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "#Using Random Forest Classifier\n\nclf = RandomForestClassifier()\n\n#Choose some parameter combinations to try\nparameters = {'n_estimators': [10, 12, 15, 20], \n              'max_features': ['log2', 'sqrt','auto'], \n              'criterion': ['entropy', 'gini'],\n              'max_depth': [10, 12, 15], \n              'min_samples_split': [2, 3, 5],\n              'min_samples_leaf': [1,5,8]\n             }\n\n# Type of scoring used to compare parameter combinations\nacc_scorer = make_scorer(accuracy_score)", "metadata": {"_execution_state": "idle", "_uuid": "67b524c81c1491111055a71eb84b611b35d5a991", "collapsed": false}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "# Run the grid search\ngrid_obj = GridSearchCV(clf, parameters, scoring=acc_scorer)\ngrid_obj = grid_obj.fit(X_train, Y_train)\n\n# Set the clf to the best combination of parameters\nclf = grid_obj.best_estimator_\n\n# Fit the best algorithm to the data. \nclf.fit(X_train, Y_train)", "metadata": {"_execution_state": "idle", "_uuid": "4ff693a9f13a5c12a81980d4d3ea9c1325f67820", "_cell_guid": "b5121824-543e-4793-9bf3-79ba63451900", "collapsed": false, "trusted": false}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "predictions = clf.predict(X_test)\n\nprint(accuracy_score(Y_test, predictions))", "metadata": {"_execution_state": "idle", "_uuid": "a16fbb83b44a8bf1c08c48bb31c79be9f5a4828c", "_cell_guid": "822018bf-3613-440d-b079-e1dcca2f7736", "collapsed": false, "trusted": false}}, {"execution_count": null, "cell_type": "markdown", "outputs": [], "source": "## Validation using KFold ##", "metadata": {"_execution_state": "idle", "_uuid": "203f507e8fee65bb1e044f3989a0ff23124d89cf", "_cell_guid": "3839c31c-9bdf-42c6-8885-6525a1d46442", "collapsed": false}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "from sklearn.cross_validation import KFold\n\ndef run_kfold(clf):\n    kf = KFold(891, n_folds=10)\n    outcomes = []\n    fold = 0\n    for train_index, test_index in kf:\n        fold += 1\n        X_train, X_test = X_all.values[train_index], X_all.values[test_index]\n        Y_train, Y_test = Y_all.values[train_index], Y_all.values[test_index]\n        clf.fit(X_train, Y_train)\n        predictions = clf.predict(X_test)\n        accuracy = accuracy_score(Y_test, predictions)\n        outcomes.append(accuracy)\n        print(\"Fold {0} accuracy: {1}\".format(fold, accuracy))     \n    mean_outcome = np.mean(outcomes)\n    print(\"Mean Accuracy: {0}\".format(mean_outcome)) \n\nrun_kfold(clf)", "metadata": {"_execution_state": "idle", "_uuid": "27fe953726cc1035e676b136d8a7a59e80b2d23f", "_cell_guid": "c73c03b8-7558-4004-82ff-d8e28cc3b284", "collapsed": false, "trusted": false}}, {"execution_count": null, "cell_type": "markdown", "outputs": [], "source": "## Predict the actual test data ##", "metadata": {"_execution_state": "idle", "_uuid": "b0ba74d974e81eea552505fb0e927bb33e55b0b4", "_cell_guid": "907ae4ab-5a07-4fc9-9c3e-e3397a90d671", "collapsed": false}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "ids = data_test['PassengerId']\npredictions = clf.predict(data_test.drop('PassengerId', axis=1)).astype(int)\n\n\noutput = pd.DataFrame({ 'PassengerId' : ids, 'Survived': predictions })\noutput.to_csv('titanic-predictions.csv', index = False)\noutput.head()", "metadata": {"_execution_state": "idle", "_uuid": "07d01315e16fb03b607966ac848a5888f3c7133c", "_cell_guid": "0ca235f6-3cf8-4626-ae15-e7cf2447acfc", "collapsed": false, "trusted": false}}, {"execution_count": null, "cell_type": "markdown", "outputs": [], "source": "**Hope you enjoyed this notebook!! <br><br>\nThe extra step with grid search and KFold-validation helped me to increase my accuracy, since I can get the most optimum parameters for the model. <br><br>\nPlease provide feedback for increasing the efficiency or regarding any suggestions**", "metadata": {"_execution_state": "idle", "_uuid": "47cb0ecd827ddff6a2fdd56b93537e76e3fa85da", "_cell_guid": "6c6ae950-7ac8-4398-ba07-c39e39b64831", "collapsed": false, "trusted": false}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "", "metadata": {"_execution_state": "idle", "_uuid": "cd2d080415f78fc016027d2ab84e3284807e30f3", "collapsed": false}}], "nbformat_minor": 0, "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}, "language_info": {"name": "python", "file_extension": ".py", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "version": "3.6.1", "pygments_lexer": "ipython3", "nbconvert_exporter": "python"}}}