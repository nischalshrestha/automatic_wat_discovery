{"cells":[{"metadata":{"_uuid":"b8f0a5f013cf6d19ce6fb2894a9df2849307255e"},"cell_type":"markdown","source":"# My Take on the Beginner Titanic Kernel\nWelcome to my first kernel! I will be exploring the [Titanic data set](https://www.kaggle.com/c/titanic) as a way to practice and learn data science techniques. Many of the ideas I use here will be shamelessly stolen from other kernels. I will cite any code or plots that are lifted directly from someone else.\n\n### Table of Contents\n1. [Introduction](#introduction-what-we-expect-to-see)\n2. [Import the data and first observations](#import-the-data-and-first-observations)\n3. [Super Simple Model](#super-simple-model)\n4. [Improved Model](#improved-model)\n5. [Predictions](#predictions)"},{"metadata":{"_uuid":"f99c0d2582618490bd5052860ab6d6cbebc2142a"},"cell_type":"markdown","source":"## Introduction: What we expect to see\nThe Titanic data set contains records of passengers aboard the ship and whether they survived or died when the ship sank.\n\nHere are the columns in the training set from the [data dictionary](https://www.kaggle.com/c/titanic/data)\n\n| Variable | Definition                                 | Key                                            |\n|:---------|:-------------------------------------------|:-----------------------------------------------|\n| survival | Survival                                   | 0 = No, 1 = Yes                                |\n| pclass   | Ticket class                               | 1 = 1st, 2 = 2nd, 3 = 3rd                      |\n| sex      | Sex                                        |                                                |\n| Age      | Age in years                               |                                                |\n| sibsp    | # of siblings / spouses aboard the Titanic |                                                |\n| parch    | # of parents / children aboard the Titanic |                                                |\n| ticket   | Ticket number                              |                                                |\n| fare     | Passenger fare                             |                                                |\n| cabin    | Cabin number                               |                                                |\n| embarked | Port of Embarkation                        | C = Cherbourg, Q = Queenstown, S = Southampton |\n\n\nI expect from prior knowledge of this disaster that women, children, and the upper class will have better survival rates. We could make a baseline model using only those features before attempting to engineer others."},{"metadata":{"_uuid":"0bddd2801d9ce91a8015c801b9af2e7f488a9eec"},"cell_type":"markdown","source":"## Import the data and first observations"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Read train and test data into pandas dataframes\ntrain = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\n\n# Take a look at the training data\ntrain.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2499c100381389f3e3e796b762bf854a1e20beb9"},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3dd14dc2ad4bf6ead81cded18304c3cf84ed7266"},"cell_type":"markdown","source":"**Observations**\n* We want to use the gender of the passengers in our model. The values in the column `Sex` are encoded as strings: `female` and `male`. We will have to convert this column to a numerical encoding to use it for modeling.\n* `Age` has a lot of `NaN` values (its count in the training set is 714, compared to 891 rows). Since we expect we will want to use this, we need to either drop all the rows with `NaN` values (which would throw away 177 rows, kind of a lot) or come up with a way to impute the missing data."},{"metadata":{"_uuid":"ee6ccc0c4f99a6ccb957fd15dca0067d4dc7d302"},"cell_type":"markdown","source":"## Super simple model\nWe'll make a basic data set, with the most obvious features: `Age`, `Sex`, and `Pclass`. We will have to clean `Age` and extract `Sex` into a usable form. \n\nOnce we construct the training and test data with these columns, we will use the Gaussian Naïve Bayes model to make predictions and get a score. Later, when we make more sophisticated features and use other models, we can use this score as a baseline."},{"metadata":{"_uuid":"fb716e72e77dff12deb972a067283275d281ee9c"},"cell_type":"markdown","source":"### Encoding Categorical Sex Column\nTo use the `Sex` column, we must convert it from a categorical encoding (which has as its values strings `female` and `male`) to a one-hot encoding (with two columns, `Sex_female` and `Sex_male`, taking values 0 and 1). The way this is often done is with the pandas function `get_dummies`. However, this function can have unwanted side effects on test data. If the training data and the test data do not have the same set of values, then the resulting one-hot encoded training and test data will not have the same number of columns. This causes a problem with downstream models, which require training and test data columns to be identical. So, to use `get_dummies`, we must resolve any problems by hand. At best, this is a fiddly, manual process; at worst this can introduce data leakage of the test set into the training set. Both of these can and should be avoided!\n\nThere are encoders in scikit-learn to take care of this, by using the training set to create the encoded columns and fitting the test set onto those same columns. But as far as I know, they don't handle string categories well (or at all). There is a CategoricalEncoder that is present in the source repository, but has not been released yet. As of now, we can use the `OneHotEncoder` from the `category_encoders` library to do the same function.\n\nI will not do what I have seen in other tutorials, which is to simply map values in the `Sex` column to 1 if `male` and 0 if `female`. For one reason, that approach does not work on categorical variables in general; it presumes we know all the values beforehand. And for another, gender isn't a binary, and I as a matter of princliple will never use a single binary column for it."},{"metadata":{"trusted":true,"_uuid":"1f2a11966b7d560e8097038cb96f6a4136a9a0e7"},"cell_type":"code","source":"import category_encoders as ce\n\nohe = ce.one_hot.OneHotEncoder(cols=['Sex'], handle_unknown='ignore', use_cat_names=True)\ntrain_basic = ohe.fit_transform(train[['Pclass', 'Age', 'Sex']])\n\n# If this were our actual model for submission, we would transform the test data as well\n# test_basic = ohe.transform(test[['Pclass', 'Age', 'Sex']])\n\ntrain_basic.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bdefbec8eb298d070c09414559a1fa6578e0d874"},"cell_type":"markdown","source":"### Impute Missing Age Data: Basic Approach\nNow that we have one-hot encoded `Sex`, we need to impute missing data for `Age`. There are a lot of different ways we could do this, but for this simple model we take the simplest one: find the mean of all values of `Age`, and use that mean to fill in the missing values.\n\nNote that we only take the mean over the training data, and use that mean to fill missing values in both training and test data. Always remember to avoid data leakage!"},{"metadata":{"trusted":true,"_uuid":"09379c19ab2c5e73b7f71b2a02520624c41c8ac7"},"cell_type":"code","source":"from sklearn.impute import SimpleImputer\nimp = SimpleImputer(strategy='mean')\ntrain_basic = imp.fit_transform(train_basic)\n# test_basic = imp.transform(test_basic)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3c7cb86b8b2c916eeafaba3ee41fb602529820af"},"cell_type":"markdown","source":"### Training and Validation Data\nWe need to have a way to score our models without using the testing data. For the purposes of model exploration, we split the training data into a smaller training set and a validation set."},{"metadata":{"trusted":true,"_uuid":"8a36cce026ba8dede6a09b53853c51061de1c4fd"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_validation, y_train, y_validation = train_test_split(train_basic, train['Survived'], random_state=43210)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"27b3d8edd178dac507bd0163d63baca1c2f339af"},"cell_type":"markdown","source":"### Fit a Model and Make Predictions\nIt is time now to predict the survival from passenger records in the test set. \n\nThere are many models we could choose. But, again, this model should stick to basics. We will go through other models later when we have a more interesting data set. So, for now, we will choose Gaussian Naïve Bayes, simply to have some baseline for later comparisons."},{"metadata":{"trusted":true,"_uuid":"a56ad9ad6d950e027213927a8a309b42b8f8b146"},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\n\ngnb = GaussianNB()\ngnb.fit(X_train, y_train)\ngnb.score(X_validation, y_validation)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"30d68bcdd0261535bff0077b89f1162e86761ecd"},"cell_type":"markdown","source":"We got 80% accuracy for our simple model. That isn't terrible! But let's see what else we can do to make it better."},{"metadata":{"_uuid":"11edd3ab662cae5fa8b3011a056b67fd96c56afb"},"cell_type":"markdown","source":"## Improved Model\nHow can we improve on this model? \n* Of the features we included in the simple model, `Pclass` and `Sex` seem to be pretty finished. There isn't much else we can do to improve them. We could improve `Age` by finding a better way to impute the missing values.\n* There are lots of other columns in the data set that we simply ignored when making our basic model. We might be able to pull something out of those.\n\nIn looking at these other columns, we should compare the values we see to `Survival`. We want to see whether some difference in the column's values correlates to a difference in `Survival` values. If we find that, then hopefully we have found a meaningful signal our model can use to improve its accuracy."},{"metadata":{"trusted":true,"_uuid":"3d8ad5d90526e65ca040e2bd3b2eb6761239d093"},"cell_type":"markdown","source":"### Cabin\nSince we expect that passenger class predicts survival, might cabin numbers also be predictive? Let's take a look."},{"metadata":{"trusted":true,"_uuid":"7841c5edaf9ba71616cae8938baf058d65bec939"},"cell_type":"code","source":"train['Cabin'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bf25a9c0778329d2dcb01900e44ab798bbb86e0c"},"cell_type":"code","source":"train['Cabin'].unique()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f9b83519eddb75d819089b13c6ddf222a54f7fa2"},"cell_type":"markdown","source":"#### Cabin Observations\n* A ton of unique values. Which we would expect. Most cabins would only hold a few people.\n* Also a lot of `nan` values.\n* The cabin numbers begin with a letter, which identifies the deck. The lower the letter, the higher the class of the passenger.\n* Given that we see more lower letters and fewer higher letters (few `E`s and almost no `F`s, for instance) we infer that it is more likely that higher-class passengers had their cabins recorded.\n* There is one `T` in there, which is weird. There was no deck `T` on Titanic. We'll check on that passenger.\n* Several entries have more than one cabin. I assume those are families that booked a block of cabins together, and each person had all the cabins on their records. We'll check that.\n* A couple records are anomalous in that they have a letter with no number, but then a different letter+number combo. For instance: \"`F G73`\" and \"`F E69`\". I don't know what to think about those.\n\nLet's look at some of the outlier entries."},{"metadata":{"trusted":true,"_uuid":"7578d960d48bb0d7ae93da5427e05f6c3242ab62"},"cell_type":"code","source":"train[(train['Cabin'] == 'T') | (train['Cabin'] == 'B51 B53 B55') | (train['Cabin'] == 'F E69') | (train['Cabin'] == 'F G73')]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7544fa599b323976a881a222537b28739dd82321"},"cell_type":"markdown","source":"#### Cabin Observations, Continued\nDon't know what to think about those anamalous entries.\n* My hypothesis about multiple cabin numbers being for families is wrong. There are two passengers with cabin entry `B51 B53 B55`. \n    * I thought it would have been three, since there are three cabins.\n    * They aren't in the same family. They have different names. One of them is traveling alone, the other with a parent or child.\n    * They didn't even embark from the same port.\n* Other anomalous cabins are similarly perplexing. I can't find a pattern in what I'm seeing.\n\nI guess I'll just ignore these strange entries and proceed as if they aren't anomalous at all. "},{"metadata":{"_uuid":"92f1f26d3788e4957f22c06ee07ea6a68c6589b0"},"cell_type":"markdown","source":"What we'll do with the cabin is create a numeric feature:\n* We extract one deck letter from the cabin value.\n* We map that letter onto a numeric value (A->7, B->6, etc.)\n* For any missing values, we impute the mean for passengers of the same class."},{"metadata":{"trusted":true,"_uuid":"92af6e177c0636c05250d20ac05dc26f44cc441f"},"cell_type":"code","source":"import re\n\nsingleLetterRe = re.compile(r\"^[A-Z]$\") # This will clean the weird 'T' value\ncabinRe = re.compile(r\"^([A-Z] )?([A-Z])\\d+.*$\")\ndecks = dict(zip('ABCDEFG', range(7, 0, -1)))\n\n# First, make the numeric deck column for train and test, preserving nan\nfor df in (train, test):\n    df['Deck'] = (df['Cabin'].replace(singleLetterRe, np.nan)\n                  .replace(cabinRe, '\\\\2')\n                  .map(decks, na_action='ignore'))\n\n# Next, fill in missing deck values\n# We group decks by pclass and take the mean, then fill all the missing values\n# with the mean for their plass\ndeckmeans = train[['Pclass', 'Deck']].groupby('Pclass')['Deck'].mean()\nfor pclass in 1,2,3:\n    train.loc[(train['Pclass'] == pclass) & (train['Deck'].isna()), 'Deck'] = deckmeans[pclass]\n    test.loc[(test['Pclass'] == pclass) & (test['Deck'].isna()), 'Deck'] = deckmeans[pclass]\nprint(train.groupby('Deck')['Deck'].count())\n\nplt.figure(figsize=(14,6))\nsns.barplot(x=\"Deck\", y=\"Survived\", data=train, ax=plt.gca());","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f0fd753a3d53849d1c2b835058cea7a98c9ec58b"},"cell_type":"markdown","source":"### Cabin: present or not?\nAnother signal we can interpret from the `Cabin` column is whether a cabin was recorded for a passenger or not. "},{"metadata":{"trusted":true,"_uuid":"6681a0ea5782a205b3765017496e000af4371ec0"},"cell_type":"code","source":"for df in train, test:\n    df['cabin_was_recorded'] = ~df['Cabin'].isna()\nsns.barplot(x='cabin_was_recorded', y='Survived', data=train);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5137fc8dad1e4f406a494f57e6e57dd136dd2e39"},"cell_type":"markdown","source":"This looks like it separates the values pretty well. We can include it and see if it will be helpful."},{"metadata":{"_uuid":"443bb423501e7c5a353a4183071a5a8a2247f6e7"},"cell_type":"markdown","source":"### Title\nWe can't really use the passenger's names directly. There is a ton of variation, most of which is noise. Or so I assume; maybe there is a signal lurking in there that I can't see.\n\nOne thing that is somewhat regular in the name is the title. Every passenger's name has some kind of title, like `Mr.` or `Mrs.`. Some are only used for younger passengers, like `Master.` or `Miss.` We can extract this title into a column.\n\nWhile it may or may not be useful on its own, the title can give us a better way to impute missing age values."},{"metadata":{"trusted":true,"_uuid":"0c38aa858ef20b64fcb213dc7fda923881743558"},"cell_type":"code","source":"# This process of splitting gets us the word immediately before a '.'\nfor df in train,test:\n    df['Title'] = df['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\ntrain.groupby('Title')['Title'].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"107b0ca8d713b3b8ae49707ca88e83a9038863b2"},"cell_type":"code","source":"plt.figure(figsize=(18,6));\nsns.barplot(x='Title', y='Survived', data=train, ax=plt.gca());","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8d38654112a347bbf81e73cfbbeedf3a44a86f1f"},"cell_type":"markdown","source":"Okay, so that's all the titles. Doesn't look like we can use it in a model super directly. But how to they break down by age? Let's look at the age distributions for the four most common titles."},{"metadata":{"trusted":true,"_uuid":"75fb94f25b2de12855ce07f8239e57b52cc12362"},"cell_type":"code","source":"fig, axes = plt.subplots(2, 2, figsize=(18,6))\nbins = range(0, 70, 5)\nfor title, ax in zip(('Master', 'Miss', 'Mr', 'Mrs'), axes.flatten()):\n    sns.distplot(train.loc[train['Title']==title, 'Age'].dropna(), bins=bins, kde=False, ax=ax, label=title)\n    ax.legend()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ae99961cb9f3274ae269743571d7d87c1e4ca5bb"},"cell_type":"markdown","source":"This looks to me like you can guess pretty well the age of the passengers by looking at their titles. So that's how we will impute the age values. We'll group by Title, take the mean of the ages, and fill missing values for Age based on their Title's mean. (If any are still missing after that, we can try the same procedure with Sex.)"},{"metadata":{"trusted":true,"_uuid":"49d123accd4b06f66741d03ed9bfa8af0971894c"},"cell_type":"code","source":"titleagemeans = train[['Title', 'Age']].groupby('Title')['Age'].mean()\nfor title in train['Title'].unique():\n    if titleagemeans[title] == np.nan:\n        # If, say, one of the rare titles is missing all age values,\n        # its mean will still be nan.\n        # Skip it for now. We can check later if there are \n        # still nan values for Age to fill in\n        continue\n    train.loc[(train['Title'] == title) & (train['Age'].isna()), 'Age'] = titleagemeans[title]\n    test.loc[(test['Title'] == title) & (test['Age'].isna()), 'Age'] = titleagemeans[title]    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c5538acc1f3d5043d7ee731a51e9f8297b4f658a"},"cell_type":"code","source":"# Now sweep up any values left over\nimp = SimpleImputer(strategy='mean')\ntrain['Age'] = imp.fit_transform(train['Age'].values.reshape(-1, 1))\ntest['Age'] = imp.transform(test['Age'].values.reshape(-1, 1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8e67c6b13c0faca6f0fe08170ebf3cc4026ad8ee"},"cell_type":"code","source":"np.any(train['Age'].isna())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"19755448702f455de44ee02fe3fe70aa6cc257a9"},"cell_type":"markdown","source":"### Age Bins\nNow that we have filled in the missing Age values, let's discretize the data. We can convert these data into bins; let's say we want 5 bins, which would make them of size (80-0)/5=16. \n\nWe'll create this column by hand."},{"metadata":{"trusted":true,"_uuid":"0ec6da625d07fc431530551863666ceae1a3684a"},"cell_type":"code","source":"binsize = 16\nfor df in train, test:\n    df['Age band'] = (df['Age'] - df['Age'].mod(binsize)).div(binsize).astype(int)\ntrain['Age band'].value_counts().to_frame()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3f127d64b04c14e6e71edb4b135faceaa3c64be2"},"cell_type":"markdown","source":"### Traveling with Family or Alone\nNext we try to pull something out of `SibSp` and `Parch`. A reminder of the column definitions:\n*  **sibsp** # of siblings / spouses aboard the Titanic\n* **parch** # of parents / children aboard the Titanic\n\nIn particular, I'm going to look at whether a passenger was traveling alone. Does that have any predictive value we can pull out?"},{"metadata":{"trusted":true,"_uuid":"ac8a0e029e7d15ca8a1c49e54dbb410e3bda2e88"},"cell_type":"code","source":"for df in train, test:\n    df['Alone'] = 0\n    df.loc[(df['SibSp'] == 0) & (df['Parch'] == 0), 'Alone'] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e304c580ee31c1fef02e129a9af8a59cd7345e8f"},"cell_type":"code","source":"sns.barplot(x='Alone', y='Survived', hue='Sex', data=train);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"371ca6ac5873fb9cbbc6c8868ff3519a59aee533"},"cell_type":"markdown","source":"It looks as if being alone gives a small improvement to the survival chance of women, but decreases the survival chance of men.\n\nWhat about different ages? Can we see an effect on whether being alone impacts survival based on age?"},{"metadata":{"trusted":true,"_uuid":"15d19a5a0f0ba54ade7a3f735b87ed5bf10177ad"},"cell_type":"code","source":"g = sns.FacetGrid(train, col='Alone', row='Sex', margin_titles=True, size=5)\ng.map(sns.barplot, 'Age band', 'Survived');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c06df3c31d273eb89f03e4f6d1da176959a2506b"},"cell_type":"markdown","source":"I can see two useful features we could engineer out of here. \n* We could make a binary column for \"Alone_male\". This seems pretty correlated to a low survival rate. (Not depicted in the plots, however, is the 80-year-old man traveling alone who did survive.)\n* The survival rate appears linear for women who are not traveling alone. We can make a column for this, with the values for women not alone being equal to their age band, and the value for everyone else set to -1."},{"metadata":{"trusted":true,"_uuid":"7f8ddf452804d75970916b2e1b92e05fe6fdba4a"},"cell_type":"code","source":"for df in train, test:\n    df['Alone_male'] = 0\n    df.loc[(df['Alone'] == 1) & (df['Sex'] == 'male'), 'Alone_male'] = 1\n    \n    df['Accompanied_female_age_band'] = -1\n    accompanied_females = (df['Alone'] == 0) & (df['Sex'] == 'female')\n    df.loc[accompanied_females, 'Accompanied_female_age_band'] = df.loc[accompanied_females, 'Age band']\n\nfig, axes = plt.subplots(1, 2, figsize=(14,6))\nsns.barplot(x='Alone_male', y='Survived', data=train, ax=axes[0]);\nsns.barplot(x='Accompanied_female_age_band', y='Survived', data=train, ax=axes[1]);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fb9b35184fa58e4d53c0b09fc829dd762816ba03"},"cell_type":"markdown","source":"### Fare\nI don't really know anything about the Fare column. I don't see how it could give us more information than Pclass. But let's check it out."},{"metadata":{"trusted":true,"_uuid":"d54e6acfcb1effaca798b828c63228cd14453dae"},"cell_type":"code","source":"plt.figure(figsize=(10,6))\nsns.distplot(train['Fare'], ax=plt.gca());","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b7fa9dbba71378235698580f22d687a895cdc10e"},"cell_type":"markdown","source":"#### Observations\n* Almost everyone paid very little for their tickets.\n* A few paid more, some a lot more.\n\nWho paid over £500 for their tickets?"},{"metadata":{"trusted":true,"_uuid":"b4be528325e8ac93fe35817684140b2da3f7d32c"},"cell_type":"code","source":"train[train['Fare'] > 500]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d4433d5f6e5e771f01e85c3af80d25a20efd5ae"},"cell_type":"markdown","source":"Since I assume fares are correlated with Pclass, let's examine those together."},{"metadata":{"trusted":true,"_uuid":"5f6e26bd844103c4fa87790cd7ad5e561e11a6bb"},"cell_type":"code","source":"fig, axes = plt.subplots(1, 3, figsize=(18,6))\n\nfor i in range(3):\n    sns.distplot(train.loc[train['Pclass'] == i+1, 'Fare'], ax=axes[i])\n    axes[i].set_title('Fares in Pclass {}'.format(i+1));","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2fdb8d3f8fd12fd117157ab94042992ca68e4b59"},"cell_type":"markdown","source":"I can't make anything out of this. And given that I don't expect the fare a passenger paid would have any correlation to their survival, I'm not going to include it."},{"metadata":{"_uuid":"d2a267f9f6cd1c72c631ce407a207a0c3f59bf54"},"cell_type":"markdown","source":"## Predictions\nLet's return to our Gaussian Naïve Bayes model. Are we doing better than we did before?\n\n### Features\nThe features we are going to keep:\n* Pclass\n* Deck\n* cabin_was_recorded\n* Age band\n* Alone\n* Alone_male\n* Accompanied_female_age_band\n* Sex_male\n* Sex_female\n\nThe features we are going to drop:\n* PassengerId\n* Name\n* Age\n* SibSp\n* Parch\n* Ticket\n* Fare\n* Cabin\n* Embarked\n* Title"},{"metadata":{"trusted":true,"_uuid":"3d87149ee9be4ff2b193913af094c89f776aa8e8"},"cell_type":"code","source":"drop_cols = ['PassengerId', 'Name', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked', 'Title']\ntest_ids = test['PassengerId']\nsurvived = train['Survived']\ntrain = train.drop(drop_cols + ['Survived'], axis=1)\ntest = test.drop(drop_cols, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4f1e6550e8949f849646df72ca1ac6a2843c0652"},"cell_type":"markdown","source":"Lastly, don't forget to encode Sex as a categorical feature"},{"metadata":{"trusted":true,"_uuid":"ab1de1a1040136e08dc777a05bcbfccc119010fc"},"cell_type":"code","source":"ohe = ce.one_hot.OneHotEncoder(cols=['Sex'], handle_unknown='ignore', use_cat_names=True)\ntrain = ohe.fit_transform(train)\ntest = ohe.transform(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"a52d879863e2cede0783e96906a051356c510adc"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b98bd57e016b517d94485ead709a881f53022735"},"cell_type":"code","source":"X_train, X_validation, y_train, y_validation = train_test_split(train, survived, random_state=43210)\n\ngnb = GaussianNB()\ngnb.fit(X_train, y_train)\ngnb.score(X_validation, y_validation)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"58e5e0e1d1b84b41832c726f120906f57e55307f"},"cell_type":"markdown","source":"Ok, our score got worse. That's a bit disheartening."},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"6f8ce41351b62d408845adcb2a0f0da314c8fc1e"},"cell_type":"code","source":"X_train, X_validation, y_train, y_validation = train_test_split(train[['Pclass', 'Age band', 'Sex_male', 'Sex_female']], survived, random_state=43210)\n\ngnb = GaussianNB()\ngnb.fit(X_train, y_train)\ngnb.score(X_validation, y_validation)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fc8ce5ec9c6809bbba90af493fc5d195486a18fa"},"cell_type":"markdown","source":"Well, at least we can recover our 80% score by keeping only the few features we had for our simple model. Still, it doesn't feel great to have done all the work of putting together those features and seeing it make things worse."},{"metadata":{"_uuid":"3bb6ad3bdaf2195ecfb8d788d0c041530fcfeff3"},"cell_type":"markdown","source":"### XGBoost"},{"metadata":{"trusted":true,"_uuid":"4a55a578591f64fbe64e215b5409dacb3110bf4f"},"cell_type":"code","source":"import xgboost as xg\nfrom sklearn.model_selection import cross_val_score\n\nxgb = xg.XGBClassifier(n_estimators=900, learning_rate=0.1)\nresult=cross_val_score(xgb, train, survived, cv=5, scoring='accuracy')\nprint('The cross validated score for XGBoost is:',result.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"04c23412d6ba48cc284de7e4d42fb4c60b7a0912"},"cell_type":"code","source":"xgb.fit(train, survived)\npredictions = xgb.predict(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5c0e9efbd6562becdc94477b9c5a43b3ff09b314"},"cell_type":"code","source":"results = pd.DataFrame()\nresults['PassengerId'] = test_ids\nresults['Survived'] = predictions\nresults.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"66482f3b8b74de2f46701dafa4e60786effacd1a"},"cell_type":"code","source":"results.to_csv('results.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c9f500cce41e415985b2bcd7b2428a45822dead6"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}