{"cells":[{"metadata":{"_cell_guid":"55f8d49b-b528-426c-8e5b-c31b46dc1bbc","_uuid":"ac6bcc3e16a40dedc8180b4b5ccf684621df9c85","collapsed":true},"cell_type":"markdown","source":"# Experimenting with Classification Stacking\n\n**Peformance:** ~ 0.78\n\n_by Nick Brooks, Janurary 2018_\n\n- [**Github**](https://github.com/nicapotato)\n- [**Kaggle**](https://www.kaggle.com/nicapotato/)\n- [**Linkedin**](https://www.linkedin.com/in/nickbrooks7)\n\nNotebook building on my [Exploration of Machine Learning Paradigms](https://www.kaggle.com/nicapotato/titanic-voting-pipeline-stack-and-guide)\n\nThe answer I am trying to answer is \"What is the best way to combine models for Stacking?\". I see a lot of stacking taking place without much thought, merely just averaging out the most performing models.\n\nSince I hope to make observations about model paradigms in this project, my stacked ensembles from my previous notebook will be ommitted. \n\n**Experiments:** <br>\n1. Correlation Matrix\n2. Accuracy Matrix\n3. Mean, Median, Max, MinMax Stacking\n\nI believe that the titanic competition is not the greatest playground for the experiment, since the data-distribution between train and submission set seem out of whack. Nevertheless, lets see what I can find.\n***","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"f69bed4f-8c72-45a1-8b46-b6dbc7878c37","_uuid":"f9caa7a601719b3cbb3eb06a193353ec25dbf87e","collapsed":true,"trusted":true},"cell_type":"code","source":"# General\nimport numpy as np\nimport pandas as pd\n\n# Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nget_ipython().magic('matplotlib inline')\nplt.rcParams['figure.figsize'] = (16, 8)\n#import scikitplot as skplt\nfrom sklearn import metrics\n\nsave = True","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e6328e65-648b-4a71-a0d1-6437ba69b3c2","_uuid":"bed60ad1bf4326ed4e82e4161a8567d0936ea24e"},"cell_type":"markdown","source":"**Train and Test:** <br>\nTrain is the data with survival label, and test is the submission data used for the leaderboard. Train matrix includes the ground truth label.\n\nSoft includes the probabilities, while hard are the binary outputs.","outputs":[],"execution_count":null},{"metadata":{"trusted":true,"_uuid":"ac8f2ea7f48ccbae85c7567da69f45cfda1adb37"},"cell_type":"code","source":"from subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9600f0d0-3dd2-4993-9f5b-f77d33e20a9b","_uuid":"c3a6b1273954db4fd39e950c636db859b10aa0c4","trusted":true},"cell_type":"code","source":"# Hard / Soft for Train / Test\ntrain_hard_pred_matrix = pd.read_csv(\"../input/titanic-voting-pipeline-stack-and-guide/train_hard_pred_matrix.csv\", index_col='PassengerId')\ntrain_soft_pred_matrix = pd.read_csv(\"../input/titanic-voting-pipeline-stack-and-guide/train_soft_pred_matrix.csv\", index_col='PassengerId')\ntest_hard_pred_matrix = pd.read_csv(\"../input/titanic-voting-pipeline-stack-and-guide/test_hard_pred_matrix.csv\", index_col='PassengerId')\ntest_soft_pred_matrix = pd.read_csv(\"../input/titanic-voting-pipeline-stack-and-guide/test_soft_pred_matrix.csv\", index_col='PassengerId')\n\n# CV and Validation from my model building\nresults = pd.read_csv(\"../input/titanic-voting-pipeline-stack-and-guide/titanic_clf_results.csv\")\n\n# Subset top models, excluding previous voting ensembles\nresults = results.loc[(~results[\"Model\"].str.contains(\"Voting\"))\n                      &(results[\"Model\"] != \"XGBsklearn\")\n                      &(results[\"Model\"] != \"stacked\")\n                      &(results[\"CV Mean\"] >= 0.8048),:]\n# View Results\nresults","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"16fc5215-a0df-418a-81fd-6a09a599c9e2","_uuid":"4148a7f9a2924efdafa332d9b9627fc35eb83196"},"cell_type":"markdown","source":"## Binary Stacking:\n<a id=\"COR\"></a>\n\nWhat does it mean to correlate these values?\n\n- Standard pearson correlation isn't optimal for binary variables\n- I wonder if I can introduce traditional classification metrics, such as accuracy","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"0395e555-deae-4d6d-8a30-9b2024161a75","_uuid":"0a519e533eed468f24b5c9e5804a10366479c078","trusted":true},"cell_type":"code","source":"def topmodels(df, isin=results[\"Model\"]):\n    return [x for x in df if x in list(isin)+[\"Survived\"]]\n\n# Subset Models\ntrain_hard_pred_matrix = train_hard_pred_matrix.loc[:,topmodels(df=train_hard_pred_matrix)]\ntest_hard_pred_matrix = test_hard_pred_matrix.loc[:,topmodels(df=test_hard_pred_matrix)]\n\ntrain_hard_pred_matrix.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9e82f910-de4d-4255-aba6-0f903f88a28b","_uuid":"6c4b8855a419865e42167c554ea8cea2fc64fb3b"},"cell_type":"markdown","source":"**Testing Various Metrics:** <br>\nNot quite sure about the different correlation function. Seems like alot of them automatically adjust to binary..","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"95a1d99a-e044-4f59-88d0-425295a0ccd8","_uuid":"40b59c8a9b1739cde42421596875bc1a7b4634cd","trusted":true},"cell_type":"code","source":"np.corrcoef(train_hard_pred_matrix[\"Random_Forest\"], train_hard_pred_matrix[\"Survived\"])[0, 1]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"97b388db-83c2-4cad-ae29-4cad1e7728c7","_uuid":"f0244e0ccb22ef2c38ab06f121c269f149dded12","trusted":true},"cell_type":"code","source":"print(\"Mathew Corr: \",metrics.matthews_corrcoef(train_hard_pred_matrix[\"Random_Forest\"],\n                          train_hard_pred_matrix[\"Survived\"]))\nprint(\"Pandas Corr: \",train_hard_pred_matrix[\"Random_Forest\"].corr(train_hard_pred_matrix[\"Survived\"]))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"236acca8-9696-4e1f-8939-4ba4aae8d484","_uuid":"9c55e8a989c2afc9f86e02aff5b6d140c3cfa793"},"cell_type":"markdown","source":"***\n## Experimental: Introducing the \"Accuracy Matrix\" Heatmap\nInstead of taking the correlation coefficient, I calculate the accuracy score between models. Purely Experimental.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"17f6aa01-8a58-470e-8fe5-ab68ced10d16","_uuid":"959967b8763857242cbe2531177544bb1d36567b","collapsed":true,"trusted":true},"cell_type":"code","source":"def accuracy_matrix(data):\n    df = pd.DataFrame(columns = data.columns, index = data.columns)\n    for row in data.columns:\n        for col in data.columns:\n            df.loc[row,col] = metrics.accuracy_score(data.loc[:,row],data.loc[:,col]).astype(float)\n    for x in df.columns:\n        df[x] = df[x].astype(float)\n    return df\n\ndef acc_n_corr(df):\n    # Train Hard Correlation, All Models\n    f,ax = plt.subplots(1,2, figsize=[18,5])\n    sns.heatmap(df.corr(),annot=True,cmap=\"coolwarm\",cbar_kws={'label': 'Correlation Coefficient'},ax=ax[0])\n    ax[0].set_title(\"Correlation Matrix\")\n\n    # Hard Train Accuracy Matrix, All Models\n    sns.heatmap(accuracy_matrix(df),annot=True,cmap=\"coolwarm\",cbar_kws={'label': 'Accuracy Score'}\n               ,ax=ax[1])\n    ax[1].set_title(\"Accuracy Score Matrix\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ab726db6-d03b-4fd8-af4a-a4998768ee3d","_uuid":"1b498229d6dd220cc9afc9be71ab1ed673879927","trusted":true},"cell_type":"code","source":"acc_n_corr(df=train_hard_pred_matrix)\nplt.show()\nacc_n_corr(df=test_hard_pred_matrix)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"37dd3f00-244d-48da-9736-6dab24f67af2","_uuid":"eea43e4f12c4a7d6ad6785602fcf565fa18da634"},"cell_type":"markdown","source":"**Build a Smart Binary Value Stacker:** <br>\nPretty much, I want to start with the most accurate model, then find the two least similar model, then make them hardvote.\n\nPerhaps I can even iterate this multiple times.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"a89f3d6e-597b-46e7-b13f-bd1b797df764","_uuid":"43037d1625a2ca57990173ee73879f67f53b92fb","collapsed":true,"trusted":true},"cell_type":"code","source":"def voter(df, test_df,results=results.loc[:,\"Model\"],top=1, nsecond=2, target=\"Survived\", submit= False):\n    ref = accuracy_matrix(df.drop(target,axis=1))\n    base = [list(results)[top]]\n    additional = list(ref[base].sort_values(by=base)[:nsecond].index)\n    vote = df[base+additional].mode(axis=1).iloc[:,0]\n    score = metrics.accuracy_score(vote, df[target])\n    test_vote = pd.DataFrame(test_df[base+additional].mode(axis=1).iloc[:,0]).rename(columns={0:\"Survived\"})\n    \n    if save == True & submit == True:\n        test_vote.to_csv(\"{}_{}.csv\".format(base[0],nsecond))\n    \n    return base, additional, test_vote, score","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ab66b6df-1509-4001-83f4-03df7dff9dbe","_uuid":"b43fb64f2c01a01e236805c4f2f5ae0d4cb34e00"},"cell_type":"markdown","source":"**All Combination:** <br>","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"b40b91c5-9da1-47b3-a3c4-02f0290dd892","_uuid":"43f94c4c489fa6e98b6a47010dcca61d00a781db","collapsed":true,"trusted":true},"cell_type":"code","source":"# Save\noutput = pd.DataFrame(columns=[\"Baseline\",\"Secondary_Models\",\"Score\", \"Base_rank\",\"Add_num\"])\n\n# Iterate\nfor x in range(len(list(results[\"Model\"]))):\n    for y in range(1,len(list(results[\"Model\"]))-1):\n        base, additional, temp, score = voter(df=train_hard_pred_matrix,test_df=test_hard_pred_matrix, results=results[\"Model\"],\n                    top=x, nsecond=y, target=\"Survived\", submit=False)\n        output = output.append({'Baseline': base,\"Secondary_Models\": additional,\"Score\": score,\n                               \"Base_rank\": x,\"Add_num\": y}, ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"18d322bc-6486-4b4c-a16d-18358262a5ea","_uuid":"bf121bbe8646c793d1a38f86d96b41366442f5a6","trusted":true},"cell_type":"code","source":"# Submit\nfor x,y in output.sort_values(by=\"Score\", ascending=False).loc[:,[\"Base_rank\",\"Add_num\"]].values[:5]:\n    voter(df=train_hard_pred_matrix,test_df=test_hard_pred_matrix, results=results[\"Model\"],top=x,\n                    nsecond=y, target=\"Survived\", submit=True)\n# View\noutput.sort_values(by=\"Score\", ascending=False)[:5]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f295c742-904c-4917-9d24-c20684b81339","_uuid":"4cd62908b85e8cc06513b37ea195cd863b34c72e"},"cell_type":"markdown","source":"### Probabilistic Stacking\n\nGoing to use a metric for probabilistic model selection.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"d37babd1-b0a2-4a5d-ad92-03736102a2bd","_uuid":"39c1c47a51345bcba224b5380915d8f147508077","collapsed":true,"trusted":true},"cell_type":"code","source":"def roc_AUC(data):\n    df = pd.DataFrame(columns = data.columns, index = data.columns)\n    for row in data.columns:\n        for col in data.columns:\n            df.loc[row,col] = metrics.roc_auc_score(data.loc[:,row],data.loc[:,col]).astype(float)\n    for x in df.columns:\n        df[x] = df[x].astype(float)\n    return df\n\ndef prob_n_corr(df):\n    # Train Hard Correlation, All Models\n    f,ax = plt.subplots(1,2, figsize=[18,5])\n    sns.heatmap(df.corr(),annot=True,cmap=\"coolwarm\",cbar_kws={'label': 'Correlation Coefficient'},ax=ax[0])\n    ax[0].set_title(\"Correlation Matrix\")\n\n    # Hard Train Accuracy Matrix, All Models\n    sns.heatmap(roc_AUC(df),annot=True,cmap=\"coolwarm\",cbar_kws={'label': 'ROC_AUC Score'}\n               ,ax=ax[1])\n    ax[1].set_title(\"ROC AUC Score Matrix\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"db6890f7-6672-444b-83f8-582e04dd8ea7","_uuid":"0e8cd5cb2d517b034bdb4198a6248ab09c6a405b","trusted":true},"cell_type":"code","source":"acc_n_corr(df=train_hard_pred_matrix)\nplt.show()\nacc_n_corr(df=test_hard_pred_matrix)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"96901fe2-942c-4abb-a5ab-9e8e0cba8ea7","_uuid":"790cfb48e485609cd68f1718e07e0cb45b77efc6"},"cell_type":"markdown","source":"## Stacking Pobabilistic Output: [Explore Stacking](https://www.kaggle.com/dongxu027/explore-stacking-lb-0-1463)\n\n1. **Mean Stacking**\n2. **Median Stacking**\n3. **PushOut + Median Stacking**\n4. **MinMax + Mean Stacking**\n5. **MinMax + Median Stacking**\n6. **MinMax + BestBase Stacking**\n\nI wish to combine models based on the accuracy score matrix from the previous section.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"1a85bc13-14c9-4cfe-992c-bf480b3720ac","_uuid":"57b7d3ea57b857170a2ec2b1b79741e954383b29","collapsed":true,"trusted":true},"cell_type":"code","source":"# set up cutoff threshold for lower and upper bounds, easy to twist \ncutoff_lo = 0.8\ncutoff_hi = 0.2\n\n# Gather Descriptive Statistics for Stacking\ndef manual_stack(data):\n    df = pd.DataFrame()\n    df['max'] = data.max(axis=1) # axis = By Row\n    df['min'] = data.min(axis=1)\n    df['mean'] = data.mean(axis=1)\n    df['median'] = data.median(axis=1)\n    #df.index = data.index\n    return df","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7ecb9c74-bbeb-4ff7-bd49-207abbc84d1a","_uuid":"ed374bea32f800ed47a260e7e6a14c5c6025f4c2","collapsed":true,"trusted":true},"cell_type":"code","source":"test_soft_stack = manual_stack(data=test_soft_pred_matrix)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"06dca4b8-58fa-4ddb-8bf2-5f2d534e6caa","_uuid":"78881e82925d1d3f13b08662b645b21557c529db"},"cell_type":"markdown","source":"**Simple Probablistic Stacks:**","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"8700e8b5-7ba6-442f-9853-f61df60e0911","_uuid":"8182f0e11c756dbf1d256addb2085c25981539e3","collapsed":true,"trusted":true},"cell_type":"code","source":"train_soft_pred_matrix = train_soft_pred_matrix.loc[:,topmodels(df=train_soft_pred_matrix)]\ntest_soft_pred_matrix = test_soft_pred_matrix.loc[:,topmodels(df=test_soft_pred_matrix)]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e761aa06-d31d-46d1-b4df-dc0249c2494a","_uuid":"18acdfc449199d90bda0b2bc0977d6d66c2d56d2","collapsed":true,"trusted":true},"cell_type":"code","source":"def simple_submit(df):\n    stacks = manual_stack(df)\n    for x in stacks.columns:\n        stacks[[x]].rename(columns = {x:\"Survived\"}, inplace=True)\n        if save == True:\n            stacks[[x]].to_csv(\"{}_simple_stack.csv\".format(x))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b42ab61b-0727-455a-92b4-a1258f377a45","_uuid":"c90829db7b2fcc7986b0ef208ba0689d1a287374","collapsed":true,"trusted":true},"cell_type":"code","source":"simple_submit(df=test_soft_pred_matrix)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c6d54eca-cee6-4f0d-b222-7b330c5e1576","_uuid":"d0e63b116cf43ec72da6e9c26b68adc4960cd803","collapsed":true,"trusted":true},"cell_type":"code","source":"test_soft_pred_matrix.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"366d02ad-5b10-43b8-8a36-f063c22d7804","_uuid":"8f09a7464dc3b153b486cca1139135f0b8cca1f0","collapsed":true,"trusted":true},"cell_type":"code","source":"def pushout_median(df, median):\n    temp = pd.DataFrame(np.where(np.all(df > cutoff_lo, axis=1), 1, \n                    np.where(np.all(df < cutoff_hi, axis=1),\n                    0, median)), columns=[\"Survived\"]).set_index(df.index)\n    temp.iloc[:,0] = round(temp.iloc[:,0]).astype(int)\n    return temp","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f0d3ff65-db8e-4304-ad06-a47919328b03","_uuid":"6f37dd9c2df08a81826b32257e4c3d7bbca7d677","collapsed":true,"trusted":true},"cell_type":"code","source":"prob_eval = pd.DataFrame()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b0e8ce80-3bc0-406a-bd34-9fe2d1c401f7","_uuid":"7d4742ca71aba9e4ad6a0ced526c436130e91548","collapsed":true,"trusted":true},"cell_type":"code","source":"pushoutmedian = pushout_median(df=test_soft_pred_matrix, median=test_soft_stack[\"median\"])\nprob_eval[\"pushout_median\"] = pushoutmedian.iloc[:,0]\nif save == True:\n    pushoutmedian.to_csv(\"pushout_median.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5c3b5b71-cf12-4142-9971-9c81ab72d59c","_uuid":"c223854461291bba9fa0b8636bc0013368e0a786","collapsed":true,"trusted":true},"cell_type":"code","source":"def stack_evaluation(data, eval_set):\n    stack_results = pd.DataFrame()\n    modelname = data.columns\n    acc = metrics.accuracy_score(data, eval_set)\n    stack_results = results.append({'Model': \"Stack_{}\".format(modelname),\n                                    'Test_Score': acc,}, ignore_index=True)\n    return stack_results","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}