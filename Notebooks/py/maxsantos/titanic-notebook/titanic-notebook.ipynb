{"metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"version": 3, "name": "ipython"}, "mimetype": "text/x-python", "file_extension": ".py", "version": "3.6.3", "name": "python", "pygments_lexer": "ipython3", "nbconvert_exporter": "python"}}, "nbformat_minor": 1, "cells": [{"metadata": {"_uuid": "1953291784aaff3be1ec1c3caca9f65e3e2e0ceb", "_cell_guid": "b6bea7b5-57bf-43b4-9a02-03b3af26de0a"}, "source": ["**Predicting Survival on the Titanic**\n", "\n", "This notebook covers each of the steps taken during my first Kaggle submission, in which I predict whether or not passengers survived on the Titanic. I have used the process to practice what I have learnt so far in Data Science, and to try and demonstrate the different components of a Data Science/Machine Learning project from start to finish. \n", "\n", "**1. Importing Data**\n", "\n", "Firstly, I will import the test and train data sets provided by Kaggle as DataFrames, along with a few useful packages. Throughout this project, I will be using the 'train' data set for my analysis and modelling. The 'test' data set will only be used at the very last stage, to make predictions on for submission to Kaggle. \n"], "cell_type": "markdown"}, {"metadata": {"_uuid": "d9b3dc6e9e4d3e27269a8ae0fab2992f3a430eae", "_cell_guid": "82b31862-72fa-477f-acec-a0e49293b35c", "collapsed": true}, "execution_count": null, "outputs": [], "source": ["import numpy as np \n", "import pandas as pd \n", "from IPython.display import display\n", "\n", "test = pd.read_csv(\"../input/test.csv\")\n", "train = pd.read_csv(\"../input/train.csv\")"], "cell_type": "code"}, {"metadata": {"_uuid": "a81227c10aa97cddeec14947a8d791a34d4315e2", "_cell_guid": "a2b096f0-97bd-4054-a778-273ef5b09159"}, "source": ["**2. Exploratory Data Analysis and Data Visualisation**\n", "\n", "It is important to now inspect the data, to see it's structure and what different features we are dealing with. The table below shows each of the columns (or 'features') that are available, along with their type. It is also clear that both Age and Cabin have a significant number of missing values, whilst Embarked is missing two values. This will be dealt with later on in the project, but is useful to know from the start. \n", "\n", "The obvious first statistic to calculate is the percentage of passengers that survived, which is shown to be 38.4%. In order to predict survival, it is now a good idea to inspect what effect each feature has on this survival rate."], "cell_type": "markdown"}, {"metadata": {"_uuid": "0227c3087e9cabb61e7af275077aacf6d552de04", "_cell_guid": "62beddad-884c-4df7-87da-ea379c27b38f", "collapsed": true}, "execution_count": null, "outputs": [], "source": ["display(train.head())\n", "print(train.info())\n", "\n", "PercSurvived = train[\"Survived\"].value_counts(normalize=True)*100\n", "print(\"\\n\",\"Percentage Survived\",\"\\n\",PercSurvived)"], "cell_type": "code"}, {"metadata": {"_kg_hide-input": false, "_kg_hide-output": false, "_uuid": "a5f0b735016db50d310ba71ea4a8af3f14f16018", "_cell_guid": "ca4b1045-9e99-46ff-96b2-9fdfb19cfbbc", "collapsed": true}, "source": ["At first glance, the features that are likely to have affected survival are Sex, Embarked (showing where passengers boarded) and Pclass (showing the passengers class). The first plot below demonstrates the large difference between the Male and Female survival rates, due to Women and Children being given priority on the lifeboats. "], "cell_type": "markdown"}, {"metadata": {"_uuid": "9fd9b634cee9d302f896e69c763b818b4968a3fb", "_cell_guid": "a57cc345-f090-44c7-b1b8-c88e733dbe67", "collapsed": true}, "execution_count": null, "outputs": [], "source": ["import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "\n", "sex_plot = sns.factorplot(x=\"Sex\", y=\"Survived\", data=train,\n", "                   size=5, kind=\"bar\", palette=\"muted\")\n", "sex_plot.set_ylabels(\"Probability of Survival\")\n", "plt.show()"], "cell_type": "code"}, {"metadata": {"_uuid": "7bc6eef3eac8ca4caff6e2381d509b53445d2c92", "_cell_guid": "0453007d-4e88-4da5-891e-0013d74ba5c8"}, "source": ["There also appears to be a significant difference in survival depending on where passengers Embarked, showing that this may also be a useful feature to consider when fitting an algorithm. The second chart shows a similar (although not identical) trend between Males and Females, and again demonstrates the large affect that gender had on survival. "], "cell_type": "markdown"}, {"metadata": {"_uuid": "257b025b22c8f87ed31671bf418f8a662f86de8d", "_cell_guid": "ad84985a-11f9-4ec0-a0ec-0dd314b36c7d", "collapsed": true}, "execution_count": null, "outputs": [], "source": ["embarked_plot1 = sns.factorplot(x=\"Embarked\", y=\"Survived\", data=train,\n", "                   size=6, kind=\"bar\", palette=\"muted\")\n", "embarked_plot1.set_ylabels(\"Probability of Survival\")\n", "\n", "plt.show()\n"], "cell_type": "code"}, {"metadata": {"_uuid": "581b6d290f16c3a6692be91748f7c10fe876ac0b", "_cell_guid": "c872c2b1-ee2b-46c9-ab1f-18b21a7bd34c", "collapsed": true}, "execution_count": null, "outputs": [], "source": ["embarked_plot2 = sns.factorplot(x=\"Embarked\", y=\"Survived\", hue=\"Sex\", data=train,\n", "                   size=6, kind=\"bar\", palette=\"muted\")\n", "embarked_plot2.set_ylabels(\"Probability of Survival\")\n", "\n", "plt.show()"], "cell_type": "code"}, {"metadata": {"_uuid": "277af94226eebe0f2929a4b8cb07e22c6938247e", "_cell_guid": "ae3cf4a1-4ab5-4a4c-8b85-9a4edf587860"}, "source": ["Lastly, the plot below demonstrates the effect of a passengers class on survival. There is a significant drop in survival with increasing Pclass, especially in females going from 2 to 3. I would be useful to create a similar plot for the 'Age' feature, however, this will be done at a later stage, as this column has many missing values."], "cell_type": "markdown"}, {"metadata": {"_uuid": "9211d4527c280d94e52cae6d0755c726b82b415a", "_cell_guid": "656feb04-fa89-4841-b0b6-0d8c0220d0a5", "collapsed": true}, "execution_count": null, "outputs": [], "source": ["class_plot = sns.pointplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=train, palette=\"muted\")\n", "\n", "plt.show()"], "cell_type": "code"}, {"metadata": {"_uuid": "8c93f07533e3bb7066d38b85b1c440975e48019e", "_cell_guid": "a1c8c60f-3a2e-4c4f-a206-3250d95f69d9"}, "source": ["**3. Cleaning and Preparing the Data**\n", "\n", "Before doing any further analysis or fitting of algorithms, I am going to impute the missing values for the Age column (as this clearly has the potential to be an important feature). I have done this using the median of the Age column, as opposed to the mean, which could be affected by extreme values. \n", "\n", "In order to help with the analysis of the Age column, I will create a new feature called 'Child', which will signify whether a passenger was above or below the age of 13.\n", "\n", "I have also created a 'Family Size' feature, denoting the number of people in a given passengers' family (including siblings, parents/children and themselves), as I feel that this would have been an important factor in a passengers' chances of survival. \n", "\n", "Lastly, I have simplified the Cabin column, to help with determining whether this is an important feature. "], "cell_type": "markdown"}, {"metadata": {"_uuid": "c1d43a5f78b950c01c4ab674ddee54c01aa6f599", "_cell_guid": "f0143080-0e99-4caf-875c-5506d4b005ec", "collapsed": true}, "execution_count": null, "outputs": [], "source": ["# Impute missing values for age in training set\n", "train[\"Age\"] = train[\"Age\"].fillna(train[\"Age\"].median())\n", "\n", "# Create Child column in training set ('Feature Engineering')\n", "train[\"Child\"] = float(\"NaN\")\n", "train.loc[train[\"Age\"] < 13, \"Child\"] = 1\n", "train.loc[train[\"Age\"] >= 13, \"Child\"] = 0\n", "\n", "# Create Family Size column for training set\n", "train[\"Family_Size\"] = train[\"SibSp\"] + train[\"Parch\"] + 1\n", "\n", "# Simplify Cabin column, by slicing off numbers\n", "# NaN Cabin values labelled as 'N'\n", "train[\"Cabin\"] = train[\"Cabin\"].fillna(\"N\")\n", "train[\"Cabin\"] = train[\"Cabin\"].apply(lambda x: x[0])\n"], "cell_type": "code"}, {"metadata": {"_uuid": "e39f13977962efb5b8d7060384f41a961cd0de8b", "_cell_guid": "f619b420-5ebe-4f47-b77d-cb4195f1468f"}, "source": [], "cell_type": "markdown"}, {"metadata": {"_uuid": "3332c2c7e4121a25a993148b54bd6502d885a178", "_cell_guid": "ecb25d2e-98c1-41a0-8ac8-7ce186c19249"}, "source": ["Next I am going to visualise the new features that have been created. The 'Child plot' shows the large difference in survival between male children and adults. Interestingly, the survival rate for females is slightly higher for adults, however, this may be due to the random nature of the data, and may not be representative of the whole data set (especially as there is a large confidence interval on the statistic). \n", "\n", "Family Size appears to have an affect on the survival of males, however the survival rate of females is fairly consistent and appears independant of Family Size. For families of size 5 and above, there is no clear trend, which may be due to a lack of data and small numbers of these large families (which is again demonstrated by the large confidence intervals on these values). \n", "\n", "Similarly, the Cabin plot shows large confidence intervals, which is due to the fact that there are a very large number of missing values in that column. Due to the majority of the Cabin column being NaN values, I am going to ignore this feature for the rest of the project, although I do believe it may have had an affect on survival and would be worth exploring in future analyses. "], "cell_type": "markdown"}, {"metadata": {"_uuid": "6d90facc2e4035aa4c48d340b54f540660a24b1d", "_cell_guid": "76e87fd4-099f-433b-9602-0cc01212de56", "collapsed": true}, "execution_count": null, "outputs": [], "source": ["# Visualise new features\n", "\n", "# Child plot\n", "child_plot = sns.factorplot(x=\"Child\", y=\"Survived\", hue=\"Sex\", data=train,\n", "                   size=6, kind=\"bar\", palette=\"muted\")\n", "child_plot.set_ylabels(\"Probability of Survival\")\n", "\n", "plt.show()\n", "\n", "# Famliy_Size plot\n", "family_plot = sns.factorplot(x=\"Family_Size\", y=\"Survived\", hue=\"Sex\", data=train,\n", "                   size=6, kind=\"bar\", palette=\"muted\")\n", "family_plot.set_ylabels(\"Probability of Survival\")\n", "\n", "plt.show()\n", "\n", "# Cabin plot\n", "cabin_plot = sns.factorplot(x=\"Cabin\", y=\"Survived\", hue=\"Sex\", data=train,\n", "                   size=6, kind=\"bar\", palette=\"muted\")\n", "cabin_plot.set_ylabels(\"Probability of Survival\")\n", "\n", "plt.show()\n"], "cell_type": "code"}, {"metadata": {"_uuid": "74a11bc74541f04fb26cd355f528a9ea37b4facc", "_cell_guid": "df968346-b24b-4dd0-94e3-571b86b814fe"}, "source": ["Next, it is necessary to convert categorical columns to integer values (such as Sex and Embarked), and fill in any last missing values,  in order to fit an algorithm to the data. "], "cell_type": "markdown"}, {"metadata": {"_uuid": "113eeaaac169f364856cc0d1bc2c96be884ce43b", "_cell_guid": "c371ef18-7a2a-4189-a83a-221c6b4090f0", "collapsed": true}, "execution_count": null, "outputs": [], "source": ["# Convert sex to integer values in training set\n", "train.loc[train[\"Sex\"] == \"male\", \"Sex\"] = 0\n", "train.loc[train[\"Sex\"] == \"female\", \"Sex\"] = 1\n", "\n", "# Convert embarked to integer values, and impute missing values\n", "train.loc[train[\"Embarked\"] == \"S\", \"Embarked\"] = 0\n", "train.loc[train[\"Embarked\"] == \"C\", \"Embarked\"] = 1\n", "train.loc[train[\"Embarked\"] == \"Q\", \"Embarked\"] = 2\n", "train[\"Embarked\"] = train[\"Embarked\"].fillna(train[\"Embarked\"].median())\n", "\n", "display(train.head())\n"], "cell_type": "code"}, {"metadata": {"_uuid": "d87f7ca2bbdeb61ade1b46ce880207d5c1f2e6d3", "_cell_guid": "e8a08cf6-8293-4fc8-b19f-5d3ba01cb70d"}, "source": ["**4. Modelling the Data and Making Predictions**\n", "\n", "**Setting Up Training and Test Data**\n", "\n", "I am going to split the 'train' data set in to two sub-sets, one for training different algorithms, and one for fitting them (the 'test' sub-set). I will use a test size of 20%. \n", "\n", "To do this, I will first define the features I want to use, which are Pclass, Sex, Age, Embarked, Child and Family_Size (for reasons discussed above). My target variable (which I will be predicting), is Survived. "], "cell_type": "markdown"}, {"metadata": {"_uuid": "da1a86af46589afefd925bcc339660efa130f40b", "_cell_guid": "70a19031-885c-436f-b40e-d4483c0dddd5", "collapsed": true}, "execution_count": null, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\n", "\n", "# Create feature and target arrays\n", "X = train.drop(['Survived', 'PassengerId','Name','SibSp','Parch','Ticket','Fare','Cabin'], axis=1)\n", "y = train['Survived']\n", "\n", "# Split into training and test set\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=11)"], "cell_type": "code"}, {"metadata": {"_uuid": "36e4cc6a80080a95438fd28bc3b06b28773e3638", "_cell_guid": "6d3018d7-0563-4ffd-b164-3c517be93584"}, "source": ["**First Prediction: Creating and Fitting Random Forest Algorithm**\n", "\n", "For my first prediction I have decided to use the Random Forest algorithm, which makes use of Decision Trees. Decision Trees are a popular algorithm in Data Science, and involve splitting the data in to buckets based on a yes/no condition of different variables. The aim of this splitting is to group the data as precisely as possible according to the target variable ('Survived'). In a perfect tree, this would result in all of the buckets only containing passengers of one type (either Survived=0, or Survived=1). \n", "\n", "However, Decision Trees can lead to overfitting of data, and the Random Forest algorithm aims to solve this by creating a large number of trees (or a 'forest'!), and then using them to 'vote' on the target variable for each observation. \n", "\n", "I will also be using a grid search to help choose which parameters to use in the Random Forest classifier. A grid search consists of specifying a 'grid' of different parameter combinations to try, fitting each of them separately, noting how well they perform, and choosing the best performing combination for the final model. It is also essential to use cross-validation whilst doing a grid search, to avoid overfitting. Lastly, I will calculate the importance of each feature in predicting the target variable, giving me an indication of how much each feature is contributing to the model. "], "cell_type": "markdown"}, {"metadata": {"scrolled": true, "_uuid": "a9882985075af19f05acd341308c6154d2f5683c", "_cell_guid": "2242a88b-3e51-4673-a21d-8d0794d9a1e7", "collapsed": true}, "execution_count": null, "outputs": [], "source": ["from sklearn.ensemble import RandomForestClassifier\n", "from sklearn.metrics import make_scorer, accuracy_score\n", "from sklearn.model_selection import GridSearchCV\n", "\n", "# Create random forest classifier\n", "forest = RandomForestClassifier(random_state=11)\n", "\n", "# Choose some parameter combinations to try (these values were borrowed from another user)\n", "parameters = {'n_estimators': [4, 6, 9, 100], \n", "              'max_features': ['log2', 'sqrt','auto'], \n", "              'criterion': ['entropy', 'gini'],\n", "              'max_depth': [2, 3, 5, 10], \n", "              'min_samples_split': [2, 3, 5],\n", "              'min_samples_leaf': [1,5,8]\n", "             }\n", "\n", "# Type of scoring used to compare parameter combinations\n", "accuracy = make_scorer(accuracy_score)\n", "\n", "# Run the grid search with 10-fold cross-validation\n", "grid = GridSearchCV(forest, parameters,scoring=accuracy,cv=10)\n", "grid = grid.fit(X_train, y_train)\n", "print(\"Tuned Random Forest Parameters: {}\".format(grid.best_params_),'\\n')\n", "print(\"Best score is {}\".format(grid.best_score_),'\\n')\n", "\n", "# Set the classifier to the best combination of parameters\n", "forest = grid.best_estimator_\n", "\n", "# Fit the best algorithm to the data, and print feature importances & prediction score\n", "forest.fit(X_train, y_train)\n", "print('Feature Importances','\\n','Pclass, Sex, Age, Embarked, Child, Family_Size')\n", "print(forest.feature_importances_)\n", "\n", "predictions = forest.predict(X_test)\n", "print(accuracy_score(y_test, predictions))\n", "\n"], "cell_type": "code"}, {"metadata": {"_uuid": "30adf1afe179d82600c3bebb37e4947923bc3d36", "_cell_guid": "4489f4fe-18d0-4e6d-a9ec-aa26659fbba3"}, "source": [], "cell_type": "markdown"}, {"metadata": {"_uuid": "7119f97b25f3c052853692261a032893cf555754", "_cell_guid": "f8b960c7-7838-4fa9-a7d2-cf321db538fe"}, "source": ["**Predicting on the Kaggle Test Data**\n", "\n", "Before predicting on the Kaggle 'test' data set, I will do some data cleaning and preparation (as was done before to the 'train' data set). In total, I am going to try three different algorithms for making predictions, and will discuss the accuracy of each at the end of the notebook. "], "cell_type": "markdown"}, {"metadata": {"_uuid": "1a49653264798ca5aee36395b5b502d4cdbbe940", "_cell_guid": "47e39395-1d7e-484e-ac80-f93f573be111", "collapsed": true}, "execution_count": null, "outputs": [], "source": ["#impute missing values in test\n", "test[\"Age\"] = test[\"Age\"].fillna(test[\"Age\"].median())\n", "#convert to integer values in test\n", "test.loc[test[\"Sex\"] == 'male', 'Sex'] = 0\n", "test.loc[test[\"Sex\"] == 'female', 'Sex'] = 1\n", "test.loc[test[\"Embarked\"] == 'S', 'Embarked'] = 0\n", "test.loc[test[\"Embarked\"] == 'C', 'Embarked'] = 1\n", "test.loc[test[\"Embarked\"] == 'Q', 'Embarked'] = 2\n", "#Create Family Size Column for test\n", "test[\"Family_Size\"] = test[\"SibSp\"] + test[\"Parch\"] + 1\n", "#Create Child column in test set \n", "test[\"Child\"] = float(\"NaN\")\n", "test.loc[train[\"Age\"] < 13, \"Child\"] = 1\n", "test.loc[train[\"Age\"] >= 13, \"Child\"] = 0\n", "\n", "display(test.head())"], "cell_type": "code"}, {"metadata": {"_uuid": "573484f7098bf178d03ff4d9d1613e823b666739", "_cell_guid": "460b5ef5-7e47-46f2-b8f3-490211b27cdb", "collapsed": true}, "execution_count": null, "outputs": [], "source": ["#fit tree to test data\n", "test_features = test.drop(['PassengerId','Name','SibSp','Parch','Ticket','Fare','Cabin'], axis=1)\n", "prediction = forest.predict(test_features)\n", "\n", "#create submission file\n", "forestsubmission = pd.DataFrame({\n", "        \"PassengerId\": test[\"PassengerId\"],\n", "        \"Survived\": prediction\n", "   })\n", "\n", "forestsubmission.to_csv(\"forestsubmission.csv\",index=False)\n", "\n", "display(forestsubmission.head())"], "cell_type": "code"}, {"metadata": {"_uuid": "334ab20f3c5928fc458bb15fb476cb309bdf837b", "_cell_guid": "4352cf06-ae40-440c-8d35-eab05e4c1c8b", "collapsed": true}, "source": ["**Second Prediction: Creating and Fitting Logistic Regression Model**\n", "\n", "Logistic Regression is another common method used, and is worth exploring here. It measures the relationship between each of the features and the categorical target variable (which must have only two possible outcomes), using a logistic function. \n", "\n", "I have also displayed below the Confusion Matrix for this model, which shows the actual number of correct/incorrect predictions for each value of the target variable. The accuracy of the model can calculated from the matrix, however, accuracy is not always the most suitable measure of a model's success. Therefore, I have also display the Classification Report, which shows the Precision and Recall values, which can be very useful statistics. In this case, a high Precision indicates a low rate of incorrect survival predictions. A high Recall, indicates that a large number of survivals were predicted correctly. "], "cell_type": "markdown"}, {"metadata": {"_uuid": "12b59fb3471e40bdfacbc856aa283fe7168df0d9", "_cell_guid": "3434a65a-731d-4fb5-b283-93d5fda78343", "collapsed": true}, "execution_count": null, "outputs": [], "source": ["from sklearn.linear_model import LogisticRegression\n", "from sklearn.metrics import confusion_matrix, classification_report\n", "\n", "logreg = LogisticRegression()\n", "logreg.fit(X_train, y_train)\n", "predictions2 = logreg.predict(X_test)\n", "print('Training Set Score: ',logreg.score(X_train, y_train))\n", "\n", "# Print confusion matrix, showing actual numbers of correct and incorrect predictions\n", "print('\\n','Confusion Matrix:','\\n',confusion_matrix(y_test, predictions2))\n", "\n", "# Accuracy on test set (diagonal divided by total in confusion matrix)\n", "print('\\n','Test Set Score: ',logreg.score(X_test, y_test))\n", "\n", "# Print classification report, showing precision and recall calculated from confusion matrix\n", "# high precision = a low rate of incorrect survival predictions\n", "# high recall = predicted a large number of survivals correctly\n", "print('\\n','Classification Report: ','\\n',classification_report(y_test, predictions2))\n"], "cell_type": "code"}, {"metadata": {"_uuid": "47cbf6e28fbb4dd5ad2f7172811ade113682415b", "_cell_guid": "7617ed4a-6a7f-4cd4-83a1-e8ef4e3d83d0", "collapsed": true}, "execution_count": null, "outputs": [], "source": ["# Fit logreg to Kaggle test data\n", "prediction2 = logreg.predict(test_features)\n", "\n", "# Create submission file\n", "logregsubmission = pd.DataFrame({\n", "        \"PassengerId\": test[\"PassengerId\"],\n", "        \"Survived\": prediction2\n", "   })\n", "\n", "logregsubmission.to_csv(\"logregsubmission.csv\",index=False)\n", "\n", "display(logregsubmission.head())"], "cell_type": "code"}, {"metadata": {"_uuid": "19c41007c196ada7a841b148dceb6828efeb2c86", "_cell_guid": "51dd5989-779b-4116-864e-6f7c3053fb77"}, "source": ["**Third Prediction: Creating and Fitting k-Nearest Neighbors Algorithm**\n", "\n", "The last algorithm that I am going to use is k-Nearest Neighbors (or k-NN). This method involves looking at the *k* nearest neighbours to an observation, and using them to 'vote' on which value of the target variable should be assigned. Therefore, for a particular passenger, whatever value of Survived is most common amongst it's neighbors, is what will be used to classify whether that passenger survived or not. "], "cell_type": "markdown"}, {"metadata": {"_uuid": "a5ac149ce8479b780b93c9332adbbd05616faae2", "_cell_guid": "d0355631-e2bb-49bf-9c43-8ef0a660390a", "collapsed": true}, "execution_count": null, "outputs": [], "source": ["from sklearn.neighbors import KNeighborsClassifier\n", "\n", "knn = KNeighborsClassifier(n_neighbors = 3)\n", "knn.fit(X_train, y_train)\n", "predictions3 = knn.predict(X_test)\n", "print('Training Set Score: ',knn.score(X_train, y_train))\n", "print('Test Set Score: ',knn.score(X_test, y_test))\n", "\n"], "cell_type": "code"}, {"metadata": {"_uuid": "f3d7cbdb14296664a9f67e9bd0b2e294d23fb8e8", "_cell_guid": "8e16b47a-53d6-453d-a62a-a7d2d7c76c13", "collapsed": true}, "execution_count": null, "outputs": [], "source": ["# Fit knn to kaggle test data\n", "prediction3 = knn.predict(test_features)\n", "\n", "# Create submission file\n", "knnsubmission = pd.DataFrame({\n", "        \"PassengerId\": test[\"PassengerId\"],\n", "        \"Survived\": prediction3\n", "   })\n", "\n", "knnsubmission.to_csv(\"knnsubmission.csv\",index=False)\n", "\n", "display(knnsubmission.head())"], "cell_type": "code"}, {"metadata": {}, "source": ["**5. Conclusion**\n", "\n", "Each of the models got the following scores in the Kaggle competition:\n", "\n", "Random Forest:  0.75598\n", "\n", "Logistic Regression: 0.52631\n", "\n", "k-Nearest Neighbors: 0.59808\n", "\n", "As expected, the Random Forest model performed best out of the three, with a score of 76%. This is a reasonable value, however, there are clearly a lot of improvements that could be made. For example, the Cabin and Fare features could be taken in to account, and some more fine tuning of the algorithms could help performance. \n"], "cell_type": "markdown"}], "nbformat": 4}