{"cells":[{"metadata":{"_uuid":"7855b4003f0d20fe6a0ccfc0c10669ff5c82e9ba"},"cell_type":"markdown","source":"# Environment Setup"},{"metadata":{"_uuid":"0ba3c004176c04a5c85059f6e87aac5c1e4d8e18","trusted":true},"cell_type":"code","source":"# Check who is the user running Jupyter.\nwho_am_i = !whoami\n\n# Define our data base path.\nbase_path_data = '../input' if who_am_i[0] == 'root' else '../../data'","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(base_path_data))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2a0155e97de3253cef0514d706ec87e9db888a48"},"cell_type":"markdown","source":"# Data Set Loading"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df = pd.read_csv(base_path_data + '/train.csv', sep=\",\", header=0, encoding='utf-8')\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1560dfae8f249f082e24e5ced582d6f50b55554c"},"cell_type":"markdown","source":"# Statistics"},{"metadata":{"trusted":true,"_uuid":"ed4b302c9f3a3d0ed405d8f7366ab5f4da8be35f"},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"015fb0adbe84eabf4d2fbf85b77ce4298bf2a6d1"},"cell_type":"code","source":"print('\\nStats')\nprint(df.describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"08a4f013c336b4e117bf2f4b061befd879ec52e6"},"cell_type":"code","source":"# Check which columns have missing data.\nprint('\\nMissing values')\nprint(df.isnull().any())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"641a579a033fd30e533294ad9b508a3928d8f859"},"cell_type":"code","source":"print('Column types')\nprint(df.dtypes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"26c546095f8f0fa44896ed2df6d347485756662c"},"cell_type":"code","source":"# Copied from article: https://www.analyticsvidhya.com/blog/2016/01/12-pandas-techniques-python-data-manipulation\ndef num_missing(x):\n    return sum(x.isnull())\n\ndef print_missing_values(df, axis):\n    print(\"Missing values per %s:\" % ('column' if axis == 0 else 'row'))\n    print(df.apply(num_missing, axis=axis)[:df.shape[1]]) # axis=0 to apply on each column\n\n# Applying per column\nprint_missing_values(df, 0)\n\n# Applying per row:\nprint_missing_values(df, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"63913ecd9d69e88ee6b8835156087e09afa2a1ae"},"cell_type":"code","source":"{'Survived':df.query('Survived == 1').count()[0], 'Did not':df.query('Survived == 0').count()[0]}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0cba874bed3ec74469cd48e9830eda25c42dc5db"},"cell_type":"code","source":"# Check people age under 1.\ndf.query('Age < 1')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"92ee469342046cc551f5ad861cc449e85f890590"},"cell_type":"markdown","source":"# Visualization of Data"},{"metadata":{"trusted":true,"_uuid":"c224686425df231e6747672d99c08cc9c54102a1"},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nsns.set(style=\"darkgrid\")\n\n# Remove outliers.\n# df = df[df.Fare < 300]\n\n# http://seaborn.pydata.org/tutorial/relational.html\nsns.relplot(x=\"Age\", y=\"Fare\", hue=\"Sex\", data=df) # size=\"Fare\", sizes=(0, 100), \nsns.relplot(x=\"Age\", y=\"Fare\", hue=\"Sex\", col=\"Embarked\", palette=\"ch:r=-.5,l=.75\", data=df) # size=\"Fare\", sizes=(0, 100), \n# plt.title('Fare per age')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f8a1ded393584239d39a1befcbe605b1ecd8fa96"},"cell_type":"code","source":"import seaborn as sns\nsns.set(style=\"darkgrid\")\n\n# Check the correlation between features.\ncorr = df.corr()\nsns.heatmap(corr, cmap=sns.diverging_palette(20, 50, as_cmap=True), square=True, annot=True, linecolor='k', linewidths=.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"28d653c2e16a0b2b95302b45649120414b795cc6"},"cell_type":"code","source":"# Group by age and count to see the distribution.\nage_2_count = df.groupby('Age')['PassengerId'].count().reset_index(name=\"Count\")\n\n# Transpose for the sake of visibility.\nage_2_count.loc[:15,].transpose()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"efbec5728a4edcef2ceae7b4879e9708633826ca"},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\n# Set the plot.\nsns.scatterplot(x=\"Age\", y=\"Count\", hue=\"Count\", palette=\"ch:r=-.5,l=.75\", data=age_2_count) # size=\"Fare\", sizes=(0, 100), \n\nplt.title('Number of persons per age')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"909e76e1b4b8b0c88a82afe3f162b486f821b5b8"},"cell_type":"markdown","source":"# Data Set Preparation (Pre-processing)"},{"metadata":{"trusted":true,"_uuid":"a30671c5dbf4da45fbf77ad2d6532d4f55dcc4ea"},"cell_type":"code","source":"df_tmp = df.copy(deep=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ce2cb9c10d90ab5b7b97c1aac6249246f8189719"},"cell_type":"markdown","source":"## Feature Engineering"},{"metadata":{"trusted":true,"_uuid":"293ad82d579bf762c14c12c8568067f8f9bd8feb"},"cell_type":"code","source":"import re\n\ndef extract_title(name):\n#     print(name)\n    m = re.search(\".*,((\\s*\\w)+\\.).*\", name)\n    if not m:\n        print(name)\n    return m.groups(0)[0].strip()\n\n# print(extract_title('Pain, Dr. Alfred').strip())\n\n# Find passenger with Dr title\n# print(df[df['Name'].str.contains('Dr')])\n\n# Title needs to be engineered using values from the test set or use it differently.\n# For now, leave it out.\n# df_tmp['Title'] = df_tmp['Name'].apply(extract_title)\n\ndf_tmp.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"02217fd9bd7c610630ea61af06f141180d48e659"},"cell_type":"code","source":"sns.boxplot(x=\"Sex\", y=\"Age\", hue='Pclass', palette=[\"m\", \"g\"], data=df_tmp)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c5eb8d41acfcd3d3181e98cd71c6087b35a2b468"},"cell_type":"code","source":"sns.boxplot(x=\"Survived\", y=\"Age\", palette=[\"m\", \"g\"], data=df_tmp)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"51bf434710a3fb841a0f9a683013900dd9253786"},"cell_type":"markdown","source":"## Imputation of Missing Values"},{"metadata":{"trusted":true,"_uuid":"a8a31feade4f895b2c5637bbe2997bcb22797204"},"cell_type":"code","source":"print_missing_values(df_tmp, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8376fa2bc539d805dd5c25ca80dfb941afb98843"},"cell_type":"code","source":"# One method to show entries with NA values for Embarked column.\n# nans = lambda df: df[df.isnull().any(axis=1)]\n# nans(df_raw.loc[:, ['Embarked']])\n\n# Another method to show entries, not working\n# print(df_raw[df_raw['Embarked'].apply(np.isnan)])\n\n# Preferred method to show entries where Embarked is null, NaN, etc.\nprint(df_tmp.query('Age != Age or Age == Age').loc[:, ['Name', 'Parch', 'SibSp', 'Age']].iloc[:15,])\nprint('\\n' + str(df_tmp.isnull().any()))\n\n# Show the most frequent value in the features.\ndf_tmp['Age'].mode().values[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a453f100cf0f9ce3390a0c8cb457dfc8d6340d23"},"cell_type":"code","source":"from sklearn.impute import SimpleImputer\nclass DataPreparator:\n    def impute(self, df, df_for_fitting=None, strategy='mean', column=None):\n        \"\"\"\n        Impute the data frame using a strategy on a column.\n\n        Parameters\n        ----------\n        df: DataFrame: the data frame to transform with the fitted value.\n        df_for_fit: None or the data frame to fit the Imputer. If None then df is used for fitting.\n        strategy: a string for the strategy name defined from sklearn.preprocessing.Imputer.\n        column: a string for the name of the column to apply the imputation.\n\n        Returns\n        -------\n        DataFrame: the 'df' with a new column called 'column'_imputed instead of the 'column'.\n\n        \"\"\"\n        # Define the imputer working on columns. \n        imp = SimpleImputer(missing_values=np.nan, strategy=strategy)\n\n        # Get the dataframe used to fit the imputer.\n        df_fit = df_for_fitting if df_for_fitting is None else df\n\n        # Column extractor\n        extract_column_df = lambda df: np.array([df[column]]).transpose()\n\n        # Fit the imputer with a DF of shape (*, 1).\n        df_ = extract_column_df(df_fit)\n        model = imp.fit(df_)\n\n        # Transform the df of shape (*, 1).\n        df_ = extract_column_df(df)\n        res = pd.DataFrame(model.transform(df_))\n        res.columns = [column]\n\n        # Make a copy to add the new column and remove the old one.\n        df_tmp = df.drop(column, axis=1)\n        new_column = column + '_imputed'\n        df_tmp[new_column] = np.array(res[column])\n\n        return df_tmp\n\ndata_prep = DataPreparator()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f9e7d778dff434cd0ae4ac56be841bc11d08fbbd"},"cell_type":"code","source":"# Proceed to impute the Age column.\n# X_aug = X_train2 # pd.concat([X, X_train.loc[:, 'Age']], axis=1)\n# print(X_aug['Age'])\n\n# from sklearn.impute import SimpleImputer\n# imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n# cc = 'Age'\n# aaa = np.array([X_train2[cc]]).transpose()\n# print(aaa.shape)\n# model_imp2 = imp.fit(aaa)\n# model_imp2.transform(np.array([X_test[cc]]).transpose())\n\n# print(X_train.query('Age != Age').loc[:, ['PassengerId','Age']].iloc[:10, :])\ndf_tmp = data_prep.impute(df_tmp, df_tmp, column='Age')\ndf_tmp = data_prep.impute(df_tmp, df_tmp, column='Cabin', strategy='most_frequent')\ndf_tmp = data_prep.impute(df_tmp, df_tmp, column='Embarked', strategy='most_frequent')\n\nprint(df.shape, df_tmp.shape)\n\nprint_missing_values(df_tmp, 0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"88a38be7d1b13ace34fdaf1b0a5a92a048692a15"},"cell_type":"markdown","source":"## Features Selection"},{"metadata":{"trusted":true,"_uuid":"3afd62c54359f798c62cb8ce8ea8ecb539491ede"},"cell_type":"code","source":"# First keep the columns of interest.\n\ndef select_features(df, features_to_keep=['Pclass', 'Sex', 'Fare', 'Age_imputed']):\n    return df.loc[:, features_to_keep]\n\n# Add the function as a class method to the DataPreparator.\n#     features_to_keep = list(set(df.columns) - set(['PassengerId', 'Survived', 'Name', 'SibSp', 'Ticket', 'Cabin_imputed']))\n\nDataPreparator.select_features = lambda self, df, features_to_keep=['Pclass', 'Sex', 'Fare', 'Age_imputed']: select_features(df, features_to_keep)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc803c57223a1df6c41b5b6e289ac70693c21c07"},"cell_type":"code","source":"df_tmp = data_prep.select_features(df_tmp)\n\n# Check the data.\nprint_missing_values(df_tmp, 0)\nprint(df_tmp.dtypes)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f26ab418e2472a468a303fd516775b7b9a280546"},"cell_type":"markdown","source":"## One-hot Encoding of Categorical Features"},{"metadata":{"trusted":true,"_uuid":"e71b612117bcd7098bc9879bf787b25b6212e0e3"},"cell_type":"code","source":"def get_one_hot(df, features=None):\n    \"\"\"\n    Does the one-hot vectorization of features.\n    \"\"\"\n    X = df.loc[:, features]\n\n    # Use one-hot encoding for categorical data.\n    X_dummies = pd.get_dummies(X, columns=features, dtype=np.uint8)\n    X = df.drop(features, axis=1)\n    return pd.concat([X, X_dummies], axis=1)\n\n# Add the one-hot method to the DataPreparator class.\nDataPreparator.get_one_hot = lambda self, df, features=[]: get_one_hot(df, features)    \n    \n# Prepare the data sets.\nX_final = data_prep.get_one_hot(df_tmp, features=['Sex', 'Pclass'])\nY_final = df.loc[:, 'Survived'] # Get the label from the original data frame df.\n\n# Check data.\nprint(X_final.shape, Y_final.shape)\nX_final.iloc[:5,], df_tmp.iloc[:5,]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d5b8708012a55e267d5984f0076c5ec5d1de665f"},"cell_type":"code","source":"# Check the data.\nprint_missing_values(X_final, 0)\nprint(X_final.dtypes)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"942391f5cbde0d0ea450a3fc78020467c8f329c5"},"cell_type":"markdown","source":"## Scaling the Dataframe"},{"metadata":{"trusted":true,"_uuid":"86a2ee3c2c1a771fd3e12e9618e13550bb594b1c"},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nss = StandardScaler()\nX_scaled = pd.DataFrame(ss.fit_transform(X_final))\nX_scaled.columns = X_final.columns\nX_scaled.iloc[:2,]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"211fd29810b4b2e18a889ca3ec414bcb88307e2e"},"cell_type":"code","source":"type(X_final)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d3341c5fe7b05bbe63851c1e6c69e52995235921"},"cell_type":"markdown","source":"# Split Dataset into Training and Validation datasets"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Divide into training and cross-validation datasets.\nX_train, X_validation, Y_train, Y_validation = train_test_split(X_scaled, Y_final, test_size=0.2, random_state=42)\n# X_train, X_validation, Y_train, Y_validation = X_final, X_final, Y_final, Y_final\n\n# Check the data.\nprint(X_train.shape, Y_train.shape)\nprint(X_validation.shape, Y_validation.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"12ae5734a411c14495d45a6cf8e0958cfec0d739"},"cell_type":"markdown","source":"# 5-fold Cross Validation on 4 Different Classifiers"},{"metadata":{"trusted":true,"_uuid":"2328e1208b65b98f6089e541f588a49db33ce6bd"},"cell_type":"code","source":"from sklearn.base import clone\n\nclass HelperCrossValidation:\n    \"\"\"\n    Helper class for running cross validations.\n    \"\"\"\n    def print_features_importance(self, df, clf):\n        if not hasattr(clf, 'feature_importances_'):\n            return\n        \n        print(type(df))\n        print(type(clf))\n        \n        # Display the features by descending importance.\n        df_disp = pd.DataFrame(list(zip(list(df.columns), clf.feature_importances_)))\n        df_disp.columns = ['Feature', 'Percentage']\n        print(df_disp.sort_values(by='Percentage', ascending=False)[:10].to_string(index=False))\n\n    def cross_val_score_do(self, name, clf, X, Y, cv):\n        # Cross validate the classifier.\n        scores = cross_val_score(clf, X, Y, cv=cv)\n        print(\"\\n%s Score: %.10f\" % (name, scores.mean()))\n\n        # Fit the classifier and show the feature importance.\n        clf_cloned = clone(clf)\n        model = clf_cloned.fit(X, Y)\n\n        # Display the features by descending importance.\n        self.print_features_importance(X, clf_cloned)\n\n        return model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"db3e7f1d183ec2dd3c9088ddd26bb3913037de5c"},"cell_type":"markdown","source":"# Setting Training Dataset and Classifiers"},{"metadata":{"trusted":true,"_uuid":"5e02f20dec58f50f3c082f72e6fe610781a0ee21"},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n# Setup.\nX = X_train\nY = Y_train\n\n# Instanciate a HelperCrossValidation\nhelper_cv = HelperCrossValidation()\n\n# Model.\nclf_lr = LogisticRegression(penalty='l2', C=0.1)\nmodel_lr = helper_cv.cross_val_score_do('RandomForestClassifier', clf_lr, X, Y, 5)\n\nclf_rfc = RandomForestClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=101)\nmodel_rfc = helper_cv.cross_val_score_do('RandomForestClassifier', clf_rfc, X, Y, 5)\n\nclf_etc = ExtraTreesClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=290)\nmodel_etc = helper_cv.cross_val_score_do('ExtraTreesClassifier', clf_etc, X, Y, 5)\n\nclf_dtc = DecisionTreeClassifier(max_depth=None, min_samples_split=2, random_state=13)\nmodel_dtc = helper_cv.cross_val_score_do('DecisionTreeClassifier', clf_dtc, X, Y, 5)\n\nclf_gbc = GradientBoostingClassifier(n_estimators=70, learning_rate=0.5, max_depth=1, random_state=43)\nmodel_gbc = helper_cv.cross_val_score_do('GradientBoostingClassifier', clf_gbc, X, Y, 5)\n\n# SGDClassifier(max_iter=5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7dc9680025f59a8cc508386fafc11f77a7c1ed5f"},"cell_type":"markdown","source":"# ROC Visualization"},{"metadata":{"trusted":true,"_uuid":"3a83e7238cfacbeb37e9b050bf3d30d07383dfbe"},"cell_type":"code","source":"from sklearn import metrics\n\ndef plot_roc(models={}, X=None, Y=None):\n    for name in models:\n        model = models[name]\n        \n        # Evaluate.\n        y_pred_proba = model.predict_proba(X)[::, 1] # Dont know why take index=1 or 0?!?!\n        print(\"%s Accuracy :%.15f\" % (name, metrics.accuracy_score(Y, model.predict(X))))\n\n        # Compute the probabilities.\n        fpr, tpr, _ = metrics.roc_curve(Y, y_pred_proba, pos_label=1)\n        roc_auc = metrics.auc(fpr, tpr)\n\n        # Plot the ROC.\n        fpr_tpr = pd.DataFrame(list(zip(fpr.ravel(), tpr.ravel())))\n        fpr_tpr.columns = [ 'fpr', 'tpr']\n        sns.lineplot(x=\"fpr\", y=\"tpr\", data=fpr_tpr, label='%s ROC fold (AUC = %0.2f)' % (name, roc_auc))\n\n    sns.lineplot([0, 1], [0, 1], linestyle='--', lw=2, label='Chance', alpha=.8)\n    plt.legend(loc=4)\n    plt.title('Receiver Operating Characteristic')\n    plt.show()\n    \nplot_roc({'lr' : model_lr, 'dtc' : model_dtc, 'etc' : model_etc, 'gbc' : model_gbc, 'rfc' : model_rfc}, X, Y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1b5a7bc8fd416fb0a45d1e395c26714d15456e59"},"cell_type":"markdown","source":"# 5-fold Cross Validation on Best Classifier for Hyperparameters Tuning"},{"metadata":{"trusted":true,"_uuid":"593362ae04510633cf2543e8dfd544cc94889cce"},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\ndef do_cross_validation(clf, X, Y, k):\n    \"\"\"\n    Run the k-fold GridSearchCV on X and Y\n    \n    Parameters\n    ----------\n    clf: the original classifier to clone for testing the hyperparameters.\n    X: the data set to use for splitting into training and cross validation datasets.\n    Y: the labels.\n    k; the number of cross validation datasets split from X.\n    \n    Returns\n    -------\n    clf: the best classifier after evaluating all the possible hyperparameter settings.\n    \n    \"\"\"\n    parameters = {'max_features': [1, len(X.columns)], 'n_estimators': [5, 50, 250, 400, 1000]}\n\n    clf_cv = GridSearchCV(clf, parameters, cv=k)\n\n    %timeit\n    clf_model = clf_cv.fit(X, Y)\n\n    # Display the scores.\n#     for row in clf_model.cv_results_:\n#         print(row)\n\n    return clf_model.best_estimator_\n\nclf = do_cross_validation(clf_etc, X, Y, 5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"584704c5c9f3c324deb0dfe42bd458ee98cb9f8a"},"cell_type":"markdown","source":"# Evaluation of the Best Classifier"},{"metadata":{"trusted":true,"_uuid":"25ea68e333b48ee6c33bfa6f503d258b033d664b"},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\n# Print features importance.\nhelper_cv.print_features_importance(X, clf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"81f8d62ce7e6a1edc74e78f485662379b144ed2b"},"cell_type":"code","source":"import itertools\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()\n\ndef display_metrics(clf, X, Y):\n    # Predict the values on X.    \n    X_prediction = clf.predict(X)\n\n    print(\"Score: %.15f\" % clf.score(X, Y))\n\n    # Compute confusion matrix\n    cnf_matrix = confusion_matrix(Y, X_prediction)\n    # tn, fp, fn, tp = cnf_matrix.ravel()\n\n    # Compute confusion matrix\n    np.set_printoptions(precision=2)\n\n    class_names = ['Did Not', 'Survived']\n\n    # Plot non-normalized confusion matrix\n    plt.figure()\n    plot_confusion_matrix(cnf_matrix, classes=class_names,\n                          title='Confusion matrix, without normalization')\n\n    # Plot normalized confusion matrix\n    plt.figure()\n    plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n                          title='Normalized confusion matrix')\n\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"89315139e8d62e5ec915655837bc76ed7abe336e"},"cell_type":"code","source":"display_metrics(clf, X, Y)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a15a5d4721c4826fc260207496fc258e5f932c6d"},"cell_type":"markdown","source":"# Test on X_test Dataset"},{"metadata":{"trusted":true,"_uuid":"fd7900b7f62b035bed06d3183220c73f8241984d"},"cell_type":"code","source":"# Create the test sets.\nXt = X_validation\nYt = Y_validation\n\nprint(Xt.shape, Yt.shape)\nprint(Xt.columns)\n\nprint_missing_values(Xt, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eab319f27bc9c6136b836c6e83b5bb01224e2377"},"cell_type":"code","source":"# Score the Classifier that performed best on the test set.\nprint(\"Score: %.15f\" % clf.score(Xt, Yt))\n\n# Print features importance.\nhelper_cv.print_features_importance(Xt, clf)\n\n# Display metrics.\ndisplay_metrics(clf, Xt, Yt)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9944b0530ef7dfdcc547553c46ff095185b5f3dd"},"cell_type":"markdown","source":"# CSV of Test Data Set Prediction"},{"metadata":{"trusted":true,"_uuid":"aea1a127618bc8b262476736088b2f2ad40201e5"},"cell_type":"code","source":"# Load the test set.\ndf_test_raw = pd.read_csv(base_path_data + '/test.csv', sep=\",\", header=0, encoding='utf-8')\n\nprint(df_test_raw.isna().any())\nprint(df_test_raw.dtypes)\n\n# Show the result.\nprint_missing_values(df_test_raw, 0)\n\n# print(df_test_raw.iloc[:10,])\ndf_test_raw.query('Fare != Fare')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2516586416a12bee2216b836fdf2e8685f6739e3"},"cell_type":"code","source":"from sklearn.preprocessing import Imputer\n\ndf_tmp = df_test_raw\n# Leave this out for the time being.\n# df_tmp['Title'] = df_test_raw['Name'].apply(extract_title)\n\ndf_tmp.head()\n\ndf_tmp = data_prep.impute(df_tmp, df_tmp, column='Age')\ndf_tmp = data_prep.impute(df_tmp, df_tmp, column='Cabin', strategy='most_frequent')\ndf_tmp = data_prep.impute(df_tmp, df_tmp, column='Fare', strategy='mean')\ndf_tmp['Fare'] = df_tmp['Fare_imputed']\n\n# Check the columns of the trained model based on features of X.\nprint(X.columns)\nprint(df_tmp.columns)\n\ndf_tmp = data_prep.select_features(df_tmp, features_to_keep=['Fare', 'Age_imputed', 'Sex', 'Pclass'])\n\n# TODO: jh 2018/11/02\n# Should check that the columns of the test dataset are the same as the X dataset used for training.\n\nprint(df_tmp.columns)\n\nXt_raw = data_prep.get_one_hot(df_tmp, features=['Sex', 'Pclass']) # 'Cabin_imputed', \n\n# Add scaling.\nXt_scaled = pd.DataFrame(ss.transform(Xt_raw))\nXt_scaled.columns = Xt_raw.columns\n\nXt_raw = Xt_scaled\nprint(df_test_raw.shape, Xt_raw.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"27636511d1125c6373adab38eff9ed8e6d40a600"},"cell_type":"code","source":"# Show the result.\nprint_missing_values(Xt, 0)\nprint_missing_values(Xt_raw, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"34d37b5c8ecc155d6076c9d72a106284086485e6"},"cell_type":"code","source":"X_test_given_predicted = clf.predict(Xt_raw)\n\n# print(X_test.iloc[:10,])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"852e5625d2c40c735631bf383f02bb19653ba76e"},"cell_type":"code","source":"import datetime\n\ndf_to_submit = pd.DataFrame(list(zip(df_test_raw.loc[:,'PassengerId'], X_test_given_predicted)))\ndf_to_submit.columns = ['PassengerId', 'Survived']\ndf_to_submit.PassengerId = df_to_submit.PassengerId.astype(np.int32)\ndf_to_submit.Survived = df_to_submit.Survived.astype(np.int32)\n\ntoday = datetime.datetime.today()\ntoday_s = today.strftime('%Y%m%d-%H%M')\n\ncsv_dest = \"%s/csv_submission_%s.csv\" % (base_path_data, today_s)\n\n# Save the CSV\nif who_am_i[0] != 'root':\n    df_to_submit.to_csv(csv_dest, index=False)\n    print(\"test prediction save in:\\n %s\" % csv_dest)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}