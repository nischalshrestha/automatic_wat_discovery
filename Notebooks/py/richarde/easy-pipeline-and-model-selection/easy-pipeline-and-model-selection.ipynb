{"nbformat_minor": 1, "nbformat": 4, "cells": [{"cell_type": "markdown", "source": ["This notebook is my submission for Titanic Kaggle competition.  It references two good texts on machine learning:\n", "- Hands-On Machine Learning and Deep Learning with Scikit-Learn and TensorFlow by Aurelien Geron\n", "- Machine Learning Mastery by Jason Brownlee\n", "\n", "I also found the following websites useful:\n", "- http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html\n", "- http://michelleful.github.io/code-blog/2015/06/20/pipelines/\n", "\n", "It does not focus on EDA or on achieving a great score.   \n", "\n", "The main aim of this notebook to utilise methods that make it easier to change the features selected and to tune the hyperparameters in a efficient way.\n", "\n", "Alot of the code has been taken from the sources given above.  I found them very useful as one of the most frustrating aspects of the competitions is having to fiddle with the features normalising, scaling etc.  After a while it becomes very messy if not organised in a efficient manner.\n", "\n", "I hope that other users can also gain benefit from the methods.\n"], "metadata": {"_uuid": "096a5ff41fe34ad047bee6751098843340a43ffe", "_cell_guid": "0e2223f7-c245-4168-89f7-521564382a21"}}, {"source": ["import numpy as np\n", "import pandas as pd\n", "from matplotlib import pyplot as plt\n", "from pandas import read_csv\n", "from pandas import set_option\n", "from pandas.tools.plotting import scatter_matrix\n", "from sklearn.preprocessing import StandardScaler\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.model_selection import KFold\n", "from sklearn.model_selection import cross_val_score\n", "from sklearn.model_selection import GridSearchCV\n", "from sklearn.metrics import classification_report\n", "from sklearn.metrics import confusion_matrix\n", "from sklearn.metrics import accuracy_score\n", "from sklearn.pipeline import Pipeline\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.tree import DecisionTreeClassifier\n", "from sklearn.neighbors import KNeighborsClassifier\n", "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n", "from sklearn.naive_bayes import GaussianNB\n", "from sklearn.svm import SVC\n", "from sklearn.ensemble import AdaBoostClassifier\n", "from sklearn.ensemble import GradientBoostingClassifier\n", "from sklearn.ensemble import RandomForestClassifier\n", "from sklearn.ensemble import ExtraTreesClassifier\n", "%matplotlib inline\n", "\n", "import warnings\n", "warnings.filterwarnings('ignore')\n", "\n", "#allows printing of all data in cell\n", "from IPython.core.interactiveshell import InteractiveShell\n", "InteractiveShell.ast_node_interactivity = \"all\""], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_uuid": "3a4c09998d98a8390c3463ffcada3aff99d7cdb9", "collapsed": true, "_cell_guid": "c6069685-8efa-4ad7-8e9a-b3acad5dba4e"}}, {"source": ["train_data = pd.read_csv('../input/train.csv')\n", "test_data = pd.read_csv('../input/test.csv')"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_uuid": "e72b7a983b37360b0c8c14c709d2c1149c3a94fc", "collapsed": true, "_cell_guid": "0b159c07-7870-4796-9962-0cd918336ef9"}}, {"cell_type": "markdown", "source": ["## 1.0 Investigate Data"], "metadata": {"_uuid": "28ebd17505ec211634eb3e7c04f3d8f6c0008df8", "_cell_guid": "b33e17d2-ffca-421b-8a5a-5e0db0b04aab"}}, {"source": ["train_data.head()"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_uuid": "d50b6cb58f10ce0eea2c665fa1e962ad31129686", "collapsed": true, "_cell_guid": "5f20137d-2331-4be2-a989-58bf24bc2ccb"}}, {"source": ["train_data.info()"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_uuid": "b4d15c16e8dbb1837ccfc6d1190a1c6879b30cf6", "collapsed": true, "_cell_guid": "4a9900d9-21e7-414b-a8c7-9ffcd7c2ff66"}}, {"cell_type": "markdown", "source": ["The attributes have the following meaning:\n", "- **Survived**: the target, 0 means the passenger did not survive, while 1 means he/she survived.\n", "- **Pclass**: passenger class.\n", "- **Name**, Sex, Age: self-explanatory\n", "- **SibSp**: how many siblings & spouses of the passenger aboard the Titanic.\n", "- **Parch**: how many children & parents of the passenger aboard the Titanic.\n", "- **Ticket**: ticket id\n", "- **Fare**: price paid (in pounds)\n", "- **Cabin**: passenger's cabin number\n", "- **Embarked**: where the passenger embarked the Titanic"], "metadata": {"_uuid": "79e40413675ad3542e9a91a12a4088b2906348fa", "_cell_guid": "29908a44-5b03-429c-ad4e-e638160ce138"}}, {"source": ["train_data.describe()\n", "train_data['Age'].mean()"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_uuid": "d9639bef6e2d9612171fa98ec3c8ea63040f6533", "collapsed": true, "_cell_guid": "8ee9f77d-e091-4f02-85be-b64e3b5f3bcc"}}, {"source": ["#check the target\n", "train_data['Survived'].value_counts()"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_uuid": "473481f357bb49924f939940d4087d5019213265", "collapsed": true, "_cell_guid": "46295b38-8a3a-4d38-99a3-2e8fbe4f28c1"}}, {"source": ["#age has 177 missing data values, calculate a mean value and insert\n", "average_age = train_data['Age'].mean()\n", "std_age = train_data['Age'].std()\n", "count_age = train_data['Age'].isnull().sum()\n", "\n", "#generate rand numbers between (mean - std) & (mean = std) of length count_age\n", "random_1 = np.random.randint(average_age - std_age, average_age + std_age,size = count_age)\n", "#replace nan values with calculated random values\n", "train_data['Age'][np.isnan(train_data['Age'])] = random_1\n", "#float not needed, convert to integer\n", "train_data['Age'] = train_data['Age'].astype(int)\n", "\n", "\n", "average_age = test_data['Age'].mean()\n", "std_age = test_data['Age'].std()\n", "count_age = test_data['Age'].isnull().sum()\n", "\n", "#generate rand numbers between (mean - std) & (mean = std) of length count_age\n", "random_1 = np.random.randint(average_age - std_age, average_age + std_age,size = count_age)\n", "#replace nan values with calculated random values\n", "test_data['Age'][np.isnan(test_data['Age'])] = random_1\n", "#float not needed, convert to integer\n", "test_data['Age'] = test_data['Age'].astype(int)"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_uuid": "90781165aea3e178b59255aaae7dd33728adfc58", "collapsed": true, "_cell_guid": "3c0900a2-0c95-4653-8a8b-fef753c303ba"}}, {"source": ["train_data['Age'].mean()"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_uuid": "647d68e244a0d3e73df29f3a4e3b60cce6848c27", "collapsed": true, "_cell_guid": "6dfbfd5f-b147-4d87-b238-d4f95dc0a798"}}, {"cell_type": "markdown", "source": ["## 2.0 Process the Data"], "metadata": {"_uuid": "5e050f0bbcae4f28136a5036195cd8baf4c1ffe1", "_cell_guid": "79a139ed-eac4-4f54-88fb-3cdb627a1986"}}, {"source": ["#Try using using a categorical variable for age\n", "train_data['AgeBucket'] = train_data['Age'] // 15 * 15\n", "train_data[[\"AgeBucket\", \"Survived\"]].groupby(['AgeBucket']).mean()\n", "\n", "test_data['AgeBucket'] = test_data['Age'] // 15 * 15"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_uuid": "bb3b701cb3eb3ff9bcdc1ed8140e33e39c749a48", "collapsed": true, "_cell_guid": "3a36a640-ab57-453e-a6f7-63b9855a7205"}}, {"source": ["#add relatives on board category\n", "train_data[\"RelativesOnboard\"] = train_data[\"SibSp\"] + train_data[\"Parch\"]\n", "train_data[[\"RelativesOnboard\", \"Survived\"]].groupby(['RelativesOnboard']).mean()\n", "train_data.dtypes\n", "\n", "test_data[\"RelativesOnboard\"] = test_data[\"SibSp\"] + test_data[\"Parch\"]"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_uuid": "93613fefa6e5ad4ea9051d61027f6a48aab87306", "collapsed": true, "_cell_guid": "0dbd9df5-613b-4df2-bd9f-003f70146a6a"}}, {"cell_type": "markdown", "source": ["A method used by Geron to allow efficient pipelining of categorical variables:"], "metadata": {"_uuid": "4b7ac8e50c2c207c7094f430d6e4ed0cd7d6a652", "_cell_guid": "264b306e-312e-4f2a-8836-efac11347cc2"}}, {"source": ["#A method taken from stakoverflow that uses encodes categorical features into numeric.  It is similar to get_dummies\n", "#but allows  \n", "\n", "from sklearn.base import BaseEstimator, TransformerMixin\n", "from sklearn.utils import check_array\n", "from sklearn.preprocessing import LabelEncoder\n", "from scipy import sparse\n", "\n", "class CategoricalEncoder(BaseEstimator, TransformerMixin):\n", "    \"\"\"Encode categorical features as a numeric array.\n", "    The input to this transformer should be a matrix of integers or strings,\n", "    denoting the values taken on by categorical (discrete) features.\n", "    The features can be encoded using a one-hot aka one-of-K scheme\n", "    (``encoding='onehot'``, the default) or converted to ordinal integers\n", "    (``encoding='ordinal'``).\n", "    This encoding is needed for feeding categorical data to many scikit-learn\n", "    estimators, notably linear models and SVMs with the standard kernels.\n", "    Read more in the :ref:`User Guide <preprocessing_categorical_features>`.\n", "    Parameters\n", "    ----------\n", "    encoding : str, 'onehot', 'onehot-dense' or 'ordinal'\n", "        The type of encoding to use (default is 'onehot'):\n", "        - 'onehot': encode the features using a one-hot aka one-of-K scheme\n", "          (or also called 'dummy' encoding). This creates a binary column for\n", "          each category and returns a sparse matrix.\n", "        - 'onehot-dense': the same as 'onehot' but returns a dense array\n", "          instead of a sparse matrix.\n", "        - 'ordinal': encode the features as ordinal integers. This results in\n", "          a single column of integers (0 to n_categories - 1) per feature.\n", "    categories : 'auto' or a list of lists/arrays of values.\n", "        Categories (unique values) per feature:\n", "        - 'auto' : Determine categories automatically from the training data.\n", "        - list : ``categories[i]`` holds the categories expected in the ith\n", "          column. The passed categories are sorted before encoding the data\n", "          (used categories can be found in the ``categories_`` attribute).\n", "    dtype : number type, default np.float64\n", "        Desired dtype of output.\n", "    handle_unknown : 'error' (default) or 'ignore'\n", "        Whether to raise an error or ignore if a unknown categorical feature is\n", "        present during transform (default is to raise). When this is parameter\n", "        is set to 'ignore' and an unknown category is encountered during\n", "        transform, the resulting one-hot encoded columns for this feature\n", "        will be all zeros.\n", "        Ignoring unknown categories is not supported for\n", "        ``encoding='ordinal'``.\n", "    Attributes\n", "    ----------\n", "    categories_ : list of arrays\n", "        The categories of each feature determined during fitting. When\n", "        categories were specified manually, this holds the sorted categories\n", "        (in order corresponding with output of `transform`).\n", "    Examples\n", "    --------\n", "    Given a dataset with three features and two samples, we let the encoder\n", "    find the maximum value per feature and transform the data to a binary\n", "    one-hot encoding.\n", "    >>> from sklearn.preprocessing import CategoricalEncoder\n", "    >>> enc = CategoricalEncoder(handle_unknown='ignore')\n", "    >>> enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])\n", "    ... # doctest: +ELLIPSIS\n", "    CategoricalEncoder(categories='auto', dtype=<... 'numpy.float64'>,\n", "              encoding='onehot', handle_unknown='ignore')\n", "    >>> enc.transform([[0, 1, 1], [1, 0, 4]]).toarray()\n", "    array([[ 1.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.],\n", "           [ 0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.]])\n", "    See also\n", "    --------\n", "    sklearn.preprocessing.OneHotEncoder : performs a one-hot encoding of\n", "      integer ordinal features. The ``OneHotEncoder assumes`` that input\n", "      features take on values in the range ``[0, max(feature)]`` instead of\n", "      using the unique values.\n", "    sklearn.feature_extraction.DictVectorizer : performs a one-hot encoding of\n", "      dictionary items (also handles string-valued features).\n", "    sklearn.feature_extraction.FeatureHasher : performs an approximate one-hot\n", "      encoding of dictionary items or strings.\n", "    \"\"\"\n", "\n", "    def __init__(self, encoding='onehot', categories='auto', dtype=np.float64,\n", "                 handle_unknown='error'):\n", "        self.encoding = encoding\n", "        self.categories = categories\n", "        self.dtype = dtype\n", "        self.handle_unknown = handle_unknown\n", "\n", "    def fit(self, X, y=None):\n", "        \"\"\"Fit the CategoricalEncoder to X.\n", "        Parameters\n", "        ----------\n", "        X : array-like, shape [n_samples, n_feature]\n", "            The data to determine the categories of each feature.\n", "        Returns\n", "        -------\n", "        self\n", "        \"\"\"\n", "\n", "        if self.encoding not in ['onehot', 'onehot-dense', 'ordinal']:\n", "            template = (\"encoding should be either 'onehot', 'onehot-dense' \"\n", "                        \"or 'ordinal', got %s\")\n", "            raise ValueError(template % self.handle_unknown)\n", "\n", "        if self.handle_unknown not in ['error', 'ignore']:\n", "            template = (\"handle_unknown should be either 'error' or \"\n", "                        \"'ignore', got %s\")\n", "            raise ValueError(template % self.handle_unknown)\n", "\n", "        if self.encoding == 'ordinal' and self.handle_unknown == 'ignore':\n", "            raise ValueError(\"handle_unknown='ignore' is not supported for\"\n", "                             \" encoding='ordinal'\")\n", "\n", "        X = check_array(X, dtype=np.object, accept_sparse='csc', copy=True)\n", "        n_samples, n_features = X.shape\n", "\n", "        self._label_encoders_ = [LabelEncoder() for _ in range(n_features)]\n", "\n", "        for i in range(n_features):\n", "            le = self._label_encoders_[i]\n", "            Xi = X[:, i]\n", "            if self.categories == 'auto':\n", "                le.fit(Xi)\n", "            else:\n", "                valid_mask = np.in1d(Xi, self.categories[i])\n", "                if not np.all(valid_mask):\n", "                    if self.handle_unknown == 'error':\n", "                        diff = np.unique(Xi[~valid_mask])\n", "                        msg = (\"Found unknown categories {0} in column {1}\"\n", "                               \" during fit\".format(diff, i))\n", "                        raise ValueError(msg)\n", "                le.classes_ = np.array(np.sort(self.categories[i]))\n", "\n", "        self.categories_ = [le.classes_ for le in self._label_encoders_]\n", "\n", "        return self\n", "\n", "    def transform(self, X):\n", "        \"\"\"Transform X using one-hot encoding.\n", "        Parameters\n", "        ----------\n", "        X : array-like, shape [n_samples, n_features]\n", "            The data to encode.\n", "        Returns\n", "        -------\n", "        X_out : sparse matrix or a 2-d array\n", "            Transformed input.\n", "        \"\"\"\n", "        X = check_array(X, accept_sparse='csc', dtype=np.object, copy=True)\n", "        n_samples, n_features = X.shape\n", "        X_int = np.zeros_like(X, dtype=np.int)\n", "        X_mask = np.ones_like(X, dtype=np.bool)\n", "\n", "        for i in range(n_features):\n", "            valid_mask = np.in1d(X[:, i], self.categories_[i])\n", "\n", "            if not np.all(valid_mask):\n", "                if self.handle_unknown == 'error':\n", "                    diff = np.unique(X[~valid_mask, i])\n", "                    msg = (\"Found unknown categories {0} in column {1}\"\n", "                           \" during transform\".format(diff, i))\n", "                    raise ValueError(msg)\n", "                else:\n", "                    # Set the problematic rows to an acceptable value and\n", "                    # continue `The rows are marked `X_mask` and will be\n", "                    # removed later.\n", "                    X_mask[:, i] = valid_mask\n", "                    X[:, i][~valid_mask] = self.categories_[i][0]\n", "            X_int[:, i] = self._label_encoders_[i].transform(X[:, i])\n", "\n", "        if self.encoding == 'ordinal':\n", "            return X_int.astype(self.dtype, copy=False)\n", "\n", "        mask = X_mask.ravel()\n", "        n_values = [cats.shape[0] for cats in self.categories_]\n", "        n_values = np.array([0] + n_values)\n", "        indices = np.cumsum(n_values)\n", "\n", "        column_indices = (X_int + indices[:-1]).ravel()[mask]\n", "        row_indices = np.repeat(np.arange(n_samples, dtype=np.int32),\n", "                                n_features)[mask]\n", "        data = np.ones(n_samples * n_features)[mask]\n", "\n", "        out = sparse.csc_matrix((data, (row_indices, column_indices)),\n", "                                shape=(n_samples, indices[-1]),\n", "                                dtype=self.dtype).tocsr()\n", "        if self.encoding == 'onehot-dense':\n", "            return out.toarray()\n", "        else:\n", "            return out"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_uuid": "0c752b36a7bd7151a269b5f554cd2a891cd36949", "collapsed": true, "_cell_guid": "7a85dc6c-c370-4009-9beb-a04d92635f97"}}, {"cell_type": "markdown", "source": ["DataframeSelector to select specific attributes from the DataFrame:"], "metadata": {"_uuid": "ba8203809b1fd02270b34576c3d805297f4d9a7e", "_cell_guid": "3782eebe-2b95-4202-acc4-904770dd37db"}}, {"source": ["#Use to select specific attributes\n", "from sklearn.base import BaseEstimator, TransformerMixin\n", "\n", "# A class to select numerical or categorical columns \n", "# since Scikit-Learn doesn't handle DataFrames yet\n", "class DataFrameSelector(BaseEstimator, TransformerMixin):\n", "    def __init__(self, attribute_names):\n", "        self.attribute_names = attribute_names\n", "    def fit(self, X, y=None):\n", "        return self\n", "    def transform(self, X):\n", "        return X[self.attribute_names]"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_uuid": "9464d4c896e22fe81c60e7996c35e7fb5e2c42cb", "collapsed": true, "_cell_guid": "5bdf7123-e2c7-42a8-8416-91a78e1094e5"}}, {"cell_type": "markdown", "source": ["Pipeline for the numerical attributes:"], "metadata": {"_uuid": "6534ef4b7ee3b0f9274793271942c8607c11ac35", "_cell_guid": "126a328b-5f7a-486b-9818-f1c7596de95f"}}, {"source": ["from sklearn.pipeline import Pipeline\n", "from sklearn.preprocessing import Imputer\n", "\n", "imputer = Imputer(strategy=\"median\")\n", "\n", "num_pipeline = Pipeline([\n", "        (\"select_numeric\", DataFrameSelector([\"SibSp\", \"Parch\", \"Fare\",'RelativesOnboard'])),\n", "        (\"imputer\", Imputer(strategy=\"median\")),\n", "        ('Scaler', StandardScaler())\n", "    ])"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_uuid": "9777d86be86acfbf13f864c6ab80487978f325ca", "collapsed": true, "_cell_guid": "03a74a6c-8756-4d04-b320-440809ac4593"}}, {"cell_type": "markdown", "source": ["Imputer for the string categorical columns (the regular Imputer does not work on those):"], "metadata": {"_uuid": "839deabfaae3330b823331136a559c97c0ec3c4e", "_cell_guid": "b0a15d94-631b-466c-b573-10bb689d9902"}}, {"source": ["# Inspired from stackoverflow.com/questions/25239958\n", "class MostFrequentImputer(BaseEstimator, TransformerMixin):\n", "    def fit(self, X, y=None):\n", "        self.most_frequent = pd.Series([X[c].value_counts().index[0] for c in X],\n", "                                       index=X.columns)\n", "        return self\n", "    def transform(self, X, y=None):\n", "        return X.fillna(self.most_frequent)"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_uuid": "fe37e139cfb49cadf0590cb0cfb701dde7e6a598", "collapsed": true, "_cell_guid": "cdf35c3b-d150-457e-a34b-1539370486fa"}}, {"cell_type": "markdown", "source": ["Pipeline for the categorical attributes:"], "metadata": {"_uuid": "1f435b58aa3dfc7d7477addafa740d67ce15b8fc", "_cell_guid": "bd6fb325-8cd7-48ae-a527-2dd605dbe912"}}, {"source": ["cat_pipeline = Pipeline([\n", "        (\"select_cat\", DataFrameSelector([\"Pclass\", \"Sex\", \"Embarked\",\"AgeBucket\"])),\n", "        (\"imputer\", MostFrequentImputer()),\n", "        (\"cat_encoder\", CategoricalEncoder(encoding='onehot-dense')),\n", "    ])"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_uuid": "ed4716adbbd57651cd0aee98c8fa05fceea138b3", "collapsed": true, "_cell_guid": "7bce238e-5564-4063-924a-7ab3d42b4039"}}, {"source": ["cat_pipeline.fit_transform(train_data)"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_uuid": "62aad896be5e97ab3a146370e018bcbc72133523", "collapsed": true, "_cell_guid": "b4fc8acd-8a6a-44c9-8df9-3d670997d90e"}}, {"cell_type": "markdown", "source": ["Join the numerical and categorical pipelines:"], "metadata": {"_uuid": "26f1d38677bb32c80b9812587ad4b471a37dc6a8", "_cell_guid": "b675e859-ed44-4147-a9e1-af7a91e8b76b"}}, {"source": ["from sklearn.pipeline import FeatureUnion\n", "preprocess_pipeline = FeatureUnion(transformer_list=[\n", "        (\"num_pipeline\", num_pipeline),\n", "        (\"cat_pipeline\", cat_pipeline),\n", "    ])"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_uuid": "e7e52a0ab0fe34734825964e86c2c0adc80c3c66", "collapsed": true, "_cell_guid": "9b03b17a-feda-4d83-9b85-6610b0c01640"}}, {"cell_type": "markdown", "source": ["Preprocessing pipeline that takes the raw data and outputs numerical input features that we can feed to any Machine Learning model we want."], "metadata": {"_uuid": "2731e28ba6822bb648cd195d40d7ec529059bff1", "_cell_guid": "9fa57fe9-8cc5-445b-a9c0-57d9c3334136"}}, {"source": ["X_train = preprocess_pipeline.fit_transform(train_data)\n", "X_train"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_uuid": "e5e77ffec6f4750e3c2937112070993a23251f51", "collapsed": true, "_cell_guid": "0fe795fe-c662-4d1e-b752-e01c2a403ea0"}}, {"cell_type": "markdown", "source": ["Get the labels:"], "metadata": {"_uuid": "0e9c62186d0a70608d6cc6bc7b0c032d73e32b48", "_cell_guid": "38a8ede6-0d21-459b-890e-77c7620711b6"}}, {"source": ["y_train = train_data['Survived']\n", "\n", "validation_size = 0.2\n", "seed = 7\n", "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=validation_size, \n", "                                                                random_state=seed)"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_uuid": "d7fbb82dc7967a3264de9b740e694672d580bee9", "collapsed": true, "_cell_guid": "1b367b14-7663-4cea-b962-85c276e4d801"}}, {"source": ["X_train.shape\n", "X_test.shape\n", "y_train.shape\n", "y_test.shape"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_uuid": "d50e6f301525bfc5f8a39531eb052b02e834f24e", "collapsed": true, "_cell_guid": "3b76b43e-2664-4b7a-b75a-5e2471969f13"}}, {"cell_type": "markdown", "source": ["## 3.0 Evaluate non-ensemble Baseline methods  \n"], "metadata": {"_uuid": "463d8c4419ef49cf6079eb71eb1d52ca97653b5b", "_cell_guid": "14ad03b2-71d4-42a2-828e-6c3986cdda83"}}, {"source": ["#evaluation - baselines\n", "num_folds = 10\n", "seed = 7\n", "scoring = 'accuracy'\n", "models = []\n", "models.append(('LR', LogisticRegression()))\n", "models.append(('LDA', LinearDiscriminantAnalysis()))\n", "models.append(('KNN', KNeighborsClassifier()))\n", "models.append(('CART', DecisionTreeClassifier()))\n", "models.append(('NB', GaussianNB()))\n", "models.append(('SVM', SVC()))\n", "\n", "results = []\n", "names = []\n", "for name, model in models:\n", "    kfold = KFold(n_splits=num_folds, random_state=seed)\n", "    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n", "    results.append(cv_results)\n", "    names.append(name)\n", "    msg = \"%s %f %f \" % (name, cv_results.mean(), cv_results.std())\n", "    print(msg)"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_uuid": "8c95eb7c9894e2198cc10aa02782ce53ed9ea197", "collapsed": true, "_cell_guid": "1a7c941a-2e65-44b8-bb52-035410a1a4db"}}, {"source": ["# compare algorithms\n", "fig = plt.figure()\n", "fig.suptitle('Comparison of non-ensemble methods')\n", "ax = fig.add_subplot(111)\n", "plt.boxplot(results)\n", "ax.set_xticklabels(names)\n", "plt.show();"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_uuid": "cffab66a47038a71f8e6030d465123ef4ef610a7", "collapsed": true, "_cell_guid": "56579d93-81de-4848-b763-c6437632a198"}}, {"cell_type": "markdown", "source": ["## 3.1 Tune the best non-ensemble methods"], "metadata": {"_uuid": "b03e4be1064d704feba982f6a3494fdc995c322e", "_cell_guid": "d0f1a079-7b6b-4cf1-aca3-dc73418d7874"}}, {"source": ["neighbors = [1, 3, 5, 7, 9, 15, 19, 21]\n", "param_grid = dict(n_neighbors=neighbors)\n", "model = KNeighborsClassifier()\n", "kfold = KFold(n_splits=num_folds, random_state=seed)\n", "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n", "grid_result = grid.fit(X_train, y_train)\n", "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n", "means = grid_result.cv_results_['mean_test_score']\n", "stds = grid_result.cv_results_['std_test_score']\n", "params = grid_result.cv_results_['params']\n", "for mean, stdev, param in zip(means, stds, params):\n", "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_uuid": "43134a72bc11081e55d1c80c223bc6999b04256e", "collapsed": true, "_cell_guid": "90c74367-9ddd-4071-a314-f4beeb37ba0a"}}, {"source": ["# Tune scaled SVM\n", "c_values = [1,1.3,1.5,1.7]\n", "kernel_values = ['rbf']\n", "param_grid = dict(C=c_values, kernel=kernel_values)\n", "model = SVC()\n", "kfold = KFold(n_splits=num_folds, random_state=seed)\n", "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n", "grid_result = grid.fit(X_train, y_train)\n", "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n", "means = grid_result.cv_results_['mean_test_score']\n", "stds = grid_result.cv_results_['std_test_score']\n", "params = grid_result.cv_results_['params']\n", "for mean, stdev, param in zip(means, stds, params):\n", "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_uuid": "ed36deed1806235e554ffc131a886c0bd52bc755", "collapsed": true, "_cell_guid": "a49ba707-4560-4d93-b372-3ffc81ab6d35"}}, {"source": ["svm_clf = SVC(C=1.3,kernel='rbf')\n", "svm_clf.fit(X_train,y_train)"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_uuid": "e6306ac25f970cc373d823a8590b766f8c839f11", "collapsed": true, "_cell_guid": "add1b3a1-227e-497d-b7c6-d1815be563dd"}}, {"cell_type": "markdown", "source": ["Try on the test data and make submission"], "metadata": {"_uuid": "a2c1eb24e26c5b5a61ef1513f6ef591f4aa91a40", "_cell_guid": "d7a03408-0092-4537-b5f1-1dfce54ca5dd"}}, {"source": ["X_test = preprocess_pipeline.transform(test_data)\n", "y_pred = svm_clf.predict(X_test)"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_uuid": "1eb36d539743eb5a20cf01718d9f3aa6c9f37f6e", "collapsed": true, "_cell_guid": "2a40a60c-8169-4325-b2d2-3a6de6a3518b"}}, {"source": ["from sklearn.model_selection import cross_val_score\n", "scores = cross_val_score(svm_clf, X_train, y_train, cv=10)\n", "scores.mean()"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_uuid": "831e1d82788a038a8286f3c3b45ce7754d55861a", "collapsed": true, "_cell_guid": "d52891ae-4670-4fc0-9f70-996662b7bd64"}}, {"source": ["output = pd.DataFrame({ 'PassengerId' : test_data['PassengerId'], 'Survived': y_pred })"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_uuid": "b67e37bced31e588b074aeb9d5e7ce57e274cff0", "collapsed": true, "_cell_guid": "7fee492c-2c27-4c59-acff-4207ff19f8fd"}}, {"source": ["output.to_csv('submission.csv', index=False)"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_uuid": "8c3aed69e1cb23eeef1248182f31843896f59820", "collapsed": true, "_cell_guid": "0829bde7-c49b-45ec-9562-96ca42dc1da5"}}, {"cell_type": "markdown", "source": ["## 4.0 Evaluate ensemble methods"], "metadata": {"_uuid": "c792ce866525805ff2c26d7ef0acf494dc9620f9", "_cell_guid": "d29ae6e1-258c-4370-916a-b4840b6b2b2d"}}, {"source": ["ensembles = []\n", "ensembles.append(('AB', AdaBoostClassifier()))\n", "ensembles.append(('GBM', GradientBoostingClassifier()))\n", "ensembles.append(('RF', RandomForestClassifier()))\n", "ensembles.append(('ET', ExtraTreesClassifier()))\n", "results = []\n", "names = []\n", "for name, model in ensembles:\n", "    kfold = KFold(n_splits=num_folds, random_state=seed)\n", "    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n", "    results.append(cv_results)\n", "    names.append(name)\n", "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n", "    print(msg)\n", "\n", "# Compare Algorithms\n", "fig = plt.figure()\n", "fig.suptitle('Ensemble Algorithm Comparison')\n", "ax = fig.add_subplot(111)\n", "plt.boxplot(results)\n", "ax.set_xticklabels(names)\n", "plt.show();"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_uuid": "9c75bf3d7c0cae3f3878719e6c7ba29808c4f976", "collapsed": true, "_cell_guid": "dc7f922b-618e-4d31-a07e-ab5bf6d4d699"}}, {"cell_type": "markdown", "source": ["## 4.1 Evaluate Ensemble method"], "metadata": {"_uuid": "b3eabfd7befc58c871156c15e15fa8eeb5c1049f", "_cell_guid": "97fef4e4-a7be-43f5-9b43-3abeab3b04fa"}}, {"source": ["# Tune scaled GBM\n", "num_trees = [10,50,100,150,200,250,300]\n", "#kernel_values = ['linear', 'poly', 'rbf', 'sigmoid']\n", "param_grid = dict(n_estimators=num_trees)\n", "#param_test2 = {'max_depth':range(5,16,2), 'min_samples_split':range(200,1001,200),'n_estimators':[100,200,300]}                 \n", "model = GradientBoostingClassifier()\n", "kfold = KFold(n_splits=num_folds, random_state=seed)\n", "grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n", "grid_result = grid.fit(X_train, y_train)\n", "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n", "means = grid_result.cv_results_['mean_test_score']\n", "stds = grid_result.cv_results_['std_test_score']\n", "params = grid_result.cv_results_['params']\n", "for mean, stdev, param in zip(means, stds, params):\n", "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_uuid": "bbe9b271763b0642338a5650c3675bde4a395695", "collapsed": true, "_cell_guid": "f947d9dc-a782-4796-b407-6a57f72146b4"}}, {"source": ["GB_clf = GradientBoostingClassifier(max_depth=9,n_estimators = 150,min_samples_split=400)\n", "GB_clf.fit(X_train,y_train)\n", "X_test = preprocess_pipeline.transform(test_data)\n", "y_pred = GB_clf.predict(X_test)\n", "from sklearn.model_selection import cross_val_score\n", "\n", "scores = cross_val_score(GB_clf, X_train, y_train, cv=10)\n", "scores.mean()\n", "output = pd.DataFrame({ 'PassengerId' : test_data['PassengerId'], 'Survived': y_pred })\n", "output.to_csv('submission.csv', index=False)"], "execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_uuid": "64adf8bf41945d13dc6c6ce57e1b285656c54031", "collapsed": true, "_cell_guid": "8f01a50e-cc6e-42ab-8370-85f1439d56c2"}}], "metadata": {"language_info": {"name": "python", "mimetype": "text/x-python", "file_extension": ".py", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "version": "3.6.3", "nbconvert_exporter": "python"}, "kernelspec": {"name": "python3", "language": "python", "display_name": "Python 3"}}}