{"cells": [{"source": ["# Plan\n", "1. Load Data\n", "1. Data Insights\n", "1. Preprocess Data\n", "1. Build Models\n", "1. Train Models\n", "1. Test Models\n", "1. Evaluate Models"], "metadata": {"_cell_guid": "6f7746a0-ee97-4a77-b41c-64a823067376", "_uuid": "bfc53bb96498f02c109381dcff19946511098ac5"}, "cell_type": "markdown"}, {"source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "import matplotlib.pyplot as plt # Visualization\n", "from sklearn.model_selection import KFold\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "\n", "# Any results you write to the current directory are saved as output."], "metadata": {"_cell_guid": "df506eb3-c787-4f68-a7d7-4de6bbb20355", "_uuid": "d2c05eaeadddbcd0750105be59d1f2258659d6cc"}, "outputs": [], "execution_count": null, "cell_type": "code"}, {"source": ["# Load Data\n", "train_raw = pd.read_csv(\"../input/train.csv\")\n", "test_raw = pd.read_csv(\"../input/test.csv\")\n", "gs_raw = pd.read_csv(\"../input/gender_submission.csv\")"], "metadata": {"collapsed": true, "_cell_guid": "b66922f9-e5cd-4007-abe2-c227e2d99438", "_uuid": "ef7ae2a569fd1d9a756c9036930b4b3528f800ed"}, "outputs": [], "execution_count": null, "cell_type": "code"}, {"source": ["# Data Insights - Training\n", "train_raw.describe(include=\"all\")\n"], "metadata": {"_cell_guid": "74e6619b-3e61-4877-b19a-94339d540ff8", "_uuid": "ea7b68e4e0f9f31b0ae336926251a3e2823676ca"}, "outputs": [], "execution_count": null, "cell_type": "code"}, {"source": ["print(np.nan_to_num(train_raw['Cabin']))"], "metadata": {"_cell_guid": "c32cd72d-dc8f-431e-ba26-3ad15e0aa85b", "_uuid": "3518af41c1ab9f6d28b166154c646448475673bb"}, "outputs": [], "execution_count": null, "cell_type": "code"}, {"source": ["train_raw.dtypes"], "metadata": {"_cell_guid": "51a3e998-b2fa-4348-8213-ca6b63ca4ad2", "_uuid": "003b5027f1fb41940f7774bcd4a484b7c729a7d1"}, "outputs": [], "execution_count": null, "cell_type": "code"}, {"source": ["# Training Data Insigts\n", "* $m = 891$\n", "* $n_x = 10$ explicit features\n", "\n", "Data Types\n", "\n", "Field              |   Datatype \n", "------------------|---------------\n", "PassengerId |    int64\n", "Survived       |  int64\n", "Pclass          | int64\n", "Name           | object\n", "Sex              | object\n", "Age              |float64\n", "SibSp           | int64\n", "Parch           | int64\n", "Ticket          |object\n", "Fare           |float64\n", "Cabin          | object\n", "Embarked    |    object\n"], "metadata": {"_cell_guid": "47e32e03-6f69-4038-b5c6-932d358e2ab7", "_uuid": "c791ac3ff7bf1e132ae4b6507143b4dba15e83e1"}, "cell_type": "markdown"}, {"source": ["\n", "print(train_raw['Pclass'].T)\n", "print(train_raw['Cabin'].T)"], "metadata": {"_cell_guid": "e715b466-0de2-4083-a05f-af91b678cfbc", "_uuid": "3f9904c9890344b6eff63fdfdf870ebace2d3179"}, "outputs": [], "execution_count": null, "cell_type": "code"}, {"source": ["# Data Insigts (Testing)\n", "test_raw.describe(include=\"all\")"], "metadata": {"_cell_guid": "4a187e3e-5b30-4a73-b09e-d1c4499d6b3a", "_uuid": "09d6d812e718c0b5d186d5ce8670148045e1f687"}, "outputs": [], "execution_count": null, "cell_type": "code"}, {"source": ["# Data Insigts (Gender Submission)\n", "gs_raw.describe(include=\"all\")\n"], "metadata": {"_cell_guid": "5dc5001f-37ac-4e9c-9fe9-012913c9f84f", "_uuid": "a1477b8a8b45ae0c85ec343756ca409438238a8e"}, "outputs": [], "execution_count": null, "cell_type": "code"}, {"source": ["# Preprocess Data\n", "Find optimal feature mapping function $\\mathcal{M} : \\phi \\mapsto \\phi'$ such that $\\forall \\phi'_i \\in \\phi', \\phi'_i \\in \\mathbb{R}$, and $\\mathbb{E}(\\phi'_i) \\approx 0$\n", "\n", "with loose constraint of $ \\phi'_i \\in [-1, 1]$ \n", "\n", "Of course this will depend on feature data type, moreover, some NaN values when found will be substituted by 0 \n", "\n", "So I propose defining the following mapping function\n", "\n", "## Pclass\n", "Since this is an integer value $\\phi_{Pclass} = \\phi_0 \\in  \\{1,2,3  \\} \\in \\mathbb{Z}$, this mapping function will be ~~identity mapping ~~ normalized\n", "i.e. $ \\mathcal{M}_0 \\colon \\phi_0 \\mapsto \\phi_0 - 2$ \n", "\n", "## Name            \n", "Although passenger name is a rich feature, containing multidimensional data, including ethnicity, sex, and social class, I will bypass it in this iteration of work and have our mapping function map to a 0\n", "\n", "$ \\mathcal{M}_1 \\colon \\phi_1 \\mapsto 0$ \n", "\n", "## Sex             \n", "I will arbitrarily assign 1 and -1 to either sex, say $ \\mathcal{M}_2  \\colon female \\mapsto 1$ and $ \\mathcal{M}_2  \\colon male \\mapsto -1$   \n", "\n", "## Age            \n", "Age $\\phi_{age} = \\phi_3 \\in \\mathbb{R}^{+} \\sim \\mathcal{N}(30,14)$ \n", "\n", "hence $ \\mathcal{M}_3  \\colon \\phi_3 \\mapsto \\frac{\\phi_3 - 30}{14}$\n", "\n", "both $\\mu$ and $\\sigma$ are approximate values\n", "\n", "## SibSp            \n", "Similar to Pclass, this feature will ~~have identity mapping~~ be normalized\n", "\n", "i.e. $ \\mathcal{M}_4 \\colon \\phi_4 \\mapsto \\phi_4 - 3$ \n", "## Parch   \n", "Similar to Pclass, this feature will ~~have identity mapping~~ be normalized\n", "\n", "i.e. $ \\mathcal{M}_5 \\colon \\phi_5 \\mapsto \\phi_5 - 2$ \n", "## Ticket          \n", "Similar to name, ticket might hide some useful implicit features, but I will ignore it for simplicity \n", "$ \\mathcal{M}_6 \\colon \\phi_6 \\mapsto 0$ \n", "\n", "## Fare           \n", "Similar to age, Fare $\\phi_{fare} = \\phi_7 \\in \\mathbb{R}^{+} \\sim \\mathcal{N}(33,52)$ \n", "hence $ \\mathcal{M}_7  \\colon \\phi_7 \\mapsto \\frac{\\phi_7 - 33}{52}$\n", "both $\\mu$ and $\\sigma$ are approximate values\n", "\n", "## Cabin           \n", "Cabin will be defferred, hence $ \\mathcal{M}_8 \\colon \\phi_8 \\mapsto 0$ \n", "Experiment: I will create a binary feature vector of  8 features, each one corresponds to the presence of one of the following letters in cabin $\\{ A, B, C, D, E, F, G, T\\}$\n", "\n", "\n", "## Embarked        \n", "Embarked $\\phi_9 \\in \\{C, Q, S\\}$, I will use a simple trinary mapping function $ \\mathcal{M}_9 \\colon C \\mapsto -1$, $ \\mathcal{M}_9 \\colon Q \\mapsto 0$, and  $ \\mathcal{M}_9 \\colon S \\mapsto 1$ \n"], "metadata": {"_cell_guid": "cbad3504-84b9-4e86-90ba-25e7f5e38a60", "_uuid": "e2c3ea71c75839fc979e7f532f1aec208485a397"}, "cell_type": "markdown"}, {"source": ["# Mapping function Extracting PassengerID, Survival, Feature vector phi_prime \n", "def get_features(X):\n", "    # PassengerId\n", "    PassID = X['PassengerId']\n", "    m = PassID.shape[0]\n", "    nx = 20 # Number of features, this is hardcoded for the timebeing\n", "    # Add two more columns for embarked, and one more for sex\n", "    # Add 7 more for cabin\n", "    # Features \n", "    X_prime = np.zeros((m,nx))\n", "    # 0 Pclass\n", "    X_prime[:,0] = X['Pclass'] - 2\n", "    # 1 Name           \n", "    # 2,3 Sex\n", "    X_prime[:,2] = 1* (X['Sex'] == 'female')\n", "    X_prime[:,3] = 1* (X['Sex'] == 'male')\n", "    \n", "    # 4 Age           \n", "    mu = 30\n", "    sigma = 14 \n", "    X_prime[:,4] = (X['Age'] - mu ) / sigma\n", "    # 5 SibSp          \n", "    X_prime[:,5] = X['SibSp'] - 3\n", "    # 6 Parch     \n", "    X_prime[:,6] = X['Parch'] - 2\n", "    # 7 Ticket         \n", "    # 8 Fare    \n", "    mu = 33\n", "    sigma = 52 \n", "    X_prime[:,8] = (X['Fare'] - mu ) / sigma\n", "    # 9 Cabin   \n", "    X_prime[:,9] = X['Cabin'].str.contains('A')*1\n", "    X_prime[:,10] = X['Cabin'].str.contains('B')*1\n", "    X_prime[:,11] = X['Cabin'].str.contains('C')*1\n", "    X_prime[:,12] = X['Cabin'].str.contains('D')*1\n", "    X_prime[:,13] = X['Cabin'].str.contains('E')*1\n", "    X_prime[:,14] = X['Cabin'].str.contains('F')*1\n", "    X_prime[:,15] = X['Cabin'].str.contains('G')*1\n", "    X_prime[:,16] = X['Cabin'].str.contains('T')*1\n", "    # Extend Cabin to pick all possible letters, and the numeric value \n", "    # 17,18,19 Embarked  {C,Q,S}\n", "    selector = X['Embarked'] == 'C'\n", "    X_prime[:,17] = selector * 1 \n", "\n", "    selector = X['Embarked'] == 'Q'\n", "    X_prime[:,18] = selector * 1 \n", "\n", "    selector = X['Embarked'] == 'S'\n", "    X_prime[:,19] = selector *1\n", "    return PassID, np.nan_to_num(X_prime)"], "metadata": {"collapsed": true, "_cell_guid": "a6a476b9-81b1-40a6-9842-3bdc8d118318", "_uuid": "ca50f417340548ff8c73b0b6db77eb95d4451990"}, "outputs": [], "execution_count": null, "cell_type": "code"}, {"source": ["Attempt to create a kernel of features $\\kappa(\\phi')$ to find deeper insights, one idea is to create a new feature vector $\\kappa_r(\\phi') = (\\phi' \\phi'^2 ...\\phi'^r) $\n"], "metadata": {"_cell_guid": "fa5afb29-5150-47f9-85e0-c13ea6e77e8b", "_uuid": "17bb46c2cbf7483ddeb375e4666a20af51549897"}, "cell_type": "markdown"}, {"source": ["def kernel(X):\n", "    K_x = np.concatenate((X, np.exp(-X**2)), axis=1)\n", "    return K_x"], "metadata": {"collapsed": true, "_cell_guid": "fc85a60b-3a57-4ccb-8142-3b099a395fdd", "_uuid": "f49fa809e39767eb9d8e301822b1981c6be78f9b"}, "outputs": [], "execution_count": null, "cell_type": "code"}, {"source": ["## Neural Network\n", "it seems that Logistic regression is stuck at 77.99%, despite non-linearity simulated by my kernel function.\n", "Let me try a new method, using neural network with different activation functions in the hidden layers.\n", "\n", "### Method\n", "I will simulate each neuron behaviour as follows\n", "$$ \\nu_\\psi(\\mathbf{x})  = \\psi(\\mathbf{\\omega}^T \\mathbf{x} + b)$$\n", "where $\\mathbf{x}$ is input feature vector, $\\psi$ is an activation function, $\\mathbf{\\omega}$ is weights vector, and $b$ is bias \n", "\n", "Learning will be done using normal back propagation, since this will be a shallow neural network and will be as follows \n", "\n", "$$  \\mathbf{\\omega} \\gets \\mathbf{\\omega} - \\alpha \\frac{\\partial}{\\partial \\mathbf{\\omega}} \\mathbf{J}_\\psi(\\mathbf{x},y;\\mathbf{\\omega},b) \\\\\n", "b \\gets b - \\alpha \\frac{\\partial}{\\partial b} \\mathbf{J}_\\psi(\\mathbf{x},y;\\mathbf{\\omega},b) $$\n", "\n", "where $\\mathbf{J}_\\psi(\\mathbf{x},y;\\mathbf{\\omega},b)$ is an objective function to minimize \n", "\n", "### Activation functions\n", "I will start by three simple functions \n", "#### Sigmoid \n", "$$ \\psi_\\sigma(\\mathbf{z}) = \\frac{1}{1 + e^{-\\mathbf{z}}}$$\n", "$$ \\frac{\\partial}{\\partial \\mathbf{z}} \\psi_\\sigma(\\mathbf{z}) = \\psi_\\sigma(\\mathbf{z})(1-\\psi_\\sigma(\\mathbf{z}))$$\n", "$$ \\frac{\\partial}{\\partial \\mathbf{\\omega}} \\psi_\\sigma(\\mathbf{z}) = \\psi_\\sigma(\\mathbf{z})(1-\\psi_\\sigma(\\mathbf{z})) \\mathbf{x}^T \\\\\n", "\\frac{\\partial}{\\partial b} \\psi_\\sigma(\\mathbf{z}) = \\psi_\\sigma(\\mathbf{z})(1-\\psi_\\sigma(\\mathbf{z}))$$\n", "#### Hyperbolic TAN (tanh)\n", "$$ \\psi_{\\tanh}(\\mathbf{z}) = \\frac{2}{1 + e^{-2\\mathbf{z}}} - 1 $$\n", "$$ \\frac{\\partial}{\\partial \\mathbf{z}} \\psi_{\\tanh}(\\mathbf{z}) =1-\\psi_{\\tanh}(\\mathbf{z})^2 $$\n", "$$ \\frac{\\partial}{\\partial \\mathbf{w}} \\psi_{\\tanh}(\\mathbf{z}) =1-\\psi_{\\tanh}(\\mathbf{z})^2 \\mathbf{x}^T$$\n", "$$ \\frac{\\partial}{\\partial b} \\psi_{\\tanh}(\\mathbf{z}) =1-\\psi_{\\tanh}(\\mathbf{z})^2 $$\n", "#### Rectified Linear Unit (ReLU)\n", "$$ \\psi_{ReLU}(\\mathbf{z}) = \\left\\{\n", "                \\begin{array}{ll}\n", "                  \\mathbf{z}\\  |\\  \\mathbf{z} > 0\\\\\n", "                  0\\ |\\ else\n", "                \\end{array}\n", "              \\right. $$\n", "$$ \\frac{\\partial}{\\partial \\mathbf{z}} \\psi_{ReLU}(\\mathbf{z}) = \\left\\{\n", "                \\begin{array}{ll}\n", "                  1  |\\  \\mathbf{z} > 0\\\\\n", "                  0\\ |\\ else\n", "                \\end{array}\n", "              \\right. $$\n", "  $$ \\frac{\\partial}{\\partial \\mathbf{\\omega}} \\psi_{ReLU}(\\mathbf{z}) = \\left\\{\n", "                \\begin{array}{ll}\n", "                  \\mathbf{x}  |\\  \\mathbf{z} > 0\\\\\n", "                  0\\ |\\ else\n", "                \\end{array}\n", "              \\right. $$\n", "$$ \\frac{\\partial}{\\partial b} \\psi_{ReLU}(\\mathbf{z}) = \\left\\{\n", "                \\begin{array}{ll}\n", "                  1  |\\  \\mathbf{z} > 0\\\\\n", "                  0\\ |\\ else\n", "                \\end{array}\n", "              \\right. $$\n"], "metadata": {"collapsed": true, "_cell_guid": "6664f5e0-acd6-4733-884a-688c73588846", "_uuid": "563955058121c60cb5b2eef878990f08ff46cf4e"}, "cell_type": "markdown"}, {"source": ["# Neuron \n", "class Neuron:\n", "    def __init__(self, feature_size, alpha, act=2):\n", "        self.w = np.random.random_sample((feature_size,1))*0.1\n", "        self.b = 0.0\n", "        self.alpha = alpha\n", "        self.activation_function = act\n", "    def activate(self, z):\n", "        # based on set activation function set make the calculation\n", "        if self.activation_function == 2: # Sigmoid\n", "            return 1 / (1 + np.exp(-z))\n", "        if self.activation_function == 3: # tanh\n", "            return 2 / (1 + np.exp(-z)) - 1\n", "        if self.activation_function == 7: # ReLU\n", "            return (z>0)*z\n", "        if self.activation_function == 21: # Gaussian\n", "            return np.exp(-z**2)\n", "    def gradient(self, y_hat, err):\n", "        \n", "        m = y_hat.shape[0] # Sample size\n", "        #da = y/y_hat + (1-y)/(1-y_hat)\n", "        # based on set activation function set make the calculation\n", "        if self.activation_function == 2: # Sigmoid\n", "            return  err*y_hat*(1-y_hat)\n", "        if self.activation_function == 3: # tanh\n", "            return err*(1-y_hat**2)\n", "        if self.activation_function == 7: # ReLU\n", "            return (y_hat>0)*err\n", "        if self.activation_function == 21: # Gaussian\n", "            return -2*y_hat*np.exp(-y_hat**2)*err\n", "\n", "    def forward(self, X):\n", "        # given feature vector X, find activation response\n", "        z = np.dot(X,self.w)+self.b\n", "        a = self.activate(z)\n", "        return a\n", "    def backward(self, X, err):\n", "        m = y.shape[0]\n", "        y_hat = self.forward(X)\n", "        grad = self.gradient(y_hat,err)\n", "        self.b -= self.alpha * np.sum(grad)/m\n", "        self.w -= self.alpha * np.dot(X.T,grad)/m\n", "    def print_params(self):\n", "        print(\"b = {}, w = {}\".format(self.b,self.omega.T))"], "metadata": {"collapsed": true, "_cell_guid": "13b70109-3599-4fb0-91df-b5e6d984132b", "_uuid": "b663ba0d9370f1921433990de8a011c623a39936"}, "outputs": [], "execution_count": null, "cell_type": "code"}, {"source": ["## Cross Validation \n", "I need to create some cross validation generation function capable of pulling random samples from my training data, train on n-1 samples and test on 1\n", "\n", "I will start first by pure randomization, then by applying t-test on sample data to avoid bias"], "metadata": {"_cell_guid": "a7fcaf91-b58c-46f0-95fc-5a9c46825b7f", "_uuid": "b84acdaf33370098ad8e9744342eced90e43a501"}, "cell_type": "markdown"}, {"source": ["# TODO: Cross Validation Generator, Expects input features and labels, and returns equally sized samples \n", "def cross_validated(X, n_samples):\n", "    kf = KFold(n_samples, shuffle = True)\n", "    result = [group for group in kf.split(X)]\n", "    \n", "    return result"], "metadata": {"collapsed": true, "_cell_guid": "e2da7578-225c-4ce3-9dab-2b4556d397fa", "_uuid": "29e0da2b9636c23e6345f4ab45400fb4aaca29f2"}, "outputs": [], "execution_count": null, "cell_type": "code"}, {"source": ["# Performance Measure\n", "I think I was doing something wrong measuring performance not taking into account false positives and false negatives\n", "i will use instead mean SSD (Sum of Squared Difference)\n", "\n", "$$ SSD(\\hat{y},y^{*}) = \\frac{1}{m} \\sum_i^m ( {y^{*}}^{(i)} - \\hat{y}^{(i)} )^2 $$ "], "metadata": {"_cell_guid": "0984c095-89b1-42d9-8fdf-029ad36d9987", "_uuid": "bd191d7822790b05ef076b8bc776b0b8c0d84e9d"}, "cell_type": "markdown"}, {"source": ["def ssd(y_hat, y):\n", "    m = y.shape[0]\n", "    return np.dot((y_hat-y).T,y_hat-y)/m"], "metadata": {"collapsed": true, "_cell_guid": "a0368769-3902-4a19-8b23-9bfab78a913f", "_uuid": "1eca728f1a62b9d6b4f52f47a69cbdd5247c6a09"}, "outputs": [], "execution_count": null, "cell_type": "code"}, {"source": ["\n", "PassID, X = get_features(train_raw)\n", "X = X\n", "y = np.array(train_raw['Survived'])\n", "y = np.reshape(y,(-1,1))\n", "iterations = 3000\n", "instances = 5\n", "\n", "folds = 3\n", "oinst = folds\n", "cv_groups = cross_validated(X, folds)\n", "\n", "n_x = X.shape[1] # Features\n", "alpha = 0.05\n", "alph = np.ones(oinst)*alpha + (np.random.random_sample(oinst)-0.5)*0.0001\n", "# Setup single neurons\n", "N_g = [Neuron(n_x,alpha,21) for j in range(oinst)]\n", "final_cost = []\n", "\n", "for j in range(oinst):\n", "    cost_g = []\n", "    N_g[j].alpha = alph[j]\n", "    \n", "    # Prepare Training and testing sets \n", "    \n", "    X_train = X[cv_groups[j][1],:] \n", "    y_train = np.reshape(y[cv_groups[j][1],0],(-1,1)) \n", "    X_test = X[cv_groups[j][0],:] \n", "    y_test = np.reshape(y[cv_groups[j][0],0],(-1,1))\n", "    \n", "    for i in range(iterations):\n", "        # Evaluate\n", "        a_g = N_g[j].forward(X_train)\n", "        \n", "        # Learn\n", "        N_g[j].backward(X_train,a_g - y_train)\n", "        \n", "        # Evaluate\n", "        # a_g = N_g[j].forward(X_test)\n", "        \n", "        # Performance vote\n", "        # cost_g.append(np.sum(np.abs(a_g-y_test))/y.shape[0])\n", "    \n", "    a_g = (N_g[j].forward(X_test)>0.5)*1\n", "    m = y_test.shape[0]\n", "    \n", "    final_cost.append(1-ssd(a_g,y_test))\n", "\n", "    print(\"Testing:[alpha ={}]  Success = {}\".format(N_g[j].alpha,1-ssd(a_g,y_test)))\n", "\n", "    #plt.plot(cost_g)\n", "    #plt.title(\"Cost over time\")\n", "    #plt.show()\n", "\n", "    #plt.scatter(a_g, y)\n", "    #plt.show()\n", "final_cost = np.reshape((final_cost),(-1,1)).tolist()     \n", "\n", "plt.plot(final_cost)    \n", "plt.show()\n", "\n"], "metadata": {"_cell_guid": "1c67d8a8-009e-4e77-ac0a-585850827e24", "_uuid": "78306485d36db3e870f0b320f0e6198b63892bef"}, "outputs": [], "execution_count": null, "cell_type": "code"}, {"source": ["PassID, X = get_features(test_raw)\n", "X2 = X\n", "\n", "best_of_breed = final_cost.index(max(final_cost))\n", "a_gm = N_g[best_of_breed].forward(X2)\n", "    \n", "y_hat = np.reshape(a_gm,(-1,1))\n", "\n", "print(y_hat.shape)\n", "data = y_hat > 0.5\n", "data = data*1\n", "s0 = pd.Series(PassID, index=PassID)\n", "s1 = pd.Series(data[:,0], index=PassID)\n", "\n", "df = pd.DataFrame(data = s1,index = PassID)\n", "df.columns = ['Survived']\n", "df.to_csv('best_of_breed_gauss_nn_3.csv', sep=',')"], "metadata": {"_cell_guid": "8039bda4-2ead-4355-9b29-73ea294d932a", "_uuid": "3b2a4ca96da91d6efcda2d8116188ad9e2e7b237"}, "outputs": [], "execution_count": null, "cell_type": "code"}, {"source": ["# Try it with 2 layered NN"], "metadata": {"collapsed": true, "_cell_guid": "25a6169b-1dcb-4be7-ae89-fc3bf28efd2e", "_uuid": "285c2cdd4aeb424265618c4c1cbf880f36b8bee5"}, "cell_type": "markdown"}, {"source": ["\n", "PassID, X = get_features(train_raw)\n", "X = X\n", "y = np.array(train_raw['Survived'])\n", "y = np.reshape(y,(-1,1))\n", "iterations = 5000\n", "instances = 5\n", "\n", "folds = 3\n", "oinst = folds\n", "cv_groups = cross_validated(X, folds)\n", "\n", "n_x = X.shape[1] # Features\n", "alpha = 0.25\n", "alph = np.ones(oinst)*alpha + (np.random.random_sample(oinst)-0.5)*0.0001\n", "# Setup Hidden Layer\n", "N_21 = [Neuron(n_x,alpha,3) for j in range(oinst)]\n", "N_22 = [Neuron(n_x,alpha,3) for j in range(oinst)]\n", "# Setup output layer\n", "N_g = [Neuron(2,alpha,2) for j in range(oinst)]\n", "final_gain = []\n", "all_gain = []\n", "for j in range(oinst):\n", "    cost_g = []\n", "    N_g[j].alpha = alph[j]\n", "    N_21[j].alpha = alph[j]\n", "    N_22[j].alpha = alph[j]\n", "    \n", "    # Prepare Training and testing sets \n", "    X_train = X[cv_groups[j][1],:] \n", "    y_train = np.reshape(y[cv_groups[j][1],0],(-1,1)) \n", "    X_test = X[cv_groups[j][0],:] \n", "    y_test = np.reshape(y[cv_groups[j][0],0],(-1,1))\n", "    \n", "    for i in range(iterations):\n", "        # Evaluate\n", "        a_21 = N_21[j].forward(X_train) \n", "        a_22 = N_22[j].forward(X_train)  \n", "        a_2 = np.concatenate((a_21,a_22),axis=1)\n", "        a_g = N_g[j].forward(a_2)\n", "        \n", "        # Learn\n", "        grad = N_g[j].gradient(a_g,a_g - y_train)\n", "        N_g[j].backward(a_2,a_g - y_train)\n", "        \n", "        N_21[j].backward(X_train,grad*N_g[j].w[0])\n", "        N_22[j].backward(X_train,grad*N_g[j].w[1])\n", "    \n", "    \n", "    # Finally Evaluate Model \n", "    a_21 = N_21[j].forward(X_test) \n", "    a_22 = N_22[j].forward(X_test)  \n", "    a_2 = np.concatenate((a_21,a_22),axis=1)\n", "    a_g = N_g[j].forward(a_2)\n", "    #plt.scatter(a_g, y_test)\n", "    #plt.show()\n", "    a_g = (a_g>0.5)*1\n", "    m = y_test.shape[0]\n", "    final_gain.append(1-ssd(a_g,y_test))\n", "\n", "    print(\"Testing:[alpha ={}]  Success = {}\".format(N_g[j].alpha,1-ssd(a_g,y_test)))\n", "\n", "    # Evaluate All\n", "    a_21 = N_21[j].forward(X) \n", "    a_22 = N_22[j].forward(X)  \n", "    a_2 = np.concatenate((a_21,a_22),axis=1)\n", "    a_g = N_g[j].forward(a_2)\n", "    #plt.scatter(a_g, y)\n", "    #plt.show()\n", "    a_g = (a_g>0.5)*1\n", "    m = y.shape[0]\n", "    all_gain.append(1-ssd(a_g,y))\n", "    print(\"Testing All:[alpha ={}]  Success = {}\".format(N_g[j].alpha,1-ssd(a_g,y)))\n", "    #plt.plot(cost_g)\n", "    #plt.title(\"Cost over time\")\n", "    #plt.show()\n", "\n", "\n", "final_gain= np.reshape((final_gain),(-1,1)).tolist()         \n", "all_gain= np.reshape((all_gain),(-1,1)).tolist()   \n", "plt.plot(final_gain)\n", "plt.plot(all_gain)\n", "plt.show()\n"], "metadata": {"_cell_guid": "c9c8bc33-5543-464c-81fd-1b8d69483a27", "_uuid": "0db7e6dbc52fdedd15114b520a9fa309d2d85c42"}, "outputs": [], "execution_count": null, "cell_type": "code"}, {"source": ["PassID, X = get_features(test_raw)\n", "X2 = X\n", "\n", "best_of_breed = final_gain.index(max(final_gain))\n", "a_21 = N_21[best_of_breed].forward(X2) \n", "a_22 = N_22[best_of_breed].forward(X2)  \n", "a_2 = np.concatenate((a_21,a_22),axis=1)\n", "a_gm = N_g[best_of_breed].forward(a_2)\n", "    \n", "y_hat = np.reshape(a_gm,(-1,1))\n", "\n", "print(y_hat.shape)\n", "data = y_hat > 0.5\n", "data = data*1\n", "s0 = pd.Series(PassID, index=PassID)\n", "s1 = pd.Series(data[:,0], index=PassID)\n", "\n", "df = pd.DataFrame(data = s1,index = PassID)\n", "df.columns = ['Survived']\n", "df.to_csv('best_of_2tanh_1sig.csv', sep=',')"], "metadata": {"_cell_guid": "eef0587f-65b2-49c3-a924-34946408d63a", "_uuid": "405835933b727bb4580dda46da8054fc0b81e9df"}, "outputs": [], "execution_count": null, "cell_type": "code"}, {"source": ["# Search Optimal Learning Parameters \n", "* Search for best $\\alpha$ at 3000 Epochs\n", "* Search for best number of iterations at optimal $\\alpha$"], "metadata": {"_cell_guid": "6c5062ef-5063-43a1-aa01-166f46c0576d", "_uuid": "a987fd65d718ca49e7b911d7f67a1831ae66aabf"}, "cell_type": "markdown"}, {"source": ["\n", "PassID, X = get_features(train_raw)\n", "X = X\n", "y = np.array(train_raw['Survived'])\n", "y = np.reshape(y,(-1,1))\n", "iterations = 5000\n", "instances = 4\n", "alph = np.linspace(0.1, 0.40, instances)\n", "\n", "folds = 3\n", "oinst = folds\n", "cv_groups = cross_validated(X, folds)\n", "\n", "n_x = X.shape[1] # Features\n", "final_gain = []\n", "all_gain = []\n", "alphas = []\n", "for a in range(instances):\n", "    alpha = alph[a]+(np.random.random_sample()-0.5)*0.0001\n", "    # Setup Hidden Layer\n", "    N_21 = [Neuron(n_x,alpha,3) for j in range(oinst)]\n", "    N_22 = [Neuron(n_x,alpha,3) for j in range(oinst)]\n", "    # Setup output layer\n", "    N_g = [Neuron(2,alpha,2) for j in range(oinst)]\n", "\n", "    for j in range(oinst):\n", "        alpha = alph[a]+(np.random.random_sample()-0.5)*0.00001\n", "        N_g[j].alpha = alpha\n", "        N_21[j].alpha = alpha\n", "        N_22[j].alpha = alpha\n", "        alphas.append(alpha)\n", "        # Prepare Training and testing sets \n", "        X_train = X[cv_groups[j][1],:] \n", "        y_train = np.reshape(y[cv_groups[j][1],0],(-1,1)) \n", "        X_test = X[cv_groups[j][0],:] \n", "        y_test = np.reshape(y[cv_groups[j][0],0],(-1,1))\n", "    \n", "        for i in range(iterations):\n", "            # Evaluate\n", "            a_21 = N_21[j].forward(X_train) \n", "            a_22 = N_22[j].forward(X_train)  \n", "            a_2 = np.concatenate((a_21,a_22),axis=1)\n", "            a_g = N_g[j].forward(a_2)\n", "        \n", "            # Learn\n", "            grad = N_g[j].gradient(a_g,a_g - y_train)\n", "            N_g[j].backward(a_2,a_g - y_train)\n", "        \n", "            N_21[j].backward(X_train,grad*N_g[j].w[0])\n", "            N_22[j].backward(X_train,grad*N_g[j].w[1])\n", "    \n", "    \n", "        # Finally Evaluate Model \n", "        a_21 = N_21[j].forward(X_test) \n", "        a_22 = N_22[j].forward(X_test)  \n", "        a_2 = np.concatenate((a_21,a_22),axis=1)\n", "        a_g = N_g[j].forward(a_2)\n", "        print(\"Testing Sample:[alpha ={}]  Success = {}\".format(N_g[j].alpha,1-ssd(a_g,y_test)))\n", "        #plt.scatter(a_g, y_test)\n", "        #plt.show()\n", "        a_g = (a_g>0.5)*1\n", "        m = y_test.shape[0]\n", "        final_gain.append(1-ssd(a_g,y_test))\n", "\n", "\n", "        # Evaluate All\n", "        a_21 = N_21[j].forward(X) \n", "        a_22 = N_22[j].forward(X)  \n", "        a_2 = np.concatenate((a_21,a_22),axis=1)\n", "        a_g = N_g[j].forward(a_2)\n", "        #plt.scatter(a_g, y)\n", "        #plt.show()\n", "        a_g = (a_g>0.5)*1\n", "        m = y.shape[0]\n", "        all_gain.append(1-ssd(a_g,y))\n", "        print(\"Testing All:[alpha ={}]  Success = {}\".format(N_g[j].alpha,1-ssd(a_g,y)))\n", "        #plt.plot(cost_g)\n", "        #plt.title(\"Cost over time\")\n", "        #plt.show()\n", "\n", "\n", "final_gain= np.reshape((final_gain),(-1,1)).tolist()         \n", "all_gain= np.reshape((all_gain),(-1,1)).tolist()   \n", "plt.scatter(alphas,final_gain)\n", "plt.scatter(alphas,all_gain)\n", "plt.show()\n"], "metadata": {"_cell_guid": "b3043c13-2730-4f1b-9b53-f2e5d72269bf", "_uuid": "635f78693cad6c181ce55c790dfa30ecef908c66"}, "outputs": [], "execution_count": null, "cell_type": "code"}, {"source": ["## Results - Hidden Layer of 1 TANH and 1 ReLU and Output is Gaussian\n", "\n", "### 7 Fold Expirements \n", "For 7 folds, training on one fold and testing on the rest \n", "\n", "Epochs | optimal $\\alpha$ (range) | estimate score (range)\n", "-----------|----------------------------------|-------------------------------\n", "5000     | 0.065 ~ 0.085                  |  0.76 ~ 0.82\n", "3000     |  0.175 ~ 0.2                     |  0.75 ~ 0.83\n", "1000     |  0.38  ~ 0.45                    |  0.77 ~ 0.83\n", "500       |  0.745 ~ 0.765                 |  0.75 ~ 0.82\n", "100       |  0.45 ~ 1.0                       |  0.55 ~ 0.8\n", "\n", "\n", "## Results - Hidden Layer of 2 TANH and Output is Gaussian\n", "\n", "### 3 Fold Expirements \n", "For 3 folds, training on one fold and testing on the rest \n", "\n", "Epochs | optimal $\\alpha$ (range) | estimate score (range)\n", "-----------|----------------------------------|--------------------------\n", "1000     |  0.25 ~ 0.325                   |  0.78 ~ 0.81\n", "2000     |  0.055 ~ 0.07                   |  0.77 ~ 0.81\n", "3000     |  0.04 ~ 0.1                       |  0.78 ~ 0.8\n", "5000     |   0.02 ~ 0.04                    |  0.78 ~ 0.8"], "metadata": {"_cell_guid": "6ff75687-9cf6-4143-a890-327d4b00b5e9", "_uuid": "f6de9a11aa501b8489d72a0343155ed4641c226f"}, "cell_type": "markdown"}, {"source": [" "], "metadata": {"collapsed": true, "_cell_guid": "b66236bd-1ab8-4da8-8508-4a6831e0d693", "_uuid": "5c0e7274e25c186e794fdc0909c1f59ff5921c93"}, "outputs": [], "execution_count": null, "cell_type": "code"}], "nbformat_minor": 1, "metadata": {"kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}, "language_info": {"codemirror_mode": {"version": 3, "name": "ipython"}, "file_extension": ".py", "version": "3.6.4", "mimetype": "text/x-python", "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "name": "python"}}, "nbformat": 4}