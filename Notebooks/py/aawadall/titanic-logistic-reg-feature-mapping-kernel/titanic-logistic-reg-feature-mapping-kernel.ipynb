{"nbformat_minor": 1, "cells": [{"cell_type": "markdown", "source": ["# Plan\n", "1. Load Data\n", "1. Data Insights\n", "1. Preprocess Data\n", "1. Build Models\n", "1. Train Models\n", "1. Test Models\n", "1. Evaluate Models"], "metadata": {"_uuid": "bfc53bb96498f02c109381dcff19946511098ac5", "_cell_guid": "6f7746a0-ee97-4a77-b41c-64a823067376"}}, {"cell_type": "code", "metadata": {"_uuid": "d2c05eaeadddbcd0750105be59d1f2258659d6cc", "_cell_guid": "df506eb3-c787-4f68-a7d7-4de6bbb20355"}, "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "import matplotlib.pyplot as plt # Visualization\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "\n", "# Any results you write to the current directory are saved as output."], "outputs": [], "execution_count": 1}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "ef7ae2a569fd1d9a756c9036930b4b3528f800ed", "_cell_guid": "b66922f9-e5cd-4007-abe2-c227e2d99438"}, "source": ["# Load Data\n", "train_raw = pd.read_csv(\"../input/train.csv\")\n", "test_raw = pd.read_csv(\"../input/test.csv\")\n", "gs_raw = pd.read_csv(\"../input/gender_submission.csv\")"], "outputs": [], "execution_count": 2}, {"cell_type": "code", "metadata": {"_uuid": "ea7b68e4e0f9f31b0ae336926251a3e2823676ca", "_cell_guid": "74e6619b-3e61-4877-b19a-94339d540ff8"}, "source": ["# Data Insights - Training\n", "train_raw.describe(include=\"all\")\n"], "outputs": [], "execution_count": 3}, {"cell_type": "code", "metadata": {"_uuid": "3518af41c1ab9f6d28b166154c646448475673bb", "_cell_guid": "c32cd72d-dc8f-431e-ba26-3ad15e0aa85b"}, "source": ["print(np.nan_to_num(train_raw['Cabin']))"], "outputs": [], "execution_count": 4}, {"cell_type": "code", "metadata": {"_uuid": "003b5027f1fb41940f7774bcd4a484b7c729a7d1", "_cell_guid": "51a3e998-b2fa-4348-8213-ca6b63ca4ad2"}, "source": ["train_raw.dtypes"], "outputs": [], "execution_count": 5}, {"cell_type": "markdown", "source": ["# Training Data Insigts\n", "* $m = 891$\n", "* $n_x = 10$ explicit features\n", "\n", "Data Types\n", "\n", "Field              |   Datatype \n", "------------------|---------------\n", "PassengerId |    int64\n", "Survived       |  int64\n", "Pclass          | int64\n", "Name           | object\n", "Sex              | object\n", "Age              |float64\n", "SibSp           | int64\n", "Parch           | int64\n", "Ticket          |object\n", "Fare           |float64\n", "Cabin          | object\n", "Embarked    |    object\n"], "metadata": {"_uuid": "c791ac3ff7bf1e132ae4b6507143b4dba15e83e1", "_cell_guid": "47e32e03-6f69-4038-b5c6-932d358e2ab7"}}, {"cell_type": "code", "metadata": {"_uuid": "3f9904c9890344b6eff63fdfdf870ebace2d3179", "_cell_guid": "e715b466-0de2-4083-a05f-af91b678cfbc"}, "source": ["\n", "print(train_raw['Pclass'].T)\n", "print(train_raw['Cabin'].T)"], "outputs": [], "execution_count": 6}, {"cell_type": "code", "metadata": {"_uuid": "09d6d812e718c0b5d186d5ce8670148045e1f687", "_cell_guid": "4a187e3e-5b30-4a73-b09e-d1c4499d6b3a"}, "source": ["# Data Insigts (Testing)\n", "test_raw.describe(include=\"all\")"], "outputs": [], "execution_count": 7}, {"cell_type": "code", "metadata": {"_uuid": "a1477b8a8b45ae0c85ec343756ca409438238a8e", "_cell_guid": "5dc5001f-37ac-4e9c-9fe9-012913c9f84f"}, "source": ["# Data Insigts (Gender Submission)\n", "gs_raw.describe(include=\"all\")\n"], "outputs": [], "execution_count": 8}, {"cell_type": "markdown", "source": ["# Preprocess Data\n", "Find optimal feature mapping function $\\mathcal{M} : \\phi \\mapsto \\phi'$ such that $\\forall \\phi'_i \\in \\phi', \\phi'_i \\in \\mathbb{R}$, and $\\mathbb{E}(\\phi'_i) \\approx 0$\n", "\n", "with loose constraint of $ \\phi'_i \\in [-1, 1]$ \n", "\n", "Of course this will depend on feature data type, moreover, some NaN values when found will be substituted by 0 \n", "\n", "So I propose defining the following mapping function\n", "\n", "## Pclass\n", "Since this is an integer value $\\phi_{Pclass} = \\phi_0 \\in  \\{1,2,3  \\} \\in \\mathbb{Z}$, this mapping function will be ~~identity mapping ~~ normalized\n", "i.e. $ \\mathcal{M}_0 \\colon \\phi_0 \\mapsto \\phi_0 - 2$ \n", "\n", "## Name            \n", "Although passenger name is a rich feature, containing multidimensional data, including ethnicity, sex, and social class, I will bypass it in this iteration of work and have our mapping function map to a 0\n", "\n", "$ \\mathcal{M}_1 \\colon \\phi_1 \\mapsto 0$ \n", "\n", "## Sex             \n", "I will arbitrarily assign 1 and -1 to either sex, say $ \\mathcal{M}_2  \\colon female \\mapsto 1$ and $ \\mathcal{M}_2  \\colon male \\mapsto -1$   \n", "\n", "## Age            \n", "Age $\\phi_{age} = \\phi_3 \\in \\mathbb{R}^{+} \\sim \\mathcal{N}(30,14)$ \n", "\n", "hence $ \\mathcal{M}_3  \\colon \\phi_3 \\mapsto \\frac{\\phi_3 - 30}{14}$\n", "\n", "both $\\mu$ and $\\sigma$ are approximate values\n", "\n", "## SibSp            \n", "Similar to Pclass, this feature will ~~have identity mapping~~ be normalized\n", "\n", "i.e. $ \\mathcal{M}_4 \\colon \\phi_4 \\mapsto \\phi_4 - 3$ \n", "## Parch   \n", "Similar to Pclass, this feature will ~~have identity mapping~~ be normalized\n", "\n", "i.e. $ \\mathcal{M}_5 \\colon \\phi_5 \\mapsto \\phi_5 - 2$ \n", "## Ticket          \n", "Similar to name, ticket might hide some useful implicit features, but I will ignore it for simplicity \n", "$ \\mathcal{M}_6 \\colon \\phi_6 \\mapsto 0$ \n", "\n", "## Fare           \n", "Similar to age, Fare $\\phi_{fare} = \\phi_7 \\in \\mathbb{R}^{+} \\sim \\mathcal{N}(33,52)$ \n", "hence $ \\mathcal{M}_7  \\colon \\phi_7 \\mapsto \\frac{\\phi_7 - 33}{52}$\n", "both $\\mu$ and $\\sigma$ are approximate values\n", "\n", "## Cabin           \n", "Cabin will be defferred, hence $ \\mathcal{M}_8 \\colon \\phi_8 \\mapsto 0$ \n", "Experiment: I will create a binary feature vector of  8 features, each one corresponds to the presence of one of the following letters in cabin $\\{ A, B, C, D, E, F, G, T\\}$\n", "\n", "\n", "## Embarked        \n", "Embarked $\\phi_9 \\in \\{C, Q, S\\}$, I will use a simple trinary mapping function $ \\mathcal{M}_9 \\colon C \\mapsto -1$, $ \\mathcal{M}_9 \\colon Q \\mapsto 0$, and  $ \\mathcal{M}_9 \\colon S \\mapsto 1$ \n"], "metadata": {"_uuid": "e2c3ea71c75839fc979e7f532f1aec208485a397", "_cell_guid": "cbad3504-84b9-4e86-90ba-25e7f5e38a60"}}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "ca50f417340548ff8c73b0b6db77eb95d4451990", "_cell_guid": "a6a476b9-81b1-40a6-9842-3bdc8d118318"}, "source": ["# Mapping function Extracting PassengerID, Survival, Feature vector phi_prime \n", "def get_features(X):\n", "    # PassengerId\n", "    PassID = X['PassengerId']\n", "    m = PassID.shape[0]\n", "    nx = 20 # Number of features, this is hardcoded for the timebeing\n", "    # Add two more columns for embarked, and one more for sex\n", "    # Add 7 more for cabin\n", "    # Features \n", "    X_prime = np.zeros((m,nx))\n", "    # 0 Pclass\n", "    X_prime[:,0] = X['Pclass'] - 2\n", "    # 1 Name           \n", "    # 2,3 Sex\n", "    X_prime[:,2] = 1* (X['Sex'] == 'female')\n", "    X_prime[:,3] = 1* (X['Sex'] == 'male')\n", "    \n", "    # 4 Age           \n", "    mu = 30\n", "    sigma = 14 \n", "    X_prime[:,4] = (X['Age'] - mu ) / sigma\n", "    # 5 SibSp          \n", "    X_prime[:,5] = X['SibSp'] - 3\n", "    # 6 Parch     \n", "    X_prime[:,6] = X['Parch'] - 2\n", "    # 7 Ticket         \n", "    # 8 Fare    \n", "    mu = 33\n", "    sigma = 52 \n", "    X_prime[:,8] = (X['Fare'] - mu ) / sigma\n", "    # 9 Cabin   \n", "    X_prime[:,9] = X['Cabin'].str.contains('A')*1\n", "    X_prime[:,10] = X['Cabin'].str.contains('B')*1\n", "    X_prime[:,11] = X['Cabin'].str.contains('C')*1\n", "    X_prime[:,12] = X['Cabin'].str.contains('D')*1\n", "    X_prime[:,13] = X['Cabin'].str.contains('E')*1\n", "    X_prime[:,14] = X['Cabin'].str.contains('F')*1\n", "    X_prime[:,15] = X['Cabin'].str.contains('G')*1\n", "    X_prime[:,16] = X['Cabin'].str.contains('T')*1\n", "    # Extend Cabin to pick all possible letters, and the numeric value \n", "    # 17,18,19 Embarked  {C,Q,S}\n", "    selector = X['Embarked'] == 'C'\n", "    X_prime[:,17] = selector * 1 \n", "\n", "    selector = X['Embarked'] == 'Q'\n", "    X_prime[:,18] = selector * 1 \n", "\n", "    selector = X['Embarked'] == 'S'\n", "    X_prime[:,19] = selector *1\n", "    return PassID, np.nan_to_num(X_prime)"], "outputs": [], "execution_count": 9}, {"cell_type": "code", "metadata": {"_uuid": "4d849ebd619846d03eb1d45eb889c3c936112490", "_cell_guid": "638ff5f3-2f0d-43b3-9f68-4b93bd669304"}, "source": ["PassID, X = get_features(train_raw)\n", "\n", "# Survived\n", "y = np.array(train_raw['Survived'])\n", "y = np.reshape(y,(-1,1))\n", "\n", "print(X)\n", "plt.plot(X)\n", "plt.show()"], "outputs": [], "execution_count": 10}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "dcebdda4264bbab105c1885b67c8d0b18620e491", "_cell_guid": "7ef8d818-86ea-47e8-9e87-cb444b078196"}, "source": ["#print(np.char.count(np.array(train_raw['Cabin']),'A'))\n", "#print(train_raw['Cabin'].str.contains('A')*1)"], "outputs": [], "execution_count": 11}, {"cell_type": "markdown", "source": ["# Build Models\n", "I propose the following models\n", "* Logistic Regression \n", "* Neural Network\n", "* Support Vector Machine\n"], "metadata": {"_uuid": "7d0cfe6171ff14163d8daa9011ce75c7b6613ebc", "_cell_guid": "80416d2f-0375-4a16-92ff-a05169c04735"}}, {"cell_type": "markdown", "source": ["## Logistic Regresion \n", "Using simple sigmoid activation function $\\sigma(z) = \\frac{1}{1+e^{-z}}$\n", "\n", "where $z = \\omega^T X + b$\n", "\n", "Cost function used here will be $\\mathcal{L}(\\omega,b,x,y) = -y log(\\sigma(b+\\omega^TX))+(1-y)log(1-\\sigma(b+\\omega^TX))$\n", "\n", "Gradient descent will conveniently use $\\frac{\\partial}{\\partial \\omega} \\mathbf{J} = \\mathbf{X}^T . (\\hat{y} - y)$ and $\\frac{\\partial}{\\partial b} \\mathbf{J} = \\sum_i^m \\hat{y}^{(i)} - y^{(i)}$\n"], "metadata": {"_uuid": "9c7f411874e5da4ba86a84ff1de300f4153d631c", "_cell_guid": "c2f5e9ee-1ada-437e-a75f-4fc04ca9c1c1"}}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "164979483b7850832359c94c3b89311f62383887", "_cell_guid": "ce904c94-6d6e-4b87-8ac2-9c880838e247"}, "source": ["# Initialize Model\n", "def init(X):\n", "    w = np.zeros((X.shape[1],1))\n", "    b = 0.0\n", "    return w, b\n", "\n", "# Sigmoid function \n", "def sigmoid(z):\n", "    return 1/(1+np.exp(-z))\n", "\n", "# Predict \n", "def predict(X, w, b):\n", "    return sigmoid(np.dot(X, w)+b)\n", "\n", "# Cost function\n", "def lcost(X, w, b, y):\n", "    m = y.shape[0]\n", "    y_hat = predict(X, w, b)\n", "    return -1/m*np.sum(y*np.log(y_hat)+(1-y)*np.log(1-y_hat))\n", "\n", "# gradients\n", "def gradient(X, y_hat, y):\n", "    \n", "    db = np.sum(y_hat - y)\n", "    dw = np.dot(X.T,y_hat-y)\n", "    \n", "    return db, dw\n", "\n", "# Learn \n", "def learn(X, w, b, y, alpha):\n", "    y_hat = predict(X, w, b)\n", "    db, dw = gradient(X, y_hat, y)\n", "    w -= alpha * dw \n", "    b -= alpha * db\n", "    return w, b"], "outputs": [], "execution_count": 12}, {"cell_type": "code", "metadata": {"_uuid": "163b249289fa36466f3982db557f278809047bc7", "_cell_guid": "59135628-3c78-4bbe-a46b-319c3cea2e09"}, "source": ["# Test Model\n", "alpha = 0.001 # Learning Rate\n", "w, b = init(X)\n", "\n", "y_hat = predict(X, w, b)\n", "\n", "\n", "plt.plot(y_hat)\n", "plt.show()\n", "\n", "j = []\n", "\n", "for i in range(2000):\n", "    w, b = learn(X, w, b, y, alpha)\n", "    j.append(lcost(X, w, b, y))\n", "    \n", "y_hat = predict(X, w, b)\n", "plt.plot(j)\n", "plt.show()\n", "\n", "\n", "m = y.shape[0]\n", "measure_p = (y_hat > 0.5) == y\n", "measure_n = (y_hat <= 0.5) != y\n", "print(\"Training: Positive Success = {}, Negative Success = {}\".format(np.sum(measure_p)/m,np.sum(measure_n)/m))\n", "\n", "\n", "plt.scatter( y_hat, y)\n", "plt.show()\n", "\n"], "outputs": [], "execution_count": 13}, {"cell_type": "markdown", "source": ["Attempt to create a kernel of features $\\kappa(\\phi')$ to find deeper insights, one idea is to create a new feature vector $\\kappa_r(\\phi') = (\\phi' \\phi'^2 ...\\phi'^r) $\n"], "metadata": {"_uuid": "17bb46c2cbf7483ddeb375e4666a20af51549897", "_cell_guid": "fa5afb29-5150-47f9-85e0-c13ea6e77e8b"}}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "f49fa809e39767eb9d8e301822b1981c6be78f9b", "_cell_guid": "fc85a60b-3a57-4ccb-8142-3b099a395fdd"}, "source": ["def kernel(X):\n", "    K_x = np.concatenate((X, X**2,X**3,  np.exp(-X)), axis=1)\n", "    return K_x"], "outputs": [], "execution_count": 14}, {"cell_type": "code", "metadata": {"_uuid": "eadfc7a5703b46d4885e035db4cc525491e14cdd", "_cell_guid": "22536707-7ad1-4adc-bb2b-377b20d43c2c"}, "source": ["X2 = kernel(X)\n", "# Test Model\n", "alpha = 0.00001 # Learning Rate\n", "w, b = init(X2)\n", "\n", "j = []\n", "\n", "for i in range(1500):\n", "    w, b = learn(X2, w, b, y, alpha)\n", "    j.append(lcost(X2, w, b, y))\n", "    \n", "y_hat = predict(X2, w, b)\n", "plt.plot(j)\n", "plt.show()\n", "\n", "plt.scatter( y_hat, y)\n", "plt.show()\n", "\n", "m = y.shape[0]\n", "measure_p = (y_hat > 0.5) == y\n", "measure_n = (y_hat <= 0.5) != y\n", "print(\"Training: Positive Success = {}, Negative Success = {}\".format(np.sum(measure_p)/m,np.sum(measure_n)/m))\n", "\n", "# Validate for test set \n", "PassID, X = get_features(test_raw)\n", "X2 = kernel(X)\n", "\n", "# Assuming sorted lists - Survived\n", "y = np.array(gs_raw['Survived'])\n", "y = np.reshape(y,(-1,1))\n", "\n", "y_hat = predict(X2, w, b)\n", "m = y.shape[0]\n", "measure_p = (y_hat > 0.5) == y\n", "measure_n = (y_hat <= 0.5) != y\n", "print(\"Testing: Positive Success = {}, Negative Success = {}\".format(np.sum(measure_p)/m,np.sum(measure_n)/m))\n"], "outputs": [], "execution_count": 15}, {"cell_type": "markdown", "source": ["~~At this stage no need to test other methods, as it seems LR is sufficient to identify survival data at this rate 96.17%~~\n", "\n", "Score was 77.99%\n", "maybe I should build a better kernel function to extract more features from selected feature vector\n", "also, I might work on extracting features from censored features i.e. Name, Ticket, and Cabin\n"], "metadata": {"_uuid": "4f77fa7836ea8582aeae6e6c4285ff645c4109df", "_cell_guid": "5fbe4af6-eeed-4432-baf5-3204f006d86c"}}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "dbb0cff8ef2b2c15efdfc1884ce67c9de22a5e04", "_cell_guid": "e4e6c068-971f-4165-ad54-5da819a98e1e", "scrolled": true}, "source": ["# Prepare Data for submission\n", "data = y_hat > 0.5\n", "data = data*1\n", "s0 = pd.Series(PassID, index=PassID)\n", "s1 = pd.Series(data[:,0], index=PassID)\n", "\n", "df = pd.DataFrame(data = s1,index = PassID)\n", "df.columns = ['Survived']\n", "df.to_csv('lr_submission.csv', sep=',')"], "outputs": [], "execution_count": 16}, {"cell_type": "markdown", "source": ["## Neural Network\n", "it seems that Logistic regression is stuck at 77.99%, despite non-linearity simulated by my kernel function.\n", "Let me try a new method, using neural network with different activation functions in the hidden layers.\n", "\n", "### Method\n", "I will simulate each neuron behaviour as follows\n", "$$ \\nu_\\psi(\\mathbf{x})  = \\psi(\\mathbf{\\omega}^T \\mathbf{x} + b)$$\n", "where $\\mathbf{x}$ is input feature vector, $\\psi$ is an activation function, $\\mathbf{\\omega}$ is weights vector, and $b$ is bias \n", "\n", "Learning will be done using normal back propagation, since this will be a shallow neural network and will be as follows \n", "\n", "$$  \\mathbf{\\omega} \\gets \\mathbf{\\omega} - \\alpha \\frac{\\partial}{\\partial \\mathbf{\\omega}} \\mathbf{J}_\\psi(\\mathbf{x},y;\\mathbf{\\omega},b) \\\\\n", "b \\gets b - \\alpha \\frac{\\partial}{\\partial b} \\mathbf{J}_\\psi(\\mathbf{x},y;\\mathbf{\\omega},b) $$\n", "\n", "where $\\mathbf{J}_\\psi(\\mathbf{x},y;\\mathbf{\\omega},b)$ is an objective function to minimize \n", "\n", "### Activation functions\n", "I will start by three simple functions \n", "#### Sigmoid \n", "$$ \\psi_\\sigma(\\mathbf{z}) = \\frac{1}{1 + e^{-\\mathbf{z}}}$$\n", "$$ \\frac{\\partial}{\\partial \\mathbf{z}} \\psi_\\sigma(\\mathbf{z}) = \\psi_\\sigma(\\mathbf{z})(1-\\psi_\\sigma(\\mathbf{z}))$$\n", "$$ \\frac{\\partial}{\\partial \\mathbf{\\omega}} \\psi_\\sigma(\\mathbf{z}) = \\psi_\\sigma(\\mathbf{z})(1-\\psi_\\sigma(\\mathbf{z})) \\mathbf{x}^T \\\\\n", "\\frac{\\partial}{\\partial b} \\psi_\\sigma(\\mathbf{z}) = \\psi_\\sigma(\\mathbf{z})(1-\\psi_\\sigma(\\mathbf{z}))$$\n", "#### Hyperbolic TAN (tanh)\n", "$$ \\psi_{\\tanh}(\\mathbf{z}) = \\frac{2}{1 + e^{-2\\mathbf{z}}} - 1 $$\n", "$$ \\frac{\\partial}{\\partial \\mathbf{z}} \\psi_{\\tanh}(\\mathbf{z}) =1-\\psi_{\\tanh}(\\mathbf{z})^2 $$\n", "$$ \\frac{\\partial}{\\partial \\mathbf{w}} \\psi_{\\tanh}(\\mathbf{z}) =1-\\psi_{\\tanh}(\\mathbf{z})^2 \\mathbf{x}^T$$\n", "$$ \\frac{\\partial}{\\partial b} \\psi_{\\tanh}(\\mathbf{z}) =1-\\psi_{\\tanh}(\\mathbf{z})^2 $$\n", "#### Rectified Linear Unit (ReLU)\n", "$$ \\psi_{ReLU}(\\mathbf{z}) = \\left\\{\n", "                \\begin{array}{ll}\n", "                  \\mathbf{z}\\  |\\  \\mathbf{z} > 0\\\\\n", "                  0\\ |\\ else\n", "                \\end{array}\n", "              \\right. $$\n", "$$ \\frac{\\partial}{\\partial \\mathbf{z}} \\psi_{ReLU}(\\mathbf{z}) = \\left\\{\n", "                \\begin{array}{ll}\n", "                  1  |\\  \\mathbf{z} > 0\\\\\n", "                  0\\ |\\ else\n", "                \\end{array}\n", "              \\right. $$\n", "  $$ \\frac{\\partial}{\\partial \\mathbf{\\omega}} \\psi_{ReLU}(\\mathbf{z}) = \\left\\{\n", "                \\begin{array}{ll}\n", "                  \\mathbf{x}  |\\  \\mathbf{z} > 0\\\\\n", "                  0\\ |\\ else\n", "                \\end{array}\n", "              \\right. $$\n", "$$ \\frac{\\partial}{\\partial b} \\psi_{ReLU}(\\mathbf{z}) = \\left\\{\n", "                \\begin{array}{ll}\n", "                  1  |\\  \\mathbf{z} > 0\\\\\n", "                  0\\ |\\ else\n", "                \\end{array}\n", "              \\right. $$\n"], "metadata": {"collapsed": true, "_uuid": "563955058121c60cb5b2eef878990f08ff46cf4e", "_cell_guid": "6664f5e0-acd6-4733-884a-688c73588846"}}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "b663ba0d9370f1921433990de8a011c623a39936", "_cell_guid": "13b70109-3599-4fb0-91df-b5e6d984132b"}, "source": ["# Neuron \n", "class Neuron:\n", "    def __init__(self, feature_size, alpha, act=2):\n", "        self.w = np.random.random_sample((feature_size,1))*0.1\n", "        self.b = 0.0\n", "        self.alpha = alpha\n", "        self.activation_function = act\n", "    def activate(self, z):\n", "        # based on set activation function set make the calculation\n", "        if self.activation_function == 2: # Sigmoid\n", "            return 1 / (1 + np.exp(-z))\n", "        if self.activation_function == 3: # tanh\n", "            return 2 / (1 + np.exp(-z)) - 1\n", "        if self.activation_function == 7: # ReLU\n", "            return (z>0)*z\n", "        if self.activation_function == 21: # Gaussian\n", "            return np.exp(-z**2)\n", "    def gradient(self, y_hat, err):\n", "        \n", "        m = y_hat.shape[0] # Sample size\n", "        #da = y/y_hat + (1-y)/(1-y_hat)\n", "        # based on set activation function set make the calculation\n", "        if self.activation_function == 2: # Sigmoid\n", "            return  err*y_hat*(1-y_hat)\n", "        if self.activation_function == 3: # tanh\n", "            return err*(1-y_hat**2)\n", "        if self.activation_function == 7: # ReLU\n", "            return (y_hat>0)*err\n", "        if self.activation_function == 21: # Gaussian\n", "            return -2*y_hat*np.exp(-y_hat**2)*err\n", "\n", "    def forward(self, X):\n", "        # given feature vector X, find activation response\n", "        z = np.dot(X,self.w)+self.b\n", "        a = self.activate(z)\n", "        return a\n", "    def backward(self, X, err):\n", "        m = y.shape[0]\n", "        y_hat = self.forward(X)\n", "        grad = self.gradient(y_hat,err)\n", "        self.b -= self.alpha * np.sum(grad)/m\n", "        self.w -= self.alpha * np.dot(X.T,grad)/m\n", "    def print_params(self):\n", "        print(\"b = {}, w = {}\".format(self.b,self.omega.T))"], "outputs": [], "execution_count": 17}, {"cell_type": "markdown", "source": ["## Test a single neuron with different activation function"], "metadata": {"_uuid": "7b7170b3591884990d3f7e23e57a05a89490ea42", "_cell_guid": "119276f2-4652-4005-9acd-1307e77ee2c1"}}, {"cell_type": "code", "metadata": {"_uuid": "6b0b9b74e5cb876efbc362231b0185dcb1c99cbb", "_cell_guid": "5f17ae1a-c9eb-4c4a-b524-8f5324d59026"}, "source": ["PassID, X = get_features(train_raw)\n", "X2 = kernel(X)\n", "n_x = X2.shape[1] # Features\n", "alpha = 0.0014\n", "# Survived\n", "y = np.array(train_raw['Survived'])\n", "y = np.reshape(y,(-1,1))\n", "\n", "instances = 4\n", "# Setup single neurons\n", "N_s = [Neuron(n_x,alpha,2) for j in range(instances)]  # Sigmoid Activation\n", "N_t = [Neuron(n_x,alpha,3) for j in range(instances)]  # tanh Activation\n", "N_r = [Neuron(n_x,alpha,7) for j in range(instances)]  # ReLU Activation\n", "N_g = [Neuron(n_x,alpha,21) for j in range(instances)]  # Gaussian Activation\n", "\n", "cost_s = []\n", "cost_t = []\n", "cost_r = []\n", "cost_g = []\n", "\n", "for i in range(1000):\n", "    c_s = []\n", "    c_t = []\n", "    c_r = []\n", "    c_g = []\n", "    for j in range(instances):\n", "        # Evaluate \n", "        a_s = N_s[j].forward(X2)\n", "        a_t = N_t[j].forward(X2)\n", "        a_r = N_r[j].forward(X2)\n", "        a_g = N_g[j].forward(X2)\n", "        \n", "        # Learn\n", "        N_s[j].backward(X2,a_s - y)\n", "        N_t[j].backward(X2,a_t - y)\n", "        N_r[j].backward(X2,a_r - y)\n", "        N_g[j].backward(X2,a_g - y)\n", "        \n", "        # Evaluate\n", "        a_s = N_s[j].forward(X2)\n", "        a_t = N_t[j].forward(X2)\n", "        a_r = N_r[j].forward(X2)\n", "        a_g = N_g[j].forward(X2)\n", "        \n", "        # Performance vote\n", "        c_s.append(np.sum(np.abs(a_s-y))/y.shape[0])\n", "        c_t.append(np.sum(np.abs(a_t-y))/y.shape[0])    \n", "        c_r.append(np.sum(np.abs(a_r-y))/y.shape[0])\n", "        c_g.append(np.sum(np.abs(a_g-y))/y.shape[0])\n", "    \n", "    cost_s.append(np.mean(c_s))\n", "    cost_t.append(np.mean(c_t))    \n", "    cost_r.append(np.mean(c_r))\n", "    cost_g.append(np.mean(c_g))\n", "\n", "\n", "plt.plot(cost_s)\n", "plt.title(\"Average Sigmoid Performance\")\n", "plt.show()\n", "plt.plot(cost_t)\n", "plt.title(\"Average tanh Performance\")\n", "plt.show()\n", "plt.plot(cost_r)\n", "plt.title(\"Average ReLU Performance\")\n", "plt.show()\n", "plt.plot(cost_g)\n", "plt.title(\"Average Gaussian Performance\")\n", "plt.show()"], "outputs": [], "execution_count": 18}, {"cell_type": "markdown", "source": ["## Test Neurons with simple features\n", "test each activation type with original feature vector (no kernel)"], "metadata": {"_uuid": "91a85aebb1bffac33c1f0fb0ba69fd4b30bb6bd4", "_cell_guid": "c65c5b9d-b842-4d9a-9dd4-388be847fb00"}}, {"cell_type": "code", "metadata": {"_uuid": "44356a503beec7b82e036aefb9f639f8c92823d3", "_cell_guid": "d2f57571-c29f-43d7-b477-1e6ca21a4c5f"}, "source": ["PassID, X = get_features(train_raw)\n", "X2 = X\n", "n_x = X2.shape[1] # Features\n", "alpha = 0.008\n", "# Survived\n", "y = np.array(train_raw['Survived'])\n", "y = np.reshape(y,(-1,1))\n", "\n", "instances = 4\n", "# Setup single neurons\n", "N_s = [Neuron(n_x,alpha,2) for j in range(instances)]  # Sigmoid Activation\n", "N_t = [Neuron(n_x,alpha,3) for j in range(instances)]  # tanh Activation\n", "N_r = [Neuron(n_x,alpha,7) for j in range(instances)]  # ReLU Activation\n", "N_g = [Neuron(n_x,alpha,21) for j in range(instances)]  # Gaussian Activation\n", "\n", "cost_s = []\n", "cost_t = []\n", "cost_r = []\n", "cost_g = []\n", "\n", "for i in range(10000):\n", "    c_s = []\n", "    c_t = []\n", "    c_r = []\n", "    c_g = []\n", "    for j in range(instances):\n", "        # Evaluate\n", "        a_s = N_s[j].forward(X2)\n", "        a_t = N_t[j].forward(X2)\n", "        a_r = N_r[j].forward(X2)\n", "        a_g = N_g[j].forward(X2)\n", "        \n", "        # Learn\n", "        N_s[j].backward(X2,a_s - y)\n", "        N_t[j].backward(X2,a_t - y)\n", "        N_r[j].backward(X2,a_r - y)\n", "        N_g[j].backward(X2,a_g - y)\n", "        \n", "        # Evaluate\n", "        a_s = N_s[j].forward(X2)\n", "        a_t = N_t[j].forward(X2)\n", "        a_r = N_r[j].forward(X2)\n", "        a_g = N_g[j].forward(X2)\n", "        \n", "        # Performance vote\n", "        c_s.append(np.sum(np.abs(a_s-y))/y.shape[0])\n", "        c_t.append(np.sum(np.abs(a_t-y))/y.shape[0])    \n", "        c_r.append(np.sum(np.abs(a_r-y))/y.shape[0])\n", "        c_g.append(np.sum(np.abs(a_g-y))/y.shape[0])\n", "    \n", "    cost_s.append(np.mean(c_s))\n", "    cost_t.append(np.mean(c_t))    \n", "    cost_r.append(np.mean(c_r))\n", "    cost_g.append(np.mean(c_g))\n", "\n", "\n", "plt.plot(cost_s)\n", "plt.title(\"Average Sigmoid Performance\")\n", "plt.show()\n", "plt.plot(cost_t)\n", "plt.title(\"Average tanh Performance\")\n", "plt.show()\n", "plt.plot(cost_r)\n", "plt.title(\"Average ReLU Performance\")\n", "plt.show()\n", "plt.plot(cost_g)\n", "plt.title(\"Average Gaussian Performance\")\n", "plt.show()"], "outputs": [], "execution_count": 19}, {"cell_type": "markdown", "source": ["## Testing a network of neurons \n", "Build a Simple Neural Network with one hidden layer of 2 neurons and one output layer of a single neuron\n", "with one of the hidden neurons with tanh activation function, while the other with ReLU"], "metadata": {"_uuid": "43970ff4340bd25005645ea08657e7ac2fe232ee", "_cell_guid": "19a3e535-ecbb-4a5d-aee5-9c4169538cad"}}, {"cell_type": "code", "metadata": {"_uuid": "02686994d7d50801b5eb0d11b3227828f0ae006f", "_cell_guid": "cd810f66-a603-41f5-9432-3846b6623a86"}, "source": ["PassID, X = get_features(train_raw)\n", "X2 = X\n", "n_x = X2.shape[1] # Features\n", "alpha = 0.02\n", "# Survived\n", "y = np.array(train_raw['Survived'])\n", "y = np.reshape(y,(-1,1))\n", "iterations = 1000\n", "# Setup Layers\n", "N_21 = Neuron(n_x, alpha, 7)  # ReLU Activation\n", "N_22 = Neuron(n_x, alpha, 7)  # ReLU Activation\n", "N_23 = Neuron(n_x, alpha, 7)  # ReLU Activation\n", "N_31 = Neuron(3 ,alpha, 21)  # Sigmoid Activation\n", "cost= []\n", "\n", "for i in range(iterations):\n", "    # Forward \n", "    a_21 = N_21.forward(X2)\n", "    a_22 = N_22.forward(X2)\n", "    a_23 = N_23.forward(X2)\n", "    a_2 = np.concatenate((a_21,a_22, a_23),axis=1)\n", "    \n", "    a_3 = N_31.forward(a_2)\n", "\n", "    # Backward Learning\n", "    grad = N_31.gradient(a_3,a_3 - y)\n", "    N_31.backward(a_2,a_3 - y)\n", "    \n", "    N_21.backward(X2,grad*N_31.w[0])\n", "    N_22.backward(X2,grad*N_31.w[1])\n", "    N_23.backward(X2,grad*N_31.w[2])\n", "    \n", "    # Performance \n", "    cost.append(np.sum(np.abs(a_3-y))/y.shape[0])\n", "\n", "plt.plot(cost)\n", "plt.show()\n", "\n", "m = y.shape[0]\n", "measure_p = (a_3 > 0.5) == y\n", "measure_n = (a_3 <= 0.5) != y\n", "print(\"Training: Positive Success = {}, Negative Success = {}\".format(np.sum(measure_p)/m,np.sum(measure_n)/m))\n"], "outputs": [], "execution_count": 20}, {"cell_type": "code", "metadata": {"_uuid": "94266812b0d44d9458a8af794c497ca32cb0cb83", "_cell_guid": "10f481c0-aab7-49f3-9629-c12246f7fdb7"}, "source": ["PassID, X = get_features(test_raw)\n", "X2 = X\n", "\n", "a_21 = N_21.forward(X2)\n", "a_22 = N_22.forward(X2)\n", "a_23 = N_23.forward(X2)\n", "a_2 = np.concatenate((a_21,a_22, a_23),axis=1)\n", "    \n", "a_3 = N_31.forward(a_2)\n", "\n", "\n", "y_hat = np.reshape(a_3,(-1,1))\n", "print(y_hat.shape)\n", "data = y_hat > 0.5\n", "data = data*1\n", "s0 = pd.Series(PassID, index=PassID)\n", "s1 = pd.Series(data[:,0], index=PassID)\n", "\n", "df = pd.DataFrame(data = s1,index = PassID)\n", "df.columns = ['Survived']\n", "df.to_csv('simple_nn_2.csv', sep=',')"], "outputs": [], "execution_count": 21}, {"cell_type": "markdown", "source": ["Test with a bigger hidden layer "], "metadata": {"_uuid": "be85cbe70b44b50513e208e8a5a6f621b078f793", "_cell_guid": "5500749f-4005-4344-88b5-bb9e380ece39"}}, {"cell_type": "code", "metadata": {"_uuid": "e6325c2ae69f47de525069db8835bc71e75752ca", "_cell_guid": "c02a1376-a082-4b13-966b-76b63ac4d0ed"}, "source": ["PassID, X = get_features(train_raw)\n", "X2 = X #kernel(X)\n", "n_x = X2.shape[1] # Features\n", "alpha = 0.15/n_x\n", "# Survived\n", "y = np.array(train_raw['Survived'])\n", "y = np.reshape(y,(-1,1))\n", "iterations = 5000\n", "# Setup Layers\n", "# 2 - sigmoid\n", "# 3 - tanh\n", "# 7 - ReLU\n", "# 21- Gaussian\n", "N_21 = Neuron(n_x, alpha, 7)  \n", "N_22 = Neuron(n_x, alpha, 7)   \n", "N_23 = Neuron(n_x, alpha, 7) \n", "N_24 = Neuron(n_x, alpha, 7)\n", "N_25 = Neuron(n_x, alpha, 7)   \n", "N_26 = Neuron(n_x, alpha, 7)   \n", "N_27 = Neuron(n_x, alpha, 7)   \n", "N_28 = Neuron(n_x, alpha, 7)    \n", "\n", "n_hidden = 8\n", "N_31 = Neuron(n_hidden ,alpha, 21)  # \n", "cost= []\n", "\n", "for i in range(iterations):\n", "    # Forward \n", "    a_21 = N_21.forward(X2)\n", "    a_22 = N_22.forward(X2)\n", "    a_23 = N_23.forward(X2)\n", "    a_24 = N_24.forward(X2)\n", "    a_25 = N_25.forward(X2)\n", "    a_26 = N_26.forward(X2)\n", "    a_27 = N_27.forward(X2)\n", "    a_28 = N_28.forward(X2)\n", "    \n", "    a_2 = np.concatenate((a_21,a_22, a_23, a_24, a_25,a_26, a_27, a_28),axis=1)\n", "    a_3 = N_31.forward(a_2)\n", "\n", "    # Backward Learning\n", "    grad = -N_31.gradient(a_3,y)\n", "    N_31.backward(a_2,a_3 - y)\n", "    \n", "    N_21.backward(X2,grad*N_31.w[0])\n", "    N_22.backward(X2,grad*N_31.w[1])\n", "    N_23.backward(X2,grad*N_31.w[2])\n", "    N_24.backward(X2,grad*N_31.w[3])\n", "    N_25.backward(X2,grad*N_31.w[4])\n", "    N_26.backward(X2,grad*N_31.w[5])\n", "    N_27.backward(X2,grad*N_31.w[6])\n", "    N_28.backward(X2,grad*N_31.w[7])\n", "\n", "    # Performance \n", "    cost.append(np.sum(np.abs(a_3-y))/y.shape[0])\n", "\n", "plt.plot(cost)\n", "plt.show()\n", "\n", "m = y.shape[0]\n", "measure_p = (a_3 > 0.5) == y\n", "measure_n = (a_3 <= 0.5) != y\n", "print(\"Training: Positive Success = {}, Negative Success = {}\".format(np.sum(measure_p)/m,np.sum(measure_n)/m))\n", "plt.scatter(a_3,y)\n", "plt.show()"], "outputs": [], "execution_count": 22}, {"cell_type": "markdown", "source": ["# TODO\n", "*  Fix neural networks backpropagation to learn using the gradient of outer layers instead of pure delta\n", "* in case of categorical features with more than two classes, split it into a set of binary features one for each class \n", "* implement neural nets with seed weights instead of random ones "], "metadata": {"collapsed": true, "_uuid": "43fcb9d72b0117720534b04d12f4c0786132145b", "_cell_guid": "d5ee3fd3-da6a-4d7e-a246-752d88305b62"}}, {"cell_type": "code", "metadata": {"_uuid": "fb169a93bfc378879216ed849e690235d6814345", "_cell_guid": "30c84afc-ee25-42d3-9e6f-89b6b5a0f208"}, "source": ["# Prepare Data for submission\n", "data = a_3 > 0.5\n", "data = data*1\n", "s0 = pd.Series(PassID, index=PassID)\n", "s1 = pd.Series(data[:,0], index=PassID)\n", "\n", "df = pd.DataFrame(data = s1,index = PassID)\n", "df.columns = ['Survived']\n", "df.to_csv('nn_submission.csv', sep=',')\n", "print('Done')"], "outputs": [], "execution_count": 23}, {"cell_type": "markdown", "source": ["# Experiment\n", "in feature extraction portion, I will use single value decoposition as mapped feature vector \n", "try to build a model on it, and then attempt to reproduce U from s and V to repeat the same operation on test data"], "metadata": {"_uuid": "e5f65650720a178aadfb84a891b5f64488556a03", "_cell_guid": "01caa5c1-6843-4562-82d7-cdd5d4a85943"}}, {"cell_type": "code", "metadata": {"_uuid": "82cec8ea244c26db74acca9609f4b354ee04a1bb", "_cell_guid": "33cf9f69-5ba7-4d21-b260-95737d0e10ce"}, "source": ["PassID, X = get_features(train_raw)\n", "X2 = kernel(X)\n", "\n", "Ux, sx, Vx = np.linalg.svd(X2, full_matrices=False)\n", "X2 = Ux\n", "n_x = X2.shape[1] # Features\n", "alpha = 0.0015\n", "# Survived\n", "y = np.array(train_raw['Survived'])\n", "y = np.reshape(y,(-1,1))\n", "iterations = 25000\n", "# Setup Layers\n", "# 2 - sigmoid\n", "# 3 - tanh\n", "# 7 - ReLU\n", "# 21- Gaussian\n", "N_21 = Neuron(n_x, alpha, 2)  \n", "\n", "cost= []\n", "\n", "for i in range(iterations):\n", "    # Forward \n", "    a_2 = N_21.forward(X2)\n", "\n", "    # Backward Learning\n", "    grad = N_21.gradient(a_2,a_2 - y)\n", "    N_21.backward(X2,a_2 - y)\n", "\n", "    # Performance \n", "    cost.append(np.sum(np.abs(a_2-y))/y.shape[0])\n", "\n", "plt.plot(cost)\n", "plt.show()\n", "\n", "m = y.shape[0]\n", "measure_p = (a_2 > 0.5) == y\n", "measure_n = (a_2 <= 0.5) != y\n", "print(\"Training: Positive Success = {}, Negative Success = {}\".format(np.sum(measure_p)/m,np.sum(measure_n)/m))\n", "plt.scatter(a_2,y)\n", "plt.show()"], "outputs": [], "execution_count": 24}, {"cell_type": "markdown", "source": ["Test with Logistic Regression instead"], "metadata": {"_uuid": "a314cbf9675d8347ef323193627d028dace1d66c", "_cell_guid": "879964d4-da9f-43b0-8974-2c0bc73c73bf"}}, {"cell_type": "code", "metadata": {"_uuid": "70f4d1df5676abe0a0ca7cb789367aef304c039a", "_cell_guid": "b343d95a-4f8e-4f3a-b0be-74a5172db9d2"}, "source": ["PassID, X = get_features(train_raw)\n", "\n", "# Survived\n", "y = np.array(train_raw['Survived'])\n", "y = np.reshape(y,(-1,1))\n", "\n", "X2 = kernel(X)\n", "# Test Model\n", "alpha = 0.005 # Learning Rate\n", "Ux, sx, Vx = np.linalg.svd(X2, full_matrices=False)\n", "X2 = Ux\n", "print(Ux.shape)\n", "w, b = init(X2)\n", "print(w.shape)\n", "cost = []\n", "\n", "for i in range(500):\n", "    w, b = learn(X2, w, b, y, alpha)\n", "    cost.append(lcost(X2, w, b, y))\n", "    \n", "y_hat = predict(X2, w, b)\n", "print(w.shape)\n", "plt.plot(cost)\n", "plt.show()\n", "\n", "plt.scatter( y_hat, y)\n", "plt.show()\n", "\n", "m = y.shape[0]\n", "measure_p = (y_hat > 0.5) == y\n", "measure_n = (y_hat <= 0.5) != y\n", "print(\"Training: Positive Success = {}, Negative Success = {}\".format(np.sum(measure_p)/m,np.sum(measure_n)/m))\n"], "outputs": [], "execution_count": 25}, {"cell_type": "code", "metadata": {"_uuid": "53db7da47e08f7e890a2ed83c5b5a9ffe884570c", "_cell_guid": "bf48bac8-bd9c-4f02-a029-ba68e352274b"}, "source": ["# Validate for test set \n", "PassID, X = get_features(test_raw)\n", "X2 = kernel(X)\n", "U2 = np.dot(np.dot(X2,Vx),np.linalg.inv(np.diag(sx)))\n", "print(U2.shape)\n", "print(w.shape)\n", "y_hat = predict(U2, w, b)\n", "\n", "# Prepare Data for submission\n", "data = y_hat > 0.5\n", "data = data*1\n", "s0 = pd.Series(PassID, index=PassID)\n", "s1 = pd.Series(data[:,0], index=PassID)\n", "\n", "df = pd.DataFrame(data = s1,index = PassID)\n", "df.columns = ['Survived']\n", "df.to_csv('lr_svd_submission.csv', sep=',')"], "outputs": [], "execution_count": 26}, {"cell_type": "code", "metadata": {"_uuid": "c3bf6871c7b1b1d9503c8b7bfeafeacf7ef0d04d", "_cell_guid": "9ed5aaf2-858d-4c8f-926f-8b22f56708f9"}, "source": ["PassID, X = get_features(train_raw)\n", "X2 = kernel(X)\n", "Ux, sx, Vx = np.linalg.svd(X2, full_matrices=False)\n", "X2 = Ux\n", "\n", "n_x = X2.shape[1] # Features\n", "alpha = 0.00125/n_x\n", "# Survived\n", "y = np.array(train_raw['Survived'])\n", "y = np.reshape(y,(-1,1))\n", "iterations = 2500\n", "# Setup Layers\n", "# 2 - sigmoid\n", "# 3 - tanh\n", "# 7 - ReLU\n", "# 21- Gaussian\n", "N_21 = Neuron(n_x, alpha, 7)  \n", "N_22 = Neuron(n_x, alpha, 7)   \n", "N_23 = Neuron(n_x, alpha, 7) \n", "N_24 = Neuron(n_x, alpha, 7)\n", "N_25 = Neuron(n_x, alpha, 7)   \n", "N_26 = Neuron(n_x, alpha, 7)   \n", "N_27 = Neuron(n_x, alpha, 7)   \n", "N_28 = Neuron(n_x, alpha, 7)    \n", "\n", "n_hidden = 8\n", "N_31 = Neuron(n_hidden ,alpha/n_hidden, 21)  \n", "cost= []\n", "\n", "for i in range(iterations):\n", "    # Forward \n", "    a_21 = N_21.forward(X2)\n", "    a_22 = N_22.forward(X2)\n", "    a_23 = N_23.forward(X2)\n", "    a_24 = N_24.forward(X2)\n", "    a_25 = N_25.forward(X2)\n", "    a_26 = N_26.forward(X2)\n", "    a_27 = N_27.forward(X2)\n", "    a_28 = N_28.forward(X2)\n", "    \n", "    a_2 = np.concatenate((a_21,a_22, a_23, a_24, a_25,a_26, a_27, a_28),axis=1)\n", "    a_3 = N_31.forward(a_2)\n", "\n", "    # Backward Learning\n", "    grad = N_31.gradient(a_3,a_3 - y)\n", "    N_31.backward(a_2,a_3 - y)\n", "\n", "    N_21.backward(X2,grad*N_31.w[0])\n", "    N_22.backward(X2,grad*N_31.w[1])\n", "    N_23.backward(X2,grad*N_31.w[2])\n", "    N_24.backward(X2,grad*N_31.w[3])\n", "    N_25.backward(X2,grad*N_31.w[4])\n", "    N_26.backward(X2,grad*N_31.w[5])\n", "    N_27.backward(X2,grad*N_31.w[6])\n", "    N_28.backward(X2,grad*N_31.w[7])\n", "\n", "    # Performance \n", "    cost.append(np.sum(np.abs(a_3-y))/y.shape[0])\n", "\n", "plt.plot(cost)\n", "plt.show()\n", "\n", "m = y.shape[0]\n", "measure_p = (a_3 > 0.5) == y\n", "measure_n = (a_3 <= 0.5) != y\n", "print(\"Training: Positive Success = {}, Negative Success = {}\".format(np.sum(measure_p)/m,np.sum(measure_n)/m))\n", "plt.scatter(a_3,y)\n", "plt.show()"], "outputs": [], "execution_count": 27}, {"cell_type": "code", "metadata": {"_uuid": "9887fd201a928c4a83e0f59a36cffad89aae82c7", "_cell_guid": "2858c38d-9d6e-4e08-b996-a819acd35d9a"}, "source": ["PassID, X = get_features(train_raw)\n", "X2 = kernel(X)\n", "#Ux, sx, Vx = np.linalg.svd(X2, full_matrices=False)\n", "#X2 = Ux\n", "\n", "n_x = X2.shape[1] # Features\n", "alpha = 0.00125/n_x\n", "# Survived\n", "y = np.array(train_raw['Survived'])\n", "y = np.reshape(y,(-1,1))\n", "iterations = 1500\n", "# Setup Layers\n", "# 2 - sigmoid\n", "# 3 - tanh\n", "# 7 - ReLU\n", "# 21- Gaussian\n", "N_21 = Neuron(n_x, alpha, 7)  \n", "N_22 = Neuron(n_x, alpha, 7)   \n", "\n", "n_hidden = 2\n", "N_31 = Neuron(n_hidden ,alpha/n_hidden, 21)  # Sigmoid Activation\n", "cost= []\n", "\n", "for i in range(iterations):\n", "    # Forward \n", "    a_21 = N_21.forward(X2)\n", "    a_22 = N_22.forward(X2)\n", "    \n", "    a_2 = np.concatenate((a_21,a_22),axis=1)\n", "    a_3 = N_31.forward(a_2)\n", "\n", "    # Backward Learning\n", "    \n", "    grad = N_31.gradient(a_3,y)\n", "    N_31.backward(a_2,a_3 - y)\n", "    N_21.backward(X2,grad*N_31.w[0])\n", "    N_22.backward(X2,grad*N_31.w[1])\n", "\n", "    # Performance \n", "    cost.append(np.sum(np.abs(a_3-y))/y.shape[0])\n", "\n", "plt.plot(cost)\n", "plt.show()\n", "\n", "m = y.shape[0]\n", "measure_p = (np.abs(a_3) > 0.5) == y\n", "measure_n = (np.abs(a_3) <= 0.5) != y\n", "print(\"Training: Positive Success = {}, Negative Success = {}\".format(np.sum(measure_p)/m,np.sum(measure_n)/m))\n", "plt.scatter(a_3,y)\n", "plt.show()"], "outputs": [], "execution_count": 28}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "9634a77956a26c11c1bea0e1ff475c7c0f3d450b", "_cell_guid": "8c35b198-4de2-4094-b37d-fe5f5fb867ec"}, "source": ["# Prepare Data for submission\n", "\n", "# Validate for test set \n", "PassID, X = get_features(test_raw)\n", "X2 = kernel(X)\n", "\n", "a_21 = N_21.forward(X2)\n", "a_22 = N_22.forward(X2)\n", "    \n", "a_2 = np.concatenate((a_21,a_22),axis=1)\n", "y_hat = N_31.forward(a_2)\n", "\n", "data = y_hat > 0.5\n", "data = data*1\n", "s0 = pd.Series(PassID, index=PassID)\n", "s1 = pd.Series(data[:,0], index=PassID)\n", "\n", "df = pd.DataFrame(data = s1,index = PassID)\n", "#print(df)\n", "df.columns = ['Survived']\n", "df.to_csv('nn_gauss.csv', sep=',')\n", "\n"], "outputs": [], "execution_count": 29}, {"cell_type": "code", "metadata": {"_uuid": "74a9ce9cb65d7d78989d325f84a9c011f48dcc21", "_cell_guid": "4891b944-e4c3-4537-9ae6-32cd736e34d5"}, "source": ["# Experiment, Random ReLUs without learning, with output fed to Gaussian \n", "PassID, X = get_features(train_raw)\n", "X2 = X\n", "#Ux, sx, Vx = np.linalg.svd(X2, full_matrices=False)\n", "#X2 = Ux\n", "n_x = X2.shape[1] # Features\n", "alpha = 0.05\n", "# Survived\n", "y = np.array(train_raw['Survived'])\n", "y = np.reshape(y,(-1,1))\n", "iterations = 500\n", "instances = 5\n", "oinst = 300\n", "N_r0 = Neuron(n_x,alpha,7)\n", "N_r = [Neuron(n_x,alpha,7) for j in range(instances)]  # ReLU Activation \n", "\n", "a_2 = N_r0.forward(X2)\n", "\n", "for i in range(instances):\n", "    a_2i = N_r[i].forward(X2)\n", "    a_2 = np.concatenate((a_2,a_2i),axis=1)\n", "\n", "# Setup single neurons\n", "N_g = [Neuron(n_x,alpha,21) for j in range(oinst)]\n", "final_cost = []\n", "\n", "alph = np.linspace(0.55,0.6,oinst)\n", "\n", "for j in range(oinst):\n", "    cost_g = []\n", "    N_g[j].alpha = alph[j]\n", "    for i in range(iterations):\n", "        # Evaluate\n", "        a_g = N_g[j].forward(X2)\n", "        \n", "        # Learn\n", "        N_g[j].backward(X2,a_g - y)\n", "        \n", "        # Evaluate\n", "        a_g = N_g[j].forward(X2)\n", "        \n", "        # Performance vote\n", "        cost_g.append(np.sum(np.abs(a_g-y))/y.shape[0])\n", "    \n", "    m = y.shape[0]\n", "    measure_p = (a_g > 0.5) == y\n", "    measure_n = (a_g <= 0.5) != y\n", "    final_cost.append(cost_g[-1])\n", "\n", "    print(\"Training: Positive Success = {}, Negative Success = {}\".format(np.sum(measure_p)/m,np.sum(measure_n)/m))\n", "\n", "    #plt.plot(cost_g)\n", "    #plt.title(\"Cost over time\")\n", "    #plt.show()\n", "\n", "    #plt.scatter(a_g, y)\n", "    #plt.show()\n", "    \n", "plt.plot(alph,final_cost)    \n", "plt.show()"], "outputs": [], "execution_count": 45}, {"cell_type": "code", "metadata": {"_uuid": "3b2a4ca96da91d6efcda2d8116188ad9e2e7b237", "_cell_guid": "8039bda4-2ead-4355-9b29-73ea294d932a"}, "source": ["PassID, X = get_features(test_raw)\n", "X2 = X\n", "\n", "a_gm = N_g[0].forward(X2)\n", "for i in range(oinst-1):\n", "    a_gi = N_g[i+1].forward(X2)\n", "    a_gm = np.concatenate((a_gm,a_gi),axis=1)\n", "    \n", "y_hat = np.mean(a_gm, axis=1)\n", "y_hat = np.reshape(y_hat,(-1,1))\n", "print(y_hat.shape)\n", "data = y_hat > 0.5\n", "data = data*1\n", "s0 = pd.Series(PassID, index=PassID)\n", "s1 = pd.Series(data[:,0], index=PassID)\n", "\n", "df = pd.DataFrame(data = s1,index = PassID)\n", "df.columns = ['Survived']\n", "df.to_csv('voting_gauss_nn_3.csv', sep=',')"], "outputs": [], "execution_count": 46}, {"cell_type": "markdown", "source": ["I think I should use some cross validation before declaring a result\n", "my models are overfitting to training data"], "metadata": {}}, {"cell_type": "code", "metadata": {"_uuid": "4c2d5ff5cd7825b4d8ef1b0589cf9f483436944f", "_cell_guid": "ca049287-3f8a-473e-973c-6dc982e9ccfb"}, "source": ["# Search optimal alpha\n", "PassID, X = get_features(train_raw)\n", "X2 = X\n", "#Ux, sx, Vx = np.linalg.svd(X2, full_matrices=False)\n", "#X2 = Ux\n", "n_x = X2.shape[1] # Features\n", "alpha = 0.24\n", "# Survived\n", "y = np.array(train_raw['Survived'])\n", "y = np.reshape(y,(-1,1))\n", "iterations = 3000\n", "instances = 5\n", "oinst = 150\n", "N_r0 = Neuron(n_x,alpha,7)\n", "N_r = [Neuron(n_x,alpha,7) for j in range(instances)]  # ReLU Activation \n", "\n", "a_2 = N_r0.forward(X2)\n", "m = y.shape[0]\n", "\n", "\n", "# Setup single neurons\n", "N_g = [Neuron(n_x,alpha,21) for j in range(oinst)]\n", "final_gain = []\n", "alph = np.linspace(0.0,0.20,oinst)\n", "for j in range(oinst):\n", "    gain_g = []\n", "    N_g[j].alpha = alph[j]\n", "    for i in range(iterations):\n", "        # Evaluate\n", "        a_g = N_g[j].forward(X2)\n", "        \n", "        # Learn\n", "        N_g[j].backward(X2,a_g - y)\n", "        \n", "        # Evaluate\n", "        a_g = N_g[j].forward(X2)\n", "        \n", "        # Performance vote\n", "        gain_g.append(np.sum(measure_p)/m)\n", "    \n", "    \n", "    measure_p = (a_g > 0.5) == y\n", "    measure_n = (a_g <= 0.5) != y\n", "\n", "    final_gain.append(np.sum(measure_p)/m)\n", "\n", "    #plt.plot(cost_g)\n", "    #plt.title(\"Cost over time\")\n", "    #plt.show()\n", "\n", "    #plt.scatter(a_g, y)\n", "    #plt.show()\n", "    \n", "plt.scatter(alph,final_gain)    \n", "plt.show()"], "outputs": [], "execution_count": 32}, {"cell_type": "code", "metadata": {"_uuid": "2a473c67e8d24b1c56151d52145963e6fc3b30c6", "_cell_guid": "9d24cc05-95be-477b-afb6-a1515264681d"}, "source": ["# Search optimal alpha\n", "PassID, X = get_features(train_raw)\n", "X2 = X\n", "#Ux, sx, Vx = np.linalg.svd(X2, full_matrices=False)\n", "#X2 = Ux\n", "n_x = X2.shape[1] # Features\n", "alpha = 0.051\n", "# Survived\n", "y = np.array(train_raw['Survived'])\n", "y = np.reshape(y,(-1,1))\n", "iterations = 100\n", "instances = 5\n", "oinst = 150\n", "\n", "a_2 = N_r0.forward(X2)\n", "m = y.shape[0]\n", "\n", "\n", "# Setup single neurons\n", "N_g = [Neuron(n_x,alpha,3) for j in range(oinst)]\n", "final_gain = []\n", "alph = np.linspace(0.05,0.055,oinst)\n", "iters = [100+i for i in range(oinst)]\n", "\n", "for j in range(oinst):\n", "    gain_g = []\n", "    #N_g[j].alpha = alph[j]\n", "    for i in range(iterations):\n", "        # Evaluate\n", "        a_g = N_g[j].forward(X2)\n", "        \n", "        # Learn\n", "        N_g[j].backward(X2,a_g - y)\n", "        \n", "        # Evaluate\n", "        a_g = N_g[j].forward(X2)\n", "        \n", "        # Performance vote\n", "        gain_g.append(np.sum(measure_p)/m)\n", "    \n", "    \n", "    measure_p = (a_g > 0.5) == y\n", "    measure_n = (a_g <= 0.5) != y\n", "\n", "    final_gain.append(np.sum(measure_p)/m)\n", "\n", "    #plt.plot(cost_g)\n", "    #plt.title(\"Cost over time\")\n", "    #plt.show()\n", "\n", "    #plt.scatter(a_g, y)\n", "    #plt.show()\n", "    \n", "plt.scatter(alph,final_gain)    \n", "plt.show()"], "outputs": [], "execution_count": 33}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "285c2cdd4aeb424265618c4c1cbf880f36b8bee5", "_cell_guid": "25a6169b-1dcb-4be7-ae89-fc3bf28efd2e"}, "source": [], "outputs": [], "execution_count": null}, {"cell_type": "code", "metadata": {"collapsed": true, "_uuid": "0db7e6dbc52fdedd15114b520a9fa309d2d85c42", "_cell_guid": "c9c8bc33-5543-464c-81fd-1b8d69483a27"}, "source": [], "outputs": [], "execution_count": null}], "nbformat": 4, "metadata": {"language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "name": "python", "pygments_lexer": "ipython3", "version": "3.6.4", "mimetype": "text/x-python", "file_extension": ".py", "nbconvert_exporter": "python"}, "kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}}}