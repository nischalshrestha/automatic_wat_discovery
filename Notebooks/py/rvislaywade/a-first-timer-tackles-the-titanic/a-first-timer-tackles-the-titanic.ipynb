{"cells":[{"metadata":{"_uuid":"2f9381932ca3443a089219b76d442534975e0031"},"cell_type":"markdown","source":"## Tackling the Titanic\nRebecca Vislay Wade","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"e04a9535-efb4-47e3-bc9e-a05bc5e22130","_uuid":"bd1a2465c7f104b74e828edb2a64b6f91c1bc8e4"},"cell_type":"markdown","source":"#### Overview\nThis notebook describes a solution to the Titanic problem implemented in Python 3. There are many like it but this one is mine :) Thanks for checking it out!\n\n1. Data Load & Inspection  \n2. Feature Engineering & Missing Value Imputation  \n    A. New variable, 'Deck'  \n    B. New variables describing family size  \n    C. New variables from 'Name'  \n    D. New variables from 'Ticket'  \n    E. Indicator variables for imputed values  \n    F. Imputing missing values of 'Fare' & 'Embarked'  \n    G. Using 'Title' to impute missing 'Age' values  \n    H. Classification tree model to impute 'Deck'  \n3. Model Construction  \n    A. Final data prep  \n    B. Model 1: L1-Penalized Logistic Regression (0.77511 public leaderboard score)  \n    C. Model 2: ElasticNet Logistic Regression (0.77950)  \n    D. Model 3: Random Forest (*best performing model* 0.79425)  \n    E. Model 4: Gradient Boosted Tree (0.77033)  \n    F. Model 5: Gradient Boosted Tree with Reduced Predictor Set (0.75598)  \n4. Model Comparison  \n\n**Some of the model parameter grid searches take a long time to run.**","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"44385fde-1334-4030-b180-aafed4c6f60b","_uuid":"ad9eaa57130273e380f5403fc27f16466edf2ac1"},"cell_type":"markdown","source":"### Data Load & Inspection","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"954251f8-0f7e-48b4-85d7-23ce739cb340","_uuid":"ff003486e21f947929f04c68403e5ce3edc6ae8a"},"cell_type":"markdown","source":"First, we import set a working directory and import the train and test csv file. I'm using the index_col argument in the read_csv function to set the index equal to 'PassengerId'.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"f0f63201-9f8d-4d04-be99-c19ec24d57ec","_uuid":"48186c23c4730ebb46997962ab2796143bacee68","collapsed":true,"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"48dd4525-712e-4c5d-a399-3d8b342dc5f9","_uuid":"e0bdf3d136858166336833959fe7e47918e7a7a5","collapsed":true,"trusted":true},"cell_type":"code","source":"# read individual CSVs into pandas DataFrames\ntrain = pd.read_csv('../input/train.csv', index_col = 'PassengerId')\ntest = pd.read_csv('../input/test.csv', index_col = 'PassengerId')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f52b9c5e-2422-4a19-8845-c9d41abd6c3a","_uuid":"7db7762bf1a51f791beb545c13bbba1e31b7a98c"},"cell_type":"markdown","source":"Here, I add a column of NaNs as placeholders for the 'Survived' variable in the test dataset and then combine it with the train data into a single dataframe, 'data'.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"1029f3ef-8537-4bba-b849-21550df12f24","_uuid":"d44fa52032aafb6c3c2e9dc488b948e9687463f6","collapsed":true,"trusted":true},"cell_type":"code","source":"# add \"Survived\" column to test & combine into a single DataFrame\nimport numpy as np\n\ntest['Survived'] = np.nan\ndata = train.append(test)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6380f0b3-6ce0-4539-aebe-3731246c73d7","_uuid":"63a67ed58928fbec38b756a3b0fa3b25f13a597f"},"cell_type":"markdown","source":"Take a look at the first few rows...","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"4bb809de-debf-49d6-9a92-d47b75c1f955","_uuid":"1a40fcf2d944e96cd9d8e06c7e56cac21ffb8bf9","collapsed":true,"trusted":true},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"de45ee90-596f-46ba-9f6f-8afbd733eee1","_uuid":"64058b6b34565ab6398666e216f5a8cafd451175"},"cell_type":"markdown","source":"Now we'll look at a frequency table for the target variable, 'Survived'...","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"1890a7b7-dbc8-4bc7-8799-11b4be36fc1d","_uuid":"ede6797bef8db22019b0b8fe151a3cdc468d55d7","collapsed":true,"trusted":true},"cell_type":"code","source":"# look at counts for target variable 'survived'\npd.crosstab(index = data['Survived'], columns = 'Count')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4da1dd30-9dbb-459d-8613-dded3678b229","_uuid":"0e25aaaacd2b1f5418f5b5ed10f84db97261e953"},"cell_type":"markdown","source":"...and the number of missing values for each variable.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"9c970269-b2e1-4b2c-b800-d335ed35b32f","_uuid":"72577b248fe2da473ec79ce54b33ac74585fa7d5","collapsed":true,"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f06e4818-5fe8-4283-a243-cb3a5761fee8","_uuid":"063ddccfb6d3a8b046c444d1fc4e98a952ea3c95"},"cell_type":"markdown","source":"### Feature Engineering & Missing Value Imputation","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"2ee786da-bc90-4680-9b38-922b4af6db95","_uuid":"77a465305b6d2f715cc6fa93b055b83857314d6c"},"cell_type":"markdown","source":"Age, Cabin, Embarked, and Fare are all missing one or more values in the dataset that will have to be imputed. We will take care of these a little later. First, let's create some new variables.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"75a32db5-3820-47b5-a447-cf9d9a5f7625","_uuid":"5797318c43d7ea0c1b0f610f1cf4b3c4710f1d73"},"cell_type":"markdown","source":"##### Create new variable 'Deck'\nThe Titanic struck the fateful iceberg at 11:40pm at night. Presumably, most passengers would have been turning in for the night. The Cabin varible contains information about where on the ship passengers most likely were when it had the collision and began to sink. Such information could be predictive of survival. \n\nFor the passengers that have one indicated, the Cabin variable consists of a letter and a number. The letter corresponds to a deck on the ship and the number to the passenger's room(s) on that deck.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"06988ff4-641d-4d42-b23d-91330eee9fff","_uuid":"9771d090d0f40da28067d38a6709b135af88238d","collapsed":true,"trusted":true},"cell_type":"code","source":"data.Cabin.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b40d463f-9f5c-4ed6-9185-6a3c7fe8fae6","_uuid":"d974b0f958d7299397549d186d599b4e15caef04"},"cell_type":"markdown","source":"Let's extract the letter character and make it a new variable, 'Deck'.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"57e2afb5-6dc8-45ac-b457-79cec3f1b2f5","_uuid":"a80abe8a302ea791398db432de919ff3e01c2570","collapsed":true,"trusted":true},"cell_type":"code","source":"# Define new variable 'Deck' from first character of the string in 'Cabin'\ndata['Deck'] = data['Cabin'].astype(str).str[0]\n\n# replace n with NaNs\ndata['Deck'] = data.Deck.replace('n', np.nan)\n\n# check again\ndata.Deck.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"27fc4f70-4235-4f0e-9b62-0e8d25c482ed","_uuid":"519fefefa8a2d5f512440d7498404a8b496ac9a3"},"cell_type":"markdown","source":"##### Create new variables related to family size\nAnother factor that could have impacted survival is whether or not passengers were traveling with family members or alone. The dataset contains two variables pertaining to family size: 'Parch', the number of parents and/or children the passenger was traveling with, and ' SibSp', the number of siblings and/or spouses. Together, these two variables add to give us the size of the passenger's family.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"f3a743f7-c06d-4628-94b4-b43d9f2caff6","_uuid":"9965fc29e1e2305e5b28d6427b34e6d582420c8d","collapsed":true,"trusted":true},"cell_type":"code","source":"# Adding 1 (the passenger) + Parch (# of parents & children traveling with) + SibSp (# of siblings & spouses traveling with)\ndata['FamilySize'] = data['SibSp'] + data['Parch'] + 1\n\n# check new variable\ndata.FamilySize.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b708166b-8459-4096-af92-55ec498aa70a","_uuid":"b9a16a910d50ae9926f7e123b2e279a551664059"},"cell_type":"markdown","source":"Let's look at FamilySize in more detail. Here's a contingency table looking at FamilySize and Survived.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"a5376e1d-9c27-4be0-aaaf-3912d8b4114d","_uuid":"7f46ea003a56d720cb20befd5fd1fead6e6ae46f","collapsed":true,"trusted":true},"cell_type":"code","source":"# crosstab of FamilySize\npd.crosstab(data['FamilySize'], data['Survived'])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"51ac9c74-21f4-4ce8-ad46-a3e96b40b032","_uuid":"cf39f89d51db6610ca990397e369bd583a3cee33"},"cell_type":"markdown","source":"It looks like traveling with 1-3 family members (FamilySize = 2-4) could have positively affected the probability of survival. Here's a way of visualizing that. Since Survived is coded (0,1), a barplot of Survived versus FamilySize gives us the sample proportions of of surivivors in each group. ","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"f4b290b4-b805-4117-a80c-c57944b4ab40","_uuid":"0e026d3115b46949f55d2a984eadb1365d687de4","collapsed":true,"trusted":true},"cell_type":"code","source":"# make a dataframe of just FamilySize and Survived\nfamsizes = data[['FamilySize','Survived']]\n\nsns.set_context('talk')\nsns.barplot(x = 'FamilySize', y = 'Survived', data = famsizes, color = 'green')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7ed5bd99-ac51-41af-892e-e18e2fbf543e","_uuid":"a368f96943e9d372cc8e658fdd87a3a9287c71cf"},"cell_type":"markdown","source":"FamilySizes of 2-4 are associated with a greater than 50% chance of survival per the sample. Let's create some variables based on these observations.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"f178b4f4-18d1-4a4c-b28f-dc45435ca35e","_uuid":"ebd442a683bd521d758211d886617cf7cb808a80","collapsed":true,"trusted":true},"cell_type":"code","source":"# new variables (fun with list comprehensions!)\ndata['Alone'] = [1 if familysize == 1 else 0 for familysize in data['FamilySize']]\ndata['LargeFamily'] = [1 if familysize >= 5 else 0 for familysize in data['FamilySize']]\ndata['SmallFamily'] = [1 if familysize >= 2 and familysize < 5 else 0 for familysize in data['FamilySize']]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d4e12b02-3755-41a7-bdd9-7248587de5d7","_uuid":"2ee22b69ed57c6588241c53fa1785518c4f56288"},"cell_type":"markdown","source":"Crosstabs to check for correct totals.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"cc069a26-8aad-455e-b49e-856ab640ddc6","_uuid":"0426a4e12223e4bce4f810223cbc47b4cb16ba2d","collapsed":true,"trusted":true},"cell_type":"code","source":"# crosstab of FamilySize\npd.crosstab(data['FamilySize'], 'Count')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8737867d-34ec-4db7-9715-a9eeeb0185b5","_uuid":"0263a5bed39d8ea56f148dd2cb34d7d73c65c75a","collapsed":true,"trusted":true},"cell_type":"code","source":"# crosstabs of new variables\npd.crosstab(data['Alone'], 'Count')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b7703a56-d241-4600-8709-b6328fff8248","_uuid":"7f9b11678e20ce0d2cd3a95c387e48cd91714534","collapsed":true,"trusted":true},"cell_type":"code","source":"pd.crosstab(data['LargeFamily'], 'Count') # 22 + 25 + 16 + 8 + 11 = 82","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8c4f2b31-56a1-44c9-aae1-ff0e51094229","_uuid":"af0f0a12edc57813c25c57325d62c6fd60edb53a","collapsed":true,"trusted":true},"cell_type":"code","source":"pd.crosstab(data['SmallFamily'], 'Count') # 235 + 159 + 43 = 437","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ffd5e4df-5f74-4a15-aded-db2a7fa2ef1b","_uuid":"8afe45b1204211bee4e5d1ddab92f88db7f3c9e0"},"cell_type":"markdown","source":"##### What's in a name? A whole lot of information!\nThe 'Name' variable is rich with potential information that could be predictive of survival.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"6074fe82-fdc8-4a4f-8ab7-6673a9385de5","_uuid":"36e57f16de70bd8869548e85244459d470a0172d","collapsed":true,"trusted":true},"cell_type":"code","source":"data.Name.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"182d72b4-c9b3-44a5-b634-283c3fa076a6","_uuid":"b9ca17eee8e3ff67fbf7ef6486187e712793bfb1"},"cell_type":"markdown","source":"We begin by chopping up the Name variable to give us Title, LastName, MaidenName (in parentheses), Nickname (between double quotes), and FirstName.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"6c5f4a7e-862d-4a02-9e5a-69779c9bae0f","_uuid":"8a09af0ab0d4c53ba3476929093c8b90ec0b6347","collapsed":true,"trusted":true},"cell_type":"code","source":"# Split off last name into new column\ndata['LastName'] = data['Name'].str.split(',').str.get(0)\ndata['LastName'] = data['LastName'].str.strip() # strip whitespace from ends\n\n# Split off Title into new column\ndata['Title'] = data['Name'].str.split('.').str.get(0)\ndata['Title'] = data['Title'].str.split(',').str.get(1)\ndata['Title'] = data['Title'].str.strip()\n\n# Excise parenthetical full maiden names\ndata['MaidenName'] = data['Name'].str.split('(').str.get(1)\ndata['MaidenName'] = data['MaidenName'].str.split(')').str.get(0)\n\n# Excise nicknames\ndata['Nickname'] = data['Name'].str.split('\"').str.get(1)\ndata['Nickname'] = data['Nickname'].str.strip()\n\n# Get first name\ndata['FirstName'] = data['Name'].str.split('.').str.get(1)\ndata['FirstName'] = data['FirstName'].str.split(' ').str.get(1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5b1a20e9-197b-4c93-b13e-761ba98908c3","_uuid":"7bb0d8d0838464fff6765d1270b696f0a5bf2d88"},"cell_type":"markdown","source":"Married women are listed under their husband's full name with their actual first names now in MaidenName. Let's replace their husbands' names with their actual first names.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"c91e4cd9-dedb-4d83-84bd-33cf4e698313","_uuid":"e982182d36c0e9001e430f248bb42ea0616c415a","collapsed":true,"trusted":true},"cell_type":"code","source":"# get maiden first names\ndata['MaidenFirstName'] = data['MaidenName'].str.split(' ').str.get(0)\n# Replace FirstName with MaidenFirstName except with the NaNs filled with \n# FirstName and strip\ndata['FirstName'] = data['MaidenFirstName'].fillna(data['FirstName'])\ndata['FirstName'] = data['FirstName'].str.strip()\n# drop MaidenFirstName\ndata = data.drop(['MaidenFirstName'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"42d9a9c6-4347-4ff8-af31-4aaf4609784b","_uuid":"768f2702e5d602b5c1c94be71f18523b3601ded5"},"cell_type":"markdown","source":"Let's grab the wives' maiden last names, too.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"393bce11-1e9e-45a8-980e-84bf56033176","_uuid":"a1ae91fa766378c17f6f071e1526143045a9bae5","collapsed":true,"trusted":true},"cell_type":"code","source":"# Get MaidenLastName from MaidenName\ndata['MaidenLastName'] = data['MaidenName'].str.rsplit(' ', expand = True, n=1)[1]\n# replace 'None' with NaN and strip MaidenLastName\ndata['MaidenLastName'] = data.MaidenLastName.replace('None', np.nan)\ndata['MaidenLastName'] = data['MaidenLastName'].str.strip()\n# Drop MaidenName\ndata = data.drop('MaidenName', axis =1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"41fdce7a-3795-4e27-8a5c-eeae981dbafd","_uuid":"76a8110b9ee129151d8e15f5f83d4c2eea77e849"},"cell_type":"markdown","source":"##### Extracting & Combining Ticket Prefixes\nAnother potentially information-rich variable is 'Ticket'. These are alphanumeric ticket codes that may indicate things such as class, fare, embarkment, and originating ticket agent. ","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"7d39bfb9-d619-4fad-ae54-5f8463645d73","_uuid":"5145b4a110d5515e541b4e6118222b88377a6d50","collapsed":true,"trusted":true},"cell_type":"code","source":"data.Ticket.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9617acb2-b2e0-495d-9b88-bc344be9b96f","_uuid":"dc95b6a30d0bc78e3542fdff2837a79ec66bca32"},"cell_type":"markdown","source":"Families or groups traveling together may have had similar prefix codes, for example. Let's extract those prefixes...","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"d58ed3fa-0430-455a-a278-45f2299d0aa2","_uuid":"83c3429bd3236f9840ea78f569cd423861a2b3aa","collapsed":true,"trusted":true},"cell_type":"code","source":"# Let's define a ticket prefix as all the letters coming before the first \n# space. Some have '.' or '/' in the letters so we first have to remove those.\ndata['Ticket'] = data.Ticket.str.replace('.','')\ndata['Ticket'] = data.Ticket.str.replace('/','')\n\ndata.Ticket.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7456eea9-adfc-4ee7-b964-2fb64fc29130","_uuid":"1a8e022d2dba09c08fd72c3a51a8ddc697d4243f","collapsed":true,"trusted":true},"cell_type":"code","source":"# Now split at the spaces\ndata['TicketPrefix'] = data.Ticket.str.split().str.get(0)\n\n# list comprehension to replace numeric values of TicketPrefix with 'None'    \ndata['TicketPrefix'] = ['None' if prefix.isnumeric() == True else prefix for prefix in data.TicketPrefix]\n\n# take a look again at the cleaned up TicketPrefix Variable\ndata['TicketPrefix'].tail(25)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f64238c4-acfc-40cf-976d-56e41c8b36e3","_uuid":"ef4fbd50009c92fcfe7fd1473dfac4da3e04d5a9"},"cell_type":"markdown","source":"Here is the number of passengers in each of the ticket prefix groups...","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"4450aa13-1b36-432d-9032-a9afe423aa87","_uuid":"99b999e5db8eb054c63b59b1d2ce559b92b05c94","collapsed":true,"trusted":true},"cell_type":"code","source":"# counts of TicketPrefix\ndata['TicketPrefix'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"76012c3b-b5bd-4617-9d0a-e4d5a440e82f","_uuid":"d53b6bcfd2618d9e1c55c238819b8fab29b85f6d"},"cell_type":"markdown","source":"Some of these could be duplicate groups. For example, 'SCPARIS' and 'SCParis' are most likely the same prefix so let's combine them.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"a688f097-5f53-45a0-acc7-3c5ae0f59946","_uuid":"3e7d281d23e475b5d04d8064c2a25a2c1dd8712d","collapsed":true,"trusted":true},"cell_type":"code","source":"# Replace 'SCParis' with 'SCPARIS'\ndata.TicketPrefix = data.TicketPrefix.str.replace('SCParis', 'SCPARIS')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2837b8e3-fbf7-4c34-99f4-7016e8c079c7","_uuid":"a10ea957cb484aa77630f61b469c37ed53ad6b5b"},"cell_type":"markdown","source":"Also, let's put all the prefixes with only one member into a single category called 'Unique'.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"c81c80d8-7889-45d8-a60b-bbe2d8915a48","_uuid":"5478a118bf0a422b6e90dd3692531b5c5eec5aa1","collapsed":true,"trusted":true},"cell_type":"code","source":"# combine prefix categories with single members into new cateogry, 'Unique'.\nprefixes = data['TicketPrefix'].value_counts()\nuniquePrefixes = list(prefixes[prefixes == 1].index)\ndata.TicketPrefix = ['Unique' if prefix in uniquePrefixes else prefix for prefix in data.TicketPrefix]\n\n# look at value counts again\ndata['TicketPrefix'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f434b20e-ff0f-4a65-91f7-3e2781b4aa2d","_uuid":"4959dad20d1b81bbe56f5edac5ee01092d9e2fa2"},"cell_type":"markdown","source":"Excellent! Now we'll make dummy variables for each TicketPrefix category...","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"da5714ba-1780-4362-bd60-9fd06b9a4e7f","_uuid":"c18c8e95d3eefb2aa30271707ef9ce9d8a73cd3e","collapsed":true,"trusted":true},"cell_type":"code","source":"# make dummies for TicketPrefix\nprefixDummies = pd.get_dummies(data['TicketPrefix'], prefix = 'TicketPrefix')\ndata = pd.concat([data, prefixDummies], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f408f058-f64a-4abe-96bc-bc6ff26424a8","_uuid":"23adfbc4c08f820648597bc4ab50260c9930a291"},"cell_type":"markdown","source":"##### Make indicators for imputed values\nSometimes the fact that a varible is missing in the dataset is predictive. Below, indicator variables are made with the suffix '_M' that take the value 1 if it was imputed and 0 if not.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"0ce04af2-afa7-4767-9077-f37b8364d740","_uuid":"2a4cac468ff94f0a5bb338a08df6ca17298b4bf3","collapsed":true,"trusted":true},"cell_type":"code","source":"# create flag variables for each column with missing values where 1 means value was imputed and 0 means value was not imputed\nfor column in data.columns:\n    if data[column].isnull().sum() != 0:\n        data[column + '_M'] = data[column].isnull()\n        data[column + '_M'] = data[column + '_M'].astype('int64').replace('True', 1)\n        data[column + '_M'] = data[column + '_M'].astype('int64').replace('False', 0)\n\n# Rename the 'Survived_M' variable as Test since it identifies the members of the test set\ndata.rename(columns = {'Survived_M':'Test'}, inplace = True)\n\ndata.columns","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ae7da753-4b27-4398-beaa-48b0206a6227","_uuid":"2ad33897e80193f2c293f3a84c0b8a536a8e699c"},"cell_type":"markdown","source":"Let's drop the 'Cabin' and 'Cabin_M' variable for now since we'll focus on 'Deck' instead.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"26496711-6e88-4404-ae3b-f82601f5c2f6","_uuid":"dbab233da08bfbddceab2a59ed3f1dd254aa766e","collapsed":true,"trusted":true},"cell_type":"code","source":"data = data.drop(['Cabin', 'Cabin_M'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e5fea2ed-be36-4bcd-b89f-90d50c8ab312","_uuid":"fcf85f72e7c9ed3d74c7030c566bbfadec0b3669"},"cell_type":"markdown","source":"##### Impute missing values of Embarked and Fare\nTwo values of Embarked and one value of Fare are missing in the dataset","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"f5fe21e5-1ae1-4b65-884c-f386c853d846","_uuid":"b02a767d67eca06a6bc0bf316b08a3b7c7f9ee84","collapsed":true,"trusted":true},"cell_type":"code","source":"# check number of missing values for each variable again\ndata.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"322e7989-4228-4ba9-bf1a-6d247de21465","_uuid":"b1eab3cf716fff45feb7582399dc4fd1935336a1"},"cell_type":"markdown","source":"Embarked is a categorical variable that can take one of three values S (Southampton), Q (Queenstown), or C (Cherbourg). ","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"1c89dee5-eebc-4f55-919d-e31ffa178fce","_uuid":"cfae760cd82d63845f5373b490b1e94fd7809573","collapsed":true,"trusted":true},"cell_type":"code","source":"# Look at Embarked\npd.crosstab(index = data['Embarked'], columns = 'Count') # 914 'S'","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"77880bd8-4237-4bd5-86bf-3d60bdb8ced1","_uuid":"711ba74c4d96de4c627f72e77668434dabe38f1d"},"cell_type":"markdown","source":"Let's replace the two missing values of Embarked with the most common value, S.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"314403b2-dc17-497e-92e2-9961b10cd8cd","_uuid":"6a36d13b16bdd30b3e657939f1f043efeb8e9627","collapsed":true,"trusted":true},"cell_type":"code","source":"# replace those with the most common value ('S')\ndata.Embarked = data.Embarked.astype('str').replace('nan', 'S')\n\n# Check again\npd.crosstab(index = data['Embarked'], columns = 'Count') # 916 'S'","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"287ed80e-74bb-4a56-b5f1-c69ac42f9844","_uuid":"3a6a25d5318799513cd3f5ac174e49d00681c358"},"cell_type":"markdown","source":"Now we make some dummy variables.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"06c34f62-7631-43f7-a9e3-84d4e4d46bff","_uuid":"97fd3e31297aaacb6f6adf84f430d809786de808","collapsed":true,"trusted":true},"cell_type":"code","source":"# Make dummies for the Embarked variable\nembarkedDummies = pd.get_dummies(data['Embarked'], prefix = 'Embarked')\ndata = pd.concat([data, embarkedDummies], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"06b5679c-ce37-4696-8bfc-14c886d6a690","_uuid":"53ef1de305399fd0d5668da3ff5a629fdb7b5676"},"cell_type":"markdown","source":"Fare values vary greatly depending on the accomodation class (1st, 2nd, or 3rd) encoded by the variable 'Pclass'. First we find the value of Pclass for the passenger with the missing Fare value.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"5256527d-b39f-4efc-9563-4289c544db5d","_uuid":"4fcea2ebe087e43762af9518f7d3fd0fb5f62533","collapsed":true,"trusted":true},"cell_type":"code","source":"# Find the Pclass value for the passenger missing an entry for Fare\ndata['Pclass'][data['Fare'].isnull() == True] # Pclass = 3","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"17290822-2fbd-4f2b-b1be-f7abfb5fd9b1","_uuid":"b94a184b18bdaabef5652153b66dd259a201c90d"},"cell_type":"markdown","source":"The passenger was a 3rd class ticket holder. Now we set the missing Fare value equal to the mean Fare value among 3rd class passengers.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"b3c8354f-a94b-455f-8931-6c26bbe6d93d","_uuid":"7daff8e16ad471a9875a4f425808a6a3212e78f6","collapsed":true,"trusted":true},"cell_type":"code","source":"# Set missing Fare value equal to the average for 3rd Class\ndata['Fare'] = data['Fare'].fillna(data.groupby('Pclass').Fare.mean()[3])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"df060138-16d8-466f-b48d-2d7a4a09e626","_uuid":"215ee0ed7d5199448930009584a82828dbc05e72"},"cell_type":"markdown","source":"While we're at it, let's change the 'Sex' variable so that male/female is encoded 0/1.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"dc98b9d2-693b-4618-884d-a024ea69773b","_uuid":"de99c5599ccf4d6df8fbe6ec1440c33f94c708f4","collapsed":true,"trusted":true},"cell_type":"code","source":"# replace female/male with 1/0\ndata['Sex'] = data['Sex'].replace('female', 1)\ndata['Sex'] =data['Sex'].replace('male', 0)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"775d493d-eaab-4a7c-a721-1a7bcc24d0b0","_uuid":"cf7557744f5783832e15bb2dade864fb37832c71"},"cell_type":"markdown","source":"##### Exploring the 'Title' variable & imputing missing Age values\nAge could be a very important variable in predicting survival considering the 'women and children first' rule that seems to have been followed on the Titanic. As such, we want to impute the missing values of Age with a greater degree of precision than we'd have if we just used the mean or median of the entire sample.\n\nLuckily, we have the 'Title' variable.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"25df3edc-5898-4802-9a7e-a0d98afb3ff5","_uuid":"97efb8b9b5183f70a9e4bac7812a15abe633e17b","collapsed":true,"trusted":true},"cell_type":"code","source":"pd.crosstab(data['Title'], 'Count')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"536199fa-4646-4953-892f-db28392e55b4","_uuid":"f01965fcecc08eaa16f20340cd6fc76b7c628a17"},"cell_type":"markdown","source":"Let's start by cleaning up the categories a little. First, some of the titles are non-English versions of English ones. Let's begin by changing them to their English versions.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"b635adcd-0e73-465e-9c9c-d5b397f9827d","_uuid":"368c40b7c1356d8772c81adef0b6930916d5b91b","collapsed":true,"trusted":true},"cell_type":"code","source":"# replace non-English, non-honorific titles with English versions\ndata.Title = data.Title.str.replace('Don', 'Mr')\ndata.Title = data.Title.str.replace('Dona', 'Mrs')\ndata.Title = data.Title.str.replace('Mme', 'Mrs')\ndata.Title = data.Title.str.replace('Ms', 'Mrs')\ndata.Title = data.Title.str.replace('Mra', 'Mrs')\ndata.Title = data.Title.str.replace('Mlle', 'Miss')\n\npd.crosstab(data['Title'], 'Count')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3bae5ff9-df58-4ee0-99f1-da34bd249dad","_uuid":"a13b4252aa54a517a3aa978a70051e1eb9f4b6be"},"cell_type":"markdown","source":"Some titles, like Dr and Rev, we'll create specific indicator variables for.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"d325c06e-885a-4a0e-9f47-c954a7d978a8","_uuid":"40388b5b2b4c3e604e3641311a619473bc9dd3ba","collapsed":true,"trusted":true},"cell_type":"code","source":"data['Title_Dr'] = [1 if title in ['Dr'] else 0 for title in data['Title']]\n\ndata['Title_Rev'] = [1 if title in ['Rev'] else 0 for title in data['Title']]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"47425a27-c1bc-4dba-8c18-4bd7bb390b9b","_uuid":"f32958376e435eb983dbae2fcea928a9e37c08ca"},"cell_type":"markdown","source":"Others, like military or noble titles, we'll group together and create indicators for.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"f3c97f78-fdaf-4b8f-b1bc-3e40f63eed13","_uuid":"cb27019ff4a8b868c6dfd1ae58e3eb63162bbe58","collapsed":true,"trusted":true},"cell_type":"code","source":"militaryTitles = ['Capt', 'Col', 'Major']\ndata['MilitaryTitle'] = [1 if title in militaryTitles else 0 for title in data['Title']]\n\nnobleTitles = ['Jonkheer', 'Lady', 'Sir', 'the Countess']\ndata['NobleTitle'] = [1 if title in nobleTitles else 0 for title in data['Title']]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2aef6ca5-6772-483a-9729-5fbc32dc5019","_uuid":"c1cbc55020958900dc41ebf0f6255f4579a1c818"},"cell_type":"markdown","source":"Aside from those special titles, we have four categories: Master, Mr, Miss, and Mrs. For those with learned (Dr, Rev), military (Capt, Col, Major), or noble (Jonkheer, Lady, Sir, the Countess), we want to put them into one of the four major title categories. We'll start with the masculine titles. Military titles as well as 'Sir' most likely refer to men ('Mr') but 'Jonkheer' is a Dutch honorific that could be applied to a younger male (or, 'Master'). Let's chack the age of our Jonkheer...","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"264b19d9-82e4-42f4-a610-5cbff1f10e10","_uuid":"9df86c8daf8adbccd626c8c612ea041796ab50df","collapsed":true,"trusted":true},"cell_type":"code","source":"data.loc[data['Title'].isin(['Jonkheer'])]['Age']    # 38","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4fd719e0-59d9-441e-b3df-f9673a826238","_uuid":"9b9b8bd0560d97317d43397a0477596f37673d83"},"cell_type":"markdown","source":"Since our Jonkheer is 38 years old, let's include him in the list of titles we change to 'Mr'.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"4eda9d98-03dd-48c8-8ceb-550f6f4dc592","_uuid":"6e63e20fba90510ab72b57d866571f859ce0f1c7","collapsed":true,"trusted":true},"cell_type":"code","source":"# replace male special titles with Mr.\nmale = dict.fromkeys(['Dr','Rev', 'Capt', 'Col', 'Major', 'Jonkheer', 'Sir'], 'Mr')\ndata['Title'] = data.Title.replace(male)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a9344bc3-7a56-4613-8665-0b2a050cce0e","_uuid":"2b24b5bea1ec03ae0766672f642ca78ec8908712"},"cell_type":"markdown","source":"Let's check the ages of the passengers with the female honorifics in Title - 'Lady' and 'the Countess'.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"65c4b60d-5f31-4b27-91d6-418d3df1be6c","_uuid":"1ef7541be379996f707236a9dca5c2a6aba0379f","collapsed":true,"trusted":true},"cell_type":"code","source":"# check ages of the Lady and the Countess\ndata.loc[data['Title'].isin(['Lady'])]['Age']   # 48","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"167b0094-2c14-4cf6-ad6c-f252843b3a3a","_uuid":"f12689576d4ad853892c93e81d6fcf571742ecef","collapsed":true,"trusted":true},"cell_type":"code","source":"data.loc[data['Title'].isin(['the Countess'])]['Age']    # 33","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e77d78a2-2945-4b9d-995e-bb0f2129de2f","_uuid":"1c2cd2f2c66f0a9f0f655945de0dedd0d5a7b1a9"},"cell_type":"markdown","source":"It's reasonable to put both of these in the 'Mrs' category.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"a1cdd611-71f6-4c2a-967f-cb2312bca665","_uuid":"953244f3d6652f2674db96e19cba24419e1d609e","collapsed":true,"trusted":true},"cell_type":"code","source":"# replace Lady and the Countess with Mrs in Title column\nfemale = dict.fromkeys(['Lady', 'the Countess'], 'Mrs')\ndata['Title'] = data.Title.replace(female)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f5c5282f-e848-4f93-bdd0-dc3f85b7f45a","_uuid":"12baed3e8cc9e8f07e6cb3d22b5b02c5612ac29e"},"cell_type":"markdown","source":"Remember that ultimately we'd like to use the Title information to impute missing values of Age. Here's a boxplot showing the distribution of ages in the different Title groups.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"df546b77-d2f9-41fc-8946-a0c251999023","_uuid":"3f9be4dfdc3275bd14f616032e14e6c276446120","collapsed":true,"trusted":true},"cell_type":"code","source":"sns.set_context('talk')\nsns.set_style('darkgrid')\n\n# boxplot of Age by Title\nsns.boxplot(x='Title', y='Age', data=data)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7255a898-202f-4747-8f90-60c3590b118c","_uuid":"ea5020563f918b88dc7e05919b487a6f1c5b303c"},"cell_type":"markdown","source":"The male title groups appear to be distinct, possibly separable distributions. This is even more evident from the histograms.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"4b3a0b11-4b37-4f38-bde3-10ac7064492b","_uuid":"e73d0b198bd89eea67c51ac294c5de0f9e87b26d","collapsed":true,"trusted":true},"cell_type":"code","source":"# two histogram plots, one for males + one for females\ngents = ['Master', 'Mr']\ncolGents = ['blue', 'green']\nfor i in range(len(gents)):\n    a_gents = sns.distplot(data[data['Title'] == gents[i]].Age.dropna(), \n                                label = gents[i],\n                                color = colGents[i])\na_gents.legend()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a115ff02-269c-4c32-8296-f90c576db5d2","_uuid":"c76d8b4fcf311c303fd487d2395851c7704eee7c"},"cell_type":"markdown","source":"The situation is different for the female titles.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"02343c78-ca33-4c33-9ff0-8967205a5904","_uuid":"911beb659b75b94404c9d45c303952a656394cf0","collapsed":true,"trusted":true},"cell_type":"code","source":"ladies = ['Miss', 'Mrs']\ncolLadies = ['orange', 'red']\nfor i in range(len(ladies)):\n    a_ladies = sns.distplot(data[data['Title'] == ladies[i]].Age.dropna(),\n                            label = ladies[i],\n                            color = colLadies[i])\na_ladies.legend()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a1aa3bba-a20a-45bb-a5da-b2ec540ba552","_uuid":"f7373561a891cd622b54f700b7842b0beb9edc9a"},"cell_type":"markdown","source":"Even though the female title distributions overlap considerably, using the median Age of the passenger's Title group to impute missing values seems reasonable.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"99ddc49c-1f57-4082-b9b6-484d7698de8f","_uuid":"579b47f6b711a55811a6c62fc21f346b1c676bbb","collapsed":true,"trusted":true},"cell_type":"code","source":"# Masters\nmasters = pd.DataFrame(data[data.Title == 'Master'].Age, columns = ['Age'])\nmasters.Age = masters.fillna(masters.median())\ndata = data.combine_first(masters)\n\n# Miss's\nmisses = pd.DataFrame(data[data.Title == 'Miss'].Age, columns = ['Age'])\nmisses.Age = misses.fillna(misses.median())\ndata = data.combine_first(misses)\n\n# Mr's\nmisters = pd.DataFrame(data[data.Title == 'Mr'].Age, columns = ['Age'])\nmisters.Age = misters.fillna(misters.median())\ndata = data.combine_first(misters)\n\n# Mrs's\nmissuses = pd.DataFrame(data[data.Title == 'Mrs'].Age, columns = ['Age'])\nmissuses.Age = missuses.fillna(missuses.median())\ndata = data.combine_first(missuses)\n\n# check missing values\ndata.Age.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1b93104b-93d1-4219-a503-19c4538af318","_uuid":"285bcc29497d575f976c6d370347987109bb7a8b"},"cell_type":"markdown","source":"Let's check that Age-Title boxplot again...","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"b23b9b71-07cc-41bd-bdd0-7478c3a838b6","_uuid":"53baea3743505136f34e9ecec772c50d4e48cc08","collapsed":true,"trusted":true},"cell_type":"code","source":"sns.boxplot(x='Title', y='Age', data=data)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"402dc423-8faa-4d34-96b5-4afd0ea3aa20","_uuid":"04113497ba33e134b70d848825b60d74d2f035b3"},"cell_type":"markdown","source":"Now we make some dummy variables for Title and add them to the dataframe.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"22bff58d-b17c-48b9-aced-7fdf8db7c5af","_uuid":"523677371a8d56516417ebd266813527e92e9abf","collapsed":true,"trusted":true},"cell_type":"code","source":"# get dummies\ntitledummies = pd.get_dummies(data['Title'], prefix = 'Title')\n# add to dataset\ndata = pd.concat([data, titledummies], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c2e5db24-6f68-4b6f-ae94-6b14e097756c","_uuid":"be06ea48ec1965c0be4ad529dce9a46491c4f6af"},"cell_type":"markdown","source":"##### Decision tree-based imputation of missing 'Deck' values\nThe 'Deck' variable is missing values for ~1/3 of the total dataset. Let's have a look at a frequency table for the Deck values we *do* have.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"b86934b7-cc71-41b6-b0af-e3b546fbfbfb","_uuid":"e283c2ca3f1ea8edef11f734e72cc9f24420e0fb","collapsed":true,"trusted":true},"cell_type":"code","source":"# crosstab of Deck\npd.crosstab(data['Deck'], columns = 'Count')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"06e5a65a-3bfc-4b18-b8be-b0ceb5ddcb3a","_uuid":"bc7d4cd8df0d81154df7b665d24afe9584a30226"},"cell_type":"markdown","source":"The T Deck looks like an outlier (and it was in a sense). T deck cabin belonged to Mr. Stephen Blackwell, a first class passenger.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"bffc01e8-6ef3-43c9-89c9-3cfe62b64ba1","_uuid":"d18894818f553cb04e4c71a91b9b198923acd4c4","collapsed":true,"trusted":true},"cell_type":"code","source":"data[data.Deck == 'T'].Name","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3cb3a068-163f-4fb6-8ad6-9ef39ae8e1de","_uuid":"435531078f59cebcb3bfcb9197cf02b6493e8af9","collapsed":true,"trusted":true},"cell_type":"code","source":"data[data.Deck == 'T'].Pclass","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a7e8736f-a35c-4f9b-9b4c-35797df74f35","_uuid":"91604d00949211a44f63fb6364c7d147803f718f"},"cell_type":"markdown","source":"For the sake of modeling, let's add Mr. Blackwell to the exclusively 1st Class 'A' Deck.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"2c31cd22-a304-4075-8926-b2a381217ab7","_uuid":"379af325c2a62d1f59d7a1da98069c23a7163949","collapsed":true,"trusted":true},"cell_type":"code","source":"# crosstab of Pclass and Deck\npd.crosstab(data['Deck'], data['Pclass'])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c13a6e76-589f-406d-b92b-9c35f55558c8","_uuid":"3c18e99a9e9740d6d2e75bdaede4f78bdb6db83a","collapsed":true,"trusted":true},"cell_type":"code","source":"data['Deck'] = data.Deck.str.replace('T', 'A')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"01f32901-a8a0-4d0f-a79c-11d56ab979de","_uuid":"d569c1c92aabc1deba458cab74befe1b4e8ee6b1"},"cell_type":"markdown","source":"The majority of 3rd class passengers would have been in steerage with no cabin designation. Let's look at the distribution of Deck among 3rd class passengers.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"2c438fc3-48f3-4040-87a8-3f67234174f3","_uuid":"8274dcb9e0942d82e930486d01985aab9404908b","collapsed":true,"trusted":true},"cell_type":"code","source":"# fill nan's with 'missing'\ndata.Deck = data.Deck.fillna('missing')\n# crosstab\npd.crosstab(data[data['Pclass'] == 3]['Deck'], 'Count', margins = True)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bdfd6c10-2c42-45ea-9cd7-0fbb3722f220","_uuid":"ceba6dcfdaf73a7e1e916804ff87c62ff88ef3de"},"cell_type":"markdown","source":"Let's assume the 693 3rd class passengers missing a Deck were in steerage. We'll create a new category of 'Deck' called 'S' for steerage. All 3rd class passengers with missing Deck values will be assigned to the 'S' class. ","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"4b303efc-c526-4b0a-ab53-85aef3a639a2","_uuid":"452d19418f176510bacac8cd26bc10edd8da88b0","collapsed":true,"trusted":true},"cell_type":"code","source":"data.loc[data['Pclass'] == 3, 'Deck'] = data.loc[data['Pclass'] == 3, 'Deck'].str.replace('missing', 'S')\n\n# crosstab of Pclass and Deck\npd.crosstab(data['Deck'], data['Pclass'], margins = True)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"42e9e584-87e7-410f-84cb-4e16d541fb86","_uuid":"0c0decb5ca00a8afac80a841bb6fba7ce7eca31a"},"cell_type":"markdown","source":"That leaves us with 67 1st Class and 254 2nd Class passengers to find Deck vlaues for. I thought it would be fun to impute these using a decision tree model. First, we create train and test datasets then isolate the inputs and target for the model.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"69bd4903-b8d1-42ed-96fd-7389b652a0d0","_uuid":"8357453ae7a109e8c1a23dd8eef2bc49e0e8794a","collapsed":true,"trusted":true},"cell_type":"code","source":"# isolate train and test subsets for Deck imputation model\ntrain_deck = data[data['Deck'] != 'missing'].drop(['Deck_M', 'Embarked', 'Fare_M',\n                 'FirstName', 'LastName', 'MaidenLastName', 'Name', 'Nickname',\n                 'Survived', 'Test', 'Ticket', 'TicketPrefix', 'Title'], axis = 1)\n\n# drop the ticket prefix indicators for now\ntrain_deck = train_deck[['Age', 'Age_M', 'Alone', 'Deck', 'Embarked_C', 'Embarked_M', 'Embarked_Q', 'Embarked_S', \n                         'FamilySize', 'Fare', 'LargeFamily', 'MaidenLastName_M', 'MilitaryTitle', 'Nickname_M', \n                         'NobleTitle', 'Parch', 'Pclass', 'Sex', 'SibSp', 'SmallFamily', 'Title_Dr', 'Title_Master', \n                         'Title_Mr', 'Title_Mrs', 'Title_Miss']]\n\n# since we only have 1st and 2nd class passengers to predict Deck for, we'll only use 1st and 2nd class passengers to build \n# the model\ntrain_deck = train_deck[train_deck.Pclass != 3] \n\nmissing_deck = data[data['Deck'] == 'missing'].drop(['Deck_M', 'Embarked', 'Fare_M',\n                 'FirstName', 'LastName', 'MaidenLastName', 'Name', 'Nickname',\n                 'Survived', 'Test', 'Ticket', 'TicketPrefix', 'Title'], axis = 1)\n\nmissing_deck = missing_deck[['Age', 'Age_M', 'Alone', 'Deck', 'Embarked_C', 'Embarked_M', 'Embarked_Q', 'Embarked_S', \n                             'FamilySize', 'Fare', 'LargeFamily', 'MaidenLastName_M', 'MilitaryTitle', 'Nickname_M', \n                             'NobleTitle', 'Parch', 'Pclass', 'Sex', 'SibSp', 'SmallFamily', 'Title_Dr', 'Title_Master', \n                             'Title_Mr', 'Title_Mrs', 'Title_Miss']]\n\n# separate into inputs and outputs\nX_train_deck = train_deck.drop('Deck', axis = 1)\nX_missing_deck = missing_deck.drop('Deck', axis = 1)\ny_train_deck = train_deck['Deck']\n\n# feature names\nX_names = X_train_deck.columns","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5474a024-174e-4832-a315-8aa1386d8630","_uuid":"14b225bbfb3ae7ff306932121e6d4ede1aa18c37"},"cell_type":"markdown","source":"Here's a correlation heatmap for the predictors in the Deck training set.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"c9a86111-b2ad-4383-8a6f-7211017323a7","_uuid":"f6de669acd55868b1ab62e7bd193b1c274ce1b1d","collapsed":true,"trusted":true},"cell_type":"code","source":"# take a look at correlations among predictors for Deck\ncorrDeckX = X_train_deck.corr()\n\n# plot the heatmap\nfig, ax = plt.subplots()\nfig.set_size_inches(14, 10)\nsns.set_context('talk')\nsns.set_style('darkgrid')\nsns.heatmap(corrDeckX, \n            xticklabels=corrDeckX.columns,\n            yticklabels=corrDeckX.columns,\n            cmap = 'PiYG')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"70a5ac97-5dde-4c3f-a2ac-d4fdd6df2e38","_uuid":"c86a53fd4d0fcd809cf40ee6f3f62054d30b6408"},"cell_type":"markdown","source":"To tune the parameters of the tree model, we'll use a grid search. First, we construct the grid and the random forest classifier object.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"3be5a013-c80a-46ad-a767-8d86485f3069","_uuid":"48ce9ef3ecfeb739c54a35bfbc7230cfd5210ea7","collapsed":true,"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier \nfrom sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.grid_search import GridSearchCV\n\n# Set up a dict with values to test for each parameter/argument in the model object\ndeck_grid = {'max_depth'         : np.arange(1,30),\n             'min_samples_split' : np.arange(2,20),\n             'min_samples_leaf'  : np.arange(1,20)}\n\n# Construct random forest object\ntree_deck = DecisionTreeClassifier(random_state = 538, \n                                   max_features = 'sqrt',\n                                   presort = True)\ntreeGrid = GridSearchCV(tree_deck, deck_grid)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ee2d0fca-7600-4e17-af80-2f01f900804b","_uuid":"c452c7d82355e72a730759165d96bd8dc2d5aa86","collapsed":true,"trusted":true},"cell_type":"code","source":"# fit trees\ntreeGridFit = treeGrid.fit(X_train_deck,y_train_deck)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b1d49922-af4c-4208-9f73-805937e92019","_uuid":"5945213ac446dfad04189471452e137b3ac5d732","collapsed":true,"trusted":true},"cell_type":"code","source":"treeGrid.best_score_","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"095721ce-c3eb-4b26-b877-7535d84f3686","_uuid":"1e55674aa2b3a43aa106cc0ce12bf9596361a914"},"cell_type":"markdown","source":"Here are the parameters for the model with the highest accuracy on the training set.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"4e02f19d-a65c-4944-bcf0-024cfb6c1670","_uuid":"489ba0b02af84cbb3df5aa0b08993a22c9c64546","collapsed":true,"trusted":true},"cell_type":"code","source":"treeGrid.best_params_","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"cbe762cc-30f8-4ff9-a11b-04613bf24d36","_uuid":"fa3af6ca205fa6084f44c3452f417a0a22d69d15"},"cell_type":"markdown","source":"Now we construct a random forest classifier object with them.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"c88e2006-7539-4da5-bad9-b356f7811912","_uuid":"f1469dd7b37d338ee7112cd51b8b94355c925a43","collapsed":true,"trusted":true},"cell_type":"code","source":"# Build the best tree model\ntreeDeckBest = DecisionTreeClassifier(random_state = 538, \n                                      max_features = 'sqrt',\n                                      presort = True,\n                                      max_depth = 14,\n                                      min_samples_split = 8,\n                                      min_samples_leaf = 1)\n\ntreeDeckBestFit = treeDeckBest.fit(X_train_deck, y_train_deck)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"32eff344-9cae-4af5-962a-40c3c593f6e2","_uuid":"6baa1dd0d369d587170d0732cbd04d80dd2f536c"},"cell_type":"markdown","source":"We can estimate the performance of the model out-of-sample using cross-validation. ","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"f4610989-028e-43bd-be80-a6bbd4bbf768","_uuid":"d44be147c7fca6f7ad62dd32de38633d2e2f224f","collapsed":true,"trusted":true},"cell_type":"code","source":"# calculate test accuracy estimate for best model (use default 3-fold CV)\ncv_error_tree = np.mean(cross_val_score(treeDeckBest, X_train_deck, y_train_deck,\n                                      scoring = 'accuracy'))\nprint('Est. Test Accuracy: ', cv_error_tree)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1ccd5871-b2af-4785-9a56-f788a19b96e6","_uuid":"08d0f72784eac54be172fd906f28cea4f751924a"},"cell_type":"markdown","source":"Even though this model doesn't seem too great (accuracy < 0.5), we replace the missing values of Deck with predictions from the model and add them back to the original dataset.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"89ecd90e-d7d5-452a-8e61-e6cd43e35210","_uuid":"0e3069d244ba35f16af6144046ba44d0191e7a1b","collapsed":true,"trusted":true},"cell_type":"code","source":"# Predict the missing values of Deck\nmissing_deck['Deck'] = treeDeckBest.predict(X_missing_deck)\n\n# replace 'missing' entries in data 'Deck' column with NaNs again\ndata.Deck = data.Deck.replace('missing', np.nan)\n\n# use combine_first to replace missing values in data with imputed values now\n# in missing_deck\ndata = data.combine_first(missing_deck)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2cc7202f-84f1-4251-9584-70e86f30f103","_uuid":"fa02d15f2f39bc86d530ce5c4ac0311879a3b7e5"},"cell_type":"markdown","source":"Here's the contingency table for Deck and Pclass again. We can see that, even though the model had a low accuracy, it distributed the missing values among the 1st and 2nd classes.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"c9569ea7-e707-42a3-9b64-dd097120bf30","_uuid":"c64286cb0e89805a47a9e55a58144086717ddcc6","collapsed":true,"trusted":true},"cell_type":"code","source":"# crosstab of Pclass and Deck\npd.crosstab(data['Deck'], data['Pclass'], margins = True)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e07d3248-2d84-459b-8a2d-cea181d39a0c","_uuid":"48b9ef27976086beb23f219d35711385db9852ee"},"cell_type":"markdown","source":"Taking a look again at the missing values of each variable, we can see that only values for Nickname and MaidenName are missing. (Those missing 'Survived' are the test observations.)","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"59d7db34-e610-4b2c-9c3c-17fa5b19084f","_uuid":"3300776955fa563fc0e3cde3e5de74c24f65fcdd","collapsed":true,"trusted":true},"cell_type":"code","source":"data.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"82e61724-17b5-46d2-a487-f5f176e19650","_uuid":"d912abd813bdf362bd38cbde42eed4704d8df8ae"},"cell_type":"markdown","source":"Finally, we make some dummy variables for the Deck variable and add them to the dataset.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"bac2201b-d93e-418d-8948-05f9366cb0ec","_uuid":"12ec563aaf578d0de3e63a0f6843cbcc08612ccc","collapsed":true,"trusted":true},"cell_type":"code","source":"# Make dummies for the imputed Deck variable\ndeckDummies = pd.get_dummies(data['Deck'], prefix = 'Deck')\ndata = pd.concat([data, deckDummies], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7fced82c-63a6-47b5-a37b-f55581511875","_uuid":"7db7995cfde883eca24bf1efe8d591e1ffac800c","collapsed":true,"trusted":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"89236ad2-0df3-4cc9-b8d1-c4deceb6b7dd","_uuid":"39481bfe4201e39c739880431125b1a6933dccb2"},"cell_type":"markdown","source":"## Model Construction","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"611dfd84-14e0-448c-a01c-919fea431520","_uuid":"96a94c113266fc04c06151f9efcce2943e6657e7"},"cell_type":"markdown","source":"### Final preparations for model building\nWe have some cleaning up and organizing to do before we build models of Survived. First, we'll separate out the test and train datasets and drop some of the variables we don't need.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"073d04cb-2855-495f-89a1-f8ca094d94c0","_uuid":"3d5f295a23597fbf28fc4db499eef8c6148bc6d1","collapsed":true,"trusted":true},"cell_type":"code","source":"# Separate out into train and test sets; drop text variables\ntest_model = data[data['Survived'].isnull()].drop(['Deck', 'Embarked', 'Fare_M',\n            'FirstName', 'LastName', 'MaidenLastName', 'Name', 'Nickname',\n            'Test', 'Ticket', 'TicketPrefix', 'Title', 'Deck_M'], axis = 1)\ntrain_model = data[data['Survived'].notnull()].drop(['Deck', 'Embarked', 'Fare_M',\n            'FirstName', 'LastName', 'MaidenLastName', 'Name', 'Nickname',\n            'Test', 'Ticket', 'TicketPrefix', 'Title', 'Deck_M'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"55f8024d-5cde-4e5f-97a7-ce9f68603f91","_uuid":"fc38a85bd2f24e1ed6137bc960f15053175305e4"},"cell_type":"markdown","source":"Here's the correlation heatmap for the predictors in the training set.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"4014702c-1919-48cb-b3c0-1060b5c382fe","_uuid":"8caf28cc84024432680c1f57388064b4fc195cdd","collapsed":true,"trusted":true},"cell_type":"code","source":"# take a look at correlations among the final set of predictors and the target, Survived\ncorrModelX = train_model.corr()\n\nfig, ax = plt.subplots()\nsns.set_context('talk')\nsns.set_style('darkgrid')\nfig.set_size_inches(15,10)\n\n# plot the heatmap\nsns.heatmap(corrModelX, \n            xticklabels=corrModelX.columns,\n            yticklabels=corrModelX.columns,\n            cmap = 'PiYG')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"60de414c-9026-409a-af33-72b650384a87","_uuid":"26075b891e8bf8daf7490c80f6f348543a3dfa6e"},"cell_type":"markdown","source":"Split into input and output dataframes and define a variable to hold the names of the features.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"cbcc80d6-71c6-4535-ba9d-c41ee0c1e7ca","_uuid":"6bb26e030054231abe425d44bdd97e16b12f088c","collapsed":true,"trusted":true},"cell_type":"code","source":"# separate into inputs and outputs\nX_train = train_model.drop(['Survived'], axis = 1)\ny_train = train_model['Survived']\nX_test = test_model.drop(['Survived'], axis = 1)\n\n# Feature names\nfeatures = X_train.columns","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2762054a-d529-47e1-b3e0-4f8f8359f4fe","_uuid":"7355869c5f59fd346d1b975f2488f51e0a32fb1f"},"cell_type":"markdown","source":"We'll construct a few models so here we initialize a dataframe to hold the results. Along with the model name, the dataframe will contain 'TrainAcc' (the accuracy of the model on the training set) and 'TestAccCVEst' (the cross-validation estimate of the out-of-sample accuracy).","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"513f5d42-1d2f-48c1-8e69-cdb076234c5e","_uuid":"dbfa44d209c39dbd5bcb6fc98d2a611a02c2bc0f","collapsed":true,"trusted":true},"cell_type":"code","source":"# Create a dataframe to hold model results\nmodel_results = pd.DataFrame(columns = ['Model', 'TrainAcc', 'TestAccCVEst'])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"38308b7a-b31b-4e75-a439-794955f6f74b","_uuid":"8af48e50318467315a0babf0b87998ccf421ab42"},"cell_type":"markdown","source":"Finally, we set up some cross-validation schemes using the KFold function from sklearn.model_selection. I've got one for 5-fold and one for 10-fold ross-validation.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"6f6793c3-9d0d-4c20-8ee3-236faf9a070f","_uuid":"9e4fc89ae16ad1251c02f981421a088d8488dc42","collapsed":true,"trusted":true},"cell_type":"code","source":"# set up 5-fold and 10-fold cross-validation schemes for test error estimation\nfrom sklearn.model_selection import KFold\ncv_5fold = KFold(n_splits = 5, shuffle = True, random_state = 237) \n\ncv_10fold = KFold(n_splits = 10, shuffle = True, random_state = 237)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6f3ac618-5b2d-47b5-b81e-f13063a5cb7f","_uuid":"0be34cb85e1b935533cc98f667a08a136f37c90f"},"cell_type":"markdown","source":"### Model 1: L1-Penalized Logistic Regression","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"e0ea5068-3a06-402f-b807-8c284ac2d9d1","_uuid":"dd8189840530917a60e30f6291f9e40efe0cec65"},"cell_type":"markdown","source":"L1-regularization can be used for variable selection as it shrinks the coefficients of some variables to 0. I try two different functions from the sklearn.linear_model package to do regularized logistic regression: LogisticRegression and SGDClassifier. For the first - LogisticRegression - I'll build an L1-penalized model. For the second, I'll construct and ElasticNet model that combines L1 and L2 (ridge) regularization.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"2700e3eb-eacd-433a-aaa5-b2304f41b790","_uuid":"fb993343bce548be05cd5ad732166a1be7d5e985","collapsed":true,"trusted":true},"cell_type":"code","source":"# First, LogisticRegression:\nfrom sklearn.linear_model import LogisticRegression\nlr1 = LogisticRegression(penalty = 'l1',\n                         random_state = 555,\n                         solver = 'liblinear')\n\n# fit the model\nlr1_fit = lr1.fit(X_train, y_train)\n\n# make predictions on the training set\nlr1_preds = lr1.predict(X_train)\n\n# confusion matrix for training set\nfrom sklearn.metrics import confusion_matrix, classification_report\nconfusion_matrix(y_train, lr1_preds)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"090bd7e8-1774-4e99-90c0-ff1fb2e66df5","_uuid":"e50abf018a3bb3a506601a41064f1aebfd634f95"},"cell_type":"markdown","source":"Here's a classification report...","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"aea2cfb3-962a-4739-905e-c17d4b71741e","_uuid":"347e4fd8de0af02c39247c340504dc71872dbfa8","collapsed":true,"trusted":true},"cell_type":"code","source":"# classification report\nprint(classification_report(y_train, lr1_preds,\n                            target_names = ['Died', 'Survived']))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0c6e8a66-afe4-4665-87b3-267c78a216ab","_uuid":"b17b36ad2e7f409ac33be90c7b3071ad4c05f4a2"},"cell_type":"markdown","source":"For this model, 'precision' can be thought of as the ability of the model to pick out the true survivors and not label passengers as survivors when they in fact perished. 0.81 means that 81% of the passengers that the model labeled as survivors were actually survivors.\n\n'Recall' in this case would be the ability of the model to pick out all the survivors. 0.76 in this case means that the model was able to correctly identify 76% of the survivors. \n\nLet's explicitly calculate the in-sample training accuracy...","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"1b48f2d9-3e5e-400f-bf6a-0ef31bbe6a8d","_uuid":"978b0ea28dbff922fdc1621ab83604dd83ad5913","collapsed":true,"trusted":true},"cell_type":"code","source":"# training accuracy\nlr1_trainAcc = lr1.score(X_train, y_train) # 0.8395\nlr1_trainAcc","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9fd6cb8e-a9ed-4854-ba51-2649fcd8d2cf","_uuid":"51f285daf3e913e794bb9a56f212114c10f80fed"},"cell_type":"markdown","source":"...and make an estimate of the out-of-sample test accuracy using 5-fold cross-validation.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"20495b84-6c56-49f8-a63b-6e2b4976f75d","_uuid":"a8aa9a50d413bca36893cdc47afcd751fc897b07","collapsed":true,"trusted":true},"cell_type":"code","source":"# calculate test accuracy estimate\nlr1_testErrEst = np.mean(cross_val_score(lr1, X_train, y_train,\n                                         scoring = 'accuracy', \n                                         cv = cv_5fold))\nprint('Est. Test Accuracy of Best Model: ', lr1_testErrEst)    # 0.8283","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ec6ecc4b-6029-4102-a2a0-20012f591eda","_uuid":"497d21d2516f2cdca5cd7c203b82b73ea6c3c170"},"cell_type":"markdown","source":"As mentioned earlier, L1 regularization shrinks some of the coefficients to exactly zero and in this way does variable selection. Here's a list of the predictors and their coefficients.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"62e07b90-9d9d-484e-bc25-3568bedf117d","_uuid":"dc8da3fbdd39ab1bfdf16325455895eb5e735bd8","collapsed":true,"trusted":true},"cell_type":"code","source":"# construct a coefficent table\nlr1_coefs = [coef for coef in lr1.coef_[0]]\nfeaturesList = list(features)\nlr1_coefs = pd.DataFrame(list(zip(featuresList, lr1_coefs)), columns = ['Feature', 'Coef'])\nlr1_coefs","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bbf6a35b-03c0-45ee-9e30-04c771d93a63","_uuid":"4222fa055223b26ba5593263721a3a3fa0bae743"},"cell_type":"markdown","source":"Let's get a list of the non-zero coefficients.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"47d33f47-fcb6-468a-a883-b11538f11f18","_uuid":"5b87bf882d34fe45e9a162d46901a2b87e300c6d","collapsed":true,"trusted":true},"cell_type":"code","source":"lr1_nonzeroCoef = lr1_coefs[lr1_coefs.Coef != 0]\nlr1_nonzeroCoef","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"aa41c746-386e-401b-abf5-7323154dbb0a","_uuid":"f407d32f47c37420bf8f37f8254e5df629e701a6"},"cell_type":"markdown","source":"This model appears to put the strongest emphasis on Sex, Title_Master, ticket prefixes STONO and SWPP, and Deck_E.\n\nLet's add the results to the model_results dataframe that we'll use to compare models.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"d90c270d-299e-4318-9f49-cccb73afe882","_uuid":"145cb26e58edd7d13ef6e9f9372f4c7b961f64c4","collapsed":true,"trusted":true},"cell_type":"code","source":"# add metrics to model_results dataframe\nlr1_results = pd.DataFrame([['l1LogReg', lr1_trainAcc, lr1_testErrEst]],\n                           columns = ['Model', 'TrainAcc', 'TestAccCVEst'])\nmodel_results = model_results.append(lr1_results)\nmodel_results","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6931e46f-bd3a-467e-9a88-6fccf6240822","_uuid":"be6660ed3c4c863be1b18772f7295ed72ee39dbf"},"cell_type":"markdown","source":"These models are built for Kaggle's Titanic competition so the below code makes predictions on the test set and outputs the results to a csv file for submission.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"0f5053cf-b812-46ed-a7bc-acaae2f63146","_uuid":"23c2bcb393dc071c776fab214e11eb6dbec6556b","collapsed":true,"trusted":true},"cell_type":"code","source":"# make predictions on test set\nlr1_test = pd.DataFrame(lr1.predict(X_test).astype(int), \n                        columns = ['Survived'],\n                        index = test.index)\n\n# write to csv\nlr1_test.to_csv('L1lr_test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a9b8d74a-268c-4459-900c-6947950df951","_uuid":"5ff35cd0b102ce5cae80b66a445427591f620014"},"cell_type":"markdown","source":"### Model 2: ElasticNet\nsklearn.linear_model's SGDClassifier function can be used to produce a number of different linear models (logistic regression, SVMs), all with training by stochastic gradient descent. ElasticNet models are linear models that combine L1 and L2 regularization. The ratio between them is a tunable parameter as is 'alpha', the regularization parameter that controls the total amount of penalty applied to the linear model. \n\nHere I use a grid search to find optimal parameters for both alpha and the L1-to-L2 ratio.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"75ba2d45-ff37-419e-abc7-cf7570f8d9c2","_uuid":"18e67b08a8e454fbe6b37d21cefde197d2214ea1","collapsed":true,"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import SGDClassifier\n\n# Set up a dict with values to test for each parameter/argument in the model object\nen1_grid = {'alpha'    : [0.005, 0.01, 0.015], \n            'l1_ratio' : np.arange(0, 1, 0.05)}\n\n# SGD Classifier object; log loss makes this logistic regression\nen1 = SGDClassifier(loss = 'log',\n                    penalty = 'elasticnet',\n                    random_state = 237,\n                    learning_rate = 'optimal',\n                    max_iter = 500)\n\n# set up the grid search\nen1_GridSearch = GridSearchCV(en1, en1_grid)\n\n# fit trees\nen1_fit = en1_GridSearch.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b64f4848-6338-4822-8352-8c0c459f1538","_uuid":"aecf67a55bd182f845ffe9ce520ee708604b2730"},"cell_type":"markdown","source":"Let's look at the score and the parameters for the 'best' model.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"57c72308-ca26-41dd-9db4-782c646a8802","_uuid":"52aa89e0e350ae54246afe203950c87795145fc5","collapsed":true,"trusted":true},"cell_type":"code","source":"en1_GridSearch.best_score_","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5ab77dcb-50e1-4a6a-92d6-650ebdd8fee0","_uuid":"f7fe9464ab4909c39d14e33ad8c22a693f1a47c0","collapsed":true,"trusted":true},"cell_type":"code","source":"en1_GridSearch.best_params_","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"cd1cf489-50e0-4317-b26d-c2d913e9b37a","_uuid":"8f165da19eef4af7e5d97b26da07beab6a03c10a"},"cell_type":"markdown","source":"Construct and fit the 'best' elasticnet model.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"c98beb1f-f893-48d3-a79b-c62d0f1fccb0","_uuid":"b28e2d9404952f5ddeebecc3cabdf131dd4ef8e1","collapsed":true,"trusted":true},"cell_type":"code","source":"# best model\nen1_best = SGDClassifier(alpha = 0.005,\n                         l1_ratio = 0.15,\n                         loss = 'log',\n                         penalty = 'elasticnet',\n                         random_state = 237,\n                         learning_rate = 'optimal',\n                         max_iter = 500)\n\nen1BestFit = en1_best.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9a907af4-8dd5-4018-83cf-9b247675f4cd","_uuid":"a8138b1f4dc50d0780542b9ba718995c2925c700"},"cell_type":"markdown","source":"Make predictions on the training set and construct a confusion matrix...","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"3c196de0-98ec-490f-a992-56d6eec42ac0","_uuid":"5646289ef19c3cd2683655af882cda1e462f5af4","collapsed":true,"trusted":true},"cell_type":"code","source":"# make prediction on the training set\nen1_preds = en1_best.predict(X_train)\n\n# confusion matrix for training set\nfrom sklearn.metrics import confusion_matrix, classification_report\nconfusion_matrix(y_train, en1_preds)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c712a00e-4d46-4a02-9370-d4b2d1b02888","_uuid":"622b05f48afe42e6e676b02a2bd962ec993a97b1"},"cell_type":"markdown","source":"...and a classification report...","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"d404313e-a24f-4764-b478-3dd2870c29f3","_uuid":"3737c85d4c2d393abe702de96f69de90458d4c15","collapsed":true,"trusted":true},"cell_type":"code","source":"# classification report\nprint(classification_report(y_train, en1_preds,\n                            target_names = ['Died', 'Survived']))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"62148f2a-e20f-4f71-a16b-eac1ce12a412","_uuid":"53c05c9b8b3fd3b87fc9092d891c1767f901847c"},"cell_type":"markdown","source":"This model appears to do a slightly better job at picking out the true survivors. Let's get the training and estimated test accuracy to include in model_results...","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"10dfd371-9c4a-4769-b4dd-93da87a56beb","_uuid":"5daa8a556fe2e6f5a55fd103248a767cd7845c7f","collapsed":true,"trusted":true},"cell_type":"code","source":"# training accuracy\nen1_trainAcc = en1_best.score(X_train, y_train)\n\n# calculate test accuracy estimate for best model using 5-fold CV\nen1_testErrEst = np.mean(cross_val_score(en1_best, X_train, y_train,\n                                        scoring = 'accuracy',\n                                        cv = cv_5fold))\nprint('Est. Test Accuracy of Best Model: ', en1_testErrEst)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"32b87de6-8ae0-4331-96f9-d96e60b2f1de","_uuid":"4592f918dc8b4aee7ca3c083655b82ae03efda2d","collapsed":true,"trusted":true},"cell_type":"code","source":"# add metrics to model_results dataframe\nen1_results = pd.DataFrame([['ElasticNet', en1_trainAcc, en1_testErrEst]],\n                           columns = ['Model', 'TrainAcc', 'TestAccCVEst'])\nmodel_results = model_results.append(en1_results)\nmodel_results","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e2469cca-a11f-4d67-abcc-28a7f0b8d6ff","_uuid":"6567c4b75b41cf37165e864acaf881bbb569f81a"},"cell_type":"markdown","source":"Finally, make predictions and write to csv...","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"18bb20ca-8ef7-45a8-b615-5d78321f2da4","_uuid":"7a56d9506b4d53fe0b068b9b119c21a8524154fb","collapsed":true,"trusted":true},"cell_type":"code","source":"# make predictions on test set\nen1_test = pd.DataFrame(en1_best.predict(X_test).astype(int), \n                       columns = ['Survived'],\n                       index = test.index)\n\n# write to csv\nen1_test.to_csv('en1_test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0700334a-8f76-4241-b929-d849d3a8f833","_uuid":"fbb90b308e36e86c54ab461985bbbd4a8ba092bd"},"cell_type":"markdown","source":"### Model 3: Random Forest\nThe random forest model is a favorite for the Titanic problem for several reasons. Serveral of the predictors in the dataset are correlated with each other. Such multicollinearity can cause problems for linear models, like Models 1 and 2. Random forests can handle highly correlated predictors since they are an ensemble of smaller tree models that use only a few predictors each.\n\nSeveral parameters can be adjusted inan attempt to optimize a random forest model. Here we create a grid of parameter values to test and fit models using them.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"568ab27a-c9b4-4ca4-83e5-e20de7ec7c4e","_uuid":"4e43211f70ad0d1cf7dc326c711f6db3a5c532ab","collapsed":true,"trusted":true},"cell_type":"code","source":"# Set up a dict with values to test for each parameter/argument in the model object\nrf1_grid = {'n_estimators'      : [20, 50, 100],\n            'max_depth'         : np.arange(1,5),\n            'min_samples_split' : np.arange(6,20,2),\n            'min_samples_leaf'  : np.arange(3,15,3),\n            'max_leaf_nodes'    : [5, 10, 15]}\n\n# Construct random forest object\nfrom sklearn.ensemble import RandomForestClassifier\nrf1 = RandomForestClassifier(random_state = 555)\n\n# set up the grid search\nrf1_GridSearch = GridSearchCV(rf1, rf1_grid)\n\n# fit trees\nrf1_fit = rf1_GridSearch.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6b2819f3-c9ec-4f4b-9547-71130265b499","_uuid":"007781b33f1731fe2734ef646b271c14a949a36e"},"cell_type":"markdown","source":"The best score in the grid search...","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"7f8294df-444f-47cb-be65-46d8d890b801","_uuid":"d5ab2eaccfabd81c304367b1cea5be743cabe359","collapsed":true,"trusted":true},"cell_type":"code","source":"rf1_GridSearch.best_score_","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f1307a05-4d3a-47f8-93e9-ed59d2da4e7c","_uuid":"ad09196699d75917b7afff31d4bc5a4e130aa458"},"cell_type":"markdown","source":"The parameters of the model with that score...","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"8289e177-b2a1-4dfa-b906-890d9899e6a4","_uuid":"a2745d21bcfcef5821e354619ed772565b8fba3d","collapsed":true,"trusted":true},"cell_type":"code","source":"rf1_GridSearch.best_params_","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"acc4c378-6594-41b7-b729-470d8189fda3","_uuid":"0ec4db60e7c28ecd51406cd82c9c47e664dcefe0"},"cell_type":"markdown","source":"Construct the best model and fit it to the training set...","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"3353969d-72e3-411c-8e67-3283c0c85c03","_uuid":"02d9f61c8582241de3e71ed7f8fa6567691069f5","collapsed":true,"trusted":true},"cell_type":"code","source":"# Build the best RF model with hyperparameters determined above\nfrom sklearn.ensemble import RandomForestClassifier\nrf1_best = RandomForestClassifier(max_depth = 3,\n                                  max_leaf_nodes = 15,\n                                  min_samples_leaf = 3,\n                                  min_samples_split = 18,\n                                  n_estimators = 20)\n\nr1fBestFit = rf1_best.fit(X_train, y_train)\n\n# make prediction on the training set\nrf1_preds = rf1_best.predict(X_train)\n\n# confusion matrix for training set\nconfusion_matrix(y_train, rf1_preds)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"596b382c-196c-42f3-a015-7a32c29a54b5","_uuid":"b75ce98c3a611a5489f9fb49e090cc830b865369"},"cell_type":"markdown","source":"Classification report...","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"598abf30-c376-4f9d-83ec-5e11e310dff7","_uuid":"ac440345bdbbd8a96c99b06abe6fcf9150bcb410","collapsed":true,"trusted":true},"cell_type":"code","source":"# classification report\nprint(classification_report(y_train, rf1_preds,\n                            target_names = ['Died', 'Survived']))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3cccc1f1-4e48-4cbd-8f51-60942588c101","_uuid":"2abb6dcdfa2c3762874a5e34b09a1531129ed219"},"cell_type":"markdown","source":"Get the in-sample and estimated out-of-sample accuracies...","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"083bcb2d-5938-4fc4-be9d-38a66187ff6a","_uuid":"7c15140ac32c1c2f2a6b5481cd966e10c525eaf7","collapsed":true,"trusted":true},"cell_type":"code","source":"# training accuracy\nrf1_trainAcc = rf1_best.score(X_train, y_train)\n\n# calculate test accuracy estimate for best model using 5-fold CV\nrf1_testErrEst = np.mean(cross_val_score(rf1_best, X_train, y_train,\n                                        scoring = 'accuracy',\n                                        cv = cv_5fold))\nprint('Est. Test Accuracy of Best Model: ', rf1_testErrEst)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"67e61533-10b9-42fe-ad46-872364a95d2c","_uuid":"08e211cea580eab104f27044147597036f2c37e9"},"cell_type":"markdown","source":"Add to model_results...","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"3e7d2cc7-f8b9-4a24-97ba-1226b3957cb0","_uuid":"46140c9ed8f6c1f8750229ae4a24141ab3e2936b","collapsed":true,"trusted":true},"cell_type":"code","source":"# add metrics to model_results dataframe\nrf1_results = pd.DataFrame([['RandomForest', rf1_trainAcc, rf1_testErrEst]],\n                           columns = ['Model', 'TrainAcc', 'TestAccCVEst'])\nmodel_results = model_results.append(rf1_results)\n\nmodel_results","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c8edc0ab-a9a9-4c84-8585-e3f3882829d7","_uuid":"6567cf8eb3b82b4e1f590a2e9c415734a68dace6"},"cell_type":"markdown","source":"Make predictions on the test set and output csv...","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"5d00eefd-b63b-4435-b5f2-20af03d336dc","_uuid":"514f8028252e219c68c6f9bc0dbf24ab563e7b95","collapsed":true,"trusted":true},"cell_type":"code","source":"# make predictions on test set\nrf1_test = pd.DataFrame(rf1_best.predict(X_test).astype(int), \n                       columns = ['Survived'],\n                       index = test.index)\n\n# write to csv\nrf1_test.to_csv('rf1_test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e6670892-32d7-4f08-a19e-0fa7af1f7e17","_uuid":"431ce85e43a84fa6ba6e10307745650f2e27bb02"},"cell_type":"markdown","source":"Random forest models have the added benefit of providing variable importance information. We can make a bar graph of variable importances.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"c66244e3-91cf-479b-a5d0-66bfbe4a4460","_uuid":"94b9b0add7bdd4e56a9e196760a067a8448fbbb3","collapsed":true,"trusted":true},"cell_type":"code","source":"# look at feature importances\nrf1_importances = rf1_best.feature_importances_\n\nheaders_rf1 = [\"Variable\", \"Importance\"]\nvalues_rf1 = pd.DataFrame(sorted(zip(X_train.columns, rf1_importances), key=lambda x: x[1] * -1), columns = headers_rf1)\n\n# horizontal bar plot of importances\nfig, ax = plt.subplots()\nfig.set_size_inches(12, 13)\nsns.set_context('talk')\nsns.set_style('darkgrid')\nsns.barplot(x = 'Importance', y = 'Variable', data = values_rf1, orient = 'h', color = 'green')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a0fcbdf3-00e8-42a7-b5bf-d040c389eb78","_uuid":"db9f5fb30f5a72eca779f9b6a37c3af38c391583"},"cell_type":"markdown","source":"### Model 4: Gradient Boosted Tree Classifier\nGradient boosting is a way to improve the predictive ability of an ensemble model through recursive addition and weighting of the weak learners averaged to construct it. In this case, we recursively construct small tree models using only a few randomly-chosen features. Each tree is added to the ensemble after being weighted based on it's individual accuracy. More accurate weak learners are weighted more and therefore contribute more to final model.\n\nAs with the random forest ensemble model, there are several parameters that require tuning in order to find the settings that produce the 'best' version of the classifier.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"7bafb1b1-38ab-4409-80ca-d64b0220337a","_uuid":"af57974247afc91fb441939af9e2a51d935274a7","collapsed":true,"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\n# Set up a dict with values to test for each parameter/argument in the model\n# object\ngbc_grid = {'n_estimators'        : np.arange(20,100,20),\n            'learning_rate'       : [0.05, 0.1, 0.15],\n            'max_features'        : np.arange(1,6),\n            'max_depth'           : np.arange(1,4),\n            'min_samples_split'   : np.arange(10,20,5),\n            'min_samples_leaf'    : np.arange(3,21,7),\n            'subsample'           : [0.3, 0.5, 0.7],\n            'max_leaf_nodes'      : np.arange(5,20,5)}\n\n\n# Construct random forest object\ngbc = GradientBoostingClassifier(random_state = 555)\n\n# set up the grid search\ngbc_GridSearch = GridSearchCV(gbc, gbc_grid)\n\n# fit learners\ngbc_fit = gbc_GridSearch.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4130af71-7b94-40e9-b343-a2b88bc4ceed","_uuid":"e7a1b55553a1f982eedc6e6077c4f1cd3e28ab86"},"cell_type":"markdown","source":"Best score...","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"55f029b7-287a-46b4-a2c9-0d235318fd7c","_uuid":"3cd30bda1ed423d51b0f95d04f3155ab2e0d9174","collapsed":true,"trusted":true},"cell_type":"code","source":"gbc_GridSearch.best_score_","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4e7e524f-6ba6-46d5-a484-7d6b9bf904aa","_uuid":"88de0cc6b80b3a5087c431edcda4cb6deb685e65"},"cell_type":"markdown","source":"Parameters for the 'best' model...","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"b9418692-06a5-4592-a5ca-4e19b5717e1b","_uuid":"721db116d1bbd74d74b163236d79f0aff0eb0ca9","collapsed":true,"trusted":true},"cell_type":"code","source":"gbc_GridSearch.best_params_","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ebd25934-d2e6-469f-93ff-94205420f829","_uuid":"7c467f9932f6c802d716caaad31c0a8e22fa8293"},"cell_type":"markdown","source":"Fit the best model and produce a confusion matrix","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"dfbca4c0-7a27-41ed-89eb-52f08302c7e9","_uuid":"8b25c6c7a1c5d6558e2505e84bf771dead15326f","collapsed":true,"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\ngbc_best = GradientBoostingClassifier(n_estimators = 40,\n                                      learning_rate = 0.15,\n                                      max_depth = 3,\n                                      max_features = 5,\n                                      min_samples_leaf = 3,\n                                      min_samples_split = 10,\n                                      max_leaf_nodes = 10,\n                                      subsample = 0.5,\n                                      random_state = 555)\n\ngbcBestFit = gbc_best.fit(X_train, y_train)\n\n# make prediction on the training set\ngbc_preds = gbc_best.predict(X_train)\n\n# confusion matrix for training set\nconfusion_matrix(y_train, gbc_preds)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6432026b-d4fe-4517-8533-db98032636b9","_uuid":"7eb6144f904ba22ad02eb37ca5cb07affd5ec9e1"},"cell_type":"markdown","source":"Classification report...","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"93b40b90-8691-4ea2-b4de-84e874d7a0be","_uuid":"9bc51ef661aa0c70fbfe7e7ea3001d248e3c89f0","collapsed":true,"trusted":true},"cell_type":"code","source":"# classification report\nprint(classification_report(y_train, gbc_preds,\n                            target_names = ['Died', 'Survived']))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3547b546-7347-4c9e-acc9-6a5d3d871a97","_uuid":"32cc37171503048273d04cce5f99ff45bff8a9a7"},"cell_type":"markdown","source":"Training and estimated test accuracy...","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"0143f7cd-7e56-46d6-8927-265561367180","_uuid":"f058d9a1d71e69b024e34575d0f8d7428cd14201","collapsed":true,"trusted":true},"cell_type":"code","source":"# training accuracy\ngbc_trainAcc = gbc_best.score(X_train, y_train)\n\n# calculate test accuracy estimate for best model using 5-fold CV\ngbc_testErrEst = np.mean(cross_val_score(gbc_best, X_train, y_train,\n                                         scoring = 'accuracy',\n                                         cv = cv_5fold))\nprint('Est. Test Accuracy of Best Model: ', gbc_testErrEst)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d840f4f4-d605-436a-905b-ef430bff33b6","_uuid":"38e1d7e1e1790c80940506bf50ce31339805df19"},"cell_type":"markdown","source":"Add metrics to model_results...","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"54ed20a3-6ba9-4c80-8fc6-dc5dd0a95191","_uuid":"9c6d1f13a01ae37296fd3334502d1b0b8b2092c0","collapsed":true,"trusted":true},"cell_type":"code","source":"# add metrics to model_results dataframe\ngbc_results = pd.DataFrame([['GradientBoostedTree', gbc_trainAcc, \n                             gbc_testErrEst]],\n                             columns = ['Model', 'TrainAcc', 'TestAccCVEst'])\nmodel_results = model_results.append(gbc_results)\n\nmodel_results","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"feaa81cf-6d9b-4d16-8727-5a9a8a262d4c","_uuid":"74833d85244a9d9c3ad202a54ef84c6ed54fb9db"},"cell_type":"markdown","source":"Make predictions on the test set and write them to csv...","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"7bb9e468-38e8-46fd-8c07-13418c839edf","_uuid":"b7001149095c9bf10beed3a6ef272c13f6c155bc","collapsed":true,"trusted":true},"cell_type":"code","source":"# make predictions on test set\ngbc_test = pd.DataFrame(gbc_best.predict(X_test).astype(int), \n                        columns = ['Survived'],\n                        index = test.index)\n\n# write to csv\ngbc_test.to_csv('gbc_test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0edb5f7b-a8f8-4971-9fb3-2dc4865ede47","_uuid":"0fded2f5e3198fb7364ff7258f5fea1b94e32e88"},"cell_type":"markdown","source":"As with the random forest model, a gradient boosted classifier can provide us with information about variable importance. Here's a graph of those importances.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"59ee5420-4815-457d-9fb7-98f7e8863c60","_uuid":"8ff07550578a0dc66aea32357d3e3c8a5131b03a","collapsed":true,"trusted":true},"cell_type":"code","source":"# look at feature importances\ngbc_importances = gbc_best.feature_importances_\n\nheaders_gbc = [\"Variable\", \"Importance\"]\nvalues_gbc = pd.DataFrame(sorted(zip(X_train.columns, gbc_importances), key=lambda x: x[1] * -1), columns = headers_gbc)\n\n# horizontal bar plot of importances\nfig, ax = plt.subplots()\nfig.set_size_inches(12, 13)\nsns.set_context('talk')\nsns.set_style('darkgrid')\nsns.barplot(x = 'Importance', y = 'Variable', data = values_gbc, orient = 'h',color = 'green')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ce284de3-eaa7-4241-b0ab-023907e25e45","_uuid":"cfbe9608a454fe5655f64fc927bcf770f1161b9c"},"cell_type":"markdown","source":"It is interesting to note the differences between the variable importances from the random forest model and the gradient boosted tree. The random forest seems to put more emphasis on a passenger's sex as a predictor of survival while the gradient boosted tree sees age and fare/class as most important.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"28d95b26-4ab4-44bb-9eec-c8ee3d686030","_uuid":"aeeb3d6f88faef6373aaebd146f60e3b959d62a5"},"cell_type":"markdown","source":"### Model 5: Gradient Boosted Tree with Reduced Parameter Set\nThe models built so far seem to have a problem with *overfitting*. That is, they learn to predict survivor status on the training set *so well* that they have a difficult time with other data. One potential issue is that there are predictors in the models that aren't associated with survival status at all. These predictors only contribute noise. In a sense, an overfitting model 'learns' the noise and that can diminish the model's performance on data other than that with which it was trained.\n\nTo explore this possibility, I've constructed a second gradient boosted tree model on a subset of the predictors used to build the previous four models. The subset consists of the top 25 most important variables as determined from the previous gradient boosted model.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"88d84b38-a288-46dd-a771-fd91d5e60a88","_uuid":"c02bd9fc6abaaab6b3418d1016e7caed3e76ab77","collapsed":true,"trusted":true},"cell_type":"code","source":"# let's take the top 25 predictors\nreducedFeatures = values_gbc.Variable.head(25)\n\n# Extract only those features from the test and train sets\ntest_model_red = test_model[reducedFeatures]\ntrain_model_red = train_model[reducedFeatures]\ntrain_model_red.loc[:,'Survived'] = train_model['Survived']","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3583d0b3-1b0c-4abd-a3e7-94bb0eed9824","_uuid":"e8a352eda9e9d73e4f453b17f1a7f7d1a1848486"},"cell_type":"markdown","source":"Let's look at the correlations between this new predictor set and Survived...","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"9e0961c6-cc0e-4978-880d-311b5f08217a","_uuid":"e5438c63c893b537d0eedf427b9a7ba08a2f1a06","collapsed":true,"trusted":true},"cell_type":"code","source":"# Correlations in the new training set\ncorrRedModelX = train_model_red.corr()\n\nfig, ax = plt.subplots()\nsns.set_context('talk')\nsns.set_style('darkgrid')\nfig.set_size_inches(15,10)\n\n# plot the heatmap\nsns.heatmap(corrRedModelX, \n            xticklabels=corrRedModelX.columns,\n            yticklabels=corrRedModelX.columns,\n            cmap = 'PiYG')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"27824879-818e-46d6-9ec0-f59bd60fb6df","_uuid":"11cec58867d585f0a255298990146ab5e594bdd0"},"cell_type":"markdown","source":"Separate into inputs and outputs...","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"ab86f0ac-5caf-4fd6-99c9-821fc0db4763","_uuid":"6b5714c380544ab5e54c3501e6ac184b70fb7a21","collapsed":true,"trusted":true},"cell_type":"code","source":"# separate into inputs and outputs\nX_train_red = train_model_red.drop(['Survived'], axis = 1)\ny_train_red = train_model_red['Survived']\nX_test_red = test_model_red","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"32d4771a-e42e-4400-baaf-b2873954daa3","_uuid":"97a9cc22f0e1444a77355cc8543894aaa0abba3a"},"cell_type":"markdown","source":"Do our grid search for good parameters...","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"5a55d14c-d64e-45f5-a6e8-dfeab8091252","_uuid":"af382f1ca96be81f3fe6cbcda7aad7c08249a1b4","collapsed":true,"trusted":true},"cell_type":"code","source":"# Set up a dict with values to test for each parameter/argument in the model\n# object\ngbcRed_grid = {'n_estimators'        : [50, 75, 100],\n               'learning_rate'       : [0.05, 0.1, 0.15],\n               'max_features'        : np.arange(1,4),\n               'max_depth'           : np.arange(1,4),\n               'min_samples_split'   : np.arange(8,20,4),\n               'min_samples_leaf'    : [5, 7, 9],\n               'subsample'           : [0.3, 0.5, 0.7],\n               'max_leaf_nodes'      : [10, 15, 20]}\n\n# Construct random forest object\ngbcRed = GradientBoostingClassifier(random_state = 555)\n\n# set up the grid search\ngbcRed_GridSearch = GridSearchCV(gbcRed, gbcRed_grid)\n\n# fit trees\ngbcRed_fit = gbcRed_GridSearch.fit(X_train_red,y_train_red)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e382e7db-b180-4c13-aa56-6d9b78bdeec2","_uuid":"3472b7bcdcb97f5221fc85ec15ac00df862449b2"},"cell_type":"markdown","source":"Best score from the grid search...","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"ad135da6-bab0-4655-b1c0-c01a715b803e","_uuid":"d264090dd390ca0439cd42edf3ca3b522796a302","collapsed":true,"trusted":true},"cell_type":"code","source":"gbcRed_GridSearch.best_score_","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4f17c1bc-5cac-415a-bd9b-2a688c4b6eb2","_uuid":"68d8a2ba061fd52070e47b19c2c2d2b8dcc51dcc"},"cell_type":"markdown","source":"Parameters of the 'best' model...","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"8978a599-fb5f-4a80-aeb1-475aa0036fe1","_uuid":"654dec60462ef37c6346047e8be4894747a8e9e5","collapsed":true,"trusted":true},"cell_type":"code","source":"gbcRed_GridSearch.best_params_","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d09f700b-2e50-43a9-8192-f0e964bbed5e","_uuid":"85e6b69ee3600385946c5f9a5f86d4969adbfe3a"},"cell_type":"markdown","source":"Construct the 'best' model, fit it, make predictions on the training set, and produce a confusion matrix..","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"c89c20c2-5b80-4bdc-953c-01a27dfb622f","_uuid":"ef43ff60d193159eac63e9c977997454d92c53de","collapsed":true,"trusted":true},"cell_type":"code","source":"# Build the best model\ngbcRed_best = GradientBoostingClassifier(n_estimators = 80,\n                                         learning_rate = 0.15,\n                                         max_depth = 3,\n                                         max_features = 4,\n                                         min_samples_leaf = 3,\n                                         min_samples_split = 15,\n                                         max_leaf_nodes = 10,\n                                         subsample = 0.7,\n                                         random_state = 555)\n\ngbcRedBestFit = gbcRed_best.fit(X_train_red, y_train_red)\n\n# make prediction on the training set\ngbcRed_preds = gbcRed_best.predict(X_train_red)\n\n# confusion matrix for training set\nconfusion_matrix(y_train_red, gbcRed_preds)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"37da627f-b27e-41e5-b934-dca81a391669","_uuid":"ddc67f20b4da133cc6d9512768d473b19a12b66b"},"cell_type":"markdown","source":"Classification report...","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"58bfd911-39a2-4fbb-93e5-182ffdec294e","_uuid":"df2333f363bdc6a85a778463799b3cd411336628","collapsed":true,"trusted":true},"cell_type":"code","source":"# classification report\nprint(classification_report(y_train_red, gbcRed_preds,\n                            target_names = ['Died', 'Survived']))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e654457d-c1e5-4a0b-a778-851fb5172cfa","_uuid":"e5dd6599b7bef7663ed2082fc6722e3345db1597"},"cell_type":"markdown","source":"This model appears to be the best preforming in terms of precision and recall on the training set. \n\nHere's the training and estimated test accuracy...","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"03fc4afa-7ceb-487a-b7bd-9011718153c9","_uuid":"d83ef8214d2eab8685c1c62fc7514be6ca01340f","collapsed":true,"trusted":true},"cell_type":"code","source":"# training accuracy\ngbcRed_trainAcc = gbcRed_best.score(X_train_red, y_train_red)\n\n# calculate test accuracy estimate for best model using 1-fold CV\ngbcRed_testErrEst = np.mean(cross_val_score(gbcRed_best, X_train_red, y_train_red,\n                                         scoring = 'accuracy',\n                                         cv = cv_5fold))\nprint('Est. Test Accuracy of Best Model: ', gbcRed_testErrEst)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"52d11bb2-3d96-46ab-9399-c9ba1379cdce","_uuid":"fe407f2797da2e27ce5e454dc4a43a4c7951dd58"},"cell_type":"markdown","source":"Add metrics to model_results...","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"becc4c52-02a8-4b94-9e23-87cc97344f42","_uuid":"6769427a4cd4867249035e424e425a746c50335f","collapsed":true,"trusted":true},"cell_type":"code","source":"# add metrics to model_results dataframe\ngbcRed_results = pd.DataFrame([['GBC_ReducedX', gbcRed_trainAcc, \n                                 gbcRed_testErrEst]],\n                              columns = ['Model', 'TrainAcc', 'TestAccCVEst'])\nmodel_results = model_results.append(gbcRed_results)\n\nmodel_results","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3e43803d-8878-45f0-a2f9-99f091ff0635","_uuid":"7d43911f94fa94e3b0d94c9d3440a98ee1c7e68e"},"cell_type":"markdown","source":"Make predictions on the test set and write to csv...","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"f2050faa-babd-4941-a190-581bae871488","_uuid":"6c49ae82b8f304754ee03d81cc8db1ff59fed0bd","collapsed":true,"trusted":true},"cell_type":"code","source":"# make predictions on test set\ngbcRed_test = pd.DataFrame(gbcRed_best.predict(X_test_red).astype(int), \n                        columns = ['Survived'],\n                        index = test.index)\n\n# write to csv\ngbcRed_test.to_csv('gbcRed_test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c76aed77-86d8-4632-98b7-5a149aed7d87","_uuid":"5fcf361a52a868bf3f2a255531db8d6010c03c75"},"cell_type":"markdown","source":"The variable importances from the boosted model on the reduced dataset...","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"1f5eada1-2084-4717-ab21-67421b2eff56","_uuid":"03a60eacaaea6529c78e827eadf4ff73a7a869be","collapsed":true,"trusted":true},"cell_type":"code","source":"# look at feature importances\ngbcRed_importances = gbcRed_best.feature_importances_\n\nheaders_gbcRed = [\"Variable\", \"Importance\"]\nvalues_gbcRed = pd.DataFrame(sorted(zip(X_train_red.columns, gbcRed_importances), key=lambda x: x[1] * -1), \n                             columns = headers_gbcRed)\n\n# horizontal bar plot of importances\nfig, ax = plt.subplots()\nfig.set_size_inches(12, 13)\nsns.set_context('talk')\nsns.set_style('darkgrid')\nsns.barplot(x = 'Importance', y = 'Variable', data = values_gbcRed, orient = 'h', color = 'green')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"45b91417-809b-44a7-8d96-3796c9333bf3","_uuid":"ae7cc84ad1c6fc74f232ac53aeb572b31375f587"},"cell_type":"markdown","source":"### Final Model Comparison\nConveniently, we've saved the in- and estimated out-of-sample accuracy metrics for each of the five models in the dataframe, model_results. We'll add a column for the public leaderboard results.","outputs":[],"execution_count":null},{"metadata":{"_cell_guid":"0559f147-c31a-445c-85fa-9a4ab7b1681a","_uuid":"cca47d4636ab5ad235f468bb8586d9e906b2e055","collapsed":true,"trusted":true},"cell_type":"code","source":"# Add a column with Kaggle public leaderboard results\nmodel_results['PublicLeaderboard'] = [0.77511, 0.77950, 0.79425, 0.77033, 0.75598]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ccde33e7-7509-4f9b-96aa-455d6197c6fb","_uuid":"6ed14265b08727bd5453e005734a05d50d6b2311","collapsed":true,"trusted":true},"cell_type":"code","source":"model_results","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}