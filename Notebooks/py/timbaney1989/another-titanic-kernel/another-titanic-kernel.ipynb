{"metadata": {"language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "mimetype": "text/x-python", "file_extension": ".py", "name": "python", "nbconvert_exporter": "python", "version": "3.6.1", "pygments_lexer": "ipython3"}, "kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}}, "nbformat_minor": 2, "cells": [{"metadata": {"_uuid": "8456bb1740a9cec758c99dce6fff840955f2e29b", "_cell_guid": "4fae6a3a-3717-48ac-a0e0-34cfbfe02ead"}, "execution_count": null, "outputs": [], "cell_type": "markdown", "source": "<div style=\"padding: 15px; color: white; background: url('http://lukehoney.typepad.com/.a/6a00e54ef13a4f88340168e9b04e0a970c-pi') no-repeat bottom; background-size: cover; height: 200px; position: relative;\">\n<div style=\"position: absolute; left: 0; right: 0; top: 0; bottom: 0; background: black; opacity: .2;\"></div>\n<h1 style=\"font-size: 34px; position: absolute; left: 15px; top: 15px; margin: 0;\">Titanic Machine Learning - Kaggle</h1>\n<p style=\"font-size: 20px; position: absolute; left: 15px; top: 40px; color: white; line-height: 1px; font-style: italic; font-weight: bold;\">Timothy Baney</p>\n</div>"}, {"metadata": {"_uuid": "8336855ec55687d003332cffa255127c13b156be", "_cell_guid": "912d7be6-6e97-43a0-b699-097c0f1805a3"}, "execution_count": null, "outputs": [], "cell_type": "markdown", "source": "## <p id=\"intro\">Introduction</p>\nThe sinking of the RMS Titanic in 1912 is considered to one of the worst commecial maritime disasters ever. 2,224 passengers of all ages, class, and demographic were on their way to New York, a lot emigrating to the United States for a better life.3 hours After the Titanic's collision with an iceberg sometime around midnight, the Titanic had completely submerged, killing almost 75% of all the passengers on board. Claimed to be unsinkable, the decision was made to not put an adequate amount of lifeboats on board to save space on the deck, as they were thought to be redundant. This plus the crew of the ship not filling the lifeboats to full capacity led to the devestating number of casualties. What kind of factors played a role in your survival ? Was it all about speed, whoever got to the lifeboats first got to survive, or did the crew discriminate against poorer passengers ? We will try to discover all of this, and more. \n\n\n\n## <p id=\"intro\">Import Libraries</p>"}, {"metadata": {"_execution_state": "idle", "trusted": false, "_uuid": "46f1fb844c074a598758e8bc89b3c58aab0e485a", "_cell_guid": "4617c762-ba32-4434-a3a3-d2637e3e3f93", "collapsed": false}, "execution_count": null, "outputs": [], "cell_type": "code", "source": "%matplotlib inline\n\nfrom datetime import datetime as dt\nimport re\nimport math\nimport datetime\n\nimport pylab\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport matplotlib.pylab as pylab\n\nimport numpy as np\n\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n\nfrom sklearn import datasets, tree, metrics, cross_validation\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.neighbors import KNeighborsClassifier \nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\nfrom sklearn.linear_model import LogisticRegression, LinearRegression, Perceptron, SGDClassifier\nfrom sklearn.feature_selection import SelectKBest, chi2, VarianceThreshold, RFE\nfrom sklearn.svm import SVC\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\nimport scipy\n\npylab.rcParams[ 'figure.figsize' ] = 15 , 8\nplt.style.use(\"fivethirtyeight\")"}, {"metadata": {"_uuid": "d03aeb05b7d5f2944095dd0c893f43618fd798e7", "_cell_guid": "825ee242-103a-4674-a5c8-87624d97ceee"}, "execution_count": null, "outputs": [], "cell_type": "markdown", "source": "Load in training and test csv files"}, {"metadata": {"_execution_state": "idle", "trusted": false, "_uuid": "01d30f3f2dc3145de1fc9a32660edc96ff65236d", "_cell_guid": "604cdbde-0f49-4234-9dc1-012b92ffd6ff", "collapsed": true}, "execution_count": null, "outputs": [], "cell_type": "code", "source": "train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/train.csv')"}, {"metadata": {"_uuid": "78b100f7c6b8b1f7525634c31dcda72c810ec40f", "_cell_guid": "6380981b-883d-4b79-bb0a-c0f12b2ac15c"}, "execution_count": null, "outputs": [], "cell_type": "markdown", "source": "For reference, lets create a data dictionary with the name of the column, example data from that column, the meaning behind it, and the data type. This will help us get a good understanding of what direction were going to go in while starting our feature engineering."}, {"metadata": {"_execution_state": "idle", "trusted": false, "_uuid": "2711ef4e80aab70eb1ae5a5add290e83bbf4b480", "_cell_guid": "46d1f715-60bc-4128-895f-4c40699d1041", "collapsed": false}, "execution_count": null, "outputs": [], "cell_type": "code", "source": "col_meanings = [\n    'The ID of the passenger',\n    'Did the passenger survive ? 1 = Yes, 0 = No',\n    'Ordinal Value for passenger class, 1 being the highest',\n    'Name',\n    'Gender',\n    'Age',\n    'Passenger\\'s siblings and spouses on board with',\n    'Passenger\\'s parents and children on board',\n    'Ticket Number',\n    'Passenger Fare',\n    'Cabin Number',\n    'Port of Embarkation'\n]\n\ndata_dict = pd.DataFrame({\n    \"Attribute\": train.columns,\n    \"Type\": [train[col].dtype for col in train.columns],\n    \"Meaning\": col_meanings,\n    'Example': [train[col].iloc[2] for col in train.columns]\n})\n\ndata_dict"}, {"metadata": {"_uuid": "5fcc36a8e7c2618c0d01456bb51a462d7f52d09f", "_cell_guid": "54e1f573-f420-4559-8d5a-52009e8779c7"}, "execution_count": null, "outputs": [], "cell_type": "markdown", "source": "### <p>Port of Embarkation</p>\nWhen the RMS Titanic set sail in 1912, it first docked at 3 different ports in 3 different countries to pick up passengers. The first port, where the bulk of the passengers came from, was Southampton England. The RMS Titanic than stopped by Cherbourg France, and lastly picked up the remaining passengers from Queenstown Ireland, which today is known as Cobh Ireland. Lets take a look at the Ticket feature of the data, and see if we can make any discoveries about it's connection to survival rate.\n\n<img src=\"http://upload.wikimedia.org/wikipedia/commons/thumb/5/51/Titanic_voyage_map.png/660px-Titanic_voyage_map.png\" style=\"width: 100%;\">"}, {"metadata": {"_uuid": "6e6d49d3588abd45abac43d9ab9ece6849b45b06", "_cell_guid": "38de4cb3-6606-43e0-ac3d-e18286ecc75a"}, "execution_count": null, "outputs": [], "cell_type": "markdown", "source": "To dive deeper into the possible significance of a ticket code, we will first have to extrapolate a unique list of prefixes from all passengers"}, {"metadata": {"_execution_state": "idle", "trusted": false, "_uuid": "0d36c0dc228d5726517ac58b218f010604397078", "_cell_guid": "b865a4da-84a4-4f43-aade-826fc14ac9dd", "collapsed": false}, "execution_count": null, "outputs": [], "cell_type": "code", "source": "prefix_dict = {}\ncleaned_list = []\n\nfor raw in [item.split(' ')[0] for item in [pre for pre in train['Ticket'].value_counts().index.tolist() if not pre.isalnum()]]:\n    cleaned = re.sub(r'\\W+', '', raw)\n    \n    if raw not in prefix_dict:\n        prefix_dict[raw] = cleaned\n        \n    if cleaned not in cleaned_list:\n        prefix_dict[cleaned] = raw\n        cleaned_list.append(cleaned)\nprint(cleaned_list)"}, {"metadata": {"_uuid": "5bf367f5ea859d3dc1d2aa5c7ea15210b009b829", "_cell_guid": "ac65adfc-6d9d-458d-944a-d8004fc0d76a"}, "execution_count": null, "outputs": [], "cell_type": "markdown", "source": "### Ticket Price"}, {"metadata": {"trusted": false, "_execution_state": "idle", "_uuid": "9d9030916d735bdb865176ebd987197696674848", "scrolled": false, "_cell_guid": "d7907d34-8f24-4d30-bad0-453ada6a8ec9", "collapsed": false}, "execution_count": null, "outputs": [], "cell_type": "code", "source": "pre_list_fare = cleaned_list\n\ndef getMeans(prefix_list):\n    clean_means = []\n    for pre in prefix_list:\n        matches = [x for x in prefix_dict if prefix_dict[x] == pre]\n        if len(matches) == 1:\n            mean = train[train['Ticket'].str.contains(matches[0])]['Fare'].mean()\n        elif len(matches) == 2:\n            mean = train[(train['Ticket'].str.contains(matches[0])) | (train['Ticket'].str.contains(matches[1]))]['Fare'].mean()\n        elif len(matches) == 3:\n            mean = train[(train['Ticket'].str.contains(matches[0])) | (train['Ticket'].str.contains(matches[1])) | (train['Ticket'].str.contains(matches[2]))]['Fare'].mean()\n        elif len(matches) == 4:\n            mean = train[(train['Ticket'].str.contains(matches[0])) | (train['Ticket'].str.contains(matches[1])) | (train['Ticket'].str.contains(matches[2])) | (train['Ticket'].str.contains(matches[3]))]['Fare'].mean()\n        clean_means.append(mean)\n    \n    clean_means.append(train[train['Ticket'].str.isdigit()]['Fare'].mean())\n        \n    return clean_means\n\nx = pre_list_fare\ny = getMeans(pre_list_fare)\n\nif 'Non Alpha' not in pre_list_fare:\n    pre_list_fare.append('Non Alpha')"}, {"metadata": {"_execution_state": "idle", "trusted": false, "_uuid": "60afe58206d4d7fa33dd596ab6346c10ea1adb5e", "_cell_guid": "0c753843-5d4e-4e06-838f-cdcb403df4a7", "collapsed": false}, "execution_count": null, "outputs": [], "cell_type": "code", "source": "sns.barplot(x, y)\nplt.title('Type of Ticket Avg Fare')\nplt.ylabel('Price of Ticket')\nplt.xlabel('Ticket Prefix')\nplt.xticks(rotation=60)\nplt.show()"}, {"metadata": {"_uuid": "c83fd25979833201d65eec9142c85545a363268d", "_cell_guid": "721e4fd8-9d93-4ecc-aed1-13ad47f632ef"}, "execution_count": null, "outputs": [], "cell_type": "markdown", "source": "There is currently no information or surviving documents about what the id of Titanic tickets mean. However it appears as though tickets with a prefix of PC are much more expensive than others. Perhaps if we take a look at the number of people from each port of emarkation for each prefix it will shine some light on what the ticket code means. To do this we must first use one hot encoding to turn all the non ordinal categorical data into boolean value columns. We will need to look at any values in those columns that are null first and decide what to do with them. **It is important to perform feature engineering on the test set while you perfrom it on the training set so it is compatible when you test a model on it later on, \" *clean while you cook.* **\""}, {"metadata": {"_execution_state": "idle", "trusted": false, "_uuid": "e2370c672254fb2c8b2c1a823a82049b5717144a", "_cell_guid": "a4973198-4c87-4e7f-aaab-25b566ea4f31", "collapsed": false}, "execution_count": null, "outputs": [], "cell_type": "code", "source": "gender_oh = pd.get_dummies(train['Sex']) \ngender_oh_test = pd.get_dummies(test['Sex'])\n\ntrain = train.drop('Sex', axis=1)\ntrain = train.join(gender_oh)\ntrain = train.rename(columns={'female': 'Female', 'male': 'Male'})\n\ntest = test.drop('Sex', axis=1)\ntest = test.join(gender_oh_test)\ntest = test.rename(columns={'female': 'Female', 'male': 'Male'})\n\nembarked_oh = pd.get_dummies(train['Embarked'])\nembarked_oh_test = pd.get_dummies(test['Embarked'])\n\ntrain = train.drop('Embarked', axis=1)\ntrain = train.join(embarked_oh)\ntrain = train.rename(columns={'C': 'Cherbourg', 'Q': 'Queenstown', 'S': 'Southampton'})\n\ntest = test.drop('Embarked', axis=1)\ntest = test.join(embarked_oh_test)\ntest = test.rename(columns={'C': 'Cherbourg', 'Q': 'Queenstown', 'S': 'Southampton'})"}, {"metadata": {"_uuid": "24b670dd7ba6f27f42bc1893fbcfdcc52b061cb8", "_cell_guid": "c9f40194-a69a-4f68-8b0a-7f979c408a5f"}, "execution_count": null, "outputs": [], "cell_type": "markdown", "source": "### Ticket by Port"}, {"metadata": {"_execution_state": "busy", "trusted": false, "_uuid": "e6e962caa9676d4aa6ffbea9f58ee9644ef6e899", "_cell_guid": "59a7bdaf-a652-4c07-b9c9-2d4380c36441", "collapsed": false}, "execution_count": null, "outputs": [], "cell_type": "code", "source": "fig, axes = plt.subplots(5, 6,figsize=(20, 15))\nfig.legend_out = True\n\npre_port_list = cleaned_list\n\ndef graphPortTickets(prefix_list):\n    col, row, loop = (0, 0, 0)\n    for pre in prefix_list:\n        row = math.floor(loop/6)\n        \n        matches = [x for x in prefix_dict if prefix_dict[x] == pre]\n        if len(matches) == 1:\n            df = train[train['Ticket'].str.contains(matches[0])]\n        elif len(matches) == 2:\n            df = train[(train['Ticket'].str.contains(matches[0])) | (train['Ticket'].str.contains(matches[1]))]\n        elif len(matches) == 3:\n            df = train[(train['Ticket'].str.contains(matches[0])) | (train['Ticket'].str.contains(matches[1])) | (train['Ticket'].str.contains(matches[2]))]\n        elif len(matches) == 4:\n            df = train[(train['Ticket'].str.contains(matches[0])) | (train['Ticket'].str.contains(matches[1])) | (train['Ticket'].str.contains(matches[2])) | (train['Ticket'].str.contains(matches[3]))]\n\n        df_c = df['Cherbourg'].sum()/df['PassengerId'].count()\n        df_q = df['Queenstown'].sum()/df['PassengerId'].count()\n        df_s = df['Southampton'].sum()/df['PassengerId'].count()\n        \n        x = ['Cherbourg', 'Queenstown', 'Southampton']\n        y = [df_c, df_q, df_s]\n        \n        ax = sns.barplot(x, y, ax=axes[row, col])\n        ax.set_xticks([])\n        axes[row, col].set_title('-{}- by Port'.format(pre))\n        \n        col += 1\n        loop += 1\n    \n        if col == 6:\n            col = 0\n    \n    non_alpha = train[train['Ticket'].str.isdigit()]\n    na_c = non_alpha['Cherbourg'].sum()/non_alpha['PassengerId'].count()\n    na_q = non_alpha['Queenstown'].sum()/non_alpha['PassengerId'].count()\n    na_s = non_alpha['Southampton'].sum()/non_alpha['PassengerId'].count()\n    \n    y = [na_c, na_q, na_s]\n    \n    ax = sns.barplot(x, y, ax=axes[row, col])\n    ax.set_xticks([])\n    axes[row, col].set_title('No Prefix by Port')\n            \ngraphPortTickets(pre_port_list)"}, {"metadata": {"_uuid": "db361784c4ddc237bcabd0ce20d039bb7b93cd65", "_cell_guid": "1df271e2-05ab-4bdf-b80d-3b3cbe390e32"}, "execution_count": null, "outputs": [], "cell_type": "markdown", "source": "From Looking at the number of tickets with certain prefixes comes from what port of embarkation, we can make a few observations. One being that a vast majority of all passengers boarded the Titanic in Southampton as mentioned earlier, and had tickets with identification containing all kinds of prefixes. The wealthiest passengers boarded mainly in France, and England while the poorest boarded in Ireland. This makes sense because many people boarding the Titanic from Ireland were poor immigrants looking to emigrate to America, and Ireland wasn't as wealthy, and prosperous as countries like England, and France at the time. Anybody boarding in France had a ticket with prefix of 'PC' or no prefix at all."}, {"metadata": {"_uuid": "6720366ca055740d22a83783da35e99216e17f13", "_cell_guid": "3110543a-c6ce-4af2-a648-dee11ba77d56"}, "execution_count": null, "outputs": [], "cell_type": "markdown", "source": "### Ticket by Survival Rate\n\nlastly we will compare each ticket prefix with the number of people who survived with that ticket."}, {"metadata": {"_execution_state": "busy", "trusted": false, "_uuid": "a65176a5c03d6de0c359c4b2f425dd3d9954ecab", "_cell_guid": "a5f76366-394b-4717-a72b-9c6ea7f87fe3", "collapsed": false}, "execution_count": null, "outputs": [], "cell_type": "code", "source": "pre_list = cleaned_list\n\ndef getTicketSurvivalPerc(prefix_list):\n    survived_count = []\n    for pre in prefix_list:\n        matches = [x for x in prefix_dict if prefix_dict[x] == pre]\n        if len(matches) == 1:\n            df = train[train['Ticket'].str.contains(matches[0])]\n            survived = df['Survived'].sum()/df['PassengerId'].count()\n        elif len(matches) == 2:\n            df = train[(train['Ticket'].str.contains(matches[0])) | (train['Ticket'].str.contains(matches[1]))]\n            survived = df['Survived'].sum()/df['PassengerId'].count()\n        elif len(matches) == 3:\n            df = train[(train['Ticket'].str.contains(matches[0])) | (train['Ticket'].str.contains(matches[1])) | (train['Ticket'].str.contains(matches[2]))]\n            survived = df['Survived'].sum()/df['PassengerId'].count()\n        elif len(matches) == 4:\n            df = train[(train['Ticket'].str.contains(matches[0])) | (train['Ticket'].str.contains(matches[1])) | (train['Ticket'].str.contains(matches[2])) | (train['Ticket'].str.contains(matches[3]))]\n            survived = df['Survived'].sum()/df['PassengerId'].count()\n        \n        survived_count.append(survived)\n    \n    survived_count.append(train[train['Ticket'].str.isdigit()]['Survived'].sum()/train[train['Ticket'].str.isdigit()]['PassengerId'].count())\n        \n    return survived_count\n\ndef xWithTotalNumber(x_inp):\n    matches = [z for z in prefix_dict if prefix_dict[z] == x_inp]\n    if len(matches) == 1:\n        total = train[train['Ticket'].str.contains(matches[0])]['PassengerId'].count()\n    elif len(matches) == 2:\n        total = train[(train['Ticket'].str.contains(matches[0])) | (train['Ticket'].str.contains(matches[1]))]['PassengerId'].count()\n    elif len(matches) == 3:\n        total = train[(train['Ticket'].str.contains(matches[0])) | (train['Ticket'].str.contains(matches[1])) | (train['Ticket'].str.contains(matches[2]))]['PassengerId'].count()\n    elif len(matches) == 4:\n        total = train[(train['Ticket'].str.contains(matches[0])) | (train['Ticket'].str.contains(matches[1])) | (train['Ticket'].str.contains(matches[2])) | (train['Ticket'].str.contains(matches[3]))]['PassengerId'].count()\n        \n    x_str = '{} - {}'.format(x_inp, total)\n    return x_str\n                        \ny = getTicketSurvivalPerc(pre_list)[:-1]\n\nnew_x = [xWithTotalNumber(i) for i in pre_list[:-1]]\n\nif 'Non Alpha' not in new_x:\n    new_x.append('Non Alpha')"}, {"metadata": {"_execution_state": "busy", "trusted": false, "_uuid": "94b66408cfb09eddccfe79bb9aed372700662472", "_cell_guid": "19f406ea-5e0a-42b6-9fc8-91738fa021d2", "collapsed": false}, "execution_count": null, "outputs": [], "cell_type": "code", "source": "sns.barplot(new_x, y)\nplt.title('Survival Percentage by Ticket')\nplt.ylabel('Percent Survived')\nplt.xlabel('Type of Ticket')\nplt.xticks(rotation=75)\nplt.show()"}, {"metadata": {"_uuid": "6ea1dad6fa1e4210a3119eab0b72417440ef14f3", "_cell_guid": "f4f1059a-75fb-4403-8399-a7a8a7d4d113"}, "execution_count": null, "outputs": [], "cell_type": "markdown", "source": "As expected the probability of surviving for passengers that hold a ticket with the prefix **PC** on it have a relatively high chance of surviving. The interesting thing to note however is that tickets with a prefix beginning with FC, or PP, have an even higher chance of surviving. However there are only 6 people with a ticket beginning with FC in the training data, and only 7 people for tickets containing PP. For now anyone who has a ticket with a prefix with a percent of survival higher than 60%, and with more than 5 tickets, will receive a 1 under a new column named **Golden Ticket** before I drop the ticket table."}, {"metadata": {"_execution_state": "busy", "trusted": false, "_uuid": "d6768015650cb170f206909f850359e74e6257a6", "_cell_guid": "6719b4a0-713f-47b9-8fed-f6362861c6a5", "collapsed": false}, "execution_count": null, "outputs": [], "cell_type": "code", "source": "def golden(row):\n    if 'PC' in row['Ticket'] or 'PP' in row['Ticket'] or 'F.C.' in row['Ticket'] or 'FC' in row['Ticket']:\n        return 1\n    else:\n        return 0\n        \n    return row['Ticket'].str.contains('PC')\n    \ntrain['Golden Ticket'] = train.apply(lambda x: golden(x), axis=1)\ntest['Golden Ticket'] = test.apply(lambda x: golden(x), axis=1)\n\ntrain = train.drop('Ticket', axis=1)\ntest = test.drop('Ticket', axis=1)"}, {"metadata": {"_uuid": "0315e8ae25abf40d2203a51899efa2070169f342", "_cell_guid": "df256215-6e78-4ea2-8168-92319f55e276"}, "execution_count": null, "outputs": [], "cell_type": "markdown", "source": "### <p>Cabin</p>\n\n<img src=\"https://www.encyclopedia-titanica.org/files/1/figure-one-side-view.gif\" style=\"width: 100%;\">"}, {"metadata": {"_uuid": "dccef64bb2306c24bc3a7d489f2a0d43f1270ab4", "_cell_guid": "5e2b0b4c-2e8b-4e0e-b036-e834db3833e9"}, "execution_count": null, "outputs": [], "cell_type": "markdown", "source": "On April 14th, 1912 at about 10 to 11 pm, the Titanic made contact with the infamous iceberg that would eventually lead to its demise 2 hours later. The RMS Titanic consisted of seven decks, A through G, which the cabin letter in the data corresponds to. C103 for instance would be on the third deck down in room 103. Converting the cabin into an ordinal value might be of some help since most of the passengers would be sleeping, or about to sleep in their cabins when the Titanic started to sink. This means it would be more accurate to say that passengers who booked a cabin closer to the top (A - C) would've gotten to the top deck the fastest, and gotten a lifeboat the soonest. It should also be noted that there is a 'T' value as well for 'Tank Tops'. This is where the crew for the engine, and boilers stayed."}, {"metadata": {"trusted": false, "_execution_state": "busy", "_uuid": "1623b0120f4b79816e8049f43d96a63d35a11ced", "scrolled": false, "_cell_guid": "6e736df9-d9f9-42bf-a107-42dc07714cd0", "collapsed": false}, "execution_count": null, "outputs": [], "cell_type": "code", "source": "cabin_dict = {\n    'A': 1,\n    'B': 2,\n    'C': 3,\n    'D': 4,\n    'E': 5,\n    'F': 6,\n    'G': 7,\n    'T': 8\n}\n\ntrain['Cabin'] = train['Cabin'].fillna(value='G')\ntest['Cabin'] = test['Cabin'].fillna(value='G')\n\ndef getCabinOrd(row):\n    cabin = row['Cabin']\n    deck = cabin[0]\n    return cabin_dict[deck]\n\ntrain['Cabin_Ord'] = train.apply(lambda x: getCabinOrd(x), axis=1)\ntest['Cabin_Ord'] = test.apply(lambda x: getCabinOrd(x), axis=1)\n\ntrain = train.drop(['Cabin', 'PassengerId'], axis=1)\ntest = test.drop('Cabin', axis=1)"}, {"metadata": {"_uuid": "88df741054d1eb5ba39a7033423098c02a7369c5", "_cell_guid": "e2326b24-5c6d-43bb-bd1a-1ba801f4d789"}, "execution_count": null, "outputs": [], "cell_type": "markdown", "source": "### Age Handling\nNow that all the data is an int or float, I am going to use linear regression to predict the age of the passengers that have missing age values. One last thing to take care of before we do this is the name. A lot can be taken from a name, the suffix, the prefix, just how it sounds e.g. John Smith versus Felipe Santiago. We will look for any suffixes like Sr. or Jr., and than we will look for any other title or prefix, like Mr., Mrs. or Captain. Instead of filling in the age data with the average value of **ALL** passengers, we will break it down by name suffix average."}, {"metadata": {"_execution_state": "busy", "trusted": false, "_uuid": "92e0ecf907b1277a23e155e0e174efaaf203e7b2", "_cell_guid": "b55dae13-bcce-487f-92ff-ecc5e07551e9", "collapsed": false}, "execution_count": null, "outputs": [], "cell_type": "code", "source": "def refineName(row):\n    if 'Mrs' in row['Name']:\n        return 'Mrs'\n    elif 'Mr.' in row['Name']:\n        return 'Mr'\n    elif 'Miss' in row['Name']:\n        return 'Miss'\n    elif 'Master' in row['Name']:\n        return 'Master'\n    else:\n        return 'Other'\n\ntrain['Name'] = train.apply(lambda x: refineName(x), axis=1)\ntest['Name'] = test.apply(lambda x: refineName(x), axis=1)"}, {"metadata": {"_uuid": "3fc8919bcec4c93c36d25d31cb1fac0f191efcb7", "_cell_guid": "7a40a1b2-8736-4c44-8a49-e043d85e161c"}, "execution_count": null, "outputs": [], "cell_type": "markdown", "source": "Turn Categorical Names into One Hot Encoding Values."}, {"metadata": {"_execution_state": "busy", "trusted": false, "_uuid": "7a6299074f0e43b7b1cce9c057c0e7043748fd17", "_cell_guid": "51d295d5-c4f1-4e30-a9ad-d28aa46352b2", "collapsed": false}, "execution_count": null, "outputs": [], "cell_type": "code", "source": "suffixes = pd.get_dummies(train['Name'])\ntest_suffixes = pd.get_dummies(test['Name'])\n\ntrain = train.join(suffixes, lsuffix='left', rsuffix='right')\ntrain = train.drop('Name', axis=1)\n\ntest = test.join(test_suffixes, lsuffix='left', rsuffix='right')\ntest = test.drop('Name', axis=1)\n\ntrain.head(1)"}, {"metadata": {"_uuid": "97605eb6be0597da03595d02368cd1ff42b19fb9", "_cell_guid": "7b95298e-20d9-4ddd-81e5-d551fcc71944"}, "execution_count": null, "outputs": [], "cell_type": "markdown", "source": "Peform Multivariate Linear Regression on data to find missing values"}, {"metadata": {"_execution_state": "busy", "trusted": false, "_uuid": "457e5da12a37d8b15a5519fee5de924aac622b75", "_cell_guid": "930ead13-a6b4-4d6c-8320-6fbeac87e563", "collapsed": false}, "execution_count": null, "outputs": [], "cell_type": "code", "source": "avg_test_fare = test['Fare'].mean()\ntest['Fare'] = test['Fare'].fillna(value=avg_test_fare)\n\nwith_age = train[train['Age'] > 0]\n\nno_age = train[train['Age'].isnull()]\nno_age = no_age.drop('Age', axis=1)\n\ntest_with_age = test[test['Age'] > 0]\n\ntest_no_age = test[test['Age'].isnull()]\ntest_no_age = test_no_age.drop('Age', axis=1)\n\nty = test_with_age['Age'].values\ntx = test_with_age.drop('Age', axis=1)\n\ny = with_age['Age'].values\nx = with_age.drop('Age', axis=1)\n\nlinreg = LinearRegression()\ntest_lin = LinearRegression()\n\nlinreg.fit(x, y)\ntest_lin.fit(tx, ty)\n\npredictions = [abs(math.ceil(pred)) for pred in linreg.predict(no_age)]\ntest_predictions = [abs(math.ceil(pred)) for pred in test_lin.predict(test_no_age)]\n\ntrain.head()"}, {"metadata": {"_execution_state": "busy", "trusted": false, "_uuid": "07f37cf32a74391a573716b51438b468dd2a3ddf", "_cell_guid": "f2da5991-a31a-46b6-b84c-674e81d54517", "collapsed": false}, "execution_count": null, "outputs": [], "cell_type": "code", "source": "no_age['Age'] = predictions\ntest_no_age['Age'] = test_predictions\n\ntrain = with_age.append(no_age)\ntest = test_with_age.append(test_no_age)"}, {"metadata": {"trusted": false, "_execution_state": "busy", "_uuid": "2587d5ffcae68b2cfea9a282270b7bcd693e6d2e", "scrolled": false, "_cell_guid": "c366e321-3deb-4a95-80dc-9f8a5dd55ee0", "collapsed": false}, "execution_count": null, "outputs": [], "cell_type": "code", "source": "colormap = plt.cm.viridis\nplt.figure(figsize=(12,12))\nplt.title('Feature correlations', y=1.05, size=15)\nsns.heatmap(train.corr(),linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True)"}, {"metadata": {"_uuid": "f9dfae9ed6a7fbe21003b504213df02949a1e666", "_cell_guid": "bce4530a-4b19-4b33-96f3-3652ade4faa7"}, "execution_count": null, "outputs": [], "cell_type": "markdown", "source": "After looking at the logistic regression coefficients, and heat map of correlation between features, age seems to be coming out strikingly low. The sibling/spouse, and parent/child columns are also weak in correlation. Lets try and narrow down those features by getting two boolean value columns, isKid, and hasFamily. We may also be able to see if turning 'Fare' into a boolean value might strengthen its correlation with survival. Also since, not being a female means that you are a male, we can drop the male column since it is redundant."}, {"metadata": {"_execution_state": "busy", "trusted": false, "_uuid": "82901652a6c193f22bda803301f8459dd9ad5790", "_cell_guid": "c0f6ccbc-19bf-4ed2-ba05-8d2270e22593", "collapsed": false}, "execution_count": null, "outputs": [], "cell_type": "code", "source": "def isKid(row):\n    age = row['Age']\n    \n    if age < 6:\n        return 1\n    else:\n        return 0\n    \ndef hasFamily(row):\n    if row['SibSp'] > 0 or row['Parch'] > 0:\n        return 1\n    else:\n        return 0\n    \ndef fatWallet(row):\n    if row['Fare'] >= 50:\n        return 1\n    else:\n        return 0\n        \ntrain['Has Family'] = train.apply(lambda x: hasFamily(x), axis=1)\ntrain['Is Kid'] = train.apply(lambda x: isKid(x), axis=1)\ntrain['Fat Wallet'] = train.apply(lambda x: fatWallet(x), axis=1)\n\ntest['Has Family'] = test.apply(lambda x: hasFamily(x), axis=1)\ntest['Is Kid'] = test.apply(lambda x: isKid(x), axis=1)\ntest['Fat Wallet'] = test.apply(lambda x: fatWallet(x), axis=1)\n\ntrain = train.drop(['Male', 'Fare', 'SibSp', 'Parch', 'Age'], axis=1)\ntest = test.drop(['Male', 'Fare', 'SibSp', 'Parch', 'Age'], axis=1)"}, {"metadata": {"_execution_state": "busy", "trusted": false, "_uuid": "6df18581d6f2b2db6ec05353f7b77332f32e4b5d", "_cell_guid": "bfaae051-054d-48d8-9110-39d1eda8966a", "collapsed": false}, "execution_count": null, "outputs": [], "cell_type": "code", "source": "colormap = plt.cm.viridis\nplt.figure(figsize=(12,12))\nplt.title('Feature correlations', y=1.05, size=15)\nsns.heatmap(train.corr(),linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True)\nplt.yticks(rotation=35)\nplt.xticks(rotation=35)"}, {"metadata": {"_execution_state": "idle", "_uuid": "b567c6d5f24c1d229e652678ad1523142752964c", "_cell_guid": "79fe3f0d-d925-4e26-bf7d-14d6091021a5", "collapsed": false}, "execution_count": null, "outputs": [], "cell_type": "markdown", "source": "### Feature Selection 1 - Recursive Selection"}, {"metadata": {"_execution_state": "busy", "trusted": false, "_uuid": "47490cc3412f1b06549533b604030aede8f7be1d", "_cell_guid": "2b89fcd2-568b-4865-b354-6eb68ab3be12", "collapsed": false}, "execution_count": null, "outputs": [], "cell_type": "code", "source": "y = train['Survived'].values\nx = train.drop('Survived', axis=1)\n\nsvc_lin = SVC(kernel=\"linear\")\nlr = LogisticRegression()\ndt = DecisionTreeClassifier()\ngrd = GradientBoostingClassifier()\nrf = RandomForestClassifier(n_estimators=100)\nper = Perceptron()\n\nlr = RFE(estimator=lr, n_features_to_select=1, step=1)\ndt = RFE(estimator=dt, n_features_to_select=1, step=1)\ngrd = RFE(estimator=grd, n_features_to_select=1, step=1)\nrf = RFE(estimator=rf, n_features_to_select=1, step=1)\nper = RFE(estimator=per, n_features_to_select=1, step=1)\nsvc = RFE(estimator=svc_lin, n_features_to_select=1, step=1)\n\n\nlr.fit(x, y)\ndt.fit(x, y)\ngrd.fit(x, y)\nrf.fit(x, y)\nper.fit(x, y)\nsvc.fit(x, y)\n\nlr_ranking = lr.ranking_\ndt_ranking = dt.ranking_\ngrd_ranking = grd.ranking_\nrf_ranking = rf.ranking_\nper_ranking = per.ranking_\nsvclin_ranking = svc.ranking_\n\nnew_df = pd.DataFrame({\n    'LogReg Ranking': lr_ranking,\n    'DTree Ranking': dt_ranking,\n    'GRD Boost Ranking': grd_ranking,\n    'rf_ranking': rf_ranking,\n    'per_ranking': per_ranking\n})\n\nfselection = pd.DataFrame(list(zip(x.columns, svclin_ranking)), columns=['features', 'svc_ranking'])\nfselection = fselection.join(new_df)\nfselection"}, {"metadata": {"_execution_state": "idle", "_uuid": "b959bf9e249e805740837e4ee11b3a69092d5b37", "_cell_guid": "63f558ff-faf9-4d60-90cf-f2d64d1014ff", "collapsed": false}, "execution_count": null, "outputs": [], "cell_type": "markdown", "source": "### Feature Selection 2 - Univariate Selection\n\"Univariate feature selection examines each feature individually to determine the strength of the relationship of the feature with the response variable. These methods are simple to run and understand and are in general particularly good for gaining a better understanding of data (but not necessarily for optimizing the feature set for better generalization). There are lot of different options for univariate selection.\" - http://blog.datadive.net/selecting-good-features-part-i-univariate-selection/\n\n### Coming Soon\nI don't fully understand how to use univariate selection, and principal component analysis to select features. **If you could explain how, please do in the comments section.** ## Heading ##"}, {"metadata": {"_execution_state": "idle", "_uuid": "93498cb4d81709a2635d0704ccd5f9df30e82e6d", "_cell_guid": "98bfd32a-a883-4171-9d7e-cf2d128ff958", "collapsed": false}, "execution_count": null, "outputs": [], "cell_type": "markdown", "source": "### Feature Selection 3 - Principal Component Analysis\nThe main idea of principal component analysis (PCA) is to reduce the dimensionality of a data set consisting of many variables correlated with each other, either heavily or lightly, while retaining the variation present in the dataset, up to the maximum extent. - https://www.dezyre.com/data-science-in-python-tutorial/principal-component-analysis-tutorial\n\n**Why is this useful?** Imagine that the dimensionality of the feature set is larger than just two or three. Using a PCA we can now identify what are the most important dimensions and just keep a few of them to explain most of the variance we see in out data. Hence we can drastically reduce the dimensionality of the data and make EDA feasible again. Moreover, it will also enable us to identify what the most important variables in the original feature space are, that contribute most to the most important PCs. Intuitively, one can imagine, that a dimension that has not much variability cannot explain much of the happenings and thus is not as important as more variable dimensions. - http://jotterbach.github.io/2016/03/24/Principal_Component_Analysis/\n\n### Coming Soon"}, {"metadata": {"_uuid": "6ed13f696b52891fc53e0b4f60d96fd9b3eba649", "_cell_guid": "973c8ff6-57f1-48c4-99d1-4a3a18c4f8b8"}, "execution_count": null, "outputs": [], "cell_type": "markdown", "source": "Now that we have widdled down the variables to just the ones we want to use, we can start scoring our fitted models using different algorithms. We will test performance of each algorithm with its ROC_AUC, and its basic accuracy of predictions. When spliting our data into a training and test set, we will set a random state, which sets the seed for the computer to make random predictions off of the same, meaning that every time we split the data it will always be the same split, making it easier to replicate !"}, {"metadata": {"trusted": false, "_execution_state": "busy", "_uuid": "48a497caeaf18dd18b9e1329a2e9aea603abf12b", "scrolled": false, "_cell_guid": "296f52e7-8cd2-4407-9ce7-58d743d11ef7", "collapsed": false}, "execution_count": null, "outputs": [], "cell_type": "code", "source": "rf = RandomForestClassifier(n_estimators=100)\ngaus = GaussianNB()\nlogreg = LogisticRegression()\ndtree = DecisionTreeClassifier()\nsvc_rbf = SVC(kernel=\"rbf\")\nsvc_lin = SVC(kernel=\"linear\")\nknn = KNeighborsClassifier(n_neighbors = 3)\nper = Perceptron()\ngrd = GradientBoostingClassifier()\n\nfor num in range(1, 3000):\n    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = .3, stratify=y)\n    \n    grd.fit(X_train, y_train)\n    grd_score = grd.score(X_test, y_test)\n    \n    if grd_score > .87:\n\n        algorithms = [{'algo': rf, 'color': '#4285f4', 'name': 'Random Forest'}, {'algo': gaus, 'color': 'red', 'name': 'Gaussian'}, \n                      {'algo': logreg, 'color': 'blue', 'name': 'Logistic Regressions'},{'algo': dtree, 'color': 'orange', 'name': 'Decision Tree'}, \n                      {'algo': svc_rbf, 'color': 'lime', 'name': 'SVC-RBF'}, {'algo': svc_lin, 'color': 'purple', 'name': 'Linear SVC'},\n                      {'algo': knn, 'color': 'yellow', 'name': 'KNN'},{'algo': per, 'color': 'indigo', 'name': 'Perceptron'}, \n                      {'algo': grd, 'color': 'black', 'name': 'Gradient Boosting'}\n                     ]\n\n        for alg in algorithms:\n            algo = alg['algo']\n            algo.fit(X_train, y_train)\n            predictions = algo.predict(X_test)\n            fpr, tpr, threshold = metrics.roc_curve(y_test, predictions)\n            auc = metrics.auc(fpr, tpr)\n            plt.plot(fpr, tpr, alg['color'], label='{} AUC = {:.2f}'.format(alg['name'], auc))\n\n        plt.title('Receiver Operating Characteristic')\n\n        plt.legend(loc = 'lower right')\n        plt.plot([0, 1], [0, 1],'r--')\n        plt.xlim([0, 1])\n        plt.ylim([0, 1])\n        plt.ylabel('True Positive Rate')\n        plt.xlabel('False Positive Rate')\n        plt.show()\n\n        scores = [algorithm.score(X_test, y_test) for algorithm in [logreg, dtree, svc_rbf, svc_lin, knn, gaus, per, rf, grd]]\n\n        scoring_df = pd.DataFrame({\n            'algorithms': ['Logistic Regression', 'Decision Tree', 'SVC Radial Basis Function',\n                           'Linear SVC', 'KNearest Neighbors', 'Gaussian Naive Bayes', 'Perceptron',\n                           'Random Forest', 'Gradient Boosting'],\n            'score': scores\n        })\n\n        print(scoring_df)\n        break"}, {"metadata": {"_uuid": "201e64966079a2506d6a2a57c8cdec10decf1d18", "_cell_guid": "a7f0de2b-37d6-42c6-a1aa-e855a7f5e96a"}, "execution_count": null, "outputs": [], "cell_type": "markdown", "source": "Based on its receiver operating characteristics, and its number of correct predictions on our test set, we can see that our hot ticket algorithms for making predictions is Gradient Boosting. Before we start making predictions however, we will try and tune our algorithm to get the best performance possible, and test again."}, {"metadata": {"_uuid": "4b0b8f972f200a54411f4377a9f7c9c3d7d5541d", "_cell_guid": "b69a6ecb-90c1-4308-9a8b-d3511b97e046"}, "execution_count": null, "outputs": [], "cell_type": "markdown", "source": "### Optimize Gradient Boosting"}, {"metadata": {"_execution_state": "busy", "trusted": false, "_uuid": "a1841617142a66e5074b3b5e5595b29fea1e3cda", "_cell_guid": "8948e2b5-f6a0-4179-bf18-e1818524dd23", "collapsed": false}, "execution_count": null, "outputs": [], "cell_type": "code", "source": "# estimators = range(1, 400)\nlrate = [x/1000 for x in range(1, 1000)]\n# mleaf = range(1, 300)\n\ngrd_scores = []\nfor n in lrate:\n    grd = GradientBoostingClassifier(n_estimators=17, learning_rate=.162)\n    grd.fit(X_train, y_train)\n    grd_preds = grd.predict(X_test)\n    grd_fpr, grd_tpr, threshold = metrics.roc_curve(y_test, grd_preds)\n    grd_auc = metrics.auc(grd_fpr, grd_tpr)\n    grd_score = grd.score(X_test, y_test)\n    grd_scores.append({'criteria': n, 'score': grd_score})\n    \ngrd_scores = sorted(grd_scores, key=lambda k: k['score'], reverse=True)  \ngrd_scores[:1]"}, {"metadata": {"_execution_state": "busy", "trusted": false, "_uuid": "7a35c241bd290388868bceacde7dedd2aeda819a", "_cell_guid": "1e479a9a-8978-42d7-a074-772ebdbc1908", "collapsed": false}, "execution_count": null, "outputs": [], "cell_type": "code", "source": "# passenger_ids = test['PassengerId'].values\n# test = test.drop('PassengerId', axis=1)\n\n# grd = GradientBoostingClassifier(n_estimators=17, learning_rate=.162)\n# grd.fit(X_train, y_train)\n# predictions = grd.predict(test)\n\n# ------------ PREPARE FOR SUBMISSION -------------- #\n# new_df = pd.DataFrame(columns=['PassengerId', 'Survived'])\n# new_df['PassengerId'] = passenger_ids\n# new_df['Survived'] = predictions\n# new_df.to_csv('titanic_final.csv', index=False)\n# ---------- END PREPARE FOR SUBMISSION ----------- #"}, {"metadata": {"_execution_state": "busy", "trusted": false, "_uuid": "255f08bdd679bdb77c42a448a28f5b94fd28bfea", "_cell_guid": "e42d2df7-8df0-40c3-aa0d-27588a312716", "collapsed": true}, "execution_count": null, "outputs": [], "cell_type": "code", "source": ""}], "nbformat": 4}