{"cells": [{"execution_count": null, "metadata": {"_uuid": "9e78a967cc221127b36b6d94456569f4258c00c3"}, "cell_type": "markdown", "source": ["# Getting Started"], "outputs": []}, {"execution_count": null, "metadata": {"_uuid": "f9f4c1c5249fe6279e8d6cad1246819a88c3848d"}, "cell_type": "markdown", "source": ["This notebook tells about step to step cleaning the data until predicting the testing dataset.\n", "Basically this notebook inspired from another most voted notebook.\n", "\n", "* https://www.kaggle.com/mrisdal/exploring-survival-on-the-titanic/notebook\n", "* https://www.kaggle.com/omarelgabry/a-journey-through-titanic\n", "\n", "Especially, how them facing the nan values and fill it (Preprocessing). :)\n", "\n", "The notebook contain:\n", "\n", "* Peeking the datasets \n", "* Preprocessing the Data\n", "    - filling the age, fare, and embarked columns\n", "    - dropping unused columns\n", "    - creating dummy variables.\n", "* Feature Engineering\n", "* Predicting using RF and basic Tree."], "outputs": []}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "58cf7d8a8ba3413d0ab4b1fe43fef6a9dcf2d9f7", "ExecuteTime": {"end_time": "2017-08-01T10:34:34.599280", "start_time": "2017-08-01T10:34:33.737975"}, "_execution_state": "idle"}, "cell_type": "code", "source": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline\n\ntrain_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')", "execution_count": 1}, {"execution_count": null, "metadata": {"_uuid": "0ed53d685c6c65ceb2c4ccb6490862a5b3f6af69"}, "cell_type": "markdown", "source": ["# Peeking the Datasets"], "outputs": []}, {"outputs": [], "metadata": {"collapsed": false, "_uuid": "9ee564572177df60dbd231386a4b937d88a078e6", "ExecuteTime": {"end_time": "2017-08-01T10:34:34.614628", "start_time": "2017-08-01T10:34:34.600914"}, "_execution_state": "idle"}, "cell_type": "code", "source": ["#checking number of null values in each columns\n", "print(train_df.isnull().sum())\n", "print('--'*10)\n", "print(test_df.isnull().sum())\n", "#They're several missing values in columns, so we must look after of it."], "execution_count": 2}, {"execution_count": null, "metadata": {"_uuid": "0f5aa10a8bbb0e0aa4608602f7dcda4ad98a1903"}, "cell_type": "markdown", "source": ["## Preprocessing Data"], "outputs": []}, {"execution_count": null, "metadata": {"_uuid": "4c39a75bf032e7b99a0b95e11e28a4021e52f43e"}, "cell_type": "markdown", "source": ["### Filling Age Column, but First we make new Feature, Called 'Title.'"], "outputs": []}, {"outputs": [], "metadata": {"collapsed": false, "_uuid": "cdc7cb9ea1406d07c63b06754349f82ea89a29f9", "ExecuteTime": {"end_time": "2017-08-01T10:34:34.832863", "start_time": "2017-08-01T10:34:34.617051"}, "_execution_state": "idle"}, "cell_type": "code", "source": ["train_df.head(1)"], "execution_count": 3}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "180a73546adfb2c387b23b256aa5660288cd968a", "ExecuteTime": {"end_time": "2017-08-01T10:34:34.932694", "start_time": "2017-08-01T10:34:34.834708"}, "_execution_state": "idle"}, "cell_type": "code", "source": ["# Title created, for filling the missing 'Age'\n", "# we make function that can make the title from name can standalone, and make new columns called 'Title'\n", "# this function is used for deleting the first 'word' from column 'name', like index 0 at example above\n", "# then name is Braund, so we deleting Braud first to get `Mr.` for title, and ignoring the rest.\n", "\n", "def hapusD(kalimat,kata): \n", "    pos = kalimat.find(kata)\n", "    #print(kata,pos)\n", "    if pos!=-1:\n", "        pjg = len(kata)\n", "        kalimat = kalimat[pos+1:]\n", "    return kalimat\n", "\n", "#then for the second function we deleting the all string after we found the title Mr.\n", "def hapusT(kalimat,kata):\n", "    pos = kalimat.find(kata)\n", "    #print(pos)\n", "    if pos!=-1:\n", "        kalimat = kalimat[:pos+len(kata)]\n", "        #print(kalimat)\n", "    return kalimat\n", "\n", "#this function is for use another function, function hapusD and hapusT.\n", "def hapusName(desc):\n", "    desc = hapusD(desc,', ')\n", "    desc = hapusT(desc,'. ')\n", "    \n", "    return desc.strip()"], "execution_count": 4}, {"outputs": [], "metadata": {"collapsed": false, "_uuid": "4c5433d7f08556e616703cc9c6296c0062d41a42", "ExecuteTime": {"end_time": "2017-08-01T10:34:35.104748", "start_time": "2017-08-01T10:34:34.934406"}, "_execution_state": "idle"}, "cell_type": "code", "source": ["#applying function to Name for both train and test dataframe then create the new columns, Title.\n", "train_df['Title'] = train_df['Name'].apply(hapusName)\n", "test_df['Title'] = test_df['Name'].apply(hapusName)"], "execution_count": 5}, {"execution_count": null, "metadata": {"_uuid": "9b410302ff9a6fcc26525efb9238d869420f43f5"}, "cell_type": "markdown", "source": ["### Distribution of Age in Each Title"], "outputs": []}, {"execution_count": null, "metadata": {"_uuid": "ca900d1d0d7dea6d5a9d7f12772ef6435245692f"}, "cell_type": "markdown", "source": ["there are several missing values in this data."], "outputs": []}, {"outputs": [], "metadata": {"collapsed": false, "_uuid": "0d124856a13c388ed0b0fc103b366e566b29a663", "ExecuteTime": {"end_time": "2017-08-01T10:34:35.236518", "start_time": "2017-08-01T10:34:35.106551"}, "_execution_state": "idle"}, "cell_type": "code", "source": ["#we want to know what title of each person had which has missing value.\n", "print(train_df.loc[train_df['Age'].isnull(),'Title'].value_counts())\n", "#there's almost common Title that has missing values."], "execution_count": 6}, {"outputs": [], "metadata": {"collapsed": false, "_uuid": "6a6a9dd0dfcba0fade9381517eb8c3659c61450d", "ExecuteTime": {"end_time": "2017-08-01T10:34:35.745860", "start_time": "2017-08-01T10:34:35.238010"}, "_execution_state": "idle"}, "cell_type": "code", "source": ["#we want to plotting the mean distribution and deviation for each title\n", "avg_age_title = train_df[['Title','Age']].loc[train_df['Age'].notnull()].groupby('Title').mean()\n", "std_age_title = train_df[['Title','Age']].loc[train_df['Age'].notnull()].groupby('Title').std().fillna(0)\n", "\n", "avg_age_title.index.names = std_age_title.index.names = [\"\"]\n", "fig, ax = plt.subplots(figsize = (15.7,6.27))\n", "avg_age_title.plot(yerr=std_age_title,kind='bar',ax = ax, title = \"Age and Deviation In each Title\")\n", "# so we will input missing values with random value around mean and deviation for each Title."], "execution_count": 7}, {"outputs": [], "metadata": {"collapsed": false, "_uuid": "12423f3e757c89c3dcf610b0fd2df72e438a8fbb", "ExecuteTime": {"end_time": "2017-08-01T10:34:35.762686", "start_time": "2017-08-01T10:34:35.747577"}, "_execution_state": "idle"}, "cell_type": "code", "source": ["#so this is the function, we gather sex and the title together\n", "def fillAge(inputs):    \n", "    sex,title = inputs\n", "\n", "    try:\n", "        #if the title is known in by the training set, we random number around mean with deviation for each title.\n", "        avg = train_df.loc[(train_df['Title'] == title),'Age'].mean()\n", "        std = train_df.loc[(train_df['Title'] == title),'Age'].std()\n", "        #cnt_nan = train_df.loc[(train_df['title'] == title),'Age'].isnull().sum()\n", "\n", "        rand_1 = np.random.randint(avg - std, avg + std)\n", "        \n", "\n", "    except:\n", "        #but if,we the cant recognise the title from training set, so it'll throw the error, we pass it to another method.\n", "        #we we randoming the value around the mean of its sex (male/female).\n", "        avg = train_df.loc[train_df['Sex'] == sex, 'Age'].mean()\n", "        std = train_df.loc[train_df['Sex'] == sex, 'Age'].std()\n", "\n", "        rand_1 = np.random.randint(avg - std, avg + std)\n", "    \n", "    return rand_1"], "execution_count": 8}, {"outputs": [], "metadata": {"collapsed": false, "_uuid": "02cc74bb81a4e191d1304e7dc47ed1e4efb387d1", "ExecuteTime": {"end_time": "2017-08-01T10:34:36.453411", "start_time": "2017-08-01T10:34:35.765658"}, "_execution_state": "idle"}, "cell_type": "code", "source": ["#Action.\n", "train_df.loc[train_df['Age'].isnull(),'Age'] = train_df[['Sex','Title']].loc[train_df['Age'].isnull()].apply(fillAge, axis = 1)\n", "test_df.loc[test_df['Age'].isnull(),'Age'] = test_df[['Sex','Title']].loc[test_df['Age'].isnull()].apply(fillAge, axis = 1)\n", "\n", "#so that's enough name column, we can drop it.\n", "train_df.drop('Name', axis = 1, inplace = True)\n", "test_df.drop('Name', axis = 1, inplace = True)"], "execution_count": 9}, {"execution_count": null, "metadata": {"_uuid": "ef0e199d63e25474eb111c5c9f559e224f8ec87e"}, "cell_type": "markdown", "source": ["### Replacing Uncommon Title with More Common Ones."], "outputs": []}, {"outputs": [], "metadata": {"collapsed": false, "_uuid": "a2af24b4905307e799673cea441256dc4128a8d0", "ExecuteTime": {"end_time": "2017-08-01T10:34:36.465248", "start_time": "2017-08-01T10:34:36.455084"}, "_execution_state": "idle", "scrolled": true}, "cell_type": "code", "source": ["#Counting of each title.\n", "train_title = train_df[['Title','PassengerId']].groupby('Title', as_index = False).count()\n", "train_title.columns = ['Title','count_training']\n", "#train_title"], "execution_count": 10}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "524260b2f09012f05de0a6dac63006e06239383a", "ExecuteTime": {"end_time": "2017-08-01T10:34:36.578637", "start_time": "2017-08-01T10:34:36.467711"}, "_execution_state": "idle"}, "cell_type": "code", "source": ["#Replacing Title into Closest Age Mean and Deviation (or maybe the nearest sounds like)\n", "\n", "#mlle -> mrs\n", "train_df['Title'] = train_df['Title'].replace('Mlle.','Mrs.')\n", "test_df['Title'] = test_df['Title'].replace('Mlle.','Mrs.')\n", "\n", "#mme -> mrs.\n", "train_df['Title'] = train_df['Title'].replace('Mme.','Mrs.')\n", "test_df['Title'] = test_df['Title'].replace('Mme.','Mrs.')\n", "\n", "#ms. -> miss\n", "train_df['Title'] = train_df['Title'].replace('Ms.','Miss.')\n", "test_df['Title'] = test_df['Title'].replace('Ms.','Miss.')\n", "\n", "#but we ignore uncommon title that maybe occour in test_df."], "execution_count": 11}, {"execution_count": null, "metadata": {"_uuid": "8592af259a87b9ce7aff8e2d7b9058b89e4f3c4d"}, "cell_type": "markdown", "source": ["### Filling The Embarked Column (train_df) and Fare (test_df)."], "outputs": []}, {"outputs": [], "metadata": {"collapsed": false, "_uuid": "dd4083b890ee06e1b2242a089af1e70563766824", "ExecuteTime": {"end_time": "2017-08-01T10:34:36.774714", "start_time": "2017-08-01T10:34:36.580108"}, "_execution_state": "idle"}, "cell_type": "code", "source": ["#Fill Embarked With Nearest value of Fare\n", "train_df[train_df['Embarked'].isnull()]\n", "#Logically, the nearest correlation between Embarked is the fare, so we want to know how the distribution of 'Embark' vs 'Fare'"], "execution_count": 12}, {"outputs": [], "metadata": {"collapsed": false, "_uuid": "73689ffddc7cb2ee781830fc11745783b9f226c7", "ExecuteTime": {"end_time": "2017-08-01T10:34:37.144816", "start_time": "2017-08-01T10:34:36.776051"}, "_execution_state": "idle"}, "cell_type": "code", "source": ["fig, ax = plt.subplots(figsize = (15.7,6.27))\n", "sns.barplot(x = 'Embarked', y = 'Fare', data = train_df)"], "execution_count": 13}, {"outputs": [], "metadata": {"collapsed": false, "_uuid": "9aa1869faf81e602a9b68042c42ffb2c7dab5c93", "ExecuteTime": {"end_time": "2017-08-01T10:34:37.151155", "start_time": "2017-08-01T10:34:37.147000"}, "_execution_state": "idle"}, "cell_type": "code", "source": ["#so the most logical thing, if the fare was 80 the nearest value and deviation is the 'C'\n", "train_df['Embarked'] = train_df['Embarked'].fillna('C')"], "execution_count": 14}, {"outputs": [], "metadata": {"collapsed": false, "_uuid": "6c763afc361343a87d8d5307302e7accb90de612", "ExecuteTime": {"end_time": "2017-08-01T10:34:37.288782", "start_time": "2017-08-01T10:34:37.152517"}, "_execution_state": "idle"}, "cell_type": "code", "source": ["#filling the missing values in fare simply with mean of the 'S' values in Embarked in training datasets.\n", "test_df[test_df['Fare'].isnull()]"], "execution_count": 15}, {"outputs": [], "metadata": {"collapsed": false, "_uuid": "54926214f337863e389afdde6bbcd1faf6b57815", "ExecuteTime": {"end_time": "2017-08-01T10:34:37.404031", "start_time": "2017-08-01T10:34:37.290376"}, "_execution_state": "idle"}, "cell_type": "code", "source": ["test_df.loc[test_df['Fare'].isnull(), 'Fare'] = train_df.loc[train_df['Embarked'] == 'S','Fare'].mean()\n", "test_df.iloc[152]"], "execution_count": 16}, {"execution_count": null, "metadata": {"_uuid": "233bf5a30b41e354bf0d224d83965262b48d8791"}, "cell_type": "markdown", "source": ["# Feature Engineering"], "outputs": []}, {"execution_count": null, "metadata": {"_uuid": "acf658e3c00764b7ed177f4fb7735ec00b25e2d3"}, "cell_type": "markdown", "source": ["## Feature Engineeing I"], "outputs": []}, {"outputs": [], "metadata": {"collapsed": false, "_uuid": "f2f056ca81dc28bb872ee32c82724db15e094e89", "ExecuteTime": {"end_time": "2017-08-01T10:34:37.795125", "start_time": "2017-08-01T10:34:37.406471"}, "_execution_state": "idle"}, "cell_type": "code", "source": ["#We want to make title columns to more general. \n", "#So, we make most common Title are ['Mr.','Mrs.','Miss.','Master.', 'Dr.', 'Rev.'] and Cluster another uncommon title to one label -> 'Rare'.\n", "\n", "import seaborn as sns\n", "\n", "fig, ax = plt.subplots(figsize = (15.7,6.27))\n", "sns.countplot(x = 'Title', data = train_df)"], "execution_count": 17}, {"outputs": [], "metadata": {"collapsed": false, "_uuid": "af62795cb53303bd82f66d70f416c8daacb99c06", "ExecuteTime": {"end_time": "2017-08-01T10:34:37.805681", "start_time": "2017-08-01T10:34:37.797253"}, "_execution_state": "idle"}, "cell_type": "code", "source": ["#make function to make new values.\n", "nonRareTitle = ['Mr.','Mrs.','Miss.','Master.','Dr.','Rev.']\n", "def generalizingTitle(title):\n", "    if (title == nonRareTitle[0]) | (title == nonRareTitle[1]) | (title == nonRareTitle[2]) | (title == nonRareTitle[3]):\n", "        title = title\n", "    # and throw the uncommon title into 'Rare.'\n", "    else:\n", "        title = 'Rare.'\n", "            \n", "    return title"], "execution_count": 18}, {"outputs": [], "metadata": {"collapsed": false, "_uuid": "203859733d4c986ff7c46bad196417ac55763467", "ExecuteTime": {"end_time": "2017-08-01T10:34:37.824333", "start_time": "2017-08-01T10:34:37.808102"}, "_execution_state": "idle"}, "cell_type": "code", "source": ["#results\n", "\n", "train_df['Title'] = train_df['Title'].apply(generalizingTitle)\n", "test_df['Title'] = test_df['Title'].apply(generalizingTitle)\n", "\n", "#printing the results.\n", "print(train_df['Title'].value_counts())\n", "print('-'*10)\n", "print(test_df['Title'].value_counts())"], "execution_count": 19}, {"execution_count": null, "metadata": {"_uuid": "36102db3dab3c2572448937cec308a7c9fa5432c"}, "cell_type": "markdown", "source": ["## Feature Engineering II"], "outputs": []}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "a22f06f1393facd838aaa0e394186fdbe96c9d8e", "ExecuteTime": {"end_time": "2017-08-01T10:34:38.055481", "start_time": "2017-08-01T10:34:37.828941"}, "_execution_state": "idle"}, "cell_type": "code", "source": ["#we want to make new feature called 'Family', family represents the sum between columns 'Parch' and 'SibSp', \n", "#which's sum between #of parents and #of Siblings\n", "\n", "train_df['Family'] = train_df['SibSp'] + train_df['Parch'] + 1\n", "test_df['Family'] = test_df['SibSp'] + test_df['Parch'] + 1\n", "#we add +1 bcs it counts with him/herself."], "execution_count": 20}, {"outputs": [], "metadata": {"collapsed": false, "_uuid": "17fda88a8f630564c0404ba27f41793488b09300", "ExecuteTime": {"end_time": "2017-08-01T10:34:38.754186", "start_time": "2017-08-01T10:34:38.057174"}, "_execution_state": "idle"}, "cell_type": "code", "source": ["#we want to know the distribution number of Family\n", "fig, ax = plt.subplots(figsize = (15.7,6.27))\n", "sns.countplot(x = 'Family', data = train_df)\n", "\n", "##of Family\n", "grap = sns.FacetGrid(train_df, hue=\"Survived\",aspect=4)\n", "grap.map(sns.kdeplot,'Family',shade= True)\n", "grap.set(xlim=(0, train_df['Family'].max()))\n", "grap.add_legend()\n", "#as we can see, the lower family number, if family number = 1. The passengger more likely not survived."], "execution_count": 21}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "88a766652996a22a2b90d2e9b35e7f04bd58a778", "ExecuteTime": {"end_time": "2017-08-01T10:34:38.764336", "start_time": "2017-08-01T10:34:38.758147"}, "_execution_state": "idle"}, "cell_type": "code", "source": ["#so we classify the number of family.\n", "\n", "def changeFamily(family):\n", "    if family == 1:\n", "        family = 'Alone'\n", "    elif family >= 2 & family <= 5:\n", "        family = 'Family'\n", "    else:\n", "        family = 'Huge Family'\n", "    \n", "    return family"], "execution_count": 22}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "ebf65e5042e6390c95876548aa9c0c20bda1d39b", "ExecuteTime": {"end_time": "2017-08-01T10:34:38.932168", "start_time": "2017-08-01T10:34:38.768162"}, "_execution_state": "idle"}, "cell_type": "code", "source": ["train_df['Family'] = train_df['Family'].apply(changeFamily)\n", "test_df['Family'] = test_df['Family'].apply(changeFamily)\n", "\n", "#dropping unused columns\n", "train_df.drop('SibSp', axis = 1, inplace = True)\n", "train_df.drop('Parch', axis = 1, inplace = True)\n", "#--\n", "test_df.drop('SibSp', axis = 1, inplace = True)\n", "test_df.drop('Parch', axis = 1, inplace = True)"], "execution_count": 23}, {"execution_count": null, "metadata": {"_uuid": "c7f259de3704c1bd9a065f0fdec7154b8da48501"}, "cell_type": "markdown", "source": ["## Feature Engineering III"], "outputs": []}, {"outputs": [], "metadata": {"collapsed": false, "_uuid": "d1edb2fd8a649b840f6170a771ed7ad6dd98fc23", "ExecuteTime": {"end_time": "2017-08-01T10:34:39.402824", "start_time": "2017-08-01T10:34:38.934406"}, "_execution_state": "idle"}, "cell_type": "code", "source": ["# the person with lower age (child) more likely to survive then the person have higher number of age\n", "# so we can create new value based on this rule.\n", "# inserting new value child into sex columns.\n", "\n", "grap = sns.FacetGrid(train_df, hue=\"Survived\",aspect=4)\n", "grap.map(sns.kdeplot,'Age',shade= True)\n", "grap.set(xlim=(0, train_df['Age'].max()))\n", "grap.add_legend()"], "execution_count": 24}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "ffca69b471b8386f02e5a07aa666b305335635a3", "ExecuteTime": {"end_time": "2017-08-01T10:34:39.491535", "start_time": "2017-08-01T10:34:39.407124"}, "_execution_state": "idle"}, "cell_type": "code", "source": ["#function to return child\n", "def addChild(pasengger):\n", "    sex,age = pasengger\n", "    return 'child' if age < 16 else sex\n", "\n", "#applying into columns sex,\n", "train_df['Sex'] = train_df[['Sex','Age']].apply(addChild, axis = 1)\n", "test_df['Sex'] = test_df[['Sex','Age']].apply(addChild, axis = 1)"], "execution_count": 25}, {"execution_count": null, "metadata": {"_uuid": "9b6249a84c4ac307777bbce73ba03e1fe652945c"}, "cell_type": "markdown", "source": ["# Dropping Un-used Columns."], "outputs": []}, {"outputs": [], "metadata": {"collapsed": false, "_uuid": "31f011ec92cbff90995107644fa865edfc627993", "ExecuteTime": {"end_time": "2017-08-01T10:34:39.640146", "start_time": "2017-08-01T10:34:39.493840"}, "_execution_state": "idle"}, "cell_type": "code", "source": ["#we drop passenger, because this column more likely to be a bug (i mean, make training accuracy very high if we keep this).\n", "test_id = test_df.iloc[:,0]\n", "\n", "train_df.drop('PassengerId', axis = 1, inplace = True)\n", "test_df.drop('PassengerId', axis = 1, inplace = True)\n", "\n", "#drop chaotic columns, cabin and ticket.\n", "train_df.drop('Cabin', axis = 1, inplace = True)\n", "test_df.drop('Cabin', axis = 1, inplace = True)\n", "\n", "train_df.drop('Ticket', axis = 1, inplace = True)\n", "test_df.drop('Ticket', axis = 1, inplace = True)"], "execution_count": 26}, {"execution_count": null, "metadata": {"_uuid": "20f46ff7ab4f57243b74feafd171feb50be67e08"}, "cell_type": "markdown", "source": ["## Dividing the Predictor"], "outputs": []}, {"execution_count": null, "metadata": {"_uuid": "967494c91195ed19206b62e8a204fa80a057bd1e"}, "cell_type": "markdown", "source": ["---"], "outputs": []}, {"outputs": [], "metadata": {"collapsed": false, "_uuid": "fa5bd925e46dfbcc601f51dc4465faf4e5ff2c4b", "ExecuteTime": {"end_time": "2017-08-01T10:34:39.750039", "start_time": "2017-08-01T10:34:39.641363"}, "_execution_state": "idle"}, "cell_type": "code", "source": ["y_train = train_df.iloc[:,0]\n", "x_train = train_df.iloc[:,1:]"], "execution_count": 27}, {"outputs": [], "metadata": {"collapsed": false, "_uuid": "0f5f8ed397e2d95e4424225d0e78da30f1e09b45", "ExecuteTime": {"end_time": "2017-08-01T10:34:39.897309", "start_time": "2017-08-01T10:34:39.752589"}, "_execution_state": "idle"}, "cell_type": "code", "source": ["columns = ['Sex','Embarked','Title','Family']\n", "Y_columns = ['Survived']\n", "\n", "#this function to get dummies columns from categorical value in dataframe in both train and test.\n", "def exCategories(df):\n", "\n", "    catg = df[columns]\n", "    catg = pd.get_dummies(catg)\n", "    \n", "    nocatg = df.drop(columns, axis = 1)\n", "    res = pd.concat([nocatg,catg], axis = 1)\n", "    \n", "    if ''.join(Y_columns) in res:\n", "        y = res[Y_columns]\n", "        x = res.drop(Y_columns, axis = 1)\n", "    else:\n", "        y = None\n", "        x = res\n", "        \n", "    return y,x\n", "\n", "y_train, x_train  = exCategories(train_df)\n", "y_test, x_test = exCategories(test_df)"], "execution_count": 28}, {"execution_count": null, "metadata": {"_uuid": "99a78e4944619c342a6b6f0a2bcbfc613c8da1dc"}, "cell_type": "markdown", "source": ["# Predicting"], "outputs": []}, {"outputs": [], "metadata": {"collapsed": false, "_uuid": "38b1e7494018c5dab0bf6803e9329a7eaccda3bd", "ExecuteTime": {"end_time": "2017-08-01T10:34:40.066891", "start_time": "2017-08-01T10:34:39.898938"}, "_execution_state": "idle"}, "cell_type": "code", "source": ["from sklearn.neighbors import KNeighborsClassifier\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.ensemble import RandomForestClassifier\n", "from sklearn.tree import DecisionTreeClassifier\n", "\n", "from sklearn.cross_validation import cross_val_score"], "execution_count": 29}, {"outputs": [], "metadata": {"collapsed": false, "_uuid": "bf4aa9d89e46d62408c56838d2ba7050ab68b1ff", "ExecuteTime": {"end_time": "2017-08-01T10:34:40.100149", "start_time": "2017-08-01T10:34:40.071139"}, "_execution_state": "idle"}, "cell_type": "code", "source": ["knn = KNeighborsClassifier()\n", "logreg = LogisticRegression(random_state = 7, n_jobs= -1)\n", "decisiontree = DecisionTreeClassifier(random_state = 7)\n", "randomforest = RandomForestClassifier(random_state = 7, n_estimators= 100, n_jobs= -1)\n", "\n", "classifiers = [knn,logreg,decisiontree,randomforest]"], "execution_count": 30}, {"outputs": [], "metadata": {"collapsed": false, "_uuid": "c83c4a73256ef9e67aa3a1764fc3683b49fab430", "ExecuteTime": {"end_time": "2017-08-01T10:34:42.363386", "start_time": "2017-08-01T10:34:40.104701"}, "_execution_state": "idle"}, "cell_type": "code", "source": ["print('7-fold cross validation:\\n')\n", "\n", "for clf, label in zip(classifiers, \n", "                      ['KNN',\n", "                       'Logistic Regression', \n", "                       'Decision Trees',\n", "                       'Random Forest']):\n", "\n", "    scores = cross_val_score(clf, x_train, y_train.Survived, cv=5, scoring='accuracy')\n", "    print(\"Accuracy: %0.4f (+/- %0.4f) [%s]\" \n", "          % (scores.mean(), scores.std(), label))"], "execution_count": 31}, {"execution_count": null, "metadata": {"_uuid": "1dce39d28d98801828a5a4579000bd38ec3bb287"}, "cell_type": "markdown", "source": ["# GridSearch"], "outputs": []}, {"outputs": [], "metadata": {"collapsed": false, "_uuid": "28a7dee1c521314cc889fb47853a3e50bbed454a", "ExecuteTime": {"end_time": "2017-08-01T10:36:02.536083", "start_time": "2017-08-01T10:34:42.369777"}, "_execution_state": "idle"}, "cell_type": "code", "source": ["from sklearn.grid_search import GridSearchCV\n", "\n", "param_RF = {'max_depth' : [4, 5],\n", "           'n_estimators': [50, 150, 100, 200, 300, 350, 400],\n", "           'min_samples_split': [2, 3, 4],\n", "           'min_samples_leaf': [2, 5]}\n", "\n", "randomRF = GridSearchCV(randomforest, param_RF,cv = 5,n_jobs=-1)\n", "randomRF.fit(x_train, y_train.Survived)"], "execution_count": 32}, {"outputs": [], "metadata": {"collapsed": false, "_uuid": "ae5df77a840bdcfdbc462aec04c757ac6d7e5796", "ExecuteTime": {"end_time": "2017-08-01T10:36:02.544358", "start_time": "2017-08-01T10:36:02.539106"}, "_execution_state": "idle"}, "cell_type": "code", "source": ["print(randomRF.best_params_)\n", "print(randomRF.best_score_)"], "execution_count": 33}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "0634c6c34d4f07e36ae2b08427517edf7413916b", "ExecuteTime": {"end_time": "2017-08-01T10:36:02.781512", "start_time": "2017-08-01T10:36:02.546577"}, "_execution_state": "idle"}, "cell_type": "code", "source": ["yres_train = randomRF.predict(x_train)"], "execution_count": 34}, {"outputs": [], "metadata": {"collapsed": false, "_uuid": "e6d15d3f31247a910a88b26278f654c5d320acec", "ExecuteTime": {"end_time": "2017-08-01T10:36:02.898850", "start_time": "2017-08-01T10:36:02.785388"}, "_execution_state": "idle"}, "cell_type": "code", "source": ["from sklearn.metrics import accuracy_score\n", "from sklearn.metrics import confusion_matrix\n", "\n", "print(accuracy_score(yres_train, y_train))\n", "print(confusion_matrix(yres_train, y_train))"], "execution_count": 35}, {"outputs": [], "metadata": {"collapsed": true, "_uuid": "8eaf31ec28454f5f698601642680a4c856fbd279", "ExecuteTime": {"end_time": "2017-08-01T10:36:03.191511", "start_time": "2017-08-01T10:36:02.903093"}, "_execution_state": "idle"}, "cell_type": "code", "source": ["y_test = randomRF.predict(x_test)"], "execution_count": 36}, {"outputs": [], "metadata": {"collapsed": false, "_uuid": "01bde6a12b8c201935df20278db715a8ba1db56e", "ExecuteTime": {"end_time": "2017-08-01T10:36:03.244862", "start_time": "2017-08-01T10:36:03.200883"}, "_execution_state": "idle"}, "cell_type": "code", "source": ["res = pd.DataFrame(\n", "    {'PassengerId': test_id,\n", "     'Survived': y_test\n", "    })\n", "res.to_csv('res.csv', index = None)"], "execution_count": 37}], "nbformat": 4, "metadata": {"language_info": {"version": "3.6.1", "mimetype": "text/x-python", "file_extension": ".py", "nbconvert_exporter": "python", "codemirror_mode": {"version": 3, "name": "ipython"}, "pygments_lexer": "ipython3", "name": "python"}, "kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}, "toc": {"nav_menu": {"height": "55px", "width": "251px"}, "toc_window_display": true, "sideBar": true, "colors": {"running_highlight": "#FF0000", "selected_highlight": "#FFD700", "hover_highlight": "#DAA520"}, "toc_section_display": "block", "toc_cell": false, "moveMenuLeft": true, "navigate_menu": true, "number_sections": true, "threshold": 4}}, "nbformat_minor": 2}