{"cells": [{"cell_type": "markdown", "source": ["So, what if you are not one of the many brilliant data scientist here in Kaggle (even if there is only one that\u2019s not: me!)? \n", "What if you are just an data aficionado trying to have some fun with numbers? Maybe an accountant, we do have fun with numbers (hard to believe, huh?). What would you do if you don't know ML?\n", "Well, this would be the very simple, plain approach to the Titanic challenge for an unskilled, but dedicated accountant.\n", "\n", "Any feedback and correction, always welcome. I'm learning as I go."], "metadata": {"_cell_guid": "2c9491b2-0e83-4528-a853-4d56aa9a9704", "_uuid": "437f1a77e3b241d41caa16ad4906ffa9c4a7f3f6"}}, {"cell_type": "markdown", "source": ["First thing: the goal. We want to be able to predict if passangers included in a list will/did survive or not the Titanic disaster, by training our machine with some data we know about other passangers, including if they survived or not.\n", "\n", "Let's start loading some modules and having a look at the data."], "metadata": {"_cell_guid": "3c78e1c9-77c8-4898-bbc5-443e18162352", "_uuid": "c5d8fe6561de4675f694e5f5bba1aa9a0a88ecb3"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "\n", "# Any results you write to the current directory are saved as output."], "metadata": {"_cell_guid": "c0e36b9d-98d5-4697-ac14-e104c01b6631", "collapsed": true, "_uuid": "94a796da4bae48293ba955636591727a6b17901e"}}, {"cell_type": "markdown", "source": ["Ok, so the data we know and we will use to train our machine is in the 'train.csv' file and the list of passangers we want to predict in the 'test.csv' file (very intuitive). Let's upload the train file and see inside."], "metadata": {"_cell_guid": "77d8a4a1-f03a-4d74-a558-9669f23c0b68", "_uuid": "1b92cf851844132967e39ba5e0aff3d3fa290f64"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["df_train = pd.read_csv('../input/train.csv')\n", "print (df_train.info())"], "metadata": {"_cell_guid": "0ca50a5e-418b-4f26-a52d-a0cf57aa35c9", "collapsed": true, "_uuid": "34a84b26ee731864265600535e2b8f9fc3ec98da"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["print (df_train.head())"], "metadata": {"_cell_guid": "4f8d85d0-87d3-4706-8d25-80d03847e01e", "collapsed": true, "_uuid": "df9d01b6d165c8e0f9564b666eef2770cbe8c8d3"}}, {"cell_type": "markdown", "source": ["So, we have 891 records of passagers split by survided or not and some info about them. How can any of the variables have any influence in the survival of the passanger? My thoughts:  passanger class, sex and age. But, they are text format (object) so I create new columns in 0/1 format so the machine can read it (female = 1, and three age groups).\n", "\n", "Some age information is missing, so I assume that if missing I give them the mid category."], "metadata": {"_cell_guid": "6dc2b3c3-8537-4625-be1a-25d99ac98907", "_uuid": "c93a943df161f37b2c46093f9e26b398bcf4a340"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["df_train['sex_female'] = df_train['Sex'].apply(lambda x: 1 if x=='female' else 0)\n", "df_train['age_snr'] = df_train['Age'].apply(lambda x: 1 if x >= 50 else 0)\n", "df_train['age_mid'] = df_train['Age'].apply(lambda x: 1 if (x > 10 and x < 50) else 0)\n", "df_train['age_jnr'] = df_train['Age'].apply(lambda x: 1 if x <= 10 else 0)\n", "df_train['known_age'] = df_train['Age'].apply(lambda x: 0 if pd.isnull(x) else 1)\n", "df_train.loc[df_train['known_age'] == 0, 'age_mid'] = 1"], "metadata": {"_cell_guid": "7540915e-f6d9-4560-a485-d69fb027a3fb", "collapsed": true, "_uuid": "2d5d9910dd8d6f2224be5379a7b132ca65045c8d"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["print (df_train.head())"], "metadata": {"_cell_guid": "a269c81e-d34f-4e2a-afef-4c36f76bff41", "collapsed": true, "_uuid": "2fd090eda17353eb1faffa4d8c7e7cf25706cf26"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["print (df_train.describe())"], "metadata": {"_cell_guid": "a600bfbd-47d4-4832-b8d6-b25c7c409544", "collapsed": true, "_uuid": "c803774f3bbd927e1620a8d1f0497a4aa878c2d7"}}, {"cell_type": "markdown", "source": ["By looking at other kernels (God gave you eyes, so COPY) I found a heatmap chart for seaborn. I select only some columns and see if there is any correlation between pairs of columns."], "metadata": {"_cell_guid": "7dd24d1d-4bc4-4e3e-beee-b1fd2a4ab658", "_uuid": "5136f4ff7c4cf7162950940383cb5f988bdd7578"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["train = df_train.drop(['PassengerId', 'Name', 'Sex', 'Ticket', 'Cabin', 'Embarked', 'Age'], axis=1)\n", "fig = plt.subplots(figsize=(20,10))\n", "sns.heatmap(train.astype(float).corr(), annot=True, cmap='plasma') # my daugther's favourite color"], "metadata": {"_cell_guid": "8d5ef011-d35d-4816-a23c-523f035e8d5f", "collapsed": true, "_uuid": "b7fec65c527b3ba8e4f7062da956923dce193635"}}, {"cell_type": "markdown", "source": ["What does this mean (to me)? Just looking at the Survived column, there is a negative correlation to Pclass (the higher the class, lower the survival), and positive to fare and sex_female. About age, 0.12 to age_jnr, it sounds low to me."], "metadata": {"_cell_guid": "1fa34126-f14f-4bc1-b971-ae5706a7f211", "_uuid": "eca269c6e8051136adaab94d185422081cfed103"}}, {"cell_type": "markdown", "source": ["Now, to the model. I found some books about ML (main one, Python Machine Learning, by Sebastian Raschka) and one model of the models we can use to predict Yes/No situations is the Logistic Regression. A very quick summary: the model  gives some weight to certain variables so when the same weight is applied to the test data, it returns the probability of being 0 or 1. When probability of being 1 is equal or higher than 50%, then it returns 1. Otherwise, returns 0.\n", "\n", "As we want to know in advance how good the model is, we use scikit-learn modules to split the train data in 70% for learning and 30% fo validating. Once we have a model with the train subsample, we apply it to the validating subsample and can actually compare to the results we know.\n"], "metadata": {"_cell_guid": "f9413115-1497-4054-9be3-01c22813c2a9", "_uuid": "48b8b62c388954136b93e085aa9428f57a4f0537"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["from sklearn.cross_validation import train_test_split\n", "X = df_train.loc[:, ['PassengerId', 'Pclass', 'sex_female', 'age_jnr', 'known_age']]\n", "y = df_train['Survived']\n", "X_train, X_valid, y_train, y_valid = train_test_split(\n", "X, y, test_size = 0.3, random_state = 0)"], "metadata": {"_cell_guid": "a279d724-e132-4070-bb3f-054bd9083485", "collapsed": true, "_uuid": "c5e82093d74480cf04bc677332ffd277a52b13a0"}}, {"cell_type": "markdown", "source": ["Same book suggests the standarization of data. Not sure what this is, but I will do anyway. I will investigate later."], "metadata": {"_cell_guid": "e6ebe7d0-603d-42a1-9d8e-1cd6e3a0910c", "_uuid": "cd571742feb93cbab57ca89b912714283fb78d94"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["from sklearn.preprocessing import StandardScaler\n", "sc = StandardScaler()\n", "sc.fit(X_train)\n", "X_train_std = sc.transform(X_train)\n", "X_valid_std = sc.transform(X_valid)"], "metadata": {"_cell_guid": "a379311a-83df-4a05-b9fa-6ef92a3ca8ef", "collapsed": true, "_uuid": "2b59caf539daac3626ed45ecd089be8c369e70dd"}}, {"cell_type": "markdown", "source": ["And now the model, also from the same book. There are some values (C, random_state) applied in the model, but I will use just the same as in the book.\n", "Also, with the accuracy score, we can see how good the model is in the validating subsample."], "metadata": {"_cell_guid": "d5744e87-e630-4faa-85fc-f769a7138994", "_uuid": "6e8ecaab218ec918348407f43c459fc01c716d71"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["from sklearn.linear_model import LogisticRegression\n", "from sklearn.metrics import accuracy_score\n", "lr = LogisticRegression(C=1000.0, random_state=0)\n", "lr.fit(X_train_std, y_train)\n", "y_pred = lr.predict(X_valid_std)\n", "print ((y_valid != y_pred).sum())\n", "print (accuracy_score(y_valid, y_pred))"], "metadata": {"_cell_guid": "b541172d-ab34-4e73-bbfb-26bcf2539840", "collapsed": true, "_uuid": "5cfaad0be2fb4aeca038f9906d77acf45947aaad"}}, {"cell_type": "markdown", "source": ["So, we have a model that successfully predicts about 79% of the subsample. This is no guarantee for success, as the model can perform well with the train information and not with the test data. This is called Overfitting.\n", "\n", "Now, to the test data, the list of passangers we don't know the answer (until we submit it). We have to process it the same way we did for the train data."], "metadata": {"_cell_guid": "c5763ad0-8666-4b33-9883-66f49564f287", "_uuid": "162cc24d752d022626004220956ca882145cde5d"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["df_test = pd.read_csv('../input/test.csv')\n", "print (df_test.info())\n", "print (df_test.head())"], "metadata": {"_cell_guid": "5b8389e2-995d-4a3c-b382-bd828365ceaa", "collapsed": true, "_uuid": "a9a68f6c084e218e32fd23b095105b7cbd387e00"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["df_test['sex_female'] = df_test['Sex'].apply(lambda x: 1 if x=='female' else 0)\n", "df_test['age_snr'] = df_test['Age'].apply(lambda x: 1 if x >= 50 else 0)\n", "df_test['age_mid'] = df_test['Age'].apply(lambda x: 1 if (x > 10 and x < 50) else 0)\n", "df_test['age_jnr'] = df_test['Age'].apply(lambda x: 1 if x <= 10 else 0)\n", "df_test['known_age'] = df_test['Age'].apply(lambda x: 0 if pd.isnull(x) else 1)\n", "df_test.loc[df_test['known_age'] == 0, 'age_mid'] = 1\n", "print (df_test.head())"], "metadata": {"_cell_guid": "e8703729-d53c-45c3-8ea3-5931ca4afe22", "collapsed": true, "_uuid": "494731755d42b32160af688d5468de24e740635a"}}, {"cell_type": "markdown", "source": ["Select our variables, standarize the data and do the prediction based on the lr model we defined above."], "metadata": {"_cell_guid": "2b4989e3-46ee-43b8-b318-ef9616f8ec73", "_uuid": "4dc79c5111e6e932f396b3ec54f525f25670d250"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["test = df_test.loc[:, ['PassengerId', 'Pclass', 'sex_female', 'age_jnr', 'known_age']]\n", "test_std = sc.transform(test)\n", "yy_test = lr.predict(test_std)"], "metadata": {"_cell_guid": "0f8f82ff-5a38-40e6-968c-93ac1a02caa5", "collapsed": true, "_uuid": "bb89acbf204c08a7f0e7ba3539a5c2ed458f0ca5"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["df_submission = pd.DataFrame({'PassengerId': test['PassengerId'],\n", "                                                 'Survived': yy_test})\n", "print (df_submission.head())\n", "df_submission.to_csv('accountant_titanic_01.csv', index=False)"], "metadata": {"_cell_guid": "4ac023ab-bf46-4508-afb4-94ac1a797c76", "collapsed": true, "_uuid": "b15871f71a4b204b890ddae56f1277038a81f0bb"}}, {"cell_type": "markdown", "source": ["Score: 0.77033, #6161 in the ranking (as of 11.56h CET Dec 24th). Not too bad for an accountant, I think. Can it be improved with limited ML knowledge? That will be in the next journal.\n", "Thanks all."], "metadata": {"_cell_guid": "936469de-e17a-453c-96c5-fcb1f99cabf7", "_uuid": "1e30588640d07d24d4e63330022103b49964d702"}}, {"cell_type": "markdown", "source": ["Next step: what if we use a list of classifiers and evaluate them using the valid subset? Then take the best performer and run the prediction in the test database."], "metadata": {"_cell_guid": "61c409cf-eff9-4a81-907f-e1ae514edb3c", "_uuid": "d69bfe723a52a7ffe143bbc631faf983eb579668"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["from sklearn.svm import SVC\n", "from sklearn.neighbors import KNeighborsClassifier\n", "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n", "from sklearn.naive_bayes import GaussianNB"], "metadata": {"_cell_guid": "50806def-b047-447c-b94c-c944f06e6eb9", "collapsed": true, "_uuid": "9805c012d67b19d075144d44d505cdafbad71bff"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["classif_list = [SVC(kernel='linear', C=0.025), SVC(gamma=2, C=1), KNeighborsClassifier(10),\n", "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n", "    AdaBoostClassifier(), GaussianNB()]"], "metadata": {"_cell_guid": "b3eb8614-6eaf-4892-8a50-31853a5f71f4", "collapsed": true, "_uuid": "adc55534409078d2b419ed0c4f0a9347e19eec99"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["def classif_func(data_train, label_train, data_valid, label_valid, classif):\n", "    classif.fit(data_train, label_train)\n", "    y_pred = classif.predict(data_valid)\n", "    return ((label_valid != y_pred).sum()), accuracy_score(label_valid, y_pred)"], "metadata": {"_cell_guid": "744d33bc-18c9-4087-ace0-8dbe12f89832", "collapsed": true, "_uuid": "ebd09127c32398c7db99794991c04ae82815062a"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["for classif in classif_list:\n", "    print (classif, classif_func(X_train_std, y_train, X_valid_std, y_valid, classif))"], "metadata": {"_cell_guid": "2d9840d6-aba8-4513-8986-bc112b9c09af", "collapsed": true, "_uuid": "74e88d5c740a22049dfe6f368a0c6e7fbbf4bf76"}}, {"cell_type": "markdown", "source": ["From the list, I take those that returned the lower number of errors (and higher accuracy rate), SVC(gamma) and KNeighborsClassifier, and run a range of parameters for each. I must admit, I know nothing about them, but will investigate."], "metadata": {"_cell_guid": "58e192f8-42c0-4221-ac5c-551097144e1a", "_uuid": "19aba330927510035b47279da3edc6a3b653d669"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["def classif_Kne(data_train, label_train, data_valid, label_valid, k):\n", "    classif = KNeighborsClassifier(k)\n", "    classif.fit(data_train, label_train)\n", "    y_pred = classif.predict(data_valid)\n", "    return ((label_valid != y_pred).sum()), accuracy_score(label_valid, y_pred)"], "metadata": {"_cell_guid": "3a024216-2e20-4ab9-829c-d18923fba5a8", "collapsed": true, "_uuid": "e5f52dbe5de79e0e01b333880b4fb2a28509bd23"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["for i in range(1,30):\n", "    print (i, classif_Kne(X_train_std, y_train, X_valid_std, y_valid, i))"], "metadata": {"_cell_guid": "a7bfd031-fe6f-48f0-9125-0650053261dc", "collapsed": true, "_uuid": "c80a7dd6dda3f3d621936b5b963bf92e539d0930"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["def classif_SVC(data_train, label_train, data_valid, label_valid, k):\n", "    classif = SVC(gamma=2, C=k)\n", "    classif.fit(data_train, label_train)\n", "    y_pred = classif.predict(data_valid)\n", "    return ((label_valid != y_pred).sum()), accuracy_score(label_valid, y_pred)"], "metadata": {"_cell_guid": "132acab7-423d-47c8-8acc-3b17e2a7f947", "collapsed": true, "_uuid": "827133bcdb61aec1f7ab748bd2542d78aa2f8249"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["for i in range(1,30):\n", "    print (i, classif_SVC(X_train_std, y_train, X_valid_std, y_valid, i))"], "metadata": {"_cell_guid": "540b84be-a62f-4ab5-aa9b-f719e1826700", "collapsed": true, "_uuid": "489ef614e0412d9da55ff5c67402aba73c60b337"}}, {"cell_type": "markdown", "source": ["Best results (not supergreat, but over 80%) are 20 for Kne and 3 for SVC. So let's run the submission with these classifiers and parameters."], "metadata": {"_cell_guid": "674adafb-a871-4b24-88c4-1a907fa102c3", "_uuid": "e1c0eaf1b30dbc4bfd333b76a51f77cf9497de46"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["classif = KNeighborsClassifier(20)\n", "classif.fit(X_train_std, y_train)\n", "yy_test = classif.predict(test_std)\n", "df_submission = pd.DataFrame({'PassengerId': test['PassengerId'],\n", "                                                 'Survived': yy_test})\n", "print (df_submission.head())\n", "df_submission.to_csv('accountant_titanic_02.csv', index=False)"], "metadata": {"_cell_guid": "8cfcac66-7308-423d-8c79-a199e96d6836", "collapsed": true, "_uuid": "228d36b298a9cbc40ce5d62eab6398565235c41d"}}, {"cell_type": "markdown", "source": ["Result 0.76076, worse than Logistic."], "metadata": {"_cell_guid": "93ae9d84-847b-463a-a8cc-014c0cab9ecd", "_uuid": "83f6e80bcb9e6d6dc88bfaa88cc950a20d4e4b9b"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["classif = SVC(gamma=2, C=3)\n", "classif.fit(X_train_std, y_train)\n", "yy_test = classif.predict(test_std)\n", "df_submission = pd.DataFrame({'PassengerId': test['PassengerId'],\n", "                                                 'Survived': yy_test})\n", "print (df_submission.head())\n", "df_submission.to_csv('accountant_titanic_03.csv', index=False)"], "metadata": {"_cell_guid": "dd7f22b0-f983-4976-accc-0c75099a9193", "collapsed": true, "_uuid": "67a5336c3e08ffa7c2cbbe9161233bcdfa8e1bf9"}}, {"cell_type": "markdown", "source": ["Result 0.68421, even worse!\n", "\n", "What happened? My interpretation, this is my first case of Overfitting, the model is good, not even very good, for the validation subset but does not perform well when out of this subset.\n", "\n", "How do you correct Overfitting? Some responses from Quora [here](https://www.quora.com/How-do-you-correct-overfitting)\n", "\n", "* You do not correct Overfitting but prevent it\n", "* Use regularization methods\n", "* Find more data (we don't have any more here)\n", "* Simplify the model (good idea, not sure what I did myself)\n", "* Gradient descent\n", "\n", "However, there is something I haven't done yet, improve the data. Let's work on this in my next journal entry.\n"], "metadata": {"_cell_guid": "9da662c5-b5bf-45fa-a034-0e562569c5fa", "_uuid": "f513a00b5f6eb4cf3047041baa27fc1849aa598c"}}, {"cell_type": "markdown", "source": ["First, I run another submission to be used as comparison."], "metadata": {"_cell_guid": "37c6581e-8597-43c6-819e-0a241f7b6a15", "_uuid": "a362c933c3f22209b8051084eab092f30c3bd3e6"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["classif = SVC(probability=True)\n", "classif.fit(X_train_std, y_train)\n", "yy_test = classif.predict(test_std)\n", "df_submission = pd.DataFrame({'PassengerId': test['PassengerId'],\n", "                                                 'Survived': yy_test})\n", "print (df_submission.head())\n", "df_submission.to_csv('accountant_titanic_04.csv', index=False)"], "metadata": {"_cell_guid": "cdb3c6ad-fdb2-4baf-b9ff-7372f7701b21", "collapsed": true, "_uuid": "f4f1cced3f9611148cb5d2bcb14ca4bad312550b"}}, {"cell_type": "markdown", "source": ["0.76555"], "metadata": {"_cell_guid": "a8667dfa-6138-45c0-a6e1-8de1b5233cfd", "_uuid": "8628e263d9ac6e778f26aa5ce7ec679e252ef0e0"}}, {"cell_type": "markdown", "source": ["**Journal 3**"], "metadata": {"_cell_guid": "d7927c61-6f90-41ce-bb38-fa754de4e7f5", "_uuid": "5f16b87a354efd43b2aa35db1983d184b2c4aea5"}}, {"cell_type": "markdown", "source": ["So far we have used data with basic transformations and just a few classifiers to prepare our submissions. Best score was 77% with the Logistic Regression, the first classifier. None of the other classifiers (KNeighbors, SVC, in two separated forms) actually worked. Surely, because I don't understand how they work.\n", "\n", "Next step will be to add and improve the data to achieve better results. For this, I will use the brilliant tutorial [a-data-science-framework-to-achieve-99-accuracy](https://www.kaggle.com/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy/notebook) by [LD Freeman](https://www.kaggle.com/ldfreeman3). Reading is a step by step tour on ML projects and a must-read (IMHO).\n", "\n"], "metadata": {"_cell_guid": "091886cf-2458-4b9e-be8f-3973960760ee", "_uuid": "0830a92356e270443be4b638f2357ddaefd5134a"}}, {"cell_type": "markdown", "source": ["Let's start with a clean set of data again."], "metadata": {"_cell_guid": "711c4226-afc8-4980-a96d-b01348dc2ae5", "_uuid": "149cadb259db8a09e215b25f402983e05a6c8ea5"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["df_train = pd.read_csv('../input/train.csv')\n", "df_test = pd.read_csv('../input/test.csv')\n", "df_tr1 = df_train.copy()\n", "df_te1 = df_test.copy()\n", "dataset = [df_tr1, df_te1]"], "metadata": {"_cell_guid": "5648ece5-82a3-4210-a047-f514118edc14", "collapsed": true, "_uuid": "981fa18ee1128c945de3c30e2591ca3f24824f70"}}, {"cell_type": "markdown", "source": ["The Completing (NaNs using median, mode) and Create (Title from name, Family Size and IsAlone atributes) steps.\n", "\n", "I'm confused about how title can have any influence in the survived yes/no, I guess it is a proxy for Sex? Same case for Embarked. Family Size and IsAlone is probaby an indicator, as maybe bigger families tried to stay together and were more difficult to save. Fare should be related to Pclass, maybe we can analyse correlation later.\n"], "metadata": {"_cell_guid": "99044b29-6cd1-4f51-87ec-3100803ee576", "_uuid": "d78a848d58af19f5bc9e5a685f08f6ad52c2ab76"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["for data in dataset:\n", "    data['Title'] = data['Name'].str.split(\",\", expand=True)[1].str.split(\".\", expand=True)[0]\n", "    data['Title_count'] = data.groupby('Title')['Title'].transform('count')\n", "    data['Title'].loc[data['Title_count'] <= 10] = 'Misc'\n", "    data['Age'].fillna(data['Age'].median(), inplace=True)\n", "    data['FamilySize'] = data['SibSp'] + data['Parch'] + 1\n", "    data['IsAlone'] = 1\n", "    data['IsAlone'].loc[data['FamilySize'] > 1] = 0\n", "    data['Embarked'].fillna(data['Embarked'].mode()[0], inplace=True)\n", "    data['Fare'].fillna(data['Fare'].median(), inplace=True)\n", "    "], "metadata": {"_cell_guid": "c552198d-59e7-4958-915f-15be04fbdd3c", "collapsed": true, "_uuid": "afa8db729fd6d34f3fa87912ba242dbcae8ccd92"}}, {"cell_type": "markdown", "source": ["Create new variables with grups of age (equal-width) and quantiles for Fare."], "metadata": {"_cell_guid": "41c2d2d2-d9f9-494b-a4e2-92020007c47b", "_uuid": "bf3e24eec44e4c343dbe12e091ea5b77bc52ce5b"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["for data in dataset:\n", "    data['AgeBin'] = pd.cut(data['Age'].astype(int), 5)\n", "    data['FareBin'] = pd.qcut(data['Fare'], 4)"], "metadata": {"_cell_guid": "b6563198-6d21-420a-a4ff-c94b2537d836", "collapsed": true, "_uuid": "33a5b4a558d607ceb70ed8ddafb4570b1d5dcc5e"}}, {"cell_type": "markdown", "source": ["And Convert.\n", "\n", "SK-Learn provides a tool (LabelEncoder) to create new columns assigning a value (0, 1, 2...) per each value in the variable. This way, we convert strings into integers, easier to be processed by algorithms.\n", "\n", "There is another tool called OneHotEncoder, to create yes/no (1/0) values. Something to explore later, for the moment, we continue with LabelEnconder."], "metadata": {"_cell_guid": "6b231af0-2702-48dc-9942-5f8dcc8a9a78", "_uuid": "291f327dba8e19116579e6431f98816ae03c2819"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["from sklearn.preprocessing import LabelEncoder\n", "encoder = LabelEncoder()\n", "for data in dataset:\n", "    data['Sex_code'] = encoder.fit_transform(data['Sex'])\n", "    data['Pclass_code'] = encoder.fit_transform(data['Pclass'])\n", "    data['Title_code'] = encoder.fit_transform(data['Title'])\n", "    data['Age_code'] = encoder.fit_transform(data['AgeBin'])\n", "    data['Fare_code'] = encoder.fit_transform(data['FareBin'])\n", "    data['Embarked_code'] = encoder.fit_transform(data['Embarked'])"], "metadata": {"_cell_guid": "dc55e76d-7837-47dd-9f60-889674baa1b4", "collapsed": true, "_uuid": "21edba155eace148e4f9573df69456b097889977"}}, {"cell_type": "markdown", "source": ["And now we can remove the string columns that will not add any calculation value."], "metadata": {"_cell_guid": "10277d19-30e6-4a79-8330-c32f9a8c1355", "_uuid": "429302e41597b84229d0de668f05b18f09cd766a"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["for data in dataset:\n", "    data.drop(['Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', \n", "               'Embarked', 'Title', 'Title_count', 'AgeBin', 'FareBin'], axis=1, inplace=True)"], "metadata": {"_cell_guid": "0d911e6c-6ef5-426f-a728-1b97513eacf7", "collapsed": true, "_uuid": "e02f929795933a620c09a3079daf880c182f49b7"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["for data in dataset:\n", "    print (data.info())"], "metadata": {"_cell_guid": "7e3d0d70-ae26-4bd6-910d-853e1e4aa9e2", "collapsed": true, "_uuid": "216962841fd4be09e8b2411a93cd173783302110"}}, {"cell_type": "markdown", "source": ["Now we can define a function to try a few classifiers and see results in the validation subsample."], "metadata": {"_cell_guid": "a0e8a4c9-fe72-49cd-b34f-817e5c180d08", "_uuid": "1dc3b36d6848363553b9f7a032969bb08f7f7b52"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["def model_classif(dataset, columnt_list, classif):\n", "    X = dataset.loc[:, column_list]\n", "    y = dataset['Survived']\n", "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.3, random_state = 0)\n", "    sc = StandardScaler()\n", "    sc.fit(X_train)\n", "    X_train_std = sc.transform(X_train)\n", "    X_valid_std = sc.transform(X_valid)\n", "    classif.fit(X_train_std, y_train)\n", "    y_pred = classif.predict(X_valid_std)\n", "    return (classif.__class__.__name__, (y_valid != y_pred).sum(), \n", "            accuracy_score(y_valid, y_pred), classif.get_params())"], "metadata": {"_cell_guid": "dceb2b06-9326-4a9e-9bae-8386ee142e77", "collapsed": true, "_uuid": "d818c906140201f4c721c0022de6692b4e176ed8"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["classif_list = [SVC(probability=True), \n", "                LogisticRegression(C=10.0, random_state=0), \n", "                KNeighborsClassifier(4), \n", "               GaussianNB()]\n", "column_list = ['Pclass', 'FamilySize', 'Sex_code', 'Title_code',\n", "                       'Age_code', 'Fare_code', 'Embarked_code']\n", "for classif in classif_list:\n", "    print (model_classif(df_tr1, column_list, classif))\n", "    print ('----------------')"], "metadata": {"_cell_guid": "81ffffdf-8c72-41b3-b949-c2f97e3e27dc", "collapsed": true, "_uuid": "e22c6a17a5d2fc45b34a9c3cc9fbe34d32ce0f7b"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["X = df_tr1.loc[:, column_list]\n", "y = df_tr1['Survived']\n", "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.3, random_state = 0)\n", "sc = StandardScaler()\n", "sc.fit(X_train)\n", "X_train_std = sc.transform(X_train)\n", "X_valid_std = sc.transform(X_valid)\n", "X = df_te1.loc[:, column_list]\n", "X_std = sc.transform(X)"], "metadata": {"_cell_guid": "22e06385-460a-40f7-bec8-13994027c6af", "collapsed": true, "_uuid": "c4eeb758712828865536f8678e96ad4155a781cc"}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["classif = SVC(probability=True)\n", "classif.fit(X_train_std, y_train)\n", "y_pred = classif.predict(X_std)\n", "y_hat = pd.DataFrame({'PassengerId': df_te1['PassengerId'], 'Survived': y_pred})\n", "print (y_hat.info())\n", "print (classif.__class__.__name__, classif.get_params())\n", "y_hat.to_csv('accountant_titanic_05.csv', index=False)"], "metadata": {"_cell_guid": "1c8d114e-0f51-42ed-be73-7eeea7773559", "collapsed": true, "_uuid": "5c80be6ab09d73ba5c9a86392672a3a0e2d87401"}}, {"cell_type": "markdown", "source": ["Score 0.78947, improvement from previous results!!! Jump to position 3,326 as of 12.00 pm CET (December 31st). Thanks to LD Freeman's data processing and also the SVC he used (SVC(Probability=True), not the one I used earlier)."], "metadata": {}}, {"cell_type": "code", "execution_count": null, "outputs": [], "source": ["classif = KNeighborsClassifier(4)\n", "classif.fit(X_train_std, y_train)\n", "y_pred = classif.predict(X_std)\n", "yy_hat = pd.DataFrame({'PassengerId': df_te1['PassengerId'], 'Survived': y_pred})\n", "print (yy_hat.info())\n", "print (classif.__class__.__name__, classif.get_params())\n", "yy_hat.to_csv('accountant_titanic_06.csv', index=False)"], "metadata": {"_cell_guid": "713c711a-2c26-4ac5-8178-693d96fc1309", "collapsed": true, "_uuid": "0e8055c59f260e0cabb257c09ae08b3ff2e5fa45"}}, {"cell_type": "markdown", "source": ["Score 0.77511, lower than SVC but also better than the previous, simple data Logistic Regression."], "metadata": {}}], "metadata": {"language_info": {"codemirror_mode": {"version": 3, "name": "ipython"}, "version": "3.6.3", "mimetype": "text/x-python", "name": "python", "pygments_lexer": "ipython3", "file_extension": ".py", "nbconvert_exporter": "python"}, "kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}}, "nbformat": 4, "nbformat_minor": 1}