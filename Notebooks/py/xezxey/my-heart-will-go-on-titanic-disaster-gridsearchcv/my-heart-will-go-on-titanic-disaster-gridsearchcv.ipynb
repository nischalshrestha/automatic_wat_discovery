{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":false,"_kg_hide-output":false},"cell_type":"markdown","source":"# Titanic Disaster\n![RMS Titanic](http://irishamerica.com/wp-content/uploads/2012/03/Titanic-at-Southhampton-docks.jpg)\n\n**History** : \nRMS Titanic is a largest ship afloat at a time(est. 1911). Titanic was under command by Edward Smith who also went down with the ship. Titanic start the journey from Southampton -> Cherbough -> Queenstown -> America. RMS Titanic hit an iceberg at 11.50 pm and sunk in 2.30 hours at 2.20 am.\nTitanic contains 2,435 passengers and 892 crews. Total is 3,327 lives (or 3,547 according to other sources).\n***\n*Titanic Route Map*\n![RMS Titanic Route Map Picture](https://localtvkstu.files.wordpress.com/2012/04/04-13-titanic-4.jpg?quality=85&strip=all)\n***\n[More Titanic Information By Wikipedia](https://en.wikipedia.org/wiki/RMS_Titanic)\n\n"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"# Dive into the data\n***\n**Features** \n1. Survival : 1 = Survive, 0 = Not Survive\n2. Pclass : Ticket class of passengers (1, 2 and 3)\n3. Sex : Male and Female\n4. Age : Age in years\n5. Sibsp : # of siblings / spouses aboard the Titanic\n6. Parch : # of parents / children aboard the Titanic\n7. Ticket : Ticket number\n8. Fare : Passenger fare\n9. Cabin : Cabin number\n10. Embarked : C = Cherboug, Q = Queenstown, S = Southampton\n***\nNaN is Not a Number.\n"},{"metadata":{"_kg_hide-input":false,"trusted":true,"_uuid":"f5f0abc58ec4efd0db128744cc5cb74335babb5a"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n#Import data sets\ntitanic_train_dataset = pd.read_csv(\"../input/train.csv\")\ntitanic_test_dataset = pd.read_csv(\"../input/test.csv\")\ntitanic_submission_form_dataset = pd.read_csv(\"../input/gender_submission.csv\")\n\n#Preview the data\ntitanic_train_dataset.head(n=5)\n#By running .info method you will see some missing value in ['Age'] and ['Cabin'] Columns than we will clean this in the next part\ntitanic_train_dataset.info()\n","execution_count":12,"outputs":[]},{"metadata":{"_uuid":"00c9b2101299198a57b53dd23812fdc936ed312a"},"cell_type":"markdown","source":"# Part I : Data Cleaning\n**Goal : Cleaning some missing data**\nFrom previous part, you can see the missing value from 2 columns\n1. Age : has 714 non-null float64 ------> from 814 records \n\n    *     First attempt : Find the regression model for predicting missing age. The regressors come from correlation between age and each feature. \n    *     Second attempt : I will use mean and std to fill missing values by random value in range [mean-std, mean+std].\n    \n2. Cabin : has 204 non-null object ------> from 814 records\n\n    *       I will assume that the 3rd class passengers will stay in any cabin of floor F or G by randoming.\n      \n    *       ** Note : In fact, only 1st and 2nd class passengers will have a cabin room number. So the other records make me assume that they 're 3rd class passengers. So it will effect to the CabinFloorScore and probability to survive too because 3rd class passengers will stay in only F or G floors. So the point that i want to explain is Cabin columns doesnt contain the missing value but in Titanic 3rd class passengers have no room, no cabin to stay. **\n      "},{"metadata":{"trusted":true,"_uuid":"af5c5125308ec8250d485d43c6f3cec3bbb27d06"},"cell_type":"code","source":"#Filling some missing data\n#Age\nmean_age = titanic_train_dataset['Age'].mean()\nstd_age = titanic_train_dataset['Age'].std()\ntitanic_train_dataset['Age'] = titanic_train_dataset['Age'].fillna(np.random.randint(low = mean_age - std_age, high = mean_age + std_age))\ntitanic_train_dataset[['Age']].describe()    #Re-Check that we have fill all of missing data\n#print(titanic_train_dataset['Name'].loc[titanic_train_dataset['Age'] < 1])\ntitanic_train_dataset['CategoricalAge'] = pd.qcut(titanic_train_dataset['Age'], q = 4)\n\n#CabinFloorScore\n#Use regular expression to match floor pattern\n#Set default floor for 3rd class ticket(GuestRoom in floor F and G)\nimport re\ncabin_pattern = re.compile(\"[a-zA-Z]\")\nGuestRoom_random_floor = [\"F\", \"G\"]\n\ncabin_floor_list = []\nfor cabin in titanic_train_dataset['Cabin']:\n    if pd.isnull(cabin):\n        cabin_floor_list.append(GuestRoom_random_floor[np.random.randint(2)])\n    else:\n        cabin_floor = re.findall(cabin_pattern, cabin)\n        cabin_floor_list.append(min(cabin_floor))\n\ntitanic_train_dataset['Cabin'] = cabin_floor_list\n\ntitanic_train_dataset.info()\ntitanic_train_dataset.head(5)\n","execution_count":13,"outputs":[]},{"metadata":{"_uuid":"226b2e6b510ab3e5065f2be773d04a3374d38482"},"cell_type":"markdown","source":"> > > "},{"metadata":{"_uuid":"8e76fef52a19758fe794769d4e0249ab1d02d3b3"},"cell_type":"markdown","source":"# Part II : Feature Engineer\n**GOAL : Creating new features from original data. Following below**\n1. FamilySize \n2. IsAlone\n3. CabinFloorScore\n4. FarePerPerson\n5. TitleScore"},{"metadata":{"_uuid":"5523abdebe729d5a8f202b7d2665f609640cf833"},"cell_type":"markdown","source":"**FamilySize : ** \nFamily size of passengers can be calculate by using following equation.\n> FamilySize = Sibsp + Parch + 1 > \n\n**IsAlone : **\nAlone passenger status can be find by using FamilySize Column\n> If FamilySize == 1 : Passenger is alone else : No\n\n**CabinFloorScore : **\n![CabinFloor](http://www.eyeopening.info/wp-content/uploads/building-titanic.jpg)\n\n\nFrom an evacuate plan, lifeboats will release to the sea from a deck. So the people are nearest to the deck will have more chance to survive. and i will give score like following this \n> \n* CabinFloor T (Special Cabin for First Class Ticket)  : Score = 7                                                                                                          \n* CabinFloor A                                                                   : Score = 6                                                                                                         \n* CabinFloor B                                                                   : Score = 5                                                                                                                \n* CabinFloor C                                                                   : Score = 4                                                                                                         \n* CabinFloor D                                                                   : Score = 3                                                                                                               \n* CabinFloor E                                                                   : Score = 2                                                                                                              \n* CabinFloor F                                                                   : Score = 1                                                                                                                  \n* CabinFloor G                                                                  : Score = 0                               \n\n\n**FarePerPerson : **\nFarePerPerson can be calculate by using following equation.\n> FarePerPerson = Fare / FamilySize\n\n**TitleScore : **\nFrom Name Column, we can know that there's a lot of name title given. So each name title has different priority and different priority make the chance of survive not the same.\n**Note : Name title observes from training data only ===> Title that not found in training set will be set as default and score will give to 0\n> I will give each title a score  following these conditions :\n1. Influence : Don., Col., Major., Capt., Sir., Mme., Lady., Countess.    => Score = 3\n2. Adult : Mr., Mrs., Miss., MS., Mlle., Dr., Rev.,                                               => Score = 2\n3. Child : Jonkheer, Master                                                                                        => Socre = 1\n\n***\n"},{"metadata":{"trusted":true,"_uuid":"39d59f54c9a44a58a1b3ade9c7713d8f143041c1","_kg_hide-input":false},"cell_type":"code","source":"#FamilySize\ntitanic_train_dataset['FamilySize'] = titanic_train_dataset['SibSp'] + titanic_train_dataset['Parch'] + 1\n\n#IsAlone\ntitanic_train_dataset.loc[titanic_train_dataset['FamilySize'] > 1, 'IsAlone'] = 0\ntitanic_train_dataset.loc[titanic_train_dataset['FamilySize'] == 1, 'IsAlone'] = 1\n\n#FarePerPerson\ntitanic_train_dataset['FarePerPerson'] = titanic_train_dataset['Fare'] / titanic_train_dataset['FamilySize']\ntitanic_train_dataset['CategoricalFarePerPerson'] = pd.qcut(titanic_train_dataset['FarePerPerson'], q = 4)\n\n#CabinFloorScore\n#Use regular expression to match floor pattern\n#Set default floor for 3rd class ticket(GuestRoom in floor F and G)\nimport re\ncabin_pattern = re.compile(\"[a-zA-Z]\")\nGuestRoom_random_floor = [\"F\", \"G\"]\n\ncabin_floor_list = []\nfor cabin in titanic_train_dataset['Cabin']:\n    if pd.isnull(cabin):\n        cabin_floor_list.append(GuestRoom_random_floor[np.random.randint(2)])\n    else:\n        cabin_floor = re.findall(cabin_pattern, cabin)\n        cabin_floor_list.append(min(cabin_floor))\n\ntitanic_train_dataset['CabinFloor'] = cabin_floor_list\n\n#Giving a score for each cabin floor.\ntitanic_train_dataset[\"CabinFloorScore\"] = 0\ntitanic_train_dataset[\"CabinFloorScore\"].loc[titanic_train_dataset['CabinFloor'] == \"T\"] = 7 \ntitanic_train_dataset[\"CabinFloorScore\"].loc[titanic_train_dataset['CabinFloor'] == \"A\"] = 6\ntitanic_train_dataset[\"CabinFloorScore\"].loc[titanic_train_dataset['CabinFloor'] == \"B\"] = 5\ntitanic_train_dataset[\"CabinFloorScore\"].loc[titanic_train_dataset['CabinFloor'] == \"C\"] = 4\ntitanic_train_dataset[\"CabinFloorScore\"].loc[titanic_train_dataset['CabinFloor'] == \"D\"] = 3\ntitanic_train_dataset[\"CabinFloorScore\"].loc[titanic_train_dataset['CabinFloor'] == \"E\"] = 2\ntitanic_train_dataset[\"CabinFloorScore\"].loc[titanic_train_dataset['CabinFloor'] == \"F\"] = 1\ntitanic_train_dataset[\"CabinFloorScore\"].loc[titanic_train_dataset['CabinFloor'] == \"G\"] = 0\n\n#TitleScore\nname_title = []\ntitle_pattern = re.compile(\"[a-zA-Z]{1,}\\.\")\nfor name in titanic_train_dataset['Name']:\n    name_title.append(re.findall(title_pattern, name)[0])\n    \ntitanic_train_dataset['Title'] = name_title\n#print(set(titanic_train_dataset['Title']))\n                                                                                            \n#We will give each title a score \n#1.Have power : Don., Col., Major., Capt., Sir., Mme., Lady., Countess.    => Score = 3\n#2.Adult : Mr., Mrs., Miss., MS., Mlle., Dr., Rev.,                        => Score = 2\n#3.Child : Jonkheer, Master                                                => Socre = 1\n\nprior1_title_influence = ['Don.', 'Col.', 'Major.', 'Capt.', 'Sir.', 'Mme.', 'Lady.', 'Countess.']\nprior2_title_adult = ['Mr.', 'Miss.', 'Ms.', 'Mlle.', 'Mrs.', 'Dr.', 'Rev.']\nprior3_title_kid = ['Jonkheer.', 'Master.']\n\ntitanic_train_dataset[\"TitleScore\"] = 0\ntitleScore_list = []\n\ndef give_title_score(title):\n    if title in prior1_title_influence:\n        return 3\n    elif title in prior2_title_adult: \n        return 2\n    elif title in prior3_title_kid: \n        return 1\n    else: \n        return 0\n\nfor i in range(len(titanic_train_dataset)):\n    titleScore_list.append(give_title_score(titanic_train_dataset['Title'][i]))\n\ntitanic_train_dataset['TitleScore'] = titleScore_list\n\n#Try to use the linear regression to predict missing age\nplt.figure(9)\nplt.scatter(titanic_train_dataset['Pclass'], titanic_train_dataset['Age'], c='red')\nplt.xlabel('Pclass')\nplt.ylabel('Age')\n\nplt.figure(10)\nplt.scatter(titanic_train_dataset['FarePerPerson'], titanic_train_dataset['Age'], c='blue')\nplt.xlabel('FarePerPerson')\nplt.ylabel('Age')\n\nplt.figure(11)\nplt.scatter(titanic_train_dataset['Sex'], titanic_train_dataset['Age'], c='green')\nplt.xlabel('Sex')\nplt.ylabel('Age')\n\nplt.figure(12)\nplt.scatter(titanic_train_dataset['IsAlone'], titanic_train_dataset['Age'], c='yellow')\nplt.xlabel('IsAlone')\nplt.ylabel('Age')\n\nplt.figure(13)\nplt.scatter(titanic_train_dataset['FamilySize'], titanic_train_dataset['Age'], c='black')\nplt.xlabel('FamilySize')\nplt.ylabel('Age')\n#There're no correlation between [Age] feature and other features. So I will use second approach(Random between [mean-std, mean+std]).\n","execution_count":14,"outputs":[]},{"metadata":{"_uuid":"da18ac8a653c9f336ff175c241e5d6c751a11716"},"cell_type":"markdown","source":"# Part III : Data Visualization\n** Goal : Visualizing the data for presentation easier**\n\nI will visualize the data to make you understand the data better and easier. Each features that I will visualize will be compare with 'Survival' columns.\n\n** Bar Plot : **\nComapring each feature to survived rate show us how much impact it does.\n 1. Sex Vs. Survival\n 2. Age Vs. Survival : Since [Age] is interval value and can't use as input in plt.bar(). So i will change \n 3. Pclass Vs. Survival\n 4. FamilySize Vs. Survival\n 5. IsAlone Vs. Survival\n 6. CabinFloorScore Vs. Survival\n 7. FarePerPerson Vs. Survival\n 8. TitleScore Vs. Survival\n "},{"metadata":{"_uuid":"b377352814880e0016e91da84f74144fd075cb1c","trusted":true,"scrolled":false},"cell_type":"code","source":"plt.rcParams[\"figure.figsize\"] = (10, 7)\n#1. Sex Vs. Survived\nplt.figure(1)\nsex_vs_survived = titanic_train_dataset[['Sex', 'Survived']].groupby(['Sex'], as_index=False).mean()\nprint(sex_vs_survived)\nplt.bar(sex_vs_survived['Sex'], sex_vs_survived['Survived'], tick_label=sex_vs_survived['Sex'], width = 0.5)\nplt.title('Sex Vs. Survived')\nplt.xlabel('Sex')\nplt.ylabel('Survived Rate')\n\n#2. Age Vs. Survived\nplt.figure(2)\nage_vs_survived = titanic_train_dataset[['CategoricalAge', 'Survived']].groupby(['CategoricalAge'], as_index=False).mean()\nage_vs_survived['AgeGroup'] = ['Child to Youth', 'Youth to Middle Aged', 'Middle Aged' ,'Middle Aged to Old']\nprint(age_vs_survived)\nplt.bar(age_vs_survived['AgeGroup'], age_vs_survived['Survived'], width=0.5)\nplt.title('Age Vs. Survived')\nplt.ylabel('Survived Rate')\nplt.xlabel('Age')\n\n#3. Pclass Vs. Survived\nplt.figure(3)\ntitanic_train_dataset[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean()\npclass_vs_survived = titanic_train_dataset[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean()\nprint(pclass_vs_survived)\nplt.bar(pclass_vs_survived['Pclass'], pclass_vs_survived['Survived'], tick_label=pclass_vs_survived['Pclass'])\nplt.title('Pclass Vs. Survived')\nplt.ylabel('Survived Rate')\nplt.xlabel('Pclass')\n\n#4. FamilySize Vs. Survived\nplt.figure(4)\nfamsize_vs_survived = titanic_train_dataset[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean()\nprint(famsize_vs_survived)\nplt.bar(famsize_vs_survived['FamilySize'], famsize_vs_survived['Survived'], tick_label=famsize_vs_survived['FamilySize'])\nplt.title('FamilySize Vs. Survived')\nplt.xlabel('FamilySize')\nplt.ylabel('Survived Rate')\nplt.plot(famsize_vs_survived['FamilySize'], famsize_vs_survived['Survived'], color='Red')\n\n#5. IsAlone Vs. Survived\nplt.figure(5)\nisalone_vs_survived = titanic_train_dataset[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()\nprint(isalone_vs_survived)\nplt.bar(isalone_vs_survived['IsAlone'], isalone_vs_survived['Survived'], tick_label=isalone_vs_survived['IsAlone'])\nplt.title('IsAlone Vs. Survived')\nplt.xlabel('IsAlone')\nplt.ylabel('Survived Rate')\n\n#6. CabinFloorScore Vs. Survived\nplt.figure(6)\ncabinfloorscore_vs_survived = titanic_train_dataset[['CabinFloorScore', 'Survived']].groupby(['CabinFloorScore'], as_index=False).mean()\nprint(cabinfloorscore_vs_survived)\nplt.bar(cabinfloorscore_vs_survived['CabinFloorScore'], cabinfloorscore_vs_survived['Survived'], tick_label=cabinfloorscore_vs_survived['CabinFloorScore'])\nplt.title('CabinFloorScore Vs. Survived')\nplt.xlabel('CabinFloorScore')\nplt.ylabel('Survived Rate')\n\n#7. FarePerPerson Vs. Survived\nplt.figure(7)\nfareperperson_vs_survived = titanic_train_dataset[['CategoricalFarePerPerson', 'Survived']].groupby(['CategoricalFarePerPerson'], as_index=False).mean()\nfareperperson_vs_survived['TicketGrade'] = ['Very Cheap', 'Cheap', 'Moderate', 'Expensive']\nprint(fareperperson_vs_survived)\nplt.bar(fareperperson_vs_survived['TicketGrade'], fareperperson_vs_survived['Survived'])\nplt.title('FarePerPerson Vs. Survived')\nplt.xlabel('FarePerPerson')\nplt.ylabel('Survived Rate')\n\n#8. TitleScore Vs. Survived\nplt.figure(8)\ntitlescore_vs_survived = titanic_train_dataset[['TitleScore', 'Survived']].groupby(['TitleScore'], as_index=False).mean()\nprint(titlescore_vs_survived)\nplt.bar(titlescore_vs_survived['TitleScore'], titlescore_vs_survived['Survived'], tick_label=['Influence', 'Adult', 'Kids'])\nplt.title('TitleScore Vs. Survived')\nplt.ylabel('Survived Rate')\nplt.xlabel('Title')","execution_count":15,"outputs":[]},{"metadata":{"_uuid":"9a01b3598466739e1b492ddd233c5c8ca4c7225f"},"cell_type":"markdown","source":"# ** Part III.I : Conclusion    ། ﹒︣ ‸ ﹒︣ །**\n\nFrom each feature, you can see there're statistic signaficant impact to the survived rate and they follow to the evacuation plan(1st class, women and kids are first priority). \n1. Sex Vs. Survival\n> [Female] has more chance to survived than [Male] because of the evacuation plan.     \n\n2. Age Vs. Survival\n> [Middle Aged] has the most chance to survived at 40%. Following by [Child to Youth] with 38%. This feature still have impact from the evacuation plan that let kids to get help first.\n\n3. Pclass Vs. Survival\n> Absolutely that higher [Pclass] has more chance to survived because of 2 factor.\n    1. Cabin : Only 1st class and 2nd class will have own cabin number(private room). They stay in \"floor T to floor E\". The evacuate ship will deploy from the deck(top of the ship) so the higher floor you are the higher chance you survive.\n    2. Priority : High class people are always taken care by staffs. So this will increase the survive rate. \n\n4. FamilySize Vs. Survival\n> [FamilySize] feature lead us to think in many different ways. From my hypothesis I will break down in many ways.\n    1. Too many cooks spoil the broth : You will see the most chance to survived is [FamilySize == 4] other will decrease either side(even many or less) like a \"Normal Distribution Curve\". So the number of people is a bit important.\n    ** In the other hand **\n    2. More people = More helpful & powerful : Just think you have someone to take care of you through the trouble. So this may increase the survived rate (Just insticnt!!!)\n    3. Old man & Kids : Big family maybe come from old people or many kids. So the survived rate will increase in this case\n    4. Big family = Probability to be a high class people and this will make some impact to survived rate from previous feature that we use.\n    \n    ** Note : There are so many way to think and every one impact will have some chaining to other impact too.** \n\n5. IsAlone Vs. Survival\n> [IsAlone] is opposite to the [FamilySize]. Alone has more chance to survived than not alone.\n\n6. CabinFloorScore Vs. Survival\n> As I say from evacuation plan, the evacuate ship will deploy from the deck.\"Higher floor you are, Higher chance you survive.\". So [CabinFloorScore] will impact to survived rate significally statistic.\n\n7. FarePerPerson Vs. Survival\n> More FarePerPerson = More you paid = More Class = More chance to survive => #MakeSense!!!\n\n8. TitleScore Vs. Survival\n> Title is one way to define your \"Social Status\". Some people have more influence than the others, and this will give your more chance to survive. Like a bias!!!. So you will see from correlation from the \"TitleScore Vs. Survival\" graph. The influence people and kids have around 50-60% to survived. By the way adults have only 30% to survive. This can show how much powerful that bias does.\n\n# So we will try to use all of these feature to train several learning models and take the best models."},{"metadata":{"_uuid":"8a2d76f37cabf4de3d1eb0113d05f739484e8810"},"cell_type":"markdown","source":"# Part IV : Training and Testing\n** Goal : Construct the models that can predict survival from unseen data **\n\nNow we 're coming to the half of our journey. We have feature that can tell us what the survival should have. So I will use these set of features to train several models and comparing each other to find the best model, the best parameter with the best accuracy.\n\n** Training Model **\n> \n1. K-Nearest Neighbors\n2. Decision Tree\n3. Random Forest\n4. Logistic Regression\n5. Naive Bayes\n\n** Training Statregy **\n> 1. Grid Search : Algorithm to find the best hyperparameter for training model from list of parameter and applying k-fold cross validation for validate model performance.\n\n***\n** Note : 1. K-fold cross validation is one way to validate model performance by splitting data set in to \"K-fold\" and \"K-iteration\". \"You can setting size of k by changing {cv} parameter**\n![K-fold cross validation](https://qph.fs.quoracdn.net/main-qimg-92a4fda85de5ac23353af74097eb6024-c)\nFrom the above figure, you can see in each iteration model is trained by different training set and tested by different test set. This can help us solving the \"Variance accuracy\". \n\n** Variance accuracy mean that if you change test set then the accuracy will change too much. ** \n![Judge model on 1 test set is suck](https://image.ibb.co/mNkuKo/pablo.png)"},{"metadata":{"_uuid":"0615b910c469db89e7297e0337f5c3addcc5b22e"},"cell_type":"markdown","source":"** Preparing training set and test set **\nI will prepare the training set and test set that will be use in grid search for training models. I will separate the dataset into 2 parts.\n1. Training Set : 90% of dataset\n2. Test Set : 10% of dataset\nThese will use to train, tuning and evaluate model by  gridsearch library.\n\n> \n** Note : Why we need to create the function ?**\n\n>  ** Ans : When we have new unseen dataset, we need to create features that have use in training step as a parameters. Unseen data has no parameters that we create from feature engineer step. This situation can occurs repeatedly. So We will refactoring code into function.**"},{"metadata":{"_uuid":"bbade3bae618189154ba4de2b316f2e7be8b3bf2","trusted":true,"collapsed":true},"cell_type":"code","source":"#Preparing dataset for training and testing \n#Creating the function names \"create_feature\" for create features from given data.\ndef create_feature(df, mode):\n    \"\"\"\n    create_feature function \n    1. Input parameters : \n        1.1 df : Dataframe variable for input dataset that you want to create feature from it.\n        1.2 mode : Mode selection (1 is for training dataframe, 0 for testing dataframe)\n            Different between 2 mode is the selected columns\n            - Training dataframe mode will return all of features include 'Survived' Columns\n            - Testing dataframe mode will return only feature\n    2. Returned value :\n        2.1 df : return a dataframe after finish a creating feature step\n    3. Function process : Take df and mode as input. Then create features and slice columns that we need to use for training and testing (depend on mode)\n    \"\"\"\n    \n    #Family Size\n    df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n    df['CategoryFamilySize'] = pd.cut(df['FamilySize'], bins=5)\n    \n    #IsAlone\n    df.loc[df['FamilySize'] > 1, 'IsAlone'] = 0\n    df.loc[df['FamilySize'] == 1, 'IsAlone'] = 1\n    \n    #CabinFloor\n    #Split cabin out of room number\n    import re\n    cabin_pattern = re.compile(\"[a-zA-Z]\")\n    GuestRoom_random_floor = [\"F\", \"G\"]\n    \n    cabin_floor_list = []\n    #print(titanic_train_dataset[\"Name\"].loc[titanic_train_dataset['Cabin'] > \"O\"])\n    for cabin in df['Cabin']:\n        if pd.isnull(cabin):\n            cabin_floor_list.append(GuestRoom_random_floor[np.random.randint(2)])\n        else:\n            cabin_floor = re.findall(cabin_pattern, cabin)\n            cabin_floor_list.append(min(cabin_floor))\n    \n    df['CabinFloor'] = cabin_floor_list\n    \n    #Score for each cabin floor\n    df[\"CabinFloorScore\"] = 0\n    df[\"CabinFloorScore\"].loc[df['CabinFloor'] == \"T\"] = 7\n    df[\"CabinFloorScore\"].loc[df['CabinFloor'] == \"A\"] = 6\n    df[\"CabinFloorScore\"].loc[df['CabinFloor'] == \"B\"] = 5\n    df[\"CabinFloorScore\"].loc[df['CabinFloor'] == \"C\"] = 4\n    df[\"CabinFloorScore\"].loc[df['CabinFloor'] == \"D\"] = 3\n    df[\"CabinFloorScore\"].loc[df['CabinFloor'] == \"E\"] = 2\n    df[\"CabinFloorScore\"].loc[df['CabinFloor'] == \"F\"] = 1\n    df[\"CabinFloorScore\"].loc[df['CabinFloor'] == \"G\"] = 0\n    \n    #Find how many passenger in each floor\n    cabin_floor_list = ['T', 'A', 'B', 'C', 'D', 'E', 'F', 'G']\n    #for i in range(len(cabin_floor_list)):\n    #    print(\"Cabin \" + cabin_floor_list[i] + \" : \" + str(df[\"CabinFloorScore\"].loc[df['CabinFloor'] == cabin_floor_list[i]].count()))\n    \n    #In test set : There's missing [fare] value as NaN. So I will fill this with mean\n    df['Fare'] = df['Fare'].fillna(np.mean(df['Fare']))   \n    \n    #Fare per person\n    df['FarePerPerson'] = df['Fare'] / df['FamilySize']\n    df['CategoricalFarePerPerson'] = pd.qcut(df['FarePerPerson'], q = 4)\n    \n\n    \n    \n    #VIP\n    #Impact\n    name_title = []\n    title_pattern = re.compile(\"[a-zA-Z]{1,}\\.\")\n    for name in df['Name']:\n        name_title.append(re.findall(title_pattern, name)[0])\n        \n    df['Title'] = name_title\n                                                                                                \n    #We will give each title a score \n    #1.Have power : Don., Col., Major., Capt., Sir., Mme., Lady., Countess.    => Score = 3\n    #2.Adult : Mr., Mrs., Miss., MS., Mlle., Dr., Rev.,                        => Score = 2\n    #3.Child : Jonkheer, Master                                                => Socre = 1\n    \n    prior1_title_powerful = ['Don.', 'Col.', 'Major.', 'Capt.', 'Sir.', 'Mme.', 'Lady.', 'Countess.']\n    prior2_title_adult = ['Mr.', 'Miss.', 'Ms.', 'Mlle.', 'Mrs.', 'Dr.', 'Rev.']\n    prior3_title_kid = ['Jonkheer.', 'Master.']\n    \n    df[\"TitleScore\"] = 0\n    TitleScore_list = []\n    \n    def give_title_score(title):\n        if title in prior1_title_powerful:\n            return 3\n        elif title in prior2_title_adult:\n            return 2\n        elif title in prior3_title_kid : \n            return 1\n        else: \n            return 0\n    \n    for i in range(len(df.index)):\n        TitleScore_list.append(give_title_score(df['Title'][i]))\n    \n    df['TitleScore'] = TitleScore_list    \n    \n    #Age\n    #No correlation between each features and age   \n    #Age have some missing values ---> Use mean, median or std to fill the nan\n    #Random between [mean-std, mean+std]\n    mean_age = df['Age'].mean()\n    std_age = df['Age'].std()\n    df['Age'] = df['Age'].fillna(np.random.randint(low = mean_age - std_age, high = mean_age + std_age))\n    df[['Age']].describe()    #Re-Check that we have fill all of missing data\n    df['CategoricalAge'] = pd.qcut(titanic_train_dataset['Age'], q = 4)\n    \n    \n    if mode:\n        df = df.loc[:, [\"Survived\", \"Pclass\", \"Sex\", \"Age\", \"FamilySize\", \n                                                              \"IsAlone\", \"CabinFloorScore\", \"FarePerPerson\", \n                                                              \"TitleScore\"]]\n    else:\n        df = df.loc[:, [\"Pclass\", \"Sex\", \"Age\", \"FamilySize\", \n                                                              \"IsAlone\", \"CabinFloorScore\", \"FarePerPerson\", \n                                                              \"TitleScore\"]]\n    \n    #Encoding string into number : Male = 1, Female = 0\n    from sklearn.preprocessing import LabelEncoder\n    labelencoder_sex = LabelEncoder()\n    df['Sex'] = labelencoder_sex.fit_transform(df['Sex'].values)\n    return df\n","execution_count":16,"outputs":[]},{"metadata":{"_uuid":"3067e998750ed186f4298be878d88216d3b8364c","trusted":true},"cell_type":"code","source":"#Calling function\ntitanic_train_dataset = pd.read_csv(\"../input/train.csv\")    #Re-import a titanic data set\ntitanic_train_dataset_for_training_step = create_feature(df=titanic_train_dataset, mode=1)\ntitanic_test_dataset_for_testing_step = create_feature(df=titanic_test_dataset, mode=0)\n\n#Showing some data\ntitanic_train_dataset_for_training_step.head(3)\ntitanic_test_dataset_for_testing_step.head(3)\n\n#To ensure your data is ready to train and test \ntitanic_train_dataset_for_training_step.info()\ntitanic_test_dataset_for_testing_step.info()\n\nX_train = titanic_train_dataset_for_training_step.iloc[:, 1:]\ny_train = titanic_train_dataset_for_training_step.iloc[:, 0]\n#Now, It's ready for training\n","execution_count":17,"outputs":[]},{"metadata":{"_uuid":"31bc5339e68ab91b5caf9d471bf526f489036cb8","trusted":true},"cell_type":"code","source":"#Training Phase\n#1.Model Usage : D.Tree, Forest, ANNs, Logistic Regression, KNN, Naive Bayes\n#2.Training Strategy : Grid Search\n#3.Testing Strategy : K-fold Cross Validation\n\n#Step 1. Creating model object from each class\n#Importing Model libraries\nfrom sklearn.neighbors import KNeighborsClassifier    #KNN\nfrom sklearn.tree import DecisionTreeClassifier    #D.Tree\nfrom sklearn.ensemble import RandomForestClassifier #Forest\nfrom sklearn.naive_bayes import GaussianNB    #Naive Bayes\nfrom sklearn.linear_model import LogisticRegression    #Logistic Regression\n\n#Importing Model Validation\nfrom sklearn.metrics import accuracy_score, log_loss, confusion_matrix\nfrom sklearn.cross_validation import cross_val_score\n\n#Initial Model Object from Class\nclf_knns = KNeighborsClassifier()\nclf_dtree = DecisionTreeClassifier()\nclf_forest = RandomForestClassifier()\nclf_logreg = LogisticRegression()\nclf_naive = GaussianNB()\n\nclassifiers = [\n    KNeighborsClassifier(),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    LogisticRegression(),\n    ]\n\nclassifiers = [clf_knns, clf_dtree, clf_forest, clf_logreg]\n    \n#Step 2. Delcare parameters for training and do a hyperparameter tuning by gridsearch\n# You can change your parameters here!!!\nparams_knns = [{'n_neighbors' : range(1, 100)}, {'metric' : ['minkowski']}, {'p' : [2]}]\nparams_dtree = [{'criterion' : ['gini', 'entropy']}, {'splitter' : ['random', 'best']}]\nparams_forest = [{'n_estimators' : range(1, 100)}, {'criterion':['entropy', 'gini']}]\nparams_logreg = [{'penalty' : ['l1', 'l2']}, {'solver' : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}, {'max_iter' : range(100, 1000)}]\n\nparameters = [\n        params_knns,\n        params_dtree,\n        params_forest,\n        params_logreg,\n        ]\n\n#Step 2. Training model using gridsearch\n#Importing Grid Search\nfrom sklearn.model_selection import GridSearchCV\n\nclf_best_acc = []\nclf_best_params = []\nclf_best_estimator = []\n\n        \ngrid_searchs = [] #grid_search_knns, grid_search_dtree, grid_search_forest, grid_search_svm, grid_search_logreg, grid_search_naive\n\n#Training and append all of the best result from each model to a list\nfor i in range(len(classifiers)):\n    grid_searchs.append(GridSearchCV(estimator=classifiers[i], param_grid=parameters[i], scoring='accuracy', cv=10, \n                                n_jobs=-1))   \n    grid_searchs[i].fit(X_train, y_train)\n\n    clf_best_acc.append(grid_searchs[i].best_score_)\n    clf_best_params.append(grid_searchs[i].best_params_)\n    clf_best_estimator.append(grid_searchs[i].best_estimator_)\n \nprint(\"Finishing Training\")","execution_count":18,"outputs":[]},{"metadata":{"_uuid":"7abd699bf7d2d00ad1113dfc1150cba3340ff755"},"cell_type":"markdown","source":"#  A training process will take times for 5-10 minutes(It depends on your parameters). You can minimize this training time by adjust parameters into a small size.  (ง'̀-'́)ง!!!\n\n** After fininshing training step, we will compare each model performance and choose the best one to be a predictor of titanic disaster problem. **\n"},{"metadata":{"trusted":true,"_uuid":"72d885ae3d1be7d05ab9d95060ecb33966bacfdb","scrolled":true},"cell_type":"code","source":"#best_classifier variable for storing best classifier from gridsearch as a dict : Key is the name of classifier, Value is list of [best accuracy, best parameters, best estimators]\nbest_classifier = {}\n\n#Store each classifier in dictionary \nfor i in range(len(classifiers)):\n    best_classifier[classifiers[i].__class__.__name__] = [clf_best_acc[i], clf_best_params[i], clf_best_estimator[i]]\n\n#Print out the result of each best classifier can do!!!\nfor key, value in best_classifier.items():\n    print(\"Classifier name : \" + str(key), end=\"\\n\")\n    print(\"Accuracy : \" + str(value[0]), end=\"\\n\")    #value[0] is best acccuracy\n    print(\"Best parameters : \" + str(value[1]), end=\"\\n\")    #value[1] is best parameters\n    print(\"Best estimator : \" + str(value[2]), end=\"\\n\")    #value[2] is best estimators\n    print(\"************************************************************************************************\", end=\"\\n\")\n   ","execution_count":19,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2774fd504bf9a09542fdf410b3b36f5b86b638e2"},"cell_type":"code","source":"#Comparing between each model performance\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = best_classifier.keys()\ny = list(value[0]*100 for key, value in best_classifier.items())\n\nfig, ax = plt.subplots()   \n\nwidth = 0.75 # the width of the bars \nind = np.arange(len(y))  # the x locations for the groups\nax.barh(ind, y, width, color=['blue', 'red', 'green', 'purple'])    # Make bar plot in horizontal line\nax.set_yticks(ind+width/2)\nax.set_yticklabels(x, minor=False)\nfor i, v in enumerate(y):\n    ax.text(v+.7, i , str(v) + '%', color='red', fontweight='bold')\nplt.title('Model Performance')\nplt.xlabel('Classifier Performance(Percentage)')\nplt.ylabel('Classifier name')   \n","execution_count":20,"outputs":[]},{"metadata":{"_uuid":"0d784bc36979c958bbb4401f24b89053b2e98fef"},"cell_type":"markdown","source":"# Part V : Submission on kaggle\nThe final step is submitting test set 's prediction to kaggle in the same format and form of \"gender_submission.csv\" file.\n\n** Note : A \"gender_submission.csv\" file is not the result of test set. It's just example to show how submission file look like and assuming all female survived. So do not test a test set with this result file. **\n\n"},{"metadata":{"trusted":true,"_uuid":"6991cf2c6bbde9dd7f8b6d2740ae3d69887ecad2"},"cell_type":"code","source":"#Testing on test set and create a submission file for submitting to kaggle.\ny_pred_submission = pd.DataFrame(grid_searchs[2].predict(titanic_test_dataset_for_testing_step))\ny_pred_submission['PassengerId'] = titanic_test_dataset['PassengerId']\ny_pred_submission.columns = ['Survived', 'PassengerId']\ny_pred_submission = y_pred_submission[['PassengerId', 'Survived']]\n\n#Make sure we have the same format for submission to kaggle\ny_pred_submission.head(3)\ntitanic_submission_form_dataset.head(3)\n\ny_pred_submission.info()\ntitanic_submission_form_dataset.info()\n\n#Let's goooooooooooooo\ny_pred_submission.to_csv('forest_submission.csv', index=False)\n\n","execution_count":21,"outputs":[]},{"metadata":{"_uuid":"25da8c7d4c8414ce334360ce112aff4402dd0826","trusted":true,"collapsed":true},"cell_type":"markdown","source":"# Part VI : Conclusion of our journey\nFinally, the long journey has ended. We come a long way and i will conclude everything we have passed.\n1. Part I - Data cleaning : We make a data clean by filling missing data\n2. Part II - Feature Engineer : From the given feature is not enough to make a powerful model because it didn't tell about passenger enough. So we will create some feature from given feature that can explain and identify passengers.\n3. Part III - Data visualisng : We visualise the data to make it more understandable.\n4. Part IV - Training and Testing : We have prepared data set for train and test using \"GridSearch\"\n5. Part V - Submission on Kaggle : We have created submission file following the example that we can use it for submit and get score from test set in kaggle.\n6. Part VI - The end of journey\n\n***\n"},{"metadata":{"_uuid":"ed1ca25a5a79873234566401c0667f9fa8469746"},"cell_type":"markdown","source":"# Message from author\n** Author : This is my first kernel and I'm trying to learn a machine learning. If there's something wrong or you want me to edit and add more information. Please tell me, I'm happy to see every comment. And I will answer it as soon as possible. ** \n# #Many thanks\n# # Enjoy and Have fun\n# #♥(ˆ⌣ˆԅ)"},{"metadata":{"_uuid":"bb86d0ba5421d53a462d5a773cda249397169e85"},"cell_type":"markdown","source":"![](https://i.pinimg.com/736x/9b/75/9f/9b759f7145d4589eba689f667dda6802--titanic-poster-titanic-facts.jpg)\n# ... RIP"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}