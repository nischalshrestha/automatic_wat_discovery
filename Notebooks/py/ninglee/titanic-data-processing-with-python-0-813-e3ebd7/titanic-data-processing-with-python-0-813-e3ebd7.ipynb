{"metadata": {"language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "nbconvert_exporter": "python", "mimetype": "text/x-python", "pygments_lexer": "ipython3", "name": "python", "file_extension": ".py", "version": "3.6.1"}, "kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}}, "nbformat": 4, "nbformat_minor": 0, "cells": [{"metadata": {"_uuid": "49aa494b0d2ef1b510ddeb7a973ba6856db16013", "collapsed": false, "_cell_guid": "6208096b-db97-4bf2-ba23-dbc09cad451e", "_execution_state": "idle"}, "source": "All of my effort in Titanic are contained in this Notebook. You can upvote it for encouragement if you can find some help from this Notebook.\nAs my purpose is to study the basic data process techniques by Titanic data set, I believe there still have many details can be dredged to improve my model, but I think it's better enough for me.<br/>\nI am a student from China, and Waiting to go to NEU for my postgraduate study. I know my great distance from good English expression. So, please forgive my poor English if it has brought you extra difficulty in reading.<br/>\nIt's so cool that I can find so many people from all over the world. I hope my English doesn't matter you.<br/>\n**Please contact me without hesitation If you have some suggestions on my Notebook or my English : ).**<br/>\nThanks for watching!", "outputs": [], "execution_count": null, "cell_type": "markdown"}, {"metadata": {"_uuid": "113ccd16253bc599b0238841be42380caec58f17", "trusted": false, "_cell_guid": "dec56701-3397-44f5-a00a-8dbdb63928d9", "_execution_state": "idle"}, "source": "import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import VotingClassifier\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\ntrain_data = pd.read_csv('../input/train.csv')\ntest_data = pd.read_csv('../input/test.csv')\n\nid_list = test_data[\"PassengerId\"]\ntrain_data = train_data.drop(['PassengerId'], axis=1)\ntest_data = test_data.drop(['PassengerId'], axis=1)", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "3cc6b2b0ae47ed985784f3cb37e5aaae4f0cb67d", "collapsed": false, "trusted": false, "_cell_guid": "ca372f83-b5af-40dc-bb58-f45c3a4ab3e7", "_execution_state": "idle"}, "source": "train_data.head()", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "418f416d1dc49f5b67c0ff6341819b92389a147a", "collapsed": false, "trusted": false, "_cell_guid": "209ffc42-fb0b-4987-9193-03c0f14acacc", "_execution_state": "idle"}, "source": "train_data.info()", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "4745117d010ae351f9326dd8a0e3974cfcc03a83", "collapsed": false, "_cell_guid": "0bb04860-e70a-4f39-913f-f182b8b2aafc", "_execution_state": "idle"}, "source": "There are some missing values exist in columns *Age*, *Embarked* and *Cabin*.\nIn general, we using mode to fill *Embarked* because it has just 2 missing value, and using linear regression to predict *Age* because the age maybe a very important feature in this question.\nIn fact, there are some missing value in column *Fare* at test set. with respect to *Fare*, I believe use mean value is better.", "outputs": [], "execution_count": null, "cell_type": "markdown"}, {"metadata": {"_uuid": "c5e6928818be1ae62b9d9a266d40fe88af2883f2", "collapsed": false, "trusted": false, "_cell_guid": "1f94d5c8-5a6b-4336-b182-4c0f1ce9bef6", "_execution_state": "idle"}, "source": "def feature_normalization(feat_vector):\n    # scala value to [-1, +1]\n    max_value = max(feat_vector)\n    min_value = min(feat_vector)\n    mean_value = feat_vector.mean()\n    return (feat_vector - mean_value) / (max_value - min_value)\n\n\n# It's not reasonable to transfer *Embarked* or *Pclass* into continuous value\ndef dummy(data, columns):\n    for column in columns:\n        if column not in data.columns:\n            continue\n        dummy_data = pd.get_dummies(data[column], drop_first=True)\n        # rename columns: column name + 1,2,3\n        num = len(dummy_data.loc[1, :])\n        dummy_data.columns = [column+str(x+1) for x in range(num)]\n        data = pd.concat([data, dummy_data], axis=1)\n\n        data = data.drop(column, axis=1)\n    return data", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "e8e1107a9436217fe0d2edf65ac37f53b3e5d89e", "collapsed": false, "trusted": false, "_cell_guid": "02ab4b76-3d8a-4a39-9f33-e3087d096863", "_execution_state": "idle"}, "source": "def cabin_extract(data):\n    # classify Cabin by fare\n    data['Cabin'] = data['Cabin'].fillna('X')\n    data['Cabin'] = data['Cabin'].apply(lambda x: str(x)[0])\n    data['Cabin'] = data['Cabin'].replace(['A', 'D', 'E', 'T'], 'M')\n    data['Cabin'] = data['Cabin'].replace(['B', 'C'], 'H')\n    data['Cabin'] = data['Cabin'].replace(['F', 'G'], 'L')\n    data['Cabin'] = data['Cabin'].map({'X': 0, 'L': 1, 'M': 2, 'H': 3}).astype(int)\n    return data\n\ntrain_data = cabin_extract(train_data)\ntest_data = cabin_extract(test_data)\n# show the connection between Cabin and Survive rate\nax = plt.axes()\ndata = train_data.groupby(['Cabin'])[['Survived']].count()\nsns.barplot(x=data.index, y=data['Survived'], alpha=0.8, color='violet', ax=ax)\ndata = train_data.groupby(['Cabin'])[['Survived']].sum()\nsns.barplot(x=data.index, y=data['Survived'], alpha=0.8, color='cornflowerblue', ax=ax)\nax.set_title('Survived or Not')\nsns.plt.show()\ntrain_data.groupby(['Cabin'])[['Fare']].mean()  # mean fare of each cabin class", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "47edacd941407cf76f5aa050c83965f4dee9826b", "collapsed": false, "trusted": false, "_cell_guid": "95a1d1ad-7f77-44b5-ac92-78747afd3010", "_execution_state": "idle"}, "source": "def SexByPclass(data):\n    data['Sex'] = data['Sex'].map({'female': 1, 'male':0})\n    data['Sex'] = data['Sex'].astype(int)\n    data['Pclass'] = data['Pclass'].map({1: 3, 2: 2, 3:1}).astype(int)\n    # data['SexByPclass'] = feature_normalization((data['SexTemp'] * data['Pclass']).astype(int))\n    data.loc[data['Sex']==0, 'SexByPclass'] = data.loc[data['Sex']==0, 'Pclass']\n    data.loc[data['Sex']==1, 'SexByPclass'] = data.loc[data['Sex']==1, 'Pclass'] + 3\n    data['SexByPclass'] = data['SexByPclass'].astype(int)\n    return data\n\ntrain_data = SexByPclass(train_data)\ntest_data = SexByPclass(test_data)\ntemp = pd.crosstab([train_data.SexByPclass,], train_data.Survived.astype(bool))\ntemp.plot(kind='bar', stacked=True, color=['violet','cornflowerblue'], alpha=0.8, grid=False)", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "b0667eda7f3c4690cc5ca7c9a092db88b929d718", "collapsed": false, "trusted": false, "_cell_guid": "4c5d9f28-0033-4f04-b79b-7c45f2465df3", "_execution_state": "idle"}, "source": "def fill_missing_embarked(data):\n    freq_port = data['Embarked'].mode()[0]\n    data['Embarked'] = data['Embarked'].fillna(freq_port)\n    data['Embarked'] = data['Embarked'].map({'S': 0, 'Q': 1, 'C': 2}).astype(int)\n    return data\n\ntrain_data = fill_missing_embarked(train_data)\ntest_data = fill_missing_embarked(test_data)\ntemp = pd.crosstab(train_data.Embarked, train_data.Survived)\ntemp.plot(kind='barh', stacked=True, color=['darksalmon','tomato'], grid=False)", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "b6f43cf0db90708c1b54dd16f738c77ccdd46b56", "collapsed": false, "_cell_guid": "a4782286-feab-4b1f-ad6c-d8cb866889d4", "_execution_state": "idle"}, "source": "We can find that 'S' is the most frequent one, and the passenger from port  'C' are more likely to survive.", "outputs": [], "execution_count": null, "cell_type": "markdown"}, {"metadata": {"_uuid": "3bd37c155ad0bcc75806d89aa847cd12844ab68f", "collapsed": false, "trusted": false, "_cell_guid": "5a6f5972-23e0-408d-be9e-b0989f52a542", "_execution_state": "idle"}, "source": "def fare_info(data):\n    data.loc[data.Fare.isnull(), 'Fare'] = data['Fare'].mean()\n    data['Fare'] = data['Fare'].astype(int)\n    data['Fare_stage'] = (data['Fare'] / 10).astype(int)\n    return data\n\ntrain_data = fare_info(train_data)\ntest_data = fare_info(test_data)\nsns.distplot(train_data['Fare_stage'], kde = True, rug = True)   \nsns.plt.show()\n\nax = sns.boxplot(x=\"Pclass\", y=\"Fare\", hue=\"Survived\", data=train_data);\nax.set_yscale('log')\n\ntrain_data = train_data.drop('Fare_stage', axis=1)\ntest_data = test_data.drop('Fare_stage', axis=1)\nsns.plt.show()", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "c82431827eb646c33f7727302003a246275fdf4a", "collapsed": false, "trusted": false, "_cell_guid": "539c5aea-1402-4b64-8e88-047fc6bd7113", "_execution_state": "idle"}, "source": "def name_extract(data):\n    # extract Title from name\n    data['Title'] = data.Name.str.extract('([A-Za-z]+)\\.', expand=False)\n    # delete rare title\n    data['Title'] = data['Title'].replace(['Lady', 'Countess', 'Capt', 'Col',\n        'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    data['Title'] = data['Title'].replace(['Mlle', 'Ms'], 'Miss')\n    data['Title'] = data['Title'].replace('Mme', 'Mrs')\n    data['Surname'] = data['Name'].apply(lambda x: str(x).split('.')[1].split(' ')[1])\n    data['Surname'] = data.Surname.str.replace('(', '')\n    title_map = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n    data['Title'] = data['Title'].map(title_map).astype(int)\n    data['SurnameLen'] = data['Name'].apply(lambda x: len(str(x).split('.')[1])).astype(int)\n                                            \n    return data.drop('Name', axis=1) \n\n\n\ntrain_data = name_extract(train_data)\ntest_data = name_extract(test_data)\n\ntemp = pd.crosstab(train_data.SurnameLen, train_data.Survived)\ntemp.plot(kind='bar', stacked=True, color=['orchid','cornflowerblue'], grid=False, figsize=(18, 5))", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "84ee0ac21c4ce445113a7d37a0a253c22f5456f1", "collapsed": false, "trusted": false, "_cell_guid": "7abb552f-2614-482d-a6f9-b9c086c9ca7e", "_execution_state": "idle"}, "source": "#sns.distplot(train_data['Title'], kde=True, rug=False)\n#plt.show()\nax = plt.axes()\ndata = train_data.groupby(['Title'])[['Survived']].count()\nsns.barplot(x=data.index, y=data['Survived'], alpha=0.8, color='orchid', ax=ax)\ndata = train_data.groupby(['Title'])[['Survived']].sum()\nsns.barplot(x=data.index, y=data['Survived'], alpha=0.8, color='cornflowerblue', ax=ax)\nax.set_title('Survived or Not')\nsns.plt.show()\nax = plt.axes()\ndata = train_data.groupby(['Title'])[['Survived']].mean()\nsns.barplot(x=data.index, y=data['Survived'], alpha=0.8, ax=ax)\nax.set_title('Survived Rate of Each Title')\nsns.plt.show()", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "9729a6935ebc8bb15274cfde9ff1f7ea82b3083f", "collapsed": false, "trusted": false, "_cell_guid": "36be1835-cbb4-4257-ad5e-05b1e24524b8", "_execution_state": "idle"}, "source": "def surname_extract(train, test):\n    Survived = train['Survived']\n    combine = pd.concat([train_data.drop('Survived',1),test_data])\n    combine['CommonSurname'] = np.where(combine.groupby(['Surname'])['Pclass'].transform('count') > 1, 1, 0)\n    combine.loc[combine['CommonSurname']==0, 'Surname'] = 'Rare'\n    train = combine.iloc[:len(train)]\n    train['Survived'] = Survived\n    test = combine.iloc[len(train):]\n    return  train, test\n\ntrain_data, test_data = surname_extract(train_data, test_data)\ntemp = pd.crosstab(train_data.Surname, train_data.Survived)\ntemp.plot(kind='bar', stacked=True, color=['darksalmon','tomato'], grid=False, figsize=(28, 5))\n#I can't find the way to use Surname as its completely sporadic disturbution\ntrain_data = train_data.drop('Surname', axis=1)\ntest_data = test_data.drop('Surname', axis=1)", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "f711059f138d4a9e70e0731d2c94737d397cfa44", "collapsed": false, "trusted": false, "_cell_guid": "e9611a00-713d-467c-a0f5-2e3dfaa84c31", "_execution_state": "idle"}, "source": "def family_info(data):\n    data['FamliySize'] = data['SibSp'] + data['Parch'] + 1\n    data['Alone'] = data['Alone'] = (data['SibSp'] == 0) & (data['Parch'] == 0)\n    data['Alone'] = data['Alone'].astype(int)\n    # you should delete linear relation between family-size and sibsp and parch\n    return data\n    \ntrain_data = family_info(train_data)\ntest_data = family_info(test_data)", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "47c49e32452263c69aca54da73017397e38c175a", "collapsed": false, "trusted": false, "_cell_guid": "9c3029c8-e9d2-498e-bd8b-5eaf8540a9e5", "_execution_state": "idle"}, "source": "def fill_missing_age(train_set, test_set):\n    '''\n    combina train and test data set,\n    using linear regression to fill missing value of age in two data sets\n    return: tuple constructed by train and test data set\n    '''\n    train_dropped = train_set[['Pclass', 'SibSp', 'Fare', 'Cabin', 'Title', 'FamliySize', 'Alone', 'Age']]\n    test_dropped = test_set[['Pclass', 'SibSp', 'Fare', 'Cabin', 'Title', 'FamliySize', 'Alone', 'Age']]\n    combine = pd.concat([train_dropped,test_dropped])\n    # training regression model\n    train = combine[combine.Age.notnull()]\n    model = LinearRegression()\n    model.fit(train.drop(['Age'], axis=1), train['Age'])\n    \n    data = combine.loc[combine.Age.isnull()]\n    predict_ages = model.predict(data.drop('Age', axis=1))\n    combine.loc[combine.Age.isnull(), 'Age'] = predict_ages\n    combine['Age'] = combine['Age'].astype(int)\n    train_set['Age'] = combine.iloc[:len(train_set)]['Age']\n    test_set['Age'] = combine.iloc[len(train_set):]['Age']\n\n    return train_set, test_set\n\ntrain_data, test_data = fill_missing_age(train_data, test_data)\n", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "c194aa6785eba226e6b1c9476ca11261eea1f596", "collapsed": false, "trusted": false, "_cell_guid": "1704b29c-60c4-4dc2-b2f0-5337e94a3ce3", "_execution_state": "idle"}, "source": "temp = pd.crosstab(train_data.Age, train_data.Survived)\nsns.kdeplot(train_data['Age'], shade=False, label='Train age distribution')   \nsns.kdeplot(train_data.loc[train_data['Survived']==1, 'Age'], shade=False, label='Survived distribution')   \nsns.kdeplot(test_data['Age'], shade=False, label='Test age distribution')   \nsns.plt.show()", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "6c3d82efd4223876721197cb11a53954df4998cf", "collapsed": false, "trusted": false, "_cell_guid": "27b0d077-83f8-4700-9efe-a6b7e99a74b6", "_execution_state": "idle"}, "source": "def age_extract(data):\n    data['IsChild'] = data['Age'] < 14\n    \n    data.loc[data['Age'] < 16, 'Age'] = 1\n    data.loc[(data['Age'] < 35) & (data['Age'] > 15.9), 'Age'] = 2\n    data.loc[(data['Age'] < 64) & (data['Age'] > 34.9), 'Age'] = 3\n    data.loc[data['Age'] > 63.9, 'Age'] = 4\n    data['Age'] = data['Age'].astype(int)\n    return data\n\ntrain_data = age_extract(train_data)\ntest_data = age_extract(test_data)\ntemp = pd.crosstab([train_data.Age, train_data.Sex], train_data.Survived)\ntemp.plot(kind='bar', stacked=True, color=['orchid','cornflowerblue'], grid=False, figsize=(8, 5))", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "5dbf9a7b4ad1f804b01bb92cb41c7b01a1d5aa66", "collapsed": false, "trusted": false, "_cell_guid": "9d382524-843c-4a89-9e10-58c9c151c0d4", "_execution_state": "idle"}, "source": "def SexByAge(data):\n    data.loc[data['Sex']==0, 'SexByAge'] = data.loc[data['Sex']==0, 'Age']\n    data.loc[data['Sex']==1, 'SexByAge'] = data.loc[data['Sex']==1, 'Age'] + 5\n    data['SexByAge'] = data['SexByAge'].astype(int)\n    return data\n\n\ntrain_data = SexByAge(train_data)\ntest_data = SexByAge(test_data)\ntemp = pd.crosstab(train_data.SexByAge, train_data.Survived)\ntemp.div(temp.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True, color=['orchid','cornflowerblue'], grid=False, figsize=(8, 5))", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "40dd8f7fbd814e8ed68dc331b2678af931578601", "collapsed": false, "_cell_guid": "519d6029-477e-416c-bfa1-da22af09939e", "_execution_state": "idle"}, "source": "**Children and women go first**", "outputs": [], "execution_count": null, "cell_type": "markdown"}, {"metadata": {"_uuid": "f2ec5ce8aaec5da24b5fe206088f93dc52fdb946", "collapsed": false, "trusted": false, "_cell_guid": "48de7676-7601-417a-a5ec-992efdd0b46b", "_execution_state": "idle"}, "source": "def ticket_extract(train, test):\n    data = pd.concat([train.drop('Survived',1),test])\n    Survived = train['Survived']\n    data['Ticket'] = data.Ticket.str.replace('.', '')\n    data['Ticket'] = data.Ticket.str.replace('/', '')\n    data['SharedTicket'] = np.where(data.groupby('Ticket')['Fare'].transform('count') > 1, 1, 0)\n\n    data['TicketNumLen'] = data['Ticket'].apply(lambda x: len(str(x).split(' ')[-1])).astype(int)\n    data['Ticket'] = data.Ticket.str.replace(' ', '')\n    data['TicketHead'] = data.Ticket.str.extract('(\\D*)', expand=False)\n    data['TicketHead'] = data['TicketHead'].replace('', 'NULL')\n    data = data.drop('Ticket', axis=1)\n    train = data.iloc[:len(train)]\n    train['Survived'] = Survived\n    test = data.iloc[len(train):]\n    return  train, test\n\n\n\ntrain_data, test_data = ticket_extract(train_data, test_data)\n\ndata_set = pd.concat([train_data.drop('Survived',1),test_data])\ntemp = data_set.groupby(['TicketHead'])[['Fare']].mean()\ntemp['Survived'] = train_data.groupby(['TicketHead'])[['Survived']].sum()\ntemp['Count'] = data_set.groupby(['TicketHead'])['Fare'].count()\ntemp['SurviveRatio'] = temp['Survived'] / temp['Count']\n\ntemp['Count'].plot(kind='bar', title='Ticket Head:Count')\nplt.show()\ntemp['Fare'].plot(kind='bar', title='Ticket Head:Mean Fare')\nplt.show()\nprint(temp)", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "2d617342cca99ddb71ad9bfd4e9cc47649e7b734", "collapsed": false, "trusted": false, "_cell_guid": "21465a5a-513a-4839-be93-d4045a53409e", "_execution_state": "idle"}, "source": "def transfer_ticket_head(data):\n    data['TicketHead'] = data['TicketHead'].replace('PC', '4')\n    data['TicketHead'] = data['TicketHead'].replace(['FC', 'WEP', 'SOC'], '2')\n    data['TicketHead'] = data['TicketHead'].replace(['CA', 'FCC', 'PPP', 'SC', 'SCAH', 'SCPARIS', 'SCParis', 'WC', 'NULL'], '3')\n    classes = ['A', 'AS', 'AQ','C', 'CASOTON', 'Fa', 'LINE', 'LP','PP', 'SCA', 'SCAHBasle', 'SCOW' ,'SOP', 'SOPP', 'SOTONO', 'SOTONOQ', \n               'SP', 'STONO', 'STONOQ', 'SWPP']\n    data['TicketHead'] = data['TicketHead'].replace(classes, '1')\n    data['TicketHead'] = data['TicketHead'].astype(int)\n    return data\n\ntrain_data = transfer_ticket_head(train_data)\ntest_data = transfer_ticket_head(test_data)\ntemp = pd.crosstab(train_data.TicketHead, train_data.Survived)\ntemp.div(temp.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True, color=['orchid','cornflowerblue'], grid=False, figsize=(8, 5))", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "3226097da9d9bf16d6b808b97cb81adf074f5134", "collapsed": false, "trusted": false, "_cell_guid": "359e4d37-61d5-4e46-8e64-9a12ffb13b53", "_execution_state": "idle"}, "source": "temp = pd.crosstab(train_data.TicketNumLen, train_data.Survived)\ntemp.plot(kind='bar', color=['orchid','cornflowerblue'], grid=False, figsize=(8, 5))\nplt.show()", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "0acc5ff4dffc2de9d43191e5b544d83d26794e13", "collapsed": false, "trusted": false, "_cell_guid": "f0159441-c3b5-4ea2-aa51-704713810b11", "_execution_state": "idle"}, "source": "corr = train_data.corr()\nf, ax = plt.subplots(figsize=(25,16))\nsns.plt.yticks(fontsize=18)\nsns.plt.xticks(fontsize=18)\n\nsns.heatmap(corr, cmap='inferno', linewidths=0.1,vmax=1.0, square=True, annot=True)", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "6c211b36e90c4ea1844de1dc6d81c1cd07e57689", "collapsed": false, "_cell_guid": "0498e062-6988-42fb-b1b3-2fa77d765114", "_execution_state": "idle"}, "source": "The good correlation between *Embarked* and *Survived* makes me wonder.  I also find out some better score in column *Embarked* of heap map upper. So, may we can discover the mysterious of *Embarked* in the follow joint graph.", "outputs": [], "execution_count": null, "cell_type": "markdown"}, {"metadata": {"_uuid": "3ad62ca1fba58ca08a6108fcd22e7c922c34aa5e", "collapsed": false, "trusted": false, "_cell_guid": "7a91386b-0634-4362-94e5-cbf1b166029b", "_execution_state": "idle"}, "source": "combine = pd.concat([train_data.drop('Survived', axis=1), test_data])\ncombine['EmbarkedTemp'] = combine['Embarked'].map({0: 'S', 1: 'Q', 2: 'C'})\nplotVars = ['Fare', 'Cabin', 'Pclass']\nsns.set()\nsns.pairplot(combine, vars=plotVars, hue='EmbarkedTemp', kind='reg')\nsns.plt.show()\ndel combine", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "874807c34cdcfd3dcdce6ee568b204a6ac7ab0ae", "collapsed": false, "_cell_guid": "72f299fe-b7a0-492c-b105-875cdd18448f", "_execution_state": "idle"}, "source": "We can find, people from embarked port 'C' have higher ticket fare and better Pclass and better cabin than 'S' and 'Q'.", "outputs": [], "execution_count": null, "cell_type": "markdown"}, {"metadata": {"_uuid": "a1dd71f63614ce487495b01bcac322f15066eb80", "collapsed": false, "_cell_guid": "2d13c433-9c22-46b7-b8bf-1129aef65239", "_execution_state": "idle"}, "source": "We can find, the people from 'C' embarked port are in higher fare and better ticket class and better cabin than 'S' and 'Q'.", "outputs": [], "execution_count": null, "cell_type": "markdown"}, {"metadata": {"_uuid": "ea8e5ed7a00589ae5be31184dc005ea27676090c", "collapsed": false, "trusted": false, "_cell_guid": "2b5f4fa8-9d26-4aab-84a0-4f7c775a622b", "_execution_state": "idle"}, "source": "plotVars = ['Fare', 'Sex','Age', 'FamliySize', 'Cabin', 'SurnameLen', 'Pclass', 'Title']\nsns.set()\nsns.pairplot(train_data, vars=plotVars, hue='Survived')\nsns.plt.show()", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "7149f61ab28633bc015540b156d86e243e9a933f", "collapsed": false, "trusted": false, "_cell_guid": "ca8c498f-f73e-44e3-8c04-cdfda05283f9", "_execution_state": "idle"}, "source": "# Embarked should be transformed to one-hot variable\ntrain_data = dummy(train_data, ['Embarked'])\ntest_data = dummy(test_data, ['Embarked'])", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "34cbf5d8046808c127d4f17b742a530a10bf4d81", "collapsed": false, "trusted": false, "_cell_guid": "54e76b3f-90d3-444f-98e4-f9086ff15b9a", "_execution_state": "idle"}, "source": "train_data.info()", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "516b886328f641d604b52af492a0f0ce82d55fa5", "collapsed": false, "_cell_guid": "6245d256-aac6-42fc-b64d-7748ad074a52", "_execution_state": "idle"}, "source": "##Random Forest\nIn my codes, the RF will be the most important classifier in voting", "outputs": [], "execution_count": null, "cell_type": "markdown"}, {"metadata": {"_uuid": "a178d17956b6132f18df6783e016b4f46a84b27e", "collapsed": false, "trusted": false, "_cell_guid": "b83eb1d8-ee21-4567-a51f-b671e888b2a8", "_execution_state": "idle"}, "source": "# search for the best parameters of random forest\ndef parameter_evaluate(data):\n    clf_ev = RandomForestClassifier()\n    x, y = data.drop(['Survived'], axis=1), data['Survived']\n    parameters = {'n_estimators': [100, 300], 'max_features': [3, 4, 5, 'auto'],\n                  'min_samples_leaf': [9, 10, 12], 'random_state': [7]}\n    grid_search = GridSearchCV(estimator=clf_ev, param_grid=parameters, cv=10, scoring='accuracy')\n    print(\"parameters:\")\n    # train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.2, random_state=42)\n    grid_search.fit(x, y)\n    print(\"Best score: %0.3f\" % grid_search.best_score_)\n    print(\"Best parameters set:\")\n    bsp = grid_search.best_estimator_.get_params()  # the dict of parameters with best score\n    for param_name in sorted(bsp.keys()):\n        print(\"\\t%s: %r\" % (param_name, bsp[param_name]))\n    return bsp\n\n# parameters = parameter_evaluate(train_data)  \n# we don't need to search everytime after getting best parameters\nparameters = {'n_estimators': 100, 'max_features': 5, 'min_samples_leaf': 10, 'random_state': 7}\nrf = RandomForestClassifier(**parameters)\nrf.fit(train_data.drop(['Survived'], axis=1), train_data['Survived'])", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "b30c9b7649ba5e31f7e4f655cd2f400d44f0b9be", "collapsed": false, "trusted": false, "_cell_guid": "022f4e3a-69a0-430f-922d-b0f279aa1086", "_execution_state": "idle"}, "source": "names = train_data.drop(['Survived'], axis=1).columns\nratios = rf.feature_importances_\n\nfeature_important = pd.DataFrame(index=names, data=ratios, columns=['importance'])\nfeature_important = feature_important.sort_values(by=['importance'], ascending=True)\nfeature_important.plot(kind='barh', stacked=True, color=['cornflowerblue'], grid=False, figsize=(8, 5))", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "99a8ad7f88422f4aae1708ba99c74f20b9d4ebb0", "collapsed": false, "_cell_guid": "59db0521-54d0-43b1-ab06-b442fde73f27", "_execution_state": "idle"}, "source": "Then, I would like to delete some features which will make a bad influence on model. The Basis that I delete these features are **the feature importance** and **the heat map between features** Above. <br/>\nMoreover, if there is a linear relation between feature A and B, remove A or B even it's much important. Just as *SexByAge* and *SexByAgeByPclass*, I got a higher score after removing *SexByAge*. ", "outputs": [], "execution_count": null, "cell_type": "markdown"}, {"metadata": {"_uuid": "0beca500deb91d9741509e9f2cde864ea5fe9ef3", "collapsed": false, "_cell_guid": "37ef4686-cc90-48ca-b252-9c7fa1a95aa6", "_execution_state": "idle"}, "source": "##Logistic Regression\nBecause I need to keep the accuracy of RF, so I decide to bypass the process feature normalization.", "outputs": [], "execution_count": null, "cell_type": "markdown"}, {"metadata": {"_uuid": "efab8e3dbd29b0077354f9b9939cac032f6c978d", "collapsed": false, "trusted": false, "_cell_guid": "909be1a6-b6d2-4bee-b8e2-135e52be6f36", "_execution_state": "idle"}, "source": "lr = LogisticRegression(random_state=7)\nlr.fit(train_data.drop(['Survived'], axis=1), train_data['Survived'])", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "4427dcd05332ad050cd5e03bbcf046c4a7473d89", "collapsed": false, "_cell_guid": "baeef155-34e0-409c-b206-572285e541af", "_execution_state": "idle"}, "source": "##K Neighbors", "outputs": [], "execution_count": null, "cell_type": "markdown"}, {"metadata": {"_uuid": "aee7309c1255ebda4a34f397993aa94c9be5ba91", "collapsed": false, "trusted": false, "_cell_guid": "0af948a6-0ad2-42fc-b1d0-d3b404e9d94b", "_execution_state": "idle"}, "source": "knn = KNeighborsClassifier()\nknn.fit(train_data.drop(['Survived'], axis=1), train_data['Survived'])", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "fd77afac581db626b64351151a68fe5dccacd942", "collapsed": false, "_cell_guid": "562bcb36-8026-46ad-8a5b-6bb4499ea7b0", "_execution_state": "idle"}, "source": "##Ensemble RF, LR, KNN by voting", "outputs": [], "execution_count": null, "cell_type": "markdown"}, {"metadata": {"_uuid": "8d0cf0af5116fb98eade55d1711ee086c79f78db", "collapsed": false, "trusted": false, "_cell_guid": "6e54ae43-de73-4825-bd45-c547bc2cb6a1", "_execution_state": "idle"}, "source": "eclf1 = VotingClassifier(estimators=[\n        ('lr', lr), ('rf', rf), ('knn', knn)], voting='soft', weights=[1, 2, 1])\neclf1 = eclf1.fit(train_data.drop(['Survived'], axis=1), train_data['Survived'])\nresults = eclf1.predict(test_data)\noutput = pd.DataFrame({'PassengerId': id_list, \"Survived\": results})\noutput.to_csv('prediction.csv', index=False)", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "137ebcad29cd1490adc4168afdc264222208d301", "collapsed": false, "_cell_guid": "1f47154d-948d-4d14-9a61-39422e30e3e5", "_execution_state": "idle"}, "source": "After voting with appropriate features, I got 0.813. <br/>", "outputs": [], "execution_count": null, "cell_type": "markdown"}]}