{"cells":[{"metadata":{"trusted":false,"collapsed":true,"_uuid":"fe046dd0dca61b3a560f7b1b0fcece412215eede"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport nltk\nimport plotly\nimport re\n          \nplotly.offline.init_notebook_mode() # run at the start of every notebook\nimport cufflinks as cf\n\ncf.go_offline()\ncf.getThemes()\nfrom plotly import __version__\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n%matplotlib inline\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nfrom IPython.display import display\n\n# Algorithms\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"0a2c29bcaa6aedd771829fa02d05712871dfb1a0"},"cell_type":"code","source":"df_train = pd.read_csv('../input/train.csv')\ndf_test = pd.read_csv('../input/test.csv')\nfull_data = [df_train,df_test]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"20b0285b07f0b159657275aafd19c192a2a0a724"},"cell_type":"code","source":"df_train.info()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"c721d127457888d57137edd629394a042ff69987"},"cell_type":"code","source":"# Function to calculate no. of null values with percentage in the dataframe\ndef null_values(DataFrame_Name):\n    \n    sum_null = DataFrame_Name.isnull().sum()\n    total_count = DataFrame_Name.isnull().count()\n    percent_nullvalues = sum_null/total_count * 100\n    df_null = pd.DataFrame()\n    df_null['Total_values'] = total_count\n    df_null['Null_Count'] = sum_null\n    df_null['Percent'] = percent_nullvalues\n    df_null = df_null.sort_values(by='Null_Count',ascending = False)\n\n    return(df_null)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"90a8a55c62f15f8bd01d0ec1ffd3968f80ac12fa"},"cell_type":"code","source":"null_values(df_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"7c6b02c8b6ebe94de3de3f74f4ab6ec9e5f5eebb"},"cell_type":"code","source":"null_values(df_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"d1ba8f71a757d1fc1a5da3ee925680a53d2ece26"},"cell_type":"code","source":"df_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"2742925d15add5313fc32ad169b02b34d9ae5113"},"cell_type":"code","source":"df_train.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c7ace3957adb5a47afbad973a2dd4e19789f5032"},"cell_type":"markdown","source":"# Correlation"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"299a063bd9910e7c37e410ee4d9bb5a58afe42f1"},"cell_type":"code","source":"## get the most important variables. \ncorr = df_train.corr()**2\ncorr.Survived.sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"f1ed74d1cb6dead98ffa26d3400513d8bcd6799f"},"cell_type":"code","source":"## heatmeap to see the correlation between features. \n# Generate a mask for the upper triangle (taken from seaborn example gallery)\nmask = np.zeros_like(df_train.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nplt.subplots(figsize = (15,12))\nsns.heatmap(df_train.corr(), \n            annot=True,\n            mask = mask,\n            cmap = 'RdBu_r',\n            linewidths=0.1, \n            linecolor='white',\n            vmax = .9,\n            square=True)\nplt.title(\"Correlations Among Features\", y = 1.03,fontsize = 20);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0fdfeff79715a6bc70df28ea3139a5b6f8b9693d"},"cell_type":"markdown","source":"# Pclass"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"3b1d47b36f241c905b890bc8e112fc685e5d3ddd"},"cell_type":"code","source":"# Lets start with Pclass column - Already an integer - good\n# Lets check the impact of this column on the survived column in the train dataset.\n# We will calculate mean of survived people in each class - This will tell us how many survived out of total for each class\ndf_train[['Pclass','Survived']].groupby(['Pclass'], as_index=False).mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1b05e776a308d627c4876d46908eac0a8e911cb8"},"cell_type":"markdown","source":"From the above calculation, it seems like passenger from class1 ( mostly rich ) survived with a maximum percentage\nand passengers from a lower class survived least"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"2faabb2b491ae47dab599b552b55763b6da62320"},"cell_type":"code","source":"sns.barplot('Pclass','Survived', data=df_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"d78c7c61ac992372e6acb8f3a82962af9b4f6b43"},"cell_type":"code","source":"# Kernel Density Plot\nfig = plt.figure(figsize=(15,8),)\n## I have included to different ways to code a plot below, choose the one that suites you. \nax=sns.kdeplot(df_train.Pclass[df_train.Survived == 0] , \n               color='gray',\n               shade=True,\n               label='not survived')\nax=sns.kdeplot(df_train.loc[(df_train['Survived'] == 1),'Pclass'] , \n               color='g',\n               shade=True, \n               label='survived')\nplt.title('Passenger Class Distribution - Survived vs Non-Survived', fontsize = 25)\nplt.ylabel(\"Frequency of Passenger Survived\", fontsize = 15)\nplt.xlabel(\"Passenger Class\", fontsize = 15)\n## Converting xticks into words for better understanding\nlabels = ['Upper', 'Middle', 'Lower']\nplt.xticks(sorted(df_train.Pclass.unique()), labels);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bd6098c567e5c151cc1c8f6ef8473b6406b3714e"},"cell_type":"markdown","source":"# Sex"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"2b2fee2b0365be94b28f5f5f1a4a8a516a38260a"},"cell_type":"code","source":"females=df_train['Sex'].apply(lambda x: x.count('female')).sum()\nprint('Total males=',891-females)\nprint('Total females=',females)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"3ed89cfdcbd850ed089a53e8cddad4f578bf3dd3"},"cell_type":"code","source":"# Now lets focus on the Sex column and evaluate its impact on the survived column\ndf_train[['Sex','Survived']].groupby(['Sex'],as_index = False).mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0704a719232fcf3409306db8a85001057452a6fb"},"cell_type":"markdown","source":"From the above calculation it is clear that the female survival rate is much higher than the male survivor rate"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"1a056c45d8a25b0cac2a878e676ba0340ea5aa9a"},"cell_type":"code","source":"sns.barplot(x='Sex', y='Survived', data=df_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a925669c790384c75456181ece3d16f1fe9c0abd"},"cell_type":"markdown","source":"# Embarked"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"558e00977a0d537985da49474893110b1c8b18bf"},"cell_type":"code","source":"df_train[['Embarked','Survived']].groupby(['Embarked'],as_index = False).mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c1039ea5af0a41a35c12a7098761143d8e8ff9a8"},"cell_type":"markdown","source":"People with destination C = Cherbourg (C = Cherbourg, Q = Queenstown, S = Southampton) survived with highest percentage"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"510494f033571184fcae0d9397950bacb4d5f077"},"cell_type":"code","source":"sns.barplot(x='Embarked', y='Survived', data=df_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"826a71bec5bb4d8fabcac50f54b58216fe3f7a00"},"cell_type":"markdown","source":"# SibSp & Parch = Family_members"},{"metadata":{"_uuid":"ce99225480c3fc9b16c09f8494321cf8d5ada63d"},"cell_type":"markdown","source":"Lets create a new feature column by combining sibling/spouse & parent/children column"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"425361793fa4e9a536daf3071a2521a1d1988619"},"cell_type":"code","source":"df_train['Family_members'] = df_train['SibSp'] + df_train['Parch']\ndf_test['Family_members'] = df_test['SibSp'] + df_test['Parch']\ndf_train[['Family_members','Survived']].groupby(['Family_members'],as_index=False).mean()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8b5dd9297ed073dfc9f789245c59bc4e93974c19"},"cell_type":"markdown","source":"From the above calculation, we can conclude that - Survival percentage is higher when Family members are #1,2,3\nIt is less when you are alone or have family members > 3"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"54ce7513df38bffe2cc1ba99e093ef6721a5a69e"},"cell_type":"code","source":"sns.barplot(x='Family_members', y='Survived', data=df_train)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"c96324a12849eb184ebfefca83f53386af167385"},"cell_type":"code","source":"df_train = df_train.drop(['PassengerId'],axis=1)\n#df_test = df_test.drop(['PassengerId'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"601b6007ab0f93cd32a7d6c168a6e8f4bb5176c3"},"cell_type":"code","source":"full_data = [df_train,df_test]\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fd320de50e32d25ffdea6354fc39e155e82e0efe"},"cell_type":"markdown","source":"# Lets focus on the Missing Values\nCabin - Removing this column from the dataset- 80% missing values"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"aec06a044b274825a7874117b52460c0bcf93aff"},"cell_type":"code","source":"df_train = df_train.drop(['Cabin','Ticket'],axis=1)\n\ndf_test = df_test.drop(['Cabin','Ticket'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"badb95999f0b38c78b941d50e4692a7b5a8b5b01"},"cell_type":"code","source":"full_data = [df_train,df_test]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"10f185c691054b1316e49a3ec5bc7c48ae27d004"},"cell_type":"code","source":"for dataset in full_data:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(df_train['Title'], df_train['Sex'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"32c550b699a4451746322a774c6e9beb8a7a76f6"},"cell_type":"code","source":"for dataset in full_data:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \ndf_train[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"064590f480fcb709ac7301050351ca0784ecbf01"},"cell_type":"code","source":"sns.barplot(x='Title', y='Survived', data=df_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"6c6ad9cf616b5643c88f32a55d9fa1ee6d309415"},"cell_type":"code","source":"df_train","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"ce6ccf1aa85a13ac8b83cfa4019065d7c89a594d"},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder,OneHotEncoder,LabelBinarizer\n#cat_features = df_train['Title']\n#encoder = LabelBinarizer()\n#new_cat_features = encoder.fit_transform(cat_features)\n#new_cat_features\n\n#pd.get_dummies(df_train, columns=['Title'], prefix=['Title'])","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"d31c45b1d5382ab61425f4e3d130d872186ab669"},"cell_type":"code","source":"df_train = df_train.drop(['Name'],axis = 1)\ndf_test = df_test.drop(['Name'],axis = 1)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4b9736c6c88555be8dfd8ac60f7bbd3e24956dd8"},"cell_type":"markdown","source":"# Age\nWe will find the average age for every category in the age column and then impute the mean value for the respective category"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"39a3c90cbe8db88c906dd36d76465fe33aac9f09"},"cell_type":"code","source":"\ndf_train[['Title','Age']].groupby(['Title'],as_index = False).mean().sort_values(by='Age')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"d778b6121df6f15ea479b40dd70e7cf125d8ce56"},"cell_type":"code","source":"Mean_Age = df_train[['Title','Age']].groupby(['Title'],as_index = False).mean().sort_values(by='Age')\nsns.barplot(x='Title', y='Age', data=Mean_Age)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"09d900740322a6f2520d7973158fc23c2a89d951"},"cell_type":"code","source":"df_train['Age'] = df_train['Age'].fillna(-1)\ndf_test['Age'] = df_test['Age'].fillna(-1)  \nfull_data = [df_train,df_test]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c02efef1642757f59c225044c586ad7d36db4c9d"},"cell_type":"markdown","source":"# Age\nNull Values - 20% - Imputing the mean value per category as calculated above"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"6539afa3838faff334cebc98ec1763746b82836d"},"cell_type":"code","source":"\nfor dataset in full_data:\n    \n    dataset.loc[(dataset['Age'] == -1) &(dataset['Title'] == 'Master'), 'Age'] = 4.57\n    dataset.loc[(dataset['Age'] == -1) &(dataset['Title'] == 'Miss'), 'Age'] = 21.84\n    dataset.loc[(dataset['Age'] == -1) &(dataset['Title'] == 'Mr'), 'Age'] = 32.36\n    dataset.loc[(dataset['Age'] == -1) &(dataset['Title'] == 'Mrs'), 'Age'] = 35.78\n    dataset.loc[(dataset['Age'] == -1) &(dataset['Title'] == 'Rare'), 'Age'] = 45.54\n    dataset['Age'] = dataset['Age'].astype(int)   \n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7433b43a6d0a67be0c531e5d71ae7ab082da5637"},"cell_type":"markdown","source":"Now creating different age bands..."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"446ebadf3c71de92acde36466305b28051d40dce"},"cell_type":"code","source":"full_data = [df_train, df_test]\nfor dataset in full_data:\n    \n    dataset.loc[ dataset['Age'] <= 11, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 11) & (dataset['Age'] <= 18), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 18) & (dataset['Age'] <= 22), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 22) & (dataset['Age'] <= 27), 'Age'] = 3\n    dataset.loc[(dataset['Age'] > 27) & (dataset['Age'] <= 33), 'Age'] = 4\n    dataset.loc[(dataset['Age'] > 33) & (dataset['Age'] <= 40), 'Age'] = 5\n    dataset.loc[(dataset['Age'] > 40) & (dataset['Age'] <= 66), 'Age'] = 6\n    dataset.loc[ dataset['Age'] > 66, 'Age'] = 7","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"3a1a29dd334266bee87d14b4c21c9051bd6b0e9a"},"cell_type":"code","source":"df_train[['Sex','Age','Survived']].groupby(['Sex','Age'],as_index=False).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"74561639da745b6c639b1a538c0f1503a967c233"},"cell_type":"code","source":"agesexsurv = df_train[['Sex','Age','Survived']].groupby(['Sex','Age'],as_index=False).mean()\nsns.factorplot('Age','Survived','Sex', data=agesexsurv\n                ,aspect=3,kind='bar')\nplt.suptitle('AgeBand,Sex vs Survived')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"292df4f0232ff0739b23ef99ea0aca5b03a9566e"},"cell_type":"markdown","source":"# I want to create different categories for family members\nas calculated above\nFamily_members vs Survived\n\n\ncategory 0 = person is alone - survival chance = 30%,\ncategory 1 = person has family members = 1,2 - survival chance = 56%,\ncategory 2 = person has family members = 3 - survival chance = 72%\ncategory 3 = person has family members = 4,5 - survival chance = approx 17%,\ncategory 4 = person has family members = 6 - survival chance = 33%\ncategory 5 = person has family members = 7,10 - survival chance = 0%\n"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"f58cfa93893e1b592aecc5234950f2b1cf2455c1"},"cell_type":"code","source":"full_data = [df_train, df_test]\nfor dataset in full_data:\n    \n    dataset.loc[ dataset['Family_members'] == 0, 'Family_members_Band'] = 0\n    dataset.loc[(dataset['Family_members'] == 1)|(dataset['Family_members'] == 2),'Family_members_Band'] = 1\n    dataset.loc[ dataset['Family_members'] == 3, 'Family_members_Band'] = 2\n    dataset.loc[(dataset['Family_members'] == 4)|(dataset['Family_members'] == 5),'Family_members_Band'] = 3\n    dataset.loc[ dataset['Family_members'] == 6, 'Family_members_Band'] = 4\n    dataset.loc[(dataset['Family_members'] == 7)|(dataset['Family_members'] == 10),'Family_members_Band'] = 5\n    dataset['Family_members_Band'] = dataset['Family_members_Band'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9ba4c52a7f0aa2a6867b4ccce5b7ff7473029880"},"cell_type":"markdown","source":"# Creating Categories for Fare column"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"28e7dc15fe461e222c0c73c0adb116825acd6033"},"cell_type":"code","source":"df_train['FareBand'] = pd.qcut(df_train['Fare'], 4)\ndf_train[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"d8af1d89b0e2e76bf9ec9350133abddcdeb33517"},"cell_type":"code","source":"FarePlot = df_train[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand')\nsns.barplot(x='FareBand', y='Survived', data=FarePlot)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"d3ed5896741099f161b01e422a4eaf47517092a4"},"cell_type":"code","source":"df_test['Fare'] = df_test['Fare'].fillna(df_test['Fare'].dropna().mean()) # df_test has one null value","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"cbc86cd4859184ffe738ab035d0ab1efef7440e0"},"cell_type":"code","source":"full_data = [df_train,df_test]\nfor dataset in full_data:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare_Band'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare_Band'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare_Band'] = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare_Band'] = 3\n    dataset['Fare_Band'] = dataset['Fare_Band'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"f254dbb6d9ea8c4e1143a9d9d4d5d6db4f7f7f9b"},"cell_type":"code","source":"sns.factorplot('Fare_Band','Survived','Sex', data=df_train\n                ,aspect=3,kind='bar')\nplt.suptitle('FareBand,Sex vs Survived')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e8184d60c10cf5d68605581a7ef927f8c4dbdf4e"},"cell_type":"markdown","source":"From the above graph, thing to notice is that: the males survival rate increases as the fare for the ticket increases but for the females, the survival rate is almost similar for all the fare bands"},{"metadata":{"_uuid":"458fb7caaaa7e165eaf769d3dcbb311d2f3252d5"},"cell_type":"markdown","source":"# Embarked Column"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"3ed48cbf094de6fd4073fee236a221808a601ab1"},"cell_type":"code","source":"most_frequent = df_train['Embarked'].mode()[0]\nmost_frequent","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"d85f8719a9b8cc19a0dba3d56e5dae6c76c85215"},"cell_type":"code","source":"full_data = [df_train,df_test]\nfor dataset in full_data:\n    dataset['Embarked'] = dataset['Embarked'].fillna(most_frequent)\n    \ndf_train[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"223e8d78703df8c46636f0a0c94a7e362b623ce9"},"cell_type":"code","source":"embarkedgraph = df_train[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)\nsns.barplot(x='Embarked',y='Survived',data=embarkedgraph)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f3ac9de33bdaa4442b40d5fc4d52fc0a10cc3389"},"cell_type":"markdown","source":"Dropping Values"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"25b0dfac6de61ed99cd672b78945158d89568985"},"cell_type":"code","source":"df_train = df_train.drop(['SibSp','Parch','Fare','Family_members','FareBand'],axis = 1)\n","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"e513ebd36f53001c17cbecc4a3fa26add3daa8cf"},"cell_type":"code","source":"df_test = df_test.drop(['SibSp','Parch','Fare','Family_members'],axis = 1)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a0dd38e66dc85ef2d08aabcc9d8f5b5c3a1b6c55"},"cell_type":"markdown","source":"# One Hot Encoding"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"75ce85d984c5c86fc32ea8cd175e28425df1e65d"},"cell_type":"code","source":"X_train = pd.get_dummies(df_train, columns=['Pclass','Sex','Age','Embarked','Title','Family_members_Band','Fare_Band'], prefix=['Pclass','Sex'\n                                                                ,'Age','Embarked','Title','Family_members_Band','Fare_Band'])","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"ea6721b043e775dd6f998a69be9603ee7d6c32a4"},"cell_type":"code","source":"Y_train = X_train['Survived']\nX_train = X_train.drop('Survived', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"1e3eef8b6503aa901c4ccd23884e451633195540"},"cell_type":"code","source":"X_train.shape","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"39347b60411226bf5c0924a1daaa4defdf446dd7"},"cell_type":"code","source":"X_test = pd.get_dummies(df_test, columns=['Pclass','Sex','Age','Embarked','Title','Family_members_Band','Fare_Band'], prefix=['Pclass','Sex','Age','Embarked','Title','Family_members_Band','Fare_Band'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"f604de5c9735d9d18389c7b9294d9ea43a6aee1a"},"cell_type":"code","source":"X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1616b4b0eff8a4fb74ec4466b9bcf91bd6e457a2"},"cell_type":"code","source":"X_test=X_test.drop(['PassengerId'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0f7a87486707d5b1a5c113658fff4484626ced35"},"cell_type":"markdown","source":"# Testing Machine Learning Models"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"de226398965202ac823088db04c72d929eb94023"},"cell_type":"code","source":"# stochastic gradient descent (SGD) learning\nsgd = linear_model.SGDClassifier(max_iter=5, tol=None)\nsgd.fit(X_train, Y_train)\nY_pred = sgd.predict(X_test)\n\nsgd.score(X_train, Y_train)\n\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\n\n\nprint(round(acc_sgd,2,), \"%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"b962d4fdc7298d53ab564d99fe7db96f86a8015f"},"cell_type":"code","source":"# Random Forest\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\n\nY_prediction = random_forest.predict(X_test)\n\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nprint(round(acc_random_forest,2,), \"%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"3d8c2a2e87a27d76a27bb3c7b74a50fdef4c25c8"},"cell_type":"code","source":"# Logistic Regression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\n\nY_pred = logreg.predict(X_test)\n\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nprint(round(acc_log,2,), \"%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"ce76229f7b9e79fb89f5b7ce56a04b9c63084a9c"},"cell_type":"code","source":"# KNN\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, Y_train)\n\nY_pred = knn.predict(X_test)\n\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\nprint(round(acc_knn,2,), \"%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"5c012538c00f6f92d6224dd090799024f062410a"},"cell_type":"code","source":"# Gaussian Naive Bayes\ngaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\n\nY_pred = gaussian.predict(X_test)\n\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\nprint(round(acc_gaussian,2,), \"%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"33960a98a37b8638a7c2cf71b9589b332cc3d7eb"},"cell_type":"code","source":"# Perceptron\nperceptron = Perceptron(max_iter=5)\nperceptron.fit(X_train, Y_train)\n\nY_pred = perceptron.predict(X_test)\n\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\nprint(round(acc_perceptron,2,), \"%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"2466d6647728feaf10b45284e54b4888ce554924"},"cell_type":"code","source":"# Linear SVC\nlinear_svc = LinearSVC()\nlinear_svc.fit(X_train, Y_train)\n\nY_pred = linear_svc.predict(X_test)\n\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\nprint(round(acc_linear_svc,2,), \"%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"056963638c1290c46a62a5a9e221a0a05e819bf1"},"cell_type":"code","source":"# Decision Tree\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\n\nY_pred = decision_tree.predict(X_test)\n\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nprint(round(acc_decision_tree,2,), \"%\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"0315ac20b59031834ea56ae4140c99da31d24f38"},"cell_type":"code","source":"results = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', \n              'Decision Tree'],\n    'Score': [acc_linear_svc, acc_knn, acc_log, \n              acc_random_forest, acc_gaussian, acc_perceptron, \n              acc_sgd, acc_decision_tree]})\nresult_df = results.sort_values(by='Score', ascending=False)\n\nresult_df.head(9)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f54d01394457b11e68b7e96395c319850b7213f"},"cell_type":"markdown","source":"# Best Model"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"c8c25dae27cc68e7df4919d0f741506d72ac2f0e"},"cell_type":"code","source":"bestmodelgraph = result_df.head(9)\nax = sns.factorplot(\"Model\", y=\"Score\", data=bestmodelgraph,\n                palette='Blues_d',aspect=3.5,kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"574d4170e0cd179c5a17dece48cc65415f220482"},"cell_type":"markdown","source":"# K-FOLD Validation"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"96f10d72754760496c711ecdbe246b642ee86f77"},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nrf = RandomForestClassifier(n_estimators=100)\nscores = cross_val_score(rf, X_train, Y_train, cv=10, scoring = \"accuracy\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"997a0ff61812d5d2251e153be43d6793b8856be8"},"cell_type":"code","source":"print(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"ea5116581b844a3ef1db9737686e429086b9f84e"},"cell_type":"code","source":"# Plot learning curve\ndef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"33f0fbf92a39ea83d7ce02bb9e79b185ff7b0c44"},"cell_type":"code","source":"# Plot learning curves\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import validation_curve\n\ntitle = \"Learning Curves (Random Forest)\"\ncv = 10\nplot_learning_curve(rf, title, X_train, Y_train, ylim=(0.7, 1.01), cv=cv, n_jobs=1);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a2df77e4b1d78cfe8d3e126383d12244d942ce20"},"cell_type":"markdown","source":"# Feature Importance"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"bc284c25f9a85fcab673685bacc9c0b59ab69059"},"cell_type":"code","source":"importances = pd.DataFrame({'feature':X_train.columns,'importance':np.round(random_forest.feature_importances_,3)})\nimportances = importances.sort_values('importance',ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"ad7d49699ddb476e8b2dee540033c4560b833f28"},"cell_type":"code","source":"importances_most = importances.head(10) # 10 most important features\naxes = sns.factorplot('feature','importance', \n                      data=importances_most, aspect = 4, )","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"deee7a55421d5f509aedf7cfbbcf34de59cd4f6e"},"cell_type":"code","source":"importances_least = importances.tail(10) # least 10 important features\naxes = sns.factorplot('feature','importance', \n                      data=importances_least, aspect = 4,)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"43527ed8f087f4f10e7b021dd9a592cf4aa63985"},"cell_type":"code","source":"# Random Forest , Testing with oob score\n\nrandom_forest = RandomForestClassifier(n_estimators=100, oob_score = True)\nrandom_forest.fit(X_train, Y_train)\nY_prediction = random_forest.predict(X_test)\n\nrandom_forest.score(X_train, Y_train)\n\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nprint(round(acc_random_forest,2,), \"%\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"59a85916e624f1d94250c3428cb9cff614c8cc48"},"cell_type":"markdown","source":"# OOB Score"},{"metadata":{"_uuid":"de486be56d0c9ebbeb01929c6c6093f49d70b68f"},"cell_type":"markdown","source":"Our random forest model predicts as good as it did before. A general rule is that, the more features you have, the more likely your model will suffer from overfitting and vice versa. But I think our data looks fine for now and hasn't too much features.\n\nThere is also another way to evaluate a random-forest classifier, which is probably much more accurate than the score we used before. What I am talking about is the out-of-bag samples to estimate the generalization accuracy. I will not go into details here about how it works. Just note that out-of-bag estimate is as accurate as using a test set of the same size as the training set. Therefore, using the out-of-bag error estimate removes the need for a set aside test set."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"f0a8cb372738d3aec81fdf987527fe85666dd1ab"},"cell_type":"code","source":"print(\"oob score:\", round(random_forest.oob_score_, 4)*100, \"%\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f2bc876712d4fc819f4ebbd12987d0326ef7cfe4"},"cell_type":"markdown","source":"# Hyperparameter Tuning\nBelow you can see the code of the hyperparamter tuning for the parameters criterion, min_samples_leaf, min_samples_split and n_estimators.\n\nI put this code into a markdown cell and not into a code cell, because it takes a long time to run it. Directly underneeth it, I put a screenshot of the gridsearch's output.\n\nparam_grid = { \"criterion\" : [\"gini\", \"entropy\"], \"min_samples_leaf\" : [1, 5, 10, 25, 50, 70], \"min_samples_split\" : [2, 4, 10, 12, 16, 18, 25, 35], \"n_estimators\": [100, 400, 700, 1000, 1500]}\n\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\n\nrf = RandomForestClassifier(n_estimators=100, max_features='auto', oob_score=True, random_state=1, n_jobs=-1)\n\nclf = GridSearchCV(estimator=rf, param_grid=param_grid, n_jobs=-1)\n\nclf.fit(X_train, Y_train)\n\nclf.bestparams"},{"metadata":{"_uuid":"1d7904a48e08efef68f2de24b0f3baf32a9c5f09"},"cell_type":"markdown","source":"# Testing new parameters from hypertuning"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"23388929f54a87ac5e8c346b8950ebc90bdfc558"},"cell_type":"code","source":"# Random Forest\nrandom_forest = RandomForestClassifier(criterion = \"gini\", \n                                       min_samples_leaf = 1, \n                                       min_samples_split = 10,   \n                                       n_estimators=100, \n                                       max_features='auto', \n                                       oob_score=True, \n                                       random_state=1, \n                                       n_jobs=-1)\n\nrandom_forest.fit(X_train, Y_train)\nY_prediction = random_forest.predict(X_test)\n\nrandom_forest.score(X_train, Y_train)\n\nprint(\"oob score:\", round(random_forest.oob_score_, 4)*100, \"%\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"067ba3c1762fb306e29758da840161ff0fce945b"},"cell_type":"markdown","source":"Now that we have a proper model, we can start evaluating it's performace in a more accurate way. Previously we only used accuracy and the oob score, which is just another form of accuracy. The problem is just, that it's more complicated to evaluate a classification model than a regression model. We will talk about this in the following section."},{"metadata":{"_uuid":"f4bb66109261ad3b7374bf93c94030687d2cfc6b"},"cell_type":"markdown","source":"# Confusion Matrix"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"e932943050c2d30d7b14aefb64b6c6fe14f94ea2"},"cell_type":"code","source":"from sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\npredictions = cross_val_predict(random_forest, X_train, Y_train, cv=3)\nconfusion_matrix(Y_train, predictions)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e78cdfd070ab1f828a65af21fffd0e59308cdc0e"},"cell_type":"markdown","source":"The first row is about the not-survived-predictions: 489 passengers were correctly classified as not survived (called true negatives) and 60 were wrongly classified as not survived (false positives).\n\nThe second row is about the survived-predictions: 100 passengers where wrongly classified as survived (false negatives) and 242 were correctly classified as survived (true positives)."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"41ffb2980b1b158b86d0127853c5130f860d3022"},"cell_type":"code","source":"conf_mat = confusion_matrix(Y_train, predictions)\nTP = conf_mat[0][0]\nFP = conf_mat[0][1]\nFN = conf_mat[1][0]\nTN = conf_mat[1][1]\n\n# Sensitivity, hit rate, recall, or true positive rate\nTPR = TP/(TP+FN)\nprint('Sensitivity, hit rate, recall, or true positive rate=',TPR)\n\n# Specificity or true negative rate\nTNR = TN/(TN+FP) \nprint('Specificity or true negative rate=',TNR)\n\n# Precision or positive predictive value\nPPV = TP/(TP+FP)\nprint('Precision or positive predictive value=',PPV)\n\n# Negative predictive value\nNPV = TN/(TN+FN)\nprint('Negative predictive value=',NPV)\n\n# Fall out or false positive rate\nFPR = FP/(FP+TN)\nprint('Fall out or false positive rate=',FPR)\n\n# False negative rate\nFNR = FN/(TP+FN)\nprint('False negative rate=',FNR)\n\n# False discovery rate\nFDR = FP/(TP+FP)\nprint('False discovery rate=',FDR)\n\n# Overall accuracy\nACC = (TP+TN)/(TP+FP+FN+TN)\nprint('Overall accuracy=',ACC)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"ef8a15e2a9dc957ba86d751b280334d96286e843"},"cell_type":"code","source":"positives = pd.DataFrame({\n    'Factor': ['True Positives', 'False Positives', ],\n    'Score': [TP, FP]})\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"f1fc8fc4a160e15dfd5a81a4808d33e5cdac8846"},"cell_type":"code","source":"sns.barplot(x='Factor',y='Score',data=positives)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"195be3c4849d7cb07ac44466a9c8e4fdd30baa10"},"cell_type":"code","source":"negatives = pd.DataFrame({\n    'Factor':['True Negative', 'False Negative'],\n    'Score':[TN, FN]\n})\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"e9af5f1a06faf9f3dc8a619d290496fec0ba2a4e"},"cell_type":"code","source":"sns.barplot(x='Factor',y='Score',data=negatives)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"7bcda4c15ad685dbc2bdcdff52d12f171f123ade"},"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score\n\nprint(\"Precision:\", precision_score(Y_train, predictions))\nprint(\"Recall:\",recall_score(Y_train, predictions))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1be2cc54a069910791775d46f26f3222a5f21033"},"cell_type":"markdown","source":"Our model predicts 80% of the time, a passengers survival correctly (precision). \nThe recall tells us that it predicted the survival of 71 % of the people who actually survived."},{"metadata":{"_uuid":"d3737b595fedd735792c7c8deddfa7ce214ea450"},"cell_type":"markdown","source":"# F-Score"},{"metadata":{"_uuid":"ae11115f5a5457e484252ad769e834f9913e7165"},"cell_type":"markdown","source":"You can combine precision and recall into one score, which is called the F-score. The F-score is computed with the harmonic mean of precision and recall. Note that it assigns much more weight to low values. \nAs a result of that, the classifier will only get a high F-score, if both recall and precision are high."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"fbc970d31bb86be89e3766c1a0f59bbec22bc538"},"cell_type":"code","source":"from sklearn.metrics import f1_score\nprint('F1score',f1_score(Y_train, predictions))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"49c23dd8909bd9fed1baf0f084635d4560a2d828"},"cell_type":"markdown","source":"There we have it, a 75 % F-score. The score is not that high, because we have a recall of 70%.\n\nBut unfortunately the F-score is not perfect, because it favors classifiers that have a similar precision and recall. \nThis is a problem, because you sometimes want a high precision and sometimes a high recall. \nThe thing is that an increasing precision, sometimes results in an decreasing recall and vice versa (depending on the threshold). \nThis is called the precision/recall tradeoff. We will discuss this in the following section."},{"metadata":{"_uuid":"fd35da8c1aac06c0bc1027f61a48663a171efad9"},"cell_type":"markdown","source":"# Precision Recall Curve"},{"metadata":{"_uuid":"eb845671180e876ddb777fb36ff6fde7dbe6b573"},"cell_type":"markdown","source":"For each person the Random Forest algorithm has to classify, it computes a probability based on a function \nand it classifies the person as survived (when the score is bigger the than threshold) or \nas not survived (when the score is smaller than the threshold). \nThat's why the threshold plays an important part."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"f92b0aa9ad3febde32ff1064c3a34a4d760c752c"},"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\n\n# getting the probabilities of our predictions\ny_scores = random_forest.predict_proba(X_train)\ny_scores = y_scores[:,1]\n\nprecision, recall, threshold = precision_recall_curve(Y_train, y_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"42e0fa1e5d3d82b23ed35536dd294ee2a51ef135"},"cell_type":"code","source":"def plot_precision_and_recall(precision, recall, threshold):\n    plt.plot(threshold, precision[:-1], \"r-\", label=\"precision\", linewidth=5)\n    plt.plot(threshold, recall[:-1], \"b\", label=\"recall\", linewidth=5)\n    plt.xlabel(\"threshold\", fontsize=19)\n    plt.legend(loc=\"upper right\", fontsize=19)\n    plt.ylim([0, 1])\n\nplt.figure(figsize=(14, 7))\nplot_precision_and_recall(precision, recall, threshold)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"57e8701f408e00256387a96ee8ee29e26dcb9619"},"cell_type":"markdown","source":"Above you can clearly see that the recall is falling of rapidly at a precision of around 84%. \nBecause of that you may want to select the precision/recall tradeoff before that - maybe at around 75 %.\n\nYou are now able to choose a threshold, that gives you the best precision/recall tradeoff \nfor your current machine learning problem. If you want for example a precision of 80%, \nyou can easily look at the plots and see that you would need a threshold of around 0.4. \nThen you could train a model with exactly that threshold and would get the desired accuracy.\n\nAnother way is to plot the precision and recall against each other"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"c3527b43453d249d877c9d6c726a58f3a5c24b49"},"cell_type":"code","source":"def plot_precision_vs_recall(precision, recall):\n    plt.plot(recall, precision, \"g--\", linewidth=2.5)\n    plt.ylabel(\"recall\", fontsize=19)\n    plt.xlabel(\"precision\", fontsize=19)\n    plt.axis([0, 1.5, 0, 1.5])\n\nplt.figure(figsize=(14, 7))\nplot_precision_vs_recall(precision, recall)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f66b6426b2f54974ac829d541bc4bf48f541befa"},"cell_type":"markdown","source":"# ROC AUC Curve"},{"metadata":{"_uuid":"cb31c77fad864b8c31d2dde623a5ef0695559903"},"cell_type":"markdown","source":"Another way to evaluate and compare your binary classifier is provided by the ROC AUC Curve. \nThis curve plots the true positive rate (also called recall) against the false positive rate (ratio of incorrectly classified negative instances), instead of plotting the precision versus the recall."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"c912ef2a9b79382feaa2a2ce7a384faa63d61183"},"cell_type":"code","source":"from sklearn.metrics import roc_curve\n# compute true positive rate and false positive rate\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(Y_train, y_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"6d615717682a0aa13e748d52cf30aaaa834c23ff"},"cell_type":"code","source":"# plotting them against each other\ndef plot_roc_curve(false_positive_rate, true_positive_rate, label=None):\n    plt.plot(false_positive_rate, true_positive_rate, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], 'r', linewidth=4)\n    plt.axis([0, 1, 0, 1])\n    plt.xlabel('False Positive Rate (FPR)', fontsize=16)\n    plt.ylabel('True Positive Rate (TPR)', fontsize=16)\n\nplt.figure(figsize=(14, 7))\nplot_roc_curve(false_positive_rate, true_positive_rate)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"962a8aa5f053e384191fae6f9c5b0d7886c53416"},"cell_type":"markdown","source":"The red line in the middel represents a purely random classifier (e.g a coin flip) and therefore your classifier should be as far away from it as possible. Our Random Forest model seems to do a good job.\n\nOf course we also have a tradeoff here, because the classifier produces more false positives, the higher the true positive rate is."},{"metadata":{"_uuid":"f6ddefe2731d77698c33561ac4488579a55a691a"},"cell_type":"markdown","source":"# ROC AUC Score"},{"metadata":{"_uuid":"3b111f161be90a3f86457a13f92c3f6676f3513f"},"cell_type":"markdown","source":"The ROC AUC Score is the corresponding score to the ROC AUC Curve. It is simply computed by measuring the area under the curve, which is called AUC.\n\nA classifiers that is 100% correct, would have a ROC AUC Score of 1 and a completely random classiffier would have a score of 0.5."},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"a6f32ed2fb22e2c366eb94b6a933cd6107196672"},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nr_a_score = roc_auc_score(Y_train, y_scores)\nprint(\"ROC-AUC-Score:\", r_a_score)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1f63d491793f6cdb67884b6206d3e190d1ae75a1"},"cell_type":"markdown","source":"**Submission**"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c436fcab662eea89d8b063d69679dd46cf337688"},"cell_type":"code","source":"submission = pd.DataFrame({\n        \"PassengerId\": df_test[\"PassengerId\"],\n        \"Survived\": Y_prediction\n    })\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ef441415d304206bc353e4cd5aec61f870f673df"},"cell_type":"markdown","source":"**More to come...\nPlease provide suggestions if there is need of improvement.\nPlease upvote if the kernel was useful**"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}