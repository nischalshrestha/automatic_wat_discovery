{"cells":[{"metadata":{"_uuid":"d6e0fca104b22843258feff4fca5bfd1e4c7013a"},"cell_type":"markdown","source":"# Titanic Survival - Classification Voting"},{"metadata":{"_uuid":"802a2383993428fccf825df69a35bef0700314dc"},"cell_type":"markdown","source":"# 1. Introduction"},{"metadata":{"_uuid":"9b9e1904ab46b6278672449dc03d4476d4142822"},"cell_type":"markdown","source":"### Notebook Description"},{"metadata":{"_uuid":"0776ad71eb08a59c0be387f600b9eb55fa96db88"},"cell_type":"markdown","source":"I created this notebook for beginners (like myself) who are interested and learning how to implement a ML algorithm from scratch, importing, exploratory data analysis, feature engineering, training algorithms, evaluating algorithms, applying algorithms. \n\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\nIn this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy."},{"metadata":{"_uuid":"84492e71749d30ffcf345680576ddd05fad94460"},"cell_type":"markdown","source":"### Import Packages"},{"metadata":{"trusted":true,"_uuid":"da147c96219929858d6d0771cfc7120da3888ffe"},"cell_type":"code","source":"#Load Packages\nimport numpy as np # linear algebra\nimport matplotlib as mpl\nimport pandas as pd\nimport pandas_ml as pdml\nfrom pandas_ml import ConfusionMatrix\nimport seaborn as sns\nimport re \nfrom IPython.display import display_html\nimport itertools\nimport math\nimport random\nfrom tqdm import tqdm_notebook as tqdm\nfrom sklearn.metrics import *\nimport matplotlib.pyplot as plt\nimport matplotlib\nfrom scipy import stats\nfrom statistics import variance, stdev, mode\nfrom scipy import interp\n#Load sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.metrics import precision_recall_curve, confusion_matrix, accuracy_score, hamming_loss\nfrom sklearn import linear_model\nfrom collections import Counter\nfrom sklearn.model_selection import StratifiedKFold\n\n# Import Classifiers\nimport scikitplot as skplt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Perceptron, SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import  AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.model_selection import StratifiedKFold, GridSearchCV\nfrom sklearn.metrics import classification_report, matthews_corrcoef, log_loss, hinge_loss, cohen_kappa_score\nfrom sklearn.metrics import roc_auc_score, roc_curve, brier_score_loss\n#Learning curve\nfrom sklearn.metrics import f1_score, precision_recall_fscore_support\nfrom sklearn.model_selection import StratifiedKFold, GridSearchCV, learning_curve, ShuffleSplit\nfrom sklearn.model_selection import cross_val_predict,train_test_split, cross_val_score, validation_curve\nfrom sklearn.preprocessing import StandardScaler,  LabelEncoder\nfrom IPython.display import display_html\nimport warnings\n\n\n# for inline plots\n%matplotlib inline\nwarnings.filterwarnings('ignore')\n\nmpl.rcParams['figure.figsize'] = (8, 6)\nplt.rcParams[\"legend.fontsize\"] = 15\nplt.rcParams[\"axes.labelsize\"] = 15\nmpl.rc('xtick', labelsize = 15) \nmpl.rc('ytick', labelsize = 15)\nsns.set(style = 'whitegrid', palette = 'muted', font_scale = 2)\n    \nprint('Libraries Imported')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"53dc79a37d3de273cd53f92e1bf193ddcebc0c3f"},"cell_type":"markdown","source":"### Helper Functions"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"b1fbaaa963ffa0b7ec3c96840149639506709439"},"cell_type":"code","source":"# provide some statistics for numerics\ndef Stats(feature):\n    mean  = np.nanmean(X_train[feature])\n    median = np.nanmedian(X_train[feature])\n    mode_ = stats.mode(X_train[feature])\n    \n    variation = np.nanvar(X_train[feature])\n    stdv = np.nanstd(X_train[feature])\n    range_ = np.max(X_train[feature]) - np.min(X_train[feature])\n    Quantile = (X_train[feature])\n    \n    print('Stats of %s:'%(feature.upper()))\n    print('Mean: %.2f'%(mean))\n    print('Median: %.2f'%(median))\n    print('Mode: %.2f'%(mode_[0]))\n    print('Range: %.2f'%(range_))    \n    print('Variance: %.2f'%(variation))\n    print('Standard Deviation: %.2f'%(stdv))\n    print('Quantile:')\n    for val in [10, 25, 50, 75, 90, 100]:\n        perc = np.nanpercentile(X_train[feature],val)\n        print('\\t%s %%: %.2f'%(val, perc))  \n        \n        \n#sets up the parametes for plotting.. size and font\ndef PlotParams(Font, sizex, sizey):\n    mpl.rcParams['figure.figsize'] = (sizex,sizey)\n    plt.rcParams[\"legend.fontsize\"] = Font\n    plt.rcParams[\"axes.labelsize\"] = Font\n    mpl.rc('xtick', labelsize = Font) \n    mpl.rc('ytick', labelsize = Font)\n\n#sets up Seaborn parametes for plotting\ndef snsParams(font, colour_scheme):\n    #eaborn.set(context='notebook', style='darkgrid', palette='deep', font='sans-serif', font_scale=1, color_codes=True, rc=None)\n    sns.set(style = 'whitegrid', palette = colour_scheme, font_scale = font)\n\n#determined ht emissing data\ndef Missing (X):\n    total = X.isnull().sum().sort_values(ascending = False)\n    percent = round(X.isnull().sum().sort_values(ascending = False)/len(X)*100, 2)\n    missing = pd.concat([total, percent], axis = 1,keys= ['Total', 'Percent'])\n    return(missing) \n\n#plots number of dataframes side by side\ndef SideSide(*args):\n    html_str=''\n    for df in args:\n        html_str+=df.to_html()\n    display_html(html_str.replace('table','table style=\"display:inline\"'),raw = True)\n\n#makes heat map of correllations\ndef PlotCorr(X):\n    corr = X.corr()\n    #fig , ax = plt.figure( figsize = (6,6 ) )\n    cmap = sns.diverging_palette( 220 , 10 , as_cmap = True )\n    sns.heatmap(\n        corr, cmap = cmap, square = True, cbar = False, cbar_kws = { 'shrink' : 1 }, \n     annot = True, annot_kws = { 'fontsize' : 14 }\n    )\n    plt.yticks(rotation = 0)\n    plt.xticks(rotation = 90) \n    \n#plot top correlatins in a heat map\ndef TopCorr(X, lim):\n    corr = X.corr()\n    cmap = sns.diverging_palette( 220 , 10 , as_cmap = True )\n    #fig , ax = plt.subplots( figsize = (6,6 ) )\n    sns.heatmap(corr[(corr >= lim) | (corr <= -lim)], \n         vmax = 1.0,  cmap = cmap, vmin = -1.0, square = True, cbar = False, linewidths = 0.2, annot = True, \n                annot_kws = {\"size\": 14})\n    plt.yticks(rotation = 0)\n    plt.xticks(rotation = 90)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9ce6a4f48243130b630b2f18ffc449a3163bb63c"},"cell_type":"markdown","source":"### Import Data"},{"metadata":{"trusted":false,"_uuid":"c0af86979ec6cc78cb38a614bf1f3b6ab0defec7"},"cell_type":"code","source":"# get data from csv files\ntest  = pd.read_csv('../input/test.csv')\ntrain = pd.read_csv('../input/train.csv')\n\n#determine sizes of datasets\nn_train, m_train = train.shape\nn_test, m_test = test.shape\n\n# divide into X and y data\nX_train = pd.DataFrame(train.iloc[:,1: m_train])\ny_train = pd.DataFrame(train.iloc[0:, 1])\nX_test_original = test\nX_test = test\n\nprint('Data Imported\\n\\n')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c1bc4a780f9a95612e96a6d8982f415c10b7b93c"},"cell_type":"markdown","source":"## Data Description"},{"metadata":{"trusted":false,"_uuid":"66bcb89e4c756d7dc073d7cf095f4b8d3bcd72c4"},"cell_type":"code","source":"print('FULL DATA')\nprint('Number of features (m): %.0f'%(m_train))\nprint('Number of traing samples (n): %.0f'%(n_train))\n\nprint('\\n\\nTest DATA')\nprint('Number of features (m): %.0f'%(m_test))\nprint('Number of traing samples (n): %.0f'%(n_test))\n\ncnt = 0\n# print out the features\nprint('\\n\\nFeatures: ')\nfor feature in X_train.columns:\n    cnt += 1\n    print('%d. '%(cnt), feature,'\\t\\t')\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b63e13439f1a1aabab13ab2f9f7d55070e02b35f"},"cell_type":"markdown","source":"### Feature Description"},{"metadata":{"_uuid":"628b9d0097c2d8008fb9e343aa1f9ba24b3d2319"},"cell_type":"markdown","source":"VARIABLE DESCRIPTIONS:\n\n    Survived: Survived (1) or died (0)\n    Pclass: Passenger's class\n    Name: Passenger's name\n    Sex: Passenger's sex\n    Age: Passenger's age\n    SibSp: Number of siblings/spouses aboard\n    Parch: Number of parents/children aboard\n    Ticket: Ticket number\n    Fare: Fare\n    Cabin: Cabin\n    Embarked: Port of embarkation\n"},{"metadata":{"trusted":false,"_uuid":"8c193226178243487c7c393c1de6cbb4bdc40e78"},"cell_type":"code","source":"# take a sample of what the data looks like\nX_train.head(20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"25ae3a3b2143a92caa5ea3c12e2c36a95f316190"},"cell_type":"markdown","source":"### Data Types"},{"metadata":{"_uuid":"94b53457b3b754ee4a561bd9ff45015d78aec546"},"cell_type":"markdown","source":"it is important to know what types of data you are dealing with early on. For classification, all featured will have to be in integer format. below you can see that the data is made up of floats (i.e. numbers), objects (i.e string). The floats will need to be converted into int64 values, some of the objects (e.g. sex) will ned to be converted into numberics, all NaN and null values will need to be filled"},{"metadata":{"trusted":false,"_uuid":"844a58cc5705a16580ff1e4b196837438f912ee6"},"cell_type":"code","source":"# provide information about the types of data we are dealing with\nprint('ORIGINAL TRAINING DATA:\\n')\nX_train.info()\n\nprint('\\n\\n\\nORIGINGAL TEST DATA:\\n')\nX_test.info()\n\n#summarise the types of data\nprint('\\ndata types of features:')\n\ncnt = 0\nd_type = ['float64', 'int64','object','dtype']\nprint('\\n\\tTRAIN \\t\\t TEST')\nfor c1, c2 in zip(X_train.get_dtype_counts(), X_test.get_dtype_counts()):\n    cnt += 1\n    print(\"%s:\\t%-9s \\t%s\"%(d_type[cnt],c1, c2))\n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"133ba8ea3e4da86557c3de9d21a2af5273c047e2"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"bf71092ae1ca6851ef3f4bdfdd400710e3287ec2"},"cell_type":"markdown","source":"### Null Data"},{"metadata":{"_uuid":"309c51e8f1af7fb89202787dd654f0f6bbcf0a90"},"cell_type":"markdown","source":"Lets check for missing data. as can be seen below there are several features with missing data. A much closer look will be taken later when I will replace the missing data. below Cabine is missing most og its data (78%) an thus mostly likely will a feature which cannot be used fot modeling. the Age feature is also missing a considerable amount of data (>20%). This will be filled later by predicting the age basaed on other features (e.g. sex, class, etc). The embark feature has very littel data missing and so can be replaces easily with a mean value deermined by class."},{"metadata":{"trusted":false,"_uuid":"b4ab25f64626bad7af52d7313bfe61f1516fc02b"},"cell_type":"code","source":"#finds missing values\nmissing_train = Missing(X_train)\nmissing_test = Missing(X_test)\n    \nprint('TRAIN DATA','\\t\\t','TEST DATA')\nSideSide(missing_train, missing_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e6208fc39bc48457a90d40814480b7faad065d72"},"cell_type":"markdown","source":"### Basic Statistics"},{"metadata":{"trusted":false,"_uuid":"2e51374407a0922e7df01a8208b51f8bd8f39841"},"cell_type":"code","source":"X_train.describe(include = \"all\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e5b846949781d655a1f93d06c0e94d4c91c2900b"},"cell_type":"markdown","source":"Average Age is 29 years with a standard deviation of 14 years. The average Price of a ticket is and ticket price is 32 but has a huge standard deviation (49... over 100 %). As there are 681 unique tickets and there is no way to extract a less detailed information this feature can be dropped. There are 891 unique names but we could take a look on the title of each person to understand if the survival rate of people based on title & class"},{"metadata":{"_uuid":"b4a42e7171d174fc9f230c409d2d4c476654d768"},"cell_type":"markdown","source":"### Quick Visual Glance at Data"},{"metadata":{"trusted":false,"_uuid":"bd65d57d0ea6c0db6a2604146a3e2789a216900e"},"cell_type":"code","source":"X_train.hist(figsize = (16,10),bins = 20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"517300d4020875e2d37f9806ca7d52c999068cec"},"cell_type":"code","source":"sns.pairplot(X_train[[u'Survived', u'Pclass', u'Sex', u'Age', u'Parch', u'Fare', u'Embarked']], \n             hue = 'Survived', palette = 'muted',size = 2.2,\n             diag_kind = 'kde', dropna = True, diag_kws = dict(shade = True), plot_kws = dict(s=20) )\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e278a453cf88fef4fe396050eb81f106c9189f63"},"cell_type":"markdown","source":"### Skewness"},{"metadata":{"trusted":false,"_uuid":"497fcb9ac9c8074a79fe9bcd0652969ff9c23888"},"cell_type":"code","source":"X_train.skew()\n\nskew_train = pd.DataFrame(X_train.skew())\nskew_test = pd.DataFrame(X_test.skew())\n    \nprint('TRAIN DATA','\\t\\t','TEST DATA')\nSideSide(skew_train, skew_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"793ba2f84938a765bbce39a555904f33d9d9d181"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"dbe31c8a08941ab83746e26c00e3ffbda2e35a33"},"cell_type":"markdown","source":"### Feature Correlation\n"},{"metadata":{"_uuid":"4a810d09feab16961b60358831e492811aaee9a3"},"cell_type":"markdown","source":"it can be seen that tha Pclass (0.34) and the Fare (0.26) have the stongest correlation with the survival rate. These parameters themselves are also highly correlated with eachother. An early repiction may therefore be that the social position (class, money) may be a good indicator of survival. In the feature engineering section new features will be generated and we will also look at these correlations"},{"metadata":{"trusted":false,"_uuid":"43b806e888336f59c25a3cd1684da54aac323f9a"},"cell_type":"code","source":"#show the correlations between all the featured in a heatmap\nplt.figure(figsize = (20,6))\nPlotCorr(X_train);","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"4792bb33ff2ee93e9597e7f8f8993efcdfc28d48"},"cell_type":"code","source":"# highest correlated with correlation of features with 'Survived'\nprint('Featured hights correlation with survival')\nprint('Feature\\tCorrelation')\nSurvive_Corr = X_train.corr()[\"Survived\"]\nSurvive_Corr = Survive_Corr[1:9] # remove the 'Survived'\nSurvive_Corr= Survive_Corr[np.argsort(Survive_Corr, axis = 0)[::-1]] #sort in descending order\nprint(Survive_Corr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e1dd291802dd6844d729ade267d72dba77854d5b"},"cell_type":"code","source":"\n# plot survival count for male and female\nplt.figure(figsize = (10,6))\nax = sns.barplot(x = np.arange(len(Survive_Corr)), y = np.array(Survive_Corr.values), palette = 'muted', orient= 'v');\nax.set_xlabel(\"Feature\",fontsize = 15)\nax.set_ylabel(\"Correlation Coefficient (with Survival)\",fontsize = 15)\nax.set_xticklabels(Survive_Corr.index)\n","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"20e3dac361e8a59e7844250af6fb4c1bf8a5232b"},"cell_type":"markdown","source":"## Exploratory Data Analysis"},{"metadata":{"_uuid":"038a181b6492442cf24b13951e34df153f89d52e"},"cell_type":"markdown","source":"### Survival"},{"metadata":{"_uuid":"98436c4a78f6a8405acac12f506d272d0e195925"},"cell_type":"markdown","source":"the overall survival is shown below where 0 = died, 1 = survival. as can be seen more died than survived. however, this tells us nothing about what groups of people, age, and class of people these categories are made up of... so let's have a closer look"},{"metadata":{"trusted":false,"_uuid":"af718442be499c737286ebe7f71bfea2d8231cca"},"cell_type":"code","source":"f,ax = plt.subplots(1,2,figsize =(18,8))\nX_train['Survived'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('Survived')\nax[0].set_ylabel('')\nplt.tight_layout()\nsns.countplot('Survived',data=X_train,ax=ax[1],palette=\"muted\")\nax[1].set_title('Survived')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb140b14a0365ed46da0c102565788f4533b4725"},"cell_type":"markdown","source":"### Survival by sex"},{"metadata":{"trusted":false,"_uuid":"ecbca0a613d8fa21acab164545577ead2cbfde5c"},"cell_type":"code","source":"snsParams(2, 'muted')\n# plot survival count for male and female\nplt.figure(figsize = (20,5))\nplt.subplot(1, 3, 1)\nax = sns.countplot(x = 'Survived',hue = 'Sex', data = X_train);\nax.set_xlabel(\"Survived\",fontsize = 15)\nax.set_ylabel(\"Count\",fontsize = 15)\nax.legend(fontsize = 14)\n\n\n#survival probability of males and females\nplt.subplot(1, 3, 2)\nax = sns.barplot(x = \"Sex\", y = \"Survived\",data = X_train)\nax = ax.set_ylabel(\"Survival Probability\")\n\nplt.subplot(1, 3, 3)\nsns.violinplot(y = 'Survived', x = 'Sex', data = X_train, inner = 'quartile')\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d06c00a5adfd1b788d47e73914452d168f53f952"},"cell_type":"markdown","source":"This first bar plot above shows the distribution of female and male survived and died. the opposite happened for men, more dies than survived. This count plot shows the actual distribution of male and female passengers that survived and did not survive. It shows that among all the females ~ 230 survived and ~ 70 did not survive. While among male passengers ~110 survived and ~480 did not survive. \n\n\nThe second plots the count as a percentate of that group.  it shows that ~74% female passenger survived while only ~19% male passenger survived\n\n\nthe violin plot also reinforces the fact that more males die and more women survive and shows the density where more med die and womed survive.\n\n\nfrom this is is evident that  Males have less chance to survive than Female. this is probably due to the \"Women and children first\" mentality"},{"metadata":{"_uuid":"094eeb6b6b41ff33efec62fe0451a11af6de5fec"},"cell_type":"markdown","source":"### Survival by Age"},{"metadata":{"_uuid":"4aeab55579cdc3300071fb587a620e3aa6e962af"},"cell_type":"markdown","source":""},{"metadata":{"trusted":false,"_uuid":"b082198d9eb7c18284d7302d9acea03fc2cc1cc7"},"cell_type":"code","source":"Stats('Age')\n\n# plot survival number for age dependandcy\nfig, axes = plt.subplots(figsize = (20,6), nrows = 1, ncols = 3)\n\nax = sns.distplot(X_train[X_train['Survived'] == 1].Age.dropna(), bins = 20, label = 'Survived')\nax = sns.distplot(X_train[X_train['Survived'] == 0].Age.dropna(), bins = 20, label = 'Not Survived')\n\nax = sns.kdeplot(X_train[\"Age\"][(X_train[\"Survived\"] == 0) & (X_train[\"Age\"].notnull())], color = \"Green\", shade = False)\nax = sns.kdeplot(X_train[\"Age\"][(X_train[\"Survived\"] == 1) & (X_train[\"Age\"].notnull())], ax = ax, color = \"Blue\", shade= False)\n\nax.set_xlabel(\"Age\",fontsize = 15)\nax.set_ylabel(\"Frequency\",fontsize = 15)\nax = ax.legend([\"Not Survived\",\"Survived\"],fontsize = 15)\nplt.xlim(0,80)\nplt.ylim(0,0.04)\nplt.grid(True)\n\nwomen = X_train[X_train['Sex'] == 'female']\nmen = X_train[X_train['Sex'] == 'male']\n\n#For womwn\nax = sns.distplot(women[women['Survived'] == 1].Age.dropna(), bins = 20, label = 'survived', ax = axes[0], kde = False)\nax = sns.distplot(women[women['Survived'] == 0].Age.dropna(), bins = 20, label = 'not survived', ax = axes[0], kde = False)\nax.set_xlabel(\"Age\",fontsize = 15)\nax.set_ylabel(\"Count\",fontsize = 15)\nax.legend(fontsize = 15)\nax.set_title('Female', fontsize = 15)\nax.set(xlim = (0, X_train['Age'].max()));\nax.set(ylim = (0, 50));\n    \n    \n#For men\nax = sns.distplot(men[men['Survived'] == 1].Age.dropna(), bins = 20, label = 'survived', ax = axes[1], kde = False)\nax = sns.distplot(men[men['Survived'] == 0].Age.dropna(), bins = 20, label = 'not survived', ax = axes[1], kde = False)\nax.set_xlabel(\"Age\",fontsize = 15)\nax.set_ylabel(\"Count\",fontsize = 15)\nax.legend(fontsize = 15)\nax.set_title('Male', fontsize = 15)\nax.set(xlim = (0, X_train['Age'].max()))\nax.set(ylim = (0, 50));\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1a4c27a957856d3daf72188c1f24c866bad4f903"},"cell_type":"markdown","source":"FEMALES: have a much higher survival count than men. there are 2 intervals with a particularly high survival count: infants 0 - 5 year, and adults 16 - 38 years old\n\nMEN: have a much lower survival count than women. again there are 2 intervals with relatively high survival counts, infants 0 - 5 year, and adults 20 - 32 years old\n\n\nWhen we superimpose the two densities , we cleary see a peak correponsing (between 0 and 5) for babies and very young childrens.\n\n\nThe age distribution for survivors and non-survivors are very similar. One notable difference is that, of the survivors, a larger proportion were children. "},{"metadata":{"trusted":false,"_uuid":"547779eaf985e35a305444b3e9c71570f9991b48"},"cell_type":"code","source":"\nplt.figure(figsize=(20,12))\nplt.subplot(2,3,1)\nsns.boxplot( y = \"Age\", x = \"Survived\",data = X_train, palette = \"muted\")\n\nplt.subplot(2,3,2)\nsns.violinplot(\"Pclass\",\"Age\", hue = \"Survived\", data = X_train, split = True, palette = 'muted')\n\nplt.subplot(2,3,3)\nsns.violinplot(\"Sex\",\"Age\", hue = \"Survived\", data = X_train, split = True, palette = 'muted')\n\nplt.subplot(2,3,4)\nsns.boxplot(y = \"Age\", x = \"Sex\", data = X_train, palette = \"muted\")\n\nplt.subplot(2,3,5)\nsns.boxplot(y = \"Age\", x = \"Sex\", hue = \"Pclass\", data = X_train, palette = \"muted\")\n\nplt.subplot(2,3,6)\nsns.boxplot(y = \"Age\", x = \"Parch\", data = X_train, palette = \"muted\")\n\nplt.subplot(2,3,4)\nsns.boxplot(y = \"Age\", x = \"SibSp\", data = X_train, palette = \"muted\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ae276e5345a8e0d7605f7ebddf4cbd32648349c0"},"cell_type":"markdown","source":"1. the age distribution of both mena and women are the same\n2. for both men and women the mean survival and death age increases with class\n3. for both men an women the mean age of survival and death is the same\n4. in generatl the low the number of siblings/spouse a person has the larger the mean age and large distrubtion \n5. for both male and female, the the lower the higher the class the larger the age means and range."},{"metadata":{"_uuid":"c1781ef172c5e04c0e80a3dcde62387f942bae43"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"ad726e491fb12fdf70d31bd4505362bae18ac8bb"},"cell_type":"markdown","source":"### Survival by Class"},{"metadata":{"trusted":false,"_uuid":"00ccae6db10f2e50294645742249c2f54061ae3e"},"cell_type":"code","source":"plt.figure(figsize = (16,10))\nplt.subplot(2, 3, 1)\nsns.barplot(x = 'Pclass', y = 'Survived', data = X_train)\n\nplt.subplot(2, 3, 2)\nsns.barplot(x=\"Sex\", y=\"Survived\", hue=\"Pclass\", data=X_train);\n\nplt.subplot(2, 3, 3)\nsns.barplot(x = \"Pclass\", y = \"Survived\", hue = \"Sex\", data = X_train)\n\nplt.subplot(2, 3, 4)\nsns.countplot(x = 'Survived',hue = 'Pclass',data = X_train);\n\nplt.subplot(2, 3, 5)\nsns.violinplot(y = 'Survived', x = 'Pclass', data = X_train, inner = 'quartile')\nplt.subplot(2, 3, 6)\nsns.violinplot(x='Pclass', y = 'Age', hue = 'Survived', data = X_train, split = True)\n\n\nplt.subplots(figsize=(16,5))\nplt.subplot(131)\nsns.boxplot(x = \"Pclass\", y = \"Age\", hue = \"Sex\", data = X_train);\nplt.ylim(0,90)\n\nplt.subplot(132)\nsns.boxplot(y = \"Age\", x = \"Sex\", hue = \"Pclass\", data = X_train)\nplt.subplot(133)\nX_train.Age[X_train['Pclass'] == 1].plot(kind = 'kde')    \nX_train.Age[X_train['Pclass'] == 2].plot(kind = 'kde')\nX_train.Age[X_train['Pclass'] == 3].plot(kind = 'kde')\n # plots an axis lable\nplt.xlabel(\"Age\")    \nplt.title(\"Age Distribution within classes\", fontsize = 15)\n# sets our legend for our graph.\nplt.legend(('1st Class', '2nd Class','3rd Class'), loc = 'best') ;\nplt.xlim(0,80)\nplt.ylim(0,0.04)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"381684795f0a217a23d320d730ef547f2df018b5"},"cell_type":"markdown","source":"Here we see clearly, that Pclass is contributing to a persons chance of survival, especially if this person is in class 1.\n\n\n(1.) the higher the class hte higher the rate of survival\n\n(2. & 3). for both men and women the higher the class hte higher the rate of survival, but in each class the women more than twice as much as the the men (infact it )\n\n(4. & 5.) the age range increases with increasing class. 1st clas mmen are older than 1st class women, the ranges are closer for the other classes. \n\n(6.) the mean age increases with increasing class but the density decreases, (i.ee 1st class have lees people and there mean age is older)"},{"metadata":{"trusted":false,"_uuid":"4773e91e2a7f913c2412791ddc59ab4d16e78045"},"cell_type":"code","source":"# grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')\ngrid = sns.FacetGrid(X_train, col = 'Survived', row = 'Pclass', size = 3, aspect = 3.2)\ngrid.map(plt.hist, 'Age', alpha = 0.8, bins=20)\ngrid.add_legend();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cefb7cd2629d56382eacd2db52bb83a91582cccc"},"cell_type":"markdown","source":"The plot above confirms our assumption about pclass 1, but we can also spot a high probability that a person in pclass 3 will not survive.\n"},{"metadata":{"_uuid":"61513c8b02a58a5468ab18bf4b6b2f2be4a35b2b"},"cell_type":"markdown","source":"the mode increases with decreasing class the third class will have a a large number of infants"},{"metadata":{"_uuid":"e1bdcdedc398a78d5f164cde3c40e0a7ea892537"},"cell_type":"markdown","source":"### Survival by Embarked Lcoations"},{"metadata":{"_uuid":"e54ad98e389df326d2f6e749dd46a70f1b6a585f"},"cell_type":"markdown","source":"Passangers embarked in three different locations. The survival rate is dependand on this. pehaps passangers from a certian embark location belong to a specific class"},{"metadata":{"trusted":false,"_uuid":"ac4351f352c2a8ce335fbb51670fd5739f994843"},"cell_type":"code","source":"# Explore Embarked vs Survived \nplt.figure(figsize = (16,6))\nplt.subplot(1, 3, 1)\nsns.barplot(x = \"Embarked\", y = \"Survived\",  data = X_train)\n\n# Explore Pclass vs Survived by Sex\nplt.subplot(1, 3, 2)\nsns.barplot(x = \"Embarked\", y = \"Survived\", hue = \"Sex\", data = X_train)\n#g = g.set_ylabels(\"survival probability\")\nplt.subplot(1, 3, 3)\nsns.countplot(x = 'Survived',hue = 'Embarked',data = X_train);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"974b41e468005a96911f339ac6b02e3f25a03af4"},"cell_type":"markdown","source":"1. Embarkment from location C has the highest survival rate\n2. for the two sexes, the same patter is seem but women have a much greater chance of survival (three times)\n3. most people embarked at S, but these die the most. probably related to class."},{"metadata":{"trusted":false,"_uuid":"cccc5715f0950178cddfd862d09ba9663cbbcf98"},"cell_type":"code","source":"plt.figure(figsize = (15,5))\nsns.boxplot(y = \"Age\", x = \"Embarked\", hue = \"Pclass\", data = X_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6a41fb3f4f8f4daf1fa4dc6af904fa0c539b217a"},"cell_type":"markdown","source":"It seems that passenger coming from Cherbourg (C) have more chance to survive. Below we can see that this is not related to class. perhaps it is related to Derck level... see later"},{"metadata":{"trusted":false,"_uuid":"01a316c134aecd3e7136c8c0d69dfdf96e7046b6"},"cell_type":"code","source":"# Explore Pclass vs Embarked \nPlotParams(15, 8, 6)\nsnsParams(2,'muted')\n\ng = sns.factorplot(\"Pclass\", col = \"Embarked\",  data = X_train, size = 8, \n                   kind = \"count\", palette = \"muted\")\ng = g.set_ylabels(\"Count\")\ng = sns.factorplot(\"Pclass\", col = \"Embarked\",  data = X_train,\n                   hue = \"Sex\", size = 8, kind = \"count\", palette = \"muted\")\n\ng = g.set_ylabels(\"Count\")\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d7b6c561fb2c39d7e35eb54f35cb35d06293d777"},"cell_type":"markdown","source":"### SibSP & Parch"},{"metadata":{"trusted":false,"_uuid":"e705fd4d6005f217bc2d6b3a91a0b0e82830a14b"},"cell_type":"code","source":"PlotParams(15, 10, 6)\nplt.figure(figsize = (16,5))\nplt.subplot(1, 2, 1)\nsns.barplot(x = \"Parch\", y = \"Survived\",  data = X_train, palette = \"muted\")\nplt.subplot(1, 2, 2)\nsns.barplot(x = \"SibSp\", y = \"Survived\",  data = X_train, palette = \"muted\")\n\nplt.figure(figsize=(20,5))\nplt.subplot(1, 2, 1)\nsns.violinplot(y = 'Survived', x = 'Parch', data = X_train, palette = \"muted\", inner = 'quartile')\nplt.subplot(1, 2, 2)\nsns.violinplot(y = 'Survived', x = 'SibSp', data = X_train, palette = \"muted\", inner = 'quartile')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ffdd7157590ebd5e00130b982289129cb02f2295"},"cell_type":"markdown","source":"\n\n1. Small families have more chance to survive, more than single (Parch 0), medium sized families (Parch 3,4) and large families (Parch 5,6 ).\n\n"},{"metadata":{"_uuid":"c0d65a581469cc484769e4989f13cafb607e2afe"},"cell_type":"markdown","source":"# Fare"},{"metadata":{"trusted":false,"_uuid":"f107df48b7e6ce0f781e3a366f51441358aa16a3"},"cell_type":"code","source":"PlotParams(15, 8, 6)\n\nplt.figure(figsize = (20,6))\nplt.subplot(1,3,1)\nsns.kdeplot(X_train[\"Fare\"])\nplt.xlim(0,160)\nplt.ylim(0,.040)\nplt.xlabel('Fare')\nplt.ylabel('Survival Probability')\n\nplt.subplot(1,3,2)\nax = sns.distplot(X_train[X_train['Survived'] == 1].Fare.dropna(), bins = 80, label = 'Survived')\nax = sns.distplot(X_train[X_train['Survived'] == 0].Fare.dropna(), bins = 80, label = 'Not Survived')\nax = sns.kdeplot(X_train[\"Fare\"][(X_train[\"Survived\"] == 0) & (X_train[\"Fare\"].notnull())], color = \"Green\", shade = False)\nax = sns.kdeplot(X_train[\"Fare\"][(X_train[\"Survived\"] == 1) & (X_train[\"Fare\"].notnull())], ax = ax, color = \"Blue\", shade= False)\nax.set_xlabel(\"Fare\",fontsize = 15)\nax.set_ylabel(\"Frequency\",fontsize = 15)\nax = ax.legend([\"Not Survived\",\"Survived\"],fontsize = 15)\nplt.ylim(0,0.1)\nplt.xlim(0,160)\nplt.grid(True)\n\nplt.subplot(1,3,3)\nax1 = sns.boxplot(x = \"Embarked\", y = \"Fare\", hue = \"Pclass\", data = X_train);\nplt.ylim(0,200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c8c3ae41598b4afa423081e5e78d11b0ca7e760c"},"cell_type":"code","source":"Stats('Fare')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5fab4a5646f45dcd4997f6280c5864db4a59be86"},"cell_type":"markdown","source":"\n\nAs the distributions are clearly different for the fares of survivors vs. deceased, it's likely that this would be a significant predictor in our final model. Passengers who paid lower fare appear to have been less likely to survive. This is probably strongly correlated with Passenger Class."},{"metadata":{"_uuid":"4e8a72e8cdc75c5b8225ef2986a8cec29543f044"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"e254692b86d9b4aa052e7c7d489684382116580c"},"cell_type":"markdown","source":"## Missing Data"},{"metadata":{"_uuid":"4049fdf2a0cbfa94a583a846d959c91a8d6a3e82"},"cell_type":"markdown","source":"as can be seen from the above data, there are several featured with NaN missing values. Lets take a look at the features that have missing values missing\n"},{"metadata":{"trusted":false,"_uuid":"4eca15141d1957897344c631d8b60e74253e8351"},"cell_type":"code","source":"# Fill empty values with NaN\nX_train = X_train.fillna(np.nan)\nX_test = X_test.fillna(np.nan)\n\n#finds missing values\nmissing_train = Missing(X_train)\nmissing_test = Missing(X_test)\n    \nprint('TRAIN DATA','\\t\\t','TEST DATA')\nSideSide(missing_train, missing_test)\n\n#plot missing data in heatmap for visualisation\nprint('\\n\\n  MISSING TRAINING DATA \\t\\t\\t MISSING TEST DATA')\ncmap = sns.diverging_palette( 220 , 10 , as_cmap = True )\nplt.figure(figsize = (10,5));\nplt.subplot(1, 2, 1)\nsns.heatmap(X_train.isnull(), yticklabels = False, cbar = False, cmap = cmap)\nplt.subplot(1, 2, 2)\nsns.heatmap(X_test.isnull(), yticklabels = False, cbar = False,cmap = cmap);\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"308e2960a6dc6028938c77be759e966f2c2f43aa"},"cell_type":"markdown","source":"Age, embarked and cabin have the most missing values. These terms can be used to determine the survival (see later). thus these missing values will need to be filled in based on their relationship with other featured\n\nThe Embarked feature has only 2 missing values, which can easily be filled. \nThe 'Age' feature, which has 177 missing values, will be filled with values based on its relationship with other features. "},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"03c94b61c677af33bdfd95e03b2e267895b492f8"},"cell_type":"code","source":"#combine the tets and training data so that operations can be performed together\nfull_data = [X_train, X_test] ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"73ebec1d714a1a42f653459e3c19b3befa2598d7"},"cell_type":"markdown","source":"### Missing Embark"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"dcda52682299fb3ab9da1d5f78fe2f2dc18044cd"},"cell_type":"code","source":"#fill in Embarked datta with S as it is the most common\nfor X in full_data:\n    X['Embarked'] = X['Embarked'].fillna(\"S\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"16a9338eaf41164fbe15b45aa06c5fcb4a8a41c0"},"cell_type":"markdown","source":"### Missing Cabin"},{"metadata":{"_uuid":"744a2d30e813a4a08a6cb834a57f9aa9e28d8696"},"cell_type":"markdown","source":"Cabin has alot of data missing. the replacement of this feature is performed durign the feature engineering section... see below\n"},{"metadata":{"_uuid":"cb02ccc908e951fa32bda755860abf46a74dfa94"},"cell_type":"markdown","source":"### Missing fare"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"925e6194000c97da9e60f354cce93ef390d05396"},"cell_type":"code","source":"# fill missing Fare with median fare for each Pclass\nfor X in full_data:\n    X[\"Fare\"].fillna(X.groupby(\"Pclass\")[\"Fare\"].transform(\"median\"), inplace = True)\n    X[\"Fare Group\"] = X[\"Fare\"]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c9f614aa24130e4b0a5baa124d5afdac252d9d57"},"cell_type":"markdown","source":"### Missing Age"},{"metadata":{"_uuid":"749851e41ddbfacbd5d345ba80aeb0011bef9ff9"},"cell_type":"markdown","source":"Age distribution seems to be the same in Male and Female subpopulations, so Sex is not informative to predict Age.\n\nHowever, 1rst class passengers are older than 2nd class passengers who are also older than 3rd class passengers.\n\nMoreover, the more a passenger has parents/children the older he is and the more a passenger has siblings/spouses the younger he is.\n\nLets take a closer look at the correlations"},{"metadata":{"trusted":false,"_uuid":"dc5292eef93c5cbde2c7402499dfdfbf8c7d29b0"},"cell_type":"code","source":"PlotCorr(X_train[[\"Age\",\"Sex\",\"SibSp\",\"Parch\",\"Pclass\"]])\n\n#correlation of features with target variable\nAge_Corr = X_train.corr()[\"Age\"]\n#Age_Corr= Age_Corr[np.argsort(Age_Corr, axis = 0)[::-1]] #sort in descending order\nAge_Corr = Age_Corr[1:10] # remove the 'Survived'\nprint(Age_Corr)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ea6c0f978efe59713e6ec32083cc057742537bfe"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\n#use random forest to predict age\ndef MissingAges(X, AGE_features):\n    \n    age_data = X[age_features]\n\n    known_ages = age_data[age_data['Age'].notnull()].as_matrix()\n    unknown_ages = age_data[age_data['Age'].isnull()].as_matrix()\n\n    # Create target and eigenvalues for known ages\n    target = known_ages[:, 0]\n    eigen_val = known_ages[:, 1:]\n\n    # apply random forest regressor\n    RFR_age = RandomForestRegressor(random_state = 0, n_estimators = 2000, n_jobs = -1)\n    RFR_age.fit(eigen_val, target)\n\n\n    return (RFR_age, unknown_ages)\n\n# age distribution BEFORE filling in missing values\nfig, (axis1,axis2) = plt.subplots(1,2,figsize=(15,4))\naxis1.set_title('Original Age values')\naxis2.set_title('New Age values')\n# plot new Age Values\nX_train['Age'].hist(bins = 70, ax = axis1)\nplt.xlabel('Age')\nplt.ylabel('Counts')\nplt.xlim(0,80)\n\n#the features used to determine missing ages\nage_features = [\"Age\", \"SibSp\", \"Parch\", \"Pclass\"]\n\n# filling ing the training data\nRFR_age, unknown_ages_train = MissingAges(X_train, age_features)\nAge_predictions_train = RFR_age.predict(unknown_ages_train[:, 1::])\nX_train.loc[(X_train['Age'].isnull()), \"Age\"] = Age_predictions_train\nX_train[\"Age\"] = X_train[\"Age\"].astype(int)\n\n# filling in the test data\n_, unknown_ages_test = MissingAges(X_test, age_features)\nAge_predictions_test = RFR_age.predict(unknown_ages_test[:, 1::])\nX_test.loc[(X_test['Age'].isnull()), \"Age\"] = Age_predictions_test\nX_test[\"Age\"] = X_test[\"Age\"].astype(int)\n\n\n# age distribution AFTER filling in missing values\nX_train['Age'].hist(bins = 70, ax = axis2)\nplt.xlabel('Age')\nplt.ylabel('Counts')\nplt.xlim(0,80)\nprint('Ages filled in')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3923dbf4e4181468f2c8f866c0830bac3f76d3de"},"cell_type":"markdown","source":"### Missing Data now"},{"metadata":{"_uuid":"7ca8a76b505b9bfed51ac14aa966bf44262d1d4c"},"cell_type":"markdown","source":"# Feature Engineering"},{"metadata":{"_uuid":"84e75391b90fd4509766b291a93644b5e46dc576"},"cell_type":"markdown","source":"### Cabin and Deck level"},{"metadata":{"_uuid":"0aef0f305980914fca732bef93dd4bb265368f5e"},"cell_type":"markdown","source":"ater looking closely at the cabin number, it can be seen that it is an alpha-numeric identity. The letter indicates the deck and the number represents the cabin number on this deck. We will therefore subsitute this 'Cabin' category for a 'Deck\" categor and simply extract the deck letter\n\nrecall most cabin number are missing so lets see how may people have cabine and if it is related to surviving"},{"metadata":{"trusted":false,"_uuid":"647c3b79a36325e9e56321402c4d82a808d05bfa"},"cell_type":"code","source":"# cabin Vrs no cabine survival rates\nfor X in full_data:\n    X[\"CabinBool\"] = (X[\"Cabin\"].notnull().astype('int'))\n    \n#draw a bar plot of CabinBool vs. survival\nsns.barplot(x = \"CabinBool\", y = \"Survived\", data = X_train)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"9ec1af4c59fe586ed398f847b4c397327a36259c"},"cell_type":"code","source":"# Extract deck \ndef extract_cabin(x):\n    return x != x and 'Other' or x[0]\n\nfor X in full_data:\n    X['Cabin'] = X['Cabin'].apply(extract_cabin)\n    X['Deck'] = X['Cabin']\n\ntrain_deck = pd.DataFrame(X_train.groupby('Deck').size())\ntest_deck = pd.DataFrame(X_test.groupby('Deck').size())\n\nprint('TRAIN \\t\\t TEST')\nSideSide(train_deck,test_deck )","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"5470f220ffd4b6f678233f0ef18263ad7091851c"},"cell_type":"code","source":"snsParams(1.2, 'muted')\nplt.figure(figsize = (16,5))\n\nplt.subplot(1, 3, 1)\ng = sns.countplot(X_train[\"Cabin\"], palette = \"muted\")\nplt.subplot(1, 3, 2)\ng = sns.barplot(x = \"Deck\", y = \"Survived\",  data = X_train, palette = \"muted\")\n\nplt.subplot(1, 3, 3)\nsns.countplot(x = 'Survived',hue = 'Deck',data = X_train, palette = \"muted\");\n\nsnsParams(2, 'muted')\nplt.figure(figsize = (16,5))\ng = sns.factorplot(\"Deck\", col = \"Pclass\",  data = X_train, size = 8, \n                   kind = \"count\", palette = \"muted\")\ng = g.set_ylabels(\"Count\")\ng = sns.factorplot(\"Deck\", col = \"Embarked\",  data = X_train,\n                   hue = \"Sex\", size = 8, kind = \"count\", palette = \"muted\")\ng = g.set_ylabels(\"Count\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7e09ac1f9bd761ef32b55eb27100c5523db18cd0"},"cell_type":"markdown","source":"we can see that passengers with a cabin have generally more chance to survive than passengers without (X).\n\nIt is particularly true for cabin B, C, D, E and F.\n\nmost people where the deck is unknown are from the 3rd class"},{"metadata":{"_uuid":"0d1e87d73c338e87e81a03d4cd5ec4ccc429e3d7"},"cell_type":"markdown","source":"### Family Size, Alone"},{"metadata":{"trusted":false,"_uuid":"7a1445849d3fc26ac171ce5bb29563ce569ae77a"},"cell_type":"code","source":"PlotParams(15, 8, 6)\n# determine size of family on board\nfor X in full_data:\n    X['Family Size'] = X['SibSp'] + X['Parch'] + 1 \n    X['Alone'] = [1 if i<2 else 0 for i in X['Family Size']]\n    X['Surname'] = X['Name'].str.extract('(\\w+),', expand = False)\n    X['Large Family'] = [1 if i > 5 else 0 for i in X['Family Size']]\n    \n    #X['First Name'] = X['Name'].str.extract('(Mr\\. |Miss\\. |Master. |Mrs\\.[A-Za-z ]*\\()([A-Za-z]*)',expand = False)[1]\n    \naxes = sns.factorplot('Family Size','Survived', hue = 'Sex', data = X_train, aspect = 2)\nplt.grid(True)\naxes = sns.factorplot('Family Size','Survived',  data = X_train, aspect = 2)\nplt.grid(True)\npd.crosstab(X_train['Family Size'], X_train['Survived']).plot(kind = 'bar', stacked = True)\n    \n\nfig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(18,6))\nsns.barplot(x = \"Family Size\", y = \"Survived\", hue = \"Sex\", data = X_train, ax = axis1);\nsns.barplot(x = \"Alone\", y = \"Survived\", hue = \"Sex\", data = X_train, ax = axis2);\nsns.barplot(x = \"Alone\", y = \"Survived\", data = X_train)\nplt.show()\n   \n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"76178f987a361f69f8dd2c7633216dfa35de7888"},"cell_type":"markdown","source":"\n\nAssumption: the less people was in your family the faster you were to get to the boat. The more people they are the more managment is required. However, if you had no family members you might wanted to help others and therefore sacrifice.\n\nThe females traveling with up to 2 more family members had a higher chance to survive. However, a high variation of survival rate appears once family size exceeds 4 as mothers/daughters would search longer for the members and therefore the chanes for survival decrease.\n\nAlone men might want to sacrifice and help other people to survive.\n\n"},{"metadata":{"_uuid":"f1703cf5216416c5f13764257e4c142d780c341f"},"cell_type":"markdown","source":"### Title"},{"metadata":{"trusted":false,"_uuid":"481bb4e62f57730c7de43cbdec76b0cbac7e318c"},"cell_type":"code","source":"def get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    # If the title exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    return \"\"\n\n# Create a new feature Title, containing the titles of passenger names\nfor X in full_data:\n    X['Title'] = X['Name'].apply(get_title)\n    \n# Group all non-common titles into one single grouping \"Rare\"\nfor X in full_data:\n    X['Title'] = X['Title'].replace(['Lady', 'Countess', 'Don', 'Sir', 'Jonkheer', 'Dona'], 'Noble')\n    X['Title'] = X['Title'].replace(['Capt', 'Col', 'Dr', 'Major', 'Rev'], 'Officer')\n    X['Title'] = X['Title'].replace('Mlle', 'Miss')\n    X['Title'] = X['Title'].replace('Ms', 'Miss')\n    X['Title'] = X['Title'].replace('Mme', 'Mrs')\n\n    \nprint('TRAIN TITLE \\t TEST TITLES')\ntrain_titles = pd.DataFrame(X_train.Title.value_counts())\ntest_titles = pd.DataFrame(X_test.Title.value_counts())\n\nSideSide(train_titles,test_titles)\n\nplt.figure(figsize = (16,6))\nplt.subplot(1, 3, 1)\ng = sns.barplot(x = \"Title\", y = \"Survived\",  data = X_train)\nplt.xticks(rotation = 90)\n\nplt.subplot(1, 3, 2)\nsns.countplot(x = 'Survived', hue = 'Title',data = X_train);\nplt.xticks(rotation = 90)\n\nplt.subplot(1, 3, 3)\nsns.boxplot(data = X_train, x = \"Title\", y = \"Age\");\nplt.xticks(rotation = 90)\n\ntab = pd.crosstab(X_train['Title'], X_train['Pclass'])\ntab_prop = tab.div(tab.sum(1).astype(float), axis=0)\n\ntab_prop.plot(kind = \"bar\", stacked = True)\nplt.xticks(rotation = 90)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"115fcdef4135bab6027cddaf95efe479a5f37145"},"cell_type":"markdown","source":"### Age Category"},{"metadata":{"trusted":false,"_uuid":"f4b0cad78870ca2a5d2e6285775d27524ffcdb1c"},"cell_type":"code","source":"#sort the ages into logical categories\n## create bins for age\ndef AgeCategory(age):\n    a = ''\n    if age <= 3:\n        a = 'Baby'\n    elif age <= 12: \n        a = 'Child'\n    elif age <= 18:\n        a = 'Teenager'\n    elif age <= 35:\n        a = 'Young Adult'\n    elif age <= 65:\n        a = 'Adult'\n    elif age == 'NaN':\n        a = 'NaN'\n    else:\n        a = 'Senior'\n    return a\n        \nfor X in full_data:\n    X['Age Group'] = X['Age'].map(AgeCategory)\n    X['Age*Class'] = X['Age'] * X['Pclass']\n\nplt.figure(figsize = (16,6))\nplt.subplot(1, 3, 1)\ng = sns.barplot(x = \"Age Group\", y = \"Survived\",  data = X_train)\nplt.xticks(rotation = 90)\n\nplt.subplot(1, 3, 2)\nsns.countplot(x = 'Survived', hue = 'Age Group',data = X_train)\n\nplt.subplot(1, 3, 3)\nsns.boxplot(data = X_train, x = \"Age Group\", y = \"Age\");\nplt.xticks(rotation = 90)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"004b81235839da6b4ee34b11c0b29674325ced64"},"cell_type":"markdown","source":"### Person Type "},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"969e47e4b4c87d62d81ac8fc954c8d5713e61beb"},"cell_type":"code","source":"def GetPerson(X):\n    age, sex = X\n    return 'child' if age < 16 else sex\n\nfor X in full_data:\n    X['Person'] = X[['Age','Sex']].apply(GetPerson, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"73e7cc884c94aeecaedc33393a7f9fcb2d8b5f58"},"cell_type":"markdown","source":"### Fare Feature"},{"metadata":{"_uuid":"9ab5ec6151afdcebde098a9d45d8962a3b792079"},"cell_type":"markdown","source":"Although there now are no missing FarePP’s anymore, I also noticed that 17 Fares actually have the value 0. These people are not children that might have traveled for free. I think the information might actually be correct (have people won free tickets?), but I also think that the zero-Fares might confuse the algorithm. For instance, there are zero-Fares within the 1st class passengers. To avoid this possible confusion, I am replacing these values by the median FarePP’s for each Pclass."},{"metadata":{"_uuid":"2e1d1e924b5b4f71df039518218a4bb88ba5e126"},"cell_type":"markdown","source":"Above you can see that the Fare is very skewed. I know that this is not desirable for some algorithms, and can be solved by taking the logarithm or normalisation \n\nAnother option is to use Fare Groups instead of keeping the FarePerPerson as a numeric variable. "},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"41c1e9d8a0dbc61770fb5a2b2fff834ac85e406a"},"cell_type":"code","source":"\n    \nfor X in full_data:\n    X.loc[ X['Fare Group'] <= 7.91, 'Fare Group'] = 0\n    X.loc[(X['Fare Group'] > 7.91) & (X['Fare Group'] <= 14.454), 'Fare Group'] = 1\n    X.loc[(X['Fare Group'] > 14.454) & (X['Fare Group'] <= 31), 'Fare Group']   = 2\n    X.loc[(X['Fare Group'] > 31) & (X['Fare Group'] <= 99), 'Fare Group']   = 3\n    X.loc[(X['Fare Group'] > 99) & (X['Fare Group'] <= 250), 'Fare Group']   = 4\n    X.loc[X['Fare Group'] > 250, 'Fare Group'] = 5\n    X['Fare Group'] = X['Fare Group'].astype(int)   ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f2ef2a6f5ebcfeece040f34bef3bcccb5e693159"},"cell_type":"markdown","source":"### Mapping"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"20d7aa4c5e017641ba4975d0f7347a077d01cf8a"},"cell_type":"code","source":"#map each Sex value to a numerical value\nsex_map = {\"male\": 0, \"female\": 1}\nperson_map = {'child': 0, \"male\": 1, \"female\": 2}\nEmbark_map = {\"C\": 1,\"S\": 2, \"Q\": 3}\ndeck_map = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7, \"T\": 8, \"Other\": 9}\nage_map = {\"Baby\": 1, \"Child\": 2, \"Teenager\": 3, \"Young Adult\": 4, \"Adult\": 5, \"Senior\": 6}\ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Officer\": 5, \"Noble\": 5}\n\nfor X in full_data:\n    X[\"Sex\"] = X[\"Sex\"].map(sex_map)\n    X[\"Embarked\"] = X[\"Embarked\"].map(Embark_map)\n    X[\"Person\"] = X[\"Person\"].map(person_map)\n    X[\"Deck\"] = X[\"Deck\"].map(deck_map)\n    X[\"Age Group\"] = X[\"Age Group\"].map(age_map)\n    X[\"Title\"] = X[\"Title\"].map(title_mapping)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"c37c056e562fc9afcd09e6c81f5c97ce20f97aec"},"cell_type":"code","source":"X_train = X_train.drop(\"Name\", axis = 1) \nX_test = X_test.drop(\"Name\", axis = 1) \nX_train = X_train.drop(\"Ticket\", axis = 1) \nX_test = X_test.drop(\"Ticket\", axis = 1) \nX_train = X_train.drop(\"Cabin\", axis = 1) \nX_test = X_test.drop(\"Cabin\", axis = 1) \nX_train = X_train.drop(\"Surname\", axis = 1) \nX_test = X_test.drop(\"Surname\", axis = 1) \n#X_train = X_train.drop(\"Age\", axis = 1) \n#X_test = X_test.drop(\"Age\", axis = 1) \nX_test = X_test.drop(\"PassengerId\", axis = 1) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"742484ee7165d6ebcb6c63d0e5b07359caa172d0"},"cell_type":"code","source":"X_train.head()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"dd0d93871d63610cdda65a658e5d4a0fc1c3a60c"},"cell_type":"code","source":"# Feature Scaling\n\ndef Norm(X):\n    X1 = (X - np.mean(X)) / (np.max(X) - np.min(X))\n    return(X1)\n\nX_train['Fare'] = Norm(X_train['Fare'])\nX_train['Age'] = Norm(X_train['Age'])\nX_test['Fare'] = Norm(X_test['Fare'])\nX_test['Age'] = Norm(X_test['Age'])\n\n#X_train['Fare'] = X_train['Fare'].astype(int)\n#X_train['Age'] = X_train['Age'].astype(int)\n#X_test['Fare'] = X_test['Fare'].astype(int)\n#X_test['Age'] = X_test['Age'].astype(int)\n#X_train['Age*Class'] = X_train['Age*Class'].astype(int)\n#X_test['Age*Class'] = X_test['Age*Class'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b033a068f89bb848d5f25cf05f89c0238d3c8826"},"cell_type":"markdown","source":"### Look at hte prepared Data"},{"metadata":{"trusted":false,"_uuid":"b7accd857dc27520d9c9e7b6fb8b00314d3278cc"},"cell_type":"code","source":"X_train.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"94ebf33e2e5b24904f990c8f945c012db293c314"},"cell_type":"code","source":"plt.figure(figsize = (20,12))\nPlotCorr(X_train);","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3e7cd7cdc651c6c4e3c2e92b859b23099dc93d1f"},"cell_type":"code","source":"plt.figure(figsize = (20,12))\nTopCorr(X_train, 0.25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ae742c3c502cffe5fe1e059c1c34e5ea6f20fc63"},"cell_type":"code","source":"# highest correlated with correlation of features with 'Survived'\nprint('Featured hights correlation with survival')\nprint('Feature\\tCorrelation')\nSurvive_Corr = X_train.corr()[\"Survived\"]\nSurvive_Corr = Survive_Corr[1:20] # remove the 'Survived'\nSurvive_Corr= Survive_Corr[np.argsort(Survive_Corr, axis = 0)[::-1]] #sort in descending order\nprint(Survive_Corr)\n\n\ncorrelations = X_train.corr() # determines parameters that are correlated to Survival\n# most correlated featues = features with correlation to Survival >0.1\ntop_correlations = correlations.index[abs(correlations[\"Survived\"]) > 0.1]\nplt.figure(figsize=(12,10))\nsns.set(font_scale = 1.5)\ng = sns.heatmap(X_train[top_correlations].corr(), annot = True, cmap = cmap, annot_kws={\"size\": 10})\nplt.title('Features most correlated with Survival (>0.1)')\nplt.yticks(rotation = 0)\nplt.xticks(rotation = 90)\n\nsnsParams(2, 'muted')\nplt.figure(figsize = (10,6))\nax = sns.barplot(x = np.arange(len(Survive_Corr)), y = np.array(Survive_Corr.values), \n                 palette = 'muted', orient= 'v');\nax.set_xlabel(\"Feature\",fontsize = 15)\nax.set_ylabel(\"Correlation Coefficient\",fontsize = 15)\nax.set_xticklabels(Survive_Corr.index)\nplt.xticks(rotation = 90)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"a8e7832aaaba360730ecf49066f6576e620a0365"},"cell_type":"code","source":"# can drop a few more features\nX_train = X_train.drop(\"Age\", axis = 1) \nX_test = X_test.drop(\"Age\", axis = 1) \nX_train = X_train.drop(\"SibSp\", axis = 1) \nX_test = X_test.drop(\"SibSp\", axis = 1) \nX_train = X_train.drop(\"Parch\", axis = 1)\nX_test = X_test.drop(\"Parch\", axis = 1)\nX_train = X_train.drop(\"Family Size\", axis = 1)\nX_test = X_test.drop(\"Family Size\", axis = 1)\nX_train = X_train.drop(\"Age Group\", axis = 1)\nX_test = X_test.drop(\"Age Group\", axis = 1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f1a2d589523e7bb3348e1690184b5055a1cab9e0"},"cell_type":"code","source":"# final features\nSurvive_Corr = X_train.corr()[\"Survived\"]\nSurvive_Corr = Survive_Corr[1:9] # remove the 'Survived'\nSurvive_Corr= Survive_Corr[np.argsort(Survive_Corr, axis = 0)[::-1]] #sort in descending order\n\nsnsParams(2, 'muted')\nplt.figure(figsize = (10,6))\nax = sns.barplot(x = np.arange(len(Survive_Corr)), y = np.array(Survive_Corr.values), \n                 palette = 'muted', orient= 'v');\nax.set_xlabel(\"Feature\",fontsize = 15)\nax.set_ylabel(\"Correlation Coefficient\",fontsize = 15)\nax.set_xticklabels(Survive_Corr.index)\nplt.xticks(rotation = 90)\n\nX_train = X_train.drop(\"Survived\", axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"90e82c2733ff8c7d348c30b7c5ca2d78ba12ac43"},"cell_type":"code","source":"sns.pairplot(X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f332d5aa3f4c3507b0e98a0e6db91fb5535e7935"},"cell_type":"code","source":"print('TRAINING')\nprint(X_train.info())\nprint('\\n\\nTEST')\nprint(X_train.info())\n\nX_train.head(0)\nX_test.head(0)\n\ncnt = 0\nd_type = ['float64', 'int64','object','dtype']\nprint('\\n\\tTRAIN \\t\\t TEST')\nfor c1, c2 in zip(X_train.get_dtype_counts(), X_test.get_dtype_counts()):\n    cnt += 1\n    print(\"%s:\\t%-9s \\t%s\"%(d_type[cnt],c1, c2))\n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2eedb76bc1f8be937e44ce16b978b20e33f3488a"},"cell_type":"markdown","source":"## PREDICTION MODELS"},{"metadata":{"_uuid":"97e4a521dd60615fca4e4ec678cb35bff05958f2"},"cell_type":"markdown","source":"#### Classifiers"},{"metadata":{"_uuid":"63efa15365f04ee338afafc09a06ac6f85a24727"},"cell_type":"markdown","source":"Logistic regression\nNaive Bayes\nSupport Vector machine\ndecision tree\nBoosted Trees\nRandom Forest\nNeural Network\nNearest Neighbour\nPerceptron\n"},{"metadata":{"_uuid":"d45b46fb046c9001065df3b61481c33fe5783faa"},"cell_type":"markdown","source":"#### Evaluation of Classification"},{"metadata":{"_uuid":"e90c1d4f430703c2b317d96ea302c74c9f3f5462"},"cell_type":"markdown","source":"Confusion matrix: This is the matrix of the actual versus the predicted. This concept is better explained with the example of cancer prediction using the model:\n\n- True positives (TPs): True positives are cases when we predict the disease as yes when the patient actually does have the disease.\n\n\n- True negatives (TNs): Cases when we predict the disease as no when the patient actually does not have the disease.\n\n\n- False positives (FPs): When we predict the disease as yes when the patient actually does not have the disease. FPs are also considered to be type I errors.\n\n\n- False negatives (FNs): When we predict the disease as no when the patient actually does have the disease. FNs are also considered to be type II errors.\n\n\n- Accuracy:  Overall effectivness of a classifier (TP + TN)/(TP + TN + FP + FN)\n\n\n-  precision or positive predictive value (PPV): correct positive labels? (TP)/(TP + FP)\n\n\n- Recall/sensitivity/true positive rate: effectiveness to identify positive labels?\n(TP/TP+FN)\n\n\n- F1 score (F1): This is the harmonic mean of the precision and recall.  F1 = 2PR/(P + R)\n\n\n- specificity, selectivity or true negative rate (TNR): Effectiveness to identify negative labels (TN)/(FP + TN)\n(TN/TN+FP)\n\n\n-  Area under Curve (AUC): Ability to avoid false classiication\n\n\n- Receiver operating characteristic (ROC): Receiver operating characteristic curve is used to plot between true positive rate (TPR) and false positive rate (FPR), also known as a sensitivity and 1- specificity graph\n\n\n-  The Matthews correlation coefficient is used in machine learning as a measure of the quality of binary (two-class) classifications. It takes into account true and false positives and negatives and is generally regarded as a balanced measure which can be used even if the classes are of very different sizes. \n\n\n\n-  Cohen Kappa: a score that expresses the level of agreement between Observed Accuracy with an Expected Accuracy (random chance)\n\n\n-  Log loss, aka logistic loss or cross-entropy loss. This is the loss function used in (multinomial) logistic regression and extensions of it such as neural networks, defined as the negative log-likelihood of the true labels given a probabilistic classifier’s predictions.\n\n\n-  Zero-One Loss: return the fraction of misclassifications (float), else it returns the number of misclassifications (int). The best performance is 0.\n\n\n-  Hamming Loss: The Hamming loss is the fraction of labels that are incorrectly predicted.\n\n\n-  Hinge Loss: The cumulated hinge loss is  an upper bound of the number of mistakes made by the classifier.\n\n\n-  Brier Loss: measures the mean squared difference between the predicted probability assigned to the possible outcomes and (2) the actual outcome. \n\n\n\n"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"a68ec380a362be4a850495d097fde12246dabab7"},"cell_type":"code","source":"    classes = ['Dead','Survived']\n    cv = ShuffleSplit(n_splits = 100, test_size = 0.25, random_state = 0)\n    train_sizes = np.linspace(.1, 1.0, 10)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"ebe897dd346276f827f03e89620a1099490b626c"},"cell_type":"code","source":"def GridSearcher(model, GridParams, X, y):\n    print('Performing Grid Search...')\n    kfold = StratifiedKFold(n_splits = 10)\n    \n    model_GS = GridSearchCV(estimator = model, param_grid = GridParams, cv = kfold, scoring = \"accuracy\", n_jobs = 2, verbose = 1)\n\n    model_GS.fit(X, y['Survived'])\n    \n    model_best = model_GS.best_estimator_\n    model_best_score = model_GS.best_score_\n    model_best_params = model_GS.best_params_\n    \n    print(\"\\nBest Estimatot:\", model_GS.best_estimator_,\n          \"\\nBest Score:\", model_GS.best_score_, # Mean cross-validated score of the best_estimator\n          \"\\nBest parameters:\", model_GS.best_params_)\n          \n    return (model_best, model_best_score, model_best_params)\n    \ndef Confuse(y, y_pred, classes):\n    cnf_matrix1 = confusion_matrix(y, y_pred)\n    \n    cnf_matrix = cnf_matrix1.astype('float') / cnf_matrix1.sum(axis = 1)[:, np.newaxis] *100\n    c_train = pd.DataFrame(cnf_matrix, index = classes, columns = classes)  \n    plt.subplot(2, 3, 3)\n    ax = sns.heatmap(c_train, annot = True, cmap = cmap, square = True, cbar = False, \n                          fmt = '.2f', annot_kws = {\"size\": 20})\n    plt.title('Confusion Matrix (%)')\n    \n    return(ax, cnf_matrix1)\n\ndef FitModel(model, X, y):\n    print('Fitting Model...')\n    model.fit(X, y)\n    y_pred  = model.predict(X)\n    CV_score = round(np.median(cross_val_score(model, X, y, cv = cv)), 4) * 100\n    \n    return (model, y_pred, CV_score)\n\ndef LearningCurve(X, y, model, cv, train_sizes, title):\n    print('Evaluating Learning Curve...')\n    train_sizes, train_scores, val_scores = learning_curve(model, X, y, cv = cv, n_jobs = 4, \n                                                            train_sizes = train_sizes)\n\n    train_scores_mean = np.mean(train_scores, axis = 1)\n    train_scores_std   = np.std(train_scores, axis = 1)\n    val_scores_mean  = np.mean(val_scores, axis = 1)\n    val_scores_std   = np.std(val_scores, axis = 1)\n    \n    train_Error_mean = np.mean(1- train_scores, axis = 1)\n    train_Error_std  = np.std(1 - train_scores, axis = 1)\n    val_Error_mean  = np.mean(1 - val_scores, axis = 1)\n    val_Error_std   = np.std(1 - val_scores, axis = 1)\n\n    train_sc = train_scores_mean[-1] \n    val_sc = val_scores_mean[-1]\n    \n    train_sc_std = train_scores_std [-1]\n    val_sc_std = val_scores_std[-1]\n    \n    Learn_Results = [train_sc * 100, train_sc_std * 100, val_sc * 100, val_sc_std * 100]\n    \n    plt.figure(figsize = (20,15))\n    plt.subplot(2, 3, (1,2))\n    plt.fill_between(train_sizes, train_Error_mean - train_Error_std,\n                     train_Error_mean + train_Error_std, alpha = 0.1, color = \"r\")\n    plt.fill_between(train_sizes, val_Error_mean - val_Error_std, \n                     val_Error_mean + val_Error_std, alpha = 0.1, color = \"g\")\n    plt.plot(train_sizes, train_Error_mean, 'o-', color = \"r\",label = \"Training Error\")\n    plt.plot(train_sizes, val_Error_mean, 'o-', color = \"g\",label = \"Cross-validation Error\")\n    plt.xlabel('Training Examples (m)')\n    plt.title('Learning Curve %s'%(title))\n    plt.ylabel('Error')\n    plt.legend(loc = \"best\")\n    plt.grid(True)\n     \n    return (Learn_Results)\n    \ndef PlotPrecisionRecall(model, X, y):\n\n    # getting the probabilities of our predictions\n    y_scores = model.predict_proba(X) # probability estimates\n\n    \n    y_scores = y_scores[:,1]\n    \n    precision, recall, threshold = precision_recall_curve(y, y_scores)\n\n    plt.subplot(2,3,4)\n    plt.plot(threshold, precision[:-1], \"r-\", label=\"precision\", linewidth = 2)\n    plt.plot(threshold, recall[:-1], \"b\", label = \"Recall\", linewidth = 2)\n    plt.xlabel(\"Threshold\", fontsize = 19)\n    plt.ylabel(\"Precision or Recall\", fontsize = 19)\n    plt.title(\"Precision & Recall\", fontsize = 19)\n    plt.legend(loc = \"best\", fontsize = 19)\n    plt.ylim([0, 1])\n\n    plt.subplot(2,3,5)\n    plt.plot(recall[:-1], precision[:-1], color = \"r\", linewidth = 2)\n    plt.step(recall, precision, color = 'b', alpha = 0.2, where = 'post')\n    plt.fill_between(recall, precision, step = 'post', alpha = 0.2,\n                 color = 'b')\n    #plt.plot(threshold,  color = \"g\", label = \"recall\", linewidth = 2)\n    plt.title(\"Precision - Recall Curve\")\n    plt.xlabel(\"Recall\")\n    plt.ylabel(\"Precision\")\n    plt.ylim([0.0, 1])\n    plt.xlim([0.0, 1])\n    plt.legend(loc = \"best\")\n\ndef PlotROC(model, X, y):\n\n    print('Evaluating ROC Curve...')\n    tprs = []\n    aucs = []\n    mean_fpr = np.linspace(0, 1, 100)\n    random_state = np.random.RandomState(0)\n\n    i = 0\n    y = y['Survived']\n    \n    for train, test in cv.split(X,y):\n        prob = model.fit(X.iloc[train], y.iloc[train]).predict_proba(X.iloc[test])[:,1]\n        fpr, tpr, t = roc_curve(y[test], prob)\n        tprs.append(interp(mean_fpr, fpr, tpr))\n        roc_auc = auc(fpr, tpr)\n        aucs.append(roc_auc)\n        \n        i= i + 1\n        \n    plt.subplot(2, 3, 6)\n    plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',label='Luck', alpha=.8)\n    \n    mean_tpr = np.mean(tprs, axis=0)\n    mean_tpr[-1] = 1.0\n    mean_auc = auc(mean_fpr, mean_tpr)\n    std_auc = np.std(aucs)\n    plt.plot(mean_fpr, mean_tpr, color='b',\n         label = r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n         lw = 2, alpha=1)\n\n    std_tpr = np.std(tprs, axis=0)\n    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n    plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='red', alpha = .3,\n                 label=r'$\\pm$ 1 std. dev.')\n\n    plt.xlim([-0.0, 1.0])\n    plt.ylim([-0.0, 1.0])\n    plt.xlabel('False Positive Rate (FPR)')\n    plt.ylabel('True Positive Rate (TPR)')\n    plt.title('Receiver Operating Characteristic')\n    plt.legend(loc = \"best\")\n    plt.show()\n    \n    return()\n\ndef PlotKS(model, X, y):\n    y_probas = model.predict_proba(X)\n    \n    skplt.metrics.plot_ks_statistic(y, y_probas, title = 'Kolmogorov–Smirnov test', \n                                    text_fontsize = 13, title_fontsize = 13, figsize = [6,5])\n        \ndef PlotLift(model, X, y):\n    y_probas = model.predict_proba(X)\n    skplt.metrics.plot_lift_curve(y, y_probas,title = 'Lift Curve', \n                                  text_fontsize = 13, title_fontsize = 13, figsize = [6,5])\n\ndef PlotCumGain(model, X, y):\n    y_probas = model.predict_proba(X)\n    skplt.metrics.plot_cumulative_gain(y, y_probas, title = 'Cumulative Gain',\n                                       text_fontsize = 13, title_fontsize = 13, figsize = [6,5])\n\ndef PlotPR(model, X, y):\n\n    y_probas = model.predict_proba(X)\n    skplt.metrics.plot_precision_recall(y, y_probas,\n                                        text_fontsize = 13, title_fontsize = 13, figsize = [6,5])\n\ndef Classification_Analysis(model_best, title, title_abrv,X, X_test, y):\n    \n    #Fitting Model\n    (model_best, y_pred, CV_score) = FitModel(model_best, X, y)\n\n    y_train_pred = pd.Series(model_best.predict(X), name = title_abrv)\n    y_test_pred = pd.Series(model_best.predict(X_test), name = title_abrv)\n    \n    # Learning Curve Analysis\n    LearnResults = LearningCurve(X, y, model_best, cv, train_sizes, title)\n    #Confuson Matrix\n    Confuse_fig, cnf_matrix = Confuse(y, y_train_pred, classes)\n    #Precision - Recall Curve\n    PlotPrecisionRecall(model_best, X, y)\n    #plt scikit-plot\n    PlotROC(model_best, X, y)\n    PlotKS(model_best, X, y)\n    PlotLift(model_best, X, y)\n    PlotPR(model_best, X, y)\n    PlotCumGain(model_best, X, y)\n    \n    Summary = PrintResults(title, model_best,X_train,\n                          y, y_train_pred, CV_score, LearnResults, cnf_matrix)\n\n    return (Summary, y_train_pred, y_test_pred)\n\ndef TreeImportance():\n    nrows = ncols = 2\n    fig, axes = plt.subplots(nrows = nrows, ncols = ncols, sharex=\"all\", figsize=(15,15))\n\n    names_classifiers = [(\"AdaBoosting\", ada_best),(\"ExtraTrees\",ExtC_best),(\"RandomForest\",RFC_best),(\"GradientBoosting\",GBC_best)]\n\n    nclassifier = 0\n    for row in range(nrows):\n        for col in range(ncols):\n            name = names_classifiers[nclassifier][0]\n            classifier = names_classifiers[nclassifier][1]\n            indices = np.argsort(classifier.feature_importances_)[::-1][:40]\n            g = sns.barplot(y=X_train.columns[indices][:40],x = classifier.feature_importances_[indices][:40] , \n                            orient='h',ax=axes[row][col])\n            g.set_xlabel(\"Relative importance\",fontsize=12)\n            g.set_ylabel(\"Features\",fontsize=12)\n            g.tick_params(labelsize=9)\n            g.set_title(name + \" feature importance\")\n            nclassifier += 1\n            \n#def PrintResults(y, y_pred, Confuse_fig, learn_fig, CV_score, Score_mean, Scores_std):\ndef PrintResults(title, model,X, y, y_pred, CV_score, LearnResults, cnf_matrix ):\n    y_scores = model.predict_proba(X)[:, 1]\n    \n    precision = precision_score(y, y_pred, average = 'macro') * 100\n    recall = recall_score(y, y_pred,average = 'macro') * 100\n    f1score = f1_score(y, y_pred,average = 'macro') * 100\n    Accuracy = accuracy_score(y, y_pred)\n    MCC = matthews_corrcoef(y, y_pred) \n    Lg_loss = log_loss(y, y_pred)\n    Zero_one_loss= zero_one_loss(y, y_pred, normalize = False)\n    Hinge = hinge_loss(y, y_pred) \n    Cohen_kappa = cohen_kappa_score(y, y_pred) \n    Hamming = hamming_loss(y, y_pred)\n    AUC = roc_auc_score(y, y_scores)\n    Brier = brier_score_loss(y, y_scores )\n    \n    Population = np.sum(cnf_matrix)  \n    PP = np.sum(y == 1)\n    NP = np.sum(y == 0)\n    PP_t = np.sum(cnf_matrix[:,1])\n    NP_t = np.sum(cnf_matrix[:,0])\n    TP = cnf_matrix[1,1]\n    \n    TN = cnf_matrix[0,0]\n    FP = cnf_matrix[0,1]\n    FN = cnf_matrix[1,0]\n    \n    TPR = TP/(TP + FN)\n    TNR = TN/(TN + FP)\n    FPR = FP/(FP + TN)\n    FNR = FN/(FN + TP)\n    \n    P_sum = np.sum(cnf_matrix[:,1])\n    N_sum = np.sum(cnf_matrix[:,0])\n    PPV = TP/P_sum\n    NPV = TN/N_sum\n       \n    FDR = FP/P_sum\n    FOR = FN/N_sum\n    \n    Acc = (TP + TN)/(P_sum + N_sum)\n    F1_scr = (2 * TP) / (2*TP + FP + FN)\n    MCC = ((TP * TN) - (FP * FN))/np.sqrt(P_sum * N_sum * (TP + FN) * (TN + FP))\n    \n    BM = TPR + TNR - 1\n    MK = PPV + NPV - 1 \n    \n    LR_minus = FNR/TNR\n    LR_plus = TPR/FPR\n    DOR = LR_plus/LR_minus\n    \n    \n    print('\\nRESULTS: %s'%(title.upper()),'CLASSIFIER')\n    print('--------------------------------')\n    print('\\n Model Settings: %s'%(title.upper()),'CLASSIFIER')\n    print('\\t %s'%(model))\n    print('--------------------------------')\n    print('\\nLearning Curve Results %s'%(title.upper()),'CLASSIFIER')\n    print('\\tTraining')\n    print('\\t\\tScore: %.2f %%'%(LearnResults[0]))\n    print('\\t\\tStdv: %.2f %%'%(LearnResults[1]))\n    print('\\tValidation')\n    print('\\t\\tScore: %.2f%%'%(LearnResults[2]))\n    print('\\t\\tStdv: %.2f %%'%(LearnResults[3]))\n    print('-------------------------------------------------------')\n    print('\\nFull Fitting Results %s'%(title.upper()),'CLASSIFIER')\n    print('\\tAccuracy Score: %.2f %%'%(Accuracy*100))\n    print('\\tCross-Validation Score: %.2f %%'%(CV_score))\n    print(\"\\tPrecision: %.2f %%\"%(precision))\n    print(\"\\tRecall: %.2f %%\"%(recall))\n    print('\\tf1-score: %.2f %%'%(f1score))\n    print('\\tC-Statistic or (AUC-ROC): %.2f %%'%(AUC * 100))\n    print('\\tCohens Kappa: %.2f %%'%(Cohen_kappa * 100))\n    print('\\tKolmogorov–Smirnov (KS) Statistic: %.2f'%(00))\n    \n    print('\\nLosses: %s'%(title.upper()),'CLASSIFIER')\n    print('\\tLog Loss: %.2f'%(Lg_loss))\n    print('\\tZero-One-Loss: %.2f'%(Zero_one_loss))\n    print('\\tHamming Loss: %.2f'%(Hamming))\n    print('\\tBrier Loss: %.2f'%(Brier))\n    print('\\tHinge Loss: %.2f'%(Hinge))    \n    print('-------------------------------------------------------')\n    print('\\nConfusion Matrix: %s'%(title.upper()),'CLASSIFIER')\n    print('\\nClassification Report (weigthed results):')\n    print(classification_report(y, y_pred, digits = 4)) \n    print(' \\n\\t\\t\\t\\t\\tCounts \\t Percentage')\n    print('\\tPopulation: \\t\\t\\t%.0f'%(Population))\n    print('\\tPositive Population (P): \\t%.0f \\t%.2f %% (Prevalence)'%(PP,PP/Population * 100))     \n    print('\\tNegative Population (N): \\t%.0f \\t%.2f %%'%(NP, NP/Population * 100))\n    print('\\n')\n    print('\\tPositive Population (P_test): \\t%.0f \\t%.2f %%'%(PP_t,PP_t/Population * 100))     \n    print('\\tNegative Population (N_test): \\t%.0f \\t%.2f %%'%(NP_t, NP_t/Population * 100))\n    print('\\n')\n    print('\\tTrue Positive (TP):  \\t\\t%.0f \\t %.2f %% (Sensitivity / Recall / Hit Rate/ True Positive Rate (TPR))'%(TP,TPR * 100))\n    print('\\tTrue Negative (TN):  \\t\\t%.0f \\t %.2f %% (Specificity / Selectivity / True Negative Rate (TNR))'%(TN, TNR * 100))\n    print('\\tFalse Positive (FP): \\t\\t%.0f \\t %.2f %% (Fall-Out / False Positive Rate (FPR))'%(FP,FPR * 100))\n    print('\\tFalse Negative (FN): \\t\\t%.0f \\t %.2f %% (Miss Rate / False Negative Rate (FNR))'%(FN,FNR * 100))\n    print('\\n')\n    print('\\tPositive Predictive Value (PPV): \\t%.2f %% (Precision)'%(PPV * 100))\n    print('\\tNegative Predictive Value (NPV): \\t%.2f %%'%(NPV * 100))\n    print('\\n')\n    print('\\tFalse Discovery Rate(FDR): \\t\\t%.2f %%'%(FDR * 100))\n    print('\\tFalse Omission Rate (FOR): \\t\\t%.2f %%'%(FOR * 100))\n    print('\\n')\n    print('\\tAccuracy (Acc): \\t\\t\\t%.2f %%'%(Acc * 100))\n    print('\\tF1-Score (F1): \\t\\t\\t\\t%.2f %%'%(F1_scr * 100))\n    print('\\tMathews Correlation Coefficient (MCC): \\t%.2f %%'%(MCC * 100))\n    print('\\n')\n    print('\\tBookmaker Informedness (BM): \\t\\t%.2f %%'%(BM * 100))\n    print('\\tMarkedness (Acc): \\t\\t\\t%.2f %%'%(MK * 100))\n    print('\\n')\n    print('\\tNegative Likelihood Ratio(LR_minus): \\t%.2f '%(LR_minus))\n    print('\\tPositive Likelihood Ratio (LR_plus): \\t%.2f '%(LR_plus))\n    print('\\tDiagnostic Odds Ratio (DOR): \\t\\t%.2f'%(DOR))\n    \n    Summary = pd.DataFrame({\n                    'Model': title,\n                    'Accuracy': Accuracy,\n                    'CV Score': CV_score,\n                    'Precision': precision, \n                    'Recall': recall, \n                    'F1-Score': f1score,\n                    'Train Score': LearnResults[0],\n                    'Train Stdv': LearnResults[1],   \n                    'Val Score': LearnResults[2],\n                    'Val std': LearnResults[3],\n                    'ROC AUC':  AUC * 100, \n                    'MCC': MCC,\n                    'Cohens Kappa':Cohen_kappa},index = [0])\n\n    return (Summary)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"555b218372b7e034541c260656a160b48ca9f691"},"cell_type":"markdown","source":"### Logistic Regresion"},{"metadata":{"_uuid":"2f9e6ae9d599f203da258671fa8318e227b7cad7"},"cell_type":"markdown","source":"Wikipedia: Logistic Regression is a statistical model that is usually taken to apply to a binary dependent variable. More formally, a logistic model is one where the log-odds of the probability of an event is a linear combination of independent or predictor variables. The two possible dependent variable values are often labelled as \"0\" and \"1\", which represent outcomes such as pass/fail, win/lose, Survive/dead or healthy/sick. The binary logistic regression model can be generalized to more than two levels of the dependent variable: categorical outputs with more than two values are modelled by multinomial logistic regression, and if the multiple categories are ordered, by ordinal logistic regression, for example the proportional odds ordinal logistic model. Logistic regression has a high bias and a low variance error."},{"metadata":{"scrolled":false,"trusted":false,"_uuid":"7af1aaf64265902e1ce63f291822500d190fea83"},"cell_type":"code","source":"#Logistic Regresion\ntitle = 'Logistic Regression'\ntitle_abrv = 'LR'\n\nmodel = LogisticRegression()\n\nLR_GS_Params = {'penalty': ['l1', 'l2'],\n                 'C': np.logspace(0, 10, 10)}\n\n# the grid search was run and the resutls are shown below... for now it is commented so not repet the s \n#(model_best_LR, model_best_score, model_best_params) = GridSearcher(model, LR_GS_Params, X_train, y_train)\n\n#----------------------------------------------------------------------------------------------------\n#                                   These are the results of the GridSearch\n#----------------------------------------------------------------------------------------------------\n#Best Estimatot: LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n#          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=-1,\n#          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n#          verbose=0, warm_start=False) \n#----------------------------------------------------------------------------------------------------\n\nmodel_best_LR = LogisticRegression(C = 1.0, class_weight = None, dual = False, fit_intercept = True,\n          intercept_scaling = 1, max_iter = 500, multi_class='ovr', n_jobs = -1,\n          penalty = 'l2', random_state = None, solver ='liblinear', tol = 0.0001,\n          verbose=0, warm_start = False) \n\nSummary_LR, y_train_LR, y_test_LR = Classification_Analysis(model_best_LR, title,title_abrv, \n                                                       X_train, X_test, y_train);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0394fa8f1e2324baf3d68828a37de1a4f4e71896"},"cell_type":"markdown","source":"### Support Vector machine"},{"metadata":{"scrolled":false,"trusted":false,"_uuid":"8c67a0bbe9a8bc196cce5216d9d2e3d7ac9aacf7"},"cell_type":"code","source":"#Support Vector Maachine\ntitle = 'Support Vector Machine'\ntitle_abrv = 'SVM'\nmodel = SVC(probability = True)\n\nSVM_GS_Params = {'kernel': ['rbf'], \n                  'gamma': [0.0008, 0.005],\n                  'C': [1, 50, 100, 110,125],\n                  'decision_function_shape':('ovo','ovr'),\n                 'shrinking':(True, False)}\n\n\n# the grid search was run and the resutls are shown below... for now it is commented so not repet the s \n#(model_best_SVM, model_best_score, model_best_params) = GridSearcher(model, SVM_GS_Params, X_train, y_train)\n#----------------------------------------------------------------------------------------------------\n#                                   These are the results of the GridSearch\n#----------------------------------------------------------------------------------------------------\n\n#Best Estimatot: SVC(C=125, cache_size=200, class_weight=None, coef0=0.0,\n#  decision_function_shape='ovo', degree=3, gamma=0.0008, kernel='rbf',\n#  max_iter=-1, probability=False, random_state=None, shrinking=True,\n#  tol=0.001, verbose=False) \n#Best Score: 0.8215488215488216 \n#Best parameters: {'C': 125, 'decision_function_shape': 'ovo', 'gamma': 0.0008, 'kernel': 'rbf', 'shrinking': True}\n\n#----------------------------------------------------------------------------------------------------\nmodel_best_SVM = SVC(C = 125, cache_size = 200, class_weight = None, coef0 = 0.0,\n  decision_function_shape='ovo', degree = 3, gamma = 0.0008, kernel = 'rbf',\n  max_iter = -1, probability = True, random_state = None, shrinking = True,\n  tol = 0.001, verbose = False) \n\nSummary_SVM, y_train_SVM, y_test_SVM = Classification_Analysis(model_best_SVM, title,title_abrv, \n                                                               X_train, X_test, y_train);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d1c7962a136491985c4779bfb37a56c5f91fd8f1"},"cell_type":"markdown","source":"### Random Forest"},{"metadata":{"_uuid":"02d48b42300f0fae53434e62574689367a689aba"},"cell_type":"markdown","source":"Random forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees"},{"metadata":{"scrolled":false,"trusted":false,"_uuid":"13a0b1e08abb315a83f5fb54783aaac05a47ebba"},"cell_type":"code","source":"# Random Forest\ntitle = 'Random Forest'\ntitle_abrv = 'RF'\nmodel = RandomForestClassifier()\n\nRF_GS_Params = {\"max_depth\": [None],\n              \"max_features\": [4,5,6,7],\n              \"min_samples_split\": [3,4,5],\n              \"min_samples_leaf\": [3, 4,5],\n              \"n_estimators\" :[250, 300, 300]}\n\n# the grid search was run and the resutls are shown below... for now it is commented so not repet the s \n#(model_best_RF, model_best_score, model_best_params) = GridSearcher(model, RF_GS_Params, X_train, y_train)\n\n#----------------------------------------------------------------------------------------------------\n#                                   These are the results of the GridSearch\n#----------------------------------------------------------------------------------------------------\n#Best Estimatot: RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n#            max_depth=None, max_features=5, max_leaf_nodes=None,\n#            min_impurity_decrease=0.0, min_impurity_split=None,\n#            min_samples_leaf=4, min_samples_split=4,\n#            min_weight_fraction_leaf=0.0, n_estimators=300, n_jobs=1,\n#            oob_score=False, random_state=None, verbose=0,\n#            warm_start=False) \n#Best Score: 0.8462401795735129 \n#Best parameters: {'max_depth': None, 'max_features': 5, 'min_samples_leaf': 4, 'min_samples_split': 4, 'n_estimators': 300} \n#----------------------------------------------------------------------------------------------------\nmodel_best_RF = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=None, max_features=5, max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=4, min_samples_split=4,\n            min_weight_fraction_leaf=0.0, n_estimators=300, n_jobs=1,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False)\n\nSummary_RF, y_train_RF, y_test_RF = Classification_Analysis(model_best_RF, title,title_abrv, \n                                                            X_train, X_test, y_train);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"78f317c0072684f41f76969d2a27fcfc8c7ce5f6"},"cell_type":"markdown","source":"### Extra Tree"},{"metadata":{"scrolled":false,"trusted":false,"_uuid":"ab6f61ac82ac32e2aabffc62b5684402bdc36467"},"cell_type":"code","source":"#Extra Tree\n\ntitle = 'Extra Tree'\ntitle_abrv = 'ET'\nmodel = ExtraTreesClassifier()\n\n## Search grid for optimal parameters\nET_param_grid = {\"max_depth\": [None],\n              \"max_features\": [7,8,9,10],\n              \"min_samples_split\": [13,14],\n              \"min_samples_leaf\": [1],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[ 600, 700, 800],\n              \"criterion\": [\"gini\"]}\n\n#(model_best_ET, model_best_score, model_best_params) = GridSearcher(model, ET_param_grid, X_train, y_train)\n\n#----------------------------------------------------------------------------------------------------\n#                                   These are the results of the GridSearch\n#----------------------------------------------------------------------------------------------------\n#Best Estimatot: ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n#           max_depth=None, max_features=9, max_leaf_nodes=None,\n#          min_impurity_decrease=0.0, min_impurity_split=None,\n#           min_samples_leaf=1, min_samples_split=13,\n#           min_weight_fraction_leaf=0.0, n_estimators=600, n_jobs=1,\n#           oob_score=False, random_state=None, verbose=0, warm_start=False) \n#Best Score: 0.8462401795735129 \n#Best parameters: {'bootstrap': False, 'criterion': 'gini', 'max_depth': None, 'max_features': 9, 'min_samples_leaf': 1, 'min_samples_split': 13, 'n_estimators': 600}\n#---------------------------------------------------------------------\n\nmodel_best_ET = ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n           max_depth=None, max_features=9, max_leaf_nodes=None,\n           min_impurity_decrease=0.0, min_impurity_split=None,\n           min_samples_leaf=1, min_samples_split=13,\n           min_weight_fraction_leaf=0.0, n_estimators=600, n_jobs=1,\n           oob_score=False, random_state=None, verbose=0, warm_start=False) \n\nSummary_ET, y_train_ET, y_test_ET = Classification_Analysis(model_best_ET, title, title_abrv, X_train, X_test, y_train);\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6ce2b3f5da2ab1836825931267a97a14a28bd7f8"},"cell_type":"markdown","source":"### Gradient Boosting"},{"metadata":{"_uuid":"dd31423a3ef778fb3b956b66dbf6ed04f54c0ba5"},"cell_type":"markdown","source":"Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. It builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function."},{"metadata":{"scrolled":false,"trusted":false,"_uuid":"f4b8e5b664f87f3a0da3c2d4c27236d863dcd031"},"cell_type":"code","source":"# Gradient Boosting\n\nmodel = GradientBoostingClassifier()\n\ntitle = 'Gradient Boosting'\ntitle_abrv = 'GB'\n\n#GB_param_grid = {'loss' : [\"deviance\"],\n#              'n_estimators' : [100,200,300,500],\n#              'learning_rate': [0.1, 0.05, 0.01,],\n#              'max_depth': [4,6, 8],\n#              'min_samples_leaf': [50,100,150],\n#              'max_features': [0.1, 0.3, 0.5] \n#              }\n\nGB_param_grid = {'loss' : [\"deviance\"],\n              'n_estimators' : [400,450],\n              'learning_rate': [0.1],\n              'max_depth': [8],\n              'min_samples_leaf': [50],\n              'max_features': [0.01, 0.02, 0.05] \n              }\n\n#(model_best_GB, model_best_score, model_best_params) = GridSearcher(model, GB_param_grid, X_train, y_train)\n\n#----------------------------------------------------------------------------------------------------\n#                                   These are the results of the GridSearch\n#----------------------------------------------------------------------------------------------------\n#Best Estimatot: GradientBoostingClassifier(criterion='friedman_mse', init=None,\n#              learning_rate=0.1, loss='deviance', max_depth=8,\n#              max_features=0.02, max_leaf_nodes=None,\n#              min_impurity_decrease=0.0, min_impurity_split=None,\n#              min_samples_leaf=50, min_samples_split=2,\n#              min_weight_fraction_leaf=0.0, n_estimators=400,\n#              presort='auto', random_state=None, subsample=1.0, verbose=0,\n#              warm_start=False) \n#Best Score: 0.8406285072951739 \n#Best parameters: {'learning_rate': 0.1, 'loss': 'deviance', 'max_depth': 8, 'max_features': 0.02, 'min_samples_leaf': 50, 'n_estimators': 400}\n\n#----------------------------------------------------------------------------------------------------\n\nmodel_best_GB = GradientBoostingClassifier(criterion='friedman_mse', init=None,\n              learning_rate=0.1, loss='deviance', max_depth=8,\n              max_features=0.02, max_leaf_nodes=None,\n              min_impurity_decrease=0.0, min_impurity_split=None,\n              min_samples_leaf=50, min_samples_split=2,\n              min_weight_fraction_leaf=0.0, n_estimators=400,\n              presort='auto', random_state=None, subsample=1.0, verbose=0,\n              warm_start=False)\n\nSummary_GB, y_train_GB, y_test_GB = Classification_Analysis(model_best_GB, title,title_abrv, X_train, X_test, y_train);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"66b97201ed33f2989b426233f365f4d1714ee247"},"cell_type":"markdown","source":"### KNN"},{"metadata":{"_uuid":"e8ed4e6e0a8a7592dee56c18289a836461e89e71"},"cell_type":"markdown","source":"In pattern recognition, the k-nearest neighbors algorithm (k-NN) is a non-parametric method used for classification and regression.[1] In both cases, the input consists of the k closest training examples in the feature space. The output depends on whether k-NN is used for classification or regression:\n\nAn object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor."},{"metadata":{"scrolled":false,"trusted":false,"_uuid":"20799e3b09d7811eee81c3fd17ffee9f89ae9bec"},"cell_type":"code","source":"# KNN\ntitle = 'K-Nearest Neighbour'\ntitle_abrv = 'KNN'\nmodel = KNeighborsClassifier()\n\nKNN_param_grid = {'algorithm': ['auto'], 'n_neighbors': [1, 2, 3],\n                 'leaf_size':[1,2,3,4,5,7],\n                 'weights': ['uniform', 'distance']}\n\n#(model_best_KNN, model_best_score, model_best_params) = GridSearcher(model, KNN_param_grid, X_train, y_train)\n#----------------------------------------------------------------------------------------------------\n#                                   These are the results of the GridSearch\n#----------------------------------------------------------------------------------------------------\n#Best Estimatot: KNeighborsClassifier(algorithm='auto', leaf_size=1, metric='minkowski',\n#           metric_params=None, n_jobs=1, n_neighbors=2, p=2,\n#           weights='uniform') \n#Best Score: 0.7822671156004489 \n#Best parameters: {'algorithm': 'auto', 'leaf_size': 1, 'n_neighbors': 2, 'weights': 'uniform'}\n#----------------------------------------------------------------------------------------------------\n\nmodel_best_KNN = KNeighborsClassifier(algorithm='auto', leaf_size=1, metric='minkowski',\n           metric_params=None, n_jobs=1, n_neighbors=2, p=2,\n           weights='uniform') \n\nSummary_KNN, y_train_KNN, y_test_KNN = Classification_Analysis(model_best_KNN, title,title_abrv, X_train, X_test, y_train);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0dec7ce1055c0cb53b660a13be08b6334e6beed4"},"cell_type":"markdown","source":"### Gaussian Naive Bayes"},{"metadata":{"_uuid":"419a3537afa690a767eaa628b9c65e83afb23ed7"},"cell_type":"markdown","source":""},{"metadata":{"trusted":false,"_uuid":"1ffdd6d2b19a14f835b5902ee58853d55979d4cd"},"cell_type":"code","source":"# Gaussian Naive Bayes\ntitle = 'Gaussian Naive Bayes'\ntitle_abrv = 'GNB'\nmodel_best_GNB = GaussianNB()\nSummary_GNB, y_train_GNB, y_test_GNB = Classification_Analysis(model_best_GNB, title, title_abrv, \n                                                               X_train, X_test, y_train);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fe9084ab8c4e7f193ec0037410003d4a4c4d1f9b"},"cell_type":"markdown","source":"### Decision Tree"},{"metadata":{"_uuid":"3d0cd5dcf2856bf4ed07b71783b47a621fccba6f"},"cell_type":"markdown","source":"A decision tree is a decision support tool that uses a tree-like graph or model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. It is one way to display an algorithm that only contains conditional control statements."},{"metadata":{"trusted":false,"_uuid":"ea8763f9bcdac82a803511f33cf67cd40be7f56f"},"cell_type":"code","source":"# Decision Tree\n\ntitle = 'Decision Tree'\ntitle_abrv = 'DT'\nmodel = DecisionTreeClassifier()\n\nDT_param_grid = {'max_depth': [1, 2, 3, 4, 5],\n                  'max_features': [ 4,6, 10,11,12],\n                 'min_samples_split': [2,4,5]\n                }\n        \n#(model_best_DT, model_best_score, model_best_params) = GridSearcher(model, DT_param_grid, X_train, y_train)\n\n#----------------------------------------------------------------------------------------------------\n#                                   These are the results of the GridSearch\n#----------------------------------------------------------------------------------------------------    \n#Best Estimatot: DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4, max_features=12, \n#    max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, \n#     min_samples_split=2, min_weight_fraction_leaf=0.0, presort=False, random_state=None, splitter='best') \n\n#Best Score: 0.8327721661054994 \n#----------------------------------------------------------------------------------------------------   \nmodel_best_DT =    DecisionTreeClassifier(class_weight = None, criterion = 'gini', max_depth = 4,\n            max_features = 12, max_leaf_nodes = None,\n            min_impurity_decrease = 0.0, min_impurity_split=None,\n            min_samples_leaf = 1, min_samples_split = 2,\n            min_weight_fraction_leaf = 0.0, presort = False, random_state = None,\n            splitter = 'best')\n    \nSummary_DT, y_train_DT, y_test_DT = Classification_Analysis(model_best_DT, title, title_abrv, X_train, X_test, y_train);   ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"41142208df4946ccc3ccf659ece18512d1a59e7a"},"cell_type":"markdown","source":"### AdaBoost with Decision Tree Classifier"},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"a7e8a7af6cba7260be0099f48413e7a48bd70cee"},"cell_type":"code","source":"#AdaBoost with Decision Tree Classifier\n\ntitle = 'AdaBoost - Decision Tree'\ntitle_abrv = 'ABDT'\nmodel = AdaBoostClassifier(model_best_DT, random_state=7)\n\nABDT_param_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n              \"algorithm\" : [\"SAMME\",\"SAMME.R\"],\n              \"n_estimators\" :[1,2],\n              \"learning_rate\":  [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3,1.5]}\n\n\n#(model_best_ABDT, model_best_score, model_best_params) = GridSearcher(model, ABDT_param_grid, X_train, y_train)\n \n#----------------------------------------------------------------------------------------------------\n#                                   These are the results of the GridSearch\n#----------------------------------------------------------------------------------------------------    \n#Best Estimatot: AdaBoostClassifier(algorithm='SAMME.R',\n#   base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4, max_features=12, max_leaf_nodes=None,\n#    min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, \n#    presort=False, random_state=None, splitter='random'), learning_rate=0.001, n_estimators=2, random_state=7) \n\n#Best Score: 0.8226711560044894 \n#---------------------------------------------------------------------------------------------------- \n    \nmodel_best_ABDT =    AdaBoostClassifier(algorithm = 'SAMME.R',\n    base_estimator = DecisionTreeClassifier(class_weight = None, criterion = 'gini', max_depth = 4,\n    max_features = 12, max_leaf_nodes = None, min_impurity_decrease = 0.0, min_impurity_split = None,\n    min_samples_leaf = 1, min_samples_split = 2, min_weight_fraction_leaf = 0.0, presort = False, \n    random_state = None, splitter = 'random'), learning_rate = 0.001, n_estimators = 2, random_state = 7)\n  \nSummary_ABDT, y_train_ABDT, y_test_ABDT = Classification_Analysis(model_best_ABDT, title, title_abrv, X_train, X_test, y_train);    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cd626c1f4eba8ad7a610119d2edd83f810a12fb2"},"cell_type":"markdown","source":"### Compare Classification Models"},{"metadata":{"trusted":false,"_uuid":"2781acb0bc690f8069304311ab65a6b49839105f"},"cell_type":"code","source":"#Which is the best Model ?\n\nClass_Results = pd.concat([Summary_LR, Summary_SVM, Summary_RF, Summary_GB, Summary_KNN, Summary_ET,\n                          Summary_DT, Summary_ABDT, Summary_GNB], ignore_index = True)\n    \n    \nClass_Results = Class_Results.sort_values(by = 'CV Score', ascending=False)\n#Class_Results = Class_Results.set_index('CV Score')\nClass_Results.head(12)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a5196875aed5c75cc229417d0fff1f41db0d4fad"},"cell_type":"code","source":"g = sns.barplot(Class_Results[\"CV Score\"],Class_Results[\"Model\"],data = Class_Results, \n                palette = \"muted\",orient = \"h\",**{'xerr': Class_Results['Val std']})\ng.set_xlabel(\"Cross Validation Score\")\ng = g.set_title(\"Cross validation scores\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0e659cdaede4e8ae5e0648c38e70bed2192f53ec"},"cell_type":"markdown","source":"RESULTS IN KAGGLE\n\nGB = 74.%\n\nRF = 78.%\n\nET = 78.46%\n\nDT = 76.55%\n\nLR = 77%"},{"metadata":{"_uuid":"8f56548d6a71e39d88a2c7ee87465833391265c1"},"cell_type":"markdown","source":"### Compare Predictions by Classifiers"},{"metadata":{"trusted":false,"_uuid":"918487800bc79e5aea45d3cf7bff191b0b1132ee"},"cell_type":"code","source":"# Concatenate all classifier results\ny_test_Results = pd.concat([y_test_LR, y_test_SVM, y_test_RF, y_test_ET, y_test_GB, y_test_KNN, y_test_GNB,\n                              y_test_DT, y_test_ABDT], axis = 1)\n\ny_train_Results = pd.concat([y_train_LR, y_test_SVM, y_train_RF, y_train_ET, y_test_GB, y_train_KNN, y_train_GNB,\n                               y_train_DT, y_train_ABDT], axis = 1)\n\n\nplt.figure(figsize = (14, 7))\nplt.subplot(1,2,1)\nPlotCorr(y_train_Results)\nplt.title('Training data')\nplt.subplot(1,2,2)\nPlotCorr(y_test_Results)\nplt.title('Test data')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"71ea8d22209f8fbf921159d5eb37e32181d14a3e"},"cell_type":"markdown","source":"### Feature importance of trees"},{"metadata":{"trusted":false,"_uuid":"fe0edb82554669e8f902efbaf6443c6f9f1fa947"},"cell_type":"code","source":"nrows = ncols = 2\nfig, axes = plt.subplots(nrows = nrows, ncols = ncols, sharex=\"all\", figsize=(15,15))\n\nnames_classifiers = [(\"AdaBoosting\", model_best_ABDT),\n                     (\"Extra Trees\", model_best_ET),\n                     (\"RandomForest\",model_best_RF),\n                     (\"GradientBoosting\",model_best_GB)]\n\nnclassifier = 0\nfor row in range(nrows):\n    for col in range(ncols):\n        \n        name = names_classifiers[nclassifier][0]\n        classifier = names_classifiers[nclassifier][1]\n        indices = np.argsort(classifier.feature_importances_)[::-1][:40]\n        g = sns.barplot(y=X_train.columns[indices][:40],x = classifier.feature_importances_[indices][:40] , palette = 'muted', orient='h',ax=axes[row][col])\n        g.set_xlabel(\"Relative importance\")\n        g.set_ylabel(\"Features\")\n        g.set_title(name)\n        \n        nclassifier += 1\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1845500922c883292221196913cb9685032232ff"},"cell_type":"markdown","source":"### Voting"},{"metadata":{"trusted":false,"_uuid":"59d830df6a5e9c9519a16f6bdd01c9e92e908198"},"cell_type":"code","source":"Voting = VotingClassifier(estimators = [('RF', model_best_RF),\n                                      ('SVM', model_best_SVM),\n                                      ('ET', model_best_ET),\n                                      ('GB',model_best_GB),\n                                      #('LR',model_best_LR),\n                                      ('KNN',model_best_KNN),\n                                      ('GNB',model_best_GNB),\n                                      ('DT',model_best_DT),\n                                      ('ABDT',model_best_ABDT)], voting='soft', n_jobs = 2)\n\nVoting = Voting.fit(X_train, y_train)\n\ny_test_V = pd.Series(Voting.predict(X_test), name = \"V\")\n\n#Voting = 78.468% = highest  in Kaggle","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"8397d865762fa58387d7c35b5c2866f037966c3a"},"cell_type":"markdown","source":"# Prepare Submission Data"},{"metadata":{"trusted":false,"_uuid":"1da65b3112ce94f50fe9cec4040ba35953a77ee6"},"cell_type":"code","source":"submission = pd.DataFrame({\n        \"PassengerId\": X_test_original[\"PassengerId\"],\n        \"Survived\": y_test_V\n    })\nsubmission.to_csv('Titanic Submission Voting2.csv', index = False)\n\nprint('Done')","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"716414f6243275b59c77e6b1ffff8ad4a91a35d5"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"b853b24a050b5f350c3bf126952934ab40da62a5"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"f16a17dd9dacb00ec2fbca007358031dce5be13d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}