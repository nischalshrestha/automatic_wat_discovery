{"cells":[{"metadata":{"_uuid":"a9f86d4db45d64851bae9fa62a6d67a61cb4771a"},"cell_type":"markdown","source":"**<H1>Titanic Dataset Analysis: Visualization and Prediction<H1>**\n\n![Titanic](http://https://goo.gl/images/9kGf1u)\n\n**Titanic dataset** is a very famous set for the beginners. Also, this is my first kernel at kaggle. I would like to start from a easy dataset and try to make a **comprehensive analysis** which will contain **data visualization**, **data refromation**, **supervised learning methods**, **unsupervised learning method**, even **simple deep learning model**. For each method, I am try to use **raw code** to introduce the basic idea. Then, I use exisiting **functions** from \"sklearn\" or others to achieve easy implementation. Also, I try to **tune the model parameters** to achieve better preformance.<br>\n\nApologize my poor Engilish and let's start~! "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nfrom numpy import *\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # data visulization\n%matplotlib inline\n\nimport seaborn as sns # data visulization\nimport missingno as msno # missing data visualization\nimport math # Calcuation \nfrom math import log\n\nimport operator # Operation\nimport sys\n#import treePlotter # Visualization tool for decision tree\nfrom time import time # time info\n\nfrom sklearn.cross_validation import train_test_split # dataset split\n\nfrom sklearn import neighbors\nfrom sklearn.neighbors import KNeighborsClassifier # KNN\nfrom sklearn.ensemble import BaggingClassifier # Bagging\nfrom sklearn.tree import DecisionTreeClassifier # Decision Tree\nfrom sklearn.ensemble import RandomForestClassifier # Random Forest\nfrom sklearn import linear_model \nfrom sklearn.linear_model import LogisticRegression # Logistic Regression\nfrom sklearn.linear_model import Perceptron # Perceptron\nfrom sklearn.linear_model import SGDClassifier # Stochastic Gradient Descent \nfrom sklearn.svm import SVC, LinearSVC # Support Vector Machine (Normal, linear)\nfrom sklearn.naive_bayes import GaussianNB # Naive Bayes\n\nfrom sklearn.cluster import KMeans # K-means\n\nfrom sklearn.neural_network import MLPClassifier # Multiple Layers Perceptron\n\nfrom sklearn.metrics import accuracy_score # Accuracy Calculation\nfrom sklearn.metrics import precision_score, recall_score # calculate precision and recall\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\n\nimport tensorflow as tf # Deep Learning Library\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\n\nnp.random.seed(2)\n\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ff0e49ed414c9eac88130f6034a2cbfa18ad8b41"},"cell_type":"markdown","source":"First of the first, the dataset is **loaded**.<br>\nWe can check each data and their discription.<br>\nMoreover, **the missing records** can be viusalized."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"scrolled":false},"cell_type":"code","source":"train = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\n\n#print (train)\n#print (test)\n#print (train.head())\n#train.describe(include=\"all\")\n#train.isnull().any()\n#train.isnull().sum()\nmsno.matrix(train,figsize=(12,5))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b3134a9ee413d188ab645cc5a9ebdaaf57f595be"},"cell_type":"markdown","source":"**<H1>Data Visualization<H1>**\n\nThis is the first step I would like to do. It will help me to understand the data better.\n\n**Totally we have 12 labels:** (Most of them are discrbed in Kaggle data info. I just add some my thoughts)\n\n* PassengerID: I feel that this is just ID number, do not relate with analysis, will throw it;\n\n* Survival: It can be considered as results:  \n0 = No; 1 = Yes;\n\n* Pclass:\tTicket class, it can indicate the SES as mentioned in the data discription.\n1 = 1st(Upper); 2 = 2nd(Middle); 3 = 3rd(Lower);\n\n* Name: Passenger name. Do not relate with analysis, will throw it;\n\n* Sex:\tSex. I think this lbel is enssential since the women and child have a high priority for the lifeboat boarding;\t\n\n* Age:\tAge in years. I think this lbel is enssential since the women and child have a high priority for the lifeboat boarding. the problem with this label isthat, 177 records are missing;\n\n* SibSp: This dataset defines family relations. \nSibling means brother, sister, stepbrother, stepsister; Spouse means husband, wife(mistresses and fiances were ignored);\n\n* Parch: This dataset defines family relations as well, but different from the label \"SibSp\". \nParent means father and mother; Child means daughter, son, stepdaughter, stepson. if children travelled only with a nanny, therefore parch=0 for them;\n\n* Ticket: I feel that this is just number, do not relate with analysis, will throw it;\n\n* Fare: Passenger Fare. I do not comfirm this label is usefull or not. in my opinion, this label is related with passenger class. Normally, higher class with higher fare. So, in the next seesions, I will check the relationship between passenger class and fare. if they are positive correlation, I think this bale can be thrown as well;\n\n* Cabin: Cabin number. I feel that this is just recoards. Moreover, most of this records are missing. So I will throw it;\n\n* Embarkation: This dataset defines the port of embarkation. \nC means Cherbourg; Q means Queenstown;S means Southampton;\n\nBased on the analysis above, we will focus on the labels: Survived, Pclass, Sex, Age, SibSp, Parch. moreover, I will check labels: Fare and Embarkation first, then deciede they will be thrown or not.\n\nOK, first, let us check how many people is **\"Survived\"**.\nI will use **pie chart** to visualize it."},{"metadata":{"trusted":true,"_uuid":"b13ec8783c595eabcc92b0c781816536204c44b3"},"cell_type":"code","source":"survived = train[\"Survived\"]\ntotal = survived.shape[0]\nresult_survived = pd.value_counts(survived)\nprint (result_survived)\n\nlabels_survived = 'Survived', 'Dead'\nsize_survived = [result_survived[1]/total, result_survived[0]/total]\nexplode_survived = [0.1, 0]\n\nplt.figure(figsize = (5,5))\nplt.pie(size_survived, explode = explode_survived, labels = labels_survived, center = (0, 0), labeldistance=1.1, autopct='%1.2f%%', pctdistance=0.5, shadow=True)\nplt.title(\"Survived\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ed686d244e0413054d5d8bec72163f2a89476e2f"},"cell_type":"markdown","source":"**Over 60%** passengers are dead in this **tragedy**. \n\nNext, we will check the **passenger class**. Then, I will **combine** the labels: class and survived to check the relationship between them.\n\nI will use **pie chart** to visualize label \"**Pclass**\" as well. \n<br>\nThen, I will use **bar chart** to visualize the combination. **Seabone** is another lisualization library I use.\n<br>"},{"metadata":{"trusted":true,"_uuid":"5c3dd62b2317aeac6420e85cf0aac2effe4ba0bd","scrolled":true},"cell_type":"code","source":"passenger_class = train[\"Pclass\"]\nresult_class = pd.value_counts(passenger_class)\n\nlabels_class = 'Class 1', 'Class 2', 'Class 3'\nsize_class = [result_class[1]/total, result_class[2]/total, result_class[3]/total]\nexplode_class = [0.1, 0.1, 0.1]\n\nplt.figure(figsize = (5,4.5))\nplt.pie(size_class, explode = explode_class, labels = labels_class, center = (0, 0), labeldistance=1.1, autopct='%1.2f%%', pctdistance=0.5, shadow=True)\nplt.title(\"Passenger class\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"66fb35bfef1fe5384a81803b6f3b102d02f2dc98"},"cell_type":"code","source":"train[[\"Pclass\", \"Survived\"]].groupby([\"Pclass\"]).mean().plot.bar()\nsns.countplot(\"Pclass\", hue = \"Survived\", data = train)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f33761caa0b84f028525927230883f1dbf92b69e"},"cell_type":"markdown","source":"Last figure indicates the relation between class and survived. **First class has the largest survived number**, followed by the third class. The difference between three classes survived amounts is not so large. It seems like the importnce of class is not high. But, one we need notice that, the people in the third class is largest.\n\nNext , I will check label \"**Sex**\" by **pie chart**."},{"metadata":{"trusted":true,"_uuid":"fd14f11dad47a5ddb6dc06b3235ab5a5c84ed77b"},"cell_type":"code","source":"passenger_sex = train[\"Sex\"]\nresult_sex = pd.value_counts(passenger_sex)\n   \nlabels_sex = 'Male', 'Female'\nsize_sex = [result_sex['male']/total, result_sex['female']/total]\nexplode_sex = [0.1, 0]\n\nplt.figure(figsize = (5,4.5))\nplt.pie(size_sex, explode = explode_sex, labels = labels_sex, center = (0, 0), labeldistance=1.1, autopct='%1.2f%%', pctdistance=0.5, shadow=True)\nplt.title(\"Sex\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dcc0db039beb69012430323e56286ab965bb5457"},"cell_type":"code","source":"train[[\"Sex\", \"Survived\"]].groupby([\"Sex\"]).mean().plot.bar()\nsns.countplot(\"Sex\", hue = \"Survived\", data = train)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fafb10f0308987cdc123a5e160705b627bde2fc7"},"cell_type":"markdown","source":"Among the rescued people, the **female** is aound **1.5 times** more than **male**.  This indicates the truth that, women have a high priority to board the lifeboat.\n\nI try to add the **class** info into the result. I would like to check is there the order among the class."},{"metadata":{"trusted":true,"_uuid":"428bee6292b524aa4ec2bb5d7d26dc58714b95f2"},"cell_type":"code","source":"sns.catplot(x = \"Pclass\", y = \"Survived\", hue = \"Sex\", data = train, height = 5, kind = \"bar\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ca923e0767bf0ae2e3d1953936212b7b68361c99"},"cell_type":"markdown","source":"Although the number of rescued peopel is nearly the same among the three class. But, the last figure show that, **over 90%** femals are survived in **class 1 & 2**. Only **half** femals are rescued in **class 3**. So, \"Class\" is the important factor.\n\nNext, I will check label \"**Age**\" by **bar chart**.\n\n**Notice**: There are **177 records are \"nan\"**. For data visualization, I throw them first."},{"metadata":{"_uuid":"699ab30210a76038f3bd1f961dc3fc615612b6a4","trusted":true},"cell_type":"code","source":"age = train[\"Age\"]\nresult_age = pd.value_counts(age)\nx = np.arange(0,90,0.1)\n\n#age.isnull().sum()\nage = age.dropna(axis = 0, how = \"any\") # Delete \"nan\" recoards\n#print (age)\n\nplt.bar(x,result_age[x])\nplt.show","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a488096b093ddae2cb20b2fa4abe156c380541a4"},"cell_type":"markdown","source":"It looks like a \"**Gaussian Distribution**\". <br>\nLet us add \"**Survived**\" label and **class & sex** factors."},{"metadata":{"trusted":true,"_uuid":"fe84ab6538ead93159bce2f8d8be9cdde7e45c77"},"cell_type":"code","source":"sns.violinplot(x = \"Pclass\", y = \"Age\", hue = \"Survived\", split = True, inner = \"quart\",data = train)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"578f7c1692058a6b01a20f656400547997e9a6e4"},"cell_type":"code","source":"sns.violinplot(x = \"Sex\", y = \"Age\", hue = \"Survived\", split = True, inner = \"quart\",data = train)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d72a5a5e0cf4a4b94473e5b1a2f94a58abaedfeb"},"cell_type":"markdown","source":"For each plot, the age factor always look like  \"**Gaussian Distribution**\", which is same as the original age distribution.  The **expected value in this distribution is around 30**, which is similaer with teh original distribution as well. It seems like the childs are not send to the liftboat first.\n\nNext, I will look into the factor \"**SibSp**\" and \"**Parch**\" since they indicate the same factor. Let's check the data by **bar chart** first."},{"metadata":{"trusted":true,"_uuid":"ca350bb569204b88e32097270ef3312c969db593"},"cell_type":"code","source":"sibsp = train[\"SibSp\"]\nresult_sibsp = pd.value_counts(sibsp)\nx_1 = np.arange(0,10,1)\n\nparch = train[\"Parch\"]\nresult_parch = pd.value_counts(parch)\nx_2 = np.arange(0,10,1)\n\nf, (ax1, ax2) = plt.subplots(2, 1, figsize=(7, 5), sharex=True)\n\nsns.barplot(x_1, result_sibsp[x_1], ax = ax1)\nsns.barplot(x_2, result_parch[x_2], ax = ax2)\nplt.show","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"876b0ea417b42e73d10de8c88285a2e02a08bc6e"},"cell_type":"markdown","source":"Let us introduce \"**Survived**\" into previous figure. <br>\nFirst, we check the **SibSp/Survived** and **Parch/Survived** table.<br>\nThen, use **bar chart** to indicate the data."},{"metadata":{"trusted":true,"_uuid":"f8e46c87d6d8dbb3bd45d569f9d1eb35f1691a61"},"cell_type":"code","source":"sibsp_survived = pd.crosstab([train.SibSp],train.Survived)\nprint (sibsp_survived)\n\nparch_survived = pd.crosstab([train.Parch],train.Survived)\nprint (parch_survived)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"607b91355e9719ea9e6871f36a198c26f95911ba"},"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 5), )\n\nsns.barplot('SibSp','Survived', data=train, ax = ax1)\nsns.barplot('Parch','Survived', data=train, ax = ax2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5b7da5a0e34ed1231a22e0190c101b4027e1cacd"},"cell_type":"markdown","source":"**Passengers who travel alone** are the largest rescued number. But, **passenger with one family member** have the highest rescued percentage. <br>\nIt make sense since no matter parents or husband will give the lifeboat boarding chance to their child or wife.\n\nLast label I want to check is \"**Fare**\". I think it is related with the label \"Pclass\". Normally, the higher class, the more fare passenger need to pay.<br>\nLet's check it is correct or not."},{"metadata":{"trusted":true,"_uuid":"be6cca7c76287978a66984d9ea8cf786f070a4c7"},"cell_type":"code","source":"class_fare = pd.crosstab([train.Pclass],train.Fare)\nprint (class_fare)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"61504c0f6e79f2216f5c2c8e7bca80396b96bf63"},"cell_type":"code","source":"f, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5), )\nsns.boxenplot(x = \"Pclass\", y = \"Fare\", color = \"blue\", scale = \"linear\", data = train, ax = ax1)\nsns.violinplot(x = \"Pclass\", y = \"Fare\", hue = \"Survived\", split = True, inner = \"quart\",data = train, ax = ax2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e9b6f9fc332dbac6147f22b1e324cf9ba37d1868"},"cell_type":"markdown","source":"Yup, \"**Fare**\" is related with \"**Pclass**\". The above left one shows the price pattern of class 1, 2 nd 3. **Class 1 has the highest price. Class 2 price is higher than class 3, but the differenceis quite small.** <br>\n\nTill now, all the necessary data visualization is done. I think I have a understanding of this data.<br>\nNext, I will start to **build the machine learning model.**\n"},{"metadata":{"_uuid":"ef6400943de016c8f06fb874482e142274bf48b4"},"cell_type":"markdown","source":"**<H1>Data Preparation<H1>**\n\nBefore we start the prediction, we need to do the **data preparation**.<br>\nIf we look back to the beginning, we found that there are **177 records** missing under label \"**Age**\" and **over 650 records** missing under label \"**Cabin**\". Moreover, what we found from the visualization is that, **some label is not useful** for the prediction. Based on this, we do the data preparation by the following steps:\n\n* Delete the column \"Cabin\", as we visualize it and find it not important; \n\n* Convert the 'nan' to 'S' under label \"Embarked\". Further convert label \"Embarked\" as:<br>\n\"S\" -> \"0\";\n<br>\n\"C\" -> \"1\";\n<br>\n\"Q' -> \"2\";\n\n* Convert 'male' to '0' and 'femal' to '1' under label \"Sex\";\n\n* Fill missing records under label \"Age\". Normally, the mediation number is used for the refill. Moreover, I consider sex and class fsctor as well.\n\n* Split train data set into two parts: learning and evaluation.\n"},{"metadata":{"_kg_hide-output":false,"trusted":true,"_uuid":"ce9cee4b8a8e33268461a8baeb88c3e8d9a7565b"},"cell_type":"code","source":"train = train.drop(columns = ['PassengerId', 'Name', 'Ticket', 'Cabin'])\n\ntrain.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2112b550300c4a6b88f4af18ea9385aa5a6a055b"},"cell_type":"code","source":"train.fillna({\"Embarked\":\"S\"},inplace=True)\ntrain.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f059fd31d8f3e5f83bdb220cd49abca4cbe1ecfd"},"cell_type":"code","source":"ports = {\"S\": 0, \"C\": 1, \"Q\": 2}\ntrain['Embarked'] = train['Embarked'].map(ports)\n\ntrain.Embarked.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e8022ef1f79e5110d69a568cb69e16302e1edc9b"},"cell_type":"code","source":"genders = {\"male\": 0, \"female\": 1}\ntrain['Sex'] = train['Sex'].map(genders)\n\ntrain.Sex.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"44456116038114d843b5d63de0b9f79072a4c6ae"},"cell_type":"code","source":"age_med = train.groupby([\"Pclass\",\"Sex\"]).Age.median()\ntrain.set_index([\"Pclass\",\"Sex\"],inplace = True)\ntrain.Age.fillna(age_med, inplace = True)\ntrain.reset_index(inplace = True)\n\ntrain.Age.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"02eb4d907aa971415353e4d4ac1c506c43fb64e0"},"cell_type":"code","source":"train_test, train_eval = train_test_split(train, test_size = 0.2)\n\nprint (train_test)\nprint (train_eval)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5755e1782006cfd8f4c5189aaee5899c57f6b39e"},"cell_type":"markdown","source":"**<H1>KNN<H1>**\n\nI try the KNN method first.\nI will follow the following steps to build the KNN model:\n\n* Split the train dataset into two part: raw data and result list;\n\n* Calculate [eculidean distance](http://https://en.wikipedia.org/wiki/Euclidean_distance). The distance between each test record and learning record will be calculated;\n\n* Find the K nearest distance and return the largest value;\n\n* Based on the largest value, generate the prediction results;\n\n* Evaluate the results.\n"},{"metadata":{"trusted":true,"_uuid":"41cd8c9099e96f75c80cccbd1fd028afc263f9ab","scrolled":false},"cell_type":"code","source":"train_test_learning = train_test.drop(\"Survived\", axis = 1)\ntrain_test_results = train_test[\"Survived\"] # generate the results list\n\ntrain_eval_learning = train_eval.drop(\"Survived\", axis = 1)\ntrain_eval_results = train_eval[\"Survived\"] # generate the results list\n\nprint (train_test_learning, train_test_results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f41180027615fd6239758a1626a5a904f8b26433"},"cell_type":"code","source":"# Eculidean distance calculation\ndef euclideanDistance(instance1,instance2,length):\n    distance = 0\n    for x in range(length):\n        distance = pow((instance1[x] - instance2[x]),2)\n    return math.sqrt(distance)\n \n# Return K nearest distance\ndef getNeighbors(trainingSet,testInstance,k):\n    distances = []\n    length = len(testInstance) -1\n    # Calculate test record to each train records\n    for x in range(len(trainingSet)):\n        dist = euclideanDistance(testInstance, trainingSet[x], length)\n        distances.append((trainingSet[x],dist))\n    # Sort of all distance\n    distances.sort(key = operator.itemgetter(1))\n    neighbors = []\n    # Return K nearest value\n    for x in range(k):\n        neighbors.append(distances[x][0])\n    return neighbors\n \n# Merge all KNN and find the largest value\ndef getResponse(neighbors):\n    classVotes = {}\n    for x in range(len(neighbors)):\n        response = neighbors[x][-1]\n        if response in classVotes:\n            classVotes[response] += 1\n        else:\n            classVotes[response] = 1\n    # Sort of the KNN\n    sortedVotes = sorted(classVotes.items(),key = operator.itemgetter(1),reverse =True)\n    return sortedVotes[0][0]\n \n# Evaluate the model\ndef getAccuracy(testSet,predictions):\n    correct = 0\n    for x in range(len(testSet)):\n        if testSet[x][-1] == predictions[x]:\n            correct+=1\n    return (correct/float(len(testSet))) * 100.0\n\n# Convert the dataframe to array\ntrainingSet = pd.concat([train_test_learning,train_test_results],axis=1).values\ntestSet = pd.concat([train_eval_learning,train_eval_results],axis=1).values\n\n# Generate the prediction list\npredictions = []\n\n# Define K value\nk = 5\n\n#print (trainingSet)\n\n# Main Part\nfor x in range(len(testSet)):\n    neighbors = getNeighbors(trainingSet, testSet[x], k)\n    result = getResponse(neighbors)\n    predictions.append(result)\n    print (\">predicted = \" + repr(result) + \",actual = \" + repr(testSet[x][-1]))\naccuracy = getAccuracy(testSet, predictions)\nprint (\"Accuracy:\" + repr(accuracy) + \"%\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"05d66911a8b14990ad81224a9988b89eb5603e7e"},"cell_type":"markdown","source":"The above is the **KNN fundmental code** I find from the internet. It helps me to undertand the KNN better.<br>\nHowever, the KNN algorithm is not as simply as above.<br>\nActully,  library \"**sklearn**\" contains the [KNN module](http://http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html). I can use it directly. <br>\nPlease take care of the following parameters:\n* \"n_neighbors\" is the **K value**;\n\n* \"weight\" is the way to consider the distance. it can be considered as equal or the nearer the more importance;\n\n* \"algorithm\" contains the following 4 types:\n>* brute: heavy calculation method;\n>*  kdtree: reduce the calculation pressure. It has a good performance if the dimension is less than 20;\n>* balltree: If the dimension is higher than 20, the efficiency of KD tree is reduced. This is called \"Curse of Dimensionality\". Balltree is proposed to solve this problem;\n>*  auto: Module \"fit\" function will auto decide which method will be used;<br>\n\n* \"leaf_size\" will pass to KDtree or Balltree. **It will not affect the prediction results but the calculaton speed and store space**. Normaly, the store space is **the number of samples divided leaf_size**. Also, the number of samples should be located **between one leaf_size and double leaf_size**;\n\n* \"metric\" is the method to calculat distance."},{"metadata":{"trusted":true,"_uuid":"b61b4b0880e0dd5ca1c7beab51b317f25d70f8e5","scrolled":true},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors = 20, weights = 'uniform', algorithm = 'auto', leaf_size = 30, p = 2, metric = 'minkowski', metric_params = None, n_jobs = 1)\nknn.fit(train_test_learning, train_test_results)  \neval_pred_knn = knn.predict(train_eval_learning)  \nacc_knn = round(knn.score(train_test_learning, train_test_results) * 100, 2)\n\n#print (eval_pred)\n#print (train_eval_results)\nprint (acc_knn)\n\naccuracy_score(train_eval_results, eval_pred_knn)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5c5bfd81768a1f882212639879ca5f3262cbe457"},"cell_type":"markdown","source":"**<H1>Booststrap Aggregation (Bagging)<H1>**\n\n**Bagging** is a **classic prediction** method. It is the fundamental for many other advanced predict algorithm. In facts, bagging is the advanced method from **bootstrap** since the sampleing idea based on it. Different from the KNN, \"**class**\" is ued for module implementation, so this module can be used for other advanced method. General speaking from the publications, baggins is a good method for **unstable classification**. It has a better perforamce for **high square error/low bias error model**.\n\nThe module is built by the following steps:\n* Initialization:\n\n* Sampling: Suppose there is a **original set \"S\"** which contains the **n** samples. Each time, I pick a sample and put it into a **new set \"Si\"**, after this, the sample will be **put back to the original set**. Repeat this step, we can generate **i new set** and each set contains **j elements**. We can define hw many new set we want to generate which is parameter i; same we can define how many elements in the new set which is parameter j. If j is less than n, the sampling method is called \"**Under-Sampling**\"; If j is larger than n, the sampling method is called \"**Over-Sampling**\"; If j is equal to n, the sampling method is called \"**Bootstrap**\". The reference code is defined other method as well. Please note, the elements in new set **can be the same** since we put the sample back after pick it out.\n\n* Simple model precess: For each new set, it will be processed by the simple model. Then each model have a result or output.\n\n* Voting: For **classification purpose**, voting is a good way. The results from simple model will be summeraized to define the finnal result. However, if we want to do **regression or others**, we can choose **averaging** method or other advanced methods.\n"},{"metadata":{"trusted":true,"_uuid":"3077f255d4776d0321436ccad9e5d1348aab8050"},"cell_type":"code","source":"class Bagging(object):\n    # Initialization\n    def __init__(self,n_estimators,estimator,rate=1.0):\n        self.estimator = estimator\n        self.n_estimators = n_estimators\n        self.rate = rate\n\n    def Voting(self,data):          # Define voting method\n        term = np.transpose(data)   \n        result = list()            \n\n        def Vote(df):               # vote for each raw or each simple model output\n            store = defaultdict()\n            for kw in df:\n                store.setdefault(kw, 0)\n                store[kw] += 1\n            return max(store,key = store.get)\n\n        result = map(Vote,term)      # Generate results\n        return result\n\n    # Define Under-Sampling\n    def UnderSampling(self,data):\n        #np.random.seed(np.random.randint(0,1000))\n        data = np.array(data)\n        np.random.shuffle(data)    # Personally think shuffle is important          \n        newdata = data[0:int(data.shape[0] * self.rate),:]   # Define the number of elements in new set\n        return newdata   \n\n    def TrainPredict(self,train,test):          # Build simple model\n        clf = self.estimator.fit(train[:,0:-1],train[:,-1])\n        result = clf.predict(test[:,0:-1])\n        return result\n\n    # General sampling method\n    def RepetitionRandomSampling(self,data,number):     \n        sample = []\n        for i in range(int(self.rate * number)):\n             sample.append(data[random.randint(0,len(data)-1)])\n        return sample\n\n    def Metrics(self,predict_data,test):        # Evaluation\n        score = predict_data\n        recall = recall_score(test[:,-1], score, average = None)    # Recall\n        precision = precision_score(test[:,-1], score, average = None)  # Precision\n        return recall,precision\n\n\n    def MutModel_clf(self,train,test,sample_type = \"RepetitionRandomSampling\"):\n        print (\"self.Bagging Mul_basemodel\")\n        result = list()\n        num_estimators = len(self.estimator)   \n\n        if sample_type == \"RepetitionRandomSampling\":\n            print (\"Sample Method：\",sample_type)\n            sample_function = self.RepetitionRandomSampling\n        elif sample_type == \"UnderSampling\":\n            print (\"Sample Method：\",sample_type)\n            sample_function = self.UnderSampling \n            print (\"Sampling Rate\",self.rate)\n        elif sample_type == \"IF_SubSample\":\n            print (\"Sample Method：\",sample_type)\n            sample_function = self.IF_SubSample \n            print (\"Sampling Rate\",(1.0-self.rate))\n\n        for estimator in self.estimator:\n            print (estimator)\n            for i in range(int(self.n_estimators/num_estimators)):\n                sample = np.array(sample_function(train,len(train)))       \n                clf = estimator.fit(sample[:,0:-1],sample[:,-1])\n                result.append(clf.predict(test[:,0:-1]))      # Summerize simple model output\n\n        score = self.Voting(result)\n        recall,precosoion = self.Metrics(score,test)\n        return recall,precosoion  \n\ntrain_r = Bagging(trainingSet,100,10)\n\nprint (train_r)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"137795d92a4ef5dc1fcaba7fd5b22792f39a61d9"},"cell_type":"markdown","source":"Same as KNN,  sklearn provide \"Bagging\" function. We can use it directly. So the problem is how to set parameters. The following s are the definitions for each parameter.\n* base_estimator: basic parameter for simple model, defualt settingis \"None\" which means **decision tree**. Also, youcan change it to **random forest** or others;\n\n* n_estimators: No. of estimator. Normally, **the more estimator, the lower variance**;\n\n* max_samples: the number of parameter j. Please note, thea value an be **integer or float**.But, if you set \"1\" pnot \"1.0\" and there is only one sample in the new set, the error will occur;\n\n* max_features: number of new set features;\n\n* bootstrap & bootstrap_features: **replacement** samples and sample features or not;\n\n* oob_score: \"oob\" stands of \"**out of bag**\". Whether to use out-of-bag samples to estimate the **generalization error**.\n\n* warm_start: **Reuse** the solution of the previous call to fit and add more estimators to the ensemble if the setting is Ture;\n\n* n_jobs: How many jobs working parally;\n"},{"metadata":{"trusted":true,"_uuid":"a84ed25038e520a0c9ea05452b55a28c2367671c"},"cell_type":"code","source":"bagging = BaggingClassifier(base_estimator = None, n_estimators = 10, max_samples = 1.0, max_features = 1.0, bootstrap = True, bootstrap_features = False, oob_score = False, warm_start = False, n_jobs = 1, random_state = None, verbose = 0)\nbagging.fit(train_test_learning, train_test_results)\n\neval_pred_bg = bagging.predict(train_eval_learning)\nacc_bg = round(bagging.score(train_test_learning, train_test_results) * 100, 2)\n\n#print (eval_pred_bg)\nprint (acc_bg)\n\naccuracy_score(train_eval_results, eval_pred_bg)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"015cfea729981cf01abbae159b072c695c32580d"},"cell_type":"markdown","source":"**<H1>Decision Tree<H1>**\n\nNext, let's try other method, Decision Tree. Follow the bollowing steps to build decision tree model.\n* Define **entropy** calculation;\n\n* Calculate **infomation gain ratio** and choose the feature with largest gain ratio;\n\n* Split the data based on the feature;\n\n* Iterate the above steps to **generate the decision tree**;\n\n* If it possible, **visualize** the decision tree;\n\n* Prepare the test data set and **run** the model.\n"},{"metadata":{"_uuid":"c05c67bf2b57c78f55ee34887e4f656a315209c7"},"cell_type":"markdown","source":"Same as above other methods, \"sklearn\" provide the function as well. Let us take a look at parameters.\n\n* criterion: This determin the algorithom. For **gini impurity**,  which is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset,  simply as **CART**, the setting should be \"**gini**\". For **imformation gain** as I use for raw code, you can choose \"**entropy**\"; \n\n* splitter: Two options, \"**best**\" for best split; \"**random**\" for random split. If the set is not large, best is good, otherwise, please use \"random\";\n\n* max_depth: How deep the tree goes to.  If the **data is not big** or the **number of featrues** is not big, can use defalut value \"**None**\". Otherwise, the randm value between **10-100** is better.\n\n* min_samples_split: Defalut value is** 2**, which hs a good performance for **small data sample**. If the **set is large**, increase this value a little bit. The maximum valueI used for this parameter is **10**;\n\n* min_samples_leaf: Same as above features, **if the data set is samll, keep default value \"1\"**. If the set is large, increase this value a little bit. The maximum valueI used for this parameter is **5**;\n\n* min_weight_fraction_leaf: If we **ignore the weight issue**, set value \"**0**\". If not, or the** some samples have missing features**, or** the deviation of the distribution category is large**, please consider this value;\n\n* max_features: Very important parameter, I just copy from official website, I think it is very clear;\n> The number of features to consider when looking for the best split:\n> * If **int**, then consider max_features features at each split;\n> * If **float**, then max_features is a percentage and int(max_features * n_features) features are considered at each split;\n> * If **“auto”**, then max_features=sqrt(n_features);\n> * If **“sqrt”**, then max_features=sqrt(n_features);\n> * If **“log2”**, then max_features=log2(n_features);\n> * If **None**, then max_features=n_features;\n\n* random_state: Not use too much;\n\n* max_leaf_nodes: For set, if the feature is not to much, keep defalut value. Otherwise, set a value will have a better performance. Use cross validation to choose the value;\n\n* min_impurity_decrease: A node will be split if this split induces a decrease of the impurity greater than or equal to this value;\n\n* min_impurity_split: Threshold for early stopping in tree growth;\n\n* class_weight: This parameter is used for **blancing the set smaples**. For example, if in the set, one type of data record is too many, so the decision tree predicion is strongly related with this type of data, which is not good. In this case, we need set as \"**balance**\". If not, choose \"**None**\";\n\n* presort: It is related with the speed. Normally I ignore this parameter.\n"},{"metadata":{"trusted":true,"_uuid":"7461250302981bf5c063c31e1246c30b0b28f3e2"},"cell_type":"code","source":"decision_tree = DecisionTreeClassifier(criterion = 'gini', splitter = 'best', max_depth = None, min_samples_split = 2, min_samples_leaf = 1, min_weight_fraction_leaf = 0.0, max_features = None, random_state = None, max_leaf_nodes = None, min_impurity_decrease = 0.0, min_impurity_split = None, class_weight = None, presort = False)\ndecision_tree.fit(train_test_learning, train_test_results)\n\neval_pred_dt = decision_tree.predict(train_eval_learning)\nacc_dt = round(decision_tree.score(train_test_learning, train_test_results) * 100, 2)\n\n#print (eval_pred_dt)\nprint (acc_dt)\n\naccuracy_score(train_eval_results, eval_pred_dt)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2c0ed4b49211ee99e6524bc2045b0352f3d3ee12"},"cell_type":"markdown","source":"**<H1>Random Forest<H1>**\n\nIf two methods, bagging and decision tree, are combined, there will be an new method, Random Forest. Tuning the parameters based on the decsion tree and bagging I mentioned above."},{"metadata":{"trusted":true,"_uuid":"6569cfa18bbf5229f7a4f763ec3cddc512f4c189"},"cell_type":"code","source":"random_forest = RandomForestClassifier(n_estimators = 100, criterion = 'gini', max_depth = None, min_samples_split = 2, min_samples_leaf = 1, min_weight_fraction_leaf = 0.0, max_features = 'auto', max_leaf_nodes = None, min_impurity_decrease = 0.0, min_impurity_split = None, bootstrap = True, oob_score = True, n_jobs = 1, random_state = None, verbose = 0, warm_start = False, class_weight = None)\nrandom_forest.fit(train_test_learning, train_test_results)\n\neval_pred_rf = random_forest.predict(train_eval_learning)\n\nacc_rf = round(random_forest.score(train_test_learning, train_test_results) * 100, 2)\n\n#print (eval_pred_rf)\nprint (acc_rf)\n\naccuracy_score(train_eval_results, eval_pred_rf)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"faa58d6937d88050caee8abcf47a7b7d38557627"},"cell_type":"markdown","source":"**<H1>Logistic Regression<H1>**\n\n**Linear regression** is basic prediction method. However, the regression equation of linear regression is **linear function**. If we want to accurate regression or classification, the function may not be linear. So, **logistic regression** is introduced as advance method.\n\nTo implement the logistic regression, the following aspects are necessary:\n\n* Regression/Classification equation: a propoer function is necessary for the classification, like **step, sigmoid**....\n\n* Cost function: The **variation** between prediction and realistic. \n\n* Using **gradient decrese or other advanced method** to minimize the cost function and find **the best model parameters**.\n"},{"metadata":{"_uuid":"ed38d8db63ba49a0aeb1dfcaf106029307f2f9ec"},"cell_type":"markdown","source":"\"sklearn\" function implementation.\n\n* penalty: This parameter is related with normalization. If the purpose is **solve overfitting problem**, choose \"**l2**\"; if the **predict result is not good**, try \"**l1**\";\n\n* dual: Normally is \"false\". **Only if \"penalty = 'l2'\" and \"solve = 'liblinear'\"**, choose \"dual\"; \n\n* tol: critaria on stopping;\n\n* C: Inverse of lambda;\n\n* fit_intercept: Normally set \"True\";\n\n* intercept_scaling: **Only if \"solve='liblinear'\" and \"fit_intercept='True'\"**, this parameter affect the prediction;\n\n* class_weight: same as desicion tree;\n\n* random_state: **Only if \"solve='sag'\" or \"solve='liblinear'\"**, this parameter affect the prediction. Otherwise, keep it as default;\n\n* solver: Define the method on cost function optimization:\n> * For small datasets, ‘liblinear’ is a good choice, whereas ‘sag’ and ‘saga’ are faster for large ones.\n> * For multiclass problems, only ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’ handle multinomial loss; ‘liblinear’ is limited to one-versus-rest schemes.\n> * ‘newton-cg’, ‘lbfgs’ and ‘sag’ only handle L2 penalty, whereas ‘liblinear’ and ‘saga’ handle L1 penalty.\n\n* max_iter: maximum iter number, default is 10. ** Only if \"solve='sag'\" or \"solve='newton-cg'\" or \"solve='lbfgs'\"**, this parameter affect the prediction. \n\n* multi_class : Two types: \"**ovr**\" means \"one to rest\", and \"**mvm**\" means \"many to many\". \"ovr\" is simple and fast but perforamce is not as good as \"mvm\", but \"mvm\" is slower.\n"},{"metadata":{"trusted":true,"_uuid":"91f0dbf3488ed8238246479705620ffae04f2206"},"cell_type":"code","source":"logistic_regression = LogisticRegression(penalty = 'l2', dual = False, tol = 0.0001, C = 1.0, fit_intercept = True, intercept_scaling = 1, class_weight = None, random_state = None, solver = 'liblinear', max_iter = 100, multi_class = 'ovr', verbose = 0, warm_start = False, n_jobs = 1)\nlogistic_regression.fit(train_test_learning, train_test_results)\n\n\neval_pred_lr = logistic_regression.predict(train_eval_learning)\n\nacc_lr = round(logistic_regression.score(train_test_learning, train_test_results) * 100, 2)\n\n#print (eval_pred_lr)\nprint (acc_lr)\n\naccuracy_score(train_eval_results, eval_pred_lr)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1af7e00e81ef41cb2a1affa4ad61c44e16ea7381"},"cell_type":"markdown","source":"**<H1>Stochastic Gradient Descent (SGD)<H1>**\n\nOne of the advanced method for logistic regression optimize the SGD. **SGD has a fast speed** when the traning set is large since it only use a part of the data to optimize the cost function. However, SGD has some shortages as **large noise**. Among them, the essential disadvantege is the **accuracy**. Since SGD only some of the data to optimize, technically SGD achieve local optima not global optima. It will sffect the accuracy. The simple raw code is as below."},{"metadata":{"trusted":true,"_uuid":"d213a9ca2972f65b3100efa39a04002e80d167d8"},"cell_type":"code","source":"def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n    if test_data:\n        n_test = len(test_data)\n        n = len(training_data)\n        for j in xrange(epochs):\n            random.shuffle(training_data)\n            mini_batches = [\n                training_data[k:k+mini_batch_size] \n                for k in xrange(0,n,mini_batch_size)]\n            for mini_batch in mini_batches:\n                self.update_mini_batch(mini_batch, eta)\n            if test_data:\n                print (\"Epoch {0}: {1}/{2}\".format(j, self.evaluate(test_data),n_test))\n            else:\n                print (\"Epoch {0} complete\".format(j))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0a66f0abe8a1c2d12f6eb6ab57fc69c54712baf6"},"cell_type":"markdown","source":"SGD implemetation by \"sklearn\".\n\n1. loss: Define the loss function. Can choose from \"**hinge**\", \"**modified_huber**\", \"**log**\", \"**squared_loss**\", \"**epsilon_insensitive**\" and \"**huber**\";\n\n2. penalty: same as logistic regression;\n\n3. alpha: penalty function parameter; \n\n4. l1_ratio: The mixtrue value for \"l1\" and 'l2\". **\"0\" means \"l2\"; \"1\" means \"l1\"**;\n\n5. fit_intercept: Whether the intercept should be estimated or not;\n\n6. max_iter: The maximum number of passes over the training data (aka epochs);\n\n7. epsilon: Official description as follow:\n> Epsilon in the epsilon-insensitive loss functions; only if loss is ‘huber’, ‘epsilon_insensitive’, or ‘squared_epsilon_insensitive’. For ‘huber’, determines the threshold at which it becomes less important to get the prediction exactly right. For epsilon-insensitive, any differences between the current prediction and the correct label are ignored if they are less than this threshold.\n\n8. learning_rate:\n> The learning rate schedule:\n> * ‘**constant**’: eta = eta0\n> * ‘**optimal**’: eta = 1.0 / (alpha * (t + t0)) [default]\n> * ‘**invscaling**’: eta = eta0 / pow(t, power_t);\n\n9.  n_iter: The number of passes over the training data (aka epochs).\n"},{"metadata":{"trusted":true,"_uuid":"8b4bea08d2c17b46d177ba7dd6a55e102d305dbd"},"cell_type":"code","source":"sgd = SGDClassifier(loss = 'hinge', penalty = 'l2', alpha = 0.0001, l1_ratio = 0.15, fit_intercept = True, max_iter = None, tol = None, shuffle = True, verbose = 0, epsilon = 0.1, n_jobs = 1, random_state = None, learning_rate = 'optimal', eta0 = 0.0, power_t = 0.5, class_weight = None, warm_start = False, average = False, n_iter = None)\nsgd.fit(train_test_learning, train_test_results)\n\neval_pred_sgd = sgd.predict(train_eval_learning)\n\nacc_sgd = round(sgd.score(train_test_learning, train_test_results) * 100, 2)\n\nprint (acc_sgd)\n\naccuracy_score(train_eval_results, eval_pred_sgd)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"10d6757d9d2b7e352f10fec498ffa2afa10f24d2"},"cell_type":"markdown","source":"**<H1>Perceptron<H1>**\n\nPerceptron is the **basic** of **neural network**. It is a simplest **forward propagation network** and a **binary linear classification**. It only contains two layers: **input and output**.\n\nPreceptron can be built based on the following steps:\n\n1. Define a **node** which contains two layers:input and output;\n\n2. Define the node **activation function**;\n\n3. Define an **input list** for the node;\n\n4. Define the **tranfer function** frominout to output,the function contains weights and bias;\n\n5. Define the **optimizer** for the parameter.\n"},{"metadata":{"trusted":true,"_uuid":"f3e25f418517418bb3c14700a46c2edd7747fb41"},"cell_type":"markdown","source":"Preceptron can be implemented by \"sklearn\" as well. Most of the parameters are same as other method I introduced."},{"metadata":{"trusted":true,"_uuid":"45d830708d433d8bdacc1c160bc477384788ba32"},"cell_type":"code","source":"perceptron = Perceptron(penalty = None, alpha = 0.0001, fit_intercept = True, max_iter = None, tol = None, shuffle = True, verbose = 0, eta0 = 1.0, n_jobs = 1, random_state = 0, class_weight = None, warm_start = False, n_iter = None)\nperceptron.fit(train_test_learning, train_test_results)\n\n\neval_pred_pp = perceptron.predict(train_eval_learning)\n\nacc_pp = round(perceptron.score(train_test_learning, train_test_results) * 100, 2)\n\n#print (eval_pred_pp)\nprint (acc_pp)\n\naccuracy_score(train_eval_results, eval_pred_pp)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f63b47a457d11ac3ce2303b4d298bdfe03ee41a"},"cell_type":"markdown","source":"**<H1>Linear Supported Vector Classification<H1>**\n\nSVM is a binary classification method. Simply speaking, **SVM is to build a hyperplane or a set of hyperplane to achieve classifiction or regression**. Make sure the distance between the hyperplane bounary and each point as large as possible. The basic one is linear SVC, which means the linear function is used for hyperplane implementation."},{"metadata":{"trusted":true,"_uuid":"68f6efe11ad294fdb4c9c64c7198226f91ec45ad"},"cell_type":"code","source":"linear_svc = LinearSVC()\nlinear_svc.fit(train_test_learning, train_test_results)\n\neval_pred_liner_svc = linear_svc.predict(train_eval_learning)\n\nacc_linear_svc = round(linear_svc.score(train_test_learning, train_test_results) * 100, 2)\n\nprint (acc_linear_svc)\n\naccuracy_score(train_eval_results, eval_pred_liner_svc)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"31264a6bc7b95c7248c4b3784f5ae5747e11dd2c"},"cell_type":"markdown","source":"**<H1>Supported Vector Classification<H1>**\n\nExtend the linear SVC to general method. SVC is proposed.\n\nThe **differences** betweem lineasr SVC and SVC are as follows:\n\n1. Linear SVC is to minimize the square of \"hinge loss\". SVC is to minimize \"hinge loss\";\n\n2. Linear SVC use \"one to rest\", but SVC use \"one to one\";\n\n3. SVC can choose kernel function, but linear SVC can not;\n\n4. Linear SVC can choose penalty function, but SVC can not.\n"},{"metadata":{"trusted":true,"_uuid":"b075f4040d6c1c6fb5e37c91ef1cd62ac76631ab"},"cell_type":"code","source":"svc = SVC(C = 1.0, kernel = 'rbf', degree = 3, gamma = 'auto', coef0 = 0.0, shrinking = True, probability = False, tol = 0.001, cache_size = 200, class_weight = None, verbose = False, max_iter = -1, decision_function_shape = 'ovr', random_state = None)\nsvc.fit(train_test_learning, train_test_results)\n\neval_pred_svc = svc.predict(train_eval_learning)\n\nacc_svc = round(svc.score(train_test_learning, train_test_results) * 100, 2)\n\nprint (acc_svc)\n\naccuracy_score(train_eval_results, eval_pred_svc)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3e2a180b75a059c16f6219846b6c5463ea28f00a"},"cell_type":"markdown","source":"**<H1>Naive Bayes<H1>**\n\nNaive bayes is a very simple predict method. It is based on **Bayes' theorem**, which is a famous theorem in probability theory. \n\nSimple speaking, the idea of naive bayes is **calcuate all the potential prediction resluts probability based on the bayes' theorem, then choose the result with highest probability**. It is a good method for text mining."},{"metadata":{"trusted":true,"_uuid":"0d00c42fde0b3b05fa76a3e5af5080c18e1b8578"},"cell_type":"code","source":"gaussian_naive_bayes = GaussianNB(priors = None)\ngaussian_naive_bayes.fit(train_test_learning, train_test_results)\n\neval_pred_gnb = gaussian_naive_bayes.predict(train_eval_learning)\n\nacc_gnb = round(gaussian_naive_bayes.score(train_test_learning, train_test_results) * 100, 2)\n\nprint (acc_gnb)\n\naccuracy_score(train_eval_results, eval_pred_gnb)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b26d78055e392a9e6fbcaa782b9e04cf0977ba89"},"cell_type":"markdown","source":"**<H1>K-Means<H1>**\n\nAll the above prediction methods are **supervised learning**. Let us take a look at **unsupervised learning** one.\n\nK-means is a typical unsupervised learning method.  It is suitablefor **text mining** as well. The followings are the steps of K-means:\n\n1. Define the number of clustering center, K. Randomly choose the clustering center;\n\n2. Calculate the distance between each point and each clustering center;\n\n3. Based on the distance, implemnt K clustering;\n\n4. Calculate the clustering center again;\n\n5. Repeat step 1-3, tilll the center is not change.\n\nBased on the steps, it is obvious to see that the essential parameters is **number of clustering** and clustering center."},{"metadata":{"trusted":true,"_uuid":"cb76bf9cfcfedcf9bc609b012756d31ac030251e"},"cell_type":"code","source":"def kmeans(data,k=2):\n    def _distance(p1,p2):\n        \"\"\"\n        Return Eclud distance between two points.\n        p1 = np.array([0,0]), p2 = np.array([1,1]) => 1.414\n        \"\"\"\n        tmp = np.sum((p1-p2)**2)\n        return np.sqrt(tmp)\n    def _rand_center(data,k):\n        \"\"\"Generate k center within the range of data set.\"\"\"\n        n = data.shape[1] # features\n        centroids = np.zeros((k,n)) # init with (0,0)....\n        for i in range(n):\n            dmin, dmax = np.min(data[:,i]), np.max(data[:,i])\n            centroids[:,i] = dmin + (dmax - dmin) * np.random.rand(k)\n        return centroids\n    \n    def _converged(centroids1, centroids2):\n        \n        # if centroids not changed, we say 'converged'\n         set1 = set([tuple(c) for c in centroids1])\n         set2 = set([tuple(c) for c in centroids2])\n         return (set1 == set2)\n        \n    \n    n = data.shape[0] # number of entries\n    centroids = _rand_center(data,k)\n    label = np.zeros(n,dtype=np.int) # track the nearest centroid\n    assement = np.zeros(n) # for the assement of our model\n    converged = False\n    \n    while not converged:\n        old_centroids = np.copy(centroids)\n        for i in range(n):\n            # determine the nearest centroid and track it with label\n            min_dist, min_index = np.inf, -1\n            for j in range(k):\n                dist = _distance(data[i],centroids[j])\n                if dist < min_dist:\n                    min_dist, min_index = dist, j\n                    label[i] = j\n            assement[i] = _distance(data[i],centroids[label[i]])**2\n        \n        # update centroid\n        for m in range(k):\n            centroids[m] = np.mean(data[label==m],axis=0)\n        converged = _converged(old_centroids,centroids)    \n    return centroids, label, np.sum(assement)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e235f56133b1f540108bcb4abd74bb422f21982c"},"cell_type":"markdown","source":"\"sklearn\" has the k-means function as well.\n\n1. n_clusters: Number of clustering;\n\n2. init: The method of choosing initial clustering center;\n\n3. n_init: Number of time the k-means algorithm will be run with different centroid seeds, which is related with initial clustering center;\n\n4. precompute_distances：Need pre-calculate the distance or not. If it is \"True\", model will put the whole matrix in ram; if it is \"auto\", model willl choose \"false\" if n_samples * n_clusters > 12 million; if it is \"false\",the core algorithm is \"Cpython\";\n\n5. copy_x: When pre-computing distances it is more numerically accurate to center the data first. If copy_x is True, then the original data is not modified. If False, the original data is modified, and put back before the function returns, but small numerical differences may be introduced by subtracting and then adding the data mean;\n\n6. algorithm: K-means algorithm.\n"},{"metadata":{"trusted":true,"_uuid":"97315908d7ffdbade6cf971d5c21af03e906a327","scrolled":false},"cell_type":"code","source":"#Trainset = train_test_learning.values\n\n#print (Trainset)\n\nkmeans = KMeans(n_clusters = 2, init = 'k-means++', n_init = 10, max_iter = 300, tol = 0.0001, precompute_distances = 'auto', verbose = 0, random_state = None, copy_x = True, n_jobs = 1, algorithm = 'auto')\nkmeans.fit_predict(train_test_learning)\nlabel_pred = kmeans.labels_\ncentroids = kmeans.cluster_centers_ # Clustering center\ninertia = kmeans.inertia_ # Clustering inertia summary\n\n#print (label_pred)\nprint (centroids)\nprint (inertia)\n\nacc_k = accuracy_score(train_test_results, label_pred)*100\n\nprint (acc_k)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e0422a41375598317660065901a587e039ac926c"},"cell_type":"markdown","source":"**<H1>Neural Network (N.N)<H2>**\n\nNeural network, even deep learning, is widely used technology. It **simulate the human brain** working, especilly in some specify area which the conventional method is not good at, as **computer vision**, **speed recognition**...\n\nThe N.N contains **three components**:\n1. Topology, weights and biases: These parameters define the N.N **architechture**;\n\n2. Activity rule: It defines the how to activate the nodes (neurons) and transfer the info between each nodes (neuons);\n\n3. Learning rule or propagation rule: This is very important in the N.N. It defines the how to optimize the model parameters to achieve the best performance. If the activitry rule is considered as short-term dynamic rule, the propogation can be considered as a long-term dymanic rule.\n\nTo implement the raw N.N model, the above three components are necessary. Strongly recommend the ccousera course to build the N.N, even deep learning model step by step. It is worth to do it."},{"metadata":{"trusted":true,"_uuid":"7394ec852ab6145526dcffce24f0d917f94fd4fc"},"cell_type":"markdown","source":"\"sklearn\" provide the N.N function as well. As menthioned, perceptron is the basic of N.N, so the \"sklearn\" model is called **multi-layer perceptron (MLP)**. However, \"sklearn\" model is not intend to **handle big data**. \n\n1. hidden_layer_sizes: Define the hidden laye size, in the other words, it defines the topology. For example, (10, 5) means there are two hidden layers, the first hidden layer contains 10 neurons, the second hidden layer contains 5 neurons;\n\n2. activation: Define activation function. Can only choose **\"identity\", \"logistic\" (sigmoid), \"tanh\" and \"relu\";**\n\n3. solver: Optimization method. Can choose **\"lbfgs\", \"sgd\" and \"adam\"**;\n\n4. alpha: Normalize parameter;\n\n5. batch_size: Related with mini-batch;\n\n6. learning_rate & learning_rate_init: Define the learning rate;\n\n7. power_t: The exponent for inverse scaling learning rate;\n\n8. max_iter: Maximum number of iterations;\n\n9. shuffle: Whether to shuffle samples in each iteration;\n\n10. momentum: Momentum for gradient descent update;\n\n11. nesterovs_momentum: Whether to use Nesterov’s momentum.\n"},{"metadata":{"trusted":true,"_uuid":"e21631dce4e9fadeb07a99ecf278498e51e60531"},"cell_type":"code","source":"mlp = MLPClassifier(hidden_layer_sizes = (50, ), activation = 'relu', solver = 'adam', alpha = 0.0001, batch_size = 'auto', learning_rate = 'constant', learning_rate_init = 0.001, power_t = 0.5, max_iter = 200, shuffle = True, random_state = None, tol = 0.0001, verbose = False, warm_start = False, momentum = 0.9, nesterovs_momentum = True, early_stopping = False, validation_fraction = 0.1, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-08)\nmlp.fit(train_test_learning, train_test_results)\n\neval_pred_mlp = mlp.predict(train_eval_learning)\n\nacc_mlp = round(mlp.score(train_test_learning, train_test_results) * 100, 2)\n\nprint (acc_mlp)\n\naccuracy_score(train_eval_results, eval_pred_mlp)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"65dccf203472951b177b07a726fea9472e90492c"},"cell_type":"markdown","source":"Besides the \"sklearn\", there are some other library can be used for N.N , even deep learning, model built. **Keras** is one of them. The following is the keras N.N model. I add come comments in the code to explain the code meaning."},{"metadata":{"trusted":true,"_uuid":"5f985ee5a3ab14ec6e19d0564828485f89aa38d1"},"cell_type":"code","source":"start = time() # use \"time\" function to calculate the model process time\n\nmodel = Sequential() # very improtent, it defines the model is built one layer by one layer\nmodel.add(Dense(input_dim=7, output_dim=1)) # .add means add a layer into model; dense is the layer I added, dense layer is fully connected layer\nmodel.add(Activation(\"relu\")) # add activation function, I choose \"relu\" for classification\n# this is a single layer model, it is a simple one. If need, you can add more layers by using code .add\n# take care, before you built the model, it is better to have a whole model topology \n# after this, we define the model topology\n\n# next, we need activate model by using code .complie\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n# we define the optimizer, loss function\n# optimize can be custimize first, than add into the complie function\n# metrics is used for evaluate the model, can put accuracy, score or cost into it\n\n# train the model by using .fit\nmodel.fit(train_test_learning, train_test_results)\n# train data, train label\n# can add the number of epoch and batch size as well.\n\nloss, accuracy = model.evaluate(train_test_learning, train_test_results)\nacc_nn = 100*accuracy\n\nprint (loss, accuracy)\nprint ('\\ntime taken %s seconds' % str(time() - start))\n\n# predic the test data\ndp1_pred = model.predict_classes(train_eval_learning)\n#print (dp1_pred)\n#print (train_eval_results)\nprint (\"\\n\\naccuracy\", np.sum(dp1_pred == train_eval_results.values) / float(len(train_eval_results.values)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"222d44c8173350b6a1745762c4826cb824ecbc7d"},"cell_type":"markdown","source":"Till now, I try different types of prediction method to predict survived person. But, which method is the best?\n\nLet us create a final results to check which one is the best.\n"},{"metadata":{"trusted":true,"_uuid":"7024e1e03ff57dd4edea608ecb5fe0d5210d8961"},"cell_type":"code","source":"comparesion = pd.DataFrame({\n    'Model': ['KNN', 'bagging', 'Decision Tree', 'Random Forest', 'Logistic Regression', \n              'Stochastic Gradient Decent', 'Perceptron', 'Linear Support Vector Machines', \n              'Support Vector Machines', 'Naive Bayes', 'K-Means', 'N.N(sklearn)', 'N.N(keras)'],\n    'Score': [acc_knn, acc_bg, acc_dt, acc_rf, acc_lr, acc_sgd, acc_pp, acc_linear_svc, acc_svc,\n              acc_gnb, acc_k, acc_mlp, acc_nn\n              ]})\ncomparesion_df = comparesion.sort_values(by='Score', ascending=False)\ncomparesion_df = comparesion_df.set_index('Score')\ncomparesion_df.head(14)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"808d89d0b7569591f77a8b5ac1c8757d8499ba82"},"cell_type":"markdown","source":"Based on the above table, it is obvious to find that, **random forest** and **decision tree** has the best performance. Because of the** data set limitation**, unsupervised learning and netural network do not good performance."},{"metadata":{"_uuid":"0b834e4391bd0a3096aeceab04ac1abf4eb1b359"},"cell_type":"markdown","source":"For random forest, draw **precision & recall curve** and **ROC**"},{"metadata":{"trusted":true,"_uuid":"64d9e5b5acaa9f918017f40fa7a5bdbb18644404"},"cell_type":"code","source":"predictions = cross_val_predict(random_forest, train_test_learning, train_test_results, cv=3)\nprint (precision_score(train_test_results, predictions), recall_score(train_test_results, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"181ea1a96cd4574163b450816cef2e323e473a15"},"cell_type":"code","source":"y_scores = random_forest.predict_proba(train_test_learning)\ny_scores = y_scores[:,1]\n\nprecision, recall, threshold = precision_recall_curve(train_test_results, y_scores)\n\ndef plot_precision_vs_recall(precision, recall):\n    plt.plot(recall, precision, \"g--\", linewidth=2.5)\n    plt.ylabel(\"recall\", fontsize=19)\n    plt.xlabel(\"precision\", fontsize=19)\n    plt.axis([0, 1, 0, 1])\n\nplt.figure(figsize=(10, 5))\nplot_precision_vs_recall(precision, recall)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d5064cfe7441e1205a3ce2c919c359d5fae921d5"},"cell_type":"code","source":"false_positive_rate, true_positive_rate, thresholds = roc_curve(train_test_results, y_scores)\n# plotting them against each other\ndef plot_roc_curve(false_positive_rate, true_positive_rate, label=None):\n    plt.plot(false_positive_rate, true_positive_rate, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], 'r', linewidth=4)\n    plt.axis([0, 1, 0, 1])\n    plt.xlabel('False Positive Rate (FPR)', fontsize=16)\n    plt.ylabel('True Positive Rate (TPR)', fontsize=16)\n\nplt.figure(figsize=(10, 5))\nplot_roc_curve(false_positive_rate, true_positive_rate)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"835c6a1933d96cc8e2014d84ee7289e2cdcb8380"},"cell_type":"code","source":"final = roc_auc_score(train_test_results, y_scores)\n\nprint (final)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"04dfdd0f67e4233abf7379bf22d2c5afa1aa029d"},"cell_type":"markdown","source":"ROC score is very important value for performance evaluation. The more cvlose to 1, the better performance. Now, we can find that the random forest value is over 99%, which is good for the prefdiction."},{"metadata":{"_uuid":"eb40bd42e50f98d35ad08a2c8fb9e1c01c6be5f4"},"cell_type":"markdown","source":"Last step is to predict the test data set and submission."},{"metadata":{"trusted":true,"_uuid":"334a164e340b4e086d3bf28f3bcfae60fc0de823"},"cell_type":"code","source":"test_mod = test.drop(columns = ['Name', 'Ticket', 'Cabin'])\n\nage_med_test = test_mod.groupby([\"Pclass\",\"Sex\"]).Age.median()\ntest_mod.set_index([\"Pclass\",\"Sex\"],inplace = True)\ntest_mod.Age.fillna(age_med_test, inplace = True)\ntest_mod.reset_index(inplace = True)\n\nfare_med_test = test_mod.groupby([\"Pclass\"]).Fare.median()\ntest_mod.set_index([\"Pclass\"],inplace = True)\ntest_mod.Fare.fillna(fare_med_test, inplace = True)\ntest_mod.reset_index(inplace = True)\n\ntest_mod['Embarked'] = test_mod['Embarked'].map(ports)\ntest_mod['Sex'] = test_mod['Sex'].map(genders)\n\ntest_mod.isnull().sum()\n\ntest_mod_pred = test_mod.drop(\"PassengerId\", axis = 1)\ntest_mod_id = test_mod[\"PassengerId\"] # generate the results list\n\npred = random_forest.predict(test_mod_pred)\n\nprint (pred)\n\nsubmission = pd.DataFrame({\n        \"PassengerId\": test_mod_id,\n        \"Survived\": pred\n    })\n\nsubmission.to_csv(\"submission.csv\",index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}