{"cells":[{"metadata":{"trusted":true,"_uuid":"470578a6d93f293adf0db40a5d9594f012dfda04"},"cell_type":"code","source":"# Load numpy, pandas, other libraries, models\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\n\nfrom fastai.structured import *\nfrom fastai.column_data import *\n\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b7c2e2b2587756205d854e0acb00e68df9b4ac8"},"cell_type":"code","source":"# Load in the train and test datasets from the CSV files\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\nPassengerId = test['PassengerId']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"87dbeaab1e31d9515653a1c3ac9e0014db19c079"},"cell_type":"code","source":"\n\n# Outlier detection \ndef detect_outliers(df,n,features):\n    outlier_indices = []\n    # iterate over features(columns)\n    for col in features:\n        # 1st quartile (25%)\n        Q1 = np.percentile(df[col],25)\n        # 3rd quartile (75%)\n        Q3 = np.percentile(df[col],75)\n        # Interquartile range (IQR)\n        IQR = Q3 - Q1\n        # outlier step\n        outlier_step = 1.5 * IQR\n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index       \n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n        \n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    return multiple_outliers   \n# detect outliers from Age, SibSp , Parch and Fare\nOutliers_to_drop = detect_outliers(train,2,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"])\ntrain.loc[Outliers_to_drop] # Show the outliers rows\n\n\nfull_data = [train, test]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a4a8ec48e0bdaf041a9591b65377af32d0ae5708"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"025b34eb0a1f68174fa76bb745132af73a52a6cb"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import re\n# Feature engineering \n\n# Create new feature FamilySize as a combination of SibSp and Parch\nfor dataset in full_data:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n# Create new feature IsAlone from FamilySize\nfor dataset in full_data:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n# Remove all NULLS in the Embarked column\nfor dataset in full_data:\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n# Remove all NULLS in the Fare column and create a new feature CategoricalFare\nfor dataset in full_data:\n    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\ntrain['CategoricalFare'] = pd.qcut(train['Fare'], 4)\n# Create a New feature CategoricalAge\nfor dataset in full_data:\n    age_avg = dataset['Age'].mean()\n    age_std = dataset['Age'].std()\n    age_null_count = dataset['Age'].isnull().sum()\n    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n    dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\n    dataset['Age'] = dataset['Age'].astype(int)\ntrain['CategoricalAge'] = pd.cut(train['Age'], 5)\n# Define function to extract titles from passenger names\ndef get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    # If the title exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    return \"\"\n# Create a new feature Title, containing the titles of passenger names\nfor dataset in full_data:\n    dataset['Title'] = dataset['Name'].apply(get_title)\nfor dataset in full_data:\n    dataset['Title'] = dataset['Title'].replace(['Mrs', 'Miss'], 'MM')\n    dataset['Title'] = dataset['Title'].replace(['Dr', 'Major', 'Col'], 'DMC')\n    dataset['Title'] = dataset['Title'].replace(['Don', 'Rev', 'Capt', 'Jonkheer'],'DRCJ')\n    dataset['Title'] = dataset['Title'].replace(['Mme', 'Ms', 'Lady', 'Sir', 'Mlle', 'Countess'],'MMLSMC' )\n# Mapping titles\n    title_mapping = {\"MM\": 1, \"Master\":2, \"Mr\": 5, \"DMC\": 4, \"DRCJ\": 3, \"MMLSMC\": 0}\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(3)\n    \n    \n    \n    # Apply log to Fare to reduce skewness distribution\nfor dataset in full_data:\n    dataset[\"Fare\"] = dataset[\"Fare\"].map(lambda i: np.log(i) if i > 0 else 0)\nfor dataset in full_data:\n    dataset.loc[ dataset['Fare'] <= 2.7, 'Fare'] \t\t\t\t\t\t      = 0\n    dataset.loc[(dataset['Fare'] > 2.7) & (dataset['Fare'] <= 3.2), 'Fare']   = 1\n    dataset.loc[(dataset['Fare'] > 3.2) & (dataset['Fare'] <= 3.6), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 2.7, 'Fare'] \t\t\t\t\t\t\t  = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\ntrain['Fare'].value_counts()\n\nfor dataset in full_data:\n    # Mapping Sex\n    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n    \n    # Mapping titles\n    title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n    \n    # Mapping Embarked\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n    \n\n    \n    \n    # Feature that tells whether a passenger had a cabin on the Titanic (O if no cabin number, 1 otherwise)\nfor dataset in full_data:\n    dataset['Has_Cabin'] = dataset[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n\ntrain[[\"Has_Cabin\", \"Survived\"]].groupby(['Has_Cabin'], as_index=False).sum().sort_values(by='Survived', ascending=False)\n\n\ndeck = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7, \"U\": 8}\nfor dataset in full_data:\n    dataset['Cabin'] = dataset['Cabin'].fillna(\"U0\")\n    dataset['Deck'] = dataset['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\n    dataset['Deck'] = dataset['Deck'].map(deck)\n    dataset['Deck'] = dataset['Deck'].fillna(0)\n    dataset['Deck'] = dataset['Deck'].astype(int) \n    \nfor dataset in full_data:\n    dataset.loc[ dataset['Deck'] <= 1, 'Deck'] = 1\n    dataset.loc[(dataset['Deck'] > 1) & (dataset['Deck'] <= 6), 'Deck']  = 3\n    dataset.loc[ dataset['Deck'] > 6, 'Deck'] = 0\n    \nfor dataset in full_data:\n    dataset['Boys'] = 0\n    dataset.loc[(dataset['Age'] == 0) & (dataset['Sex']==1), 'Boys'] = 1\n\nfor dataset in full_data:\n    dataset['Gender_Embarked'] = 0\n    dataset.loc[(dataset['Sex']==0) & (dataset['Embarked']==0), 'Gender_Embarked'] = 0\n    dataset.loc[(dataset['Sex']==0) & (dataset['Embarked']==2), 'Gender_Embarked'] = 1\n    dataset.loc[(dataset['Sex']==0) & (dataset['Embarked']==1), 'Gender_Embarked'] = 2\n    dataset.loc[(dataset['Sex']==1) & (dataset['Embarked']==2), 'Gender_Embarked'] = 3\n    dataset.loc[(dataset['Sex']==1) & (dataset['Embarked']==0), 'Gender_Embarked'] = 4\n    dataset.loc[(dataset['Sex']==1) & (dataset['Embarked']==1), 'Gender_Embarked'] = 5\n\ndataset['Embarked'] = dataset['Embarked'].replace(['0', '2'], '0')\ndrop_elements = [ 'Name', 'Ticket', 'Cabin']\ntrain = train.drop(drop_elements, axis = 1)\n\n\ntrain = train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\ntest  = test.drop(drop_elements, axis = 1)\n\n\n\n# X_train (all features for training purpose but excluding Survived),\n# Y_train (survival result of X-Train) and test are our 3 main datasets for the next sections\nX_train = train.drop(\"Survived\", axis=1)\nY_train = train[\"Survived\"]\nX_train.shape, Y_train.shape, test.shape\n###\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8228bc24eaecdbe54812d117ff3e68bdb6cfa4d2"},"cell_type":"code","source":"test.columns,train.columns\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"42332e324427496301c1c903833878f28ffaa171"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"68260525b047f7fe6917ce56de3eca1762b662a9"},"cell_type":"code","source":"index = 'PassengerId'\ndep = 'Survived'\ncat_vars = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Fare', 'Embarked',\n        'FamilySize', 'IsAlone', 'Title', 'Has_Cabin', 'Deck', 'Boys',\n        'Gender_Embarked']\ncontin_vars = ['Age']\ndrop_vars = []\n\ntest.set_index(index)\ntrain.set_index(index)\n\nfor v in cat_vars:\n    test[v] = test[v].astype('category').cat.as_ordered()\n    train[v] = train[v].astype('category').cat.as_ordered()\n\nfor v in contin_vars:\n    test[v] = test[v].astype('float32')\n    train[v] = train[v].astype('float32')\n    \nfor v in drop_vars:\n    if v in test:\n        test.drop(v, axis=1, inplace=True)\n        train.drop(v, axis=1, inplace=True)\n\ntest[dep] = np.nan\n        \napply_cats(test, train)\n\ndf, y, nas, mapper = proc_df(train, dep, do_scale=True, skip_flds=[index])\ndf_test, _, nas, mapper = proc_df(test, dep, do_scale=True, skip_flds=[index], mapper=mapper, na_dict=nas)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0cdcfe0746f802b166c194ddc42ac7b4a11316e6"},"cell_type":"code","source":"#Create model/learner\ncat_sz = [(c, len(train[c].cat.categories)+1) for c in cat_vars]\nemb_szs = [(c, min(50, (c+1)//2)) for _,c in cat_sz]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"19fa5be345d98f72e5f3f87a81e61d2c7d92098b"},"cell_type":"code","source":"\nn = len(train)\nval_idxs = get_cv_idxs(n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"abf4d3bc35d19cbd52139f4193178901a5174058"},"cell_type":"code","source":"PATH = '../'\nmd = ColumnarModelData.from_data_frame(PATH, val_idxs, df, y.astype(np.float32),\n                                       cat_flds=cat_vars, bs=20, test_df=df_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b6721766220db6065b0d30765525415f69a957d9"},"cell_type":"code","source":"m = md.get_learner(emb_szs, len(df.columns)-len(cat_vars),\n                   0.04, 1, [1000,500], [0.0001,0.01], y_range=[0, 1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"02d0f7ca59ace6a33f22bf678e5cb8afdbd09eb2"},"cell_type":"code","source":"# Find LR and train\n#choose smaller batch size to view plot\nm.lr_find()\nm.sched.plot()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9bf6ae5933156776871dbaafd02f6f6e556ef0ae"},"cell_type":"code","source":"lr = 1e-3\nm.fit(lr, 3, cycle_len=4, cycle_mult=2)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c22c8d2c01cf0bcaf05ffec78f56ad3e8038270d"},"cell_type":"code","source":"m.save('val0')\nm.load('val0')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"da1162264d1cb17f9d5a4385a3411cd4b4015224"},"cell_type":"code","source":"#Submit prediction\nx,y=m.predict_with_targs()\nlen(y)\npred_test = m.predict(True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3f4da21cc77c248dec8472fe6b979d084ef44c6c"},"cell_type":"code","source":"#From probabiliy to class\nypred_bst  = pred_test > 0.5  \nypred_bst = ypred_bst.astype(int)\nypred_bst","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"17eb8cb7d630624c6735a7a0a6a88c2110664a05"},"cell_type":"code","source":"test[dep] = ypred_bst\ntest.head()\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"\nsub = test[[index, dep]]\nsub.head()\n# Submit File \nsub.to_csv(\"Submission_FAI.csv\", index=False)\nprint(\"Completed.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"11df5781f26c6b896efb8663d7d40c1ca95e1c9a"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c8d396d46e42a1d7e78cb28c845f9d5c19eeb858"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a3c089eb7fb1fe92cc9279bebcddff48889aca25"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"67bdae67c90002f8e982074a4967c9528deaadc2"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}