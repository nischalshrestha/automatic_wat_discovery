{"cells":[{"metadata":{"_uuid":"5ce651e867216b5a06490887c7508ba50a741bfa","_cell_guid":"1ab1a7b5-edd2-4866-bd2c-114fddb89db5"},"cell_type":"markdown","source":"![](https://www.revell.de/fileadmin/_processed_/csm_05210__I_RMS_TITANIC_347cc45361.jpg)\n                *Image Credit: https://www.revell.de/en/products/model-building/ships/civilian-ships/id/05210.html*"},{"metadata":{"_uuid":"13707f4b7abbaef0ac52b9f6e03d30b38c1d6d0c","_cell_guid":"06d0c547-22d8-47d2-b1ab-2bd492a8a5f5"},"cell_type":"markdown","source":"Introduction\n====\nI will be doing a exploratory analysis on the Titanic dataset, and predicting which passengers survived based on the given features in the dataset."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport sklearn as sk # models\nimport seaborn as sns# visualizations\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\n\ndata_train=pd.read_csv('../input/train.csv') #Read train data\ndata_test=pd.read_csv('../input/test.csv')#Read test data","execution_count":30,"outputs":[]},{"metadata":{"_uuid":"f2cd192e0774a05c0f3c9ccc56ac16074f214ed1","_cell_guid":"b2c26ef5-1552-494b-89b2-3f68db440ae0","trusted":true},"cell_type":"code","source":"print(\"SHAPE\")\nprint(\"Training data: \", data_train.shape) #Examine shape of data\nprint(\"Testing data: \", data_test.shape)#Examine shape of data\nprint()\n\n#Examine first 10 rows of data\ndata_train.head(10)\n","execution_count":31,"outputs":[]},{"metadata":{"scrolled":true,"_uuid":"1f56bdeb3a918962293161cd3cef0e7e094d7d32","_cell_guid":"633a2608-abc3-4248-98b4-f7c3cc6436ab","trusted":true},"cell_type":"code","source":"data_test.info()","execution_count":32,"outputs":[]},{"metadata":{"_kg_hide-input":false,"_cell_guid":"f86bc683-612c-4f15-838e-aea665084783","_uuid":"e9f7492fd0fdeb6a30d174ca460a1e0bd3c8606c","trusted":true},"cell_type":"code","source":"#Check to see how many null values are in dataframe for each column.\nprint(\"NUMBER OF NULLS IN COLUMNS data_train: \")\ndata_train.isnull().sum()#Takes all null values and displays ammount for each coloumn","execution_count":33,"outputs":[]},{"metadata":{"scrolled":true,"_uuid":"bf94876bef3a58898c52d6fa7340a8af6472619f","_cell_guid":"4cbd52a9-6900-4b01-a933-c073af5aa788","trusted":true},"cell_type":"code","source":"#Check to see how mnay null values are in dataframe for each column.\nprint(\"NUMBER OF NULLS IN COLUMNS data_test: \")\ndata_test.isnull().sum()","execution_count":34,"outputs":[]},{"metadata":{"_uuid":"b7b09b5a6c1b68d236d3b30316bb4e8293a8e90a","_cell_guid":"7254d242-ada2-40cd-aaa7-0a10249587e7"},"cell_type":"markdown","source":" There is only null/NaN values in three columns. \n \n*  For **Age** I will probably take the average age on the ship and fill in those values.\n*  For **Embarked** I will most likely just drop those rows considering there are only two...\n* Im not quite sure what is best to do with **Cabin**. Part of me thinks I should try to do something about the missing values, but there are quite a few missing values and that usually means it should be dropped... I will assess it further.\n"},{"metadata":{"_uuid":"8ebf425b585c9d89a82ce33e750b44d3067df6f4","_cell_guid":"319596d3-b3ce-4071-b0fc-681f67e45c99"},"cell_type":"markdown","source":"![Cabin Layout](http://www.visualisingdata.com/blog/wp-content/uploads/2013/04/TITANIC.jpg)"},{"metadata":{"_uuid":"458487e8ba8a26dff75a09c15f9f97a5e3112aa1","_cell_guid":"1dcf9c06-bac0-47b7-a497-5bfdce724e8b"},"cell_type":"markdown","source":"*The approximate time that the Titanic struck the iceberg was at 11:40pm, it then sank entirely at 2:20am. At these times majority of people would be in bed so I feel like it is a crucial to obtain some sort of values for **Cabin**. As you can see the first class was on the upper decks. As you go further down you have majority of second and third class on the lower decks. I think the best approach for dealing with the missing values in the **Cabin** column could potentially be to instead have the deck level 1st, 2nd, or 3rd corresponding to Upper, Middle, and Lower decks.*\n\n**EDIT: It seems that there is to much data missing in the Cabin feature for it to be beneicial. I initially thought that I could do some sort of feature engineering to fix this, but it did not seem to be benefical. I will remove the Cabin feature and see how much it improves.**\n1. Dropped Cabin Feature.\n\n**EDIT2: A new idea came to mind while going through other peoples kernels. It seems that the cabin numbers could be associated with a higher class, and more likely to survive. Thank you** @Nadin Tamer\n1. Create a boolean column named **CabinBool** showing whether or not each row inside **Cabin** column with a value survived or died.\n\n\n\n\n\nMy next thought is that there is probably a relationship between **Fare** + **Pclass**.  I first want to visually check for any obvious outliers in a plot..."},{"metadata":{"collapsed":true,"_uuid":"38fd9e3056e7b3165800f93305f4ad9ecc789178","_cell_guid":"71a26b8a-c79f-4300-93e6-5fb98bf55de4","trusted":true},"cell_type":"code","source":"# Create CabinBool feature\ndata_train[\"CabinBool\"] = (data_train[\"Cabin\"].notnull().astype('int'))\ndata_test[\"CabinBool\"] = (data_test[\"Cabin\"].notnull().astype('int'))","execution_count":35,"outputs":[]},{"metadata":{"scrolled":true,"_uuid":"f699c257da70d350aa3bb7d9f389a88109285fc0","_cell_guid":"bec9e026-59b5-4b87-b193-07f120955fc5","trusted":true},"cell_type":"code","source":"sns.lmplot(x=\"PassengerId\", y=\"Fare\", data=data_train, fit_reg=True)","execution_count":36,"outputs":[]},{"metadata":{"_uuid":"6cb6d45614a17521b676b238cbb3f532cd71a480","_cell_guid":"482c46d7-3e45-44e8-b6ea-ea2b69baf9b5","trusted":true},"cell_type":"code","source":"data_train.loc[data_train['Fare'] > 300] #Show all passengers that paid more than 300","execution_count":37,"outputs":[]},{"metadata":{"_uuid":"3ace2527d5655c32354b301a63904cd8b71145ae","_cell_guid":"66fb7eb9-f76c-448c-abb3-59f39529ed6d"},"cell_type":"markdown","source":"Not sure why they paid so much more... They all paid the same amount as well. Weird. Either way they are outliers in this data and have some unorderly information so they must go."},{"metadata":{"collapsed":true,"_uuid":"dea2361dc7add93f4193933f5489d1adc59886f9","_cell_guid":"8b0374e1-2768-40a1-b604-3b43e21273da","trusted":true},"cell_type":"code","source":"data_train = data_train[data_train.Fare < 300]","execution_count":38,"outputs":[]},{"metadata":{"_uuid":"92ba631193406575e03ff512078d9f99ef19a78b","_cell_guid":"e1637221-1dd6-40df-8f27-d116e97fce65","trusted":true},"cell_type":"code","source":"sns.lmplot(x=\"PassengerId\", y=\"Fare\", data=data_train, fit_reg=True)","execution_count":39,"outputs":[]},{"metadata":{"_uuid":"b7f2d1f0761a7bea9848a49c8c4a0728a3df44fe","_cell_guid":"c49b6791-4b52-461c-933c-39883b6a17fe"},"cell_type":"markdown","source":"There we go that looks better!\n\nNow I will go ahead and drop the **Cabin** column as it has to many missing values.\nI will also go ahead and drop the **PassengerID** column from the training set as it is not benefical for the prediction."},{"metadata":{"scrolled":true,"_uuid":"9bbcb99929b7760df2b93fd054d8f84e42aec0ad","_cell_guid":"0ef7b4a6-3dc0-4778-a4fe-788f6e35c069","trusted":true},"cell_type":"code","source":"data_train.drop('Cabin', axis = 1, inplace = True)\ndata_test.drop('Cabin', axis = 1, inplace = True)\n\ndata_train.head() # Check to see if the replacement worked...","execution_count":40,"outputs":[]},{"metadata":{"_uuid":"424d716b16f09e6904f47627555121574572bb44","_cell_guid":"7bec8225-e903-4aa9-a637-231e936ddc8c"},"cell_type":"markdown","source":"Now I have to replace the empty **Age** column values with a reasonable input. Lets look at the heatmap to see what effects **Age** the most."},{"metadata":{"_uuid":"12bf075bc20798edf82062af4fa7a258254ac121","_cell_guid":"ad4a27b7-cd7a-4eaf-9aae-f8eb755a7482","trusted":true},"cell_type":"code","source":"#Calculate correlations\ncorr=data_train.corr()\n\n#Heatmap\nsns.heatmap(corr, cmap=\"Blues\")","execution_count":41,"outputs":[]},{"metadata":{"_uuid":"d8372030410fde7993741e22219a7d9fd9476398","_cell_guid":"085cb60b-8d53-4e40-986c-409c5b39c6bf"},"cell_type":"markdown","source":"Heatmap analysis\n========\nThe correlations that stand out to me the most in relation to the **Survived** column is **Survived**+**Fare** and **Survived**+**Parch**."},{"metadata":{"collapsed":true,"_uuid":"b0a90aa7fe35a470174c098409b8093e7f6092e8","_cell_guid":"1ec0ce8d-e3f9-41fb-bd64-98e8dbbee7f3","trusted":true},"cell_type":"code","source":"data_train[\"Age\"].fillna(data_train.groupby(\"Sex\")[\"Age\"].transform(\"mean\"), inplace=True)\ndata_test['Age'].fillna(data_test.groupby('Sex')['Age'].transform(\"mean\"), inplace=True)\n\nbins = [-1, 0, 5, 12, 18, 24, 35, 60, np.inf]\nlabels = ['Unknown', 'Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Senior']\ndata_train['AgeGroup'] = pd.cut(data_train[\"Age\"], bins, labels = labels)\ndata_test['AgeGroup'] = pd.cut(data_test[\"Age\"], bins, labels = labels)\n\n# Map each age value into a numerical value\nage_mapping = {'Baby': 1, 'Child': 2, 'Teenager': 3, 'Student': 4, 'Young Adult': 5, 'Adult': 6, 'Senior': 7}\ndata_train['AgeGroup'] = data_train['AgeGroup'].map(age_mapping)\ndata_test['AgeGroup'] = data_test['AgeGroup'].map(age_mapping)\n\n\n# Drop Age column from each dataset now that new column 'FareGroups' has been made.\ndata_train = data_train.drop(['Age'], axis = 1)\ndata_test = data_test.drop(['Age'], axis = 1)\n","execution_count":42,"outputs":[]},{"metadata":{"_uuid":"d9ae9d67fef0b681540b4ffb634ff83775714d66","_cell_guid":"d50d3163-0089-4142-bd43-8596b8bf48c1"},"cell_type":"markdown","source":"Above I determined the missing **Age** values by taking the mean of each **Sex** value and filling them in.\n\nNow its time to fix the values that are missing in the **Embarked** and **Fare** columns in the train & test datatsets...\nI will fill the **Embarked** value with \"S\" because it is the most reoccuring value in that column. For missing value in the **Fare** column I will replace it with the mean value."},{"metadata":{"_uuid":"f90f102ff7beaf81d45838416b98e8fa4047e7c6","_cell_guid":"82a71e1c-ac21-4f42-98d5-78db33aa05bc","trusted":true},"cell_type":"code","source":"data_train.loc[data_train.Embarked.isnull()]","execution_count":43,"outputs":[]},{"metadata":{"_uuid":"ee9d37d497dddb440e35903d852fb835bc0f1c6e","_cell_guid":"b654e215-9602-490b-bc4d-af678e16d82d","trusted":true},"cell_type":"code","source":"data_train['Embarked'].fillna(\"S\", inplace = True)\ndata_test['Fare'].fillna(data_test['Fare'].mean(), inplace = True)\n                                                            \n#Check to see how many null values are in dataframe for each column.\nprint(\"NUMBER OF NULLS IN COLUMNS: \")\ndata_train.isnull().sum()\n","execution_count":44,"outputs":[]},{"metadata":{"_uuid":"2d496d66dd15d6f63e5209135e488bdfa598fa41","_cell_guid":"3c46f1ff-72e5-4744-86a6-63739213a944"},"cell_type":"markdown","source":"*Next I will go ahead and split the **Fare** feature into groupings.\nThis idea was originally presented to me by Nadin Tamer, Thank you! (https://www.kaggle.com/nadintamer/titanic-survival-predictions-beginner)*\n\n**EDIT**: This actually wound up lowering my previous score. Will keep it in for future use if needed, but did not actually implement what was mentioned above. **Fare** feature remains the same as prior"},{"metadata":{"collapsed":true,"_uuid":"5b4a0ed2efb7730a2facacb8021d56a05407f59b","_cell_guid":"71e0844b-4146-4b17-b5dc-5cb78328ebbd","trusted":true},"cell_type":"code","source":"# Split Fare column in each dataset into four different labels.\ndata_train['FareGroups'] = pd.qcut(data_train['Fare'], 4, labels = [1, 2, 3, 4])\ndata_test['FareGroups'] = pd.qcut(data_test['Fare'], 4, labels = [1, 2, 3, 4])\n\n\n# Drop Fare column from each dataset now that new column 'FareGroups' has been made.\ndata_train = data_train.drop(['Fare'], axis = 1)\ndata_test = data_test.drop(['Fare'], axis = 1)\n","execution_count":45,"outputs":[]},{"metadata":{"_uuid":"de7fe87fe2f0ec94e3d24cf1a2c481112623fa7f","_cell_guid":"b793d33e-70f1-44be-ba83-5698dbde34a9"},"cell_type":"markdown","source":"There we go! No empty or out of the ordinary data.\n\nThe last thing we need to do is convert the **Embarked** and **Sex** columns into numerical values.\nI will also be dropping the **PassengerID, Name,** and **Ticket** features as they have do not have a large coorelation to the survival rate."},{"metadata":{"scrolled":false,"_uuid":"0c5b4365b05972b48f0f958e6e926e1ce455c8a6","_cell_guid":"686ca154-9b83-418b-96ad-4864e73fb052","trusted":true},"cell_type":"code","source":"data_train = pd.get_dummies(data_train, columns=['Sex', 'Embarked'], drop_first=True)\ndata_test = pd.get_dummies(data_test, columns=['Sex', 'Embarked'], drop_first=True)\n\ndata_train = data_train.drop([\"PassengerId\",\"Name\",\"Ticket\"], axis=1)\ndata_test = data_test.drop(['Name','Ticket'], axis=1)\ndata_test.tail()","execution_count":46,"outputs":[]},{"metadata":{"_uuid":"9ed915fda0db5e6a2be0303fbd1af3864a8274a2","_cell_guid":"fcbcfe15-076c-4eae-aca2-4408377fed9c"},"cell_type":"markdown","source":"I am going to go ahead and take one last look at each of the datasets before we move on to the modeling to make sure everything looks good."},{"metadata":{"_uuid":"71184852e3e8f3cfd941a16561633001b6035557","_cell_guid":"f5e081e8-2464-45ad-8016-959b8c0a6517","trusted":true},"cell_type":"code","source":"data_train.head()","execution_count":47,"outputs":[]},{"metadata":{"scrolled":true,"_uuid":"80ed8b06a356cd7ba5e0b5376ef5a34a04779ba9","_cell_guid":"df8045b9-124b-47b4-9de2-0f8cf6335a0f","trusted":true},"cell_type":"code","source":"data_test.tail()","execution_count":48,"outputs":[]},{"metadata":{"_uuid":"794292e9fee6aa166938f7049fb2c77280eb1ccf","_cell_guid":"18fe72ca-8f48-4d60-9ab0-037d7d6a49b6"},"cell_type":"markdown","source":"Now it is time to begin testing different models!\nI need to split my training data into different variables and create a variable for my test data for fitting to the models."},{"metadata":{"_uuid":"3e1e0d171ef79fbc5eca709b2d26148d5d49c9ef","_cell_guid":"98fba5b3-3aca-42a7-9392-ff88b4c21762","trusted":true},"cell_type":"code","source":"X_train= data_train.drop([\"Survived\"], axis=1)\nY_train= data_train.Survived\nX_test= data_test.drop(['PassengerId'], axis=1).copy()\n\nX_train.shape, Y_train.shape, X_test.shape","execution_count":49,"outputs":[]},{"metadata":{"_uuid":"fd83b1cefa5bf5623990d18c2c372c23be8c7b6a","_cell_guid":"dad2d71a-1912-41d3-b362-8d437c140630","trusted":true},"cell_type":"code","source":"# Logistic Regression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nacc_log","execution_count":50,"outputs":[]},{"metadata":{"_uuid":"af660435ca74835f5854944a199c0510cb4fda58","_cell_guid":"4e5a8249-8edd-46a9-857c-58335801752f","trusted":true},"cell_type":"code","source":"random_forest = RandomForestClassifier()\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nacc_random_forest","execution_count":51,"outputs":[]},{"metadata":{"scrolled":true,"_uuid":"8292a64805264b6216b40d1e5791685b45d18ebb","_cell_guid":"e66234e6-6745-499c-aed5-6cbb271141f4","trusted":true},"cell_type":"code","source":"svc = SVC()\nsvc.fit(X_train, Y_train)\nY_pred = svc.predict(X_test)\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\nacc_svc","execution_count":52,"outputs":[]},{"metadata":{"_uuid":"af25a2f278aa62c7fed0ed162f01395af8425f0f","_cell_guid":"583a8743-c440-45b6-99a0-a3afbe730383","trusted":true},"cell_type":"code","source":"decision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_pred_sub = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nacc_decision_tree","execution_count":53,"outputs":[]},{"metadata":{"scrolled":true,"_uuid":"f757a1cafb5815a1e657f540132433bb8ecbb2a8","_cell_guid":"8ed36517-a725-4e61-ab97-a965f3f12597","trusted":true},"cell_type":"code","source":"knn = KNeighborsClassifier()\nknn.fit(X_train, Y_train)\nY_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\nacc_knn","execution_count":54,"outputs":[]},{"metadata":{"_uuid":"16f4407d18b134a7edd0e9057bfc32f33c7d708b","_cell_guid":"952d8da9-01fb-4ef1-9709-0612b9d3c923","trusted":true},"cell_type":"code","source":"naive_bayes = GaussianNB()\nnaive_bayes.fit(X_train, Y_train)\nY_pred = naive_bayes.predict(X_test)\nacc_naive_bayes = round(naive_bayes.score(X_train, Y_train) * 100, 2)\nacc_naive_bayes\n","execution_count":55,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"b3f6b15a8ec362e0f7ef6c759adf3a850749b23d","_cell_guid":"cbb48ed5-c623-4ca9-b9ee-719a5eeebe4c","trusted":true},"cell_type":"code","source":"#from xgboost import XGBClassifier\n\n#xgb = XGBClassifier(n_estimators=200)\n#xgb.fit(X_train, Y_train)\n#Y_pred = xgb.predict(X_test)\n#acc_xgb = round(xgb.score(X_train, Y_train)*100, 2)\n#acc_xgb","execution_count":56,"outputs":[]},{"metadata":{"scrolled":true,"_uuid":"8deaa84254353ce5264e2a8484fd9c3c3e237736","_cell_guid":"55f88623-352c-4148-84b8-a24f187f1ec1","trusted":true},"cell_type":"code","source":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Decision Tree'], \n    \n    'Score': [acc_svc, acc_knn, acc_log, acc_random_forest, acc_naive_bayes, acc_decision_tree]})\n\nmodels.sort_values(by='Score', ascending=False)","execution_count":57,"outputs":[]},{"metadata":{"_uuid":"73238ce1782685e3935e9fde1d0fecf9a63cf693","_cell_guid":"33d90eb9-2af2-462a-9bbf-03ede719663d"},"cell_type":"markdown","source":"Looking at the models it looks like **Decision Tree** scores the best! With **Random Forest** close behind it. \nIn previous submissions I have used **Decision Tree**, but for this submission I will try **Random Forest** to see if it will result in a higher prediction score."},{"metadata":{"scrolled":true,"_uuid":"2fa1782683d10e4c45c56e36abdfe5b34fad77cc","_cell_guid":"82025acf-1464-48d2-b53f-827f746b403e"},"cell_type":"markdown","source":"Time to submit."},{"metadata":{"collapsed":true,"_uuid":"90370aab028d8e58bb2ade63276ad13bd02f6d6f","_cell_guid":"fc595bc9-f7c4-48b5-8259-bca45cb679eb","trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({\"PassengerId\": data_test[\"PassengerId\"],\n                           \"Survived\": Y_pred_sub\n                          })\nsubmission.to_csv('submit.csv', index=False)","execution_count":58,"outputs":[]},{"metadata":{"_uuid":"8214caa20988db7a1a83a2896bed2de2d659fe02","_cell_guid":"efd76c55-0c34-49ff-96b8-e4e8337d7225"},"cell_type":"markdown","source":"Finished!\n====="}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}