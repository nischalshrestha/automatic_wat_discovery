{"cells": [{"metadata": {"collapsed": true, "_uuid": "9d0c2bdf647fda29f845b138373988ba44951c6e", "_cell_guid": "9a0c8230-3098-4232-b625-2aa5e3ed5b1e"}, "cell_type": "markdown", "source": ["## Machine Learning\n", "### Logistic Regression - Titanic"]}, {"metadata": {"_uuid": "32f1c26ff4df642656738fd66e1c211e21077dfc", "_cell_guid": "a63288d1-cea5-4135-8b24-6bc4c4aa6f2c"}, "cell_type": "markdown", "source": ["Layout of this notebook\n", "-------------------------------------------------------------------------------------\n", "Step 1 - Frame the problem and look at the big picture<br><br>\n", "Step 2 - Setup<br>\n", "        2.1 - Common imports <br>\n", "        2.2 - Define standard function used in this notebook<br><br>\n", "Step 3 - Get the data<br>\n", "        3.1 - Combine train and test data <br><br>\n", "Step 4 - Explore and process data<br>\n", "        4.1 - Check basic info and stats<br>\n", "        4.2 - Fill missing values ['Embarked', 'Fare']<br>\n", "\t\t4.3 - Rename categorical data<br>\n", "\t\t4.4 - Modify Cabin data<br>\n", "\t\t4.5 - Create columns for no. of pass. on same ticket and individual Fare<br>\n", "\t\t4.6 - Create a new column of family size<br>\n", "\t\t4.7 - Modify 'SibSp' and 'Parch' data<br>\n", "\t\t4.8 - Create 'Title' Column<br>\n", "\t\t4.9 - Create 'Length of Name' Column<br>\n", "\t\t4.10 - Create prefix of 'Ticket' as new Column<br>\n", "\t\t4.11 - Process data<br>\n", "        4.12 - Find and delete rows with outlier data<br>\n", "\t\t4.13 - Create a new column of Fare Group <br>\n", "        4.14 - Analyze missing age data in detail<br>\n", "        4.15 - Fill missing age data<br>\n", "        4.16 - Create a new column of Age Category<br>\n", "        4.17 - Visualize numerical features<br>\n", "        4.18 - Fix skewness and normalize<br>\n", "        4.19 - Analyze survival data - Visualizing outcome across independent variables<br><br>\n", "Step 5 - Prepare the data for Machine Learning algorithms<br>\n", "        5.1 - Process further and create train, target and test dataset <br>\n", "        5.2 - Visualize how Machine Learning models works on classification with just 2 numerical features<br><br>\n", "Step 6 - Select and train a model <br>\n", "        6.1 - Visualize Machine Learning models<br>\n", "\t\t6.2 - Comparing Classification Machine Learning models with all independent features.<br>\n", "        6.3 - Extensive GridSearch with valuable parameters / score in a dataframe<br>\n", "        6.4 - Find best estimators<br>\n", "        6.5 - Plot learning curves<br>\n", "        6.6 - Feature Importance<br><br>\n", "Step 7 - Fine-tune your model<br>\n", "\t\t7.1 - Create voting classifier<br><br>\n", "Step 8 - Predict and present your solution<br><br>\n", "Step 9 - Final words!"]}, {"metadata": {"_uuid": "cbf8631d0822c68fb166286c8134ca584d830063", "_cell_guid": "b0eeb32f-ce47-4bec-bde4-c09aec2c4e5b"}, "cell_type": "markdown", "source": [" #### **Step 1 - Frame the problem and look at the big picture**"]}, {"metadata": {"_uuid": "f2f7ef6c86e7f814cb7aa534bf2b66068ecf9b06", "_cell_guid": "e608b2c3-6b0a-4132-ba5f-e0120f3cbeaf"}, "cell_type": "markdown", "source": ["1.1  - Define the objective in business terms.<br>\n", "ans  - Complete the analysis of what sorts of people were likely to survive in the unfortunate event of sinking of Titanic. <br>In particular, it is asked to apply the tools of machine learning to predict which passengers survived the tragedy.<br>\n", "<br>1.2  - How should you frame this problem? (supervised/unsupervised, online/offline, etc.)?<br>\n", "ans  - This is supervised learning task because we know output for a set of passengers' data.<br>\n", "       This is also logistic regression (clsiification) task, since you are asked to predict a binary outcome. <br>\n", "<br>1.3  - How should performance be measured?<br>\n", "ans  - Metric - Your score is the percentage of passengers you correctly predict. <br>This is known simply as \"accuracy\u201d. I will use other performance measures as well, such as K-fold cross validation, <br>f1_score (combination of precison and recall) etc."]}, {"metadata": {"_uuid": "e524754274d3233c6168348ef14a36f1cbcf9fe3", "_cell_guid": "e022bf56-cf20-43a4-8f25-c38416961e60"}, "cell_type": "markdown", "source": [" #### **Step 2 - Setup**"]}, {"metadata": {"_uuid": "79ccb497238a1d4c572fe17dc5770b6fd2e3ffe3", "_cell_guid": "dd5244c4-da42-4589-9bfd-39e3a22102a0"}, "cell_type": "markdown", "source": ["**Common imports**"]}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "2a01b935cedf047911135abfbdc269c6bf899670", "_cell_guid": "d3a9c9ea-53c4-41e2-a4fb-18fabe4caf2b"}, "cell_type": "code", "outputs": [], "source": ["# Start time for script\n", "import time\n", "start = time.time()\n", "\n", "# pandas / numpy etc\n", "import pandas as pd\n", "import numpy as np\n", "import scipy.stats as ss\n", "from scipy.special import boxcox1p\n", "\n", "# To plot pretty figures\n", "import matplotlib.pyplot as plt\n", "from matplotlib.colors import ListedColormap\n", "%matplotlib inline\n", "plt.rcParams['axes.labelsize'] = 14\n", "plt.rcParams['xtick.labelsize'] = 12\n", "plt.rcParams['ytick.labelsize'] = 12\n", "import seaborn as sns\n", "sns.set_style('dark', {'axes.facecolor' : 'lightgray'})\n", "\n", "# for seaborn issue:\n", "import warnings\n", "warnings.filterwarnings('ignore')\n", "\n", "# machine learning [Classification]\n", "from sklearn.model_selection import (train_test_split, cross_val_score, StratifiedKFold, learning_curve, GridSearchCV)\n", "from sklearn.preprocessing import (StandardScaler)\n", "from sklearn.metrics import (accuracy_score, f1_score, log_loss, confusion_matrix)\n", "\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.ensemble import RandomForestRegressor\n", "\n", "from sklearn.svm import SVC, LinearSVC\n", "from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier)\n", "from sklearn.neighbors import KNeighborsClassifier\n", "from sklearn.naive_bayes import GaussianNB\n", "from sklearn.tree import DecisionTreeClassifier\n", "from sklearn.neural_network import MLPClassifier\n", "from sklearn.gaussian_process import GaussianProcessClassifier\n", "from sklearn.gaussian_process.kernels import RBF\n", "from xgboost import XGBClassifier\n", "\n", "kfold = StratifiedKFold(n_splits=5)\n", "rand_st =42"]}, {"metadata": {"_uuid": "a82af0e302469151ddf30c72032dae886fcd2f57", "_cell_guid": "24013137-f0bd-4d8d-af33-46214c46c5bd"}, "cell_type": "markdown", "source": ["**Define standard function used in this notebook**"]}, {"metadata": {"_uuid": "2f1eef3fe7d5f53440afdf1d5ca07e6bdf9ea910", "_cell_guid": "7a7f261e-fcd4-4f3c-8b73-f904db9f215c"}, "cell_type": "markdown", "source": ["**Function 1 - outliers_iqr [Function to find and delete ouliers]**"]}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "2275e2d532c24570a0cc074bbe729f5088d246c4", "_cell_guid": "91b2d253-2634-42a7-89ea-2cadbe25fe5d"}, "cell_type": "code", "outputs": [], "source": ["# Function to find and delete ouliers. [I have to run few steps outside function to be on safer side]\n", "# It uses multiples of IQR (Inter Quartile Range) to detect outliers in specified columns\n", "\n", "def outliers_iqr(df, columns_for_outliers, iqr_factor):\n", "    for column_name in columns_for_outliers:\n", "        if not 'Outlier' in df.columns:\n", "            df['Outlier'] = 0\n", "        q_75, q_25 = np.nanpercentile(df[column_name], [75 ,25])\n", "        iqr = q_75 - q_25\n", "        minm = q_25 - (iqr*iqr_factor)\n", "        maxm = q_75 + (iqr*iqr_factor)        \n", "        df['Outlier'] = np.where(df[column_name] > maxm, 1, np.where(df[column_name] < minm, 1, df['Outlier']))\n", "    df['Outlier'] = np.where(df.Survived.notnull(), df['Outlier'], 0) # extra step to make sure only train data rows are deleted\n", "    total_rows_del = df.Outlier.sum()\n", "    print('Total ', total_rows_del, ' rows with outliers from comb_data can be deleted')"]}, {"metadata": {"_uuid": "bac8c9d58047556de002a5e4047a19789be7f9d5", "_cell_guid": "23735ae4-41c4-4ca8-be79-81896c5a714c"}, "cell_type": "markdown", "source": ["**Function 2 - plot_class_models_two_num_features [visualize classification Machine Learning models with two numerical features]**"]}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "542911070c6f0a1535c91c6d1c4fb8ae43fcc403", "_cell_guid": "b74a2f87-2b03-4675-80b5-5b1548754d2f"}, "cell_type": "code", "outputs": [], "source": ["# Create function to visualize how different Machine Learning models look, by using just 2 numerical features.\n", "# [It is not possible to plot if more than 2 features are used] [use 'X_num' and 'y_train']\n", "\n", "# Define set of classifiers\n", "clf_dict = {\"clf_Log_reg\" : LogisticRegression(random_state=rand_st),\n", "            \"clf_Lin_SVC\" : SVC(kernel=\"linear\", C=0.1, cache_size=5000, probability=True, random_state=rand_st), \n", "            \"clf_Poly_SVC\" : SVC(kernel=\"poly\", degree=2, random_state=rand_st), \n", "            \"clf_Ker_SVC\" : SVC(kernel=\"rbf\", C=50, cache_size=5000, gamma=0.001, probability=True, random_state=rand_st), \n", "            \"clf_KNN\" : KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n", "            metric_params=None, n_jobs=-1, n_neighbors=13, p=2, weights='uniform'), \n", "            \"clf_GNB\" : GaussianNB(),\n", "            \"clf_MLP\" : MLPClassifier(alpha=0.0001, learning_rate_init=0.05, shuffle=True, random_state=rand_st),\n", "            \"clf_Dec_tr\" : DecisionTreeClassifier(criterion='entropy', max_depth=8, min_samples_leaf=1, min_samples_split=2, \n", "            splitter='best', random_state=rand_st),\n", "            \"clf_Gauss\" : GaussianProcessClassifier(random_state=rand_st),\n", "            \"clf_RF\" : RandomForestClassifier(criterion='gini', max_depth=6, n_estimators = 350, n_jobs=-1, \n", "            random_state=rand_st),\n", "            \"clf_AdaBoost\" : AdaBoostClassifier(algorithm='SAMME.R', base_estimator=DecisionTreeClassifier(criterion='entropy', max_depth=8,\n", "            max_features=12, min_samples_leaf=1, min_samples_split=2, random_state=42, splitter='best'), learning_rate=0.2, n_estimators=2, random_state=rand_st),\n", "            \"clf_GrBoost\" : GradientBoostingClassifier(learning_rate=0.1, loss='deviance', max_depth=4, n_estimators=1500, \n", "            max_features=12, min_samples_leaf=100, min_samples_split=200, subsample=0.8, random_state=rand_st),\n", "            \"clf_ExTree\" : ExtraTreesClassifier(criterion='gini', max_depth=4, min_samples_leaf=2, min_samples_split=2,\n", "            n_estimators=200, n_jobs=-1, random_state=rand_st),\n", "            \"clf_XGBoost\" : XGBClassifier(colsample_bytree=0.6, gamma=0, learning_rate=0.05, max_depth=5, n_estimators=3000,\n", "            n_jobs=-1, reg_alpha=0.01, subsample=0.8, random_state=rand_st)}\n", "\n", "# Create a function to plot different classification models\n", "# Define fixed inputs\n", "h = .02  # step size in the mesh\n", "\n", "# Define function\n", "def plot_class_models_two_num_features(X, y, clf_dict, title):\n", "    import warnings\n", "    warnings.filterwarnings('ignore')\n", "    figure = plt.figure(figsize=(27, 10))\n", "    \n", "    # preprocess dataset, split into training and test part\n", "    dataset = (X, y)\n", "    X = StandardScaler().fit_transform(X)\n", "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25, random_state=42)\n", "\n", "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n", "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n", "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n", "\n", "    # just plot the dataset first\n", "    cm = plt.cm.RdBu\n", "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n", "    ax = plt.subplot(len(dataset), len(clf_dict) + 1, 1)\n", "    ax.set_title(title)\n", "    \n", "    # Plot the training points\n", "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors='k')\n", "    # and testing points\n", "    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6, edgecolors='k')\n", "    ax.set_xlim(xx.min(), xx.max())\n", "    ax.set_ylim(yy.min(), yy.max())\n", "    ax.set_xticks(())\n", "    ax.set_yticks(())\n", "    i=2\n", "    \n", "    # iterate over classifiers\n", "    for clf_name, clf in clf_dict.items():\n", "        ax = plt.subplot(len(dataset), len(clf_dict) + 1, i)\n", "        clf.fit(X_train, y_train)\n", "        score = clf.score(X_test, y_test)\n", "\n", "        # Plot the decision boundary. For that, we will assign a color to each\n", "        # point in the mesh [x_min, x_max]x[y_min, y_max].\n", "        if hasattr(clf, \"decision_function\"):\n", "            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n", "        else:\n", "            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n", "\n", "        # Put the result into a color plot\n", "        Z = Z.reshape(xx.shape)\n", "        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n", "\n", "        # Plot also the training points\n", "        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors='k')\n", "        # and testing points\n", "        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, edgecolors='k', alpha=0.6)\n", "\n", "        ax.set_xlim(xx.min(), xx.max())\n", "        ax.set_ylim(yy.min(), yy.max())\n", "        ax.set_xticks(())\n", "        ax.set_yticks(())\n", "        ax.set_title(clf_name)\n", "        ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'), size=15, horizontalalignment='right', verticalalignment = 'top', color='black', \\\n", "                bbox=dict(facecolor='yellow', alpha=0.5))\n", "        i += 1\n", "    \n", "    plt.tight_layout()\n", "    plt.show()"]}, {"metadata": {"_uuid": "e97e498f7250d95988877e45f4bb89fbdaa6fc97", "_cell_guid": "3aea4956-98fd-4bcf-aee5-2f400288a25a"}, "cell_type": "markdown", "source": ["**Function 3 - clf_cross_val_score_and_metrics [Function to evaluate various classification models]**"]}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "6583b276518d112b9d284ff2a8f91e0e6892b2b9", "_cell_guid": "ec08fe7f-494d-462a-a3d6-34bde2dbebfc"}, "cell_type": "code", "outputs": [], "source": ["# Function to evaluate various classification models [Metrics and cross_val-score]\n", "# Cross validate model with Kfold stratified cross val\n", "\n", "def clf_cross_val_score_and_metrics(X, y, clf_dict, CVS_scoring, CVS_CV):\n", "    # Train and Validation set split by model_selection\n", "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=rand_st)\n", "    metric_cols = ['clf_name', 'Score', 'Accu_Preds', 'F1_Score', 'Log_Loss', 'CVS_Best', 'CVS_Mean', 'CVS_SD']\n", "    clf_metrics = pd.DataFrame(columns = metric_cols)\n", "    metric_dict = []\n", "    \n", "    # iterate over classifiers   \n", "    for clf_name, clf in clf_dict.items():\n", "        clf.fit(X_train, y_train)\n", "        y_pred = clf.predict(X_val)\n", "        Score = \"{:.3f}\".format(clf.score(X_val, y_val))\n", "        Accu_Preds = accuracy_score(y_val, y_pred, normalize=False)\n", "        F1_Score = \"{:.3f}\".format(f1_score(y_val, y_pred))\n", "        Log_Loss = \"{:.3f}\".format(log_loss(y_val, y_pred))\n", "        \n", "        CVS_values = cross_val_score(estimator = clf, X = X, y = y, scoring = CVS_scoring, cv = CVS_CV, n_jobs=-1)\n", "        CVS_Best = \"{:.3f}\".format(CVS_values.max())\n", "        CVS_Mean = \"{:.3f}\".format(CVS_values.mean())\n", "        CVS_SD = \"{:.3f}\".format(CVS_values.std())\n", "        \n", "        metric_values = [clf_name, Score, Accu_Preds, F1_Score, Log_Loss, CVS_Best, CVS_Mean, CVS_SD]        \n", "        metric_dict.append(dict(zip(metric_cols, metric_values)))\n", "        \n", "    clf_metrics = clf_metrics.append(metric_dict)\n", "    # Change to float data type\n", "    for column_name in clf_metrics.drop('clf_name', axis=1).columns:\n", "        clf_metrics[column_name] = clf_metrics[column_name].astype('float')\n", "    clf_metrics.sort_values('CVS_Mean', ascending=False, na_position='last', inplace=True)\n", "    print(clf_metrics)\n", "    \n", "    clf_bp = sns.barplot(x='CVS_Mean', y='clf_name', data = clf_metrics, palette='inferno',orient = \"h\",**{'xerr':clf_metrics.CVS_SD})\n", "    clf_bp.set_xlabel(\"Mean Accuracy\")\n", "    clf_bp.set_ylabel(\"Classification Models\")\n", "    clf_bp.set_title(\"Cross Validation Scores\")"]}, {"metadata": {"_uuid": "3df196076dcc1c0d244e742f76ca5a4af6980b71", "_cell_guid": "4373712e-5bf1-47d9-b9de-8f14e9ec049b"}, "cell_type": "markdown", "source": ["**Function 4 - clf_GridSearchCV_results [Function to conduct extensive GridSearch and return valuable parameters / score in a dataframe]**"]}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "e291dfcdfb29047ebffbbcee8c80cc3d6f5b9ae8", "_cell_guid": "513e94ad-5ab3-41ad-89b1-370b23dfd56d"}, "cell_type": "code", "outputs": [], "source": ["# Define estimator and parameters grid for Grid Search CV\n", "# Fitting Neural Net to the Training set\n", "\n", "clf_GP_gs = GaussianProcessClassifier(random_state=rand_st)\n", "clf_GP_pg = [{'n_restarts_optimizer': [0], \n", "              'warm_start': [True], \n", "              'max_iter_predict': [200]}]\n", "\n", "clf_RF_gs = RandomForestClassifier(random_state=rand_st)\n", "clf_RF_pg = [{'max_depth': [7, 8, 9], \n", "              'max_features': ['auto', 12],  \n", "              'criterion': ['gini'], \n", "              'n_estimators': [200],\n", "              \"min_samples_split\": [10],\n", "              \"min_samples_leaf\": [3]}]\n", "\n", "clf_MLP_gs = MLPClassifier(random_state=rand_st)\n", "clf_MLP_pg = [{'activation': ['relu'], \n", "               'solver': ['adam'], \n", "               'learning_rate': ['adaptive'], \n", "               'max_iter': [30],\n", "               'alpha': [0.01], \n", "               'shuffle': [True, False], \n", "               'learning_rate_init': [0.01]}]\n", "\n", "clf_Dec_tr = DecisionTreeClassifier(random_state=rand_st)\n", "clf_AdaBoost_gs = AdaBoostClassifier(clf_Dec_tr, random_state=rand_st)\n", "clf_AdaBoost_pg = {\"base_estimator__criterion\" : [\"entropy\"],\n", "              \"base_estimator__splitter\" :   [\"best\"],\n", "              \"algorithm\" : [\"SAMME.R\"],\n", "              \"n_estimators\" :[500],\n", "              \"learning_rate\":  [0.1]}\n", "\n", "clf_Ex_tr_gs = ExtraTreesClassifier(random_state=rand_st)\n", "clf_Ex_tr_pg = {\"max_depth\": [None, 8],\n", "              \"max_features\": ['auto', 10],\n", "              \"min_samples_split\": [5],\n", "              \"min_samples_leaf\": [3],\n", "              \"n_estimators\" :[200],\n", "              \"criterion\": [\"gini\"]}\n", "\n", "clf_XGB_gs = XGBClassifier(random_state=rand_st)\n", "clf_XGB_pg = {'learning_rate': [0.01], \n", "              'max_depth': [5],\n", "              'subsample': [0.8],\n", "              'colsample_bytree': [0.6],\n", "              'n_estimators': [3000], \n", "              'reg_alpha': [0.05]}\n", "\n", "clf_GB_gs = GradientBoostingClassifier(random_state=rand_st)\n", "clf_GB_pg = {'min_samples_split' : [100],\n", "              'n_estimators' : [3000],\n", "              'learning_rate': [0.1],\n", "              'max_depth': [4],\n", "              'subsample': [0.8],\n", "              'min_samples_leaf': [100],\n", "              'max_features': ['auto', 10]}\n", "\n", "clf_SVC_gs = SVC(random_state=rand_st)\n", "clf_SVC_pg = [{'C': [1], \n", "               'kernel': ['linear'],\n", "               'gamma': [0.5]}]\n", "\n", "clf_models_gs = [clf_GP_gs, clf_RF_gs, clf_MLP_gs, clf_AdaBoost_gs, clf_Ex_tr_gs, clf_XGB_gs, clf_GB_gs, clf_SVC_gs]\n", "clf_models_gs_name = ['clf_GP_gs', 'clf_RF_gs', 'clf_MLP_gs', 'clf_AdaBoost_gs', 'clf_Ex_tr_gs', 'clf_XGB_gs', 'clf_GB_gs', 'clf_SVC_gs']\n", "clf_params_gs = [clf_GP_pg, clf_RF_pg, clf_MLP_pg, clf_AdaBoost_pg, clf_Ex_tr_pg, clf_XGB_pg, clf_GB_pg, clf_SVC_pg]\n", "gs_metric_cols = ['clf_name', 'Best_Score', 'Mean_Train_Score', 'Mean_Test_Score', 'Mean_Test_SD', 'Best_Estimator', 'Best_Params']\n", "gs_metrics = pd.DataFrame(columns = gs_metric_cols)\n", "\n", "# Define function to conduct extensive GridSearch and return valuable parameters / score in a dataframe\n", "def clf_GridSearchCV_results(gs_metrics, X_train, y_train, GS_scoring, GS_CV):\n", "    \n", "    gs_metric_dict = []\n", "    # iterate over classifiers and param grids \n", "    for clf_gs_name, clf_gs, params_gs in zip(clf_models_gs_name, clf_models_gs, clf_params_gs):\n", "        clf_gs = GridSearchCV(clf_gs,param_grid = params_gs, cv=GS_CV, scoring=GS_scoring, n_jobs= -1, verbose = 1)\n", "        clf_gs.fit(X_train,y_train)\n", "        \n", "        clf_name = clf_gs\n", "        Best_Score = clf_gs.best_score_\n", "        Mean_Train_Score = np.mean(clf_gs.cv_results_['mean_train_score'])\n", "        Mean_Test_Score = np.mean(clf_gs.cv_results_['mean_test_score'])\n", "        Mean_Test_SD = np.mean(clf_gs.cv_results_['std_test_score'])\n", "        Best_Estimator = clf_gs.best_estimator_\n", "        Best_Params = clf_gs.best_params_\n", "        \n", "        gs_metric_values = [clf_gs_name, Best_Score, Mean_Train_Score, Mean_Test_Score, Mean_Test_SD, Best_Estimator, Best_Params]        \n", "        gs_metric_dict.append(dict(zip(gs_metric_cols, gs_metric_values)))\n", "        \n", "    gs_metrics = gs_metrics.append(gs_metric_dict)\n", "    return gs_metrics"]}, {"metadata": {"_uuid": "a7053d872972d2e0007bc2b04a89cf687e39e11a", "_cell_guid": "67655292-9415-461a-93ca-562be82a7c44"}, "cell_type": "markdown", "source": ["**Function 5 - plot_learning_curve [Function to plot learning curves]** "]}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "5e0ec45932eec339486cda3c48e0f49f53acc197", "_cell_guid": "bfe72165-74d8-412b-9974-01b7e947c392"}, "cell_type": "code", "outputs": [], "source": ["# Define function to plot learning curves\n", "def plot_learning_curve(estimator, title, X_train, y_train, ylim=None, cv=None,\n", "                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n", "    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n", "    plt.figure()\n", "    plt.title(title)\n", "    if ylim is not None:\n", "        plt.ylim(*ylim)\n", "    plt.xlabel(\"Training examples\")\n", "    plt.ylabel(\"Score\")\n", "    train_sizes, train_scores, test_scores = learning_curve(\n", "        estimator, X_train, y_train, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n", "    train_scores_mean = np.mean(train_scores, axis=1)\n", "    train_scores_std = np.std(train_scores, axis=1)\n", "    test_scores_mean = np.mean(test_scores, axis=1)\n", "    test_scores_std = np.std(test_scores, axis=1)\n", "    plt.grid()\n", "\n", "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n", "                     train_scores_mean + train_scores_std, alpha=0.1,\n", "                     color=\"r\")\n", "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n", "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n", "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n", "             label=\"Training score\")\n", "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n", "             label=\"Cross-validation score\")\n", "\n", "    plt.legend(loc=\"best\")\n", "    return plt"]}, {"metadata": {"_uuid": "277c85ae8b2ffd22775d212e17503fc2e5986220", "_cell_guid": "e962c5ec-8077-439a-b6b2-18f9d3bde569"}, "cell_type": "markdown", "source": [" #### **Step 3 - Get the data**"]}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "3445658edb77eeb4e5e2f885ebfbbbc1119049dd", "_cell_guid": "fc09f00b-7f36-4fba-96d0-139b60e7486c"}, "cell_type": "code", "outputs": [], "source": ["# Manual method\n", "train_data = pd.read_csv('../input/train.csv')\n", "test_data = pd.read_csv('../input/test.csv')\n", "\n", "# Combine dataset to create one dataframe for exploration and pre-processing purpose\n", "comb_data = pd.concat([train_data, test_data])\n", "\n", "# Mark 'train' and 'test' dataset \n", "comb_data['DataType'] = np.where(comb_data[['Survived']].isnull().all(1), 'test', 'train')\n", "\n", "comb_data.head()"]}, {"metadata": {"_uuid": "e9414e33ce4188ed6aab08683e5b69c0f2823921", "_cell_guid": "dd13ea83-54b6-4d76-95d0-53d7b1e69627"}, "cell_type": "markdown", "source": ["*For exploration and pre-processing, I combined data which can be later split in train and test set.*"]}, {"metadata": {"collapsed": true, "_uuid": "70dd25792ea2721059c3a71b019aed16eb5541f8", "_cell_guid": "a625c560-a95a-48d4-9785-a12ddc1a9854"}, "cell_type": "markdown", "source": [" #### **Step 4 - Explore and process data**"]}, {"metadata": {"_uuid": "798fc79c874f11c4db0a1428cb34ebb42d8cc638", "_cell_guid": "671f86e1-92cc-4af3-8e4f-be59bd8d9e72"}, "cell_type": "markdown", "source": ["**Check basic info and stats**"]}, {"execution_count": null, "metadata": {"scrolled": false, "collapsed": true, "_uuid": "0ca13917c970fde577afc6863279e2cf1ce43b65", "_cell_guid": "014b4ca2-c127-4abe-9894-09e3651b920c"}, "cell_type": "code", "outputs": [], "source": ["# Check basic stats [Numerical features]\n", "comb_data.describe().transpose()"]}, {"metadata": {"_uuid": "93c13e1a85fa537f6edc544f2900f9cfe2d08999", "_cell_guid": "bae3ba92-77ac-4905-98f0-8a0f7711c18c"}, "cell_type": "markdown", "source": ["*It seems Fare, Parch and SibSp features have outliers*"]}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "4436d3ecb5f2b633a58927046f7356a8164adbd5", "_cell_guid": "a3645995-7c5d-409b-b5f7-12793dc203da"}, "cell_type": "code", "outputs": [], "source": ["# Check basic stats [Categorical features]\n", "comb_data.describe(include=['object', 'category']).transpose()"]}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "751c9dc1caa8ff6f529fd4257685c627aa13f06c", "_cell_guid": "27a4c9e7-1c95-4774-8994-05811bcb567d"}, "cell_type": "code", "outputs": [], "source": ["# Check basic info\n", "print(\"-----------------------Train Data-----------------------------\")\n", "comb_data[comb_data.DataType == 'train'].info()\n", "print(\"-----------------------Test Data-----------------------------\")\n", "comb_data[comb_data.DataType == 'test'].info()\n", "print(\"-----------------------Combined Data-----------------------------\")\n", "comb_data.info()"]}, {"metadata": {"_uuid": "95dbdb89d4996a184534dc3c602e2bd7ca6195c6", "_cell_guid": "e2de86b7-95ad-4084-a141-2cbcf189e460"}, "cell_type": "markdown", "source": ["*Notice that many categorical attributes are treated as numerical or object data type. There are missing values too.*"]}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "9bec064bc93cd9500eb4f659002f9b9587ee994c", "_cell_guid": "0631c682-ef17-4a7b-9c52-a7c9d65cafa1"}, "cell_type": "code", "outputs": [], "source": ["# Check null values in each column\n", "print(comb_data.isnull().sum())"]}, {"metadata": {"_uuid": "23b4421646dc5f99d550a59acd2fab603edb3f1e", "_cell_guid": "dd94b63f-1ba9-4c1b-b5c7-1cf627c733db"}, "cell_type": "markdown", "source": ["*Inferences:*<br>\n", "*1. 'Age' and 'Fare' are two numerical variables.<br>\n", "*2. 'Cabin' column has 77.4% missing values. This column can be transformed to indicate 'No Cabin'(null values) and 'Deck' level.<br>\n", "*3. 'Embarked' column has only 2 missing values, so it can be easily filled with the most frequent category.<br>\n", "*4. 'Fare' column has only 1 missing values. It can be filled with median value.<br>\n", "*5. 'Age' column, which might be important for modeling, has 20% missing values. <br> We will handle it separately with some exploration*<br>\n", "*6. It looks like 'Parch' and 'SibSp' columns have subcategories which can be grouped into one.*"]}, {"metadata": {"_uuid": "1fae7cdde469fe43b54a34e8044130d68afe1fcf", "_cell_guid": "b0a7f7e4-ca5e-45e6-a590-15f6f8099bee"}, "cell_type": "markdown", "source": ["**Fill missing values ['Embarked', 'Fare']**"]}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "cf9e3759af5990feb44acf595ff750805d72f6f6", "_cell_guid": "5d1d3443-956e-4332-999f-c97d63b8f7c0"}, "cell_type": "code", "outputs": [], "source": ["# Fill with median values [only columns in which missing values are very few].\n", "for column_name in ['Embarked', 'Fare']:\n", "    comb_data[column_name].fillna(comb_data[column_name].value_counts().index[0], inplace=True)"]}, {"metadata": {"_uuid": "db1418dd92f1a177bcad586baa8ad64aa3fb9ff1", "_cell_guid": "d92dc51c-a5e1-4afb-a909-124ec5e5a94d"}, "cell_type": "markdown", "source": ["**Rename categorical data**"]}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "07e6bda8338d6ef07d15be6c59d8bdf054eff34d", "_cell_guid": "9c36f9e6-d3c5-4d2c-9fe8-3ad22ed83d4d"}, "cell_type": "code", "outputs": [], "source": ["# Change categorical values to more meaningful values [For visulaization only, I will convert to numerical type before running ML models]\n", "comb_data['Pclass'] = np.where(comb_data['Pclass']==1, 'UpperClass', np.where(comb_data['Pclass']==2, 'MiddleClass', 'LowerClass'))\n", "comb_data['Embarked'].replace(['C','Q', 'S'],['Cherbourg','Queenstown', 'Southampton'], inplace=True)\n", "comb_data['Sex'] = np.where(comb_data['Sex']=='male', 'Male', 'Female')\n", "comb_data['Survived'].replace([0,1],['No','Yes'], inplace=True)"]}, {"metadata": {"_uuid": "e97bd79c414838a32c902a262b0c8bcf287a69d9", "_cell_guid": "3540ac78-ff0a-4712-9bb0-477391527a2b"}, "cell_type": "markdown", "source": ["**Modify Cabin data**"]}, {"execution_count": null, "metadata": {"scrolled": true, "collapsed": true, "_uuid": "d636954ba29561cf5b98b62b716f4cc9f9d4916d", "_cell_guid": "63f5e7c7-6ce2-4a65-be71-b25995d82d46"}, "cell_type": "code", "outputs": [], "source": ["# Recreate 'Cabin' with first character which represents deck and 'N' for null values.\n", "comb_data['Cabin'] = np.where(comb_data[['Cabin']].isnull().all(1), 'N', comb_data.Cabin.str[0])\n", "comb_data['Cabin'].value_counts().sort_values(ascending=False)"]}, {"metadata": {"_uuid": "6561febaf955b50ebc2ca43e2f5af3b4fc052251", "_cell_guid": "664f6e7c-667a-4221-bc48-57d5c7bcfa5e"}, "cell_type": "markdown", "source": ["**Create columns for no. of pass. on same ticket and individual Fare**"]}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "e7afdc91c0894d44f34d8a1d6924ce72993dccdb", "_cell_guid": "47d87da3-a0a7-492a-9991-3faeda8c2a0c"}, "cell_type": "code", "outputs": [], "source": ["# There are many passengers travelling on same ticket and 'Fare' is total fare for all passengers on the same ticket.\n", "comb_data[['Ticket', 'Fare']].groupby(['Ticket'], as_index=False).count().sort_values(by='Fare', ascending=False).head()"]}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "1430b1363c8818d03301d900e1e6998b71a48a3e", "_cell_guid": "8adf49ed-4137-4796-b764-d4fa09642e4a"}, "cell_type": "code", "outputs": [], "source": ["# Create 'PassCountTicket' column to show no. of passengers on same ticket\n", "# Let us explore 11 passengers travelling on same ticket\n", "comb_data['PassCountTicket'] = comb_data['Ticket'].map(comb_data['Ticket'].value_counts())\n", "comb_data[comb_data.Ticket=='CA. 2343']"]}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "d546f7e6325d5d83ad446daff7be6a6cc7512aa4", "_cell_guid": "642a7346-fa6e-43d8-aa5c-9c2c75c4f6f7"}, "cell_type": "code", "outputs": [], "source": ["# Create 'IndFare' column by dividing 'Fare' by no. of passengers on same ticket\n", "comb_data['IndFare'] = comb_data.Fare / comb_data.PassCountTicket\n", "\n", "# Check 'IndFare' data\n", "comb_data[comb_data.Ticket=='CA. 2343']"]}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "053b0fdefd8eb1aa6e158512089f0cca41d42e48", "_cell_guid": "829da3d9-7894-4572-8554-62dfd6f23e4f"}, "cell_type": "code", "outputs": [], "source": ["# Check how many passengers are in each unique 'PassCountTicket' value\n", "comb_data['PassCountTicket'].value_counts().sort_values(ascending=False)"]}, {"metadata": {"_uuid": "1b0cc3a10d073266a7a39d233ecac9cd93196c35", "_cell_guid": "65b1ccc7-4111-47bd-88b2-79ab8a5ab9a9"}, "cell_type": "markdown", "source": ["**Create a new column of family size**"]}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "650f44bd7fbe823af7f0936ecff3a7957ff6d6b2", "_cell_guid": "9ba15522-e4e4-4e20-876b-5565e58180c7"}, "cell_type": "code", "outputs": [], "source": ["# A person with zero 'SibSp' and 'Parch' is travelling alone\n", "comb_data['FamSize'] = comb_data['SibSp'] + comb_data['Parch'] + 1\n", "print(comb_data['FamSize'].value_counts().sort_values(ascending=False))\n", "\n", "# Visulize 'FamSize' data across 'Fare' and 'Survived'\n", "v0 = sns.violinplot(data=comb_data[comb_data.DataType=='train'], x='FamSize', y='Fare', hue='Survived', scale='count', split=True, inner=\"stick\")\n", "v0.set_title('Survival across Family Size & Age', fontsize = 15)\n", "plt.show()"]}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "1aeb4c5cd1700e6cd3d5d15b0b17a6da3ff342b0", "_cell_guid": "e788a4b5-dc60-460c-b102-88fc6bed25c4"}, "cell_type": "code", "outputs": [], "source": ["# Create 'Single', 'Small' and 'Large' Category\n", "comb_data['FamSize'] = np.where(comb_data['FamSize']<2, 'Single', np.where(comb_data['FamSize']<5, 'Small', 'Large'))\n", "comb_data['FamSize'] = comb_data['FamSize'].astype('category')"]}, {"metadata": {"_uuid": "b5fea8447b068fe4093643c40f5869dddee937d9", "_cell_guid": "c78b9eb3-543a-4755-a100-22e1773070e7"}, "cell_type": "markdown", "source": ["**Modify 'SibSp' and 'Parch' data**"]}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "2d4194cb95ec7703466174d8b11381d67633b5ee", "_cell_guid": "fffabf7c-5d38-4e48-a2cf-fe76605660be"}, "cell_type": "code", "outputs": [], "source": ["# Check the count of unique nos. in 'SibSp' and 'Parch' columns\n", "print(comb_data['Parch'].value_counts().sort_values())\n", "print(comb_data['SibSp'].value_counts().sort_values())"]}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "503fca73d7ba54633e9a145cfe2a8c813955642a", "_cell_guid": "135e3c9d-e345-4b25-8a28-24f38e662e31"}, "cell_type": "code", "outputs": [], "source": ["# Reduce the number of categories in Parch and SibSp as more than 4 are insignificant.\n", "comb_data['Parch'].replace([5, 6, 9],[4, 4, 4], inplace=True)\n", "comb_data['SibSp'].replace([5, 8],[4, 4], inplace=True)"]}, {"metadata": {"_uuid": "5dec1690054cbce69d2ed8981986504b13239f28", "_cell_guid": "e96c2365-8c70-414c-a514-4d6e40a7116e"}, "cell_type": "markdown", "source": ["**Create 'Title' Column**"]}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "7992d3d0ad3f6795ea931722ce7061a0d23ba080", "_cell_guid": "765bca63-4fc7-40cd-b921-f254d891e743"}, "cell_type": "code", "outputs": [], "source": ["# Create Title columns from 'Name'\n", "comb_data['Title'] = comb_data.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n", "print(comb_data['Title'].value_counts().sort_values(ascending=False))"]}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "c6333458ffba1222663b1fb12d3f9f3bdd4a16e5", "_cell_guid": "1d696b03-81b7-4757-b705-7e89802fb739"}, "cell_type": "code", "outputs": [], "source": ["# Clean up Title column categories\n", "comb_data['Title'] = comb_data['Title'].replace('Mlle', 'Miss')\n", "comb_data['Title'] = comb_data['Title'].replace('Ms', 'Miss')\n", "comb_data['Title'] = comb_data['Title'].replace('Mme', 'Mrs')\n", "\n", "comb_data['Title'] = comb_data['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n", "'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n", "print(comb_data['Title'].value_counts().sort_values(ascending=False))"]}, {"metadata": {"_uuid": "2eb2c000bd92ff0a14fd631ab231786cc283cd8b", "_cell_guid": "bbcaaabc-fe28-40f8-b286-feb2e46e4114"}, "cell_type": "markdown", "source": ["**Create 'Length of Name' Column**"]}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "629463e4d2f8df06d3c2ee9ce9600de22f557949", "_cell_guid": "89c7bcdc-6a03-4747-b667-0dc5c3503054"}, "cell_type": "code", "outputs": [], "source": ["# Create feture for length of name \n", "# The .apply method generates a new series\n", "comb_data['NameLength'] = comb_data['Name'].apply(lambda x: len(x))"]}, {"metadata": {"_uuid": "24a4e6021a69b6633ba8ec89076d38719ce8b850", "_cell_guid": "0176dd75-7a32-4690-9dbb-30f445d79eba"}, "cell_type": "markdown", "source": ["**Create prefix of 'Ticket' as new Column**"]}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "dca9e99cab6040bb7f88f8798a0a5bcf8d2ddc59", "_cell_guid": "eb2a58d9-97b3-4abf-9706-a045942fd751"}, "cell_type": "code", "outputs": [], "source": ["# Create new column from 'Ticket' by extracting the ticket prefix. When there is no prefix it returns \"N\". \n", "\n", "TicketTrim = []\n", "for i in list(comb_data.Ticket):\n", "    if not i.isdigit() :\n", "        TicketTrim.append(i.replace(\".\",\"\").replace(\"/\",\"\").strip().split(' ')[0]) #Take prefix\n", "    else:\n", "        TicketTrim.append(\"N\")\n", "        \n", "        \n", "comb_data[\"TicketTrim\"] = TicketTrim\n", "comb_data[\"TicketTrim\"].value_counts().sort_values(ascending=False).head(10)"]}, {"metadata": {"_uuid": "93c4a6f248883508a830e67c97cdd1d73cfa5248", "_cell_guid": "af2b10fe-f9b7-45ac-b1c6-0382487808fc"}, "cell_type": "markdown", "source": ["**Process data**"]}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "2eafae3f1f8fc42619cc64c742418688b9fbd3b0", "_cell_guid": "6c6b0b13-aaf4-484a-98ee-b67cf2171e1e"}, "cell_type": "code", "outputs": [], "source": ["# Drop columns not needed further\n", "comb_data = comb_data.drop(labels = ['Name', 'Ticket', 'PassCountTicket'],axis = 1)\n", "comb_data.head()"]}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "3a884a69b51365b6ecc44966df551ae9890921c6", "_cell_guid": "3bf3c2b2-5e73-4f0a-8e16-f21d36e3697f"}, "cell_type": "code", "outputs": [], "source": ["# Check data types of columns\n", "comb_data.dtypes"]}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "a8906133c371a618ad297066cca51a1bd6c17923", "_cell_guid": "15a77ee6-0cd1-44a4-ae8f-df116899b9a3"}, "cell_type": "code", "outputs": [], "source": ["# Change PassengerId data type so that it does not show up in plots. (change back to integer before applying ML models)\n", "comb_data[\"PassengerId\"] = comb_data[\"PassengerId\"].astype(str)\n", "# Change categorical columns to category data type\n", "for column_name in ['Cabin', 'Survived', 'Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked', 'FamSize', 'Title', 'TicketTrim']:\n", "    comb_data[column_name] = comb_data[column_name].astype('category')   \n", "comb_data.dtypes"]}, {"metadata": {"_uuid": "bec364665ac2faf325222b1f0ba22934fc83da98", "_cell_guid": "43e951d9-2334-4eba-b5f8-c5887e93e16f"}, "cell_type": "markdown", "source": ["**Find and delete rows with outlier data**"]}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "1df0ec8e408716a324bebb898fab5b1203dbe92d", "_cell_guid": "0cf5a118-dbe0-46ea-9623-1d75004d55cb"}, "cell_type": "code", "outputs": [], "source": ["# Drop rows with extreme outlier data [iqr_factor=10, normally this is equal to 1.5]\n", "# List columns for outliers processing\n", "columns_for_outliers = ['Age', 'Fare', 'NameLength']\n", "\n", "# Run function\n", "outliers_iqr(comb_data, columns_for_outliers, 10)\n", "\n", "# Delete rows with outlier\n", "comb_data = comb_data[comb_data.Outlier != 1]\n", "\n", "# Drop temp. column\n", "comb_data = comb_data.drop(['Outlier'], axis=1)"]}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "408f76e425eeea78e6e75715b701b804f1000668", "_cell_guid": "e693f855-aa48-4178-8369-3f8ddda8af59"}, "cell_type": "code", "outputs": [], "source": ["# Check remaining no. of rows\n", "comb_data.shape"]}, {"execution_count": null, "metadata": {"scrolled": false, "collapsed": true, "_uuid": "532fddb6fe0973eface4015e2ce602976c08142e", "_cell_guid": "ac3972fd-326f-4ed5-921a-c7fd3da5312c"}, "cell_type": "code", "outputs": [], "source": ["# Let us check stats once again.\n", "print(\"----------Stats of numerical columns---------------\")\n", "print(comb_data.describe().transpose())\n", "print(\"----------Stats of categorical columns---------------\")\n", "print(comb_data.describe(include=['category']).transpose())\n", "print(\"-------------Count of null values------------------\")\n", "print(comb_data.isnull().sum())\n", "print(\"-------Count of values in each category------------\")\n", "for column_name in comb_data.select_dtypes(include=['category']).columns:\n", "    print(comb_data[column_name].value_counts().sort_values(ascending=False))"]}, {"metadata": {"_uuid": "765cdf8e89f3f26cbc6298a7aab8968e3da344be", "_cell_guid": "13fd15e2-ec89-4820-96b8-861948aa98ad"}, "cell_type": "markdown", "source": ["**Create a new column of Fare Group**"]}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "c0c424471284fd33efe90507a7d8cae57217d5d3", "_cell_guid": "80f6f781-f278-439f-b9aa-0f9740899ed8"}, "cell_type": "code", "outputs": [], "source": ["comb_data['FareGroup'] = np.where(comb_data['Fare']<7.73, 'Tier1', np.where(comb_data['Fare']<10.5, 'Tier2', np.where(comb_data['Fare']<52.5, 'Tier3', 'Tier4')))\n", "comb_data['FareGroup'] = comb_data['FareGroup'].astype('category')"]}, {"metadata": {"_uuid": "f954aa09a29863a14972d2ed1591c02b083b61ec", "_cell_guid": "7cb26acf-5cd1-488c-8dd8-7488c61ae07c"}, "cell_type": "markdown", "source": ["**I will analyze missing age data in detail to check if missing data is totally random or skewed.<br>\n", "The goal is to find features which have most no. of missing data and have most variation in age.,<br>\n", "Then we will use Random Forest Regressor to fill missing age data**"]}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "1ad5a001f5f348bd59d759b690dae9d0b5e18fd2", "_cell_guid": "8fc2cd5c-ece2-48b2-9dff-7888afcbdc17"}, "cell_type": "code", "outputs": [], "source": ["# Create a new column to mark missing fares\n", "comb_data['AgeData'] = np.where(comb_data[['Age']].isnull().all(1), 'No', 'Yes')"]}, {"execution_count": null, "metadata": {"scrolled": false, "collapsed": true, "_uuid": "2004b6dc7c336723ef2ecdb7c52d5f00229fe6ef", "_cell_guid": "4f387f7f-7b66-4481-9ee8-8122ee0ec82d"}, "cell_type": "code", "outputs": [], "source": ["# Find columns which have proportionally more missing age data\n", "f, axes = plt.subplots(3,4, figsize = (28, 21), sharey=True)\n", "for i, col_name in enumerate(comb_data.select_dtypes(include=['category']).columns):\n", "    row = i // 4\n", "    col = i % 4\n", "    ax_curr = axes[row, col]    \n", "    ax1 = sns.barplot(x=col_name, y='Fare', data=comb_data[comb_data.AgeData == 'Yes'], color='blue', alpha = 0.5,\\\n", "    estimator=lambda col_name: len(col_name) / len(comb_data[comb_data.AgeData == 'Yes']) * 100, ax = ax_curr)\n", "    ax2 = sns.barplot(x=col_name, y='Fare', data=comb_data[comb_data.AgeData == 'No'], color='orange', alpha = 0.5,\\\n", "    estimator=lambda col_name: len(col_name) / len(comb_data[comb_data.AgeData == 'No']) * 100, ax = ax_curr)\n", "    ax1.set_ylabel('Percentage')\n", "plt.show()"]}, {"metadata": {"_uuid": "52eb6e8cd4e73bacde900dcb96a8de8a78328e43", "_cell_guid": "97a430f6-2a9b-4798-a124-5e436889cbc8"}, "cell_type": "markdown", "source": ["*What we can infer is that missing age data is fairly random. \n", "<br>However, it seems to be more in case of Queenstown, LowerClass, Survived=0, SibSP=0, Single family size, Cabin = 'N' (null values) and Parch=0.*"]}, {"execution_count": null, "metadata": {"scrolled": false, "collapsed": true, "_uuid": "d2c341c267c34b997618100b7f25cde9d213ffe3", "_cell_guid": "8ae7e954-8a58-4a9a-834d-74981611876f"}, "cell_type": "code", "outputs": [], "source": ["# Find columns which have more variatation in age [Categorical features]\n", "f, axes = plt.subplots(3,4, figsize = (28, 21), sharey=True)\n", "for i, col_name in enumerate(comb_data.select_dtypes(include=['category']).columns):\n", "    row = i // 4\n", "    col = i % 4\n", "    ax_curr = axes[row, col]    \n", "    sns.barplot(x=col_name, y='Age', data=comb_data[comb_data.AgeData == 'Yes'], ax = ax_curr)\n", "plt.show()"]}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "7ab0772b953934e516e0b558a5d944fd46076786", "_cell_guid": "ce10f787-a5bb-4f36-a739-7053383532af"}, "cell_type": "code", "outputs": [], "source": ["# Find if a column can be explained by other column, i.e. highly dependent (correlated)\n", "# Drawing correlation matrix - Standard Pearson coefficients\n", "# Compute the correlation matrix\n", "corr_mat = comb_data[comb_data.AgeData == 'Yes'].corr()\n", "\n", "# Generate a mask for the upper triangle\n", "mask = np.zeros_like(corr_mat, dtype=np.bool)\n", "mask[np.triu_indices_from(mask)] = True\n", "\n", "# Set up the matplotlib figure\n", "f, ax = plt.subplots(figsize=(6, 6))\n", "\n", "# Generate a custom diverging colormap\n", "cmap = sns.diverging_palette(240, 10, as_cmap=True)\n", "\n", "# Draw the heatmap with the mask and correct aspect ratio\n", "sns.heatmap(corr_mat, mask=mask, cmap=cmap, vmax=.8, center=0, square=True, annot=True, linecolor='black', linewidths=0, cbar_kws={\"shrink\": .4}, fmt='.2f')\n", "plt.show()"]}, {"metadata": {"_uuid": "7904a354baeb126bc36728ae766a65c24d9681c0", "_cell_guid": "31c45543-976f-49fd-a641-e32613551e51"}, "cell_type": "markdown", "source": ["*We conclude that 'Fare', 'Sex', 'Embarked', 'Namelength', 'FareGroup' etc. do not explain variation in Age.<br>\n", "Therefore, we can exclude these columns when computing missing Age data*"]}, {"metadata": {"_uuid": "a0f50873af21c2d5229626cc4e9c0517a1218eaf", "_cell_guid": "c1be9851-c921-460a-82b6-5d490918fb47"}, "cell_type": "markdown", "source": ["**Fill missing age data**"]}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "5ebb0b2c80d8cb658864ee042b1841dd17957b99", "_cell_guid": "c342f74e-91cb-4acc-8ebb-124b7421c227"}, "cell_type": "code", "outputs": [], "source": ["# Predict missing values in age using Random Forest\n", "AgeData = comb_data[['Age', 'Parch', 'SibSp', 'TicketTrim', 'Title','Pclass','FamSize', 'Cabin']]\n", "\n", "# Transform categorical features to dummy variables\n", "\n", "cat_col_names = AgeData.select_dtypes(include=['category']).columns\n", "AgeData = pd.get_dummies(AgeData, columns=cat_col_names, prefix=cat_col_names)\n", "\n", "# Split sets into train and test\n", "train_Age  = AgeData.loc[(AgeData.Age.notnull())]\n", "test_Age = AgeData.loc[(AgeData.Age.isnull())]\n", "\n", "# Create target and feature set\n", "X_train_Age = train_Age.values[:, 1::]\n", "y_train_Age = train_Age.values[:, 0]\n", "\n", "X_test_Age = test_Age.values[:, 1::]\n", "\n", "# Create and fit a model\n", "regr = RandomForestRegressor(max_depth = 8, n_estimators=2000, n_jobs=-1)\n", "regr.fit(X_train_Age, y_train_Age)\n", "\n", "# Use the fitted model to predict the missing values\n", "Age_pred = regr.predict(X_test_Age)\n", "\n", "# Assign those predictions to the full data set\n", "comb_data.loc[(comb_data.Age.isnull()), 'Age'] = Age_pred\n", "\n", "# Check null values in each column\n", "print(comb_data.isnull().sum())"]}, {"execution_count": null, "metadata": {"scrolled": true, "collapsed": true, "_uuid": "c9bbb007a25b7077abc1cbe44ac869fbf0c6da05", "_cell_guid": "a5ddeb0d-3270-440e-a3e3-006bd0b7d677"}, "cell_type": "code", "outputs": [], "source": ["# Check how missing age data distribution looks like after imputation\n", "f, axes = plt.subplots(3,4, figsize = (28, 21), sharey=True)\n", "for i, col_name in enumerate(comb_data.select_dtypes(include=['category']).columns):\n", "    row = i // 4\n", "    col = i % 4\n", "    ax_curr = axes[row, col]    \n", "    sns.barplot(x=col_name, y='Age', data=comb_data[comb_data.AgeData == 'No'], ax = ax_curr)\n", "plt.show()"]}, {"metadata": {"_uuid": "76747fa928dc7e920af6b2f4d0e36a02ccfaacae", "_cell_guid": "70ce6770-f4bf-41aa-90de-69b55777add9"}, "cell_type": "markdown", "source": ["*Distribution of missing Age after computation looks similar. So, we are good.*"]}, {"metadata": {"_uuid": "d708971f20309de67b6f2b306c64a53a593a9173", "_cell_guid": "c9f63d99-5998-4ba3-a0ae-fa1f9c832c24"}, "cell_type": "markdown", "source": ["**Create a new column of Age Category**"]}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "b289bb29e9b3e61a8379c1c2a16801f3ef500c6d", "_cell_guid": "e4987643-ea2a-4a67-9159-a6db583a19dd"}, "cell_type": "code", "outputs": [], "source": ["comb_data['AgeCat'] = np.where(comb_data['Age']<9, 'Child', np.where(comb_data['Age']<20, 'Young', np.where(comb_data['Age']<60, 'Adult', 'Senior')))\n", "comb_data['AgeCat'] = comb_data['AgeCat'].astype('category')"]}, {"metadata": {"_uuid": "6d9515185d48d047dd4ef74d8a103158f59d8410", "_cell_guid": "93a72665-8f19-406b-922b-eab43b64c10f"}, "cell_type": "markdown", "source": ["**Visualize numerical features**"]}, {"execution_count": null, "metadata": {"scrolled": false, "collapsed": true, "_uuid": "967d926e6ff10137a70e4b457ea4041349f97ef8", "_cell_guid": "f534b0e7-4e1b-4dcd-a682-cb69389f9285"}, "cell_type": "code", "outputs": [], "source": ["# A quick way to get a feel of numerical data is to plot histograms for numerical variables\n", "comb_data.hist(bins=80, figsize=(27,6))\n", "plt.show()"]}, {"metadata": {"_uuid": "7c59cf260092488491e09cf3603fd42c96ac103f", "_cell_guid": "f0567231-5f6b-4c4f-b837-0463d10d0ba9"}, "cell_type": "markdown", "source": ["**Fix skewness and normalize**"]}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "755e223ba70a6e94682f70ca45739ac473599357", "_cell_guid": "a7a30768-711f-4178-a6a9-f089aed5e3e9"}, "cell_type": "code", "outputs": [], "source": ["# Check the skewness of all numerical features\n", "num_cols = comb_data.select_dtypes(include=['float', 'int64']).columns\n", "skewed_cols = comb_data[num_cols].apply(lambda x: ss.skew(x.dropna())).sort_values(ascending=False)\n", "\n", "skewness = pd.DataFrame({'Skew' :skewed_cols})\n", "\n", "skewness = skewness[abs(skewness) > 0.75]\n", "skewness = skewness.dropna()\n", "print(skewness)\n", "skewed_cols = skewness.index\n", "print(\"There are {} skewed [skewness > 0.75] numerical features in comb_data to fix\".format(skewness.shape[0]))"]}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "df3713e7a7a019c73038df824e8e4d2e8a4a13e3", "_cell_guid": "7328c105-b018-46ee-9499-8fa845bc9bd5"}, "cell_type": "code", "outputs": [], "source": ["# Fix skewness\n", "\n", "# Use boxcox1p method\n", "'''lam = 0.5\n", "comb_data['Fare'] = boxcox1p(comb_data['Fare'], lam)\n", "comb_data['Fare'] = boxcox1p(comb_data['NameLength'], lam)'''\n", "\n", "# Use log1p method\n", "comb_data[['Fare']] = np.log1p(comb_data[['Fare']])\n", "comb_data[['NameLength']] = np.log1p(comb_data[['NameLength']])\n", "comb_data[['IndFare']] = np.log1p(comb_data[['IndFare']])"]}, {"metadata": {"_uuid": "418c3a4f749d987f6547a0fad2a46270a8c48cbc", "_cell_guid": "ac7eb26d-e5d1-4674-9bfc-825ffc162383"}, "cell_type": "markdown", "source": ["*log1p method gave better result than boxcox1p method. I learned that method to deal with skewness will affect accuracy of the model.*"]}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "e39f48fa34cc55a489760081fa8755b4bc71c0a2", "_cell_guid": "672060b4-375a-4a70-b5c3-7aa75be0c13f"}, "cell_type": "code", "outputs": [], "source": ["# Find if a column can be explained by other column, i.e. highly dependent (correlated)\n", "# Drawing correlation matrix - Standard Pearson coefficients\n", "# Compute the correlation matrix\n", "corr_mat = comb_data.corr()\n", "\n", "# Generate a mask for the upper triangle\n", "mask = np.zeros_like(corr_mat, dtype=np.bool)\n", "mask[np.triu_indices_from(mask)] = True\n", "\n", "# Set up the matplotlib figure\n", "f, ax = plt.subplots(figsize=(6, 6))\n", "\n", "# Generate a custom diverging colormap\n", "cmap = sns.diverging_palette(240, 10, as_cmap=True)\n", "\n", "# Draw the heatmap with the mask and correct aspect ratio\n", "sns.heatmap(corr_mat, mask=mask, cmap=cmap, vmax=.8, center=0, square=True, annot=True, linecolor='black', linewidths=0, cbar_kws={\"shrink\": .4}, fmt='.2f')\n", "plt.show()"]}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "b20032d92b0406bf724cd1c1ad5374b31494d6e5", "_cell_guid": "b9afc340-c7f9-49ff-9965-7c2cccb23133"}, "cell_type": "code", "outputs": [], "source": ["# Visualizing numerical data along x and y axis\n", "comb_data.plot(kind = \"scatter\", x = \"Fare\", y = \"Age\", figsize=(24, 6), color = 'green')\n", "plt.show()"]}, {"execution_count": null, "metadata": {"scrolled": false, "collapsed": true, "_uuid": "e11a8b70ef1147e034f34fc035b1067654756e75", "_cell_guid": "374aca6b-56ff-4d8d-bdc0-a641538f5773"}, "cell_type": "code", "outputs": [], "source": ["# Now look at the stats once again.\n", "print(\"-------------Basic info of columns-----------------\")\n", "print(comb_data.info())\n", "print(\"----------Stats of numerical columns---------------\")\n", "print(comb_data.describe().transpose())\n", "print(\"-------------Count of null values------------------\")\n", "print(comb_data.isnull().sum())\n", "print(\"-------Count of values in each category------------\")\n", "for column_name in comb_data.select_dtypes(include=['category']).columns:\n", "    print(comb_data[column_name].value_counts())"]}, {"metadata": {"_uuid": "26ae832312a4ed4584f24b46f8afb50d931d544a", "_cell_guid": "11029a3f-8b5a-4434-a432-691661a051d5"}, "cell_type": "markdown", "source": ["**Analyze survival data - Visualizing outcome across independent variables**"]}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "c74d093ad1d27a89b220aec90ebfd6aa916dfb3a", "_cell_guid": "0dfe268f-6ad3-4264-a3c5-243406148fd2"}, "cell_type": "code", "outputs": [], "source": ["# Create a subset that has survival outcome of all passengers\n", "surv = comb_data[comb_data.DataType == 'train']\n", "surv.head()"]}, {"execution_count": null, "metadata": {"scrolled": false, "collapsed": true, "_uuid": "f16783ced28d5a1d508df9c215146c9eb4071ed0", "_cell_guid": "e9599b11-d8a9-48d6-a37d-ab0fb01604c5"}, "cell_type": "code", "outputs": [], "source": ["print(\"------------------------Count & %age----------   ----------------\")\n", "print(\"Survived: %i (%.1f percent), Not Survived: %i (%.1f percent), Total: %i\"\\\n", "      %(len(surv[surv.Survived == 'Yes']), 1.*len(surv[surv.Survived == 'Yes'])\\\n", "        /len(surv)*100.0,len(surv[surv.Survived == 'No']),\\\n", "        1.*len(surv[surv.Survived == 'No'])/len(surv)*100.0,\\\n", "        len(surv)))\n", "print(\"------------------------Mean Age-------------------------------------\")\n", "print(\"Mean age survivors: %.1f, Mean age non-survivers: %.1f\"\\\n", "      %(np.mean(surv[surv.Survived == 'Yes'].Age), np.mean(surv[surv.Survived == 'No'].Age)))\n", "print(\"------------------------Median Fare-------------------------------------\")\n", "print(\"Median Fare survivors: %.1f, Median Fare non-survivers: %.1f\"\\\n", "      %(np.median(surv[surv.Survived == 'Yes'].Fare), np.median(surv[surv.Survived == 'No'].Fare)))"]}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "5c9fbd2371a7c8e5cded48ebd6d4a4357537e578", "_cell_guid": "2076fb28-adc2-406d-8598-19bf72c26739"}, "cell_type": "code", "outputs": [], "source": ["# Visulaizing survival data against numerical columns\n", "# Create violin plot to compare against numerical columns.\n", "\n", "f, axes = plt.subplots(ncols=4, figsize = (24, 6))\n", "v1 = sns.violinplot(data = surv, x = 'Survived', y = 'Fare', ax = axes[0])\n", "v1.set_title('Survived vs. Fare', fontsize = 12)\n", "\n", "v2 = sns.violinplot(data = surv, x = 'Survived', y = 'IndFare', ax = axes[1])\n", "v2.set_title('Survived vs. IndFare', fontsize = 12)\n", "\n", "v3 = sns.violinplot(data = surv, x = 'Survived', y = 'NameLength', ax = axes[2])\n", "v3.set_title('Survived vs. NameLength', fontsize = 12)\n", "\n", "v4 = sns.violinplot(data = surv, x = 'Survived', y = 'Age', ax = axes[3])\n", "v4.set_title('Survived vs. Age', fontsize = 12)\n", "plt.show()"]}, {"metadata": {"_uuid": "c262ae6063c337a3e9ab34cde90409bb143d3703", "_cell_guid": "11d75f66-a343-4c1f-a14f-5eb552e54072"}, "cell_type": "markdown", "source": ["*Age data does not look different in dead or alive group.*"]}, {"execution_count": null, "metadata": {"scrolled": false, "collapsed": true, "_uuid": "d7659e8537a71a00e2c606a524022ed149e0d3ea", "_cell_guid": "1c5a4d28-ddb1-4aa2-b46e-0155f6c6de08"}, "cell_type": "code", "outputs": [], "source": ["# Visulaizing survival data against numerical columns\n", "# Create distribution plot to compare against numerical columns.\n", "\n", "f, axes = plt.subplots(ncols=4, figsize = (28, 6))\n", "d1 = sns.distplot(surv[surv.Survived == 'Yes']['Age'].dropna().values, color='Green', ax = axes[0], label = 'Survived')\n", "d2 = sns.distplot(surv[surv.Survived == 'No']['Age'].dropna().values, color='Red', ax = axes[0], label = 'Not Survived')\n", "d1.set_title('Survived vs. Age', fontsize = 12)\n", "d1.legend(loc='best')\n", "d1.set(xlabel=\"Age\", ylabel=\"No. of Passengers\")\n", "\n", "d3 = sns.distplot(surv[surv.Survived == 'Yes']['Fare'].dropna().values, color='Green', ax = axes[1], label = 'Survived')\n", "d4 = sns.distplot(surv[surv.Survived == 'No']['Fare'].dropna().values, color='Red', ax = axes[1], label = 'Not Survived')\n", "d3.set_title('Survived vs. Fare', fontsize = 12)\n", "d3.legend(loc='best')\n", "d3.set(xlabel=\"Fare\", ylabel=\"No. of Passengers\")\n", "\n", "d5 = sns.distplot(surv[surv.Survived == 'Yes']['IndFare'].dropna().values, color='Green', ax = axes[2], label = 'Survived')\n", "d6 = sns.distplot(surv[surv.Survived == 'No']['IndFare'].dropna().values, color='Red', ax = axes[2], label = 'Not Survived')\n", "d5.set_title('Survived vs. Ind. Fare', fontsize = 12)\n", "d5.legend(loc='best')\n", "d5.set(xlabel=\"IndFare\", ylabel=\"No. of Passengers\")\n", "plt.show()\n", "\n", "d7 = sns.distplot(surv[surv.Survived == 'Yes']['NameLength'].dropna().values, color='Green', ax = axes[3], label = 'Survived')\n", "d8 = sns.distplot(surv[surv.Survived == 'No']['NameLength'].dropna().values, color='Red', ax = axes[3], label = 'Not Survived')\n", "d7.set_title('Survived vs. Name Length', fontsize = 12)\n", "d7.legend(loc='best')\n", "d7.set(xlabel=\"NameLength\", ylabel=\"No. of Passengers\")\n", "plt.show()"]}, {"metadata": {"_uuid": "7344d837fd196c81edee7afbf9cad613a10a71d8", "_cell_guid": "0f40ec6d-fd00-4644-984a-0da36c907f9b"}, "cell_type": "markdown", "source": ["*Above plots show more detail in variation.*"]}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "7cf7971edea76cff7794fa084a10d64df9d132a5", "_cell_guid": "59d01fa9-c663-4f4d-a667-a45e2b886337"}, "cell_type": "code", "outputs": [], "source": ["# Caluculate assciation between 2 columns - Cramer's V score [Categorical Features]\n", "# Change Survived data type so that it does not mess up calculation below. (change back to integer before applying ML models)\n", "comb_data[\"Survived\"] = comb_data[\"Survived\"].astype(str)\n", "for i in comb_data.select_dtypes(include=['category']).columns:\n", "    col_1 = i\n", "    for j in comb_data.select_dtypes(include=['category']).columns:\n", "        col_2 = j\n", "        if col_1 == col_2:\n", "            break\n", "        confusion_matrix = pd.crosstab(comb_data[col_1], comb_data[col_2])\n", "        chi2 = ss.chi2_contingency(confusion_matrix)[0]\n", "        n = confusion_matrix.sum().sum()\n", "        phi2 = chi2/n\n", "        r,k = confusion_matrix.shape\n", "        phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))\n", "        rcorr = r - ((r-1)**2)/(n-1)\n", "        kcorr = k - ((k-1)**2)/(n-1)\n", "        Cramer_V = np.sqrt(phi2corr / min( (kcorr-1), (rcorr-1)))\n", "        if Cramer_V > 0.5:\n", "            print(\"The Cramer's V score bettween \", col_1, \" and \", col_2, \" is : \", (Cramer_V))\n", "        result = Cramer_V"]}, {"metadata": {"_uuid": "5ecacb000465e9260a394088c4cb09cc39152596", "_cell_guid": "0db307a7-3b18-4500-b3b5-64b0f914ab2b"}, "cell_type": "markdown", "source": ["*These values shows correlation among categorical features.<br>\n", "Clearly Title  and  Sex  is very highly correlated. We have to pick one to avoid Multicollinearity.*"]}, {"execution_count": null, "metadata": {"scrolled": false, "collapsed": true, "_uuid": "3e11a1346fa453d2864708e783a96ba420f1fcde", "_cell_guid": "4aebdd9b-d8f1-4ac6-ad4b-b8e5a11cbcca"}, "cell_type": "code", "outputs": [], "source": ["# Visulaizing survival data across categorical columns, using Fare numerical column on Y-axis.\n", "f, axes = plt.subplots(4,3, figsize = (28, 21), sharey=True)\n", "for i, col_name in enumerate(comb_data.select_dtypes(include=['category']).columns):\n", "    row = i // 3\n", "    col = i % 3\n", "    ax_curr = axes[row, col]    \n", "    sns.violinplot(data=surv, x=col_name, y='Fare', hue='Survived', ax = ax_curr)\n", "plt.show()"]}, {"execution_count": null, "metadata": {"scrolled": false, "collapsed": true, "_uuid": "ce7ded021e4e8fbb1a640a41ff8156dbb12f7db8", "_cell_guid": "73438837-154d-40e3-bae4-34aedcd399f7"}, "cell_type": "code", "outputs": [], "source": ["# Creating bar plots. \n", "# I will convert Survived values to number type so that it can be used for bar plots.\n", "surv['Survived'].replace(['No','Yes'],[0,1], inplace=True)\n", "\n", "f, axes = plt.subplots(3,4, figsize = (28, 16), sharey=True)\n", "for i, col_name in enumerate(comb_data.select_dtypes(include=['category']).columns):\n", "    row = i // 4\n", "    col = i % 4\n", "    ax_curr = axes[row, col]\n", "    sns.barplot(x=col_name, y='Survived', data=surv, ax = ax_curr)\n", "plt.show()"]}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "1e9887a7b55d9cb767a26e7f72669d347a257a92", "_cell_guid": "6d894e23-78d3-48e9-a25b-ceb07df7b5ec"}, "cell_type": "code", "outputs": [], "source": ["# Find columns which have proportionally more death counts\n", "f, axes = plt.subplots(3,4, figsize = (28, 21), sharey=True)\n", "for i, col_name in enumerate(comb_data.select_dtypes(include=['category']).columns):\n", "    row = i // 4\n", "    col = i % 4\n", "    ax_curr = axes[row, col]    \n", "    ax1 = sns.barplot(x=col_name, y='Fare', data=surv[surv.Survived == 0], color='orange', alpha = 0.5, estimator=lambda col_name: len(col_name) / len(surv[surv.Survived == 0]) * 100, ax = ax_curr)\n", "    ax2 = sns.barplot(x=col_name, y='Fare', data=surv[surv.Survived == 1], color='blue', alpha = 0.5, estimator=lambda col_name: len(col_name) / len(surv[surv.Survived == 1]) * 100, ax = ax_curr)\n", "    ax1.set_ylabel('Percentage')\n", "plt.show()"]}, {"execution_count": null, "metadata": {"scrolled": false, "collapsed": true, "_uuid": "ceeade78c6b20b77b815d11ede6acfff17c0d9db", "_cell_guid": "dfa2efbd-a1d6-4d77-b493-54682f57708f"}, "cell_type": "code", "outputs": [], "source": ["# In last set of charts, I will narrow down data by applying filters and combination of categories to visualize where exactly most no. of dead passengers count is.\n", "# Factorplots are good to see counts across categorical columns, as shown below.\n", "\n", "f1 = sns.factorplot(x='FamSize', data=surv, hue='Survived', kind='count', col='Sex', size=6)\n", "plt.show()"]}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "a7c87bf13477bd6c78a34d8a4a316f286d15e3b5", "_cell_guid": "0c0be0d5-1d47-4718-816c-815579d86972"}, "cell_type": "code", "outputs": [], "source": ["# Filtered passengers count data across categories\n", "f2 = sns.factorplot(x='Embarked', data=surv, hue='Survived', kind='count', col='Sex', size=6)\n", "plt.show()"]}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "4d65a5ccff0bb52d4a2fc115cb524414d51e9aa6", "_cell_guid": "f79445eb-ab98-4218-a6b4-033ac36cc61f"}, "cell_type": "code", "outputs": [], "source": ["# Filtered passengers count data across categories\n", "f8 = sns.factorplot(x='FamSize', data=surv[(surv['Title'] == 'Mr') & (surv['Cabin'] == 'N')], hue='Survived', kind='count', col='Embarked', size=6)\n", "\n", "plt.show()"]}, {"metadata": {"_uuid": "ed02c3fa646d10ace4e4c843af4eda369d9bfd0a", "_cell_guid": "be7acf24-b538-4ef5-8137-2b54da9a7603"}, "cell_type": "markdown", "source": ["*After extensive visualization of underlying data, I am ready to go on to next step.*"]}, {"metadata": {"_uuid": "65408c756727b11f211826323aa1195f156aaa6e", "_cell_guid": "9e36868f-3ab4-4a26-93c3-07cbd581224c"}, "cell_type": "markdown", "source": [" #### **Step 5 - Prepare the data for Machine Learning algorithms**"]}, {"metadata": {"_uuid": "cfb6265d48b6de2b18017f59d30bf59b6b680003", "_cell_guid": "1b9b85ef-3814-4408-bc65-dbb341fde43f"}, "cell_type": "markdown", "source": ["**Process further and create train, target and test dataset**"]}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "534b565315648620338135bc4c9aba8ab8ff439c", "_cell_guid": "9ee1b652-6024-4566-826d-22de91cc1e85"}, "cell_type": "code", "outputs": [], "source": ["# Check combined data\n", "comb_data.head()"]}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "e8816b363763d9aa1bd1c00568e16e160ba064a6", "_cell_guid": "13ce5283-0427-40a7-8453-e7e2d16fa4db"}, "cell_type": "code", "outputs": [], "source": ["# Save processed comb_data [for sanity check before splitting train and test data]\n", "'''comb_data.to_csv('comb_data_Titanic.csv')'''\n", "\n", "# Check data types\n", "comb_data.dtypes"]}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "a1caf5bd19c5c624227db287ca54c06439958fd0", "_cell_guid": "f7343bdc-6489-4ac7-a1c3-140b315e655a"}, "cell_type": "code", "outputs": [], "source": ["# Convert binary categorical columns to integer 0/1\n", "comb_data['Survived'].replace(['No','Yes'],[0,1], inplace=True)\n", "\n", "comb_data['Sex'].replace(['Male','Female'],[0,1], inplace=True)\n", "comb_data[\"Sex\"] = comb_data[\"Sex\"].astype(int)\n", "\n", "# Change back PassengerId to integer before applying ML models\n", "comb_data[\"PassengerId\"] = comb_data[\"PassengerId\"].astype(int)\n", "\n", "# Drop columns to avoid overfitting\n", "# comb_data = comb_data.drop(labels = [\"Age\", \"Sex\", \"Parch\", \"SibSp\", \"IndFare\", FareGroup\", \"AgeCat\", \"IndFare\", \"AgeData\"],axis = 1)\n", "# comb_data = comb_data.drop(labels = [\"Sex\", \"FareGroup\", \"AgeCat\", \"IndFare\", \"AgeData\"],axis = 1)\n", "comb_data = comb_data.drop(labels = [\"Sex\", \"SibSp\", \"FareGroup\", \"AgeCat\", \"AgeData\"],axis = 1)\n", "\n", "# Transform categorical features in to dummy variables\n", "comb_data[\"DataType\"] = comb_data[\"DataType\"].astype(str) # to exclude from dummy function\n", "\n", "# Get the list of category columns\n", "cat_col_names = comb_data.select_dtypes(include=['category']).columns\n", "\n", "comb_data = pd.get_dummies(comb_data, columns=cat_col_names, prefix=cat_col_names)\n", "comb_data.head()"]}, {"metadata": {"_uuid": "ccaeacdfd416a870c7333dfc4975b59e5c1c31d2", "_cell_guid": "9e84d50c-e94a-4db0-98c4-8c2cb02e78e9"}, "cell_type": "markdown", "source": ["*I tried multiple combination of features and kept the one set which gave me best accuracy.*"]}, {"execution_count": null, "metadata": {"scrolled": false, "collapsed": true, "_uuid": "fdb9baee088b3222e2985bb0f75b0a849f69470a", "_cell_guid": "de21cfad-a792-46cb-88f8-1d34ddeb9b15"}, "cell_type": "code", "outputs": [], "source": ["# Create train and test subset from survival column\n", "print(comb_data.shape)\n", "train = comb_data[comb_data.DataType == 'train']\n", "print(train.shape)\n", "\n", "test = comb_data[comb_data.DataType == 'test']\n", "print(test.shape)\n", "\n", "train_id = train[\"PassengerId\"]\n", "test_id = test[\"PassengerId\"]\n", "\n", "train[\"Survived\"] = train[\"Survived\"].astype(int)\n", "y_train = train[\"Survived\"]\n", "print(y_train.shape)\n", "print(y_train.head())\n", "\n", "X_train = train.drop(labels = [\"Survived\", \"PassengerId\", \"DataType\"],axis = 1)\n", "print(X_train.shape)\n", "print(X_train.head())\n", "\n", "X_test = test.drop(labels = [\"Survived\", \"PassengerId\", \"DataType\"],axis = 1)\n", "print(X_test.shape)\n", "print(X_test.head())\n", "\n", "# Make sure there is no null values in train and test data and also no. of categories in categorical values are equal\n", "print(\"-------------Null values in Train set------------------\")\n", "print(X_train.isnull().values.any())\n", "    \n", "print(\"-------------Null values in Test set------------------\")\n", "print(X_test.isnull().values.any())"]}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "90176b938c0ec4cc3637b57d5abb206a96c91d6e", "_cell_guid": "06eb5dbc-7a50-4fdf-8163-cb60039b0b38"}, "cell_type": "code", "outputs": [], "source": ["# Check number of columns and name of columns match between X_train and X_test\n", "print(X_train.shape)\n", "print(X_test.shape)\n", "print(set(X_train.columns) == set(X_test.columns))\n", "print('--------columns present in X_train but not in X_test-------')\n", "missing_col_tt = [i for i in list(X_train) if i not in list(X_test)]\n", "print(missing_col_tt)\n", "print('--------columns present in X_test but not in X_train-------')\n", "missing_col_tr = [i for i in list(X_test) if i not in list(X_train)]\n", "print(missing_col_tr)\n", "\n", "# Drop these columns and test again\n", "X_train = X_train.drop(missing_col_tt, axis=1)\n", "X_test = X_test.drop(missing_col_tr, axis=1)\n", "\n", "print(X_train.shape)\n", "print(X_test.shape)\n", "print(set(X_train.columns) == set(X_test.columns))\n", "print('--------columns present in X_train but not in X_test-------')\n", "missing_col_tt = [i for i in list(X_train) if i not in list(X_test)]\n", "print(missing_col_tt)\n", "print('--------columns present in X_test but not in X_train-------')\n", "missing_col_tr = [i for i in list(X_test) if i not in list(X_train)]\n", "print(missing_col_tr)"]}, {"metadata": {"_uuid": "3a6fb1163fd7ac56de61c861827b4febed0482ad", "_cell_guid": "e14898c4-9086-48ba-9f00-351c59a2f3d1"}, "cell_type": "markdown", "source": ["*I am not sure if above step was necessary. I assume order and no. columns in train and test data should be same.*"]}, {"metadata": {"_uuid": "fd0a51c42832096abd2c73712847d58bc201b738", "_cell_guid": "0d6e6042-9999-4c1a-b64c-f065ca94ca92"}, "cell_type": "markdown", "source": ["#### **Step 6 - Select and train a model** "]}, {"metadata": {"_uuid": "6e235d3a9c45f35b5ac879d3247910ee7daa6f0d", "_cell_guid": "8021caf1-fee2-4226-a8d7-4e872933737d"}, "cell_type": "markdown", "source": ["**Visualize how Machine Learning models works on classification with just 2 numerical features**"]}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "1650956c43bc2efbd7550d27352c74063a9f3fe1", "_cell_guid": "666cdeab-ce9c-4ac4-9f60-919fb69f6d71"}, "cell_type": "code", "outputs": [], "source": ["# Extract numerical columns from train dataseet\n", "X_num = X_train.iloc[:, [0, 1]]\n", "print(X_num.head())"]}, {"metadata": {"_uuid": "287ac8c0465d637fd8d2d44da7e51f839cf9b47b", "_cell_guid": "07f4ce42-eaee-4f46-a97f-665db71d722b"}, "cell_type": "markdown", "source": ["*'X_num' and 'y_train' will be used to visualize Machine Learning classification models with two numerical independent features*"]}, {"execution_count": null, "metadata": {"scrolled": false, "collapsed": true, "_uuid": "bd5ca99f8aeab101ff6dea141d04dcae5e02ef11", "_cell_guid": "b5b27f8b-1670-4aad-af05-f7cf7d5fc58e"}, "cell_type": "code", "outputs": [], "source": ["# Run function to plot graphs\n", "plt_start = time.time()\n", "plot_class_models_two_num_features(X = X_num, y = y_train, clf_dict = {k: clf_dict[k] for k in clf_dict.keys() & \\\n", "{'clf_Log_reg', 'clf_KNN', 'clf_RF', 'clf_ExTree', 'clf_XGBoost', 'clf_MLP'}}, title = 'Age & Fare')\n", "plt_end = time.time()"]}, {"metadata": {"_uuid": "fd1f293b87e2310137e0be5b272e0018e71e6222", "_cell_guid": "63d59747-8156-481c-8576-c30ebe219f44"}, "cell_type": "markdown", "source": ["*As expected, using numerical columns only (fare and age) do not give good accuracy.<br>\n", "However, nice way to see how algorithms work on classification.*"]}, {"metadata": {"_uuid": "f4865ecd2b0d642ed170f81ee3384aa1bc05964c", "_cell_guid": "f12f8ba4-f9b8-4b3c-b2ed-88509affecb0"}, "cell_type": "markdown", "source": ["**Comparing Classification Machine Learning models with all independent features. [Use 'X_train' and 'y_train']**"]}, {"execution_count": null, "metadata": {"scrolled": true, "collapsed": true, "_uuid": "546c9644892935a2ba8eb0f4c0bbdfa2c112efbc", "_cell_guid": "85dbd4da-3c1f-443c-9b2f-bc4a2fcfe04a"}, "cell_type": "code", "outputs": [], "source": ["# Run function to evaluate various classification models [Metrics and cross_val-score]\n", "clf_cross_val_score_and_metrics(X=X_train, y=y_train, clf_dict=clf_dict, CVS_scoring = \"accuracy\", CVS_CV=kfold)"]}, {"metadata": {"_uuid": "4ea578ff6cf76f7238eaa1f52e963272a24cddd2", "_cell_guid": "87a6aafc-f912-4f86-b070-2e3c430ce12b"}, "cell_type": "markdown", "source": ["**Extensive GridSearch with valuable parameters / score in a dataframe**"]}, {"execution_count": null, "metadata": {"scrolled": false, "collapsed": true, "_uuid": "c811ae6dbd5c462d5d64a2f0f667f186f71dc789", "_cell_guid": "71fc68d2-aa82-458f-b678-5e6c622a9130"}, "cell_type": "code", "outputs": [], "source": ["# Get GridSearch results in a dataframe\n", "gs_start = time.time()\n", "gs_metrics = clf_GridSearchCV_results(gs_metrics, X_train=X_train, y_train=y_train, GS_scoring = \"accuracy\", GS_CV=kfold)\n", "gs_end = time.time()"]}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "4468fc8ffd810ccf26f74af6f1b511ed9c4f7e76", "_cell_guid": "cdcec8e9-8831-4060-bbb3-232cdc106573"}, "cell_type": "code", "outputs": [], "source": ["# Check GridSearch metric data\n", "'''gs_metrics.to_csv('Titanic_GS_Result.csv')'''\n", "gs_metrics"]}, {"metadata": {"_uuid": "d0502940a68f62b4a884f0777ccc2a462c267b78", "_cell_guid": "893b8935-0a82-408e-8320-0412d0ae0f0c"}, "cell_type": "markdown", "source": ["**Find best estimators**"]}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "f219ab004d5e42c675d8d0dfe70d7e8a3849cbf6", "_cell_guid": "1053f05e-b2cc-416e-a14f-e63feb982549"}, "cell_type": "code", "outputs": [], "source": ["# Extract best estimators\n", "Best_Estimator_RF = gs_metrics.iloc[1, 5]\n", "Best_Estimator_XGB = gs_metrics.iloc[5, 5]\n", "Best_Estimator_MLP = gs_metrics.iloc[2, 5]\n", "Best_Estimator_ExT = gs_metrics.iloc[4, 5]\n", "Best_Estimator_GB = gs_metrics.iloc[6, 5]\n", "Best_Estimator_SVC = gs_metrics.iloc[7, 5]\n", "\n", "# AdaBoost for feature importance plot\n", "Best_Estimator_AdaB = gs_metrics.iloc[3, 5]"]}, {"metadata": {"_uuid": "82cea4714a921d7e7d29faee93673d5a917edd3b", "_cell_guid": "916c6582-e2ef-4650-a7c8-0c1d41e78d21"}, "cell_type": "markdown", "source": ["**Plot learning curves**"]}, {"execution_count": null, "metadata": {"scrolled": false, "collapsed": true, "_uuid": "9bb54d52723e9c637b9127bf3d6b43adf5d943ae", "_cell_guid": "aa4bd0b0-972b-4b60-a3e3-2081e98532d7"}, "cell_type": "code", "outputs": [], "source": ["# Run function to plot learning curves for top 4 models\n", "plot_learning_curve(Best_Estimator_RF,\"RF mearning curves\",X_train,y_train,cv=kfold)\n", "plot_learning_curve(Best_Estimator_ExT,\"ExtraTrees learning curves\",X_train,y_train,cv=kfold)\n", "plot_learning_curve(Best_Estimator_XGB,\"XGB learning curves\",X_train,y_train,cv=kfold)\n", "plot_learning_curve(Best_Estimator_MLP,\"MLP learning curves\",X_train,y_train,cv=kfold)"]}, {"metadata": {"_uuid": "97f5832d54970bf1966afafd25bd618037adf0c1", "_cell_guid": "fb624e68-86cb-4b1a-a0f4-cc5d35a1e948"}, "cell_type": "markdown", "source": ["**Feature Importance**"]}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "fb900832ece6d479166113d27706e9455f29efd1", "_cell_guid": "9910d385-e087-44ad-8022-5c19e46a8d66"}, "cell_type": "code", "outputs": [], "source": ["nrows = ncols = 2\n", "fig, axes = plt.subplots(nrows = nrows, ncols = ncols, sharex=\"all\", figsize=(25,20))\n", "\n", "names_clf = [(\"ExtraTrees\",Best_Estimator_ExT),(\"RandomForest\",Best_Estimator_RF),(\"XGBoosting\",Best_Estimator_XGB), (\"AdaBoosting\", Best_Estimator_AdaB)]\n", "\n", "nclf = 0\n", "for row in range(nrows):\n", "    for col in range(ncols):\n", "        name = names_clf[nclf][0]\n", "        clf = names_clf[nclf][1]\n", "        # Plot feature importance\n", "        feature_importance = clf.feature_importances_\n", "        # make importances relative to max importance\n", "        feature_importance = 100.0 * (feature_importance / feature_importance.max())\n", "        sorted_idx = np.argsort(feature_importance)[::-1][:32]\n", "        pos = feature_importance[sorted_idx][:32]\n", "        \n", "        g = sns.barplot(y=X_train.columns[sorted_idx][:32],x = pos, orient='h', palette='inferno', ax=axes[row][col])\n", "        g.set_xlabel(\"Relative importance\",fontsize=12)\n", "        g.set_ylabel(\"Features\",fontsize=12)\n", "        g.tick_params(labelsize=9)\n", "        g.set_title(name + \" feature importance\")\n", "        nclf += 1"]}, {"metadata": {"_uuid": "004cf875a6ba4300a6f6d349ebde7e3caf6c0c89", "_cell_guid": "cf16abe8-b305-4e14-8616-20c7cb3f49a2"}, "cell_type": "markdown", "source": ["#### **Step 7 - Fine-tune your model**"]}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "8c41fd950b7e498b4f7764f4aee646b5846267f3", "_cell_guid": "ea4ac348-47a8-4b0b-958f-9292e0d02192"}, "cell_type": "code", "outputs": [], "source": ["vote_clf = VotingClassifier(estimators=[('rfc', Best_Estimator_RF),\n", "('ext', Best_Estimator_ExT), ('xgb',Best_Estimator_XGB)], voting='soft', weights=[1, 1, 1], n_jobs=-1)"]}, {"metadata": {"_uuid": "1fca1666a78e99390a515ad85ae549757440f6f1", "_cell_guid": "0aa632f5-a195-48db-a763-a09d11bcb55e"}, "cell_type": "markdown", "source": ["#### **Step 8 - Predict and present your solution**"]}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "9221d5da6e522f080dc2367320d018da0d39ac20", "_cell_guid": "a0f8ada9-8140-412c-819d-89fba407b95a"}, "cell_type": "code", "outputs": [], "source": ["# Fitting / Predicting using Voting Classifier\n", "# vote_clf.fit(X_train, y_train)\n", "# y_pred = vote_clf.predict(X_test)\n", "\n", "# Fitting / Predicting using Random Forest classifier\n", "# rfc = RandomForestClassifier(max_depth=8, n_estimators = 500, n_jobs=-1, random_state=rand_st)\n", "# rfc.fit(X_train, y_train)\n", "# y_pred = rfc.predict(X_test)\n", "\n", "# Fitting / Predicting using Extra Tree classifier\n", "Best_Estimator_ExT.fit(X_train, y_train)\n", "y_pred = Best_Estimator_ExT.predict(X_test)\n", "print(y_pred)"]}, {"metadata": {"_uuid": "ae62b42d2d63dfc21a7af9fcffdd094e02ce1990", "_cell_guid": "2eb187e3-ccab-4b86-9744-e7b8ff1d9f6a"}, "cell_type": "markdown", "source": ["*Extra tree gave me better result than voting classifier or Random Forest Classifier*"]}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "a92682048d5cc12ab678b196368c6580ef122cf6", "_cell_guid": "16643c8c-d3a2-4e82-9518-b69013321c85"}, "cell_type": "code", "outputs": [], "source": ["# Combine PassengerId and prediction\n", "Titanic_prediction = np.vstack((test_id, y_pred))"]}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "aae4eed86be94e6c163f2b1f6277ea7ff5e265b2", "_cell_guid": "355820d4-f037-4d76-84df-76f3117e23a9"}, "cell_type": "code", "outputs": [], "source": ["# Create output file\n", "np.savetxt('Titanic_Kaggle_Result.csv', np.transpose(Titanic_prediction), delimiter=',', fmt=\"%s\")"]}, {"metadata": {"_uuid": "dee54f4c092a1019c92d108d243762565db39054", "_cell_guid": "336885b8-d104-46d7-9eb3-75aa00b2be29"}, "cell_type": "markdown", "source": ["#### **Step 9 - Final words!**"]}, {"metadata": {"collapsed": true, "_uuid": "d612d6ee7a400d674a67dc05485f867f50251011", "_cell_guid": "92313724-7771-41c2-b5ec-cfa236ae53f6"}, "cell_type": "raw", "source": ["# 1-  My first attempt (version 1) was without looking into any solution online.\n", "# 2-  This version is modified version after looking at works of others.\n", "      e.g. - 'Titanic Top 4% with ensemble modeling' kernel by Yassine Ghouzam, PhD\n", "# 3-  I underestimated feature engineering before.\n", "# 4- Got slighlty better result of 0.80332 [Top 8%]\n", "# 5- I am more interested in approach and doing all the steps correctly along with necessary checks.\n", "# 6- Let me know if I missed anything critical"]}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "6af5a97a491bd93a105984597d67c5db9deafd92", "_cell_guid": "35a5ab03-c34e-4507-ac64-3718667c5903"}, "cell_type": "code", "outputs": [], "source": ["end = time.time()\n", "print('Time taken to plot ML models : ' + str(\"{:.2f}\".format((plt_end - plt_start)/60)) + ' minutes')\n", "print('Time taken to perform Grid Search : ' + str(\"{:.2f}\".format((gs_end - gs_start)/60)) + ' minutes')\n", "print('Total running time of the script : ' + str(\"{:.2f}\".format((end - start)/60)) + ' minutes')"]}], "nbformat_minor": 1, "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}, "language_info": {"version": "3.6.3", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "name": "python", "codemirror_mode": {"name": "ipython", "version": 3}, "mimetype": "text/x-python", "file_extension": ".py"}}, "nbformat": 4}