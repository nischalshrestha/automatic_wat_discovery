{"cells":[{"metadata":{"_uuid":"12fd53987e2a4b54fa2b1603c162a0515aceb3ed"},"cell_type":"markdown","source":"This notebook is a practice example of a Titanic binary classification problem. It is based on Dataquest.io Titanic tutorial with the purpose of learning basics of machine learning."},{"metadata":{"_uuid":"1ce6c8392a9dd3ba779bef070330a332969703bb"},"cell_type":"markdown","source":"**Prepare the environment and data**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Load libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Load data\ntest = pd.read_csv(\"../input/test.csv\") # contains data for testing the model\ntrain = pd.read_csv(\"../input/train.csv\") # contains data for defining and fitting the model\n\n# Check out the dimensions of the dataframe\nprint(\"Dimensions of train: {}\".format(train.shape))\nprint(\"Dimensions of test: {}\".format(test.shape))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3760aec2eccc489675ad02adb6dec192113e17f1"},"cell_type":"markdown","source":"**Explore and understand the data set**"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Let's look at the first rows of the train data set\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef5d067bac533f3895b3e535327e61463dd1b237"},"cell_type":"code","source":"# The column \"Survived\" contains 0 if passenger did not survive and 1 if they did, we can segment our data by sex and calculate the mean of this column.\n# DataFrame.pivot_table()\n\nsex_pivot = train.pivot_table(index=\"Sex\", values=\"Survived\")\nsex_pivot.plot.bar()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1fadcccdefae37a38fd9ddc8fed6e4907a03c7ea"},"cell_type":"code","source":"# Let's classify and pivot the Pclass column as well\n\nclass_pivot=train.pivot_table(index=\"Pclass\", values=\"Survived\")\nclass_pivot.plot.bar()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1a63e1450e389e442048a2a36e080fe47be70ed4"},"cell_type":"code","source":"# Exploring and converting the age column\n# The Sex and PClass are CATEGORICAL(meaning that the values represented a few separate options - female/male)\n\ntrain[\"Age\"].describe()\n\n# there is only 714 values (compared to 814 in the data set) - there must be some missing values\n# also this column is a continuous NUMERICAL column\n# for continuous numerical values it is good to use histogram\n# let's compare those who survived to those who died across different age ranges","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2bf87d6fe751978fe893b1def7132ab3d091a73a"},"cell_type":"code","source":"survived = train[train[\"Survived\"]==1]\ndied = train[train[\"Survived\"]==0]\n\nsurvived[\"Age\"].plot.hist(alpha=0.5, color='red', bins=50)\ndied[\"Age\"].plot.hist(alpha=0.5, color='blue', bins=50)\nplt.legend(['Survived', 'Died'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3a6ca33a9a8241377c32427cef1d02cb97dcd050"},"cell_type":"code","source":"# We can try to separate the continuous feature into categorical by splitting it into ranges using the pandas.cut() function\n# pandas.cut() has two parameters\n    # 1. the column we would like to cut\n    # 2. list of numbers which define the boundaries of our cuts\n\n# Next we need to remember to make any changes we perform on the train data also on the test data because otherwise we will not be able to use the model\n# And we need to remember to handle the missing values\n\n# We want to create a function that will do two things:\n    # 1. Uses the pandas.fillna() method to fill all of the missing values with -0.5\n    # 2. Cuts the Age column into six segments\n\ndef process_age (df, cut_points, label_names):\n    df[\"Age\"] = df[\"Age\"].fillna(-0.5)\n    df[\"Age_categories\"] = pd.cut(df[\"Age\"], cut_points, labels=label_names)\n    return df\n\ncut_points = [-1, 0, 5, 12, 18, 35, 60, 100]\nlabel_names = [\"Missing\", \"Infant\", \"Child\", \"Teenager\", \"Young Adult\", \"Adult\", \"Senior\"]\n\ntrain = process_age(train, cut_points, label_names)\ntest = process_age(test, cut_points, label_names)\n\npivot = train.pivot_table(index=\"Age_categories\", values=\"Survived\")\npivot.plot.bar()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d0b0f250f678ca67a0b807dfe98e3b76018e83e8"},"cell_type":"code","source":"# Preparing data for machine learning\n\n# most machine learning models can't understand text labels so we have to convert our values into numbers\n\n# we need to be sure that we don't imply any numeric relationship where there is not one.\n\n# Values in the Pclass column are 1, 2, 3\n\ntrain[\"Pclass\"].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3f22a7fdbcb9a78c8e8fe110935eb08741606664"},"cell_type":"code","source":"# Now we should remove the relationships between the different types of classes\n# Class 2 is not \"worth\" double what class 1 is and class 3 is not \"worth\" triple what class 1 is\n\n# To remove this relationship, we can split this column into 3 new columns with 0,1 values\n# To do this a function pandas.get_dummies() will help us\n\ndef create_dummies(df,column_name):\n    dummies = pd.get_dummies(df[column_name], prefix=column_name)  # prefix parameter means what the new dummy columns will be named like\n    df = pd.concat([df,dummies], axis=1)  # add the new columns to my dataframe\n    return df\n\nfor column in [\"Pclass\", \"Sex\", \"Age_categories\"]:\n    train = create_dummies(train,column)  # passing into the function parameter the name of the column defined in the for loop each one by one\n    test = create_dummies(test,column) # don't forget to apply also to the test data set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bb20166ec3b7948024a55b350136bd33d665e254"},"cell_type":"code","source":"# The first model we will use is called Logistic Regression\n# This is usually the first model we will train when performing classification\n\n# 1. import the appropriate class from scikit-learn library\nfrom sklearn.linear_model import LogisticRegression\n\n# 2. create object\nlr = LogisticRegression()\n\n# 3. fit our model\ncolumns = [\"Pclass_2\", \"Pclass_3\", \"Sex_male\"]\nlr.fit(train[columns], train[\"Survived\"])  # x = train[columns] a two-dimensional array (dataframe), y = train[\"Survived\"] a one-dimensional array (series) that we want to predict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b3d98b3a6bc2964c95b55924b46daa54d1422506"},"cell_type":"code","source":"# Let's try to train our model with all dummy columns we created\n\ncolumns = ['Pclass_1', 'Pclass_2', 'Pclass_3', 'Sex_female', 'Sex_male',\n       'Age_categories_Missing','Age_categories_Infant',\n       'Age_categories_Child', 'Age_categories_Teenager',\n       'Age_categories_Young Adult', 'Age_categories_Adult',\n       'Age_categories_Senior']\n\nlr = LogisticRegression()\nlr.fit(train[columns], train[\"Survived\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"35f0f92928c6d035fd1a199fd992c235c870fd3a"},"cell_type":"code","source":"# Even though we have a test dataframe to test out our model for predictions, that dataframe does not have the column \"Survived\" against which we could compare how accurate our model is\n# Therefore we still need to split our train data into two dataframes to play around with optimization of our model\n\n# We are splitting our train data set into two:\n    # 1. one part to train our model (usually 80% of the observations)\n    # 2. one part to make predictions (usually 20% of the observations)\n    \n# Now we need to rename the Kaggle test data as \"holdout\" data\nholdout = test\n\n# Now using scikit-learn library and function train_test_split we are going to split our training data frame\nfrom sklearn.model_selection import train_test_split\n\nall_x = train[columns]\nall_y = train[\"Survived\"]\n\ntrain_x, test_x, train_y, test_y = train_test_split(all_x, all_y, test_size=0.20, random_state=0)\n# train_test_split take several parameters:\n    # x and y = all the data we want to split (features and target)\n    # test_size = we can specify the proportion we want to split the data\n    # random_state = let's us split the data everytime the same so we can get replicable results\n# train_test_split then returns four objects: train_x, test_x, train_y, test_y","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"767b81033d88fb5c54103113cab9a9043f1bc4dc"},"cell_type":"code","source":"# In the next step we are going to make some predictions and measure their accuracy\n\n# 1. Fit our model again to the split train data\nlr = LogisticRegression()\n\nlr.fit(train_x, train_y)\n\n# 2. Make predictions\npredictions = lr.predict(test_x)\n\n# 3. Measure our accuracy\nfrom sklearn.metrics import accuracy_score\naccuracy = accuracy_score(test_y, predictions)\n\nprint(accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e87214049f80673aac2c75ab558f0d8c1aa8c79a"},"cell_type":"code","source":"# Since our data set is pretty small, it is very likely that our model is overfitting\n# To understand better the performance of our model we can use CROSS VALIDATION\n# It means to train and test our model on different SPLITS of our data and then average the accuracy scores\n\n# K-FOLD cross validation\nfrom sklearn.model_selection import cross_val_score\n\nlr = LogisticRegression()\nscores = cross_val_score(lr, all_x, all_y, cv=10)\nscores.sort()\naccuracy = scores.mean()\n\nprint(scores)\nprint(accuracy)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"da268e2633f66c685210dd2ddac28648cea61ccf"},"cell_type":"code","source":"# Let's make our final predictions\n\nlr = LogisticRegression()\nlr.fit(all_x, all_y)\nholdout_predictions = lr.predict(holdout[columns])\n\nprint(holdout_predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c00f71ba61751b18b4d13c230cabff16aac77879"},"cell_type":"code","source":"# Prepare submission\nholdout_ids = holdout[\"PassengerId\"]\nsubmission_df = {\"PassengerId\": holdout_ids, \"Survived\": holdout_predictions}\nsubmission = pd.DataFrame(submission_df)\n\nsubmission.to_csv(\"submission_df\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}