{"cells":[{"metadata":{"_uuid":"2b55e09a2c7e51b9283d5755d15bcf09ff0ff20d"},"cell_type":"markdown","source":"# Introduction & References used"},{"metadata":{"_uuid":"6d68744b14e7d0bdebc870eab465a5622ad64959"},"cell_type":"markdown","source":"### Dataset was obtained from https://www.kaggle.com/c/titanic/\n\n### Goal: Train a model with 891 training examples to predict the survival outcome (0,1) of 418 test examples\n\n### 1. Importing of Libraries and dataset\n\n### 2. Categorical Variable Analysis\n\n### 3. Numerical Variable Analysis\n\n### 4. Feature Engineering & Correlation\n\n### 5. Transformation of test set\n\n### 6. Modeling & Predictions\n\n#### - Created and used a logistic regression model from scratch via gradient descent\n\n#### - Applied SVM & Random Forest by using the sklearn library with some hyperparameter tuning\n\n#### - Applied a simple artifical neural network with Keras/Tensorflow\n\n### 7. References\n\n#### - https://www.kaggle.com/yassineghouzam/titanic-top-4-with-ensemble-modeling on feature engineering\n\n#### - A - Z Machine Learning Udemy Course by Krill on Artifical Neural Network"},{"metadata":{"_uuid":"624b156f1dbd8aeeb1e62e3b85230fd972df9956"},"cell_type":"markdown","source":"# 1. Importing of libraries"},{"metadata":{"trusted":true,"_uuid":"6239d7c3e2990f04f45a75e127db2a18bd9e103f"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sb\n\nfrom pylab import rcParams\nrcParams['figure.figsize'] =5,4\n\nplt.rcParams[\"font.weight\"] = \"bold\"\nplt.rcParams[\"axes.labelweight\"] = \"bold\"\nplt.rcParams['figure.edgecolor']='black'\nplt.rcParams['patch.force_edgecolor']= True\n\nsb.set_style('whitegrid')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0263211846de9c2a36799a358d60c012e99c1ae9"},"cell_type":"code","source":"dataset = pd.read_csv(\"../input/train.csv\")\n# Dropping passenger id and ticket which have little impact on the survival\ndataset = dataset.drop(['PassengerId','Ticket'],axis=1)\ndataset.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5a8886a2804d5839407136f8a1196912f37b3f66"},"cell_type":"markdown","source":"# 2. Categorical Variable Analysis"},{"metadata":{"_uuid":"a0ea3e7f25915ea6188680f4ac45985672bfbfbb"},"cell_type":"markdown","source":"## 2A. Pclass Variable"},{"metadata":{"trusted":true,"_uuid":"a326ee4399292fc0273cdfa4ba0b38669561ea91"},"cell_type":"code","source":"sb.countplot(x = dataset['Pclass'], hue = dataset['Survived'])\nplt.legend(loc='best')\nplt.title('No of Survivals based on Pclass',weight='bold')\nplt.annotate(s='Pclass 3 highest\\nnumber of survival',xy=(1.5,200),xytext=(0,250),\n             arrowprops=dict(facecolor='black', shrink = 0.1),fontsize=11)\n\nsb.factorplot(x='Pclass',y='Survived',data = dataset,kind='bar')\nplt.title('Percentage of Survivals based on Pclass',weight='bold')\nplt.annotate(s='Pclass 1 highest\\n% of survival',xy=(0.45,0.55), xytext=(1.35,0.62),\n             arrowprops=dict(facecolor='black',shrink = 0.05),fontsize=11)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bce865c5728f5f1d106ad63c0aee77ebafb8f131"},"cell_type":"markdown","source":"## 2B. Name Variable"},{"metadata":{"trusted":true,"_uuid":"81b525167be703e46e40835ec60da3b3acea0e1d"},"cell_type":"code","source":"# Extracting title from name and giving rare titles as 'rare'\nnamelist = ['Mr.','Miss.','Mrs.','Master.']\nrows,cols=dataset.shape\nfor i in range(0,rows):\n    for n in namelist:\n        if (n in dataset.iloc[i,2]) == True:\n            dataset.iloc[i,2] = n\ncount = 0\nfor i in (~dataset['Name'].isin(namelist)):\n    if i == True:\n        dataset.iloc[count,2] = 'Rare'\n        count = count + 1\n    else:\n        count = count + 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"92698f8b7519275d03c200e59179109131bdd592"},"cell_type":"code","source":"sb.countplot(x=dataset['Name'],hue=dataset['Survived'])\nplt.title('Survival Count based on title',fontweight='bold')\nplt.xlabel('Title',fontsize = 12)\nplt.ylabel('Survival Count',fontsize=12)\nplt.annotate(s='Males have lowest\\nsurvival count',xy=(0.1,250),xytext=(1.5,310),fontsize=12,\n             arrowprops=dict(facecolor='black',shrink =0.1))\nplt.annotate(s='Females have higher\\nsurvival count',xy=(2.3,130),xytext=(1.5,240),fontsize=12,\n             arrowprops=dict(facecolor='black',shrink=0.1))                                                                                                          \nplt.annotate(s='',xy=(1.2,110),xytext=(1.7,220),fontsize=12,arrowprops=dict(facecolor='black',shrink=0.1))                                                                                                          \n\n\nsb.factorplot(x = 'Name',y='Survived',data=dataset, kind ='bar')\nplt.title('Survival Percentage based on title',fontweight='bold')\nplt.ylabel('Survival Probability',fontsize=12)\nplt.xlabel('Title',fontsize=12)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"60d520f47a1e40c1d89c9b1187bdd872ec0a1f53"},"cell_type":"markdown","source":"## 2C. Sex Variable"},{"metadata":{"trusted":true,"_uuid":"1385b330df05914e386c46cda926b376a2c5103c"},"cell_type":"code","source":"# Females have a higher survival probability than males\nsb.factorplot(x='Sex',y='Survived',data=dataset,kind='bar')\nplt.ylabel('Survival Probability',fontsize=12)\nplt.xlabel('Sex',fontsize =12)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2d9019aed6c6b151557d0c3accd956ec418e90a6"},"cell_type":"markdown","source":"## 2D. Embarked Variable"},{"metadata":{"trusted":true,"_uuid":"c767bc1e0ead69c2403834ff2e994eca307bd48c"},"cell_type":"code","source":"# Filling in the two missing embarked as Q\ndataset['Embarked']=dataset['Embarked'].fillna('Q')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6a8804253671c83aa16b5697af3c3b26da6155d5"},"cell_type":"code","source":"# Survival Probability is dependent on the passenger's embark location\nsb.countplot(dataset['Embarked'],hue=dataset['Survived'])\nplt.ylabel('Survival Count')\nsb.factorplot('Embarked','Survived',data=dataset,kind='bar')\nplt.ylabel('Survival Probability')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fc242b19c16ca2bf0b43db44cf18983392776bae"},"cell_type":"markdown","source":"# 3. Numerical Variables"},{"metadata":{"_uuid":"936436f8a9de79132e2726a8456847532bd68b30"},"cell_type":"markdown","source":"## 3A. Family Size Variable + 1 for himself"},{"metadata":{"trusted":true,"_uuid":"6cd364342ecb451340c1fdea46a4f8bfc98b1c2c"},"cell_type":"code","source":"dataset['Family']=dataset['SibSp'] + dataset['Parch'] +1\ndataset = dataset.drop(['SibSp','Parch'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"39c625e2d8d5c8c6f839db671f2d05de4a00e982"},"cell_type":"code","source":"# Outlier present in the family size variable\nsb.boxplot(dataset.Family)\ndataset.Family.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"42ac78a05865ba0dec7c84feab2dc93c6b92154b"},"cell_type":"code","source":"# Dropping family size of >=7 (outliers)\ndataset=dataset[dataset['Family']<7]\ndataset = dataset.reset_index()\ndataset = dataset.drop(['index'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"428227bb3b01263825689f7969739604f0339cee"},"cell_type":"code","source":"sb.countplot(dataset['Family'],hue = dataset['Survived'])\nplt.title('Survival Count based on family size',fontweight='bold')\nplt.ylabel('Survival Count',fontweight='bold')\nplt.xlabel('Family Size',fontweight='bold')\nplt.annotate(s='People who are alone\\nare less likely\\nto survive',xy=(0.1,190),xytext=(2.8,230),fontsize=11,\n             arrowprops=dict(facecolor='black',shrink=0.1))\n\n\nsb.factorplot('Family','Survived',data=dataset,kind='bar',aspect = 1.2)\nplt.title('Survival Percentage based on family size',fontweight='bold')\nplt.ylabel('Survival Count',fontweight='bold')\nplt.xlabel('Family Size',fontweight='bold')\n\nplt.annotate(s='Family of 4 most\\n likely to survive',xy=(2.6,0.7),xytext=(0,0.72),fontsize=11,\n             arrowprops=dict(facecolor='black',shrink=0.1))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e924bdcbb68e2a3864a4d14f3300abb68cf04b37"},"cell_type":"markdown","source":"## 3B. Fare Variable"},{"metadata":{"trusted":true,"_uuid":"354d6844b66636487afc4786745b77da87043f37"},"cell_type":"code","source":"# Checks for the distribution first, displot\nplt.subplot(1,2,1)\nsb.distplot(dataset['Fare'])\nplt.title ('Before log transformation')\n\n# After taking the log\ndataset['Fare'] = dataset['Fare'].map(lambda i: np.log(i) if i >0 else 0)\nplt.subplot(1,2,2)\nsb.distplot(dataset['Fare'])\nplt.title ('After log transformation')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"86e7c3f251e675c7d93467ca856264f036fbd5fa"},"cell_type":"code","source":"# Outlier removal for fares\nlq = np.percentile(dataset['Fare'],25)\nuq = np.percentile(dataset['Fare'],75)\nIQR = uq-lq\nlimit1 = lq-1.5*IQR\nlimit2= uq+1.5*IQR\ndataset= dataset[(dataset['Fare']<limit2) & (dataset['Fare']>limit1) ]\ndataset= dataset.reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d76d2abef6da5edfce8ea3556ec1c8e2c4104325"},"cell_type":"code","source":"# from https://www.kaggle.com/yassineghouzam/titanic-top-4-with-ensemble-modeling\nsb.kdeplot(dataset['Fare'][dataset['Survived']==1],color='blue',shade='True')\nsb.kdeplot(dataset['Fare'][dataset['Survived']==0],color='red',shade='True')\nplt.legend(['Survived','Died'])\nplt.xlabel('log(Fare)')\nplt.ylabel('Frequency')\nplt.annotate(s='Higher fares, \\ngreater survival rate', xy = (4,0.25),xytext=(3,0.5),fontsize=11,fontweight='bold',\n            arrowprops=dict(facecolor='black',shrink = 0.1))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"daae45b252e3e9e56c6ab54e2002c5fa9f336dc6"},"cell_type":"markdown","source":"## 3C: Age Variable"},{"metadata":{"trusted":true,"_uuid":"749cda623b70c8b39c9674ec03a540ac85f14eda"},"cell_type":"code","source":"# Attempt to fill in the missing age of 160 using existing variables\nplt.subplot(2,2,1)\nsb.boxplot(dataset['Sex'],dataset['Age'])\nplt.subplot(2,2,2)\nsb.boxplot(dataset['Family'],dataset['Age'])\nplt.subplot(2,2,3)\nsb.boxplot(dataset['Embarked'],dataset['Age'])\nplt.subplot(2,2,4)\nsb.boxplot(dataset['Pclass'],dataset['Age'])\n\n# Using medians for each individual PClass to predict age\nmedianages = dataset[['Pclass','Age']].groupby(by='Pclass').median()\n\ndef agefiller(x):\n    if x == 1:\n        y = medianages.iloc[0,0]\n    elif x == 2:\n        y = medianages.iloc[1,0]\n    else:\n        y = medianages.iloc[2,0]\n    return y \n\n# Replace missing age with these medians based on PClass\nmissingage = dataset['Age'].isnull()\nrows,cols = dataset.shape\n\nfor i in range(0,rows):\n    if (missingage[i] == True):\n        dataset.iloc[i,4] = agefiller(dataset.iloc[i,1])\nplt.subplots_adjust(wspace=0.5,hspace=0.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6cfc1b7680b10ca4da288f4474ea5187316b4877"},"cell_type":"code","source":"plt.subplots_adjust(wspace=0.4)\nrcParams['figure.figsize'] =7,4\n\nplt.subplot(1,2,1)\nsb.distplot(dataset['Age'])\nplt.ylabel('Frequency')\n\nplt.subplot(1,2,2)\nsb.kdeplot(dataset['Age'][dataset['Survived']==1],color='blue',shade = 'True')\nsb.kdeplot(dataset['Age'][dataset['Survived']==0],color='red',shade='True')\nplt.legend(['Survived','Died'],loc='best')\nplt.xlabel('Age')\nplt.ylabel('Frequency')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e08c55bb78a5398d83f7be6b8674766cf8a3c82f"},"cell_type":"markdown","source":"## 3D. Cabin Variable"},{"metadata":{"trusted":true,"_uuid":"1ee14ef45eb646eda8b81af1371604aa5a22af0c"},"cell_type":"code","source":"# Cabin Variable , 0 and 1. Assume that those with Nan in cabin column have no cabins\nrcParams['figure.figsize'] =5,4\ndataset.Cabin = dataset.Cabin.fillna(0)\ndataset.Cabin = dataset.Cabin.astype(bool).astype(int)\nsb.countplot(dataset.Cabin,hue=dataset.Survived)\nsb.factorplot('Cabin','Survived',data=dataset,kind='bar')\nplt.ylabel('Survival Probability')\nplt.title('Survival Probability based on cabins',fontweight='bold')\nplt.annotate(s=' Those with no\\ncabin have lower\\n  survival rate', xy = (0,0.35),xytext=(-0.4,0.6),fontsize=11,fontweight='bold',\n            arrowprops=dict(facecolor='black',shrink = 0.1))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"56da230d2212a6271a12e6707da8a2c081103e69"},"cell_type":"markdown","source":"# 4. Feature Selection / Correlation"},{"metadata":{"trusted":true,"_uuid":"38ec65db642dc9758b926f25b7a0889fb31b4cfe"},"cell_type":"code","source":"# 1 Hot Encoding for the categorical datas, namaely Sex,Embarked and Name\ndataset = pd.get_dummies(dataset, columns=['Name'],prefix =['Name'])\ndataset = pd.get_dummies(dataset, columns=['Embarked'],prefix =['Embarked'])\ndataset = pd.get_dummies(dataset, columns=['Sex'],prefix =['Sex'])\ndataset = pd.get_dummies(dataset, columns=['Pclass'],prefix =['Pclass'])\n\ndataset = dataset.drop(['Name_Rare','Name_Mr.','Embarked_S','Sex_female','Pclass_1'],axis =1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2d29d8c11deff4f7764b61b2e20ff1963f2078c0"},"cell_type":"code","source":"rcParams['figure.figsize'] =7,5\nsb.heatmap(dataset.corr(method='spearman'),annot=True,fmt='.0g')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"03e4f5f455f1966a2e9b4e3bf19e7cfccb046fcc"},"cell_type":"markdown","source":"# 5. Transforming Kaggle test dataset"},{"metadata":{"trusted":true,"_uuid":"5d31efc5b2a79f9a155fcaadaad464edc4affb30"},"cell_type":"code","source":"kaggle = pd.read_csv(\"../input/test.csv\")\n\n# To be used when submitting predictions\npassengerid = kaggle['PassengerId']\n\nkaggle = kaggle.drop(['PassengerId','Ticket'],axis=1)\n\n# Extracting title out from name\nnamelist = ['Mr.','Miss.','Mrs.','Master.']\nrows,cols=kaggle.shape\nfor i in range(0,rows):\n    for n in namelist:\n        if (n in kaggle.iloc[i,1]) == True:\n            kaggle.iloc[i,1] = n\ncount = 0\nfor i in (~kaggle['Name'].isin(namelist)):\n    if i == True:\n        kaggle.iloc[count,1] = 'Rare'\n        count = count + 1\n    else:\n        count = count + 1\n\n# Extracting family size from Sibsp and Prch\nkaggle['Family']=kaggle['SibSp'] + kaggle['Parch'] +1\n\n# Assuming those with no cabin assigned are given a value of 0\nkaggle['Cabin'] = kaggle['Cabin'].fillna(0)\nkaggle.Cabin = kaggle.Cabin.astype(bool).astype(int)\n\n# Log transformation of Fare, for better distribution (normal)\nkaggle['Fare'] = kaggle['Fare'].fillna(kaggle['Fare'].mean())\nkaggle['Fare'] = kaggle['Fare'].map(lambda i: np.log(i) if i >0 else 0)\n\n# Applying median ages (based on PClass) as seen in the training set\nmissingage = kaggle['Age'].isnull()\nrows,cols = kaggle.shape\n\nfor i in range(0,rows):\n    if (missingage[i] == True):\n        kaggle.iloc[i,3] = agefiller(kaggle.iloc[i,0])\n\n# One hot encoding for the categorical variables\nkaggle = pd.get_dummies(kaggle, columns=['Name'],prefix =['Name'])\nkaggle = pd.get_dummies(kaggle, columns=['Embarked'],prefix =['Embarked'])\nkaggle = pd.get_dummies(kaggle, columns=['Sex'],prefix =['Sex'])\nkaggle = pd.get_dummies(kaggle, columns=['Pclass'],prefix =['Pclass'])\nkaggle = kaggle.drop(['Embarked_S','Sex_female','Name_Rare','Name_Mr.','SibSp','Parch','Pclass_1'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ccb908b28d32fbc4b3883cc0060d7bf5bfd0d5db"},"cell_type":"code","source":"dataset.head(1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"58cbe92f0cd16cf460a310e4811bc4f1ecc1c089"},"cell_type":"code","source":"kaggle.head(1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"88605fbf1e59b68011e472dbb759141bdcd688fa"},"cell_type":"markdown","source":"# 6. Modeling and predictions"},{"metadata":{"_uuid":"da9f9c9e9bcb8f6e54fbc64049664bef53410728"},"cell_type":"markdown","source":"## 6A. Creation and testing of a logistic regression model from scratch"},{"metadata":{"trusted":true,"_uuid":"a2a15c207ddc12789956ab3b276f6b22af2a1789"},"cell_type":"code","source":"def sigmoid(x):\n  return 1 / (1 + np.exp(-x))\n\n\ndef logistic_regression(x,y,lr,iterations):\n    # obtain the training numbers and number of variables\n    training_number, variables = x.shape\n\n    # insert a row of ones into x as the bias\n    x = np.append(arr=np.ones((training_number,1)).astype(int),values = x, axis=1)\n\n    # randomise the weights for all the variables\n    weights = (np.random.randn(1,variables+1)).T\n\n    inputs = np.matmul(x,weights)\n\n    # Apply sigmoid function and convert to binary 0 or 1, based on the >0.5 = 1\n    outputs = sigmoid(inputs)\n\n    # Backpropagation step\n    for i in range(iterations):\n        # D_error/d(output)\n        D_error_output = -1*(np.multiply(y,(1/(outputs+0.0001))))  + np.multiply((1-y),(1/(1-outputs+0.0001)))\n\n        # D_output_d_input (sigmoid derivative), element wise multiplication!\n        matrix1 = 1- outputs\n        D_output_d_input = np.multiply(matrix1,outputs)\n\n        # Combination of D_error/d(output) & D_output_d_input & all the x variables\n        error_missing_x = (np.multiply(D_error_output,D_output_d_input))\n\n        # this computes a matrix with rows being training example, and columns x1,x2,x3. a gradient is calculated for each\n        # x variable as well as training example\n        D_error_d_weight = np.multiply(error_missing_x,x)\n\n        # takign the average of the gradient for each variable, that is each column average across all the training set\n        gradient = (D_error_d_weight.mean(axis=0)).T\n        gradient = gradient.reshape(variables+1,1)\n\n        # update the weights using the alpha learning rate\n\n        weights = weights - gradient*lr\n\n        inputs = np.matmul(x,weights)\n\n        outputs = sigmoid(inputs)\n\n    outputs = np.where(outputs>=0.5,1,0)\n\n    return outputs,weights\n\ndef fit(x,weights):\n    training_number, variables = x.shape\n    x = np.append(arr=np.ones((training_number,1)).astype(int),values = x, axis=1)\n    xtest = np.matmul(x,weights)\n    ypred = sigmoid(xtest)\n    ypred = np.where(ypred>=0.5,1,0)\n    return ypred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fa497cbd5a21a197f1e950c3ac62ffd94ecf98f6"},"cell_type":"code","source":"X = dataset.iloc[:,1:13].values\ny = dataset.iloc[:,0:1].values\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nsc = StandardScaler()\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.25)\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\ntrain_pred,weights = logistic_regression(X_train,y_train,0.05,10000)\ntest_pred = fit(X_test,weights)\n\n\nprint (\"The training accuracy score is %.3f\" % accuracy_score(y_train,train_pred))\nprint (\"The test accuracy score is %.3f\" % accuracy_score(y_test,test_pred))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b227df9ce60d10c2b1d9ba79a91d030053e282ae"},"cell_type":"markdown","source":"## Sklearn Logistic Regression"},{"metadata":{"trusted":true,"_uuid":"07c99564f22c252b354271008b09713bda824402"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlog_model = LogisticRegression()\nlog_model.fit(X_train,y_train.ravel())\nyscore = log_model.predict(X_test)\nprint (\"The test accuracy score is %.3f\" % accuracy_score(y_test,yscore))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"015de469f77a2ae4d4bea6a14af88e17dd940566"},"cell_type":"markdown","source":"## 6B. Sklearn Kernel SVM"},{"metadata":{"trusted":true,"_uuid":"fc765301a69851c4deef4ef1121a8f805c48c4ac"},"cell_type":"code","source":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.05)\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\n\nfrom sklearn.svm import SVC\nsvm_model = SVC(kernel='rbf',degree =4,gamma='auto')\nsvm_model.fit(X_train,y_train.ravel())\n\nfrom sklearn.model_selection import cross_val_score\nsvmcv = cross_val_score(estimator = svm_model, X = X, y = y.ravel(),cv = 5,scoring='accuracy')\nprint (\"The mean cross-validation training accuracy score is %.3f\" % svmcv.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"12843f3731bfe01a9a74954fc1185d91b129a846"},"cell_type":"code","source":"# Checking for overfitting or underfitting via train and test scores (y-axis) against training set size (x-axis)\nfrom sklearn.model_selection import learning_curve\n\n# Splits overall dataset into training set & test set, and varying the training set size from 100 to max possible (step by 10)\nmax_training_size, nil = X.shape\ncv = 15\nmax_training_size = int((max_training_size*((cv-1)/cv))-1)\ntrain_sizes, train_scores, test_scores = learning_curve(svm_model, X, y.ravel(), train_sizes=range(100,max_training_size,10), \n                                                         cv=cv, shuffle=True)\n\n# Taking the mean of train and test score for each cross validation test/traing set and for each training set size\n# train_scores, cols = testset1--->testset10, rows = trainingsize100 --> trainingsize(max)\ntrain_scores = [i.mean() for i in train_scores]\ntest_scores = [i.mean() for i in test_scores]\n\nplt.subplot(1,2,1)\nerrorplot = pd.concat([pd.DataFrame(train_sizes,columns = ['Training Size']),\n                       pd.DataFrame(train_scores,columns=['Training Score']),\n                       pd.DataFrame(test_scores, columns =['Test Score'])],axis=1)\n\nplt.plot(errorplot.iloc[:,0],errorplot.iloc[:,1], color='red', label='Training Set')\nplt.plot(errorplot.iloc[:,0],errorplot.iloc[:,2],color='blue',label='Test Set')\nplt.xlabel('Training Set Size')\nplt.ylabel('Accuracy Score')\nplt.legend(loc='best')\n\ndifference = errorplot\nplt.subplot(1,2,2)\ndifference['Difference']= errorplot.iloc[:,1]- errorplot.loc[:,'Test Score']\ndifference = difference.drop(['Training Score','Test Score'],axis=1)\nplt.plot(difference.iloc[:,0],difference.iloc[:,1],color='green',label='Score Difference')\nplt.xlabel('Training Set Size')\nplt.legend(loc='best')\n\nplt.subplots_adjust(hspace =1.6)\n\nprint (str(round((1/cv*100),1)) +'% '+ 'of the overall training data set was used as the cross validation test set')\nprint (\"\")\nprint ('From the graph, the SVM model has a maximum accuracy of ~81% and is obtained by taking only 5% as the test set')\nprint (\"\")\nprint ('Taking the usual 15-20% of the training set as test set yield lower acc (seen above), likely due to insufficent training data')\nprint (\"\")\nprint ('One way to improve the model is to increase the training size since the test score < training score')\nprint(\"\")\nprint('Decided to use SVM to train on 95% training set and applying it to kaggle test set. 80.61% accuracy returned')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8aa146bfe29a8e8c7d894b89a3e1f07f32e2a430"},"cell_type":"markdown","source":"# 6C. Sklearn Random Forest"},{"metadata":{"trusted":true,"_uuid":"a9d3071bcbaa779c6473a5f78857914dd1b2956d"},"cell_type":"code","source":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.20)\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\nfrom sklearn.ensemble import RandomForestClassifier\n# number of trees, min_samples_split,\n\nranforest_model = RandomForestClassifier(n_estimators = 20, criterion = 'entropy',min_samples_split=8,min_samples_leaf=2)\nranforest_model.fit(X_train,y_train.ravel())\n\nrany= ranforest_model.predict(X_test)\nprint (\"The test accuracy score is %.3f\" % accuracy_score(y_test,rany))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"02a508d13ed0f41566c05a44ea97d303c5ad9bd3"},"cell_type":"code","source":"# Checking for overfitting or underfitting via train and test scores (y-axis) against training set size (x-axis)\nfrom sklearn.model_selection import learning_curve\n\n# Splits overall dataset into training set & test set, and varying the training set size from 100 to max possible (step by 10)\nmax_training_size, nil = X.shape\ncv = 10\nmax_training_size = int((max_training_size*((cv-1)/cv))-1)\ntrain_sizes, train_scores, test_scores = learning_curve(ranforest_model, X, y.ravel(), train_sizes=range(100,max_training_size,10), \n                                                         cv=cv, shuffle=True)\n\n# Taking the mean of train and test score for each cross validation test/traing set and for each training set size\n# train_scores, cols = testset1--->testset10, rows = trainingsize100 --> trainingsize(max)\ntrain_scores = [i.mean() for i in train_scores]\ntest_scores = [i.mean() for i in test_scores]\n\nplt.subplot(1,2,1)\nerrorplot = pd.concat([pd.DataFrame(train_sizes,columns = ['Training Size']),\n                       pd.DataFrame(train_scores,columns=['Training Score']),\n                       pd.DataFrame(test_scores, columns =['Test Score'])],axis=1)\n\nplt.plot(errorplot.iloc[:,0],errorplot.iloc[:,1], color='red', label='Training Set')\nplt.plot(errorplot.iloc[:,0],errorplot.iloc[:,2],color='blue',label='Test Set')\nplt.xlabel('Training Set Size')\nplt.ylabel('Accuracy Score')\nplt.legend(loc='best')\n\ndifference = errorplot\nplt.subplot(1,2,2)\ndifference['Difference']= errorplot.iloc[:,1]- errorplot.loc[:,'Test Score']\ndifference = difference.drop(['Training Score','Test Score'],axis=1)\nplt.plot(difference.iloc[:,0],difference.iloc[:,1],color='green',label='Score Difference')\nplt.xlabel('Training Set Size')\nplt.legend(loc='best')\n\nplt.subplots_adjust(hspace =1.6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7dd72e5a39978c7388ced44a01736e1887eecd66"},"cell_type":"code","source":"# Tuning the parameters\n\nfrom sklearn.model_selection import GridSearchCV\n\nmin_samples_split=[6,7,8,9,10]\nmin_samples_leaf=[1,2,3,4,5]\nparameters = dict(min_samples_split = min_samples_split,min_samples_leaf=min_samples_leaf)\noptimizer = GridSearchCV(estimator=ranforest_model\n                         ,param_grid = parameters,\n                         scoring = 'accuracy',\n                         cv = 5)\noptimizer = optimizer.fit(X_train,y_train.ravel())\nprint('The best training score obtained is %.3f' % optimizer.best_score_)\nprint(optimizer.best_params_)\n\nrany= ranforest_model.predict(X_test)\nprint (\"The test accuracy score is %3f\" % accuracy_score(y_test,rany))\nprint (\"\")\nprint ('While test accuracy is high, the graph above shows the high variability in performance over larger training sets')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"393bc371581e323a931e3ee782a8338ce674cd32"},"cell_type":"markdown","source":"# 6D. Artifical Neural Network"},{"metadata":{"trusted":true,"_uuid":"59d252e3b43fa0c531db4fed6be4afcc33f7a27d"},"cell_type":"code","source":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f5824ee23856eaa024aa8865a8c193156b6dc97f"},"cell_type":"code","source":"# Creation of the ann model that links all the nodes together\nann_model = Sequential()\n\n# Adding the first hidden layer (20 nodes) with the input layer (having 12 nodes). Glorot uniform initializer for weight init\nann_model.add(Dense(units= 20, kernel_initializer='uniform',activation='relu', input_dim = 12))\n\n# Adding the second hidden layer (20 nodes) with the input layer. Glorot uniform initializer for weight init\nann_model.add(Dense(units = 20, kernel_initializer='uniform',activation='relu'))\n\n# Adding the third hidden layer (20 nodes) with the input layer. Glorot uniform initializer for weight init\nann_model.add(Dense(units = 20, kernel_initializer='uniform',activation='relu'))\n\n# Adding the output layer (1 node for binary classifcation). Glorot uniform initializer for weight init\nann_model.add(Dense(units= 1, kernel_initializer='uniform',activation='sigmoid'))\n\n# Compiling the ANN and selecting the optimizer,cost function\nann_model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n# Training the Model\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.20)\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\n# Fitting the ann model, batch_size which takes x number of training examples before updating the weights. \n# Epochs is the number of times the entire dataset passes through the ann network\nann_model.fit(X_train,y_train,batch_size =20, epochs =15)\n\nanny = ann_model.predict(X_test)\nanny = np.where(anny>=0.5,1,0)\nprint(\"The test score is %3f\" % accuracy_score(y_test,anny))\nprint (\"\")\nprint(\"Overall performance did not beat SVM. Tuning with grid search did not improve accuracy significantly as there are many factors involved.\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a3f04ae8c3d40c7686f3d2efbfbae0226e75ad33"},"cell_type":"markdown","source":"# Using Kaggle Test Dataset"},{"metadata":{"_uuid":"3ef3d4340106c1228d3bab637b726223f7f40093"},"cell_type":"markdown","source":"## SVM Kaggle Submission"},{"metadata":{"trusted":false,"_uuid":"b0825a145294d79597fcc971bdbcb0fe10b44d1b"},"cell_type":"code","source":"kaggle = sc.transform(kaggle)\nkaggletest =svm_model.predict(kaggle)\n# kaggletest = fit(kaggle,weights)\nt1 = pd.DataFrame(passengerid, columns = ['PassengerId'])\ns1 = pd.DataFrame(kaggletest,columns=['Survived'])\nfinal = pd.concat([t1,s1],axis = 1)\n\nfinal.to_csv('results.csv', index=False)\nfinal.Survived.value_counts()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"nbformat":4,"nbformat_minor":1}