{"cells":[
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "import pandas as pd\nfrom pandas import Series,DataFrame\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n%matplotlib inline"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "\n# get training & test csv files as a DataFrame\ntrain_df = pd.read_csv(\"../input/train.csv\", dtype={\"Age\": np.float64}, )\ntest_df    = pd.read_csv(\"../input/test.csv\", dtype={\"Age\": np.float64}, )\n\n# preview the data\ntrain_df.head()"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "test_df.head()"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "# Encode Embarked using onehot encoding\n\ntest_df=test_df.join(pd.get_dummies(test_df.Embarked, prefix='Emb'))\ntrain_df=train_df.join(pd.get_dummies(train_df.Embarked, prefix='Emb'))\n\ntest_df=test_df.join(pd.get_dummies(test_df.Sex, prefix='Sex'))\ntrain_df=train_df.join(pd.get_dummies(train_df.Sex, prefix='Sex'))"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "# Lose the data we aren't interested in\ntest_df=test_df.drop(['Embarked','Sex','Name','Ticket','Cabin'], axis=1)\ntrain_df=train_df.drop(['Embarked','Sex','Name','Ticket','Cabin'], axis=1)\ntrain_df.describe()"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "# Fill in missing Age & Fair data in train, test datasets with median age of passengers found in training data set\n\n# IMPROVEMENT OPP -- potential improvement would be to fill in median ages by \n# class and gender categories (sample code in Kaggle tutorial)\n# or drop the rows altogether\nmedian_age = train_df.Age.median(axis=0)\ntrain_df.Age = train_df.Age.fillna(median_age)\ntest_df.Age = test_df.Age.fillna(median_age)\n\nmedian_fare = train_df.Fare.median(axis=0)\ntrain_df.Fare = train_df.Fare.fillna(median_fare)\ntest_df.Fare = test_df.Fare.fillna(median_fare)"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "y_train_orig = train_df.iloc[:,1].values\n\n\n# Keep Class, Sex, Age, Relationships, Fare, Origin in model for now\nX_train_orig = train_df.iloc[:,[2, 3, 4, 5, 6, 7, 8, 9, 10, 11]]\nX_test_orig = test_df.iloc[:,[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]]\n\n\ntrain_col = X_train_orig.columns"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "test_col = X_test_orig.columns"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "# Normalize data fields\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\n# sc.fit(X_train_orig)\n\nX_test_orig = pd.DataFrame(sc.fit_transform(X_test_orig))\nX_train_orig = pd.DataFrame(sc.fit_transform(X_train_orig))\nX_test_orig.columns = test_col\nX_train_orig.columns = train_col"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "# Find the features that really matter in data set using Random Forest Classifier\nfrom sklearn.ensemble import RandomForestClassifier\nfeat_labels = X_train_orig.columns\nforest = RandomForestClassifier(n_estimators=10000, random_state=0, n_jobs=-1)\nforest.fit(X_train_orig, y_train_orig)\nimportances = forest.feature_importances_\nindices = np.argsort(importances)[::-1]\nimportances\n"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "indices"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "# identify the list of top features\n\nfor f in range(X_train_orig.shape[1]):\n    print(\"%2d) %-*s %f\" % (f + 1, 30, feat_labels[indices[f]], importances[indices[f]]))"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "# Use only top features\nX_train_orig = forest.transform(X_train_orig, threshold=.05)\nX_test_orig = forest.transform(X_test_orig, threshold=.05)\nX_train_orig"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "# Perform pre-processing to determine optimal data set size and tune model parameters\nfrom sklearn.svm import SVC\nsvm = SVC(kernel='rbf', C=100.0, gamma=0.1, random_state=0)\n\n# Determine optimal training data set size using learning curve methods\nimport matplotlib.pyplot as plt\nfrom sklearn.learning_curve import learning_curve\n\ntrain_sizes, train_scores, test_scores = learning_curve(estimator=svm, X=X_train_orig, y=y_train_orig, \n                                                        train_sizes=np.linspace(0.1, 1.0, 10), cv=10, n_jobs=1)\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\nplt.plot(train_sizes, train_mean, color='blue', marker='o', markersize=5, label='training accuracy')\nplt.fill_between(train_sizes, train_mean + train_std, train_mean - train_std, alpha=0.15, color='blue')\nplt.plot(train_sizes, test_mean, color='green', linestyle='--', marker='s', markersize=5, label='validation accuracy')\nplt.fill_between(train_sizes, test_mean + test_std, test_mean - test_std, alpha=0.15, color='green')\nplt.grid()\nplt.xlabel('Number of training samples')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.ylim([0.7, 0.9])\nplt.show()"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "# Determine optimal parameters for machine learning model\nfrom sklearn.learning_curve import validation_curve\n\nparam_range = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\ntrain_scores, test_scores = validation_curve(estimator=svm, X=X_train_orig, y=y_train_orig, param_name='C',\n                                            param_range=param_range, cv=10)\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\nplt.plot(param_range, train_mean, color='blue', marker='o', markersize=5, label='training accuracy')\nplt.fill_between(param_range, train_mean + train_std, train_mean - train_std, alpha=0.15, color='blue')\nplt.plot(param_range, test_mean, color='green', linestyle='--', marker='s', markersize=5, label='validation accuracy')\nplt.fill_between(param_range, test_mean + test_std, test_mean - test_std, alpha=0.15, color='green')\nplt.xscale('log')\nplt.grid()\nplt.xlabel('Parameter')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.ylim([0.7, 0.85])\nplt.show()"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "# Mathmatically determine optimal parameters\nfrom sklearn.grid_search import GridSearchCV\nparam_grid = [{'C': param_range,\n              'kernel': ['linear']},\n             {'C': param_range,\n             'gamma': param_range,\n             'kernel': ['rbf']}]\ngs = GridSearchCV(estimator=svm,\n                 param_grid=param_grid,\n                 scoring='accuracy',\n                 cv=10, n_jobs=1)\ngs = gs.fit(X_train_orig, y_train_orig)\nprint(gs.best_score_)"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "print(gs.best_params_)"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "# in addition to the original data sets for training (train_orig)and testing (test_orig)\n# split train_orig data into training and testing sets randomly so we can obtain a practice test set with outcomes\nfrom sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_train_orig, y_train_orig, test_size=0.25, random_state=0)"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "# Call SVC from scikitlearn library to train weights and run it on segmented data used for testing first \n# to see how accurate we can be\nsvm = SVC(kernel='rbf', C=100.0, gamma=0.1, random_state=0)\nsvm.fit(X_train, y_train)\n\n# call algo to predict using test data set\ny_pred = svm.predict(X_test)\nno_samples = len(y_test)\nprint('Misclassified samples: %d of %d' % ((y_test != y_pred).sum() , no_samples))\n\nfrom sklearn.metrics import accuracy_score\nprint('Accuracy: %.2f' % accuracy_score(y_test, y_pred))"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "# Determine number of true-positives, false-positives, true-negatives, false-negatives to see if model can be \n# optimized\nfrom sklearn.metrics import confusion_matrix\nconfmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n\nfig, ax = plt.subplots(figsize=(2.5, 2.5))\nax.matshow(confmat, cmap=plt.cm.Blues, alpha=0.3)\nfor i in range(confmat.shape[0]):\n    for j in range(confmat.shape[i]):\n        ax.text(x=j, y=i,\n               s=confmat[i,j],\n               va='center', ha='center')\nplt.xlabel('predicted label')\nplt.ylabel('true label')\nplt.show()"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "# Use k-fold cross validation scorer as a better way to predict how robust our model will be against test data\nfrom sklearn.cross_validation  import cross_val_score\nscores = cross_val_score(estimator=svm, X=X_train, y=y_train, cv=10, n_jobs=1)\nprint('CV accuracy scores: %s' % scores)\nprint('CV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "svmorig = SVC(kernel='rbf', C=100.0, gamma=0.1, random_state=0)\nsvmorig.fit(X_train_orig, y_train_orig)\n\n# call algo to predict using test data set\ny_pred_orig = svm.predict(X_test_orig)\ny_pred_orig.sum()"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "output = test_df.PassengerId\noutput = pd.DataFrame(output)\n# len(output)\npredict = pd.DataFrame(y_pred_orig)\noutput = output.join(predict)\noutput.columns = ['PassengerId', 'Survived']\noutput"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "output.to_csv(\"../input/output.csv\", index=False)"
 }
],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}}, "nbformat": 4, "nbformat_minor": 0}