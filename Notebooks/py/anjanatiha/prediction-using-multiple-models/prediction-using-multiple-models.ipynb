{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\ntrain = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/train.csv\")\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"#feature_cols = ['Pclass','Sex','Age','SibSp', 'Parch','Fare','Cabin','Embarked']\nfeature_cols = ['Pclass','Sex','Age','SibSp', 'Parch','Fare']\n#feature_cols = ['Pclass','Sex','Age','Fare']\n#feature_cols = ['Pclass','Sex','Age']\n#feature_cols = ['Pclass','Sex']\n\ntarget_col = 'Survived'\n\nfeatures = train[feature_cols]\ntarget = train[target_col]\n\n\nfeatures.describe()\ntarget.describe()\n\n# set(features['Fare'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a92a29c6514531100743090816e698ce21acf629","collapsed":true},"cell_type":"code","source":"from sklearn import preprocessing\nfeatures = features.fillna(0)\nfeatures['Age']=preprocessing.normalize([features['Age']], norm='l2').flatten()\nfeatures['Fare'] = preprocessing.normalize([features['Fare']], norm='l2').flatten()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"93b238319b290677e01fe3e8d907befe1b0098b9","collapsed":true},"cell_type":"code","source":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\nle.fit(features['Sex'])\nfeatures['Sex']=le.transform(features['Sex'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a36ddedeb6ceb477f30c71640169d0d85a6f7bc4","collapsed":true},"cell_type":"code","source":"X = features.values\nY = target","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c7011d86fe17e42fde310dcd94a1e248284aa781","collapsed":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nfrom sklearn import linear_model\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\n\n\nfrom sklearn import metrics\n\n    \ndef model_evaluation(X, Y, splitter, model, report, details):\n    accuracy = 0\n    f1 = 0\n    precision = 0\n    recall = 0\n    i=0\n    if report:\n        print(\"*\"*50, \" START \", \"*\"*50)\n        print(\"Spliter Description:\")\n        print(splitter)\n        print(\"-\"*100, \"\\n\")\n        print(\"Model Description:\")\n        print(model)\n        print(\"-\"*100,\"\\n\")\n    \n    for train_index, test_index in splitter.split(X, Y):\n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = Y[train_index], Y[test_index]\n        \n        # model fitting\n        model.fit(X_train, y_train)\n        \n        # prediction\n        predict = model.predict(X_test)\n\n        # evaluation scores\n        accuracy_temp = metrics.accuracy_score(y_test, predict)\n        precision_temp = metrics.precision_score(y_test, predict, average=\"micro\")\n        recall_temp = metrics.recall_score(y_test, predict, average=\"micro\")\n        f1_temp = metrics.f1_score(y_test, predict, average=\"micro\")\n        hamming_loss = metrics.hamming_loss(y_test, predict)\n        \n#         precision, recall, thresholds = metrics.precision_recall_curve(y_test, predict)\n#         average_precision_score = metrics.average_precision_score(y_test, predict, average=\"micro\")\n#         fbeta_score = metrics.fbeta_score(y_test, predict)\n#         roc_auc_score = metrics.roc_auc_score(y_test, predict, average=\"micro\")\n        \n    \n        accuracy += accuracy_temp\n        precision+=precision_temp\n        recall+=recall_temp\n        f1+=f1_temp\n        \n        if details:\n            print(\"*\"*25,  \" ITERATION - \", i+1, \"*\"*25)\n            #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n            print(\"Accuracy Score: \", accuracy_temp)\n            print(\"Precision Score: \", precision_temp)\n            print(\"Recall Score: \", recall_temp)\n            print(\"F1 Score: \", f1_temp)\n            print(\"Hamming Loss: \", hamming_loss)\n            print(\"-\"*35)\n            print(metrics.classification_report(y_test, predict))\n            print(\"-\"*35)\n            print(\"confusion Matrix:\\n\\n\", metrics.confusion_matrix(y_test, predict))\n            print(\"-\"*35)\n            print(\"\\n\")\n        \n        i+=1\n    split_num = splitter.get_n_splits()\n    \n    accuracy = accuracy/split_num\n    precision = precision/split_num\n    recall = recall/split_num\n    f1 = f1/split_num\n    \n    if report:\n        print(\"*\"*50, \" Average For\", i+1, \" Folds\", \"*\"*50)\n        print(\"\\n\")\n        print(\"Average Accuracy Score: \", accuracy)\n        print(\"Average pPrecision Score: \", precision)\n        print(\"Average Recall Score: \", recall)\n        print(\"Average F1 Score:\", f1)\n        print(\"\\n\")\n        print(\"*\"*50, \" END \", \"*\"*50)\n    \n    \n    \n    return accuracy, precision, recall, f1\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8ddf56ff1f532f443ccd5f3749126a06111bcb4d","collapsed":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.ensemble import (AdaBoostClassifier, ExtraTreesClassifier, GradientBoostingClassifier, RandomTreesEmbedding, RandomForestClassifier, VotingClassifier)\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB \nfrom sklearn.neighbors import KDTree, KNeighborsClassifier, NearestNeighbors\nfrom sklearn.neural_network import BernoulliRBM, MLPClassifier\nfrom sklearn.svm import LinearSVC, NuSVC\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\n\n\n\nsss = StratifiedShuffleSplit(n_splits=5, test_size=0.5, random_state=0)\n\n\nclassifiers = {\n    \"AdaBoostClassifier\": AdaBoostClassifier(),\n    \"BernoulliNB\": BernoulliNB(),\n#     \"BernoulliRBM\": BernoulliRBM(),\n    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n    \"ExtraTreesClassifier\": ExtraTreesClassifier(),\n    \"GaussianMixture\": GaussianMixture(),\n    \"GaussianNB\": GaussianNB(),\n    \"GaussianProcessClassifier\": GaussianProcessClassifier(),\n    \"GradientBoostingClassifier\": GradientBoostingClassifier(),\n#     \"KDTree\": KDTree(),\n    \"KNeighborsClassifier\": KNeighborsClassifier(3),\n    \"LogisticRegression\": LogisticRegression(),\n    \"LinearSVC\": LinearSVC(),\n    \"MLPClassifier\": MLPClassifier(),\n    \"MultinomialNB\": MultinomialNB(),\n#     \"NearestNeighbors\": NearestNeighbors(),\n    \"NuSVC\": NuSVC(),\n    \"QuadraticDiscriminantAnalysis\": QuadraticDiscriminantAnalysis(),\n    \"RandomForestClassifier\": RandomForestClassifier(),\n    \"SVC Linear\": SVC(kernel=\"linear\", C=0.025),\n    \"SVC\": SVC(),\n    \"SVC Gamma\": SVC(gamma=2, C=1)\n#     VotingClassifier: VotingClassifier(),\n}\n    \n    \nsplitter = sss\nreport = None\ndetails = 1\n\n\nevaluation = {}\n\nfor name in classifiers:\n    evaluation_temp = []\n    accuracy, precision, recall, f1 = model_evaluation(X, Y, splitter, classifiers[name], report=None, details=None)\n    evaluation_temp.append(accuracy)\n    evaluation_temp.append(precision)\n    evaluation_temp.append(recall)\n    evaluation_temp.append(f1)\n    evaluation[name] = evaluation_temp\n    \n\nrows_list = []\nfor name in evaluation:\n    rows_list.append([name]+evaluation[name])\n                           \nevaluation_pd = pd.DataFrame(rows_list, columns=['model', 'accuracy', 'precision', 'recall', 'f1']) \nevaluation_pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5795ced17a8f79d346e2bdb13d9921f89219862c","collapsed":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib.pyplot import figure\n\n\nfigure(num=None, figsize=(14, 6), dpi=250)\n\nlabels= ['accuracy', 'precision', 'recall', 'f1']\nax = plt.subplot(111)\n\nfor n in range(0,4):\n    plt.plot([name for name in evaluation],[evaluation[name][n] for name in evaluation], label = labels[n])\n\nleg = plt.legend(loc='best', ncol=2, mode=\"expand\", shadow=True, fancybox=True)\nplt.xticks(rotation=45)\n# leg.get_frame().set_alpha(0.5)\nplt.legend()\nax.tick_params(labelsize='large', width=5)\nax.grid(True, linestyle='-.')\n\nplt.tight_layout()\nplt.xlabel('x label')\nplt.ylabel('y label')\n\nplt.title(\"TITLE\")\nplt.show()\n\n\n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}