{"cells":[{"metadata":{"_uuid":"41c10d24864cfe70f0faae8e3898f0422cfcbda1"},"cell_type":"markdown","source":"We will simply apply the same pre-processing applied to the training data to prepare the test data. Please refer to my [Titanic Data Exploration Notebook](https://www.kaggle.com/inturiyam/exploring-titanic-data-using-pandas-dataframes). It has the detailed explanation for the pre-processing steps. "},{"metadata":{"_uuid":"7f2e474a406a219f62e6188302a9053db28b7b26"},"cell_type":"markdown","source":"<a id='import'> **IMPORT DATA** </a>"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#Read data from CSV input\ndf_test = pd.read_csv(\"../input/titanic/test.csv\")\nprint(df_test.dtypes)\ndf_test.info()\ndf_test.describe()\ndf_test.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"21b6bfb115123f9e127000521ac16b357c854244"},"cell_type":"markdown","source":"<a id='mva'>**MISSING VALUE ANALYSIS ON AGE**</a>\n\nAs we can see from above, there are:\n* 86 missing values in age\n* 1 row with missing values in fare and \n* 327 cabin column values being null\n\nWith the above information, we can focus on imputing age values first and consider some form of inferencing for Embarked column subsequently. We will then find some way to represent meaningfully the Cabin information.   "},{"metadata":{"trusted":true,"_uuid":"49f6d66633558d65b330be264e9bca73118bd78a","collapsed":true},"cell_type":"code","source":"#In this step, we replace missing values of Age with their average values\n#df_test[\"Age\"].describe()\n#avg = np.average(df_test[\"Age\"].fillna(value=0))\n#print(avg)\n#df_test[\"Age\"].fillna(value = avg, inplace = True)\n#df_test[\"Age\"].describe()\n#df_test.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b521419237c83a8a56688706b44cc3289ec83951"},"cell_type":"code","source":"#As in the Training data representation, we will try to infer the missing values of Age based on the individual's title.\n\ndf_test[\"title\"]=df_test[\"Name\"].str.lower().str.extract('([a-z]*\\.)', expand=True)\n#df_test[\"title\"].head()\n#Passengers in each title group whose age is missing\ndf_test[df_test[\"Age\"].isnull()].groupby(by = [\"title\"])[\"PassengerId\"].count() \n\navg_master = df_test[((df_test[\"title\"]==\"master.\") & (df_test[\"Age\"].isnull()==False))][\"Age\"].median()  \navg_miss = df_test[((df_test[\"title\"]==\"miss.\") & (df_test[\"Age\"].isnull()==False))][\"Age\"].median()  \navg_mr = df_test[((df_test[\"title\"]==\"mr.\") & (df_test[\"Age\"].isnull()==False))][\"Age\"].median()  \navg_mrs = df_test[((df_test[\"title\"]==\"mrs.\") & (df_test[\"Age\"].isnull()==False))][\"Age\"].median()  \n\n#We will now replace the missing age values in each group with the corresponding average values \n# Refer - https://stackoverflow.com/questions/20625582/how-to-deal-with-settingwithcopywarning-in-pandas\ndf_test.loc[((df_test[\"title\"]==\"ms.\") & (df_test[\"Age\"].isnull()==True)).tolist(),'Age']=avg_miss #there is only 1 ms. in the whole test set. \ndf_test.loc[((df_test[\"title\"]==\"master.\") & (df_test[\"Age\"].isnull()==True)).tolist(),'Age']=avg_master\ndf_test.loc[((df_test[\"title\"]==\"miss.\") & (df_test[\"Age\"].isnull()==True)).tolist(),'Age']=avg_miss\ndf_test.loc[((df_test[\"title\"]==\"mr.\") & (df_test[\"Age\"].isnull()==True)).tolist(),'Age']=avg_mr\ndf_test.loc[((df_test[\"title\"]==\"mrs.\") & (df_test[\"Age\"].isnull()==True)).tolist(),'Age']=avg_mrs\n\ndf_test.describe()\n#We will now scale the Age column and add it as a new column to the dataframe. \n#For this we need to use the same mean and Std.Dev. as the Training Set. I am copying the values from my other notebook here manually.  \nage_mean = 29.390202020202018\nage_std = 13.265321985344817\ndf_test[\"age_norm\"]=((df_test[\"Age\"]-age_mean)/age_std)\ndf_test[\"age_norm\"].hist()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c9c0730f27adc5c195359243e0b71f5d8d596d6a"},"cell_type":"markdown","source":"<a id = 'fe'>**FEATURE ENGINEERING**</a>\n\nNow we will embark on a little bit of feature engineering. \n* To start with we will One-Hot encode Sex, Passenger Class and Point of Embarkment\n* We will also infer a feature from title of the passengers"},{"metadata":{"trusted":true,"_uuid":"ae8d6e451ff18f0512e73111a1827d05fd323a77"},"cell_type":"code","source":"df_test[\"is_male\"] = pd.get_dummies(df_test[\"Sex\"], drop_first=True) #we use drop_first to avoid creating another correlated column is_female\ndf_test.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"54697c56df25688676b61d21ac4363ec3884cf80","scrolled":false,"collapsed":true},"cell_type":"code","source":"#We will bin the passengers into few age groups just to see if children and older passengers had any higher survival probability\nbins = [0,15,25,50,100]\ndf_test[\"age_group\"]=pd.cut(df_test[\"Age\"],bins)\n#print(pd.get_dummies(df_train[\"age_group\"]))\ndf_test[[\"age15\",\"age25\",\"age50\",\"age100\"]]=pd.get_dummies(df_test[\"age_group\"], dtype=\"uint8\")\n#print(df_test[\"age15\"])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6cad04caa65e67c36686b521563ba5c740f6c0f5"},"cell_type":"markdown","source":"We will now try to assess if some meaningful pattern can be observed between Names and survivorship. "},{"metadata":{"trusted":true,"_uuid":"bfb7b8841ab88d9c15ebfb7ccee4588d49b274b2"},"cell_type":"code","source":"namelen = []\nfor i in range(len(df_test[\"Name\"])):\n    namelen.append(len(df_test[\"Name\"][i]))\ndf_test[\"len_name\"]=namelen\n#df_train.hist(\"len_name\", by=[\"Survived\", \"Pclass\"] , bins=10,layout=[4,3], figsize = [15,15])\ndf_test[\"len_name\"].describe()\n#df_train[df_train[\"len_name\"] >= 30]\n\nlen_name_avg = 26.9652076318743\nlen_name_std = 9.28160688314506\n\ndf_test[\"norm_len_name\"]=(df_test[\"len_name\"]-len_name_avg)/len_name_std\ndf_test[\"norm_len_name\"].hist()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6e40aba7a9160de0c1405dcff68d432dcc0e1478"},"cell_type":"markdown","source":"<a id='tsr'>**MORE ON TITLES AND SURVIVAL RATE**</a>\n\nJust to be more sure about our histogram data, we can compute the survival rates by title. \n\nHere I tried 2 approaches: \na. In order to have a manageable number of features, we will create a boolean column called \"high_prob_group\" which will have persons with titles showing higher survival rate (>50%) in the training data. For now, I am not considering the sample size in doing this categorization. We will add the following titles to this group - countess, lady, master, miss, mlle, mme,mrs,ms, sir.\n\nb. Simply one hot encode a few groups of titles and mark the rest as rare titles "},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"b6e755e4efd66f4d6f96d947217a38cb2fdb965d"},"cell_type":"code","source":"#df_train.pivot(index=\"PassengerId\",columns = \"title\", values = \"Survived\")\ndf_test.groupby([\"title\"])[\"PassengerId\"].count()\n#df_train.groupby([\"title\"])[\"Survived\"].sum()/df_train.groupby([\"title\"])[\"PassengerId\"].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9b29c7a984d248caecbbcdf5490e5331e866c8fd"},"cell_type":"code","source":"#Approach 1\nlookfor = np.array(['mrs.','sir.','countess.', 'lady.', 'master.', 'miss.', 'mlle.', 'mme.','mrs.','ms.', 'sir.'])\n#s = pd.Series(lookfor)\ndf_test[\"high_prob_group\"]=df_test[\"title\"].isin(lookfor).astype('uint8')\ndf_test[\"high_prob_group\"].sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"57cac94f72632f6b03d4bbd246fb8f324e70f698"},"cell_type":"code","source":"#Approach 2\n#use of x.astype('uint8') helps convert the Boolean output of isin() to an integer (0,1) representation \ndf_test[\"title_ms\"] = df_test[\"title\"].isin([\"miss.\",\"ms.\"]).astype('uint8')\ndf_test[\"title_mrs\"] = df_test[\"title\"].isin([\"mrs.\",\"mme.\",\"mlle.\"]).astype('uint8')\ndf_test[\"title_mr\"] = df_test[\"title\"].isin([\"mr.\"]).astype('uint8')\ndf_test[\"title_others\"]=df_test[\"title\"].isin(['countess.', 'lady.', 'master.', 'dr.', 'don.','jonkheer.','rev.','major.','sir.','col.','capt.']).astype('uint8')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"94cf94379e3dc98eeed6d969ec4d5b1923c3a46a"},"cell_type":"markdown","source":"<a id='mve'>**MISSING VALUES OF  FARE COLUMN**</a>\n\nWe will now try to fill in the missing values for FARE column. To do so, we need to figure out what might be the approximate fares for a [PClass, Embarked] combination. We also need to then identify the rows where we need to infer and fill in the missing values manually.  "},{"metadata":{"trusted":true,"_uuid":"2b29c91150aff4df87ba2ca9b4ef19fbc5b97e5a"},"cell_type":"code","source":"median_fare=df_test[(df_test['Pclass'] == 3) & (df_test['Embarked'] == 'S')]['Fare'].median()\nprint(median_fare)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"845eacc1781368c06390bf4fc779b04e92f35353"},"cell_type":"code","source":"#We start with a boxplot to figure out the range of Fare values\ndf_test.boxplot(\"Fare\", by=[\"Embarked\",\"Pclass\"], figsize = [8,8])\ndf_test[df_test[\"Fare\"].isna()] \nmedian_fare= df_test[(df_test[\"Embarked\"]==\"S\") & (df_test[\"Pclass\"]==3)][\"Fare\"].median()\ndf_test[\"Fare\"].fillna(value = median_fare, inplace = True)\ndf_test[\"Fare\"].describe()\n#df_test.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"111d0513cea3990c35afca0f335741577cec96a9"},"cell_type":"code","source":"#Normalize fare values\nfare_mean = 32.204208\nfare_std = 49.693429\ndf_test[\"norm_fare\"]= (df_test[\"Fare\"]-fare_mean)/fare_std","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a5ae19f3820730d4efcc00f61dc87a68d93f452c"},"cell_type":"code","source":"df_test.hist(\"Fare\", by=[\"Embarked\", \"Pclass\"],layout=[4,3], figsize = [15,15], bins=10)\ndf_test[df_test[\"Embarked\"].isna()] \ndf_test[\"Embarked\"].fillna(value = \"C\", inplace = True)\n#df_test.hist(\"Fare\", by=[\"Embarked\", \"Pclass\"],layout=[4,3], figsize = [15,15], bins=10)\n#df_train[df_train[\"Embarked\"]==\"C\"]\ndf_test[[\"embC\",\"embQ\",\"embS\"]]=pd.get_dummies(df_test[\"Embarked\"], dtype=\"uint8\")\n#df_train.hist(\"Embarked\",by=[\"Survived\",\"Pclass\"],layout=[2,3], figsize = [10,8]) #Just ran this to see if there is any significant pattern in data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eea90a8ae8c8348ede79139a19b27def8feda520","collapsed":true},"cell_type":"code","source":"#FAMILY SIZE\n#Normalizing with same Mean and Std as Training data set\ndf_test[\"tot_family_size\"] = df_test[\"Parch\"]+df_test[\"SibSp\"]\ndf_test[\"norm_family_size\"] = (df_test[\"tot_family_size\"]-0.9046015712682379)/(1.6134585413550788)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1a15641de246d3ae0952847386d66d5b2603f2dc"},"cell_type":"code","source":"#One-hot encode passenger class\ndf_test[\"Pclass\"].hist()\ndf_test[[\"P2\",\"P3\"]]=pd.get_dummies(df_test[\"Pclass\"],drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ad6af6cd028f0d9935f253eb874361049b3c2fad","collapsed":true},"cell_type":"code","source":"#Encode cabin information\n\ndf_test[\"cab\"] = df_test[\"Cabin\"].str.lower().str.get(0)\n\ndf_test[\"cab\"].fillna(value=\"z\",inplace=True)\ndf_test[\"cab\"].unique()\ndf_test[[\"cab_b\",\"cab_c\",\"cab_d\",\"cab_e\",\"cab_f\",\"cab_g\",\"cab_z\"]] =pd.get_dummies(df_test[\"cab\"],drop_first=True)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f900b354394d26493b39989061a06a14412f54b3"},"cell_type":"markdown","source":"<a id='exp'>EXPORT RESULTS</a>\n\nExport processing results to a new CSV file for further work - that will be different notebook. Thank you for reading through till here. Hope it helped you get started. "},{"metadata":{"trusted":true,"_uuid":"0bafbc61af002e8f971cb3241623d5875e21044f"},"cell_type":"code","source":"#It appears that the file gets stored to a folder called working. See below. \ndf_test.info()\ndf_test.to_csv(path_or_buf=\"test_processed.csv\")\nprint(os.listdir(\"../\"))\nprint(os.listdir(\"../working\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d4262e87c1f6ae159eb43bb3c47102ca1e1e78ad"},"cell_type":"code","source":"import seaborn as sns\ncorr = df_test[[\"P2\",\"P3\",\"norm_len_name\",\"title_ms\",\"title_mrs\",\"title_mr\",\"title_others\",\"is_male\",\"age_norm\",\"norm_family_size\",\"norm_fare\",\n                    \"cab_b\",\"cab_c\",\"cab_d\",\"cab_e\",\"cab_f\",\"cab_g\",\"cab_z\",\"embQ\",\"embS\"]].corr()\n\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(0, 50, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7240da6d929fa66bfc7d629d1add084364688418"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}