{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"XGBOOST 공식 문서 : https://xgboost.readthedocs.io/en/latest/"},{"metadata":{"trusted":true,"_uuid":"87389726479064ec3c304534e058ee65edeb0ced"},"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import accuracy_score\nimport pandas as pd\nimport re\nimport sklearn\nimport xgboost as xgb\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0137fa375cc9d3aaa5c21ff49d39679933e43478"},"cell_type":"code","source":"# Load in the train and test datasets\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\nall = pd.concat([ train, test ],sort=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"24aea53be55f0174198a7ebed7a41f28a4b93168"},"cell_type":"code","source":"all.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"028751a860b1404bcadccc57e7a1b53ab7007cd2"},"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\nall[\"Embarked\"] = le.fit_transform(all[\"Embarked\"].fillna('0'))\nall[\"Sex\"] = le.fit_transform(all[\"Sex\"].fillna('3'))\nall.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dbfcf1ada71ed228f82890cdc91e3419c163cc4d"},"cell_type":"code","source":"# split the data back into train and test\ndf_train = all.loc[all['Survived'].isin([np.nan]) == False]\ndf_test  = all.loc[all['Survived'].isin([np.nan]) == True]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"34c20b29197b007ae812bb9a09a77258d1731ac8"},"cell_type":"code","source":"print(df_train.shape)\nprint(df_test.shape)\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"82841640b64a1f447eb59651942505f76e27e1fd"},"cell_type":"code","source":"df_test.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"trusted":true,"_uuid":"ae2ccb99b2a509e448c15345c485b6aa0844c0e8"},"cell_type":"code","source":"'''\n# XGBoost model + parameter tuning with GridSearchCV\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import GridSearchCV, train_test_split\n\nfeature_names = ['Sex','Embarked','Pclass','Survived','Age','SibSp','Parch','Fare']\n\nxgb = XGBRegressor()\nparams={\n    'max_depth': [2,3,4,5], \n    'subsample': [0.4,0.5,0.6,0.7,0.8,0.9,1.0],\n    'colsample_bytree': [0.5,0.6,0.7,0.8],\n    'n_estimators': [1000,2000,3000],\n    'reg_alpha': [0.01, 0.02, 0.03, 0.04]\n}\n\ngrs = GridSearchCV(xgb, param_grid=params, cv = 10, n_jobs=4, verbose=2)\ngrs.fit(np.array(df_train[feature_names]), np.array(df_train['Survived']))\n\nprint(\"Best parameters \" + str(grs.best_params_))\ngpd = pd.DataFrame(grs.cv_results_)\nprint(\"Estimated accuracy of this model for unseen data: {0:1.4f}\".format(gpd['mean_test_score'][grs.best_index_]))\n# TODO: why is this so bad?\n'''","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1565e4fc1c767fb4b14600476699cb25999274fe"},"cell_type":"markdown","source":"위에는 그리드서치라는 기법을 이용해서 예측한건데 여기서 선택지가 3개로 갈림.\n1. 그리드 서치   2. 랜덤 서치    3.  베이지안 최적화 \n\n그리드 < 랜덤 < 베이지안 으로 괜찮은 방법입니다.\n\nBest parameters {'colsample_bytree': 0.8, 'max_depth': 2, 'n_estimators': 3000, 'reg_alpha': 0.01, 'subsample': 0.6}"},{"metadata":{"trusted":true,"_uuid":"89acc10bc8260973e0859061a238f07d91340c73"},"cell_type":"code","source":"train_y = df_train['Survived']; train_x = df_train.drop('Survived',axis=1)\nexcluded_feats = ['PassengerId','Ticket','Cabin','Name']\nfeatures = [f_ for f_ in train_x.columns if f_ not in excluded_feats]\nfeatures","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"84579d3fa9d38eecac7427238b820e4a9a7a352f"},"cell_type":"code","source":"from sklearn.model_selection import KFold\nfolds = KFold(n_splits=4, shuffle=True, random_state=546789)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5538a93ee9ca84344ae407c7efa5e4501f2c8efb"},"cell_type":"code","source":"oof_preds = np.zeros(train_y.shape[0])\nsub_preds = np.zeros(df_test.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"59d03d7d240ef959db255c72c6970019bf3e0f9d"},"cell_type":"markdown","source":"아래는 XGBOOST 모델 만드는 부분인데 정보손실 최소화 하기 위해서 kfold방식 이용했습니다. \n\n순서는 \n\n1. X변수와 Y변수를 분리해준다.\n\n```\n    trn_x, trn_y = train_x[features].iloc[trn_idx], train_y.iloc[trn_idx]\n\n    val_x, val_y = train_x[features].iloc[val_idx], train_y.iloc[val_idx]\n```\n\n2.  모델의 하이퍼파라미터를 조정해준다. 여기에서 위에서 말한 그리드서치, 랜덤서치, 베이지안 최적화가 쓰입니다.\n\n베이지안 최적화 in XGBOOST : http://www.kwangsiklee.com/2018/06/bayesianoptimization%EC%9D%84-%EC%9D%B4%EC%9A%A9%ED%95%98%EC%97%AC-xgboost-%ED%95%98%EC%9D%B4%ED%8D%BC-%ED%8C%8C%EB%9D%BC%EB%AF%B8%ED%84%B0%EB%A5%BC-%EC%B5%9C%EC%A0%81%ED%99%94/\n\n```\n    clf = XGBClassifier(\n\n        objective = 'binary:logistic', \n\n        booster = \"gbtree\",\n\n        eval_metric = 'auc', \n\n        nthread = 4,\n\n        eta = 0.05,\n\n        gamma = 0,\n\n        max_depth = 2, \n\n        subsample = 0.6, \n\n        colsample_bytree = 0.8, \n\n        colsample_bylevel = 0.675,\n\n        min_child_weight = 22,\n\n        alpha = 0,\n\n        random_state = 42, \n\n        nrounds = 2000,\n        \n        n_estimators=3000\n\n    )\n```   \n 3.   모델을 적용시킵니다.\n \n```\n    clf.fit(trn_x, trn_y, eval_set= [(trn_x, trn_y), (val_x, val_y)], verbose=10, early_stopping_rounds=100)\n```\n\n2 번의 최적화 관련해서는 저기에 적힌것보다 많은 하이퍼 파라미터가 있는데 \nhttps://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/ 이 링크에 설명이 잘 나와있습니다. \n그리고 모든 변수에 대한 설명은 https://xgboost.readthedocs.io/en/latest/parameter.html 링크에 나와있습니다.\n\n개인적으로 사람들이 많이 다루는건 **`booster`** 와 `eta` , `gmmama`,  **`max_depth`**,  `min_child_weight`, `subsample`, `colsample_bytree`, `scale_pos_weigh`, `max_leaves`를 많이 설정하는것 같았습니다.\n\n\n"},{"metadata":{"trusted":true,"_uuid":"95c0bc8c8174b5a1d246897aa9042cc7019ee82a"},"cell_type":"code","source":"import gc\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n\n\nfor n_fold, (trn_idx, val_idx) in enumerate(folds.split(train_x)):\n\n    trn_x, trn_y = train_x[features].iloc[trn_idx], train_y.iloc[trn_idx]\n\n    val_x, val_y = train_x[features].iloc[val_idx], train_y.iloc[val_idx]\n\n    \n\n    clf = XGBClassifier(\n\n        objective = 'binary:logistic', \n\n        booster = \"gbtree\",\n\n        eval_metric = 'auc', \n\n        nthread = 4,\n\n        eta = 0.05,\n\n        gamma = 0,\n\n        max_depth = 2, \n\n        subsample = 0.6, \n\n        colsample_bytree = 0.8, \n\n        colsample_bylevel = 0.675,\n\n        min_child_weight = 22,\n\n        alpha = 0,\n\n        random_state = 42, \n\n        nrounds = 2000,\n        \n        n_estimators=3000\n\n    )\n\n\n\n    clf.fit(trn_x, trn_y, eval_set= [(trn_x, trn_y), (val_x, val_y)], verbose=10, early_stopping_rounds=100)\n\n    \n\n    oof_preds[val_idx] = clf.predict_proba(val_x)[:, 1]\n\n    sub_preds += clf.predict_proba(df_test[features])[:, 1] / folds.n_splits\n\n    \n\n    print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(val_y, oof_preds[val_idx])))\n\n    del clf, trn_x, trn_y, val_x, val_y\n\n    gc.collect()\n\n    \n\nprint('Full AUC score %.6f' % roc_auc_score(train_y, oof_preds))   \n\n\n\ntest['Survived'] = sub_preds\n\n\n\ntest[['PassengerId', 'Survived']].to_csv('xgb_submission_esi.csv', index=False, float_format='%.8f')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}