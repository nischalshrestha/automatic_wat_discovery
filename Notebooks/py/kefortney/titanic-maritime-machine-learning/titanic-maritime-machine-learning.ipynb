{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "3485241a-04df-546c-840d-2f22a94324a3"
      },
      "source": [
        "Summary\n",
        "-------\n",
        "\n",
        "On April 15, 1912 the RMS Titanic carrying 2,224 passengers and crew struck an iceberg on the ship's maiden voyage. Over the next two hours slid below the icy waves along with over 1,500 dead by the time the RMS Carpathia over 6 hours later. A recent coal strike had ensured that the Titanic was not booked with her full complement of 3,339 passengers and crew otherwise the disaster would have been much deadlier.\n",
        "\n",
        "Oddly enough, the RMS Titanic was actually carrying more lifeboats than required by law which was based on gross tonnage, not number of passengers. If every life boat has been successfully launched at full capacity (most were not fully loaded and two drifted away as she sunk) there would have only been room for 1,178 in total, still well shy the number of people aboard.\n",
        "\n",
        "This is an exploration of using machine learning to determine factors in survival and predict missing values. The dataset is of passengers only (crew numbered about 885 people and had a survival rate of around 24%). Titanic's passengers numbered approximately 1,317 people: 324 in First Class, 284 in Second Class, and 709 in Third Class but this data set has a total pf 1,309 passenger records. \n",
        "\n",
        "The Data\n",
        "--------\n",
        "In order to solve this problem there are several steps I need to go through:\n",
        "\n",
        "*  Import the data\n",
        "*  Analyze for trends\n",
        "*  Fill missing values\n",
        "*  Build features\n",
        "*  Apply machine learning\n",
        "*  Submit results\n",
        "\n",
        "Python is brand new to me and this is my first foray into machine learning, please comment below with any suggestions. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f2f08f83-bb45-2594-6be2-49d0d0b73783"
      },
      "outputs": [],
      "source": [
        "# Handle table-like data and matrices\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Modelling Algorithms\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier\n",
        "\n",
        "# Modelling Helpers\n",
        "from sklearn.preprocessing import Imputer , Normalizer , scale\n",
        "from sklearn.cross_validation import train_test_split , StratifiedKFold\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
        "\n",
        "# Visualisation\n",
        "import matplotlib \n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.pylab as pylab\n",
        "import seaborn as sns\n",
        "\n",
        "# Configure visualisations\n",
        "%matplotlib inline\n",
        "sns.set(style=\"ticks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "ffb3b853-1ce6-1a2c-9074-b075b03e8085"
      },
      "source": [
        "Data Importation and Cleaning\n",
        "---------------------------\n",
        "\n",
        "First I wanted to get an understanding of the data and see how many are missing values.  At first I need to import from csv's both the training and the test data sets. \n",
        "\n",
        "* PassengerId - Unique Identifier\n",
        "* Survival - Survival (0 = No; 1 = Yes)\n",
        "* Pclass 1 - First Class, 2 = Second Class, 3 = Third Class\n",
        "* Name - Last Name, Surname First Name and additional qualifier if needed\n",
        "* Sex - Male or Female\n",
        "* Age - Age, Fractional if Age less than One (1) If the Age is Estimated, it is in the form xx.5\n",
        "* SibSp - Number of Siblings/Spouses Aboard\n",
        "* Parch - Number of Parents/Children Aboard\n",
        "* Ticket - Ticket Number\n",
        "* Fare - Passenger Fare\n",
        "* Cabin - Cabin with the letter being deck and number is cabin, decks should be A-G\n",
        "* Embarked - Port where person board (C = Cherbourg; Q = Queenstown; S = Southampton)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e6e7efc4-2477-a727-891f-2616f2a5b679"
      },
      "outputs": [],
      "source": [
        "# get titanic & test csv files as a DataFrame\n",
        "train_df = pd.read_csv(\"../input/train.csv\")\n",
        "test_df = pd.read_csv(\"../input/test.csv\")\n",
        "\n",
        "# if you want to see where values are missing\n",
        "print(\"THIS IS THE TRAIN_DF INFO\")\n",
        "train_df.info()\n",
        "print(\"-------------------------\")\n",
        "print(\"THIS IS THE TEST_DF INFO\")\n",
        "test_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7b2bcf1a-e788-762a-4f3c-ea5260ac60cc"
      },
      "outputs": [],
      "source": [
        "# Just a quick check of the train_df data, errors are from the NaNs under Age\n",
        "train_df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "06ff11da-e083-8b05-fd0a-fe9db94193e2"
      },
      "source": [
        "Initial View\n",
        "------------------\n",
        "Before getting to far into the depths there are a couple of quick visualizations that we should do just to get a feel for the data.  Sex and Age seem to factor heavily into survival but what else can be done to make modeling it more accurate?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "19966902-1e98-fa73-dec3-fcdd464270ef"
      },
      "outputs": [],
      "source": [
        "# histogram of Sex and Age split by survival\n",
        "g = sns.FacetGrid(train_df, col=\"Sex\", row=\"Survived\", margin_titles=True)\n",
        "g.map(plt.hist,\"Age\",color=\"lightblue\")\n",
        "plt.show()\n",
        "\n",
        "# distribution of age across different classes\n",
        "train_df.Age[train_df.Pclass == 1].plot(kind='kde')    \n",
        "train_df.Age[train_df.Pclass == 2].plot(kind='kde')\n",
        "train_df.Age[train_df.Pclass == 3].plot(kind='kde')\n",
        "plt.xlabel(\"Age\")    \n",
        "plt.title(\"Age Distribution within Classes\")\n",
        "plt.xlim(0,80)\n",
        "plt.legend(('1st Class', '2nd Class','3rd Class'),loc='best')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "80f01024-bc95-259e-2794-7f207b04a489"
      },
      "source": [
        "Feature Analysis\n",
        "-------------------\n",
        "A quick check of features shows that some have greater viability than others.  Class and Sex looks to have stronger correlation and SibSp shows that being single was not in your favor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "456d5b67-a797-d802-be20-2b092774f7f9"
      },
      "outputs": [],
      "source": [
        "sns.stripplot(x=\"Pclass\", y=\"Age\", hue=\"Survived\", data=train_df, jitter=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8e6319c6-244f-2926-4f0b-81b0bd9d88fd"
      },
      "outputs": [],
      "source": [
        "train_df[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "797ad00e-9416-6e68-c01f-146a475df940"
      },
      "outputs": [],
      "source": [
        "sns.stripplot(x=\"Sex\", y=\"Age\", hue=\"Survived\", data=train_df, jitter=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "757bbada-1e7e-6431-389a-a2730c7e4427"
      },
      "outputs": [],
      "source": [
        "train_df[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a99a2b83-67dc-f036-cfd1-a073edbdfcae"
      },
      "outputs": [],
      "source": [
        "sns.stripplot(x=\"SibSp\", y=\"Age\", hue=\"Survived\", data=train_df, jitter=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7e01db9b-e587-db9d-fea1-3bde33c27453"
      },
      "outputs": [],
      "source": [
        "train_df[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f1ebdc58-9246-c4a0-a784-e3ac4ffeeb4c"
      },
      "outputs": [],
      "source": [
        "sns.stripplot(x=\"Parch\", y=\"Age\", hue=\"Survived\", data=train_df, jitter=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c9960db4-853c-6a48-46a9-33c2d3a58934"
      },
      "outputs": [],
      "source": [
        "train_df[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ff228cc8-29bd-05b4-7ac2-f3b98fea4add"
      },
      "outputs": [],
      "source": [
        "train_df.corr()[\"Survived\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "3e88ad70-63eb-fb0d-5877-2def3f9ca31c"
      },
      "source": [
        "Cleaning Data\n",
        "-------------\n",
        "There are some missing values, some are simpler than others. The first one is a quick fill for the missing single fare and embarkment points with the median value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "956e1975-9575-6f24-7819-3fd8d00c21bd"
      },
      "outputs": [],
      "source": [
        "# Plot values for embarkment\n",
        "train_df.Embarked.value_counts().plot(kind='bar', alpha=0.55)\n",
        "plt.title(\"Passengers per Boarding Location\")\n",
        "\n",
        "# Embarked only in train_df, fill the two missing values with the most occurred value, which is \"S\".\n",
        "train_df[\"Embarked\"].value_counts() \n",
        "train_df[\"Embarked\"] = train_df[\"Embarked\"].fillna(\"S\")\n",
        "\n",
        "# Fill in the single missing fare with median value\n",
        "test_df[\"Fare\"].fillna(test_df[\"Fare\"].median(), inplace=True)\n",
        "                                           \n",
        "# Convert fare from float to int\n",
        "train_df['Fare'] = train_df['Fare'].astype(int)\n",
        "test_df['Fare'] = test_df['Fare'].astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b8e578e4-0b3e-ec88-03cf-0333e9a44115"
      },
      "source": [
        "Family Size\n",
        "-------\n",
        "\n",
        "Before tackling the missing Age information, it makes sense to do a little feature engineering now. First we start by the simple creation of FamilySize by adding SibSp and Parch together.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "15da0713-6198-d605-3cf9-1571bc152aa3"
      },
      "outputs": [],
      "source": [
        "# Create a family size variable\n",
        "train_df[\"FamilySize\"] = train_df[\"SibSp\"] + train_df[\"Parch\"]\n",
        "test_df[\"FamilySize\"] = test_df[\"SibSp\"] + test_df[\"Parch\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a385ca48-f5e1-2206-fdd7-efa36f1657b0"
      },
      "source": [
        "Name\n",
        "----\n",
        "Next is taking a look at the Names and see what can be extracted. As you can see below there is the last name, a comma, title, first, middle name and then anything additional in parenthesis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8a58c8c3-9a25-979e-46ed-93e16b692abe"
      },
      "outputs": [],
      "source": [
        "train_df['Name'].head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "200e2f2e-930f-eb73-8807-d1f4ee350487"
      },
      "source": [
        "Title\n",
        "--------\n",
        "\n",
        "The next step is to split out the title and simplfy the possible iterations.  This will replace the multitude of titles with just five: Mr, Mrs, Miss, Master and Rare Title.  Poonan's work was very helpful and I recommend taking a look at her work: https://www.kaggle.com/poonaml/titanic/titanic-survival-prediction-end-to-end-ml-pipeline "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a6dfa0a7-49d3-045b-0c98-da2c85bc04d0"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# function to get the title from a name.\n",
        "def get_title(name):\n",
        "    # Use a regular expression to search for a title.  Titles always consist of capital and lowercase letters, and end with a period.\n",
        "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
        "    #If the title exists, extract and return it.\n",
        "    if title_search:\n",
        "        return title_search.group(1)\n",
        "    return \"\"\n",
        "\n",
        "# Get all the titles\n",
        "titles = train_df[\"Name\"].apply(get_title)\n",
        "\n",
        "#Add in the title column with all the current values so we can then manually change them\n",
        "train_df[\"Title\"] = titles\n",
        "\n",
        "# Titles with very low cell counts to be combined to \"rare\" level\n",
        "rare_title = ['Dona', 'Lady', 'Countess','Capt', 'Col', 'Don', \n",
        "                'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer']\n",
        "\n",
        "# Also reassign mlle, ms, and mme accordingly\n",
        "train_df.loc[train_df[\"Title\"] == \"Mlle\", \"Title\"] = 'Miss'\n",
        "train_df.loc[train_df[\"Title\"] == \"Ms\", \"Title\"] = 'Miss'\n",
        "train_df.loc[train_df[\"Title\"] == \"Mme\", \"Title\"] = 'Mrs'\n",
        "train_df.loc[train_df[\"Title\"] == \"Dona\", \"Title\"] = 'Rare Title'\n",
        "train_df.loc[train_df[\"Title\"] == \"Lady\", \"Title\"] = 'Rare Title'\n",
        "train_df.loc[train_df[\"Title\"] == \"Countess\", \"Title\"] = 'Rare Title'\n",
        "train_df.loc[train_df[\"Title\"] == \"Capt\", \"Title\"] = 'Rare Title'\n",
        "train_df.loc[train_df[\"Title\"] == \"Col\", \"Title\"] = 'Rare Title'\n",
        "train_df.loc[train_df[\"Title\"] == \"Don\", \"Title\"] = 'Rare Title'\n",
        "train_df.loc[train_df[\"Title\"] == \"Major\", \"Title\"] = 'Rare Title'\n",
        "train_df.loc[train_df[\"Title\"] == \"Rev\", \"Title\"] = 'Rare Title'\n",
        "train_df.loc[train_df[\"Title\"] == \"Sir\", \"Title\"] = 'Rare Title'\n",
        "train_df.loc[train_df[\"Title\"] == \"Jonkheer\", \"Title\"] = 'Rare Title'\n",
        "train_df.loc[train_df[\"Title\"] == \"Dr\", \"Title\"] = 'Rare Title'\n",
        "\n",
        "titles = train_df[\"Name\"].apply(get_title)\n",
        "# print(pd.value_counts(titles))\n",
        "\n",
        "#Add in the title column.\n",
        "test_df[\"Title\"] = titles\n",
        "\n",
        "# Titles with very low cell counts to be combined to \"rare\" level\n",
        "rare_title = ['Dona', 'Lady', 'Countess','Capt', 'Col', 'Don', \n",
        "                'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer']\n",
        "\n",
        "# Also reassign mlle, ms, and mme accordingly\n",
        "test_df.loc[test_df[\"Title\"] == \"Mlle\", \"Title\"] = 'Miss'\n",
        "test_df.loc[test_df[\"Title\"] == \"Ms\", \"Title\"] = 'Miss'\n",
        "test_df.loc[test_df[\"Title\"] == \"Mme\", \"Title\"] = 'Mrs'\n",
        "test_df.loc[test_df[\"Title\"] == \"Dona\", \"Title\"] = 'Rare Title'\n",
        "test_df.loc[test_df[\"Title\"] == \"Lady\", \"Title\"] = 'Rare Title'\n",
        "test_df.loc[test_df[\"Title\"] == \"Countess\", \"Title\"] = 'Rare Title'\n",
        "test_df.loc[test_df[\"Title\"] == \"Capt\", \"Title\"] = 'Rare Title'\n",
        "test_df.loc[test_df[\"Title\"] == \"Col\", \"Title\"] = 'Rare Title'\n",
        "test_df.loc[test_df[\"Title\"] == \"Don\", \"Title\"] = 'Rare Title'\n",
        "test_df.loc[test_df[\"Title\"] == \"Major\", \"Title\"] = 'Rare Title'\n",
        "test_df.loc[test_df[\"Title\"] == \"Rev\", \"Title\"] = 'Rare Title'\n",
        "test_df.loc[test_df[\"Title\"] == \"Sir\", \"Title\"] = 'Rare Title'\n",
        "test_df.loc[test_df[\"Title\"] == \"Jonkheer\", \"Title\"] = 'Rare Title'\n",
        "test_df.loc[test_df[\"Title\"] == \"Dr\", \"Title\"] = 'Rare Title'\n",
        "\n",
        "print(train_df['Title'].value_counts())\n",
        "print(test_df['Title'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "f2c89b07-9b61-5d14-9f84-d0ccf5abaf7e"
      },
      "source": [
        "Title and Survival\n",
        "------------------\n",
        "A quick plot of the titles and survival distribution (1=survived, 0=perished)shows there are certain trends that are easy to see. As we discovered earlier Sex was a factor in survival so it is no surprise that the title Mr. was hardest hit. Master was a term for a young boy but that did not help as much as you would think. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "236b9d8d-f666-77ef-3b5e-5a4d475dad18"
      },
      "outputs": [],
      "source": [
        "sns.swarmplot(x=\"Title\", y=\"Age\", hue=\"Survived\", data=train_df)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e1f143f6-4f2c-045c-3480-680025767b6d"
      },
      "outputs": [],
      "source": [
        "train_df[[\"Title\", \"Survived\"]].groupby(['Title'], as_index=False).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d7c2ddc3-0a41-28d3-4df0-9df6dd7c291a"
      },
      "source": [
        "Filling Missing Ages\n",
        "--------------------\n",
        "\n",
        "There is a fair number of missing age.  Plotting the distribution before and after adding in ages allowed me to see what the impact would be.\n",
        "\n",
        "I read through a bunch of other's work on this, some used random forest, or filled using the mean.  I felt like mean produced a spike rather than a distribution that mimiced what was there so I tried a couple of interpolation methods I settled on linear for a fairly even distribution. \n",
        "\n",
        "After the fact I went back and compared interpolation against mean and mean produced a more accurate model so I ended up returning to that but I left the alternative there if you want to try it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "02daa2b5-dbbb-f4cc-5ee0-1bbefdbe2de0"
      },
      "outputs": [],
      "source": [
        "# Plot of Age before filling missing values to visualize the distribution\n",
        "plt.hist(train_df['Age'].dropna(),bins=80)\n",
        "plt.title('Before Correcting Missing Ages')\n",
        "plt.show()\n",
        "\n",
        "# Fill in all missing values with linear interpolation\n",
        "# train_df['Age']= train_df.Age.fillna(train_df.Age.interpolate(method='linear')) \n",
        "train_df['Age']= train_df.Age.fillna(train_df.Age.mean())\n",
        "\n",
        "# Plot of Age again after linear interpolation was completed\n",
        "plt.hist(train_df['Age'],bins=80)\n",
        "plt.title('After Correcting Missing Ages')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ea80c8c3-9408-c85e-af88-9cf153211568"
      },
      "outputs": [],
      "source": [
        "# Plot of Age before filling missing values to visualize the distribution\n",
        "plt.hist(test_df['Age'].dropna(),bins=80)\n",
        "plt.title('Before Correcting Missing Ages')\n",
        "plt.show()\n",
        "\n",
        "# Fill in all missing values with linear interpolation\n",
        "#test_df['Age']= test_df.Age.fillna(test_df.Age.interpolate(method='linear')) \n",
        "\n",
        "test_df['Age']= test_df.Age.fillna(test_df.Age.mean())\n",
        "\n",
        "# Plot of Age again after linear interpolation was completed\n",
        "plt.hist(test_df['Age'],bins=80)\n",
        "plt.title('After Correcting Missing Ages')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "0cd7d674-583b-3b7b-834f-fcbc90e7ac85"
      },
      "source": [
        "Age Bins\n",
        "---------------------------\n",
        "In order to get  little more out of the Age, I decided to bin them into subsets with splits and then spent a little time working on refining them. I tried a couple of different buckets and ended up settling on 0-10, 10-21, 21-55 and 55-81 and applied labels to the grouping."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "10d4c66f-a0f7-7b7d-921e-7a495890a1fa"
      },
      "outputs": [],
      "source": [
        "# Used to bin the ages at the points\n",
        "agepercentile = [0, 10, 21, 55, 81]\n",
        "\n",
        "# Creates a new column binning the ages in to brackets and labeling them with numbers.  \n",
        "train_df[\"AgeBin\"] = pd.cut(train_df['Age'],agepercentile, labels=[\"child\",\"youth\",\"adult\",\"elder\"])\n",
        "test_df[\"AgeBin\"] = pd.cut(test_df['Age'],agepercentile, labels=[\"child\",\"youth\",\"adult\",\"elder\"])\n",
        "\n",
        "sns.swarmplot(x=\"AgeBin\", y=\"Age\", hue=\"Survived\", data=train_df)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "abe4d416-7d5f-d5d9-cbdb-8738501da0da"
      },
      "outputs": [],
      "source": [
        "train_df[[\"AgeBin\", \"Survived\"]].groupby(['AgeBin'], as_index=False).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "64f632f3-907c-f9c8-b089-ea6cd00f6ebd"
      },
      "source": [
        "Fare Binning\n",
        "---------\n",
        "Likewise there is a percentile insight into the fares. I decided to break it into thirds as 2nd Class and 3rd Class overlapped quite a bit in fares. The resulting swarmplot really says it all."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "23b38271-8e25-1375-af4d-ae20f0b5d521"
      },
      "outputs": [],
      "source": [
        "# This takes the age and breaks it into precentiles\n",
        "print(np.percentile(train_df['Fare'],[0,33,66,100]))\n",
        "print(np.percentile(test_df['Fare'],[0,33,66,100]))\n",
        "farepercentile = [0, 8, 26, 513]\n",
        "\n",
        "# Creates a new column binning the ages in to brackets and labeling them with numbers.  \n",
        "# Prencentiles In this case are 0-7, 7=14, 14-31 and 31-513)\n",
        "train_df[\"FareBin\"] = pd.cut(train_df['Fare'],farepercentile, labels=[\"Low\",\"Mid\",\"High\"])\n",
        "test_df[\"FareBin\"] = pd.cut(test_df['Fare'],farepercentile, labels=[\"Low\",\"Mid\",\"High\"])\n",
        "\n",
        "# plot the result\n",
        "sns.swarmplot(x=\"FareBin\", y=\"Age\", hue=\"Survived\", data=train_df)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a616ea3d-eb4f-6adf-d44f-2eceee2cfcc0"
      },
      "outputs": [],
      "source": [
        "train_df[[\"FareBin\", \"Survived\"]].groupby(['FareBin'], as_index=False).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "06cf7082-2fe8-dcd1-dbec-0ab81e98a40f"
      },
      "source": [
        "Assembly of Training Set\n",
        "-------\n",
        "Now it is time to take all that hard work and bring it all into a data set that can be used to predict survival. I dropped Cabin and Ticket information but kept everything else.  Splitting the binned values into individual columns with Boolean values helped with the accuracy. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "23a3d8dd-8229-968a-4892-6bfcbe29f75b"
      },
      "outputs": [],
      "source": [
        "AgeBin = pd.get_dummies( train_df['AgeBin'] , prefix = 'AgeBin')\n",
        "FareBin = pd.get_dummies( train_df['FareBin'] , prefix = 'FareBin')\n",
        "Embarked = pd.get_dummies( train_df['Embarked'] , prefix = 'Embarked')\n",
        "Title = pd.get_dummies( train_df['Title'] , prefix = 'Title')\n",
        "Sex = pd.get_dummies( train_df['Sex'] , prefix = 'Sex' )\n",
        "Pclass = pd.get_dummies( train_df['Pclass'] , prefix = 'Pclass')\n",
        "Age = train_df['Age']\n",
        "Fare = train_df['Fare']\n",
        "SibSp = train_df['SibSp']\n",
        "Parch = train_df['Parch']\n",
        "Survived = train_df['Survived']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c8f33649-f11f-3e5a-a607-508ab9a01c5a"
      },
      "outputs": [],
      "source": [
        "train_X = pd.concat([Age , Fare, SibSp , Parch, Pclass, AgeBin, FareBin, Embarked, Title, Sex], axis=1)\n",
        "train_X.head()\n",
        "\n",
        "# This is just to determing correlation with survived to see how well it worked\n",
        "train_corr = pd.concat([Survived, Age , Fare, SibSp , Parch, Pclass, AgeBin, FareBin, Embarked, Title, Sex], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "ccfb2b30-91fd-6888-46ff-e6dcda1ec297"
      },
      "source": [
        "Assembly of Test Set\n",
        "-------\n",
        "Now time repeat this step for the test data using the same features. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a01a3dd7-174e-0574-546f-4f3dfb912649"
      },
      "outputs": [],
      "source": [
        "AgeBin = pd.get_dummies( test_df['AgeBin'] , prefix = 'AgeBin')\n",
        "FareBin = pd.get_dummies( test_df['FareBin'] , prefix = 'FareBin')\n",
        "Embarked = pd.get_dummies( test_df['Embarked'] , prefix = 'Embarked')\n",
        "Title = pd.get_dummies( test_df['Title'] , prefix = 'Title')\n",
        "Sex = pd.get_dummies( test_df['Sex'] , prefix = 'Sex' )\n",
        "Pclass = pd.get_dummies( test_df['Pclass'] , prefix = 'Pclass')\n",
        "Age = test_df['Age']\n",
        "Fare = test_df['Fare']\n",
        "SibSp = test_df['SibSp']\n",
        "Parch = test_df['Parch']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "85ebff17-1d3e-83a1-a3e5-9bfa753f8811"
      },
      "outputs": [],
      "source": [
        "test_X = pd.concat([Age , Fare, SibSp , Parch, Pclass, AgeBin, FareBin, Embarked, Title, Sex], axis=1)\n",
        "test_X.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "3429b037-ad38-783f-7bd3-eca7a5273b82"
      },
      "source": [
        "Correlation\n",
        "--------------------------\n",
        "A quick plot of the training data with Survived included shows the corelation. Work on the Title seems to have paid off."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ff939eb5-59f1-3e07-be02-f893afd24fcb"
      },
      "outputs": [],
      "source": [
        "corr = train_corr.corr()\n",
        "sns.heatmap(corr)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c34580e4-e0b3-6c6e-4cf3-411399c4e52a"
      },
      "outputs": [],
      "source": [
        "train_corr.corr()['Survived']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "0261bb7e-fffa-64c3-893a-dd4a5ca63763"
      },
      "source": [
        "Time to create all the datasets that get feed into model. \n",
        "\n",
        "train_valid_X is all the features on the training data for the model to learn from\n",
        "train_valid_y is the list of correlating values for the same set with whether or not they survived \n",
        "test_X is all the features on the testing data and has been already defined\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "daa5672f-ce6c-a892-a564-6ad0caede5ef"
      },
      "outputs": [],
      "source": [
        "# Create all datasets that are necessary to train, validate and test models\n",
        "train_valid_X = train_X\n",
        "train_valid_y = train_df.Survived\n",
        "# test_X = test_X\n",
        "train_X , valid_X , train_y , valid_y = train_test_split( train_valid_X , train_valid_y , train_size = .7 )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "22cae3c0-9f1e-3fa6-ad1c-c6f1791de36d"
      },
      "source": [
        "Models\n",
        "------------\n",
        "Here are a selection of models, uncomment whichever you want to run.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "df00e415-8fa4-1bdd-ac89-4e487e5d9033"
      },
      "outputs": [],
      "source": [
        "model = RandomForestClassifier(n_estimators=700,min_samples_leaf=3)\n",
        "# model = SVC()\n",
        "# model = GradientBoostingClassifier()\n",
        "# model = KNeighborsClassifier(n_neighbors = 3)\n",
        "# model = GaussianNB()\n",
        "# model = LogisticRegression()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "600dd191-ac8e-7b4e-bd8a-3fafb01870e9"
      },
      "source": [
        "Now apply the selected model with the datasets and see what you get"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "17180334-3343-382e-fb06-dd6a0f33ef0a"
      },
      "outputs": [],
      "source": [
        "model.fit( train_X , train_y )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "7ab7c668-5a2d-2fa1-48b5-baaceb7ce9f1"
      },
      "source": [
        "This is an interesting way to score the model by comparing both the training and test data to make sure you are not over fitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9af54ed3-6577-a89b-ccfd-32f185415ea3"
      },
      "outputs": [],
      "source": [
        "# Score the model\n",
        "print (model.score( train_X , train_y ) , model.score( valid_X , valid_y ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "40d7ce7f-af6f-93ce-0ded-de3305e1299e"
      },
      "source": [
        "Submission\n",
        "Finally time to submit it all. I am sure I will be back for revisions as I learn more but it was a great way to get my hands wet with machine learning. \n",
        "Here are some of the other kernels that were very helpful:\n",
        "https://www.kaggle.com/startupsci/titanic/titanic-data-science-solutions\n",
        "https://www.kaggle.com/helgejo/titanic/an-interactive-data-science-tutorial\n",
        "https://www.kaggle.com/poonaml/titanic/titanic-survival-prediction-end-to-end-ml-pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ef948054-91e9-d71c-69e4-fa2363c9097b"
      },
      "outputs": [],
      "source": [
        "test_Y = model.predict( test_X )\n",
        "passenger_id = test_df.PassengerId\n",
        "test = pd.DataFrame( { 'PassengerId': passenger_id , 'Survived': test_Y } )\n",
        "test.shape\n",
        "test.head(10)\n",
        "test.to_csv( 'titanic_pred.csv' , index = False )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "84085db1-372b-b1bc-c29b-18185442ea04"
      },
      "source": [
        "Revision History Notes: Submitted"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}