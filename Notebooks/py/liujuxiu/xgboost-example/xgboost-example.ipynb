{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This script shows you how to make a submission using a few\n# useful Python libraries.\n# It gets a public leaderboard score of 0.76077.\n# Maybe you can tweak it and do better...?\n\nimport pandas as pd\nimport xgboost as xgb\nfrom sklearn.preprocessing import LabelEncoder\nimport numpy as np\n#load data\ntrain_df=pd.read_csv('../input/train.csv',header=0)\ntest_df=pd.read_csv('../input/test.csv',header=0)\n\n\n# We'll impute missing values using the median for numeric columns and the most\n# common value for string columns.\n# This is based on some nice code by 'sveitser' at http://stackoverflow.com/a/25562948\nfrom sklearn.base import TransformerMixin\nclass DataFrameImputer(TransformerMixin):\n    def fit(self,X,y=None):\n        self.fill=pd.Series([X[c].value_counts().index[0]\n            if X[c].dtype==np.dtype('O') else X[c].median() for c in X],index=X.columns)\n        return self\n        \n    def transform(self,X,y=None):\n        return X.fillna(self.fill)\n        \nfeature_columns_to_use=['Pclass','Sex','Age','Fare','Parch']\nnonnumeric_columns=['Sex']\n\n\n# Join the features from train and test together before imputing missing values,\n# in case their distribution is slightly different\n\nbig_X=train_df[feature_columns_to_use].append(test_df[feature_columns_to_use])\nbig_X_imputed=DataFrameImputer().fit_transform(big_X)\n\n# XGBoost doesn't (yet) handle categorical features automatically, so we need to change\n# them to columns of integer values.\n# See http://scikit-learn.org/stable/modules/preprocessing.html#preprocessing for more\n# details and options\nle=LabelEncoder()\nfor feature in nonnumeric_columns:\n    big_X_imputed[feature]=le.fit_transform(big_X_imputed[feature])\ntrain_X=big_X_imputed[0:train_df.shape[0]].as_matrix()\ntest_X=big_X_imputed[train_df.shape[0]::].as_matrix()\ntrain_y=train_df['Survived']\n\n# You can experiment with many other options here, using the same .fit() and .predict()\n# methods; see http://scikit-learn.org\n# This example uses the current build of XGBoost, from https://github.com/dmlc/xgboost\ngbm =xgb.XGBClassifier(max_depth=3,n_eatimators=300,learning_rate=0.05).fit(train_X,train_y)\npredictions=gbm.predict(test_X)\nprint(predictions)\n\n\nsubmission=pd.DataFrame({'PassengerId':test_df['PassengerId'],'Survived':predictions})\nsubmission.to_csv(\"submission.csv\",index=False)","execution_count":6,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}