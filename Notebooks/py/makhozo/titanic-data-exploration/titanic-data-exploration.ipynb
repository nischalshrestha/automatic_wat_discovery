{"cells":[{"metadata":{"_cell_guid":"5e297dc0-8b8d-46f9-89d9-c79e4c9d227d","_uuid":"211f4e98e44ad0f398f19bd4df9d7f06897fbc58"},"cell_type":"markdown","source":"<font size=10>**Titanic - Data Exploration** </font>\n\n\nThis kernel is to become familiar with the Kaggle interface and be able to generate and submit the allgender solution."},{"metadata":{"_cell_guid":"fec27f07-9be9-4be8-91ee-fc8650545eae","_uuid":"7957d40058383f8571cd7f442ca6c10e7b116e15"},"cell_type":"markdown","source":"<font size=6>**Exploring the Data** </font>\n\nImport helpful libraries (https://github.com/kaggle/docker-python)\n\n"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestClassifier\n%matplotlib inline\n\nfrom numpy import mean;\nfrom numpy import median;\n\n# Input data files are available in the \"../input/\" directory.\nimport os\nprint(os.listdir(\"../input\"))\noriginal_train = pd.read_csv('../input/train.csv')\ndata_train = original_train\n\noriginal_test = pd.read_csv('../input/test.csv')\ndata_test = original_test\n\n\n\n# Any results you write to the current directory are saved as output.\ndata_train.head()\n","execution_count":21,"outputs":[]},{"metadata":{"_cell_guid":"aa3e62ee-2f13-41ab-9901-f3a7eab7a010","_uuid":"01ac29cb7c93fb9cb7d763d455c450a42795ec81","trusted":true},"cell_type":"code","source":"    data_train.describe()","execution_count":3,"outputs":[]},{"metadata":{"_cell_guid":"5093def9-95f5-480e-b8c5-f8360095a6a5","_uuid":"40d27c33ab2483ca776bd2bb1f938b390cd7d76a"},"cell_type":"markdown","source":"Create some bar plots to look at some variable descriptive statistics."},{"metadata":{"_cell_guid":"e2feffc2-a1ac-4ea8-b420-90ce48901d30","_uuid":"5ddc6670033e66debdab2a8ceb32beeabd97c631","collapsed":true,"trusted":true},"cell_type":"code","source":"# Learning sns.barplot:\n# sns.barplot(x=\"Embarked\", y=\"Survived\", hue=\"Sex\", data=data_train);\n# sns.set_style(\"whitegrid\")\n# sns.barplot(x=\"Pclass\", y = \"Survived\", data=data_train);\n# sns.barplot(x=\"Survived\", y = \"Sex\", data=data_train, order=[\"female\", \"male\"]);\n# sns.barplot(x=\"Pclass\", y=\"Survived\", data=data_train, estimator=mean, ci=68, capsize=0.2, palette = \"Blues_d\");\n# sns.barplot(x=\"Survived\", y = \"Pclass\", orient=\"h\", data=data_train);","execution_count":4,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"sns.set_style(\"whitegrid\");\nfig, ax =plt.subplots(1,2)\nsns.barplot(x=\"SibSp\", y=\"Survived\", palette = \"Blues_d\", ci=None, data=data_train, ax=ax[0]);\nsns.barplot(x=\"Parch\", y=\"Survived\", palette = \"Blues_d\", ci=None, data=data_train, ax=ax[1]);\n# fig.show()\nfig.tight_layout() \nfig, ax =plt.subplots(1,2)\n# sns.barplot(x=\"Parch\", y=\"Survived\", palette = \"Blues_d\", ci=None, data=data_train);\nsns.barplot(x=\"Pclass\",  hue = \"Sex\", y=\"Survived\", palette = \"Blues_d\", ci=None, data=data_train, ax=ax[0]);\nsns.barplot(x=\"Survived\", y=\"Age\", palette = \"Blues_d\", orient=\"v\", data=data_train, ci= None, ax=ax[1]);\n\nfig.tight_layout() \nfig, ax =plt.subplots(1,2)\nsns.barplot(x=\"Embarked\",  y = \"Survived\", palette = \"Blues_d\", data=data_train, ax=ax[0]);\nsns.distplot(data_train['Age'].dropna(how='any'),ax=ax[1]);\nfig.tight_layout() \n\nfig, ax = plt.subplots()\nfor a in [data_train[data_train[\"Survived\"]==1][\"Age\"].dropna(how='any'),             #yellow\n          data_train[data_train[\"Survived\"]==0][\"Age\"].sample(342).dropna(how='any')]: #blue\n    sns.distplot(a, bins=range(1, 81, 10), ax=ax, kde=False)\n    \nfig.tight_layout() \n# ax.set_xlim([0, 100])\n# 342+549 #549 did not survive","execution_count":5,"outputs":[]},{"metadata":{"_cell_guid":"e2c5497e-4358-4c4b-bb16-1c507945d1bb","_uuid":"9973f728ab006ec0f6bd64f26715796019e82989"},"cell_type":"markdown","source":"Look at some of the unique values in the data"},{"metadata":{"_cell_guid":"c2cdb047-f023-42dc-8c85-4f4a4a70340f","_uuid":"1a57ba736e06837f101bb28872c62433b3604409","trusted":true},"cell_type":"code","source":"\ndata_train[\"Parch\"].describe()\n# print(\"Unique PClass values\",data_train.Pclass.unique())\nfor column in data_train.drop([\"PassengerId\", \"Name\", \"Ticket\"],axis=1): #axis=1 specifics columns; axis=0 specifies rows\n    print(\"Unique \", column, \" values\",data_train[column].unique())\n\n","execution_count":7,"outputs":[]},{"metadata":{"_cell_guid":"5bfad7ae-76b2-41c8-aaea-f7042073632c","_uuid":"0e7f9ea34ab3fbbaf2bcb27d10dd32f4777ba085","trusted":true},"cell_type":"code","source":"data_train.describe()","execution_count":8,"outputs":[]},{"metadata":{"_cell_guid":"d1fbf9b5-b05b-4b12-9492-b2a106337376","_uuid":"23ba51325f680eb5e20a51fe24d959ca3bd74fe2"},"cell_type":"markdown","source":"<font size=6>**Transforming the Data** </font>\n\nFirst let's modify some of the variables for our needs:\n* \"Cabin\" is all over the place. Let's use only the first character, since it *might* be important.\n* We can safely drop Passenger ID as it is highly unlikely it is correlated to any existing variables.\n* As interesting as it could be for text analysis, let's drop Name and Ticket too.\n* Convert 'Embarked' and 'Cabin' and 'Gender' into dummy variables"},{"metadata":{"_cell_guid":"7fa3644f-928d-45cd-97b2-c912b820cb92","_uuid":"1d3449ced1a495dd9b55a41d8f8dc7c68a7f228f","trusted":true},"cell_type":"code","source":"#Reload data just in case\ndata_train = original_train\ndata_test = original_test\n\n\n\ndata_train[\"Cabin\"] = data_train[\"Cabin\"].str[0]\ndata_test[\"Cabin\"] = data_test[\"Cabin\"].str[0]\ndata_train = data_train.drop(columns=[\"PassengerId\"])\n# data_test = data_test.drop(columns=[\"PassengerId\"])\ndata_train = data_train.drop(columns=[\"Name\", \"Ticket\"])\ndata_test = data_test.drop(columns=[\"Name\", \"Ticket\"])\n# funtemp = data_train[\"Embarked\"] == \"C\"\ndata_train = pd.get_dummies(data_train, columns=[\"Embarked\", \"Cabin\", \"Sex\"], prefix=[\"embark\", \"cabin\", \"g\"])\ndata_test = pd.get_dummies(data_test, columns=[\"Embarked\", \"Cabin\", \"Sex\"], prefix=[\"embark\", \"cabin\", \"g\"])\ndata_test[\"cabin_T\"] = 0 #because this variable never appeared in the test set, which removed it from the dummies function\ndata_test.head()","execution_count":110,"outputs":[]},{"metadata":{"_cell_guid":"4290919e-272f-4c34-9105-f290b2aebbe8","_uuid":"473171c3ad1533d5b5f23a43ccc009b5d6ca0dd1"},"cell_type":"markdown","source":"# Missing Values\n\nNotice how there are lots of missing values in the dataset. Let us investigate so we can impute new values in their place.\n\nFirst investigate existing correlations between variables; this automatically ignores records with missing values."},{"metadata":{"_cell_guid":"d58c4d5a-310e-4350-a755-c2e43b0cd94d","_uuid":"98d062e352069b15a9df4fede1f0b4efa909641a","trusted":true},"cell_type":"code","source":"\ncorr = original_train.corr()\nsns.heatmap(corr, cmap=\"Blues_r\",annot=True, \n            xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values)","execution_count":111,"outputs":[]},{"metadata":{"_cell_guid":"e3fc8b49-2793-4a50-9fe4-45d8327a2abc","_uuid":"aaebed983e9a4e5ffb8733276e7fe7bfff62864e"},"cell_type":"markdown","source":"After some experimentation, Pclass and Parch are highly correlated with Age and not each other; we use them to impute the missing ages. SibSp was also a good candidate, but including them as another subgroup creates groups made of entirely NaN ages, which we cannot take the mean of. Therefore, let us impute as follows:\n* Group all observations by Pclass and Parch (that is, 3 * 7 possible groups for the 851 observations to be in).\n* Find the mean age of each of the groups. This mean calculation ignores all missing values of Age.\n* Replace all missing values with the mean of that observation's group.\n\nNote this all happens in the first line of code."},{"metadata":{"_cell_guid":"b920da0e-abb2-4745-ae56-29748dff72db","_uuid":"e81c4f5ee254967fa641bcb3a775691ab3dfdbf0","trusted":true},"cell_type":"code","source":"\ndata_train[\"Age\"] = data_train[\"Age\"].fillna(data_train.groupby([\"Pclass\",\"Parch\"])[\"Age\"].transform(\"mean\")) \ndata_test[\"Age\"] = data_test[\"Age\"].fillna(data_train.groupby([\"Pclass\",\"Parch\"])[\"Age\"].transform(\"mean\")) \n    # Note - test set imputed FROM TRAINING set\n\nsns.distplot(data_train.Age);\n\n#Check: no records exist where Age = NaN\nprint(data_train[data_train[\"Age\"].isnull()])\nprint(data_test[data_test[\"Age\"].isnull()])\n","execution_count":112,"outputs":[]},{"metadata":{"_cell_guid":"0b7a5c0c-4dc7-49d2-9a3f-cc8e56feabd4","_uuid":"0d90cd818c56c36a959b10d2d5e41159b2d55145"},"cell_type":"markdown","source":"# Binning\n\nLet's consider the effect of binning 'Age' and 'Fare' into fewer categories.\n\n[Future project to analyze what the best bins are, and if it's even effective]"},{"metadata":{"_cell_guid":"e4e47b12-490a-4cc0-9d7a-2fd8347ab271","_uuid":"ae9a89452b8206348152264ef9691f653f474040","trusted":true},"cell_type":"code","source":"# data_train[\"Age\"] = original_train[\"Age\"]\n# data_train[\"Fare\"] = original_train[\"Fare\"]\n\nage_describe =  original_train[\"Age\"].describe()\nagebins = (0, age_describe[4], age_describe[5], age_describe[6], age_describe[7])\n\ngroup_names = [\"Child\", \"Young Adult\", \"Adult\", \"Senior\"]\ndata_train[\"Age\"] = pd.cut(data_train[\"Age\"], agebins, labels=group_names)\ndata_test[\"Age\"] = pd.cut(data_test[\"Age\"], agebins, labels=group_names) # Note - test set imputed FROM TRAINING set\nprint(data_train[\"Age\"].value_counts())\nprint(data_test[\"Age\"].value_counts())\n\nfare_describe =  original_train[\"Fare\"].describe()\nfarebins = (0, fare_describe[4], fare_describe[5], fare_describe[6], fare_describe[7])\n\ngroup_names = [\"Budget\", \"Cheap\", \"Regular\", \"Expensive\"]\ndata_train[\"Fare\"]  = pd.cut(data_train[\"Fare\"], farebins, labels=group_names)\ndata_test[\"Fare\"]  = pd.cut(data_test[\"Fare\"], farebins, labels=group_names)\nprint(data_train[\"Fare\"].value_counts())\nprint(data_test[\"Fare\"].value_counts()) # Note - test set imputed FROM TRAINING set\n\n","execution_count":113,"outputs":[]},{"metadata":{"_cell_guid":"b528abf2-3e07-4e19-a5ae-c987de18c7a2","_uuid":"2dc1d50cf891a2c6210f95995fb48d7f7244da3a","trusted":true},"cell_type":"code","source":"data_train.head()","execution_count":114,"outputs":[]},{"metadata":{"_cell_guid":"56ec6631-f0dc-4405-9129-577aa7703af6","_uuid":"ca584e9b16b4def86023825a3e45c6d312fb766a","trusted":true},"cell_type":"code","source":"data_train = data_train.drop(columns=[\"Fare\", \"Age\"])\ndata_test = data_test.drop(columns=[\"Fare\", \"Age\"])\ndata_train.head()\n","execution_count":115,"outputs":[]},{"metadata":{"_cell_guid":"86f5490f-3c16-419f-8a65-bf874349aa46","_uuid":"8117a4540f96e4fc2a4933ad7e0607be7ad56f92"},"cell_type":"markdown","source":"<font size=6>**Data Mangling to Get a Classifer Out the Door** </font>"},{"metadata":{"_cell_guid":"0ea5c97f-5ced-44f0-8ff0-6834d6fe9977","_uuid":"29094803ba3099d7fba9676e6110d9214d23b2eb","trusted":true},"cell_type":"code","source":"X_train = data_train.drop(\"Survived\", axis=1)\nY_train = data_train[\"Survived\"]\nX_test  = data_test.drop(\"PassengerId\", axis=1).copy()\nX_train.shape, Y_train.shape\nX_test.shape\n","execution_count":116,"outputs":[]},{"metadata":{"_cell_guid":"8d701f9f-767c-4a87-ab10-86855dfaa177","_uuid":"fc0a7884f05549d0e96957b8f086886d4d704ac6","trusted":true},"cell_type":"code","source":"# Y_train.describe()\n# X_train.describe()\nX_test.describe()","execution_count":117,"outputs":[]},{"metadata":{"_cell_guid":"c4c726a1-e3d1-46f9-af2a-52f1137e546e","_uuid":"a4dcacbe7860ce381839183db63f65cc3b2f4394","trusted":true},"cell_type":"code","source":"# Random Forest\n# https://www.kaggle.com/startupsci/titanic-data-science-solutions\n\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nacc_random_forest\n","execution_count":129,"outputs":[]},{"metadata":{"_cell_guid":"36da4700-d73e-42ea-b1f9-a5e89a537a0f","_uuid":"6cf0322bb12e1ea635606b0a49e71227fc3d0c72"},"cell_type":"markdown","source":"wait that's actually decent even though it's training data"},{"metadata":{"_uuid":"0d777a133a0c695f857b127a19413a43c1e17485"},"cell_type":"markdown","source":" <font size=6>**Getting a Submission Out the Door** </font>"},{"metadata":{"trusted":true,"_uuid":"5e955d1640d86d616b25c1693a87c28632bd430e"},"cell_type":"code","source":"\nsubmission = pd.DataFrame({\n        \"PassengerId\": original_test[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\nsubmission.head()\n# submission.to_csv('../output/submission.csv', index=False)\nsubmission.to_csv('submission24Apr2018.csv', index = False)\n","execution_count":143,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"111445165eeb59b9de6b49d862adbc2fa7586bbe"},"cell_type":"code","source":"\noriginal_gendersubmission =  pd.read_csv('../input/gender_submission.csv')\noriginal_gendersubmission.head()","execution_count":140,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}