{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"'''\nWorkflow stages\nThe competition solution workflow goes through seven stages described in the Data Science Solutions book.\n1) Question or problem definition.\n2) Acquire training and testing data.\n3) Wrangle, prepare, cleanse the data.\n4) Analyze, identify patterns, and explore the data.\n5) Model, predict and solve the problem.\n6) Visualize, report, and present the problem solving steps and final solution.\n7) Supply or submit the results.\n'''\n# data analysis and wrangling\nimport pandas as pd\nimport numpy as np\nimport random as rnd\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n#--------------------------------------------------------\n# 1) Question or problem definition.\n#--------------------------------------------------------\n\n#--------------------------------------------------------\n# 2) Acquire training and testing data.\n#--------------------------------------------------------\ntrain_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')\n\n#--------------------------------------------------------\n# 3) Wrangle, prepare, cleanse the data.\n#--------------------------------------------------------\ntrain_df = train_df.drop(['Ticket', 'Cabin'], axis=1)\ntest_df = test_df.drop(['Ticket', 'Cabin'], axis=1)\ncombine = [train_df, test_df]\n\n# We decide to retain the new Title feature for model training.\nfor dataset in combine: dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\n# We can replace many titles with a more common name or classify them as Rare\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \n# We can convert the categorical titles to ordinal.\ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\n# Now we can safely drop the Name feature from training and testing datasets. We also do not need the PassengerId feature in the training dataset.    \ntrain_df = train_df.drop(['Name', 'PassengerId'], axis=1)\ntest_df = test_df.drop(['Name'], axis=1)\ncombine = [train_df, test_df]\n\n#--------------------------------------------------------\n# 4) Analyze, identify patterns, and explore the data.\n#--------------------------------------------------------\n\n# Converting a categorical feature\nfor dataset in combine: dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n    \n# Now we iterate over Sex (0 or 1) and Pclass (1, 2, 3) to calculate guessed values of Age for the six combinations.    \nguess_ages = np.zeros((2,3))\nfor dataset in combine:\n    for i in range(0, 2):\n        for j in range(0, 3):\n            guess_df = dataset[(dataset['Sex'] == i) & (dataset['Pclass'] == j+1)]['Age'].dropna()\n            age_guess = guess_df.median()\n            guess_ages[i,j] = int( age_guess/0.5 + 0.5 ) * 0.5\n    for i in range(0, 2):\n        for j in range(0, 3): dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == i) & (dataset.Pclass == j+1), 'Age'] = guess_ages[i,j]\n    dataset['Age'] = dataset['Age'].astype(int)\n\n#Let us create Age bands and determine correlations with Survived.\ntrain_df['AgeBand'] = pd.cut(train_df['Age'], 5)\n\n# Let us replace Age with ordinals based on these bands.\nfor dataset in combine:    \n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age']\n\n# We can not remove the AgeBand feature.\ntrain_df = train_df.drop(['AgeBand'], axis=1)\ncombine = [train_df, test_df]\n\n# Create new feature combining existing features\n\n# We can create a new feature for FamilySize\nfor dataset in combine: dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n    \n# We can create another feature called IsAlone.\nfor dataset in combine:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n    \n# Let us drop Parch, SibSp, and FamilySize features in favor of IsAlone\ntrain_df = train_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ntest_df = test_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ncombine = [train_df, test_df]\n\n# We can also create an artificial feature combining Pclass and Age.\nfor dataset in combine: dataset['Age*Class'] = dataset.Age * dataset.Pclass\n    \n# Completing a categorical feature\nfreq_port = train_df.Embarked.dropna().mode()[0]\nfor dataset in combine: dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)\n\n# Converting categorical feature to numeric\nfor dataset in combine: dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n    \n# Quick completing and converting a numeric feature\ntest_df['Fare'].fillna(test_df['Fare'].dropna().median(), inplace=True)\n\n# We can not create FareBand.\ntrain_df['FareBand'] = pd.qcut(train_df['Fare'], 4)\n\n# Convert the Fare feature to ordinal values based on the FareBand.\nfor dataset in combine:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\ntrain_df = train_df.drop(['FareBand'], axis=1)\ncombine = [train_df, test_df]\n\n#--------------------------------------------------------\n# 5) Model, predict and solve the problem.\n#--------------------------------------------------------\nX_train = train_df.drop(\"Survived\", axis=1)\nY_train = train_df[\"Survived\"]\nX_test  = test_df.drop(\"PassengerId\", axis=1).copy()\n\n# Logistic Regression\nlogreg = LogisticRegression(solver = 'lbfgs')\nlogreg.fit(X_train, Y_train)\nY_pred_logreg = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nacc_log\n\n# Support Vector Machines\nsvc = SVC(gamma = 'auto') #gamma will change from 'auto' to 'scale'\nsvc.fit(X_train, Y_train)\nY_pred_svc = svc.predict(X_test)\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\nacc_svc\n\n# KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, Y_train)\nY_pred_knn = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\nacc_knn\n\n# Gaussian Naive Bayes\ngaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\nY_pred_gaussian = gaussian.predict(X_test)\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\nacc_gaussian\n\n# Perceptron\nperceptron = Perceptron(max_iter=1000)\nperceptron.fit(X_train, Y_train)\nY_pred_perceptron = perceptron.predict(X_test)\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\nacc_perceptron\n\n# Linear SVC\nlinear_svc = LinearSVC(max_iter=300000) #ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\nlinear_svc.fit(X_train, Y_train)\nY_pred_linear_svc = linear_svc.predict(X_test)\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\nacc_linear_svc\n\n# Stochastic Gradient Descent\nsgd = SGDClassifier(max_iter=1000)\nsgd.fit(X_train, Y_train)\nY_pred_sgd = sgd.predict(X_test)\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\nacc_sgd\n\n# Decision Tree\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_pred_decision_tree = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nacc_decision_tree\n\n# Random Forest\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nY_pred_random_forest = random_forest.predict(X_test)\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nacc_random_forest\n\n#--------------------------------------------------------\n# 6) Visualize, report, and present the problem solving steps and final solution.\n#--------------------------------------------------------\n# Model evaluation\nmodels = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', 'Random Forest', 'Naive Bayes', 'Perceptron', 'Stochastic Gradient Decent', 'Linear SVC', 'Decision Tree'],\n    'Score': [acc_svc, acc_knn, acc_log, acc_random_forest, acc_gaussian, acc_perceptron, acc_sgd, acc_linear_svc, acc_decision_tree]})\nmodels.sort_values(by='Score', ascending=False)\n# Model\tScore\n# 3\tRandom Forest\t86.76\n# 8\tDecision Tree\t86.76\n# 1\tKNN\t84.74\n# 0\tSupport Vector Machines\t83.84\n# 2\tLogistic Regression\t80.36\n# 6\tStochastic Gradient Decent\t79.69\n# 7\tLinear SVC\t78.90\n# 5\tPerceptron\t76.99\n# 4\tNaive Bayes\t72.28\n\n# Ensemble of 5 best predictions\ndef find_mean_predicton(ndarrays=[], weights=[]):\n    # 1) dublicate some arrays\n    for w in weights: \n        array = ndarrays[weights.index(w)]\n        for x in range(w-1): ndarrays.append(array)\n    # 2) find mean = sum / len\n    s = np.zeros(len(ndarrays[0]))  \n    for x in ndarrays: s += x\n    return s/len(ndarrays) # ndarray([...])\n\n# this ensenble gives 0.77511\n# Y_pred = find_mean_predicton([Y_pred_random_forest, Y_pred_decision_tree, Y_pred_knn, Y_pred_svc, Y_pred_logreg], weights=[3,3,2,1,1]).round() #array([ 13, 133, 1333, 4, 8])\n\n# Random forest gives 0.78468\nY_pred = Y_pred_random_forest\n\n# Decision gives 0.78468\n# Y_pred = Y_pred_decision_tree\n\n# KNN gives 0.77990\n# Y_pred = Y_pred_knn\n\n# this ensenble gives 0.77511\n# Y_pred = find_mean_predicton([Y_pred_random_forest, Y_pred_decision_tree, Y_pred_knn], weights=[3,3,1]).round() #array([ 13, 133, 1333, 4, 8])\n\n#--------------------------------------------------------\n# 7) Supply or submit the results.\n#--------------------------------------------------------\nsubmission = pd.DataFrame({\"PassengerId\": test_df[\"PassengerId\"], \"Survived\": Y_pred.astype(int)})\n#submission.to_csv('../output/gender_submission.csv', index=False)\nsubmission\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"074a036beff927a0c1c380baa437cee4635bc27e"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}