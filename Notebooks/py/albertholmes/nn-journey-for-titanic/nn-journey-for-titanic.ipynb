{"nbformat": 4, "cells": [{"source": ["__author__ = 'Albert: https://www.kaggle.com/albertholmes'\n", "# File nn_valid.py\n", "# Use the K-Fold corss validation to do experiment based on titanic data\n", "\n", "# Import modules\n", "import numpy as np\n", "import pandas as pd\n", "import gc\n", "from sklearn.base import TransformerMixin\n", "from sklearn.model_selection import StratifiedKFold\n", "from sklearn.preprocessing import LabelEncoder\n", "from sklearn.preprocessing import OneHotEncoder\n", "from matplotlib import pyplot as plt\n", "\n", "%matplotlib inline\n", "\n", "# Mean Square Error function\n", "mse = lambda actual, pred: np.mean((actual - pred) ** 2)"], "outputs": [], "cell_type": "code", "metadata": {"scrolled": true, "_cell_guid": "9c8cdbae-9a11-4cb7-a516-b95a5fb54c30", "collapsed": true, "_uuid": "c264a3976c9bc7fcceadf6096c518592bb4e024d"}, "execution_count": 1}, {"source": ["class DataFrameImputer(TransformerMixin):\n", "    \"\"\"\n", "    TransformerMixin is an interface that you can create your own\n", "    transformer or models.\n", "    The .fit_transform method that calls .fit and .transform methods,\n", "    you should define the two methods by yourself.\n", "    \"\"\"\n", "    def fit(self, X, y=None):\n", "        \"\"\"\n", "        The pandas.Series.value_counts method returns the object\n", "        containing counts of unique values.\n", "        The resulting object will be in descending order so that\n", "        the first element is the most frequently-occurring element.\n", "\n", "        np.dtype('O'): The 'O' means the Python objects\n", "        \"\"\"\n", "        d = [X[c].value_counts().index[0] if X[c].dtype == np.dtype('O')\n", "             else X[c].median() for c in X]\n", "\n", "        self.fill = pd.Series(d, index=X.columns)\n", "        return self\n", "\n", "    def transform(self, X, y=None):\n", "        return X.fillna(self.fill)\n", "\n", "# Garbage collection\n", "gc.enable()"], "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "83c899a5-9dca-4e9b-a328-35c815cae00e", "collapsed": true, "_uuid": "7d475ed1e9d075a78bbb89e05933c533d40853d7"}, "execution_count": 2}, {"source": ["# Read the data\n", "train = pd.read_csv('../input/train.csv')\n", "\n", "# Get target value\n", "target = train['Survived'].values\n", "del train['Survived']\n", "\n", "# Prepare the k-fold cross validation\n", "skf = StratifiedKFold(n_splits=10)\n", "\"\"\"\n", "print(skf.get_n_splits(train, target))\n", "\"\"\"\n", "# Add new features\n", "train['FamilySize'] = train['SibSp'] + train['Parch'] + 1\n", "\n", "feature_names = ['Pclass', 'Sex', 'Age', 'Fare', 'Embarked', 'FamilySize']\n", "nonnumeric_fea = ['Sex', 'Embarked']\n", "categorical_fea = ['Pclass', 'Sex', 'Embarked']\n", "\n", "# Impute the missing values\n", "imputed = DataFrameImputer().fit_transform(train[feature_names])"], "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "457effcb-cdcc-4e95-880e-097fc727df38", "collapsed": true, "_uuid": "6ff75d5917c97c7b2fa1e30dc0247b8ae5606ac5"}, "execution_count": 7}, {"source": ["\"\"\"\n", "Preprocessing the nonnumeric feature\n", "Encode labels with value between 0 and (n_classes - 1)\n", "\"\"\"\n", "le = LabelEncoder()\n", "for feature in nonnumeric_fea:\n", "    imputed[feature] = le.fit_transform(imputed[feature])\n", "\n", "\"\"\"\n", "Use one hot encoder to get new feature from categorical feature\n", "\"\"\"\n", "enc = OneHotEncoder()\n", "chosen_features = imputed[categorical_fea]\n", "new_fea = enc.fit_transform(chosen_features).toarray()\n", "for feature in categorical_fea:\n", "    del imputed[feature]\n", "\n", "train = imputed.values\n", "train = np.concatenate((train, new_fea), axis=1)"], "outputs": [], "cell_type": "code", "metadata": {}, "execution_count": 8}, {"source": ["\"\"\"\n", "Normalize the features\n", "\"\"\"\n", "for col in range(train.shape[1]):\n", "    max_value = max(train[:, col])\n", "    train[:, col] /= max_value\n", "\n", "import keras \n", "from keras.models import Sequential # intitialize the ANN\n", "from keras.layers import Dense      # create layers\n", "\n", "nn_errors = []\n", "\n", "for train_idx, valid_idx in skf.split(train, target):\n", "    # print('TRAIN:', train_idx, 'VALID:', valid_idx)\n", "    tra, val = train[train_idx], train[valid_idx]\n", "    target_tra, target_val = target[train_idx], target[valid_idx]\n", "\n", "    # Initialising the NN\n", "    model = Sequential()\n", "\n", "    # layers\n", "    model.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11))\n", "    model.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n", "    model.add(Dense(units = 5, kernel_initializer = 'uniform', activation = 'relu'))\n", "    model.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n", "\n", "    # Compiling the ANN\n", "    model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n", "\n", "    # Train the ANN\n", "    # model.fit(tra, target_tra, batch_size = 32, epochs = 200)\n", "    model.fit(tra, target_tra, batch_size=64, epochs=5)\n", "    \n", "    pred = model.predict(val)\n", "    error = mse(target_val, pred)\n", "    \n", "    print('NN MSE: %.3f' % (error))\n", "    \n", "    nn_errors.append(error)\n", "\n", "nn_final_error = sum(nn_errors) / len(nn_errors)\n", "print('NN FINAL MSE: %.3f' % (nn_final_error))"], "outputs": [], "cell_type": "code", "metadata": {}, "execution_count": 19}], "metadata": {"language_info": {"nbconvert_exporter": "python", "pygments_lexer": "ipython3", "name": "python", "version": "3.6.3", "codemirror_mode": {"name": "ipython", "version": 3}, "mimetype": "text/x-python", "file_extension": ".py"}, "kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}}, "nbformat_minor": 1}