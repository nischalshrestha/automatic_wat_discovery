{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "17f29693-c88a-d1c5-d463-fd7613ba0691"
      },
      "source": [
        "# Introduction\n",
        "This is my first time creating a kernel notebook. Please feel free to comment or make suggestions on improvments, especially with feature engineering. Thanks!\n",
        "\n",
        "In creating this Kernel I referenced a numer of other helpful Kernels and worked through the below suggested tutorials: \n",
        "* [DataCamp Python](https://www.datacamp.com/community/open-courses/kaggle-python-tutorial-on-machine-learning)\n",
        "* [Kaggle Python tutorial](https://www.kaggle.com/c/titanic/details/getting-started-with-python)\n",
        "* [Random forest benchmark r](https://www.kaggle.com/thamaliw/titanic/random-forest-benchmark-r)\n",
        "* [Could the titanic have been saved](https://www.kaggle.com/daryadedik/titanic/could-the-titanic-have-been-saved)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "19cccade-83d7-db9b-a635-d8c43edc3b1c"
      },
      "source": [
        "## Load the data & import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ae36e6d9-98ca-6e79-5f81-9ab63ad0409e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy.random import rand\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesRegressor\n",
        "from sklearn import tree\n",
        "\n",
        "# Load the train and test datasets to create two DataFrames\n",
        "base_url = \"../input\"\n",
        "train_url = base_url + \"/train.csv\"\n",
        "train = pd.read_csv(train_url)\n",
        "\n",
        "test_url = base_url + \"/test.csv\"\n",
        "test = pd.read_csv(test_url)\n",
        "\n",
        "# Combine test and training to facilitate cleanup and pre-processing\n",
        "full_data = pd.concat([train, test], axis=0)\n",
        "\n",
        "print (\"Full data {}\\n\".format(full_data.shape))\n",
        "\n",
        "# Lets see how we are doing with missing values\n",
        "print(\"Full data missing \\n{}\\n\".format(full_data.isnull().sum()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "2ab888fa-86ca-7a1c-7641-236506dd5a38"
      },
      "source": [
        "## Cleaning the Data\n",
        "### Simple imputations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "07ab1f34-e9bc-d019-3be0-43d32ffae282"
      },
      "outputs": [],
      "source": [
        "# Remove Warnings from Pandas\n",
        "pd.options.mode.chained_assignment = None  # default='warn'\n",
        "\n",
        "# Lets fill the missing 'Embarked' values with the most occurred value, which is \"S\".\n",
        "# 72.5% of people left from Southampton.\n",
        "full_data.Embarked.fillna('S', inplace=True)\n",
        "\n",
        "# There is only one missing 'Fare' and its in the test dataset\n",
        "# Let's just go ahead and fill the fare value in with the median fare\n",
        "full_data.Fare.fillna(full_data.Fare.median(), inplace=True)\n",
        "\n",
        "# Lets take a look at Cabins...\n",
        "# It looks like 77% and 78% of these fields are empty.\n",
        "#But just going to map and see what happens\n",
        "\n",
        "# Looking at the data it may be better to pull the Deck from the Cabin \n",
        "# but just statring here\n",
        "#print(\"Null Cabins in training {:.4f}\".format(1-(train[\"Cabin\"].value_counts().sum()/ len(train[\"Cabin\"]))))\n",
        "#print(\"Null Cabins in test {:.4f}\".format(1-(test[\"Cabin\"].value_counts().sum()/len(test[\"Cabin\"]))))\n",
        "def clean_cabin(x):\n",
        "    try:\n",
        "        return x[0]\n",
        "    except TypeError:\n",
        "        return \"None\"\n",
        "    \n",
        "full_data.Cabin = full_data.Cabin.apply(clean_cabin)\n",
        "full_data = pd.concat([full_data, pd.get_dummies(full_data['Cabin'], prefix='Cabin')], axis=1)\n",
        "\n",
        "      \n",
        "# Also ignoring Ticket for now as it is not clear to me what to do with it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "5b3f7f09-2b44-f114-94f7-af350502f368"
      },
      "source": [
        "### Normalize Data for use with Classification tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f64c171c-a86d-b049-5ad8-bbd2b5645851"
      },
      "outputs": [],
      "source": [
        "# Create categories for Sex and Embarked\n",
        "full_data = pd.concat([full_data, pd.get_dummies(full_data['Sex'], prefix='Sex')], axis=1)\n",
        "\n",
        "full_data = pd.concat([full_data, pd.get_dummies(full_data['Embarked'], prefix='Embarked')], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "ebe6419e-e2e4-8f6a-e2ef-5e81f618f46b"
      },
      "source": [
        "### Derive some new features from the Name field"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "75741522-4a6a-4fa4-bd63-4a4c190fc1c7"
      },
      "outputs": [],
      "source": [
        "# Extract the title from the name\n",
        "def get_title(name):\n",
        "    index_comma = name.index(',') + 2\n",
        "    title = name[index_comma:]\n",
        "    index_space = title.index('.') + 1\n",
        "    title = title[0:index_space]\n",
        "    return title\n",
        "\n",
        "# Helper method to show unique_titles\n",
        "unique_titles = {}\n",
        "def get_unique_titles(name):\n",
        "    title = get_title(name)\n",
        "    if title in unique_titles:\n",
        "        unique_titles[title] += 1\n",
        "    else:\n",
        "        unique_titles[title] = 1\n",
        "\n",
        "# Uncomment to show the unique titles in the data set\n",
        "#full_data[\"Name\"].apply(get_unique_titles)\n",
        "#print(unique_titles)\n",
        "\n",
        "#Upon review of the unique titles we consolidate on the below mappings as optimal\n",
        "def map_title(name):\n",
        "    title = get_title(name)\n",
        "    #should add no key found exception\n",
        "    title_mapping = {\"Mr.\": 1, \"Miss.\": 2, \"Ms.\": 10, \"Mrs.\": 3, \"Master.\": 4, \"Dr.\": 5,\n",
        "                     \"Rev.\": 6, \"Major.\": 7, \"Col.\": 7, \"Don.\": 7, \"Sir.\": 7, \"Capt.\": 7,\n",
        "                     \"Mlle.\": 8, \"Mme.\": 8, \"Dona.\": 9, \"Lady.\": 9, \"the Countess.\": 9,\n",
        "                     \"Jonkheer.\": 9}\n",
        "    return title_mapping[title]\n",
        "\n",
        "\n",
        "# Create a new field with a Title\n",
        "full_data[\"Title\"] = full_data[\"Name\"].apply(map_title)\n",
        "\n",
        "# Extract the last name from the title\n",
        "def get_last_name(name):\n",
        "    index_comma = name.index(',')\n",
        "    last_name = name[0:index_comma:]\n",
        "    #print(last_name)\n",
        "    return last_name\n",
        "\n",
        "# Helper method to show unique_last_names\n",
        "unique_last_names = {}\n",
        "def get_unique_last_names(name):\n",
        "    last_name = get_last_name(name)\n",
        "    if last_name in unique_last_names:\n",
        "        unique_last_names[last_name] += 1\n",
        "    else:\n",
        "        unique_last_names[last_name] = 1\n",
        "\n",
        "\n",
        "# Create a new field with last names\n",
        "full_data[\"LastName\"] = full_data[\"Name\"].apply(get_last_name)\n",
        "\n",
        "# Create a category by grouping like last names \n",
        "full_data[\"Name\"].apply(get_unique_last_names)\n",
        "full_data[\"LastNameCount\"] = full_data[\"Name\"].apply(lambda x: unique_last_names[get_last_name(x)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "586287c8-779b-bc10-4000-87dc17b868a8"
      },
      "source": [
        "### Impute missing ages using a couple of different techniques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "78c71259-b7c8-a987-fe91-54b128b2e7bb"
      },
      "outputs": [],
      "source": [
        "# To set the missing ages we will find the median age for the persons title and use that\n",
        "# as the age of the person\n",
        "def map_missing_ages1(df):\n",
        "    avg_title_age = {}\n",
        "    # Find median age for all non null passengers\n",
        "    avg_age_all= df['Age'].dropna().median()\n",
        "    # Iterate all the titles and set a median age for each title\n",
        "    for title in range(1,11):\n",
        "        avg_age = df['Age'][(df[\"Title\"] == title)].dropna().median()\n",
        "         # If the average age is null for a title defualt back to average for all passengers\n",
        "        if pd.isnull(avg_age):\n",
        "            avg_age = avg_age_all\n",
        "        avg_title_age[title] = avg_age\n",
        "\n",
        "    # Now that we have a list with average age by title we apply it to all our null passengers\n",
        "    # Map Ages without data\n",
        "    for title in range(1,11):\n",
        "        # print(\"title code:\",title,\" avg age:\",avg_title_age[title])\n",
        "        df[\"Age\"][(df[\"Title\"] == title) & df[\"Age\"].isnull()] = avg_title_age[title]\n",
        "\n",
        "\n",
        "# Set the  missing ages by createing a classifier based on the below criteria\n",
        "def map_missing_ages2(df):\n",
        "    feature_list = [\n",
        "                \"Fare\",\n",
        "                \"Pclass\",\n",
        "                \"Parch\",\n",
        "                \"SibSp\",\n",
        "                \"Title\",\n",
        "                \"Sex_female\",\n",
        "                \"Sex_male\",\n",
        "                \"Embarked_C\",\n",
        "                \"Embarked_Q\",\n",
        "                \"Embarked_S\"\n",
        "                ]\n",
        "\n",
        "    etr = ExtraTreesRegressor(n_estimators=200,random_state = 42)\n",
        "\n",
        "    train = df.loc[df.Age.notnull(),feature_list]\n",
        "    target = df.loc[df.Age.notnull(),['Age']]\n",
        "\n",
        "    test = df.loc[df.Age.isnull(),feature_list]\n",
        "    etr.fit(train,np.ravel(target))\n",
        "\n",
        "    age_preds = etr.predict(test)\n",
        "    df.loc[df.Age.isnull(),['Age']] = age_preds\n",
        "\n",
        "map_missing_ages2(full_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "79418a48-9b97-4d99-a87c-6a396c18a112"
      },
      "outputs": [],
      "source": [
        "#Exploring some of the fare data looking for patterns\n",
        "#create a Fare Category grouping fare prices\n",
        "\n",
        "full_data[\"FareCat\"]=0\n",
        "r=0\n",
        "for f in range(20,220,20):\n",
        "    full_data[\"FareCat\"][(full_data[\"Fare\"] >= r) & (full_data[\"Fare\"] < f)] = f\n",
        "    #print(\"f >= {} & f < {}\".format(r,f)) \n",
        "    r=f\n",
        "    \n",
        "full_data[\"FareCat\"][(full_data[\"Fare\"] >= 200)] = 200                        \n",
        "\n",
        "#################################################\n",
        "# Plot the Face Category\n",
        "fig, (axis1) = plt.subplots(nrows=1, ncols=1, figsize=(5, 3))\n",
        "\n",
        "survived = full_data['FareCat'][train['Survived']==1].value_counts().sort_index()\n",
        "died =     full_data['FareCat'][train['Survived']==0].value_counts().sort_index()\n",
        "\n",
        "\n",
        "width = 0.30\n",
        "x_pos = np.arange(len(survived))\n",
        "\n",
        "axis1.bar(x_pos, survived, width, color='b', label='Survived')\n",
        "axis1.bar(x_pos + width, died, width, color='r', label='Died')\n",
        "axis1.set_xlabel('Fare', fontsize=12)\n",
        "axis1.set_ylabel('Number of people', fontsize=10)\n",
        "axis1.legend(loc=\"upper right\", fontsize=\"xx-small\",\n",
        "           ncol=2, shadow=True, title=\"Legend\")\n",
        "axis1.yaxis.grid(True)\n",
        "\n",
        "plt.show()\n",
        "##########################################################\n",
        "    \n",
        "#print(full_data[\"Fare\"].round().value_counts().sort_index())\n",
        "#www = full_data[(full_data[\"Fare\"]==0.0)]\n",
        "#www.loc[:,['Survived','Name','Sex','Age','Fare']]\n",
        "#print(www)\n",
        "#print(\"Full data missing \\n{}\\n\".format(full_data.isnull().sum()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "5e54dab1-f745-765a-c687-2fae9ced5d28"
      },
      "source": [
        "## Visualization of key features\n",
        "#### Evaluate features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8a5c506c-8d7b-33c0-375c-071bc22dc4bc"
      },
      "outputs": [],
      "source": [
        "fig, ((axis1, axis2),(axis3, axis4)) = plt.subplots(nrows=2, ncols=2, figsize=(9, 7))\n",
        "\n",
        "###########################################################\n",
        "pclass_survived = train['Pclass'][train['Survived']==1].value_counts().sort_index()\n",
        "pclass_died =     train['Pclass'][train['Survived']==0].value_counts().sort_index()\n",
        "\n",
        "width = 0.30\n",
        "x_pos = np.arange(len(pclass_survived))\n",
        "\n",
        "axis1.bar(x_pos,pclass_survived, width, color='blue', label='Survived')\n",
        "axis1.bar(x_pos + width, pclass_died, width, color='red', label='Died')\n",
        "axis1.set_xlabel('Passenger Classes', fontsize=12)\n",
        "axis1.set_ylabel('Number of people', fontsize=10)\n",
        "axis1.legend(loc='upper center')\n",
        "axis1.set_xticklabels(('','First Class','','Second Class','','Third Class'))\n",
        "axis1.yaxis.grid(True)\n",
        "\n",
        "###########################################################\n",
        "embarked_survived = train['Embarked'][train['Survived']==1].value_counts().sort_index()\n",
        "embarked_died     = train['Embarked'][train['Survived']==0].value_counts().sort_index()\n",
        "\n",
        "#print(embarked_died)\n",
        "#print(embarked_survived)\n",
        "x_pos = np.arange(len(embarked_survived))\n",
        "axis2.bar(x_pos,embarked_survived, width, color='blue', label='Survived')\n",
        "axis2.bar(x_pos + width, embarked_died, width, color='red', label='Died')\n",
        "axis2.set_xlabel('Embarked From', fontsize=12)\n",
        "axis2.set_ylabel('Number of people', fontsize=10)\n",
        "axis2.legend(loc='upper center')\n",
        "axis2.set_xticklabels(('','Cherbourg','','Queenstown','','Southamton'))\n",
        "axis2.yaxis.grid(True)\n",
        "\n",
        "###########################################################\n",
        "# Age fill has an interesting spike based on the above fill of empty ages\n",
        "age_survived = train['Age'][train['Survived']==1].value_counts().sort_index()\n",
        "age_died     = train['Age'][train['Survived']==0].value_counts().sort_index()\n",
        "\n",
        "minAge, maxAge = min(train.Age), max(train.Age)\n",
        "bins = np.linspace(minAge, maxAge, 100)\n",
        "\n",
        "# You can squash the distribution with a log function but I prefered to see the outliers\n",
        "#axis3.bar(np.arange(len(age_survived)), np.log10(age_survived), color='blue', label='Survived')\n",
        "#axis3.bar(np.arange(len(age_died)), -np.log10(age_died), color='red', label='Died')\n",
        "\n",
        "axis3.bar(np.arange(len(age_survived)), age_survived, color='blue', label='Survived')\n",
        "axis3.bar(np.arange(len(age_died)), -(age_died), color='red', label='Died')\n",
        "#axis3.set_yticks(range(-3,4), (10**abs(k) for k in range(-3,4)))\n",
        "axis3.legend(loc='upper right',fontsize=\"x-small\")\n",
        "axis3.set_xlabel('Age', fontsize=12)\n",
        "axis3.set_ylabel('Number of people', fontsize=10)\n",
        "\n",
        "###########################################################\n",
        "# Chart Fare by Survived and Perished\n",
        "fair_survived = train['Fare'][train['Survived']==1].value_counts().sort_index()\n",
        "fair_died     = train['Fare'][train['Survived']==0].value_counts().sort_index()\n",
        "\n",
        "minAge, maxAge = min(train.Age), max(train.Age)\n",
        "bins = np.linspace(minAge, maxAge, 100)\n",
        "\n",
        "axis4.bar(np.arange(len(fair_survived)), fair_survived, color='blue', label='Survived')\n",
        "axis4.bar(np.arange(len(fair_died)), -(fair_died), color='red', label='Died')\n",
        "#axis4.set_yticks(range(-3,4), (10**abs(k) for k in range(-3,4)))\n",
        "axis4.legend(loc='upper right',fontsize=\"x-small\")\n",
        "axis4.set_xlabel('Fare', fontsize=12)\n",
        "axis4.set_ylabel('Number of people', fontsize=10)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "###########################################################\n",
        "fig, (axis1) = plt.subplots(nrows=1, ncols=1, figsize=(5, 3))\n",
        "\n",
        "survived = train['Sex'][train['Survived']==1].value_counts().sort_index()\n",
        "died =     train['Sex'][train['Survived']==0].value_counts().sort_index()\n",
        "\n",
        "width = 0.30\n",
        "x_pos = np.arange(len(survived))\n",
        "\n",
        "axis1.bar(x_pos, survived, width, color='b', label='Survived')\n",
        "axis1.bar(x_pos + width, died, width, color='r', label='Died')\n",
        "axis1.set_xlabel('Gender', fontsize=12)\n",
        "axis1.set_ylabel('Number of people', fontsize=10)\n",
        "axis1.set_xticklabels(('','Females','','','','','Males'))\n",
        "axis1.legend(loc=\"upper left\", fontsize=\"xx-small\",\n",
        "           ncol=2, shadow=True, title=\"Legend\")\n",
        "axis1.yaxis.grid(True)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "8bf6c2de-ae58-904b-2b52-75d46c6c640b"
      },
      "source": [
        "## Create some new features\n",
        "### New features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a4aaf54d-f4b3-47e9-8cad-c45336ef1b43"
      },
      "outputs": [],
      "source": [
        "# Lets do some feature engineering\n",
        "\n",
        "# Assign 1 to passengers under 14, 0 to those 14 or older.\n",
        "full_data[\"Child\"] = 0\n",
        "full_data[\"Child\"][full_data[\"Age\"] < 14] = 1\n",
        "\n",
        "# Create a Mother field (It seems Mother had a pretty high survival rate)\n",
        "# Note that Title \"Miss.\" = 2 in our mappings\n",
        "full_data[\"Mother\"] = 0\n",
        "full_data[\"Mother\"][(full_data[\"Parch\"] > 0) & (full_data[\"Age\"] > 18) &\n",
        "                    (full_data[\"Sex\"] == 'female') & (full_data[\"Title\"] != 2)] = 1\n",
        "\n",
        "\n",
        "full_data[\"FamilySize\"] = full_data[\"SibSp\"] + full_data[\"Parch\"]\n",
        "\n",
        "# Create a Family category none, small, large\n",
        "full_data[\"FamilyCat\"] = 0\n",
        "full_data[\"FamilyCat\"][ (full_data[\"Parch\"] + full_data[\"SibSp\"]) == 0] = 0\n",
        "full_data[\"FamilyCat\"][((full_data[\"Parch\"] + full_data[\"SibSp\"]) > 0) & ((full_data[\"Parch\"] + full_data[\"SibSp\"]) <= 3)] = 1\n",
        "full_data[\"FamilyCat\"][ (full_data[\"Parch\"] + full_data[\"SibSp\"]) > 3 ] = 2\n",
        "\n",
        "full_data[\"SingleMale\"] = 0 #0 -- Other ends up being females\n",
        "full_data[\"SingleMale\"][((full_data[\"Parch\"] + full_data[\"SibSp\"]) == 0) & (full_data[\"Sex\"] == 'male')] = 2\n",
        "full_data[\"SingleMale\"][((full_data[\"Parch\"] + full_data[\"SibSp\"]) >  0) & (full_data[\"Sex\"] == 'male')] = 1\n",
        "\n",
        "full_data[\"AdultFemale\"] = 0\n",
        "full_data[\"AdultFemale\"][(full_data[\"Age\"] > 18) & (full_data[\"Sex\"] == 'female')] = 1\n",
        "\n",
        "full_data[\"AdultMale\"] = 0\n",
        "full_data[\"AdultMale\"][(full_data[\"Age\"] > 18) & (full_data[\"Sex\"] == 'male')] = 1\n",
        "\n",
        "full_data[\"PclassXAge\"]= full_data[\"Pclass\"] * full_data[\"Age\"]\n",
        "full_data[\"FareDivPclass\"]= full_data[\"Fare\"] / full_data[\"Pclass\"]\n",
        "\n",
        "import math\n",
        "\n",
        "full_data[\"FareLog\"] = full_data[\"Fare\"].apply(lambda x: 0 if x == 0 else math.log2(x))\n",
        "full_data[\"AgeLog\"] = full_data[\"Age\"].apply(lambda x: 0 if x == 0 else math.log2(x))\n",
        "                                               \n",
        "\n",
        "train_data = full_data.iloc[:891,:]\n",
        "##########################################################\n",
        "full_data[\"Class*MWC\"]=0\n",
        "# Return the likelyhood to survive based in class and gender\n",
        "def survive_percentage_class(pclass,gender):\n",
        "        if gender == 'child':\n",
        "            x = train_data[\"Survived\"][(train_data[\"Pclass\"] == pclass) & (train_data[\"Child\"] == 1 )]\n",
        "        else:    \n",
        "            x = train_data[\"Survived\"][(train_data[\"Pclass\"] == pclass) & (train_data[\"Sex\"] == gender ) & (train_data[\"Child\"] != 1 )]\n",
        "        \n",
        "        y = x.value_counts(normalize=True).sort_index()\n",
        "        if len(y) >1:\n",
        "            y = y[1]\n",
        "        else:\n",
        "            y = 1.0\n",
        "        #print(int(round(y*100)) )\n",
        "        return int(round(y*100)) \n",
        "    \n",
        "# Iterate pclasses and gender and create new feature based on likelyhood to survive \n",
        "for gender in ['female','male','child']:\n",
        "    print(\"\")\n",
        "    for pclass in [1,2,3]:\n",
        "        if gender == 'child':\n",
        "            full_data[\"Class*MWC\"][(full_data[\"Pclass\"] == pclass) & \n",
        "                                   (full_data[\"Child\"] == 1 )] = survive_percentage_class(pclass,gender)\n",
        "        else:    \n",
        "            full_data[\"Class*MWC\"][(full_data[\"Pclass\"] == pclass) & \n",
        "                                   (full_data[\"Sex\"] == gender )] = survive_percentage_class(pclass,gender)\n",
        "\n",
        "##########################################################\n",
        "full_data[\"Fare*MWC\"]=0\n",
        "# Return the likelyhood to survive based in Fare and gender\n",
        "def survive_percentage_fare(fare_cat,gender):\n",
        "        if gender == 'child':\n",
        "            x = train_data[\"Survived\"][(train_data[\"FareCat\"] == fare_cat) & (train_data[\"Child\"] == 1 )]\n",
        "        else:    \n",
        "            x = train_data[\"Survived\"][(train_data[\"FareCat\"] == fare_cat) & (train_data[\"Sex\"] == gender ) & (train_data[\"Child\"] != 1 )]\n",
        "        \n",
        "        y = x.value_counts(normalize=True).sort_index()\n",
        "        if len(y) >1:\n",
        "            y = y[1]\n",
        "        else:\n",
        "            y = 1.0\n",
        "        #print(int(round(y*100)) )\n",
        "        return int(round(y*100)) \n",
        "    \n",
        "# Iterate Fare category and gender and create new feature based on likelyhood to survive \n",
        "for gender in ['female','male','child']:\n",
        "    print(\"\")\n",
        "    for fare_cat in range(20,220,20):\n",
        "        if gender == 'child':\n",
        "            full_data[\"Fare*MWC\"][(full_data[\"FareCat\"] == fare_cat) & \n",
        "                                   (full_data[\"Child\"] == 1 )] = survive_percentage_fare(fare_cat,gender)\n",
        "        else:    \n",
        "            full_data[\"Fare*MWC\"][(full_data[\"FareCat\"] == fare_cat) & \n",
        "                                   (full_data[\"Sex\"] == gender )] = survive_percentage_fare(fare_cat,gender)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "8980ab5d-798e-3316-dc8b-d21f08b57f4b"
      },
      "source": [
        "### Visualize our new features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "52d19d0a-3b53-cf77-cf9f-40c670dc6645"
      },
      "outputs": [],
      "source": [
        "# Create some plots of our New Features\n",
        "fig, ((axis1, axis2),(axis3, axis4)) = plt.subplots(nrows=2, ncols=2, figsize=(9, 7))\n",
        "train = full_data.iloc[:891,:]\n",
        "\n",
        "width = 0.30\n",
        "\n",
        "###########################################################\n",
        "single_male_survived = train['SingleMale'][train['Survived']==1].value_counts().sort_index()\n",
        "single_male_died =     train['SingleMale'][train['Survived']==0].value_counts().sort_index()\n",
        "\n",
        "x_pos = np.arange(len(single_male_survived))\n",
        "\n",
        "axis1.bar(x_pos, single_male_survived, width, color='b', label='Survived')\n",
        "axis1.bar(x_pos + width, single_male_died, width, color='r', label='Died')\n",
        "axis1.set_xlabel('Male Marital Status', fontsize=12)\n",
        "axis1.set_ylabel('Number of people', fontsize=10)\n",
        "axis1.set_xticklabels(('','Females','','Single Male','','Married Male'))\n",
        "axis1.legend(loc=\"upper left\", fontsize=\"xx-small\",\n",
        "           ncol=2, shadow=True, title=\"Legend\")\n",
        "\n",
        "axis1.annotate('Single Males survive \\nbetter than Married', xy=(1.2, 100),xytext=(1, 150),\n",
        "              arrowprops=dict(facecolor='black', shrink=0.05),)\n",
        "\n",
        "###########################################################\n",
        "mother_survived = train['Mother'][train['Survived']==1].value_counts().sort_index()\n",
        "mother_died =     train['Mother'][train['Survived']==0].value_counts().sort_index()\n",
        "    \n",
        "x_pos = np.arange(len(mother_survived))\n",
        "\n",
        "axis2.bar(x_pos, mother_survived, width, color='b', label='Survived')\n",
        "axis2.bar(x_pos + width, mother_died, width, color='r', label='Died')\n",
        "axis2.set_xlabel('Mother Status', fontsize=12)\n",
        "axis2.set_ylabel('Number of people', fontsize=10)\n",
        "axis2.set_xticklabels(('','All others','','','','','Mothers'))\n",
        "axis2.legend(loc=\"upper right\", fontsize=\"xx-small\",\n",
        "           ncol=2, shadow=True, title=\"Legend\")\n",
        "\n",
        "###########################################################\n",
        "family_survived = train['FamilyCat'][train['Survived']==1].value_counts().sort_index()\n",
        "family_died =     train['FamilyCat'][train['Survived']==0].value_counts().sort_index()\n",
        "    \n",
        "x_pos = np.arange(len(family_survived))\n",
        "\n",
        "axis3.bar(x_pos, family_survived, width, color='b', label='Survived')\n",
        "axis3.bar(x_pos + width, family_died, width, color='r', label='Died')\n",
        "axis3.set_xlabel('Family Status', fontsize=12)\n",
        "axis3.set_ylabel('Number of people', fontsize=10)\n",
        "axis3.set_xticklabels(('','No Kids','','1 to 3 kids','','> 3 Kids',''))\n",
        "axis3.legend(loc=\"upper right\", fontsize=\"xx-small\",\n",
        "           ncol=2, shadow=True, title=\"Legend\")\n",
        "\n",
        "###########################################################\n",
        "title_survived = train['Title'][train['Survived']==1].value_counts().sort_index()\n",
        "title_died =     train['Title'][train['Survived']==0].value_counts().sort_index()\n",
        "\n",
        "width = 0.40\n",
        "x_pos_s = np.arange(len(title_survived))\n",
        "x_pos_d = np.arange(len(title_died))\n",
        "\n",
        "axis4.bar(x_pos_s, title_survived, width, color='b', label='Survived')\n",
        "axis4.bar(x_pos_d + width, title_died, width, color='r', label='Died')\n",
        "axis4.set_xlabel('Title Status', fontsize=12)\n",
        "axis4.set_ylabel('Number of people', fontsize=10)\n",
        "axis4.set_xticklabels(('Mr','Miss','Mrs','Mst','Dr','Rev','Sir','Ml','Lady','Ms'))\n",
        "axis4.legend(loc=\"upper right\", fontsize=\"xx-small\",\n",
        "           ncol=2, shadow=True, title=\"Legend\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "04caae3e-9d98-02ec-56ca-4163890bcb23"
      },
      "source": [
        "## Use Random Forest & Predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9bed7c64-3259-fddb-e43a-88a738356e65"
      },
      "outputs": [],
      "source": [
        "# Setup calassifier and predict\n",
        "\n",
        "def titanic_predict(feature_list, n_estimators, max_features, max_depth):\n",
        "\n",
        "    train_all      = full_data.iloc[:891,:]\n",
        "    train_features = train_all.loc[:,feature_list]\n",
        "    train_target   = train_all.loc[:,['Survived']]\n",
        "    test_data      = full_data.iloc[891:,:]\n",
        "    test_features  = test_data.loc[:,feature_list]\n",
        "\n",
        "    PassengerId =np.array(test[\"PassengerId\"]).astype(int)\n",
        "    \n",
        "\n",
        "    #=============================\n",
        "    # Building and fitting the Random forest\n",
        "    #forest_classifier = RandomForestClassifier(max_depth = 6,\n",
        "    #                                           min_samples_split=2,\n",
        "    #                                           n_estimators = 450,\n",
        "    #                                           random_state = 1)\n",
        "\n",
        "    forest = RandomForestClassifier(n_estimators=n_estimators,\n",
        "                                               max_features=max_features,\n",
        "                                               max_depth=max_depth,\n",
        "                                               #criterion='entropy',\n",
        "                                               random_state=42)\n",
        "    forest.fit(train_features, np.ravel(train_target))\n",
        "    forest_pred = forest.predict(test_features)\n",
        "    print(\"RandomForestClassifier score:\",forest.score(train_features, train_target)) \n",
        "    #print(my_forest.feature_importances_)\n",
        "    forest_solution = pd.DataFrame(forest_pred, PassengerId, columns = [\"Survived\"]).astype(int)\n",
        "    forest_solution.to_csv(\"predict_random_forest.csv\", index_label = [\"PassengerId\"])\n",
        "    \n",
        "    # Check that the data frame has 418 entries\n",
        "    print(forest_solution.shape)\n",
        "    return forest_solution, forest\n",
        "\n",
        "\n",
        "feature_list = [\n",
        "                \"Age\",\n",
        "                \"Fare\",\n",
        "                \"Pclass\",\n",
        "                \"Parch\",\n",
        "                \"SibSp\",\n",
        "                \"Sex_female\",\n",
        "                \"Sex_male\",\n",
        "                \"Embarked_C\",\n",
        "                \"Embarked_Q\",\n",
        "                \"Embarked_S\",\n",
        "                \"Cabin_A\",\n",
        "                \"Cabin_B\",\n",
        "                \"Cabin_C\",\n",
        "                \"PclassXAge\",\n",
        "                \"FareDivPclass\",\n",
        "                \"Cabin_D\",\n",
        "                \"Cabin_E\",\n",
        "                \"Cabin_F\",\n",
        "                \"Cabin_G\",\n",
        "                \"Cabin_None\",\n",
        "                \"Cabin_T\",\n",
        "                \"Class*MWC\",\n",
        "                \"FareLog\",\n",
        "                \"FareCat\",\n",
        "                \"Fare*MWC\",\n",
        "                \"AgeLog\",\n",
        "                \"Title\",\n",
        "                \"Mother\",\n",
        "                \"FamilySize\",\n",
        "                \"FamilyCat\",\n",
        "                \"SingleMale\",\n",
        "                \"AdultFemale\",\n",
        "                \"AdultMale\",\n",
        "                \"LastNameCount\",\n",
        "                \"Child\"\n",
        "                ]\n",
        "\n",
        "my_solution, my_forest = titanic_predict(feature_list,500,None,5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "fb9e373f-e98d-241d-f578-2c470009aa75"
      },
      "outputs": [],
      "source": [
        "# Try other classifiers\n",
        "# None of the below produced results for me better than the Random Forest\n",
        "# But I did not try to do much tuning\n",
        "def other_classifiers(feature_list):\n",
        "    train_all      = full_data.iloc[:891,:]\n",
        "    train_features = train_all.loc[:,feature_list]\n",
        "    train_target   = train_all.loc[:,['Survived']]\n",
        "    test_data      = full_data.iloc[891:,:]\n",
        "    test_features  = test_data.loc[:,feature_list]\n",
        "\n",
        "    PassengerId =np.array(test[\"PassengerId\"]).astype(int)\n",
        "\n",
        "    #=============================\n",
        "    # Logistic Regression\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    logreg = LogisticRegression()\n",
        "    logreg.fit(train_features, np.ravel(train_target))\n",
        "    logreg_pred = logreg.predict(test_features)\n",
        "    print(\"LogisticRegression score:\",logreg.score(train_features, train_target))\n",
        "    logreg_solution = pd.DataFrame(logreg_pred, PassengerId, columns = [\"Survived\"]).astype(int)\n",
        "    logreg_solution.to_csv(\"predict_logistic_regression.csv\", index_label = [\"PassengerId\"])\n",
        "    #===============================\n",
        "\n",
        "    #===============================\n",
        "    # Gaussian Naive Bayes\n",
        "    from sklearn.naive_bayes import GaussianNB\n",
        "    gaussian = GaussianNB()\n",
        "    gaussian.fit(train_features, np.ravel(train_target))\n",
        "    gaussian_pred = gaussian.predict(test_features)\n",
        "    print(\"GaussianNB score:\",gaussian.score(train_features, train_target))\n",
        "    gaussian_solution = pd.DataFrame(gaussian_pred, PassengerId, columns = [\"Survived\"]).astype(int)\n",
        "    gaussian_solution.to_csv(\"predict_gaussian_nb.csv\", index_label = [\"PassengerId\"])\n",
        "    #===============================\n",
        "    \n",
        "    #===============================\n",
        "    # K Neighbors\n",
        "    from sklearn.neighbors import KNeighborsClassifier\n",
        "    knn = KNeighborsClassifier(n_neighbors = 3)\n",
        "    knn.fit(train_features, np.ravel(train_target))\n",
        "    knn_pred = knn.predict(test_features)\n",
        "    print(\"KNeighborsClassifier score:\",knn.score(train_features, train_target))\n",
        "    knn_solution = pd.DataFrame(knn_pred, PassengerId, columns = [\"Survived\"]).astype(int)\n",
        "    knn_solution.to_csv(\"predict_k_neighbors.csv\", index_label = [\"PassengerId\"])\n",
        "    #===============================\n",
        "    \n",
        "    #===============================\n",
        "    # Support Vector Machines\n",
        "    from sklearn.svm import SVC\n",
        "    svc = SVC()\n",
        "    best_params = {'gamma': 0.015625, 'C': 8192.0, 'kernel': 'rbf'}\n",
        "    print(best_params)\n",
        "    svc.set_params(**best_params)\n",
        "    svc.verbose=True \n",
        "    svc.fit(train_features, np.ravel(train_target))\n",
        "    svc_pred = svc.predict(test_features)\n",
        "    print(\"SupportVectorMachine score:\",svc.score(train_features, train_target))\n",
        "    svc_solution = pd.DataFrame(svc_pred, PassengerId, columns = [\"Survived\"]).astype(int)\n",
        "    svc_solution.to_csv(\"predict_support_vector_machine.csv\", index_label = [\"PassengerId\"])\n",
        "    #===============================\n",
        "    \n",
        "    #import xgboost as xgb\n",
        "    # Requires additional installation\n",
        "    # not jus pip install\n",
        "\n",
        "\n",
        "#Un comment to try other classifiers\n",
        "#other_classifiers(feature_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bc48b74c-c7f5-d303-48fb-6b44f62839f3"
      },
      "outputs": [],
      "source": [
        "#Graph the importance of the features that have been created.\n",
        "#Note: Use the categorical_variables variable to aggragate split categorier\n",
        "\n",
        "def graph_feature_importance(model, feature_names, autoscale=True, headroom=0.05, width=10, summarized_columns=None):\n",
        "\n",
        "    if autoscale:\n",
        "        x_scale = model.feature_importances_.max() + headroom\n",
        "    else:\n",
        "        x_scale = 1\n",
        "\n",
        "    feature_dict = dict(zip(feature_names, model.feature_importances_))\n",
        "    \n",
        "    if summarized_columns:\n",
        "        for col_name in summarized_columns:\n",
        "            sum_value=0.0\n",
        "            for i, x in feature_dict.items():\n",
        "                if col_name in i:\n",
        "                    sum_value += x\n",
        "            keys_to_remove = [i for i in feature_dict.keys() if col_name in i]\n",
        "            for i in keys_to_remove:\n",
        "                feature_dict.pop(i)\n",
        "\n",
        "            feature_dict[col_name] = sum_value\n",
        "            \n",
        "    import numpy as np\n",
        "    \n",
        "    #This line below was difficult to figure out!!\n",
        "    fme = np.array(list(feature_dict.values())).flatten()\n",
        "    results = pd.Series(fme, index=feature_dict.keys())\n",
        "    results.sort()\n",
        "    results.plot(kind='barh', figsize=(width,len(results)/4), xlim=(0,x_scale))\n",
        "    plt.show()\n",
        "\n",
        "categorical_variables = [\"Sex\", \"Cabin\",\"Embarked\"]\n",
        "graph_feature_importance(my_forest,feature_list,summarized_columns=categorical_variables)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "41dace11-5b52-4465-ce80-9d1d5271af9b"
      },
      "outputs": [],
      "source": [
        "# Loop through different options of the Classifier\n",
        "# Select the best also a good place to put cross validation test\n",
        "#import src.score_titanic_run as score\n",
        "\n",
        "#n_estimator_options = [30,50,100,200,500,1000,2000]\n",
        "n_estimator_options = [400,500,1000]\n",
        "max_features_options = [\"auto\", None, \"sqrt\", \"log2\", 0.9, 0.2, 0.1]\n",
        "max_features_options = [None]\n",
        "#max_depth_options = [2,3,4,5,6,7,8,9,10]\n",
        "max_depth_options = [5,6]\n",
        "#max_depth_options = [None]\n",
        "for n_estimators in n_estimator_options:\n",
        "    for max_features in max_features_options:\n",
        "        for max_depth in max_depth_options:\n",
        "            titanic_predict(feature_list, n_estimators, max_features, max_depth)\n",
        "            print(\"n_estimators=\",n_estimators,\"max_features=\",max_features,\"max_depth=\",max_depth)\n",
        "            #Add cross validation code\n",
        "            #score.run1()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "cc6b6016-3d95-7847-73f7-a0c6a2ca86bc"
      },
      "outputs": [],
      "source": [
        "# The below function helps to optimize your classifier\n",
        "# allowing you to run a series of tests and see what parameters fit best. \n",
        "# Although I was excited when I found this the results I found where \n",
        "# not as good as the Manual process that I am applying above\n",
        "#\n",
        "def grid_search():    \n",
        "    train_all      = full_data.iloc[:891,:]\n",
        "    train_features = train_all.loc[:,feature_list]\n",
        "    train_target   = train_all.loc[:,['Survived']]\n",
        "    test_data      = full_data.iloc[891:,:]\n",
        "    test_features  = test_data.loc[:,feature_list]\n",
        "\n",
        "    PassengerId =np.array(test[\"PassengerId\"]).astype(int)\n",
        "    \n",
        "    from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "    n_folds = 10\n",
        "    cv = StratifiedKFold(n_folds)\n",
        "    N_es = [50, 100, 200, 400, 500]\n",
        "    criteria = ['gini', 'entropy']\n",
        "    #\n",
        "    random_forest = RandomForestClassifier()\n",
        "    gscv = GridSearchCV(estimator=random_forest, \n",
        "                        param_grid=dict(n_estimators=N_es, criterion=criteria), \n",
        "                        n_jobs=1, \n",
        "                        cv=list(cv.split(train_features, np.ravel(train_target))), \n",
        "                        verbose=2)\n",
        "    gscv.fit(train_features, np.ravel(train_target))\n",
        "    gscv_pred = gscv.predict(test_features)\n",
        "    print(\"GridSearchCV score:\",gscv.score(train_features, train_target)) \n",
        "    forest_solution = pd.DataFrame(gscv_pred, PassengerId, columns = [\"Survived\"]).astype(int)\n",
        "    forest_solution.to_csv(\"predict_grid_search.csv\", index_label = [\"PassengerId\"])\n",
        "    \n",
        "    \n",
        "#grid_search()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "27c55be4-bfec-2acd-4d14-00194cd208c0",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}