{"metadata": {"kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "nbconvert_exporter": "python", "name": "python", "mimetype": "text/x-python", "pygments_lexer": "ipython3", "file_extension": ".py", "version": "3.6.1"}}, "nbformat_minor": 1, "nbformat": 4, "cells": [{"metadata": {"_uuid": "7557c222e9759f54992782b87c05744f2b5221f5", "_cell_guid": "c6559c88-fc53-46f6-ac6e-abeefbde4d04"}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "import re\n", "import sklearn as sk\n", "import xgboost as xgb\n", "import seaborn as sns\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline\n", "\n", "import plotly.offline as py\n", "py.init_notebook_mode(connected=True)\n", "import plotly.graph_objs as go\n", "import plotly.tools as tls\n", "\n", "import warnings\n", "warnings.filterwarnings('ignore')\n", "\n", "# Going to use these 5 base models for the stacking\n", "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n", "from sklearn.svm import SVC\n", "from sklearn.neighbors import KNeighborsClassifier\n", "from sklearn.cross_validation import KFold;\n", "from sklearn.cross_validation import train_test_split"]}, {"metadata": {"collapsed": true, "_uuid": "ef5e4868c6956b6d182e1073cfdcc796a59ed97f", "_cell_guid": "f810dbe7-1e9f-404c-bfcf-6a86ad8c91a2"}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["train = pd.read_csv(\"../input/train.csv\")\n", "test = pd.read_csv('../input/test.csv')\n", "\n", "PassengerId = test['PassengerId']"]}, {"metadata": {"_uuid": "7a08b3303f34c4a7027a417eb11f775e49649d04", "_cell_guid": "adba64de-dd92-43d9-baae-648cd474933e"}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["train_org = train\n", "test_org = test\n", "train.head(5)"]}, {"metadata": {"collapsed": true, "_uuid": "67217d6b68d6545f67182c3f5b46be4b17982903", "_cell_guid": "a4513722-af09-41b8-bfb4-930f367417f6"}, "source": ["## \u6e05\u6d17\u6570\u636e\n", "[Titanic Best Working Classfier](https://www.kaggle.com/sinakhorami/titanic/titanic-best-working-classifier) by Sina"], "cell_type": "markdown"}, {"metadata": {"collapsed": true, "_uuid": "c7470dddc1d02a84f74b846def0b58d34142c183", "_cell_guid": "0660184b-1268-4864-93a6-d0ff6a2a38f6"}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["full_data = [train, test]\n", "\n", "# Some features of my own that I have added in\n", "# Gives the length of the name\n", "train['Name_length'] = train['Name'].apply(len)\n", "test['Name_length'] = test['Name'].apply(len)\n", "# Feature that tells whether a passenger had a cabin on the Titanic\n", "train['Has_Cabin'] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n", "test['Has_Cabin'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n", "\n", "# Feature engineering steps taken from Sina\n", "# Create new feature FamilySize as a combination of SibSp and Parch\n", "for dataset in full_data:\n", "    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n", "# Create new feature IsAlone from FamilySize\n", "for dataset in full_data:\n", "    dataset['IsAlone'] = 0\n", "    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n", "# Remove all NULLS in the Embarked column\n", "for dataset in full_data:\n", "    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n", "# Remove all NULLS in the Fare column and create a new feature CategoricalFare\n", "for dataset in full_data:\n", "    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\n", "train['CategoricalFare'] = pd.qcut(train['Fare'], 4)\n", "# Create a New feature CategoricalAge\n", "for dataset in full_data:\n", "    age_avg = dataset['Age'].mean()\n", "    age_std = dataset['Age'].std()\n", "    age_null_count = dataset['Age'].isnull().sum()\n", "    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n", "    dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\n", "    dataset['Age'] = dataset['Age'].astype(int)\n", "train['CategoricalAge'] = pd.cut(train['Age'], 5)\n", "# Define function to extract titles from passenger names\n", "def get_title(name):\n", "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n", "    # If the title exists, extract and return it.\n", "    if title_search:\n", "        return title_search.group(1)\n", "    return \"\"\n", "# Create a new feature Title, containing the titles of passenger names\n", "for dataset in full_data:\n", "    dataset['Title'] = dataset['Name'].apply(get_title)\n", "# Group all non-common titles into one single grouping \"Rare\"\n", "for dataset in full_data:\n", "    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n", "\n", "    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n", "    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n", "    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n", "\n", "for dataset in full_data:\n", "    # Mapping Sex\n", "    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n", "    \n", "    # Mapping titles\n", "    title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n", "    dataset['Title'] = dataset['Title'].map(title_mapping)\n", "    dataset['Title'] = dataset['Title'].fillna(0)\n", "    \n", "    # Mapping Embarked\n", "    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n", "    \n", "    # Mapping Fare\n", "    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] \t\t\t\t\t\t        = 0\n", "    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n", "    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n", "    dataset.loc[ dataset['Fare'] > 31, 'Fare'] \t\t\t\t\t\t\t        = 3\n", "    dataset['Fare'] = dataset['Fare'].astype(int)\n", "    \n", "    # Mapping Age\n", "    dataset.loc[ dataset['Age'] <= 16, 'Age'] \t\t\t\t\t       = 0\n", "    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n", "    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n", "    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n", "    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4 ;"]}, {"metadata": {"collapsed": true, "_uuid": "b81045926f94ddb6d04245ca5779d0ede5eb79a0", "_cell_guid": "557b8c22-2ff8-4345-a7eb-65293d5dca77"}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["# Feature selection\n", "drop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']\n", "train = train.drop(drop_elements, axis = 1)\n", "train = train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\n", "test  = test.drop(drop_elements, axis = 1)"]}, {"metadata": {"_uuid": "e171994c46619e44b9baaa518e39d93a630c226c", "_cell_guid": "87ca8401-d62e-4f5c-921c-6835af51185f"}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["train.head(3)"]}, {"metadata": {"_uuid": "20400f1ee904c300db33e44b254897d10c1f35ab", "_cell_guid": "0c45f719-c6d1-48c3-975b-faf07b9622ad"}, "source": ["## \u65b9\u4fbf\u8fdb\u884c\u7f16\u7a0b\u7684Python\u7c7b"], "cell_type": "markdown"}, {"metadata": {"collapsed": true, "_uuid": "8ea8f8956aa9b9c4623bacfb3922d0f22e1e50a0", "_cell_guid": "efe7f225-dc83-49e2-8d0c-a853402d2787"}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["# Some useful parameters which will come in handy later on\n", "ntrain = train.shape[0]\n", "ntest = test.shape[0]\n", "SEED = 0 # for reproducibility\n", "NFOLDS = 5 # set folds for out-of-fold prediction\n", "kf = KFold(ntrain, n_folds= NFOLDS, random_state=SEED)\n", "\n", "# Class to extend the Sklearn classifier\n", "class SklearnHelper(object):\n", "    def __init__(self, clf, model_name, seed=0, params=None):\n", "        if seed != None:\n", "            params['random_state'] = seed\n", "        self.clf = clf(**params)\n", "        self.name = model_name\n", "\n", "    def train(self, x_train, y_train):\n", "        self.clf.fit(x_train, y_train)\n", "\n", "    def predict(self, x):\n", "        return self.clf.predict(x)\n", "    \n", "    def predict_proba(self, x):\n", "        return self.clf.predict_proba(x)\n", "    \n", "    def score(self, x, y):\n", "        return self.clf.score(x, y)\n", "    \n", "    def fit(self,x,y):\n", "        try:\n", "            return self.clf.fit(x,y)\n", "        except AttributeError:\n", "            return self.clf.train(x,y)\n", "    \n", "    def feature_importances(self,x,y):\n", "        print(self.clf.fit(x,y).feature_importances_)\n", "        \n", "    def model_name(self):\n", "        return self.name\n", "    \n", "# Class to extend XGboost classifer"]}, {"metadata": {"_uuid": "4855d22c3ccc743db138d9854947e3548b2a37e2", "_cell_guid": "e0f987b5-d42a-40c6-9cf9-ffc2563a8140"}, "source": ["## \u8bad\u7ec3\u65b9\u6cd5\n", "\u4f7f\u7528kflod\u65b9\u6cd5\u6765\u4ea7\u751f\u6b21\u7ea7\u8bad\u7ec3\u96c6\uff0c\u82e5\u76f4\u63a5\u4f7f\u7528\u521d\u7ea7\u5b66\u4e60\u5668\u7684\u8bad\u7ec3\u96c6\u4ea7\u751f\u6b21\u7ea7\u8bad\u7ec3\u96c6\u7684\u8bad\u7ec3\u96c6\u5c31\u4f1a\u6709\u8fc7\u62df\u5408\u7684\u98ce\u9669\u3002"], "cell_type": "markdown"}, {"metadata": {"collapsed": true, "_uuid": "7276df660e76ff258e6e223ee249a0063dea1b4a", "_cell_guid": "2f4b8732-d009-4d5b-b491-71a67374ac99"}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["def get_oof(clf, x_train, y_train, x_test):\n", "    oof_train = np.zeros((ntrain,))\n", "    oof_test = np.zeros((ntest,))\n", "    oof_test_skf = np.empty((NFOLDS, ntest))\n", "    train_accuracy = 0\n", "    test_accuracy = 0\n", "\n", "    for i, (train_index, test_index) in enumerate(kf):\n", "        x_tr = x_train[train_index]\n", "        y_tr = y_train[train_index]\n", "        x_te = x_train[test_index]\n", "        y_te = y_train[test_index]\n", "\n", "        clf.fit(x_tr, y_tr)\n", "\n", "        oof_train[test_index] = clf.predict_proba(x_te)[:, 0]\n", "        oof_test_skf[i, :] = clf.predict_proba(x_test)[:, 0]\n", "        train_accuracy += clf.score(x_tr, y_tr)\n", "        test_accuracy += clf.score(x_te, y_te)\n", "    \n", "    train_accuracy = train_accuracy/len(kf)\n", "    test_accuracy = test_accuracy/len(kf)\n", "    print('\u6a21\u578b%s\u8bad\u7ec3\u51c6\u786e\u7387\u4e3a%f'%(clf.model_name(), train_accuracy))\n", "    print('\u6a21\u578b%s\u6d4b\u8bd5\u51c6\u786e\u7387\u4e3a%f'%(clf.model_name(), test_accuracy))\n", "    oof_test[:] = oof_test_skf.mean(axis=0)\n", "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)"]}, {"metadata": {"_uuid": "369a254d9240ebb52a12574361111c9e2ea797ba", "_cell_guid": "3fa19e96-2123-45d9-8a00-d19a2b84ad54"}, "source": ["## \u521d\u7ea7\u5b66\u4e60\u5668\u7684\u53c2\u6570\u3002"], "cell_type": "markdown"}, {"metadata": {"collapsed": true, "_uuid": "4a43879c9590f63b7a4bf8de5be8fb6f757f4366", "_cell_guid": "5c2fb47d-18e8-4b3d-9f9b-f537a8cdc4cc"}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["# Put in our parameters for said classifiers\n", "# Random Forest parameters\n", "rf_params = {\n", "    'n_jobs': -1,\n", "    'n_estimators': 500,\n", "     'warm_start': True, \n", "     #'max_features': 0.2,\n", "    'max_depth': 6,\n", "    'min_samples_leaf': 2,\n", "    'max_features' : 'sqrt',\n", "    'verbose': 0\n", "}\n", "\n", "# Extra Trees Parameters\n", "et_params = {\n", "    'n_jobs': -1,\n", "    'n_estimators':500,\n", "    #'max_features': 0.5,\n", "    'max_depth': 8,\n", "    'min_samples_leaf': 2,\n", "    'verbose': 0\n", "}\n", "\n", "# AdaBoost parameters\n", "ada_params = {\n", "    'n_estimators': 500,\n", "    'learning_rate' : 0.75\n", "}\n", "\n", "# Gradient Boosting parameters\n", "gb_params = {\n", "    'n_estimators': 500,\n", "     #'max_features': 0.2,\n", "    'max_depth': 3,\n", "    'subsample':0.5,\n", "    'min_samples_leaf': 2,\n", "    'verbose': 0\n", "}\n", "\n", "# Support Vector Classifier parameters \n", "svc_params = {\n", "    'kernel' : 'poly',\n", "    'C' : 0.025 ,\n", "    'probability' : True\n", "    }\n", "gbm_params = {\n", "    'learning_rate' : 0.4,\n", "    'n_estimators' : 500,\n", "    'max_depth' : 4,\n", "    'min_child_weight': 2,\n", "    #gamma=1,\n", "    'gamma':0.9,                        \n", "    'subsample':0.5,\n", "    'colsample_bytree':0.8,\n", "    'objective': 'binary:logistic',\n", "    'reg_lambda':5,\n", "    'nthread':-1,\n", "    'scale_pos_weight' :1\n", "}\n", "lglr_params = {\n", "    'C' : 0.5,\n", "    'max_iter' : 1000\n", "}\n", "knn_params = {\n", "    'n_neighbors' : 10,\n", "    'weights' : 'uniform'\n", "}"]}, {"metadata": {"_uuid": "117201eae9f70da6a935f8b414e8f5714c56b37f", "_cell_guid": "00fe1def-acf2-494c-a981-dff79ea26148"}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["rf = SklearnHelper(RandomForestClassifier, 'RandomForest', seed=SEED, params=rf_params) # \n", "et = SklearnHelper(ExtraTreesClassifier, 'ExtraTrees',seed=SEED, params=et_params)\n", "ada = SklearnHelper(AdaBoostClassifier, 'adaboost', seed=SEED, params=ada_params)\n", "gb = SklearnHelper(GradientBoostingClassifier, 'GradientBoosting', seed=SEED, params=gb_params)\n", "svc = SklearnHelper(SVC, 'SVM',seed=SEED, params=svc_params)\n", "gbm = SklearnHelper(xgb.XGBClassifier, 'XGB', seed=SEED, params=gbm_params)\n", "lglr = SklearnHelper(sk.linear_model.LogisticRegression, 'logistic', seed=SEED, params=lglr_params)\n", "knn = SklearnHelper(KNeighborsClassifier, 'KNN', seed=None, params=knn_params)"]}, {"metadata": {"_uuid": "c45ba7f7ecdddb84dbef0194dbfa23e5b569d59e", "_cell_guid": "bb4e0448-ad0c-4ee8-97c7-802f2be0066c"}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["try:\n", "    y_train = train['Survived'].ravel()\n", "    train = train.drop(['Survived'], axis=1)\n", "except KeyError:\n", "    print('no need')\n", "x_train = train.values # Creates an array of the train data\n", "x_test = test.values # Creats an array of the test data"]}, {"metadata": {"_uuid": "739f17a729981dbf93fbdaa88e0a77c6157007c8", "scrolled": false, "_cell_guid": "377d752c-52f5-4f2e-b5ca-ca5ea6cef964"}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["# Create our OOF train and test predictions. These base results will be used as new features\n", "et_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test) # Extra Trees\n", "rf_oof_train, rf_oof_test = get_oof(rf,x_train, y_train, x_test) # Random Forest\n", "ada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) # AdaBoost \n", "gb_oof_train, gb_oof_test = get_oof(gb,x_train, y_train, x_test) # Gradient Boost\n", "svc_oof_train, svc_oof_test = get_oof(svc,x_train, y_train, x_test) # Support Vector Classifier\n", "gbm_oof_train, gbm_oof_test = get_oof(gbm, x_train, y_train, x_test)\n", "lglr_oof_train, lglr_oof_test = get_oof(lglr, x_train, y_train, x_test)\n", "knn_oof_train, knn_oof_test= get_oof(knn, x_train, y_train, x_test)\n", "\n", "print(\"Training is complete\")"]}, {"metadata": {"_uuid": "0b518aede3787d590ad2a0eb56291a13fe832025", "_cell_guid": "7122ff40-1c1c-4c94-8813-f2b4e26f3423"}, "source": ["## \u8bad\u7ec3\u6b21\u7ea7\u5b66\u4e60\u5668"], "cell_type": "markdown"}, {"metadata": {"_uuid": "6278366aecad33307a06f614d1388e5c12f038f1", "_cell_guid": "bdb26d13-6378-4ed7-b91d-4987e1c9b908"}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["base_predictions_train = pd.DataFrame( {'RandomForest': rf_oof_train.ravel(),\n", "     'ExtraTrees': et_oof_train.ravel(),\n", "     'AdaBoost': ada_oof_train.ravel(),\n", "      'GradientBoost': gb_oof_train.ravel(),\n", "       'SVC' : svc_oof_train.ravel(),\n", "     'gbm' :gbm_oof_train.ravel(),\n", "    'lglr' : lglr_oof_train.ravel(),\n", "    'knn' : knn_oof_train.ravel()\n", "    })\n", "base_predictions_train.head()"]}, {"metadata": {"_uuid": "7dac8005e5f1ef743cbbe5a118e9bd18ff75a09d", "_cell_guid": "3ab93705-de4b-4750-a346-eb5031a2f10c"}, "source": ["### \u67e5\u770b\u5404\u4e2a\u7279\u5f81\u7684\u76f8\u5173\u6027"], "cell_type": "markdown"}, {"metadata": {"_uuid": "556a8ed12ecabcd8a08a41a2191ee0fffa1ed357", "_cell_guid": "b26b26ff-6eec-44b6-ba12-e010bcb15957"}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["sns.heatmap(base_predictions_train.astype(float).corr(),linewidths=0.1,vmax=1.0, square=True,  linecolor='white', annot=True)"]}, {"metadata": {"collapsed": true, "_uuid": "e6eea12c52413f6039846a5080999ff3bf85f606", "_cell_guid": "a83c99f1-f5d4-4cf5-9098-ccb104656ef9"}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["sec_x_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, svc_oof_train, lglr_oof_train,knn_oof_train), axis=1)\n", "sec_x_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, svc_oof_test,lglr_oof_test,knn_oof_test), axis=1)"]}, {"metadata": {"_uuid": "6ed092271831d8d7372ade0bc38be92654f341ab", "_cell_guid": "ead8ee2f-86ed-4f48-87f2-d925303e903b"}, "source": ["### \u6b21\u7ea7\u5b66\u4e60\u5668\u53c2\u6570"], "cell_type": "markdown"}, {"metadata": {"collapsed": true, "_uuid": "6064feaa72f146ce13520c647a71d7e59774d076", "_cell_guid": "2a32f547-1171-484c-b578-eab6b6bdbc04"}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["gbm2_params = {\n", "    'learning_rate' : 0.2,\n", "    'n_estimators' : 500,\n", "    'max_depth' : 4,\n", "    'min_child_weight': 3,\n", "    #gamma=1,\n", "    'gamma':0.9,                        \n", "    'subsample':0.4,\n", "    'colsample_bytree':0.8,\n", "    'objective': 'binary:logistic',\n", "    'reg_lambda':2,\n", "    'nthread':-1,\n", "    'scale_pos_weight' :1\n", "}\n", "lglr2_params = {\n", "     'C' : 0.5,\n", "    'max_iter' : 1000\n", "}\n", "svc2_params = {\n", "    'kernel' : 'linear',\n", "    'C' : 0.025 ,\n", "    'probability' : True\n", "}\n", "ada2_params = {\n", "    'n_estimators': 30,\n", "    'learning_rate' : 0.9\n", "}"]}, {"metadata": {"collapsed": true, "_uuid": "612bacf0854026d389efeac45d1ad85d84e283fb", "_cell_guid": "72895807-b71d-4484-a664-882fce5e5d2b"}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["gbm2 = SklearnHelper(xgb.XGBClassifier, 'XGB', seed=SEED, params=gbm2_params)\n", "lglr2 = SklearnHelper(sk.linear_model.LogisticRegression, 'lglr', seed=SEED, params=lglr2_params)\n", "svc2 = SklearnHelper(SVC, 'svc', seed=SEED, params=svc2_params)\n", "adaboost2 = SklearnHelper(AdaBoostClassifier, 'adaboost', seed=SEED, params=ada2_params)"]}, {"metadata": {"collapsed": true, "_uuid": "3cfea83913cf7c6632afde206dcde63339f5400c", "_cell_guid": "4f33a158-e458-40f7-a5eb-ca963f88b2f7"}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["x_train2, x_dev2, y_train2,y_dev2 = train_test_split(sec_x_train, y_train, test_size = 0.2)"]}, {"metadata": {"_uuid": "0d644dfc54038801545e662757d371eef8ad744f", "_cell_guid": "3c13ec1b-654d-4714-8630-1a1723c9f1ad"}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["_,_ = get_oof(gbm2, sec_x_train, y_train, sec_x_test)\n", "_,_ = get_oof(lglr2, sec_x_train, y_train, sec_x_test)\n", "_,_ = get_oof(svc2, sec_x_train, y_train, sec_x_test)\n", "_,_ = get_oof(adaboost2, sec_x_train, y_train, sec_x_test)"]}, {"metadata": {"_uuid": "c6ba0afb34a042cd21829fb38857c5cbe98dd72d", "_cell_guid": "1dd68fa3-4c99-4ebf-ac8d-650922bbcced"}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["#adaboost2.fit(x_train2, y_train2)\n", "print (\"\u5728\u8bad\u7ec3\u96c6\u4e0a\u7684\u51c6\u786e\u7387\u4e3a%f\"%(adaboost2.score(x_train2, y_train2)))\n", "print (\"\u5728\u6d4b\u8bd5\u96c6\u4e0a\u7684\u51c6\u786e\u7387\u4e3a%f\"%(adaboost2.score(x_dev2, y_dev2)))"]}, {"metadata": {"_uuid": "e73d9c38b2c8c9186203afdc52179321142d1a76", "_cell_guid": "696b6a16-2185-4604-83bd-efbe0699cf6d"}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["num_tree = [1,2, 5, 10, 15, 20, 30 , 40, 50, 100, 150 ,200, 300,500]\n", "train_accuracy = []\n", "test_accuracy = []\n", "ada2_params['learning_rate'] = 1\n", "for num in num_tree:\n", "    ada2_params['n_estimators'] = num\n", "    adaboost2 = SklearnHelper(AdaBoostClassifier, 'adaboost', seed=SEED, params=ada2_params)\n", "    adaboost2.fit(x_train2, y_train2)\n", "    train_accuracy.append(adaboost2.score(x_train2, y_train2))\n", "    test_accuracy.append(adaboost2.score(x_dev2, y_dev2))\n", "plt.plot(num_tree, train_accuracy, 'r')\n", "plt.plot(num_tree, test_accuracy, 'b')\n", "plt.show()"]}, {"metadata": {"collapsed": true, "_uuid": "ed38b6cb7ca25e144c7487e037703163b9cba771", "_cell_guid": "46455cf5-e1c7-44e7-8d32-fc3d0b432cc6"}, "outputs": [], "execution_count": null, "cell_type": "code", "source": ["models = [gbm2, lglr2, svc2, adaboost2]\n", "for model in models:\n", "    predictions = model.predict(sec_x_test)\n", "    Submission = pd.DataFrame({ 'PassengerId': PassengerId,\n", "                            'Survived': predictions })\n", "    Submission.to_csv(model.name + \".csv\", index = False)"]}, {"metadata": {"collapsed": true}, "outputs": [], "execution_count": null, "cell_type": "code", "source": []}]}