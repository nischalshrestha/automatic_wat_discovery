{"cells":[{"metadata":{"_uuid":"8d5f0bfa8661995afbaf2a2ce2cabb473dd7ba30"},"cell_type":"markdown","source":"# Debiasing Imputation #\n\nThis notebook is about dealing with missing data that does not increase bias (gender bias, race bias, etc.), or even potentially reduce it\n\n"},{"metadata":{"_uuid":"98ade56477e061fd24cae9083f741e2adf697bf8"},"cell_type":"markdown","source":"## Problem statement ##\nMost common way to handle missing data is to drop them. The second most common way is to replace the missing data with the most likely value. For the categorical features it is the most frequent value. For the numerical features it is the mean. `scikit-learn` has a class available for this: [SimpleImputer](http://scikit-learn.org/dev/modules/generated/sklearn.impute.SimpleImputer.html). The problem with this approach is that even though it preserves mean, but it reduces the standard deviation, sometimes very significantly. To demonstrate this, let's consider a simple array, then remove half of the values and replace them with mean, and see what happens with STD:"},{"metadata":{"trusted":true,"_uuid":"a85b44d5d3833aaaf39e03596b2c4e94b3cb5903"},"cell_type":"code","source":"import numpy as np\nfrom scipy.stats import norm, multinomial\noriginal_data = norm.rvs(loc=1.0, scale=0.5, size=1000, random_state=1386)\noriginal_data[:20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9939a4ecf6c377110a0a9b75113328a1a7704929"},"cell_type":"code","source":"#Now replace every other element with the mean 1.0\nmissing_elements = np.asarray([0,1]*500)\nupdated_data = original_data * (1-missing_elements) + missing_elements\nupdated_data[:20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fe46d911d4777c35f6edd3d6df2d549e38158666"},"cell_type":"code","source":"#Now, let's get mean and std of the new distribution:\nmean, std = norm.fit(updated_data)\nprint(f'Mean: {mean}, std: {std}')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ee70aeb2aa6b1ef1ffdcb82f5e5025938838caaa"},"cell_type":"markdown","source":"As you see, even though the mean is the same, the standard deviation is much less. While the imputation of data this way increases the performance of the model, it also amplifies the bias that already exists in the data. In order to prevent amplification of the bias, we have to replace the missing values with a sample from the normal distribution with the same mean and standard deviation. For categorical features it would be a multinomial distribution.\n\nFor debiasing we can try to increase the standard deviation of the distribution from which we sample data for numerical features, and a similar transformation for the multinomial distribution. \n\nIn this notebook I suggest two classes for the numerical and categorical features respectively."},{"metadata":{"_uuid":"e47da8942e19ebfe2442dc3fa9325e3257ace303"},"cell_type":"markdown","source":"## Proposed solution ##"},{"metadata":{"trusted":true,"_uuid":"9d4a6a7724be243cdd0d6f158ae3add8cce01b53"},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nimport numpy.ma as ma\nfrom sklearn.utils.validation import check_is_fitted\nclass NumericalUnbiasingImputer(BaseEstimator, TransformerMixin):\n    \"\"\"Un-biasing imputation transformer for completing missing values.\n        Parameters\n        ----------\n        std_scaling_factor : number\n            We will multiply std by this factor to increase or decrease bias\n    \"\"\"\n    def __init__(self, std_scaling_factor=1, random_state=7294):\n        self.std_scaling_factor = std_scaling_factor\n        self.random_state = random_state\n\n        \n    def fit(self, X: np.ndarray, y=None):\n        \"\"\"Fit the imputer on X.\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            Input data, where ``n_samples`` is the number of samples and\n            ``n_features`` is the number of features.\n        Returns\n        -------\n        self : NumericalUnbiasingImputer\n        \"\"\"\n        if len(X.shape) < 2:\n            X = X.reshape(-1,1)\n        mask = np.isnan(X)\n        masked_X = ma.masked_array(X, mask=mask)\n\n        mean_masked = np.ma.mean(masked_X, axis=0)\n        std_masked = np.ma.std(masked_X, axis=0)\n        mean = np.ma.getdata(mean_masked)\n        std = np.ma.getdata(std_masked)\n        mean[np.ma.getmask(mean_masked)] = np.nan\n        std[np.ma.getmask(std_masked)] = np.nan\n        self.mean_ = mean\n        self.std_ = std * self.std_scaling_factor\n\n        return self\n    \n     \n    def transform(self, X):\n        \"\"\"Impute all missing values in X.\n        Parameters\n        ----------\n        X : {array-like}, shape (n_samples, n_features)\n            The input data to complete.\n        \"\"\"\n        check_is_fitted(self, ['mean_', 'std_'])\n\n        if len(X.shape) < 2:\n            X = X.reshape(-1,1)\n        mask = np.isnan(X)\n        n_missing = np.sum(mask, axis=0)\n        \n        def transform_single(index):\n            col = X[:,index].copy()\n            mask_col = mask[:, index]\n            sample = np.asarray(norm.rvs(loc=self.mean_[index], scale=self.std_[index], \n                                         size=col.shape[0], random_state=self.random_state))\n            col[mask_col] = sample[mask_col]\n            return col\n            \n        \n        Xnew = np.vstack([transform_single(index) for index,_ in enumerate(n_missing)]).T\n        \n\n        return Xnew\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c3d2409533b9dbe61923a7070cf0359452b93b5e"},"cell_type":"code","source":"imputer = NumericalUnbiasingImputer()\nmissing_indicator = missing_elements.copy().astype(np.float16)\nmissing_indicator[missing_indicator == 1] = np.nan\ndata_with_missing_values = original_data + missing_indicator\ndata_with_missing_values = np.vstack([data_with_missing_values, original_data*5]).T\nimputer.fit(data_with_missing_values)\ntransformed = imputer.transform(data_with_missing_values)\nprint(transformed[:20,:])\ntransformed.shape\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2bea516e85af438007aac6cf443c904e69cbafd6"},"cell_type":"code","source":"#Let's see how it is different from the original array:\nnew_mean, new_std = norm.fit(transformed[:,0])\nprint(f'Mean: {new_mean}, Std: {new_std}')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2a8ea611b36a7d4ea9459cae89ec3a1d245dd3bf"},"cell_type":"markdown","source":"Some difference in the standard deviation can be explained, because we fitted the model on the incomplete data.\n\nNow we need to do the same for the categorical features"},{"metadata":{"_uuid":"afd906e7e87ed9dbabee65d798edb5690a9ad1db"},"cell_type":"markdown","source":"In the multinomial distribution there is no single parameter responsible for standard deviation. However we can observe, that scaling the standard deviation of the normal distribution is equivalent to scaling `x`. If we do a similar transformation in the multinomial distribution, this would be equivalent to raising the parameters to the power of $\\frac{1}{s}$, where $s$ is the scaling factor"},{"metadata":{"trusted":true,"_uuid":"db86e649b8f17562f9079f6e5fcdbae41979e079"},"cell_type":"code","source":"import pandas as pd\nclass CategoricalUnbiasingImputer(BaseEstimator, TransformerMixin):\n    \"\"\"Un-biasing imputation transformer for completing missing values.\n        Parameters\n        ----------\n        std_scaling_factor : number\n            We will multiply std by this factor to increase or decrease bias\n    \"\"\"\n    def __init__(self, scaling_factor=1, random_state=7294):\n        self.scaling_factor = scaling_factor\n        self.random_state = random_state\n\n        \n    def fit(self, X: np.ndarray, y=None):\n        \"\"\"Fit the imputer on X.\n        Parameters\n        ----------\n        X : {array-like, sparse matrix}, shape (n_samples, n_features)\n            Input data, where ``n_samples`` is the number of samples and\n            ``n_features`` is the number of features.\n        Returns\n        -------\n        self : NumericalUnbiasingImputer\n        \"\"\"\n        if len(X.shape) < 2:\n            X = X.reshape(-1,1)\n\n        def fit_column(column):\n            mask = pd.isnull(column)\n            column = column[~mask]\n            unique_values, counts = np.unique(column.data, return_counts=True)\n            total = sum(counts)\n            probabilities = np.array([(count/total)**(1/self.scaling_factor) \n                    for count in counts])\n            total_probability = sum(probabilities)\n            probabilities /= total_probability\n            return unique_values, probabilities\n\n\n        self.statistics_ = [fit_column(X[:,column]) for column in range(X.shape[1])]\n\n        return self\n    \n     \n    def transform(self, X):\n        \"\"\"Impute all missing values in X.\n        Parameters\n        ----------\n        X : {array-like}, shape (n_samples, n_features)\n            The input data to complete.\n        \"\"\"\n        check_is_fitted(self, ['statistics_'])\n\n        if len(X.shape) < 2:\n            X = X.reshape(-1,1)\n        \n        def transform_single(index):\n            column = X[:,index].copy()\n            mask = pd.isnull(column)\n            values, probabilities = self.statistics_[index]\n\n            sample = np.argmax(multinomial.rvs(p=probabilities, n=1,\n                                         size=mask.sum(), random_state=self.random_state), axis=1)\n            column[mask] = np.vectorize(lambda pick: values[pick])(sample);\n            return column\n            \n        \n        Xnew = np.vstack([transform_single(index) for index in range(len(self.statistics_))]).T\n        \n\n        return Xnew","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c300caa7a198a7b9a8b0b754b9bd168fab06e6dc"},"cell_type":"code","source":"names = np.array(['one', None, 'two', 'three', 'four', 'one', None, 'one', 'two'])\nnames = names.reshape(-1,1)\ncat_imp = CategoricalUnbiasingImputer(random_state=121)\ncat_imp.fit(names)\nprint(cat_imp.statistics_)\nimputed = cat_imp.transform(names)\nimputed","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a796002bbddaeb7c33378af598a17f793f5b5eb2"},"cell_type":"markdown","source":"Now we test our utilities on the scikit-learn datasets. Let's try our approach on the famous titanic dataset. Let's see if any of the columns contain nans"},{"metadata":{"trusted":true,"_uuid":"4c3d915c7ec86e8061b1521ed6dd209b52d5e762"},"cell_type":"code","source":"titanic = pd.read_csv(\"../input/train.csv\")\ntitanic.isna().sum(axis=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"663cc890364fc574cacb52f33640ebabde67f9bc"},"cell_type":"code","source":"titanic.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ec27ed6a4019cde7c5f9ae10da26d3b03a12bb34"},"cell_type":"markdown","source":"We see that Age is numeric, and Cabin is an object feature. Let's update them using our imputers"},{"metadata":{"trusted":true,"_uuid":"d22e9eb621eb76c9fd4953ca074a70e05a0f482a"},"cell_type":"code","source":"n_imputer = NumericalUnbiasingImputer()\ntitanic.Age = n_imputer.fit(titanic.Age.values).transform(titanic.Age.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0ef71f3dd8ec785f7fee14a56df6aeaf26d554fc"},"cell_type":"code","source":"c_imputer = CategoricalUnbiasingImputer()\ntitanic.Cabin = c_imputer.fit(titanic.Cabin.values).transform(titanic.Cabin.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5dde8c13839b8310d7d5cb617b72b6085fceb102"},"cell_type":"code","source":"#Let's see how it transformed Age\ntitanic.Age.head(20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7f7b39e906eefc9c09adcad29cd0d0c44d286682"},"cell_type":"markdown","source":"OK, negative age is a bit too much... But keep in mind that the purpose is not to reconstruct the data, but to avoid amplifying bias in the machine learning model. Let's make sure Age and Cabin now is not null"},{"metadata":{"trusted":true,"_uuid":"2b607b17c96562de1265e4ed6b16aa94139b92ff"},"cell_type":"code","source":"print(titanic.Age.isnull().sum())\nprint(titanic.Cabin.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f68e9e528c373ff0996521ed6d95286af8400cb4"},"cell_type":"code","source":"#Unique values of the Cabin\ntitanic.Cabin.unique()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c3405594d80c4c1330f5a2c589160c397f4e8b2c"},"cell_type":"markdown","source":"# Next Steps #\n\nNow we need to find a dataset that is known to have a gender or race bias, and demonstrate, that with this technique we can avoid amplifying bias, and maybe even decrease the bias by applying a scaling factor"},{"metadata":{"trusted":true,"_uuid":"a07748daf1fa56432a4eb6456bfc241e2440f85b"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}