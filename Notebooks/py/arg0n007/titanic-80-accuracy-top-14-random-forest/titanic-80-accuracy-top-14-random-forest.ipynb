{"metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}, "language_info": {"name": "python", "nbconvert_exporter": "python", "mimetype": "text/x-python", "file_extension": ".py", "version": "3.6.3", "pygments_lexer": "ipython3", "codemirror_mode": {"name": "ipython", "version": 3}}}, "nbformat_minor": 1, "nbformat": 4, "cells": [{"metadata": {"_cell_guid": "ccffc0ff-f521-44f1-acdc-f9f3d248bddf", "_uuid": "6b50e02726e24796783d03044a9959c0ce55afe7", "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["### INTRODUCTION ###\n", "\n", "# The problem here is not to build a classifier but the real task is to Extract useful information.\n", "# In the Begining some columns may seem useless but when you Visualize the data You will notice\n", "# that there is a lot of Impotant Content Hiding in Plane Site.\n", "\n", "# I am Here to Guide you through My Kernel and help you understand how and why I did what I did\n", "\n", "#### NOTE: THE RESULT MAY VARY DUE TO THE RANDOMNESS INTRODUCED IN THE CODE BELOW ####\n", "\n", "# The code is Divied into 5 Phases:\n", "# 1st Phase is to Import and preprocess Data\n", "# 2nd Phase is to encode Data and dropiing unwanted Colomns\n", "# 3rd Phase is to Fit the data in classifier \n", "# 4th Phase is to Visualize the Result\n", "# 5th and the final Phase is to Build the Csv file for submition\n", "\n", "########## 1St Phase ##########\n", "\n", "# First Thing to Due is to Import Some Basic libraries that will help us to process data\n", "\n", "# Numpy is one of the most essential Libraries as it provides us with the Numpy array support\n", "import numpy as np\n", "\n", "# matplotlib.pyplot and Seaborn is used to Visualize the data\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "%matplotlib inline\n", "\n", "# pandas is used to import data to our code\n", "import pandas as pd\n", "\n", "import warnings\n", "warnings.filterwarnings('ignore')"]}, {"metadata": {"_cell_guid": "f3ea92c2-6fe3-49fe-a767-27085c64d949", "_uuid": "a53808c217a648a791170a12f07a18de337cb93c", "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Let us first import the dataset to our code\n", "dataset_train = pd.read_csv('../input/train.csv')\n", "dataset_test = pd.read_csv('../input/test.csv')\n", "dataset_gd = dataset_test\n", "\n", "# Now we have imported both the test set and the train set\n", "# We will then concactinated the 2 so as to increase the accuracy of the data preprocessing\n", "\n", "dataset_train = pd.concat([dataset_test,dataset_train],axis=0)"]}, {"metadata": {"_cell_guid": "b5a4a484-2a04-499d-9d65-8f1a83f3131d", "_uuid": "9016f1e9d78650b5b76a066a2f519001d0de4851", "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["dataset_train.info()"]}, {"metadata": {"_cell_guid": "68ac7a57-9171-4886-9f91-e49970c1f57d", "_uuid": "4aca2eb6cb5fd70034668a810ba750b76dd9032b", "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# From the above Information we can conclude that there are columns with null values\n", "# Hence the 1st things we need to do is to handle these null values\n", "# The columns with nulls values clearly are (Cabin , Survived, Embarked, Age)\n", "\n", "# HANDLING THE NULL VALUES\n", "\n", "#FARE\n", "# Since there is only one value missing in this Column we would simply replace it by the column's mean\n", "dataset_train['Fare'].fillna(dataset_train.Fare.mean(),inplace=True)\n", "\n", "#AGE\n", "# This is the randomness that I was talking about\n", "# What I have done is that i have replaced the missing values between \n", "# mean + standard diviation and mean - standard diviation\n", "\n", "age_mean = dataset_train.Age.mean()\n", "age_std = dataset_train.Age.std()\n", "dataset_train['Age'][dataset_train.Age.isnull()] = np.random.randint(high=age_mean+age_std,low=age_mean-age_std,size=len(dataset_train['Age'][dataset_train.Age.isnull()]))\n", "\n", "#EMBARKED\n", "# You have to understand that 'nan' means undefined and hence I have created a new category \n", "# for the missing values in the embarked column and replaced it with 'n'\n", "dataset_train.Embarked.fillna('n',inplace=True)\n", "\n", "\n", "#CABIN\n", "# As you can see that most of the values in the column is null hence will not replace these values\n", "# I will drop this column in the future after extarcting some useful information"]}, {"metadata": {"_cell_guid": "1c94a1d0-b573-4373-a024-e0b76ed799fd", "_uuid": "c9eb4480e07d8ead4e4b6afba0d293be375a5b79", "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Extracting useful Information Form the data provied \n", "\n", "# Extracting Info from Cabin\n", "\n", "\n", "# Def_Cabin\n", "# Here we create a column which would tell us that whether the cabin is defined for a customer or not\n", "dataset_train['def_Cabin'] = dataset_train.Cabin.notnull().astype(int)\n", "\n", "sns.barplot(data=dataset_train, x='def_Cabin', y='Survived')\n", "plt.show()\n", "# From the graph below we can notice a clear Pattern and hence we will keep this Column"]}, {"metadata": {"_cell_guid": "222f2383-ca58-4b65-9cd0-c2a4137b121a", "_uuid": "e0ed57122e929200cf179fc23b4b5a9befc988d5", "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# NAME\n", "\n", "# Name Length\n", "# I Can't really Explain why but name length has a clear pattern and seems to significantly improve the\n", "# result and hence I am obliged to use it in my Code\n", "dataset_train['Name_Length'] = dataset_train['Name'].apply(lambda x : len(x))\n", "dataset_train['Name_Length'] = ((dataset_train.Name_Length)/15).astype(np.int64)+1\n", "print(dataset_train[['Name_Length','Survived']].groupby(['Name_Length'], as_index = False).mean())\n", "plt.subplots(figsize=(15, 6))\n", "sns.barplot(data=dataset_train,x='Name_Length',y='Survived')\n", "\n", "# You may now think that Name is a useless column but Name contains somethings very important,'Titles'\n", "# If You observe closely you will notice that all names have a Title, example : 'MR','Mrs','Cpt',etc\n", "\n", "# EXTRACTING TITLE FORM NAME\n", "title = dataset_train.Name.values\n", "import re\n", "for i in range(len(title)):\n", "    r = re.search(', ([A-Za-z ]*)',title[i])\n", "    title[i] = r.group(1)\n", "dataset_train.loc[:,'Name'] = title \n", "plt.subplots(figsize=(15, 6))\n", "sns.barplot(data=dataset_train,x='Name',y='Survived')\n", "# Hence from the figure below show that it may play an important role in the decision making process"]}, {"metadata": {"_cell_guid": "0c196603-eadc-4b7e-8c25-d39a7a6297af", "_uuid": "9d0a53679090942de89132a95766621e1a62f717", "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# EXTRACTING INFO FROM TICKETS\n", "# Now Ticket here is unique for each cutomer but there are a few things \n", "# that are common and contains usefull information like:\n", "\n", "# TICKET LENGTH\n", "# Now You may think why ticket_length, You see this may indicated the type of Ticket one has\n", "dataset_train['Ticket_Length'] = dataset_train['Ticket'].apply(lambda x : len(x))\n", "plt.subplots(figsize=(15, 6))\n", "sns.barplot(data=dataset_train,x='Ticket_Length',y='Survived')"]}, {"metadata": {"_cell_guid": "846a1c6d-2bf8-47c7-9b52-4b48dfe80ffc", "_uuid": "f2c98b68f39f27cbb3459185378d3a7715d2dea5", "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# TICKET lETTERS\n", "# This will tell us the category of the ticket a customer has\n", "dataset_train['Ticket_Initials'] = dataset_train['Ticket'].apply(lambda x : str(x)[0]) \n", "dataset_train['Ticket_Initials'] = dataset_train['Ticket_Initials'].apply(lambda x : re.sub('[0-9]','N',x))\n", "plt.subplots(figsize=(15, 6))\n", "sns.barplot(data=dataset_train,x='Ticket_Initials',y='Survived')\n", "# A Clear Pattern in the Graph below"]}, {"metadata": {"_cell_guid": "793b78d1-2026-41a0-af27-b047cfa7edcc", "_uuid": "536122e8250820f1092eaea85cad56f8b65bc028", "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# COMBINING THE SIBSP AND PARCH INTO FAMILY\n", "# We can Combine 2 Columns from our dataset to reduced the complexicity\n", "family = dataset_train.SibSp + dataset_train.Parch+1\n", "dataset_train['Family'] = family\n", "plt.subplots(figsize=(15, 6))\n", "sns.barplot(data=dataset_train,x='Family',y='Survived')\n", "# Similarly this show a clear pattern"]}, {"metadata": {"_cell_guid": "62c72cf2-a648-4cbe-8208-88c85233b689", "_uuid": "fb2a3b4e6276374834639d6073e726f951689efc", "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# CREATING THE COLUMN IS_ALONE\n", "# The second piece of information we could extract is whether a person is travelling Alone or not\n", "dataset_train['Is_Alone'] = dataset_train['Family'].apply(lambda x : 1 if x>1 else 0)\n", "plt.subplots(figsize=(15, 6))\n", "sns.barplot(data=dataset_train,x='Is_Alone',y='Survived')"]}, {"metadata": {"_cell_guid": "b05bb78e-5cd6-4811-916a-eb1210149fc8", "_uuid": "f4da418f65c4f34c76ea33dea5b757f1072d86fa", "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# DIVIDING THE FARE INTO GROUPS\n", "# We Cannot Use Fare in the Current format as the range on the Fare is Quite High and\n", "# hence may dominate the decision. To avoid This We would Divide it into groups\n", "# This may not be the best way to divide but is simple to understand and implement\n", "dataset_train['Fare_Group'] = (dataset_train['Fare']/25).values.astype(np.int64)\n", "plt.subplots(figsize=(15, 6))\n", "sns.barplot(data=dataset_train,x='Fare_Group',y='Survived')"]}, {"metadata": {"_cell_guid": "795131c5-273d-4f19-a277-21d51133f8f7", "_uuid": "2c51dc78d65aa0e0b0397149703eaa3243dffe7b", "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# DIVIDING AGE INTO AGE GROUPS\n", "# Similarly, I will also divide age into groups using the same process\n", "dataset_train['Age_Group'] = ((dataset_train['Age']/15)+1).astype(np.int64)\n", "plt.subplots(figsize=(15, 6))\n", "sns.barplot(data=dataset_train,x='Age_Group',y='Survived')\n", "# It is a clear indication that kids and elderly are likely to survive  "]}, {"metadata": {"_cell_guid": "acc58ab1-88fc-4fdc-8e96-a70096140e42", "_uuid": "84a151ada461c3aea04730847e8ba5b684a52e35", "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["\n", "# Now We have completed Phase 1 of the Code that is Preprocesing Infromation\n", "# the 2nd Phase of the code is to encode the data and and dropiing unwanted columns\n", "\n", "# Encoding String values to Numbers\n", "from sklearn.preprocessing import LabelEncoder\n", "\n", "#SEX\n", "lb_Sex = LabelEncoder()\n", "dataset_train['Sex'] = lb_Sex.fit_transform(dataset_train.Sex)\n", "\n", "#EMBARKED\n", "lb_Emb = LabelEncoder()\n", "dataset_train['Embarked'] = lb_Emb.fit_transform(dataset_train.Embarked)\n", "\n", "#TITLE\n", "lb_Title = LabelEncoder()\n", "dataset_train['Name'] = lb_Title.fit_transform(dataset_train.Name)\n", "\n", "#TICKET_INITIAL\n", "lb_Ticket_init = LabelEncoder()\n", "dataset_train['Ticket_Initials'] = lb_Ticket_init.fit_transform(dataset_train.Ticket_Initials)\n", "\n", "# DROPPING THE EXTRA COLUMNS\n", "dataset_train.drop(labels=['SibSp','Parch','Ticket','Fare','Age','PassengerId','Cabin'],axis=1,inplace=True)\n", "\n", "dataset_train.head()"]}, {"metadata": {"_cell_guid": "5ff15eb1-eb94-4ffe-9e85-e792fa8c59a7", "_uuid": "c3e26f51b21131c3749a12bf3795fed17ac7308e", "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# This marks the Completion of the 2nd Phase and the Start of 3rd Phase\n", "# This phase is about preparing the data for our classifiers\n", "\n", "#RETREIVING THE TEST AND THE TRAIN SETS\n", "dataset_test = dataset_train[dataset_train.Survived.isnull()]\n", "dataset_train = dataset_train[dataset_train.Survived.notnull()]\n", "\n", "dataset_test = dataset_test.drop(['Survived'],axis=1)"]}, {"metadata": {"_cell_guid": "f8c7f32f-173c-4891-857b-16451062e684", "_uuid": "cb6842170404ceee51b250cc3b2326b63387a3c4", "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["#DIVIDING THE DATA INTO Y_TRAIN AND X_TRAIN AND CONVERTING THEM INTO NP ARRAYS\n", "y_train = dataset_train.loc[:,'Survived'].values\n", "x_train =dataset_train.drop(['Survived'],axis=1).values\n", "x_test = dataset_test.values"]}, {"metadata": {"_cell_guid": "4c892751-edbb-4a71-8318-f06c4b51da81", "_uuid": "456c09a250233554f89cb558d202f5e1591a6c3b", "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Feature Scaling \n", "from sklearn.preprocessing import MinMaxScaler\n", "sc_x = MinMaxScaler((-1,1))\n", "x_train  = sc_x.fit_transform(x_train)\n", "x_test = sc_x.transform(x_test)"]}, {"metadata": {"_cell_guid": "420aeb83-a5a2-423b-b41e-33d54ccdb156", "_uuid": "a8f644437647a4e4cd95c7889148e2ea1512e5c0", "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["########## Phase 3 ##########\n", "# In this Phase we would simply fitting the data into our classifier and and apply k-Fold Validation\n", "\n", "# Confusion Matrix\n", "from sklearn.metrics import confusion_matrix\n", "dict_K = {}\n", "dic = {}\n", "\n", "#Kfold Validation\n", "def get_acc(Xtrain,Ytrain,model):\n", "    from sklearn.model_selection import KFold\n", "    acc = []\n", "    k=KFold(n_splits=4)\n", "    for train , test in k.split(Xtrain,y=Ytrain):\n", "        x_train = Xtrain[train,:]\n", "        y_train = Ytrain[train]\n", "        x_test = Xtrain[test,:]\n", "        y_test = Ytrain[test]\n", "        model.fit(x_train,y_train)\n", "        y_pred = model.predict(x_test)\n", "        cm = confusion_matrix(y_true=y_test,y_pred=y_pred)\n", "        acc.append((cm[1,1]+cm[0,0])/((cm[1,0]+cm[0,1]+cm[1,1]+cm[0,0])+1e-5))\n", "    return acc\n", "  "]}, {"metadata": {"_cell_guid": "99c13038-c70c-4a53-b1cd-155af2f451eb", "_uuid": "ad22bf11eefc0709af9e77974ef0340ae9d7b6c9", "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Fitting Decision Tree Classification to the Training set\n", "from sklearn.tree import DecisionTreeClassifier\n", "classifier = DecisionTreeClassifier()\n", "dict_K['Decision'] = get_acc(x_train,y_train,classifier)\n", "classifier.fit(x_train, y_train)\n", "y_pred = classifier.predict(x_test)"]}, {"metadata": {"_cell_guid": "0a140340-3ae3-440c-8c44-360af37eb3d6", "_uuid": "ad76e4a8ae7105b3cbf068d7bf15d078edfc78ee", "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Fitting K-NN to the Training set\n", "from sklearn.neighbors import KNeighborsClassifier\n", "classifier = KNeighborsClassifier(n_neighbors=50,algorithm='auto')\n", "dict_K['KNN'] = get_acc(x_train,y_train,classifier)\n", "classifier.fit(x_train, y_train)\n", "y_pred = classifier.predict(x_test)"]}, {"metadata": {"_cell_guid": "47de811b-0797-47c7-bf84-e4348be7270d", "_uuid": "0483e0569418f28cdbb64e0807cbc31372d81cb1", "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Fitting Kernel SVM to the Training set\n", "from sklearn.svm import SVC\n", "classifier = SVC(kernel='rbf')\n", "dict_K['kernel-SVM'] = get_acc(x_train,y_train,classifier)\n", "classifier.fit(x_train, y_train)\n", "y_pred = classifier.predict(x_test)"]}, {"metadata": {"_cell_guid": "ded2dcbc-4336-44c2-b061-7a2c22245b9a", "_uuid": "6a0e0245aeeaee15793f734269b4c98de179108c", "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Fitting Logistic Regression to the Training set\n", "from sklearn.linear_model import LogisticRegression\n", "classifier = LogisticRegression(solver='newton-cg')\n", "dict_K['Logistic'] = get_acc(x_train,y_train,classifier)\n", "classifier.fit(x_train, y_train)\n", "y_pred = classifier.predict(x_test)"]}, {"metadata": {"_cell_guid": "0e9b754b-bcd1-481b-8a0d-6b580a906b98", "_uuid": "b5ad0de119104f744b7f5de32777f90d266dab4a", "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Fitting Naive Bayes to the Training set\n", "from sklearn.naive_bayes import GaussianNB\n", "classifier = GaussianNB()\n", "dict_K['Naive'] = get_acc(x_train,y_train,classifier)\n", "classifier.fit(x_train, y_train)\n", "y_pred = classifier.predict(x_test)"]}, {"metadata": {"_cell_guid": "8341ae7a-83bb-4d7a-8749-ba5297ff001d", "_uuid": "fe5ddf86139d198f2209d8c2e04bf4c27ef612e9", "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Fitting Random Forest Classification to the Training set\n", "from sklearn.ensemble import RandomForestClassifier\n", "classifier = RandomForestClassifier(n_estimators=25,criterion='entropy')\n", "dict_K['Random_forest'] = get_acc(x_train,y_train,classifier)\n", "classifier.fit(x_train, y_train)\n", "y_pred = classifier.predict(x_test)"]}, {"metadata": {"_cell_guid": "713738dd-bb19-4916-ab47-bb5434e5ec36", "_uuid": "29321b2a4046e22ff524c76874f6f9616ca8e50a", "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["########## Phase 4 ##########\n", "# This is about visualizing the result of the K-Fold Validation process\n", "\n", "#VISUALISING THE RESULTS\n", "df_k = pd.DataFrame(dict_K)\n", "s =df_k.plot(figsize=(10,10),linewidth=5.0)\n", "plt.show()"]}, {"metadata": {"_cell_guid": "013028bb-6d7a-4bd6-9f19-8699c38fe26c", "_uuid": "072f95d69eba065c94bb743146ccad39c6e05b00", "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["#Calculating the Mean of the k_fold validation\n", "df_k.mean()"]}, {"metadata": {"_cell_guid": "6f9f57fd-6e86-4001-847a-ad59bb78327f", "_uuid": "fee5dfaa54fd99d674ab68f8531c96cb0df9fda0", "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["########## 5th phase ##########\n", "# This Final Phase is about preparing the csv file for Submition\n", "# Preparing the CSV For Submition\n", "p = dataset_gd.PassengerId\n", "p = pd.concat([p,pd.DataFrame(y_pred.astype(np.int64),columns=['Survived'])],axis=1)\n", "p.to_csv('Tit_pred.csv',index=False)"]}, {"metadata": {"_cell_guid": "0a3c52a6-c56c-4850-893e-c883b14f3fd8", "_uuid": "e3dc69f8a66d5e287db859d88ee111361c9116ce", "collapsed": true}, "cell_type": "code", "execution_count": null, "outputs": [], "source": []}]}