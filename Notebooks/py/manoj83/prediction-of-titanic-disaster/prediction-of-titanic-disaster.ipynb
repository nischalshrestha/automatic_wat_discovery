{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#import the dataset\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\n#import required libraries for accuracy matrix calcuation\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, roc_curve\n\nds_train = pd.read_csv('../input/train.csv')\nds_test = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c67df016f8f5769d8b19578cd6b4ed07219f7cc9"},"cell_type":"code","source":"#Create copy of the test data set which will be needed for submission purpose\nds_test_copy = ds_test.copy(deep=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fe96e451015101f6066e8090437827aa3d0d0389"},"cell_type":"code","source":"#get train data information so that we can find missing data and datatypes\nprint(ds_train.info());\nprint('-'*10)\nprint(ds_test.info());","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"99c107051ae88407af61b5b1268f06354bf51dd5"},"cell_type":"code","source":"#find the total number of null value rows for each column\nprint(ds_train.isnull().sum())\nprint(ds_test.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dcc61618308d43b0fc491a8f1d8e43ccce1e1a23"},"cell_type":"code","source":"#combine both the train and test data sets so that we can do modification in data set and feature scalling consistently for both data sets\ndataset = [ds_train, ds_test]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"301d7269ff77cb912ff454a032c80e181254a20a"},"cell_type":"code","source":"#Need to preprocess the data.\n#Here we can see that some of the records in train data sets for column Age, Cabin and Embarked don't have data\n#Like wise test data set also don't have the data  \n#Need to replace null values with some placeholder\n#We will be using median of Age and Fare to fill the empty data\nfor dset in dataset:\n    dset['Age'].fillna(dset['Age'].median(), inplace=True)\n    dset['Fare'].fillna(dset['Fare'].mode()[0], inplace=True)\n    dset['Embarked'].fillna(dset['Embarked'].mode()[0], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a5ca729e23393146ee4ee01147ab8c88ac3dd119"},"cell_type":"code","source":"\n#Do feature scalling for test and train data.\n# We will be creating new features based on the existing feature.\n#1. Title:  Create title from name present in both data sets\n#2. Cabin_assigned: We can see that some of the passenger assigned with cabins whereas other don't have cabin assign to them\n    #So here cabin could be an important point to find the survival of the passenger. Check the existence of the cabin for a given passenger\n#3. Age_Cat: From the dataset we can see that Age is scattered from 0.4 to 80 years. We will be categorizing age in to bin of 4\n#4. Fare_Cat: We are categorizing fare again in to different 4 categories\n#5. Familly_Size: Familly size will include the passengers sibling, spouse, parents and passenger himself\n#As we can see that there is Title present in name, let's create a new feature Title from saluation present in name attribute\nfor dset in dataset:\n    dset['Title'] = dset.Name.str.split(',', expand=True)[1].str.split('.', expand=True)[0]\n    dset['Cabin_assigned'] = ~dset.Cabin.isnull()\n    dset['Age_Cat'] = pd.qcut(dset.Age, q = 4, labels = False)\n    dset['Fare_Cat'] = pd.qcut(dset.Fare, q = 4, labels = False)\n    dset['Familly_Size'] = dset.SibSp + dset.Parch + 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"938f55b39ed4577864247d1f46fc26583db832c4"},"cell_type":"code","source":"#Plot all available title in data \nsns.countplot(x='Title', data = ds_train)\nplt.xticks(rotation=45);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a5cd19786df8abd0943a383abab9b6ce7fb29495"},"cell_type":"code","source":"#From above graph we can see that there are Titles like Mme, Major, Lady etc. We will be merging all those titles to single title as Special\nds_train['Title'].replace({'Mlle':'Miss', 'Mme': 'Mrs', 'Ms':'Miss'}, inplace=True, regex=True)\nds_train['Title'].replace(['Don','Dona', 'Rev', 'Dr', 'Major', 'Lady', 'Sir', 'Col', 'Capt', 'the Countess', 'Jonkheer'], 'Special', inplace=True, regex=True)\nds_test['Title'].replace({'Mlle':'Miss', 'Mme': 'Mrs', 'Ms':'Miss'}, inplace=True, regex=True)\nds_test['Title'].replace(['Don','Dona', 'Rev', 'Dr', 'Major', 'Lady', 'Sir', 'Col', 'Capt', 'the Countess', 'Jonkheer', 'Speciala'], 'Special', inplace=True, regex=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2cea11989794fefed71e96767f4a6eec6f00c99b"},"cell_type":"code","source":"#Since there are some of the column which we don't need now like Name, Cabin, Age and Fare.\n#The reason being we have derived new attributes from the above attributes\nremove_attribute = ['Age', 'Fare', 'Cabin', 'Name', 'PassengerId', 'Ticket', 'SibSp', 'Parch']\nds_train = ds_train.drop(remove_attribute, axis = 1)\nds_test = ds_test.drop(remove_attribute, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d49f2393ca6f6698a9221a26a717d91edd194184"},"cell_type":"code","source":"#we will be trying out multiple learning models.\n#below method is used to find the accuracy and precesion of the model \n\n\"\"\"\n Find the accuracy of the model\n\"\"\"\ndef accuracy_score_method(y_test, y_predicted):\n    cm = confusion_matrix(y_test, y_predicted)\n    acc_socre = accuracy_score(y_test, y_predicted)\n    prec_score = precision_score(y_test, y_predicted)\n    roc_curve_value = roc_curve(y_test, y_predicted) \n    print(\"Confusion matrix %s\", cm)\n    print(\"accuracy score %s\", acc_socre)\n    print(\"precision score %s\", prec_score)\n    print(\"roc curve %s\", roc_curve_value)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b3577ef58568a3420c79bf5c4cceef74d5a00022"},"cell_type":"code","source":"#We have already done featuer scalling which includes features like Title, which has number of values like Mr., Mrs. etc.\n#Here we will be encoding features like Gendre, Title, etc.\n#Transforming data into binary variables\nds_train = pd.get_dummies(ds_train, drop_first=True)\nds_test = pd.get_dummies(ds_test, drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"10d723248811e3c3df8febaf49efcec55413eb09"},"cell_type":"code","source":"#Create dependent and independent variables from training data sets\nX = ds_train.iloc[:,1:]\ny = ds_train.iloc[:,0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0ed7d8a255a44ed63986fc71a0f7897de4a09b3c"},"cell_type":"code","source":"#Split dataset into train and testing\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9aa8bdb1160a4a069b8029d3ec55e8cc88ea18ca"},"cell_type":"code","source":"#Create different models and check the result one by one.\n\n#Start with Support Vector Machine SVC\nfrom sklearn.svm import SVC\nclassifier = SVC(kernel='rbf', random_state=0, gamma=0.1)\nclassifier.fit(X_train, y_train)\nsvc_prediction = classifier.predict(X_test)\naccuracy_score_method(y_test, svc_prediction)\n\n#GridSearch algo for parameter tunning for SVC\n\n #from sklearn.grid_search import GridSearchCV\n #svc_parameter = [ {\"kernel\":['rbf']}, {\"gamma\":[1e-1, 1e-2]}]\n #gridsearch = GridSearchCV(  classifier, param_grid = svc_parameter, cv=10)\n #gridsearch.fit(X_train, y_train)\n #print(\"Best parameters %s\", gridsearch.best_params_)\n #print(\"Best score %s\", gridsearch.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"587811c9244fa1399fa69c6fc9994444170bf475"},"cell_type":"code","source":"\n#Compute with RandomForest Tree classification\nfrom sklearn.ensemble import RandomForestClassifier\nrf_classifier = RandomForestClassifier( n_estimators = 10, max_depth=6)\nrf_classifier.fit(X_train, y_train)\nrf_prediction = rf_classifier.predict(X_test)\naccuracy_score_method(y_test, rf_prediction)\n\n#GridSearch algo for parameter tunning for RandomeForest\n\n#rf_parameter = [ {\"n_estimators\":[10,100,1000]}, {\"max_depth\":[1,3,5,6,7,10,100]}]\n#gridsearch = GridSearchCV(  rf_classifier, param_grid = rf_parameter, cv=10)\n#gridsearch.fit(X_train, y_train)\n#print(\"Best parameters %s\", gridsearch.best_params_)\n#print(\"Best score %s\", gridsearch.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"27bf5592fa05f88e8fb98581b9b25c9bdb08186c"},"cell_type":"code","source":"#LogisticRegression\nfrom sklearn.linear_model import LogisticRegression\nlogistic_regression = LogisticRegression(solver='lbfgs', multi_class='multinomial', random_state=0)\nlogistic_regression.fit(X_train, y_train)\nlogistic_prediction = logistic_regression.predict(X_test)\naccuracy_score_method(y_test, logistic_prediction)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b272bf91cd0586ab3a9abf3770c177b897e52536"},"cell_type":"code","source":"#SGDClassifier\nfrom sklearn.linear_model import SGDClassifier\nsgdClassifier = SGDClassifier(penalty='elasticnet')\nsgdClassifier.fit(X_train, y_train)\nsgd_prediction = sgdClassifier.predict(X_test)\naccuracy_score_method(y_test, sgd_prediction)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ffcc75f6c62d5a76a10c1e3f7d65e6dd4421b212"},"cell_type":"code","source":"#Ensemble learning which combines all the algoritham\nfrom sklearn.ensemble import VotingClassifier\nvoting_classifier = VotingClassifier(estimators=[('lr', logistic_regression), ('rf', rf_classifier), ('svc', classifier)], voting='hard')\nvoting_classifier.fit(X_train, y_train)\nvoting_prediction = voting_classifier.predict(X_test)\naccuracy_score_method(y_test, voting_prediction)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b476843528511f9064660d9e0c63aa6f47d5eeae"},"cell_type":"code","source":"#Predict the final result to be submitted to Kaggal\nds_predict = classifier.predict(ds_test)    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bbd3b5e966126a54f9e16353a0a67fd950932f92"},"cell_type":"code","source":"#prepare the submission.csv for kaggle\nds_test_copy['Survived'] = ds_predict\nds_test_copy[['PassengerId', 'Survived']].to_csv('SUBMISSION.csv', index=False)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}