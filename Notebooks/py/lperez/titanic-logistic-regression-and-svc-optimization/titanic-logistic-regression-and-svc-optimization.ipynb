{"nbformat_minor": 0, "metadata": {"language_info": {"nbconvert_exporter": "python", "file_extension": ".py", "version": "3.6.1", "name": "python", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3"}, "kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}}, "nbformat": 4, "cells": [{"cell_type": "markdown", "execution_state": "idle", "execution_count": null, "outputs": [], "source": "# Introduction\n\nIn this notebook, we will try to optimize logistic regression to predict survival rate on the Titanic.\n\nLogistic regression has two main parameters that can be tuned:\n\n - lambda: the regularization parameter\n - the polynomial degree\n\nFor more information, please refer to is a wonderful course on coursera.org which explains Logistic Regression very well: Machine Learning by Pr Andrew Ng.\n\nOf course, comments on this work are more than welcome!", "metadata": {"_cell_guid": "85501a66-137e-9fcf-2efa-fb59da2e9ab9", "_active": false, "_uuid": "fd2c282691d1cc9a91eed92ef8945d4358558a78"}}, {"cell_type": "markdown", "execution_state": "idle", "execution_count": null, "outputs": [], "source": "# Reading data", "metadata": {"_cell_guid": "7548315a-e1ec-dc3b-ca65-2cd54f3f86b8", "_active": false, "_uuid": "e28131cd74ab1c428db1ef6f3d3b6dc320f3f27a"}}, {"cell_type": "code", "execution_state": "idle", "execution_count": null, "outputs": [], "source": "%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport re as re\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ntrain = pd.read_csv('../input/train.csv', header = 0, dtype={'Age': np.float64})\noriginal_test  = pd.read_csv('../input/test.csv' , header = 0, dtype={'Age': np.float64})\ntest = original_test.copy(deep=True)\nfull_data = [train, test]\n\nprint (train.info())", "metadata": {"trusted": false, "_cell_guid": "4d2b99a7-9c9f-33ed-f783-6389040e8e8f", "_active": false, "collapsed": false, "_uuid": "6248e7cbbd84dddafd0b49d4f16e1241e40d7737", "_execution_state": "idle"}}, {"cell_type": "markdown", "execution_state": "idle", "execution_count": null, "outputs": [], "source": "# Preparing data for resolution\n\nIn this notebook, we will not get into the details of feature engineering. We will make some classic processing on the features. There may be several feature optimization that could still be done, but it is not the purpose of this notebook.\n\n## Family size categories\nLet's create LargeFamily (2 classes: 1 to 4, 5 and up)", "metadata": {"_cell_guid": "9b3e809d-20a3-afed-1de3-41e1cd01a349", "_active": false, "_uuid": "154219a1262f2fd2d1334ea8254c08d55fc0b58b"}}, {"cell_type": "code", "execution_state": "idle", "execution_count": null, "outputs": [], "source": "for dataset in full_data:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n    dataset['LargeFamily'] = dataset['FamilySize'].apply(lambda r: 0 if r<=4 else 1)", "metadata": {"trusted": false, "_cell_guid": "a4d69b2a-30dd-d233-b497-856923e4fab8", "_active": false, "_uuid": "1c240ea91765375800d98740dc0f1d043c5a417e", "_execution_state": "idle"}}, {"cell_type": "markdown", "execution_state": "idle", "execution_count": null, "outputs": [], "source": "\n\n## Names ##\ninside this feature we can find the title of people.", "metadata": {"_cell_guid": "008b965d-1470-4e85-7ad5-76fb70273d29", "_active": false, "_uuid": "de6e66fe7448c9b1138a153d223fe253f5e96bcc"}}, {"cell_type": "code", "execution_state": "idle", "execution_count": null, "outputs": [], "source": "def get_title(name):\n\ttitle_search = re.search(' ([A-Za-z]+)\\.', name)\n\t# If the title exists, extract and return it.\n\tif title_search:\n\t\treturn title_search.group(1)\n\treturn \"\"\n\nfor dataset in full_data:\n    dataset['Title'] = dataset['Name'].apply(get_title)\n\nprint(pd.crosstab(train['Title'], train['Sex']))", "metadata": {"trusted": false, "_cell_guid": "c4f5efda-4c05-40ca-a3fb-b35a706f0e1d", "_active": false, "_uuid": "91ecdd580c79cd07ea93284e0c415a6cc6480b25", "_execution_state": "idle"}}, {"cell_type": "markdown", "execution_state": "idle", "execution_count": null, "outputs": [], "source": " so we have titles. let's categorize it and check the title impact on survival rate.", "metadata": {"_cell_guid": "4ba8d40c-caa4-7eb6-f170-a733d0d69926", "_active": false, "_uuid": "cad68d8c5e9f198e3bd34ce4a692f3518b6f03a6"}}, {"cell_type": "code", "execution_state": "idle", "execution_count": null, "outputs": [], "source": "for dataset in full_data:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n\nprint (train[['Title', 'Survived']].groupby(['Title'], as_index=False).mean())", "metadata": {"trusted": false, "_cell_guid": "97457988-dffd-37a7-67f2-ea12678c77bb", "_active": false, "_uuid": "76f4d06861708f56c8ecc7c5d8dd8b515fa9d319", "_execution_state": "idle"}}, {"cell_type": "markdown", "execution_state": "idle", "execution_count": null, "outputs": [], "source": "# Other data\nNow let's clean all other fields and map our features into numerical values.", "metadata": {"_cell_guid": "4a22679a-56b7-6263-74a1-2c0e40451f14", "_active": false, "_uuid": "68713f03b9528a3b8e90758fb8929637e5eee0fe"}}, {"cell_type": "code", "execution_state": "idle", "execution_count": null, "outputs": [], "source": "for dataset in full_data:   \n    # Fill missing values in Embarked with most frequent port 'S'\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n    \n    # Fill missing values in Fare with median\n    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\n\n    # Fill missing values in age with random data based on mean and standard variation\n    age_avg \t   = dataset['Age'].mean()\n    age_std \t   = dataset['Age'].std()\n    age_null_count = dataset['Age'].isnull().sum()\n    \n    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n    dataset.loc[np.isnan(dataset['Age']), 'Age'] = age_null_random_list\n    dataset['Age'] = dataset['Age'].astype(int)\n        \n    # Mapping Sex\n    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n    \n    # Mapping titles\n    title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n    \n    # Mapping Embarked\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n    \n    # Mapping Fare\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] \t\t\t\t\t\t        = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] \t\t\t\t\t\t\t        = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n    \n    # Mapping Age\n    dataset.loc[ dataset['Age'] <= 16, 'Age'] \t\t\t\t\t       = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age']                           = 4\n    \n# Feature Selection\ndrop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp',\\\n                 'Parch', 'FamilySize']\ntrain = train.drop(drop_elements, axis = 1)\n\ntest  = test.drop(drop_elements, axis = 1)\n\nprint (train.head(10))", "metadata": {"trusted": false, "_cell_guid": "c377cc55-0060-96f2-452c-1f1da6f33390", "_active": false, "_uuid": "33094c6d14667ec12e8bfc6da337fcd9d3deb0d2", "_execution_state": "idle"}}, {"cell_type": "markdown", "execution_state": "idle", "execution_count": null, "outputs": [], "source": "Good! now we have a clean dataset.", "metadata": {"_cell_guid": "cffa50e5-cb50-abf7-078c-5f948dc4eff9", "_active": false, "_uuid": "df7bbe9cbd124c3cd3feb6978ab370e92a6f23a3"}}, {"cell_type": "markdown", "execution_state": "idle", "execution_count": null, "outputs": [], "source": "Now let's find which classifier works better on each dataset. ", "metadata": {"_cell_guid": "eb1f6f8e-64c0-fdae-efb8-4c44bb9995dc", "_active": false, "_uuid": "3121bba05b966d2a636693d50541cdbb3f9ea698"}}, {"cell_type": "markdown", "execution_state": "idle", "execution_count": null, "outputs": [], "source": "# Classifier Comparison #\n\n## Logistic regression ##\n\nLet's try different values for:\n\n - lambda: the regularization term\n - the degree of the polynomial combination of features", "metadata": {"_cell_guid": "dc991392-2a86-a9f8-1bc0-5a877d324e77", "_active": false, "_uuid": "b9ca3cf58cb7b7f4896e19c6c141f936e7f2ab22"}}, {"cell_type": "code", "execution_state": "idle", "execution_count": null, "outputs": [], "source": "import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import PolynomialFeatures\n\nn_splits = 10\nsss = StratifiedShuffleSplit(n_splits=n_splits, test_size=0.1, random_state=0)\n\nX = train.values[0::, 1::]\ny = train.values[0::, 0]\n\nlog_cols = [\"lambda\", \"Poly Degree\", \"Accuracy\"]\nlog \t = pd.DataFrame(columns=log_cols)\n\nacc_dict = {}\n\nfor train_index, test_index in sss.split(X, y):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n\n    for lambd in [0.0001, 0.01, 0.03, 0.1, 0.3, 1, 3, 10, 100]:\n        for poly_degree in range(1,4):\n            \n            # Create polynomial features\n            poly = PolynomialFeatures(poly_degree)\n            X_train_poly = poly.fit_transform(X_train)\n            X_test_poly = poly.fit_transform(X_test)\n\n            clf = LogisticRegression(C=1/lambd)        \n            clf.fit(X_train_poly, y_train)\n            train_predictions = clf.predict(X_test_poly)\n            acc = accuracy_score(y_test, train_predictions)\n            if lambd in acc_dict:\n                if poly_degree in acc_dict[lambd]:\n                    acc_dict[lambd][poly_degree] += acc\n                else:\n                    acc_dict[lambd][poly_degree] = acc\n            else:\n                acc_dict[lambd] = {}\n                acc_dict[lambd][poly_degree] = acc\n\n\nfor lambd in acc_dict:\n    for poly_degree in acc_dict[lambd]:\n        acc_value = acc_dict[lambd][poly_degree] / n_splits\n        log_entry = pd.DataFrame([[lambd, poly_degree, acc_value]], columns=log_cols)\n        log = log.append(log_entry)\n\n#print ('Classifier Accuracy')\n#print (log)\n#print ()\n\nplt.figure()\n\nplt.xlabel('Accuracy')\nplt.title('Classifier Accuracy')\n\nheatmap_data = log.pivot(\"lambda\", \"Poly Degree\", \"Accuracy\")\nax = sns.heatmap(heatmap_data, annot=True, fmt='.3f')", "metadata": {"trusted": false, "_cell_guid": "793e174c-d6f6-5b92-0cfc-3aa434671197", "_active": false, "_uuid": "b71325d3ee65400b5c074b509f5c0f8997ddbfce", "_execution_state": "idle"}}, {"cell_type": "markdown", "execution_state": "idle", "execution_count": null, "outputs": [], "source": "Great!\n\nWe can see that purely linear Logistic Regression is not optimal. It has too much bias, and that we can significantly improve results by choosing a degree 2 or more polynomial logistic regression.", "metadata": {"_cell_guid": "af4db595-f71d-9b63-741d-057365641f86", "_active": false, "_uuid": "3d572fb7f9ba8dd454ce086b3bb2d844b09c89d1"}}, {"outputs": [], "cell_type": "markdown", "source": "## Support Vector Machine ##\nLet's do the same job for Support Vector Machine classification.\nFor support Vector Machine, we do not need to use polynomial features. But we have another parameter that can be tuned: the gamma of the kernel function.\n\nSo let's try different values for:\n\n - lambda: the regularization term\n - gamma: parameter of the kernel function\n\nPlease note that for SVM, by convention, we use C=1/lambda. But here we will stick with lambda, to be coherent with logistic regression.", "metadata": {"_cell_guid": "171a2e97-5828-42af-89fc-a0a4f7242285", "collapsed": false, "_uuid": "8bccf66f793f5fc8be072b9c3c0fae3655b2b1de", "_execution_state": "idle"}, "execution_count": null}, {"outputs": [], "cell_type": "code", "source": "from sklearn.svm import SVC\n\nlog_cols = [\"lambda\", \"gamma\", \"Accuracy\"]\nlog \t = pd.DataFrame(columns=log_cols)\n\nacc_dict = {}\n\nfor train_index, test_index in sss.split(X, y):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n\n    for lambd in [0.0001, 0.01, 0.03, 0.1, 0.3, 1, 3, 10, 100]:\n        for gamma in [1.0E-3, 1.0E-2, 1.0E-1, 1.0, 10.0, 1.0E3]:\n            \n            clf = SVC(probability=True, C=1/lambd, gamma=gamma)        \n            clf.fit(X_train, y_train)\n            train_predictions = clf.predict(X_test)\n            acc = accuracy_score(y_test, train_predictions)\n            if lambd in acc_dict:\n                if gamma in acc_dict[lambd]:\n                    acc_dict[lambd][gamma] += acc\n                else:\n                    acc_dict[lambd][gamma] = acc\n            else:\n                acc_dict[lambd] = {}\n                acc_dict[lambd][gamma] = acc\n\n\nfor lambd in acc_dict:\n    for gamma in acc_dict[lambd]:\n        acc_value = acc_dict[lambd][gamma] / n_splits\n        log_entry = pd.DataFrame([[lambd, gamma, acc_value]], columns=log_cols)\n        log = log.append(log_entry)\n\n#print ('Classifier Accuracy')\n#print (log)\n#print ()\n\nplt.figure()\n\nplt.xlabel('Accuracy')\nplt.title('Classifier Accuracy')\n\nheatmap_data = log.pivot(\"lambda\", \"gamma\", \"Accuracy\")\nax = sns.heatmap(heatmap_data, annot=True, fmt='.3f')", "metadata": {"trusted": false, "_cell_guid": "81a3876f-b26a-4ed6-a747-d58468afdec9", "collapsed": false, "_uuid": "71ec845499b5dfd0081e84960fdc98b466d3608b", "_execution_state": "idle"}, "execution_count": null}, {"outputs": [], "cell_type": "markdown", "source": "We can see that gamma=0.1 is a good choice, and that the model doesn't seem to need much regularization.\n\n# Prediction with SVC #\nLet's use SVC with parameters lambda=0.3 and gamma=0.1 to predict our data.\nPlease note that optimal parameters can vary each time you run this notebook. This is because the training and cross-validation sets are randomly choosen.", "metadata": {"_cell_guid": "07b25783-8608-4a04-ab67-ca31c9261c3e", "collapsed": false, "_uuid": "0e18c8bd4e139063bb853a94ae806123c1720b90", "_execution_state": "idle"}, "execution_count": null}, {"outputs": [], "cell_type": "code", "source": "# Use candidate classifier\nlambd = 0.3\ngamma = 0.1\ncandidate_classifier = SVC(probability=True, C=1/lambd, gamma=gamma)   \n\n# Create polynomial features\nX_train = train.values[0::, 1::]\ny_train = train.values[0::, 0]\nX_test = test.values\n\ncandidate_classifier.fit(X_train, y_train)\nsurv_pred = candidate_classifier.predict(X_test)\n\nsubmit = pd.DataFrame({'PassengerId' : original_test.loc[:,'PassengerId'],\n                       'Survived': surv_pred.T})\nsubmit.to_csv(\"../working/submit_svc_gamma01.csv\", index=False)", "metadata": {"trusted": false, "_cell_guid": "19c1634e-6c23-47df-bbf2-100a7631c451", "collapsed": false, "_uuid": "8a2e7e8bb884f65c2087764330f6fff4a689f4e2", "_execution_state": "idle"}, "execution_count": 12}, {"cell_type": "markdown", "execution_state": "idle", "execution_count": null, "outputs": [], "source": "# Prediction with logistic regression #\nLet's use logistic regression with lambda=0.03 and poly_degree=3 to predict our data.", "metadata": {"_cell_guid": "af745ce7-7b06-408a-cc56-d1b34d83bb65", "_active": false, "_uuid": "fd4e540a0bc29cd7cd43cfe2d76aa67cca531b0c"}}, {"cell_type": "code", "execution_state": "idle", "execution_count": null, "outputs": [], "source": "# Use candidate classifier\nlambd = 0.03\npoly_degree = 3\ncandidate_classifier = LogisticRegression(C=1/lambd)\n\n# Create polynomial features\nX = train.values[0::, 1::]\ny = train.values[0::, 0]\npoly = PolynomialFeatures(poly_degree)\nX_train_poly = poly.fit_transform(X)\nX_test_poly = poly.fit_transform(test.values)\n\ncandidate_classifier.fit(X_train_poly, y)\nsurv_pred = candidate_classifier.predict(X_test_poly)\n\nsubmit = pd.DataFrame({'PassengerId' : original_test.loc[:,'PassengerId'],\n                       'Survived': surv_pred.T})\nsubmit.to_csv(\"../working/submit_logistic_poly_3.csv\", index=False)", "metadata": {"trusted": false, "_cell_guid": "08c2dc84-6306-cab1-b7e2-0647e722ea3e", "_active": false, "collapsed": false, "_uuid": "7e9279263244ecb3be889d48a5a4a7a37393da45", "_execution_state": "idle"}}, {"outputs": [], "cell_type": "markdown", "source": "# Check submission file #", "metadata": {"_cell_guid": "1e7b5f20-1f97-4dda-b794-8a59592aa680", "collapsed": false, "_uuid": "1df096e4624dace37377a0ae1d6ea19603b5f27e", "_execution_state": "idle"}, "execution_count": null}, {"cell_type": "code", "execution_state": "idle", "execution_count": null, "outputs": [], "source": "submit.head()", "metadata": {"trusted": false, "_cell_guid": "8558d830-893d-6ec1-d224-67673762baaf", "_active": false, "collapsed": false, "_uuid": "70372afeab3c07f626523f83cb977e8c95bae1d6", "_execution_state": "idle"}}, {"cell_type": "code", "execution_state": "idle", "execution_count": null, "outputs": [], "source": "submit.shape", "metadata": {"trusted": false, "_cell_guid": "8a261f27-f6a0-fbd1-9c4e-1b9148973d64", "_active": false, "collapsed": false, "_uuid": "ea64957a68373ea7ade28267261d269c3892159d", "_execution_state": "idle"}}, {"outputs": [], "cell_type": "markdown", "source": "# Conclusion #\nOn our dataset, we can improve significantly logistic regression by using polynomial combination of features. For SVM, the optimal values are close to the default values, so there is no great improvement to expect from optimization of sigma and lambda.", "metadata": {"_cell_guid": "16cb692a-73eb-4632-92c6-457889480e81", "collapsed": false, "_uuid": "5b0cd6bf1725ef86222fa0845eb7d7403cadeac6", "_execution_state": "idle"}, "execution_count": null}]}