{"cells":[{"metadata":{"_cell_guid":"c8a1ceb1-fb0b-44db-9fa6-3c4ab37297b3","_uuid":"8d4a5bf26a20947620ee84a58da268e63d0cff25"},"cell_type":"markdown","source":"**Data cleaning, model comparison, visualization**\n\n*This workbook compares the results of a set of classification algorithms. The idea was to automate as much as possible sothat it can most flexibily used for other tasks.*"},{"metadata":{"_cell_guid":"8b4efb40-ceed-407b-b106-d6f78b673fa3","_uuid":"ca83f2c9b901b509e66acf299fa8de22b1fd5a85"},"cell_type":"markdown","source":"**1. Importing pakages: **nothing to see here, move on!"},{"metadata":{"_kg_hide-output":true,"_cell_guid":"184a936b-4fb5-40e4-80e5-7ad3baae8a7b","_uuid":"6cd5f392f53153499992cac1d429a2e998c8c3bf","_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"# Importing fundamental datasets\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # plotting\nimport itertools #iterators for efficient looping\nimport seaborn as sns # for quickly plotting heatmaps","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b4171700-c962-4ab0-a9e4-89ec0354083b","_uuid":"5fe499e5ab4505f516a14635fc1de7bf553b55a0"},"cell_type":"markdown","source":"**2. Importing the dataset: **Looking at the data to identify whats text, whats categorical and what can be excluded"},{"metadata":{"_cell_guid":"08e4e73d-2428-4209-83a8-41650105eca5","_uuid":"8a180e17af9627cee04ddf51f9659330e7ad0a87","trusted":true},"cell_type":"code","source":"dataset = pd.read_csv(\"../input/train.csv\", index_col = \"PassengerId\") # reading input\ndataset.sample(5) # printing the first five rows to get an impression of the data","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"76c72a45031aabc07f51bc326518f72d16648dbd","trusted":true},"cell_type":"code","source":"# printing datatypes and missing values\noverview = pd.concat([dataset.dtypes, dataset.isnull().sum()], axis=1)\noverview.columns = [\"type\", \"NAs\"]\noverview","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"26c3a3db-fcdc-49d0-8b87-bda0a0da9687","_uuid":"685fc54055062d18249f7b8fcf161381ab9c1203"},"cell_type":"markdown","source":"**3. Sorting data: **decoding categorical, looking at relevance"},{"metadata":{"_cell_guid":"d4fc7352-364e-42b0-8a02-0a0b27da047c","_uuid":"f337f649f93a2e756080ef4eede575d72bd8dbae","trusted":true},"cell_type":"code","source":"# Creating a new feature: If a family member survived: 1, else: 0\ndataset[\"Family\"] = dataset[\"Name\"].str.split(',').str.get(0) # extract family name\ndataset[\"FamSurv\"] = 0\nfor index, row in dataset.iterrows():\n    for index2, row2 in dataset.iterrows():\n        if not index2 == index:\n            if row[\"Family\"] == row2[\"Family\"] and (row[\"Survived\"] == 1 or row2[\"Survived\"] == 1):\n                dataset.loc[index, \"FamSurv\"] = 1\n\nlstTextCategories = [\"Sex\", \"Ticket\", \"Embarked\", \"Family\"] # list of categorial variables","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"724582a75532f515027499e8ab66bd81892ad0d6","trusted":true},"cell_type":"code","source":"# recode cabin: 1 if there is an entry, 0 else\ndataset[\"Cabin\"] = dataset[\"Cabin\"].where(dataset[\"Cabin\"].isnull(), 1).fillna(0)\ndataset.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9b05e104-902d-4681-a874-154eb405d15f","scrolled":false,"_uuid":"a715b9ac25e8f2c97099289533814f9d242b46fb","trusted":true},"cell_type":"code","source":"for cat in lstTextCategories: # replacing categorial words with numbers\n    dataset[cat] = pd.Categorical(dataset[cat])\n    dataset[cat] = dataset[cat].cat.codes\n    \ncorr = dataset.corr() # calculating correlation matrix\nplt.figure(figsize=(10,8)) # defining plot size\nsns.heatmap(corr, # creating the heatmap plot of the correlation matrix\n            xticklabels = corr.columns.values,\n            yticklabels = corr.columns.values, \n            annot = True, # for showing the values\n            vmin=-1, vmax=1, center=0) # min and max at [-1, 1]\nplt.show() # actually plotting","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"abf89bfbcefed2770914717b825d7079e7960ca4"},"cell_type":"code","source":"# create age-classes\ndataset.hist(column='Age')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"046820b4b0271ba79b620e1eed0982d494706379"},"cell_type":"code","source":"dataset['ageClass'] = dataset['Age']/15\ndataset['ageClass'] = dataset['ageClass'].apply(np.floor)\ndataset.hist(column='ageClass')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1c147b3b-038b-4935-8a59-de1278d020ea","_uuid":"8274827970a65fc310b2ba12c776f416f1b74460"},"cell_type":"markdown","source":"**4. Preselection: **setting the target, dropping (obviously) irrelevants"},{"metadata":{"_cell_guid":"b7080dd4-10e8-4d7a-8f3f-d059cf20a6de","_uuid":"654bffc6b424fed9d78201dbba50148f2c44444b","trusted":true},"cell_type":"code","source":"strTarget = \"Survived\" # defining target variable\nlstExclude = [strTarget, \"Name\", \"Ticket\", \"Family\", \"Age\"] # variables to be excluded from the feature matrix\nlstCategorials = [\"Sex\", \"Pclass\", \"Cabin\", \"SibSp\", \"Parch\"] # categorial variables\n\nlstInclude = [column for column in dataset.columns if column not in lstExclude] # variables to be included\n\ny = dataset[strTarget] # extracting target\nX = dataset.loc[:, lstInclude] # extracting features\nX.describe() # sumarizing variables - note some outliers in the \"fare\" column","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0f17ec01-2366-4196-a024-525b546bd38b","_uuid":"99f1e5a6c3136c0e73ed2d36b7ec1683d1d2a4f4"},"cell_type":"markdown","source":"**5. Further preprocessing: **creating dummy variables, Imputing NA with average\n\n*Question: Does this always make sense? If not: where and why?*"},{"metadata":{"_cell_guid":"d38e0537-5661-4bab-aa63-df62eff9669c","_uuid":"1e603e9cc0129e148c19369ae136d91c48b6eeef","trusted":true},"cell_type":"code","source":"# creating dummy variables\nfrom sklearn.preprocessing import StandardScaler\n\nX = X.fillna(X.mean()) # Imputing NA with average\nX = pd.get_dummies(X, drop_first=True, columns=lstCategorials) # creating dummy variables\nheads = X.columns # finding (new) column heads\n\nX = pd.DataFrame(StandardScaler().fit_transform(X), columns=heads) # Feature scaling\nX.describe() # sumarizing variables","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ed7ce425-93ba-4ba6-94ac-feb432b1008c","_uuid":"84c984b973ea6f898b4ff0d63e77ca57d892f21c"},"cell_type":"markdown","source":"**6. Split: **Splitting to test and training data\n\n*(today: split within the loop)*"},{"metadata":{"_cell_guid":"f8f6de55-1774-498c-ac16-cbde23169b5b","_uuid":"6f378b42f16a7e8f656d0607b62974b823e342fa","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6319cf5d-95fe-4e04-880f-3464a096863f","_uuid":"ca4c254e9e89e1a9c80fbcdc916bd65fb1c85fdc"},"cell_type":"markdown","source":"**7. Model Selection: **comparing different models with 10 different splits - just change the dicModels Dictionary\n\n*(This is computational intensive - consider lowering the number)*"},{"metadata":{"_cell_guid":"8b223830-006c-408d-a17f-b1bb012e154d","scrolled":false,"_uuid":"b478c814b19a2560e75612f01afc8521fbcde081","trusted":true},"cell_type":"code","source":"# importing the individual classification models from scikit learn\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\n\n# definig models and parameter grids which will be tested for best results\ndicModels = {\"Logistic Regression\": {\"classifier\":LogisticRegression(),\n                                     \"param_grid\": {\"solver\": [\"lbfgs\"]}},\n             \"KN Classifier\": {\"classifier\":KNeighborsClassifier(2), \n                               \"param_grid\":{\"n_neighbors\": [4, 6, 8], \n                                             \"weights\": [\"uniform\", \"distance\"]}},\n             \"Naive Bays\": {\"classifier\":GaussianNB()}, \n             \"SVM\": {\"classifier\":SVC(random_state=0), \n                     \"param_grid\":{\"kernel\" : [\"rbf\", \"linear\", \"poly\"],\n                                   \"C\": [0.01, 0.1, 1, 10, 100], \n                                   \"gamma\": [0.001, 0.01]}},\n             \"Random Forest\": {\"classifier\":RandomForestClassifier(max_depth=None, min_samples_split=2, random_state=0),\n                               \"param_grid\":{\"n_estimators\": [150], \n                                             \"max_features\": [\"auto\", \"sqrt\", \"log2\"], \n                                             \"criterion\": [\"entropy\", \"gini\"], \n                                             \"min_samples_split\": [2, 4]}},\n             \"Gradient Boosting\": {\"classifier\":GradientBoostingClassifier(max_depth=None, min_samples_split=2, random_state=0),\n                                   \"param_grid\":{\"n_estimators\": [150], \n                                                 \"learning_rate\": [0.05, 0.1, 0.5], \n                                                 \"loss\": [\"deviance\", \"exponential\"], \n                                                 \"min_samples_split\": [2, 4]}}}\ndicResults = {} # result dictionary\ndicCMs = {} # confusion matrix dictionary\nfor name, model in dicModels.items(): # loop over all modes\n    dicCMs[name] = np.array([[0,0],[0,0]]) # creating empty confusion matrix\n    print(name) # printing name sothat we know where the algorithm stands\n    for split in range(1): # loop over 1 random split of the dataset \n        # creating split\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = split)\n        \n        if \"param_grid\" in model: # if a parameter grid is defined: find optimal configuration\n            clf = GridSearchCV(estimator=model[\"classifier\"], \n                               param_grid=model[\"param_grid\"], \n                               iid=True,\n                               cv=5)\n            clf.fit(X_train, y_train)\n            model[\"classifier\"] = clf.best_estimator_ # takes bast parameters to the model\n                \n        model[\"classifier\"].fit(X_train, y_train) # fit model to training data\n\n        y_pred = model[\"classifier\"].predict(X_test) # Predicting the Test set results\n\n        dicCMs[name] += confusion_matrix(y_test, y_pred) # Making the Confusion Matrix\n    \n    dicResults[name] = {\"correct\":dicCMs[name][0][0]+dicCMs[name][1][1], # creating result table\n                        \"false\":dicCMs[name][0][1]+dicCMs[name][1][0],\n                        \"correct positives\":dicCMs[name][0][0], \n                        \"correct negatives\":dicCMs[name][1][1],\n                        \"false positives\":dicCMs[name][0][1],\n                        \"false negatives\":dicCMs[name][1][0],\n                        \"parameters\": model[\"classifier\"].get_params()}\n\n# bringing results to a datafram (it's just more beautiful) and sorting it by accuracy\nresults = pd.DataFrame(dicResults).T.sort_values([\"correct\", \"correct positives\", \"correct negatives\"], \n                                                 ascending=[0, 0, 0])\n# plotting result tables with bars\nresults.style.bar(subset=[\"correct\", \"correct positives\", \"correct negatives\"], \n                  color=\"#2876dd\", \n                  align=\"mid\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f1939e06-d5b2-4078-91e9-e945e226feb5","_uuid":"db376ada9c86a074ff03071f433ffab610514323"},"cell_type":"markdown","source":"**8. Confusion Matrix:** Plotting for each model"},{"metadata":{"_cell_guid":"7727cfb0-46dd-4df4-9eef-6d3297715c0d","scrolled":false,"_uuid":"bdad2002cd11afabc9d24650c1d3f640237f0da2","trusted":true},"cell_type":"code","source":"for name, cm in dicCMs.items(): # looping over matrices\n    cm = cm / cm.sum(axis=1)[:,None] # making percentages of absolute values\n    plt.figure() # creating new figure\n    ax = plt.axes() \n    sns.heatmap(cm, # creating heat map\n                xticklabels = [\"pred. positive\", \"pred. negative\"],\n                yticklabels = [\"true positive\", \"true negative\"], \n                annot = True, # plotting values\n                cmap = sns.color_palette(\"Blues\"),\n                vmin = 0, vmax = 1)\n    for number in ax.texts: \n        number.set_text(\"{0:.0f}%\".format(float(number.get_text())*100)) # convert values [0, 1] to [0%, 100&]\n    ax.set_title(name)\n    plt.show() #plotting","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"715fee4c-67ff-440a-b513-020eac13d7f3","_uuid":"606c2cf3905ab7ee0703b18eaf34d122a6db2bb6"},"cell_type":"markdown","source":"**9. The job: **Going for the real data with highest rated classifier"},{"metadata":{"_cell_guid":"2ad66928-5be2-49ea-a429-6bec70be4c32","_uuid":"a69f8b752f361234fc954db8bb5eae3e2967d775","trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\ntrain = pd.read_csv(\"../input/train.csv\", index_col=\"PassengerId\")\ntest = pd.read_csv(\"../input/test.csv\", index_col=\"PassengerId\")\ndataset = train.append(test)\ndataset[\"Family\"] = dataset[\"Name\"].str.split(',').str.get(0) # extract family name\nlstTextCategories = [\"Sex\", \"Ticket\", \"Cabin\", \"Embarked\", \"Family\"] # list of categorial variables\ndataset.describe()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6047ea34-c07f-4ff3-a0ad-a78ddef2dacc","_uuid":"47b3aeb3707515792aee2177aa6b2af35347e76a","trusted":true},"cell_type":"code","source":"# Creating a new feature: If a family member survived: 1, else: 0\ndataset[\"Family\"] = dataset[\"Name\"].str.split(',').str.get(0) # extract family name\ndataset[\"FamSurv\"] = 0\nfor index, row in dataset.iterrows():\n    for index2, row2 in dataset.iterrows():\n        if not index2 == index:\n            if row[\"Family\"] == row2[\"Family\"] and (row[\"Survived\"] == 1 or row2[\"Survived\"] == 1):\n                dataset.loc[index, \"FamSurv\"] = 1\n\nlstTextCategories = [\"Sex\", \"Ticket\", \"Cabin\", \"Embarked\", \"Family\"] # list of categorial variables","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"141341065ada6ae73885b210aad05f19db641e3a","trusted":true},"cell_type":"code","source":"# recode cabin: 1 if there is an entry, 0 else\ndataset[\"Cabin\"] = dataset[\"Cabin\"].where(dataset[\"Cabin\"].isnull(), 1).fillna(0)\ndataset.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5e0475e9-14f1-41dc-8574-11f65531b09c","_uuid":"5ab3da334629a5e150a65162ce2d0cb5139a5026","trusted":true},"cell_type":"code","source":"for cat in lstTextCategories:\n    dataset[cat] = pd.Categorical(dataset[cat])\n    dataset[cat] = dataset[cat].cat.codes\n\ndataset['ageClass'] = dataset['Age']/15\ndataset['ageClass'] = dataset['ageClass'].apply(np.floor)\n    \nstrTarget = \"Survived\" # defining target variable\nlstExclude = [strTarget, \"Name\", \"Ticket\", \"Family\", \"Age\"] # variables to be excluded from the feature matrix\nlstCategorials = [\"Sex\", \"Pclass\", \"Cabin\", \"SibSp\", \"Parch\"] # categorial variables\nlstInclude = [column for column in dataset.columns if column not in lstExclude] # variables to be included\n\ny = dataset[strTarget]\nX = dataset.loc[:, lstInclude]\n\nX = X.fillna(X.mean())\nX = pd.get_dummies(X, drop_first=True, columns=lstCategorials)\nX = pd.get_dummies(X, drop_first=True)\nheads = X.columns\nX = pd.DataFrame(StandardScaler().fit_transform(X), columns=heads)\n\nX_train, X_test, y_train = X.loc[:y.notnull().sum()-1,:], X.loc[y.notnull().sum():,:], y[y.notnull()]\n\nprint(len(X_train), len(y_train))\n\nbestModel = dicModels[results.index[0]]\nif \"param_grid\" in bestModel:\n    clf = GridSearchCV(estimator=bestModel[\"classifier\"], \n                       param_grid=bestModel[\"param_grid\"], \n                       cv=5)\n    clf.fit(X_train, y_train)\n    bestModel[\"classifier\"] = clf.best_estimator_\n    \nbestModel[\"classifier\"].fit(X_train, y_train)\ny_pred = pd.DataFrame(bestModel[\"classifier\"].predict(X_test), \n                      index=test.index.values, \n                      columns=[\"Survived\"]).astype(int)\ny_pred.index.names=[\"PassengerId\"]\ny_pred.to_csv(\"submission14.csv\")\nprint(results.index[0], bestModel[\"classifier\"].get_params())\ny_pred.head(10)","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}