{"cells":[{"metadata":{"_uuid":"b399a570c7e5137a69d375f16c9607bce0b8aa08"},"cell_type":"markdown","source":"## The Titanic Dataset: Practice with IPython Widgets.\n\nThis kernel was originally written to practice using IPython Widgets. I find the interactive features of widgets to be an invaluable teaching tool, and a convenient way to visualize data when you want to change inputs on the fly (presentations with co-workers, informal meetings with professors, etc.). If you are new to IPython widgets like I am, I hope this provides some practical examples of how they can be implemented.\n\nI appreciate any feedback, and I hope this will be useful for somebody!"},{"metadata":{"trusted":true,"_uuid":"2a471ae7a695468c070e545c35bfda1fda5a61f1"},"cell_type":"code","source":"###################################\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# library imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn as learn\nfrom sklearn import preprocessing\nfrom sklearn import metrics\nimport ipywidgets as widgets\nfrom ipywidgets import interact, interactive, fixed\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import decomposition\nfrom sklearn import tree\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import tree\nimport os\n\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ff1506eb5e07eee71699cff751114190949c5c05"},"cell_type":"markdown","source":"### Data Exploration: What are the basics?\n\nHere, we just define a data importing and pre-processing function."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"397ea932866c85c6d134bd290ed92a280c218c76"},"cell_type":"code","source":"def preProcessing():\n    # the training data...\n    train_orig = pd.read_csv('../input/train.csv')\n    # the test data...\n    test_orig = pd.read_csv('../input/test.csv')\n\n    train2 = train_orig.copy()\n    \n    # here, we do some mapping to make the features more readable...\n    train2['Survived_Label'] = train2['Survived'].map({0:'No',1:'Yes'})\n    train2['Sex'] = train2['Sex'].map({'male':0,'female':1})\n    train2['Embarked'] = train2['Embarked'].map({'C':'Cherbourg',\n                                                 'Q':'Queenstown',\n                                                 'S':'Southhampton'})\n    train2['Embarked_Number'] = train2['Embarked'].map({'Cherbourg':1,'Queenstown':2,'Southhampton':3})\n    train2 = train2.drop(['Survived', 'Cabin', 'Ticket', 'PassengerId'], axis=1);\n    \n    # fill in missing age data with the median... \n    train2['Age']=train2['Age'].fillna(train2['Age'].median())\n    # fill in missing embarked number with the most common (Southhampton)...\n    train2['Embarked_Number']=train2['Embarked_Number'].fillna(3)\n    return train2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ed609bc5f1bc047aabb036aae57e1f346685dd8"},"cell_type":"code","source":"train2 = preProcessing()\ntrain2.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8c2d9598440e4004efc65db1ccaef3ed72e715e7"},"cell_type":"markdown","source":"And let's see the head of the data as a sanity check..."},{"metadata":{"trusted":true,"_uuid":"85d2c5dc5e67bfe8f03a69b6e121e7915e2abaab"},"cell_type":"code","source":"train2.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"50a1c688972e84cf5cbd521c5e7a45e1f605a671"},"cell_type":"markdown","source":"Let's also see the counts of passengers that survived and didn't survive, to get a better understanding of the target variable..."},{"metadata":{"trusted":true,"_uuid":"8a310d05cefd7f62e415a141498c276726c1e078"},"cell_type":"code","source":"# now, let's make a \"count plot.\"\n# this will count the different types of data you give it.\n# in this case, we will just give it the \"Species\" column of our data set.\n\nsns.countplot(train2['Survived_Label'], palette = 'muted');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a97a5445af24a068a022aeb2042ce0c826f049cf"},"cell_type":"markdown","source":"### Data visualization: let's put interactive pictures to what we are doing.\n\nFirst, we will look at a \"violin plot,\" which is good for seeing how features are distributed for each target category. This is the first interactive plot for this notebook. \n\nIn this case, we simply define four \"interactive\" variables (features, target, inner_style, and colors), all of which are simple selections the user can make from a drop-down list. The selected feature will be the input for the violin plot.\n\nNote that you can change the inputs of the plots using the drop-down menus to re-draw the plot as desired."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"34d3ce5eacc4c05d184ff932ea96989a1c54eaa0"},"cell_type":"code","source":"####################################\n# violin plot function\n@interactive # this line declares the following function as an \"interactive\" object\ndef interactiveViolinPlot(feature=['Age', 'Fare', 'Sex', 'SibSp', 'Parch','Pclass','Embarked_Number'], \n                          target = ['Survived_Label','Embarked'],\n                          inner_style = ['quartiles','box','stick'],\n                          colors=['muted','bright','colorblind']):\n    sns.set_style(\"whitegrid\") # this just gives us gridlines\n    sns.violinplot(x=target, # this is the \"category\" we want to make plots for\n                   y=feature, # this is the data we want to plot for each category\n                   data=train2, # and this is the pandas dataframe we are using\n                   inner=inner_style,\n                   palette=colors) # the color scheme is given by whatever selection the user gives\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cdabc29059cabee2bf98c2bb1d7b899290480c60"},"cell_type":"code","source":"# let's take a look at some of the data we have...\ninteractiveViolinPlot","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8a8cb7168dcac1bdb263ed99eba345fdbe9805a0"},"cell_type":"markdown","source":"### Scatter Plot\n\nHere's the next interactive plot for this notebook. The motivation here was to visualize individual scatter plots as desired. "},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c10ed04f41f452b48ea8bf7af535fb09ac24dc30"},"cell_type":"code","source":"#####################################\n# scatter plot to better understand individual features\n@interactive\ndef interactiveScatterPlot(feature1=['Age', 'Fare', 'Sex', 'SibSp', 'Parch','Pclass','Embarked_Number'],\n                           feature2=['Age', 'Fare', 'Sex', 'SibSp', 'Parch','Pclass','Embarked_Number'],\n                           target = ['Survived_Label','Embarked'],\n                           colors=['muted','bright','colorblind']):\n    sns.lmplot(x=feature1, \n               y=feature2, \n               data=train2, \n               fit_reg=False, \n               hue=target,\n               palette=colors,\n               scatter_kws={'alpha':0.4})\n    plt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8ffb2d32f3b6ad5c0b46ce7361af23b47c5395e7"},"cell_type":"code","source":"interactiveScatterPlot","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"619832ea2d0e6db513519b2be8a8b8185993ef57"},"cell_type":"markdown","source":"### A Pair Plot to Put Everything Together\n\nA pair plot is just a collection of scatter plots, allowing us to compare two or more features and how they vary across categories. \n\nRather than the default drop-down selection widgets, here we use the \"SelectMultiple\" widget. This takes as \"options\" a list, for which a user can select any subset from a list. The \"value\" denotes the default values when the function is initially run.\n\nControl+click or drag through features in the list to add more plots to the pair plot collection. "},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"74e66d1259d3528dfb85404f0d2de1c71262caba"},"cell_type":"code","source":"######################################\n# select multiple example\n@interactive\ndef interactivePairPlot(features=widgets.SelectMultiple(description=\"features\",\n                                                        options=['Age', 'Fare', 'Sex', 'SibSp', 'Parch','Pclass','Embarked_Number'], # all the options\n                                                        value=('Age','Fare',)), # the initial options selected by the widget\n                        target = ['Survived_Label','Embarked'], # this is just a selection as in the previous examples...\n                        colors = ['muted','bright','colorblind']): # ... and this too\n    try:\n        f = []\n        for i in features:\n            f.append(i)\n        included = [target]+f\n        sns.pairplot(train2[included], \n                     hue = target, # the \"hue\" colors the data by the tag you give (in this case, 'Survived_Label' or 'Embarked')\n                     palette = colors, \n                     diag_kind='kde');\n        plt.show()\n    except TypeError:\n        pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"369853cd792d38ff2668332c807f05cb8710260e"},"cell_type":"code","source":"interactivePairPlot","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"52ef940a1f18cc68e52ca268848fd78142d4e67b"},"cell_type":"markdown","source":"## Adding More Features: Feature Engineering\n\nHere, I was interested in visualizing features created as a function of existing features, along with its components to see if the new feature indeed seems to capture more information about the target categories. I don't think any of the proposed features here are of much use in helping with the classification, but in another context (or with more sophisiticated feature engineeirng!) such a visualization may be more useful.\n\nThere is also an option to plot the explained variance ratio of the training data's principle components if desired; note this has 8 principle component scores as there are 7 features plus the \"engineered\" feature. This remains relatively constant regardless of the input, which should be expected if the \"engineered\" feature carries little information. In fact, the 8th PC has a ratio equal to 0 if the \"average\" operation is applied, as this carries no additional information if you already have the component features in the dataset."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7556ec0ba59d185e126257155d150f9463b04b62"},"cell_type":"code","source":"#############################################\n# basic feature engineering\n@interactive\ndef engineerFeatures(operation=['multiply','average'],\n                     feature1=['Age', 'Fare', 'Sex', 'SibSp', 'Parch','Pclass','Embarked_Number'],\n                     feature2=['Age', 'Fare', 'Sex', 'SibSp', 'Parch','Pclass','Embarked_Number'],\n                     target = ['Survived_Label','Embarked'],\n                     show = ['pairplot','PCA'],\n                     colors = ['muted','bright','colorblind']):\n    \n    if feature1 != feature2:\n        training_features = ['Age', 'Fare', 'Sex', 'SibSp', 'Parch','Pclass','Embarked_Number']\n        scale = preprocessing.StandardScaler()\n        train_scaled=scale.fit(train2[training_features]) # scaling the features to make sure they are equally weighted\n        train_engr = train_scaled.transform(train2[training_features])\n        train_engr = pd.DataFrame(data=train_engr, columns = training_features)\n        \n        # here, specify the details for each operation you provide in the \"operation\" list...\n        if operation == 'multiply':\n            train_engr['engineered_feature'] = train_engr.apply(lambda row: row[feature1]*row[feature2],axis=1)\n        elif operation == 'average':\n            train_engr['engineered_feature'] = train_engr.apply(lambda row: (row[feature1]+row[feature2])/2.,axis=1)\n        \n        # and visualize...\n        included = [target]+['engineered_feature']+[feature1,feature2]\n        for i in included:\n            if i not in training_features:\n                if i != 'engineered_feature':\n                    train_engr[i] = train2[i]\n        if show == 'pairplot':\n            sns.pairplot(train_engr[included], \n                         hue = target, # the \"hue\" colors the data by the tag you give\n                         palette = colors, \n                         diag_kind='kde',\n                         plot_kws={'alpha': 0.4});\n        elif show == 'PCA':\n            train_temp = train2.copy()\n            for c in train_temp.columns:\n                if c not in training_features:\n                    train_temp = train_temp.drop([c],axis=1)\n            train_temp['engineered_feature'] = train_engr['engineered_feature']\n            train_scaled=scale.fit(train_temp)\n            train_temp = train_scaled.transform(train_temp)\n            train_temp = pd.DataFrame(data=train_temp)\n            pca = decomposition.PCA()\n            pca_fit = pca.fit(train_temp)\n            # print(pca_fit.explained_variance_ratio_)\n            sns.barplot(x=np.arange(len(pca_fit.explained_variance_ratio_)),\n                        y=pca_fit.explained_variance_ratio_,\n                        palette='muted');\n            plt.show()\n    else:\n        pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4623ac80cf9dd9d225b9b1b52a3855be64a78639"},"cell_type":"code","source":"engineerFeatures","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dab4b1d5b8ae9f2987307798b408b0785d96b970"},"cell_type":"markdown","source":"## Finally, Let's build a classifier\n\nHere, we have the last two interative functions for this notebook: A decision tree classifier where you can specify the input features, and a decision forest classifier where you can specify features as well as the number of trees and maximum tree depth for the forest.\n\nIn both cases, we will use 4-fold cross-validation to evaluate the model, and we will plot confusion matrices for each fold. The code for plotting the confusion matrices is not mine, and may be found here:\nhttp://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c6adfb4ab4c3658886c2a051ca5003951ca3ab44"},"cell_type":"code","source":"##########################################\n# confusion matrix code from github\n'''\nNote: the source for this function code may be found here:\nhttp://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n'''\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    import itertools\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n#         print(\"Normalized confusion matrix\")\n    else:\n        cm = cm\n#         print('Confusion matrix, without normalization')\n\n#     print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    return\n\n#########################################\n## decision tree\n@interactive\ndef getDecisionTreeModel(features = widgets.SelectMultiple(description='features',\n                                                           options=['Age', 'Fare', 'Sex', 'SibSp', 'Parch','Pclass','Embarked_Number'],\n                                                           value=('Age',)),\n                         target = ['Survived_Label']):\n    \n    try:\n        training_features = []\n        if 'value' not in dir(features):\n            for i in features:\n                training_features.append(i)\n        else:\n            training_features = list(features)\n        print(training_features, target)\n        print(train2['Embarked'].value_counts())\n        kf = KFold(4, shuffle = True) # we will split the data 4 times, and test to see how well the classification performs each time.\n        clf = tree.DecisionTreeClassifier(min_samples_leaf=1)\n\n        for k, (training_data, testing_data) in enumerate(kf.split(train2[training_features], train2[target])):\n\n            # train the model using the training data for this \"fold\"...\n            clf.fit(train2.iloc[training_data][training_features], # the feature data\n                    train2.iloc[training_data][target]) # the target data\n\n            # get the prediction...\n            predicted = clf.predict(train2.iloc[testing_data][training_features])\n            # and get the actual values\n            actual = train2.iloc[testing_data][target]\n            # get the confusion matrix\n            cm = confusion_matrix(actual, predicted)\n            plt.figure()\n            print('trial',k)\n            print(metrics.classification_report(actual, predicted))\n            print()\n            plot_confusion_matrix(cm, classes=train2[target].unique())\n    except TypeError:\n        pass\n\n\n#####################################\n# random forest\n# trees = [1,2,3,4,5,6,7,8,9,10,20,30,40,50,100, 150, 200, 250, 300, 500],\n# tree_depth = [1,2,3,4,5,6,7,8,9,10,15,20,30,40,50, 60, 70, 80]\n\n@interactive\ndef getDecisionForestModel(features = widgets.SelectMultiple(description='features',\n                                                           options=['Age', 'Fare', 'Sex', 'SibSp', 'Parch','Pclass','Embarked_Number'],\n                                                           value=('Age',)),\n                           target = ['Survived_Label'],\n                           trees = widgets.IntSlider(value=1,min=1,max=500,step=2,description='Trees'), # a slider for integer inputs\n                           tree_depth = widgets.IntSlider(value=1,min=1,max=80,step=2,description='Tree Depth')):\n    \n    try:\n        training_features = []\n        if 'value' not in dir(features):\n            for i in features:\n                training_features.append(i)\n        else:\n            training_features = list(features)\n        print(training_features)\n        print(train2['Embarked'].value_counts())\n        kf = KFold(4, shuffle = True) # we will split the data 4 times, and test to see how well the classification performs each time.\n        rfc = RandomForestClassifier(max_depth=tree_depth, n_estimators=trees,\n                                     min_impurity_decrease = 0.0)\n\n        for k, (training_data, testing_data) in enumerate(kf.split(train2[training_features], train2[target])):\n\n            # train the model using the training data for this \"fold\"...\n            rfc.fit(train2.iloc[training_data][training_features], # the feature data\n                    train2.iloc[training_data][target]) # the target data\n\n            # get the prediction...\n            predicted = rfc.predict(train2.iloc[testing_data][training_features])\n            # and get the actual values\n            actual = train2.iloc[testing_data][target]\n            # get the confusion matrix\n            cm = confusion_matrix(actual, predicted)\n            plt.figure()\n            print('trial',k)\n            print(metrics.classification_report(actual, predicted))\n            print()\n            plot_confusion_matrix(cm, classes=train2[target].unique())\n    except TypeError:\n        pass","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"d4d7cb9a6c7098c99767f0f9ed34991e8f40e95e"},"cell_type":"code","source":"getDecisionTreeModel","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"d4c4b6a2d0deb9f4ca71094dc012438a275d8d6d"},"cell_type":"code","source":"getDecisionForestModel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"6eab912dd2be45b4583ef55b8c19d51263218ae1"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}