{"cells":[{"metadata":{"_uuid":"b1a323e093841b725557a56d7eb7c3627a825636"},"cell_type":"markdown","source":"> ## Load Libraries & Data"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Load Libraries\nimport pandas as pd\nimport numpy as np\nimport re\nimport sklearn\nimport xgboost as xgb\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Machine learning Libraries\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nprint(\"Libraries Loaded\")","execution_count":1,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Load Titanic Data (Training & Test Data)\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\ntrain.head(5)","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"0a913a1f80ee9c06c833db1411d10b1b1f7136c0"},"cell_type":"markdown","source":"## Missing Value, Data Types & Summary Stats"},{"metadata":{"trusted":true,"_uuid":"258bfb3748a44bfe6f223c3a286798b425b50aa7"},"cell_type":"code","source":"#Look at Data Types & Missing Values (Training & Test Data)\nprint(\"------------- TRAINING DATA -------------\")\ntrain.info()\nprint(\"\\n------------- TEST DATA -------------\")\ntest.info()","execution_count":3,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a8d724d6559230a487f5f33b7033405c57ba1ed0"},"cell_type":"code","source":"# Missing Value Count & Percentage (Training Data)\ndf1 = (len(train.index)-train.count()).to_frame()\ndf2 = (100*(len(train.index)-train.count())/len(train.index)).to_frame()\ndf2.columns = ['% Missing']\ndf2['N Missing'] = df1\ndf2","execution_count":4,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a987cd41472e8a43a868d7b587a13692b370baed"},"cell_type":"code","source":"# Missing Value Count & Percentage (Test Data)\ndf1 = (len(test.index)-test.count()).to_frame()\ndf2 = (100*(len(test.index)-test.count())/len(test.index)).to_frame()\ndf2.columns = ['% Missing']\ndf2['N Missing'] = df1\ndf2","execution_count":5,"outputs":[]},{"metadata":{"_uuid":"daabf6ebd0368d0e375d090193d021726c8a763a"},"cell_type":"markdown","source":"**NOTES:**\n* **Fare -** There is a single value of \"Fare\" missing in test data set - versus no missing values for \"Fare\" in training.\n\n* **Embarked -** 2 missing values in training set.\n\n* **Age & Cabin -** Many missing values in both training & test data set."},{"metadata":{"trusted":true,"_uuid":"b1c903ce89b766e0e3371f21ac5d779f0a9a7fb9"},"cell_type":"code","source":"# Look at Summary Stats for Numerical Data Types (Training Data)\ntrain.describe()","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"7d84dcbf9bb8d9d8111743e81df292b176407306"},"cell_type":"markdown","source":"**NOTES:**\n* **Survival Base Rate -** Average rate of survival is 38.4% in training set. If we were to predict everyone dies in the test set, then should get approx. 62% accuracy.\n\n* **Age -** There are fractional ages, e.g. 0.42."},{"metadata":{"trusted":true,"_uuid":"a1c3d3eff53f01c3cee330bf5ada6452e71af6d0"},"cell_type":"code","source":"#Look at Summary Stats for Categorical Data Types (Training Data)\ntrain.describe(include=['O'])","execution_count":7,"outputs":[]},{"metadata":{"_uuid":"77df92359526790c77b3420e73fb3481bc3889dc"},"cell_type":"markdown","source":"> ## Exploratory Data Analysis"},{"metadata":{"trusted":true,"_uuid":"b54bd70de31b01e344779d48c1b1c9c6c58bd154"},"cell_type":"code","source":"#Look for Simple Trends in Survival Rates\n\ntrain[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).agg(['mean', 'count'])","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"743743c23d9fe0fbd965439b4d3f55616707ee1b"},"cell_type":"markdown","source":"**NOTES:**\n* **Gender Bias -** It appears females much more likely to survive. Many more males in training set."},{"metadata":{"trusted":true,"_uuid":"a46d5df8b4cfc944594fef6b9954deea569907c6"},"cell_type":"code","source":"train[[\"Pclass\", \"Survived\"]].groupby(['Pclass'], as_index=False).agg(['mean', 'count'])","execution_count":9,"outputs":[]},{"metadata":{"_uuid":"0decd91b612ce37399cb1f71e76ccb400dd7816e"},"cell_type":"markdown","source":"**NOTES:**\n* **Class Bias -** It appears survival rate increases with \"higher\" (1=high, 3=Low) Pclass."},{"metadata":{"trusted":true,"_uuid":"c3c8f47294675c1d3feca7386b01ba9dfaef8bec"},"cell_type":"code","source":"train[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).agg(['mean', 'count'])","execution_count":10,"outputs":[]},{"metadata":{"_uuid":"2edce37d56ef9204f5b256d9af63b2637f02c789"},"cell_type":"markdown","source":"**NOTES:**\n* **Family Relationship (Peer Grouping) Bias -** Having values of 1 or 2 for SibSp (i.e. having siblings and/or spouses on board) may increase likelihood of survival."},{"metadata":{"trusted":true,"_uuid":"78b775828ce6508b219037f36c69272ec98740c3"},"cell_type":"code","source":"train[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).agg(['mean', 'count'])","execution_count":11,"outputs":[]},{"metadata":{"_uuid":"7957ca9f4726ee0318e576089ef2f357cdd8bd9e"},"cell_type":"markdown","source":"**NOTES:**\n* **Family Relationship (Vertical Grouping) Bias -** Having Parch values of greater than 0 (i.e. having kids and/or parents on board) may increase liklihood of survival."},{"metadata":{"trusted":true,"_uuid":"11cd750685e1d8f18387748c94f4f5249fa84031"},"cell_type":"code","source":"train[[\"Embarked\", \"Survived\"]].groupby(['Embarked'], as_index=False).agg(['mean', 'count'])","execution_count":12,"outputs":[]},{"metadata":{"_uuid":"48a0cde7e5cff3bd5b22816618489e20f9213f5d"},"cell_type":"markdown","source":"**NOTES:**\n* **Emabarking Bias -** An embarked value of \"C\" may be correlated with higher survival rates, perhaps more Pclass=1 passengers embarked here."},{"metadata":{"trusted":true,"_uuid":"2e7deb9631d53f03c2d852bac7f7fffc550bdc50"},"cell_type":"code","source":"# Age Cohort Analysis, generate age groupings\n\ntrain['AgeCohort'] = train.groupby(level=0)['Age'].min().apply(lambda x: np.floor(x/10).astype(int))\ntest['AgeCohort'] = test.groupby(level=0)['Age'].min().apply(lambda x: np.floor(x/10).astype(int))\ntrain.head()","execution_count":14,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5687e18aefb65378ee642948a1bee7d8f5384757"},"cell_type":"code","source":"train[[\"AgeCohort\", \"Survived\"]].groupby(['AgeCohort'], as_index=False).agg(['mean', 'count'])","execution_count":15,"outputs":[]},{"metadata":{"_uuid":"381388fd77eaf7f98bd8fe28f0d7152dada1e999"},"cell_type":"markdown","source":"**NOTES:**\n* **Age Cohort Bias -** It appears young children may have higher survival rates. It appears that NAN values for age have about average survival rates.**"},{"metadata":{"trusted":true,"_uuid":"49cff9c087a3ec2c24c12e2e7fc2e1dbb8f94f72"},"cell_type":"code","source":"# Look at Age Distributions for Survived vs. Not Survived\nfacet = sns.FacetGrid(train, hue=\"Survived\",aspect=4)\nfacet.map(sns.kdeplot,'Age',shade= True)\nfacet.set(xlim=(0, train['Age'].max()))\nfacet.add_legend();","execution_count":16,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8e2d491767d9fbe346b4ca11a4cc48216fa9e2cd"},"cell_type":"code","source":"#Look at Class v Age v Survivals\nsns.set(style=\"ticks\")\nsns.boxplot(x=\"Pclass\", y=\"Age\", hue=\"Survived\", data=train, palette=\"PRGn\")\nsns.despine(offset=10, trim=True)","execution_count":17,"outputs":[]},{"metadata":{"_uuid":"5c4882d137d8d6444009aca42c65738efd4d65d3"},"cell_type":"markdown","source":"**NOTES:**\n* **Age  Trend -** Appers within each class, those who perished were younger than those who survived."},{"metadata":{"trusted":true,"_uuid":"ec343c202e49fa4590fda752aaaf87da26700f56"},"cell_type":"code","source":"train[[\"Pclass\",\"Sex\", \"Survived\"]].groupby(['Pclass','Sex'], as_index=False).agg(['mean', 'count'])","execution_count":18,"outputs":[]},{"metadata":{"_uuid":"d219ec21367de2cfa7fd6ca7512145f55c2498b6"},"cell_type":"markdown","source":"**NOTES:**\n* ** Gender Bias Transcends Pclass -** Females had better chance of survival regardless of Pclass."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"424bfd00f8a84c30109f9b39a9cc08e5c83c4691"},"cell_type":"markdown","source":"# Quick & Dirty Benchmark Models"},{"metadata":{"_uuid":"510c133fe9d40ac53183e57e9a345db9a5b012a6"},"cell_type":"markdown","source":"### Data Preperation\nPrepare data for machine learning models. Steps generally include:\n\n* Map categorical variables to numerical, indcator/dummy variables\n\n* Remove NaNs, either by imputing values or dropping columns containing NaNs\n\nIn there interest of getting a benchmark as quick as possible (minimum viable model) we agressively drop columns..."},{"metadata":{"trusted":true,"_uuid":"1bca32250094937552d8bf560ee7f46432d710ea","collapsed":true},"cell_type":"code","source":"# Map Sex to 1/0\ntrain['Sex'] = train['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\ntest['Sex'] = test['Sex'].map( {'female': 1, 'male': 0} ).astype(int)","execution_count":19,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a7b3385fa31583eebbd0bbefe43158f2c1969466","collapsed":true},"cell_type":"code","source":"# Drop Columns that have any NaN values\ntrain_Pre = train.drop(['PassengerId','Name','Age','Cabin','Embarked','Ticket','Cabin','AgeCohort'], axis=1)\ntest_Pre = test.drop(['PassengerId','Name','Age','Cabin','Embarked','Ticket','Cabin','AgeCohort'], axis=1)","execution_count":20,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"bad1e9b79e98cb45267df48affc9bcb5c24da1cc"},"cell_type":"code","source":"#Fix that one missing Fare value in the test set\ntest_Pre[\"Fare\"].fillna(test[\"Fare\"].median(), inplace=True)","execution_count":21,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1c0e7e31eebf47e49d14c2e29df09302b6259db4"},"cell_type":"code","source":"#Check resulting data (Training Data)\ntrain_Pre.head()","execution_count":22,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"307a8977daedfe3ecf4be615c831b9cb3c6e8f8c"},"cell_type":"code","source":"#Check resulting data (Test Data)\ntest_Pre.head()","execution_count":23,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"95bc582257b655770c4ea33100b385c51c57fc88","collapsed":true},"cell_type":"code","source":"#Prepare Traning Data into X&Y sets\nX_train = train_Pre.drop(\"Survived\", axis=1)\nY_train = train_Pre[\"Survived\"]\n\nX_test = test_Pre","execution_count":24,"outputs":[]},{"metadata":{"_uuid":"90f6efdb05fbdab4b7d9086b694946d7a7f6340b"},"cell_type":"markdown","source":"### Logistic Regression Model"},{"metadata":{"trusted":true,"_uuid":"306f410ef117b930f62cae82cb2a4f4fa42ed385"},"cell_type":"code","source":"#Initial Logistic Regression Model\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict(X_train)\nlogFullTrain = logreg.score(X_train, Y_train)\nlogFullTrain #output accuracy","execution_count":25,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"16f0900d3bd13c61917ed6ad858a3a43353fd5e1"},"cell_type":"code","source":"#Look at factors\ncoeff_df = pd.DataFrame(X_train.columns)\ncoeff_df.columns = ['Feature']\ncoeff_df[\"Correlation\"] = pd.Series(logreg.coef_[0])\ncoeff_df.sort_values(by='Correlation', ascending=False)","execution_count":26,"outputs":[]},{"metadata":{"_uuid":"225464a6778e0d876fe61e00f1760bada261a987"},"cell_type":"markdown","source":"**NOTES:**\n* **Sex  Factor -** It appears Sex is strongly correlated with survival as suspected from initial EDA. \n\n* **Pclass Factor-** According to this (crude) model, Pclass is negatively correlated (\"lower\", e.g. Pclass=3, decreases chance of survival)."},{"metadata":{"_uuid":"b36451dcf96a691e5b6c409519d0fed071495fa7"},"cell_type":"markdown","source":"> #### K-Folds Cross Validation"},{"metadata":{"trusted":true,"_uuid":"e23fee000390c95a4f2496ff03f1ed88df13a038"},"cell_type":"code","source":"#Look at crossfold accuracy\nfrom sklearn import model_selection\nfrom sklearn.model_selection import cross_val_score\n\nkfold = model_selection.StratifiedKFold(n_splits=5, random_state=7)\nmodelCV = LogisticRegression()\nresults = model_selection.cross_val_score(modelCV, X_train, Y_train, cv=kfold, scoring='accuracy')\nlogit_fold_accuracy = results.mean()\nprint(\"5-fold cross validation average accuracy: %.3f\" % (logit_fold_accuracy))","execution_count":27,"outputs":[]},{"metadata":{"_uuid":"cf5078e8207e1194d7d4592b1c84d3a511b1763a"},"cell_type":"markdown","source":"**NOTES:**\n* **Cross Fold Accuracy -** Accuracy about the same as initial training on full data."},{"metadata":{"_uuid":"420b411ecbefe13614e3fcfa3dab4a874dc2451d"},"cell_type":"markdown","source":"#### ROC Curve Analysis"},{"metadata":{"trusted":true,"_uuid":"c4b100a87d047f2a445fe71a4908049f82f8e4e4"},"cell_type":"code","source":"#Generate ROC Curve & AUC Metric for K Folds\n\nfrom scipy import interp\nimport matplotlib.pyplot as plt\nfrom itertools import cycle\n\nfrom sklearn import datasets\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.model_selection import StratifiedKFold\n\nX = X_train\ny = Y_train\n\ncv = model_selection.StratifiedKFold(n_splits=5)\nclassifier = LogisticRegression()\n\ntprs = []\naucs = []\nmean_fpr = np.linspace(0, 1, 100)\ni = 0\n\nplt.figure(figsize=(12,8))\n\n#Calculate ROC Curve/AUC for Each Split\nfor traini, testi in cv.split(X, y):\n    \n    probas_ = classifier.fit(X.loc[traini], y.loc[traini]).predict_proba(X.loc[testi])\n    \n    # Compute ROC curve and area the curve\n    fpr, tpr, thresholds = roc_curve(y.loc[testi], probas_[:, 1])\n    \n    tprs.append(interp(mean_fpr, fpr, tpr))\n    tprs[-1][0] = 0.0\n    roc_auc = auc(fpr, tpr)\n    aucs.append(roc_auc)\n    plt.plot(fpr, tpr, lw=1, alpha=0.3,label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n\n    i += 1\n\n#Generate ROC Curve Plot Details\nplt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',label='Luck', alpha=.8)\n\nmean_tpr = np.mean(tprs, axis=0)\nmean_tpr[-1] = 1.0\nmean_auc = auc(mean_fpr, mean_tpr)\nstd_auc = np.std(aucs)\nplt.plot(mean_fpr, mean_tpr, color='b',label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),lw=2, alpha=.8)\n\nstd_tpr = np.std(tprs, axis=0)\ntprs_upper = np.minimum(mean_tpr + std_tpr, 1)\ntprs_lower = np.maximum(mean_tpr - std_tpr, 0)\nplt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,label=r'$\\pm$ 1 std. dev.')\n\nplt.xlim([-0.05, 1.05])\nplt.ylim([-0.05, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve, Logistic Regression')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":28,"outputs":[]},{"metadata":{"_uuid":"278845fd9921397d2f2cc32988f3eba8a6cce296"},"cell_type":"markdown","source":"#### Classification Report"},{"metadata":{"trusted":true,"_uuid":"ea6589bc9f031f9b69299512c6fcd65b257007d1"},"cell_type":"code","source":"#Classification Report\nfrom sklearn.metrics import classification_report\nprint(classification_report(Y_train, Y_pred))","execution_count":29,"outputs":[]},{"metadata":{"_uuid":"decda34b87c40c4eb7f59416f83efccb10f383c1"},"cell_type":"markdown","source":"#### Model Probability Score Calibration"},{"metadata":{"_uuid":"53c39ee9506a2b91318f9b8325c57bcc2ff2b0ed"},"cell_type":"markdown","source":"Logistic regression outputs a \"score\" which can be interpreted as a probability of survival. The questions is: is this a good estimate of the actual probability? Is it well \"calibrated\"?"},{"metadata":{"trusted":true,"_uuid":"fb9e013bb55bc803e986ee6c0943154eaaee8350"},"cell_type":"code","source":"# Examine probability calculation calibration for model\n# Estiate probability score, bin based on predicted survival rate, compare against acutal\nfrom sklearn.calibration import CalibratedClassifierCV, calibration_curve\n\nprob_pos = logreg.predict_proba(X_train)[:,1]\nfraction_of_positives, mean_predicted_value = calibration_curve(Y_train, prob_pos, n_bins=10)\n\nfig = plt.figure(0, figsize=(10, 10))\nax = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\nax2 = plt.subplot2grid((3, 1), (2, 0))\n\nax.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\");\nax.plot(mean_predicted_value, fraction_of_positives, \"s-\");\nax.set_ylabel('Actual Portion Survived')\nax.set_title('Calibration Curve, Logistic Regression')\nax2.hist(prob_pos, range=(0, 1), bins=10,histtype=\"step\", lw=2);\nax2.set_xlabel('Predicted Portion to Survive');\nax2.set_ylabel('Count');","execution_count":30,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8dc1c36b8db201fb81a4f1294e76ac9e79548d75"},"cell_type":"code","source":"#Generate a simple linear correlation measure, so we can compare the calibration of other models below\nlogit_cal_corr = np.corrcoef(fraction_of_positives, mean_predicted_value)[0,1]\nlogit_cal_corr","execution_count":31,"outputs":[]},{"metadata":{"_uuid":"ad5bd38f05670293468303731e62074fb4192bbc"},"cell_type":"markdown","source":"**NOTES:**\n* **Model Calibration -** Roughly linear calibration curve, large portion of training cases are predicted to have 10-20% chance of survival."},{"metadata":{"_uuid":"6d92d6261b0960add154ebdfb8db91859678888c"},"cell_type":"markdown","source":"#### Precision-Recall Curve"},{"metadata":{"trusted":true,"_uuid":"4501d90e5da4271f761b0357116cf98d5b0214b3"},"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import average_precision_score\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_scores = logreg.predict_proba(X_train)[:,1]\n\nprecision, recall, _ = precision_recall_curve(Y_train, Y_scores)\naverage_precision = average_precision_score(Y_train, Y_scores)\n\nplt.figure(figsize=(12,8))\nplt.step(recall, precision, color='k', alpha=0.5,where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.2,color='k')\n\nplt.xlabel('Recall');\nplt.ylabel('Precision');\nplt.ylim([0.0, 1.05]);\nplt.xlim([0.0, 1.0]);\nplt.title('Precision-Recall Curve, Logistic Regression: Avg. Precision = {0:0.3f}'.format(average_precision));","execution_count":32,"outputs":[]},{"metadata":{"_uuid":"c2951f03e131ace0b561923f7f2300e550f935f4"},"cell_type":"markdown","source":"#### Visualize Decision Boundary"},{"metadata":{"trusted":true,"_uuid":"ec894478cb27255f06b3651c538fb8db14d4ed05"},"cell_type":"code","source":"trues = Y_scores[Y_train==1]\nfalses = Y_scores[Y_train==0]\n\nplt.figure(figsize=(12,8));\nplt.scatter([i for i in range(len(falses))], falses, s=25, c='r', marker=\"o\", label='Perished');\nplt.scatter([i for i in range(len(trues))], trues, s=25, c='g', marker=\"o\", label='Suvived');\nplt.axhline(.5, color='black');\n\nplt.legend(loc='upper right');\nplt.title(\"Default Decision Boundary (Prob = 0.5)\");\nplt.xlabel('N');\nplt.ylabel('Predicted Probability');","execution_count":33,"outputs":[]},{"metadata":{"_uuid":"bdbde9aecb04313a38c168bf07ba8d3804954f96"},"cell_type":"markdown","source":"**NOTES:**\n* **Decision Boundary Plot -** For the logistic regression model, every point shown above the 0.5 line in the plot above is considered \"survived\", while everything below is not - while the color indicates the actual outcome."},{"metadata":{"trusted":true,"_uuid":"4248b917f3b1e35012efd4fc6ec10f8ae6f72d73"},"cell_type":"code","source":"# Model Summary Data\ndf1 = pd.DataFrame({'Model': ['Logistic Regression'],'Metric':['Mean AUC'], 'Stat_Val':[mean_auc], 'Run':['Initial']})\ndf2 = pd.DataFrame({'Model': ['Logistic Regression'],'Metric':['Calibration Corr'], 'Stat_Val':[logit_cal_corr], 'Run':['Initial']})\nmodel_summary = pd.merge(df1,df2,how='outer')\ndf3 = pd.DataFrame({'Model': ['Logistic Regression'],'Metric':['K-fold Accuracy'], 'Stat_Val':[logit_fold_accuracy], 'Run':['Initial']})\nmodel_summary = pd.merge(model_summary,df3,how='outer')\ndf4 = pd.DataFrame({'Model': ['Logistic Regression'],'Metric':['Full Accuracy'], 'Stat_Val':[logFullTrain], 'Run':['Initial']})\nmodel_summary = pd.merge(model_summary,df4,how='outer')\nmodel_summary","execution_count":34,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5750cde0a0664c4befd85d9a09f37d4adfd1ca26","collapsed":true},"cell_type":"code","source":"# Predict for Test Set & Make First Submission\nY_pred = logreg.predict(X_test)\nsubmission = pd.DataFrame({\"PassengerId\": test[\"PassengerId\"],\"Survived\": Y_pred})\nsubmission.to_csv('Submission_logit.csv', index=False)","execution_count":35,"outputs":[]},{"metadata":{"_uuid":"65d791096970e2e1d70bbbf55f796ea95de53bfd"},"cell_type":"markdown","source":"### Decision Tree Model"},{"metadata":{"trusted":true,"_uuid":"1cd624945983974ebc43ba633a2acbab65e50164"},"cell_type":"code","source":"decision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_pred = decision_tree.predict(X_train)\nacc_decision_tree = decision_tree.score(X_train, Y_train)\nacc_decision_tree","execution_count":36,"outputs":[]},{"metadata":{"_uuid":"5b708d8b6d937f12e4141866b1cbc5579b7da28f"},"cell_type":"markdown","source":"#### K-Folds Cross Validation"},{"metadata":{"trusted":true,"_uuid":"9c792adf54528487a874fa50f16a425d7400d38f"},"cell_type":"code","source":"kfold = model_selection.KFold(n_splits=5, random_state=7)\nmodelCV = DecisionTreeClassifier()\nscoring = 'accuracy'\nresults = model_selection.cross_val_score(modelCV, X_train, Y_train, cv=kfold, scoring=scoring)\ndtree_fold_accuracy = results.mean()\nprint(\"5-fold cross validation average accuracy: %.3f\" % (dtree_fold_accuracy))","execution_count":37,"outputs":[]},{"metadata":{"_uuid":"e0f753383f2b0b5102bc92e01b64a5d50bf23a19"},"cell_type":"markdown","source":"#### Prune Tree"},{"metadata":{"trusted":true,"_uuid":"6f57594b117f43289adab7360c1a9d6c44737b0d"},"cell_type":"code","source":"decision_tree = DecisionTreeClassifier(max_depth=3, random_state=0)\ndecision_tree.fit(X_train, Y_train)\nY_pred = decision_tree.predict(X_train)\nacc_decision_tree = decision_tree.score(X_train, Y_train)\nacc_decision_tree","execution_count":38,"outputs":[]},{"metadata":{"_uuid":"ab1fbafa964af767c60ec9c71ba6b39f76c0d8af"},"cell_type":"markdown","source":"#### Visualize Tree"},{"metadata":{"trusted":true,"_uuid":"287de0c64faef79bc52419f49e031c4f7b51963e"},"cell_type":"code","source":"import graphviz\nfrom sklearn import tree\n\ndot_data = tree.export_graphviz(decision_tree, out_file=None,\n                                feature_names=list(X_train.columns),\n                               filled=True, rounded=True,\n                               special_characters=True)\ngraph = graphviz.Source(dot_data)\ngraph","execution_count":39,"outputs":[]},{"metadata":{"_uuid":"bac60e0860af83204bb36c56f43a4934055700e3"},"cell_type":"markdown","source":"#### ROC Curve Analysis"},{"metadata":{"trusted":true,"_uuid":"fca9100db1b1e72fb4ad17e048361a5e35ec2607"},"cell_type":"code","source":"#Generate ROC Curve & AUC Metric for K Folds\n\nfrom scipy import interp\nimport matplotlib.pyplot as plt\nfrom itertools import cycle\n\nfrom sklearn import datasets\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.model_selection import StratifiedKFold\n\nX = X_train\ny = Y_train\n\ncv = model_selection.StratifiedKFold(n_splits=5)\nclassifier = DecisionTreeClassifier()\n\ntprs = []\naucs = []\nmean_fpr = np.linspace(0, 1, 100)\ni = 0\n\nplt.figure(figsize=(12,8))\n\n#Calculate ROC Curve/AUC for Each Split\nfor traini, testi in cv.split(X, y):\n    \n    probas_ = classifier.fit(X.loc[traini], y.loc[traini]).predict_proba(X.loc[testi])\n    \n    # Compute ROC curve and area the curve\n    fpr, tpr, thresholds = roc_curve(y.loc[testi], probas_[:, 1])\n    \n    tprs.append(interp(mean_fpr, fpr, tpr))\n    tprs[-1][0] = 0.0\n    roc_auc = auc(fpr, tpr)\n    aucs.append(roc_auc)\n    plt.plot(fpr, tpr, lw=1, alpha=0.3,label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n\n    i += 1\n\n#Generate ROC Curve Plot Details\nplt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',label='Luck', alpha=.8)\n\nmean_tpr = np.mean(tprs, axis=0)\nmean_tpr[-1] = 1.0\nmean_auc = auc(mean_fpr, mean_tpr)\nstd_auc = np.std(aucs)\nplt.plot(mean_fpr, mean_tpr, color='b',label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),lw=2, alpha=.8)\n\nstd_tpr = np.std(tprs, axis=0)\ntprs_upper = np.minimum(mean_tpr + std_tpr, 1)\ntprs_lower = np.maximum(mean_tpr - std_tpr, 0)\nplt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,label=r'$\\pm$ 1 std. dev.')\n\nplt.xlim([-0.05, 1.05])\nplt.ylim([-0.05, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve, Decision Tree')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":40,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"806f902150cdabf1d8f1f1da3b9736aa12fc315e"},"cell_type":"code","source":"#Classification Report\nfrom sklearn.metrics import classification_report\nprint(classification_report(Y_train, Y_pred))","execution_count":41,"outputs":[]},{"metadata":{"_uuid":"7e9eb6cd84b9bdf8eda20bccdebc30bdaa5a79fd"},"cell_type":"markdown","source":"#### Calibration Curve"},{"metadata":{"trusted":true,"_uuid":"a6a947e4f2661cae859b18fe5de2160206807560"},"cell_type":"code","source":"# Examine probability calculation calibration for model\n# Estiate probability score, bin based on predicted survival rate, compare against acutal\nfrom sklearn.calibration import CalibratedClassifierCV, calibration_curve\n\nprob_pos = decision_tree.predict_proba(X_train)[:,1]\nfraction_of_positives, mean_predicted_value = calibration_curve(Y_train, prob_pos, n_bins=10)\n\nfig = plt.figure(0, figsize=(10, 10))\nax = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\nax2 = plt.subplot2grid((3, 1), (2, 0))\n\nax.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\");\nax.plot(mean_predicted_value, fraction_of_positives, \"s-\");\nax.set_ylabel('Actual Portion Survived')\nax.set_title('Calibration Curve, Decision Tree')\nax2.hist(prob_pos, range=(0, 1), bins=10,histtype=\"step\", lw=2);\nax2.set_xlabel('Predicted Portion to Survive');\nax2.set_ylabel('Count');","execution_count":42,"outputs":[]},{"metadata":{"_uuid":"e945ff7a40a1fc21cfec438b966d92880516e934"},"cell_type":"markdown","source":"Predicted probability is exact because it's already bucketed in tree."},{"metadata":{"trusted":true,"_uuid":"104998d6d3a7c3de8d3646d17846a4355219f3d3"},"cell_type":"code","source":"#Generate a simple linear correlation measure, so we can compare the calibration of other models below\nlogit_cal_corr = np.corrcoef(fraction_of_positives, mean_predicted_value)[0,1]\nlogit_cal_corr","execution_count":43,"outputs":[]},{"metadata":{"_uuid":"44ea940fab86a6c93c6761ff46bf3827fb3d6856"},"cell_type":"markdown","source":"#### Precision-Recall Curve"},{"metadata":{"trusted":true,"_uuid":"f5969d912f0a30c8a82a47f5e3876de914fdc8b3"},"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import average_precision_score\n\nY_scores = decision_tree.predict_proba(X_train)[:,1]\n\nprecision, recall, _ = precision_recall_curve(Y_train, Y_scores)\naverage_precision = average_precision_score(Y_train, Y_scores)\n\nplt.figure(figsize=(12,8))\nplt.step(recall, precision, color='k', alpha=0.5,where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.2,color='k')\n\nplt.xlabel('Recall');\nplt.ylabel('Precision');\nplt.ylim([0.0, 1.05]);\nplt.xlim([0.0, 1.0]);\nplt.title('Precision-Recall Curve, Logistic Regression: Avg. Precision = {0:0.3f}'.format(average_precision));","execution_count":44,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cbffcf024df5826777c66d0011bb275364f7c979"},"cell_type":"code","source":"trues = Y_scores[Y_train==1]\nfalses = Y_scores[Y_train==0]\n\nplt.figure(figsize=(12,8));\nplt.scatter([i for i in range(len(falses))], falses, s=25, c='r', marker=\"o\", label='Perished');\nplt.scatter([i for i in range(len(trues))], trues, s=25, c='g', marker=\"o\", label='Suvived');\nplt.axhline(.5, color='black');\n\nplt.legend(loc='upper right');\nplt.title(\"Default Decision Boundary (Prob = 0.5)\");\nplt.xlabel('N');\nplt.ylabel('Predicted Probability');","execution_count":45,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3b7038a5c1c99d56adece8cd18499654d3a639e0"},"cell_type":"code","source":"# Model Summary Data\ndf1 = pd.DataFrame({'Model': ['Decision Tree'],'Metric':['Mean AUC'], 'Stat_Val':[mean_auc], 'Run':['Initial']})\nmodel_summary = pd.merge(model_summary,df1,how='outer')\ndf2 = pd.DataFrame({'Model': ['Decision Tree'],'Metric':['Calibration Corr'], 'Stat_Val':[logit_cal_corr], 'Run':['Initial']})\nmodel_summary = pd.merge(model_summary,df2,how='outer')\ndf3 = pd.DataFrame({'Model': ['Decision Tree'],'Metric':['K-fold Accuracy'], 'Stat_Val':[dtree_fold_accuracy], 'Run':['Initial']})\nmodel_summary = pd.merge(model_summary,df3,how='outer')\ndf4 = pd.DataFrame({'Model': ['Decision Tree'],'Metric':['Full Accuracy'], 'Stat_Val':[acc_decision_tree], 'Run':['Initial']})\nmodel_summary = pd.merge(model_summary,df4,how='outer')\nmodel_summary","execution_count":46,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"fa24ca249093a5bc5624d03825710dff13415685"},"cell_type":"code","source":"# Predict for Test Set\nY_pred = decision_tree.predict(X_test)\nsubmission = pd.DataFrame({\"PassengerId\": test[\"PassengerId\"],\"Survived\": Y_pred})\nsubmission.to_csv('Submission_decTree.csv', index=False)","execution_count":50,"outputs":[]},{"metadata":{"_uuid":"e9fe2f3701be8b225ae16c984be3cd5169bf1d7b"},"cell_type":"markdown","source":"###  Random Forest"},{"metadata":{"trusted":true,"_uuid":"e676ec66d2a2816bc6f271df681ec66a824aa140"},"cell_type":"code","source":"random_forest = RandomForestClassifier(n_estimators=10, random_state=7)\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_train)\nacc_random_forest = random_forest.score(X_train, Y_train)\nacc_random_forest","execution_count":51,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f87953eec0c293a2146224f6b63a94ddb167fb38"},"cell_type":"code","source":"kfold = model_selection.KFold(n_splits=5, random_state=7)\nmodelCV = RandomForestClassifier(n_estimators=10)\nscoring = 'accuracy'\nresults = model_selection.cross_val_score(modelCV, X_train, Y_train, cv=kfold, scoring=scoring)\nrForest_fold_accuracy = results.mean()\nprint(\"5-fold cross validation average accuracy: %.3f\" % (rForest_fold_accuracy))","execution_count":52,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d82f8904d9216f24433f5531c6ea665608a297cd"},"cell_type":"code","source":"# Look at feature importance\nn_features = X_train.shape[1]\nplt.barh(range(n_features), random_forest.feature_importances_, align='center')\nplt.yticks(np.arange(n_features),list(X_train.columns))\nplt.xlabel('Feature Importance')\nplt.ylabel('Feature')\nplt.show()","execution_count":58,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0dfec0746d3164a8172dde83be726f3197631d9c"},"cell_type":"code","source":"#Generate ROC Curve & AUC Metric for K Folds\n\nfrom scipy import interp\nimport matplotlib.pyplot as plt\nfrom itertools import cycle\n\nfrom sklearn import datasets\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.model_selection import StratifiedKFold\n\nX = X_train\ny = Y_train\n\ncv = model_selection.StratifiedKFold(n_splits=5)\nclassifier = RandomForestClassifier(n_estimators=10, random_state=7)\n\ntprs = []\naucs = []\nmean_fpr = np.linspace(0, 1, 100)\ni = 0\n\nplt.figure(figsize=(12,8))\n\n#Calculate ROC Curve/AUC for Each Split\nfor traini, testi in cv.split(X, y):\n    \n    probas_ = classifier.fit(X.loc[traini], y.loc[traini]).predict_proba(X.loc[testi])\n    \n    # Compute ROC curve and area the curve\n    fpr, tpr, thresholds = roc_curve(y.loc[testi], probas_[:, 1])\n    \n    tprs.append(interp(mean_fpr, fpr, tpr))\n    tprs[-1][0] = 0.0\n    roc_auc = auc(fpr, tpr)\n    aucs.append(roc_auc)\n    plt.plot(fpr, tpr, lw=1, alpha=0.3,label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))\n\n    i += 1\n\n#Generate ROC Curve Plot Details\nplt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',label='Luck', alpha=.8)\n\nmean_tpr = np.mean(tprs, axis=0)\nmean_tpr[-1] = 1.0\nmean_auc = auc(mean_fpr, mean_tpr)\nstd_auc = np.std(aucs)\nplt.plot(mean_fpr, mean_tpr, color='b',label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),lw=2, alpha=.8)\n\nstd_tpr = np.std(tprs, axis=0)\ntprs_upper = np.minimum(mean_tpr + std_tpr, 1)\ntprs_lower = np.maximum(mean_tpr - std_tpr, 0)\nplt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,label=r'$\\pm$ 1 std. dev.')\n\nplt.xlim([-0.05, 1.05])\nplt.ylim([-0.05, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve, Random Forest')\nplt.legend(loc=\"lower right\")\nplt.show()","execution_count":60,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dacaa56c3343788ac32b7fe4e7d25bff2749c0fb"},"cell_type":"code","source":"# Examine probability calculation calibration for model\n# Estiate probability score, bin based on predicted survival rate, compare against acutal\nfrom sklearn.calibration import CalibratedClassifierCV, calibration_curve\n\nprob_pos = random_forest.predict_proba(X_train)[:,1]\nfraction_of_positives, mean_predicted_value = calibration_curve(Y_train, prob_pos, n_bins=10)\n\nfig = plt.figure(0, figsize=(10, 10))\nax = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\nax2 = plt.subplot2grid((3, 1), (2, 0))\n\nax.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\");\nax.plot(mean_predicted_value, fraction_of_positives, \"s-\");\nax.set_ylabel('Actual Portion Survived')\nax.set_title('Calibration Curve, Decision Tree')\nax2.hist(prob_pos, range=(0, 1), bins=10,histtype=\"step\", lw=2);\nax2.set_xlabel('Predicted Portion to Survive');\nax2.set_ylabel('Count');","execution_count":81,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"551f2a90a6a3239b128ab25021f8a5d85dcced0f"},"cell_type":"code","source":"#Classification Report\nfrom sklearn.metrics import classification_report\nprint(classification_report(Y_train, Y_pred))","execution_count":61,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ca410cc26faced306543d6a7eeda840f011b41c6"},"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import average_precision_score\n\nY_scores = random_forest.predict_proba(X_train)[:,1]\n\nprecision, recall, _ = precision_recall_curve(Y_train, Y_scores)\naverage_precision = average_precision_score(Y_train, Y_scores)\n\nplt.figure(figsize=(12,8))\nplt.step(recall, precision, color='k', alpha=0.5,where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.2,color='k')\n\nplt.xlabel('Recall');\nplt.ylabel('Precision');\nplt.ylim([0.0, 1.05]);\nplt.xlim([0.0, 1.0]);\nplt.title('Precision-Recall Curve, Logistic Regression: Avg. Precision = {0:0.3f}'.format(average_precision));","execution_count":62,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5cf0737d126d8d09903eb4b59039074a7c24ebb4"},"cell_type":"code","source":"trues = Y_scores[Y_train==1]\nfalses = Y_scores[Y_train==0]\n\nplt.figure(figsize=(12,8));\nplt.scatter([i for i in range(len(falses))], falses, s=25, c='r', marker=\"o\", label='Perished');\nplt.scatter([i for i in range(len(trues))], trues, s=25, c='g', marker=\"o\", label='Suvived');\nplt.axhline(.5, color='black');\n\nplt.legend(loc='upper right');\nplt.title(\"Default Decision Boundary (Prob = 0.5)\");\nplt.xlabel('N');\nplt.ylabel('Predicted Probability');","execution_count":71,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3f258a67341956e8b416168287ffe6526d62445f"},"cell_type":"code","source":"#x=[i for i in range(len(trues))]\nx = np.linspace(0,len(trues),len(trues))\n#len(x)\n#trues\ng = sns.jointplot(x, trues)","execution_count":80,"outputs":[]},{"metadata":{"_uuid":"8e8899de564058e903aa99d8aaf07c7e4c5835ac"},"cell_type":"markdown","source":"### KNN"},{"metadata":{"trusted":true,"_uuid":"856754c84a6c0cd9066213cf8572cb4dc507190a"},"cell_type":"code","source":"knn_model = KNeighborsClassifier(n_neighbors = 10)\nknn_model.fit(X_train, Y_train)\nY_pred = knn_model.predict(X_train)\nacc_knn_model = knn_model.score(X_train, Y_train)\nacc_knn_model","execution_count":86,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a58b4997c87d9cf8509e88717a1e38a99df2d40a"},"cell_type":"code","source":"kfold = model_selection.KFold(n_splits=5, random_state=7)\nmodelCV = KNeighborsClassifier(n_neighbors = 10)\nscoring = 'accuracy'\nresults = model_selection.cross_val_score(modelCV, X_train, Y_train, cv=kfold, scoring=scoring)\nrForest_fold_accuracy = results.mean()\nprint(\"5-fold cross validation average accuracy: %.3f\" % (rForest_fold_accuracy))","execution_count":87,"outputs":[]},{"metadata":{"_uuid":"fab190f5f70a7aa305cc37d7935ffc717711a75c"},"cell_type":"markdown","source":"### Support Vector Machines"},{"metadata":{"_uuid":"84398705c872d551c8728a1bf689b8255e547ebe"},"cell_type":"markdown","source":"### Linear SVC"},{"metadata":{"_uuid":"274a0006e3b4c65fb3d846be99555753a97cdf1b"},"cell_type":"markdown","source":"### Perceptron"},{"metadata":{"_uuid":"df91d86e28feb5f03d9979eb47ea68ac8e3f5c6a"},"cell_type":"markdown","source":"### Naive Bayes"},{"metadata":{"_uuid":"8793eca37f74076be675e1db538829e6cc58be13"},"cell_type":"markdown","source":"### Stochastic Gradient Decent"},{"metadata":{"_uuid":"c2c6e536b814b67d71bfcbc6aac2167b0fe9fe40"},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}