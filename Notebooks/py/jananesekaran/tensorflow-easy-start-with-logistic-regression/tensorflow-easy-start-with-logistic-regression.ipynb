{"cells": [{"source": ["\n", "\n", "**INTRODUCTION : **\n", "\n", "This Kernel shows how to implement a simple Logistic Regression which uses sigmoid activation layer and l2 loss using TensorFlow. \n", "\n", "Please do upvote if you find this helpful.\n", "\n", "Suggestions are welcome :)"], "metadata": {}, "cell_type": "markdown"}, {"source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "import re\n", "import seaborn as sns\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline\n", "import tensorflow as tf\n", "import time\n", "import math\n", "from sklearn import metrics\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "\n", "# Any results you write to the current directory are saved as output."], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "2080bb4ce9c131da7937fb7f797790e4a2d0ed7e", "_cell_guid": "b1613d98-1c84-4713-9ba9-dfc1fbd5298e"}, "outputs": []}, {"source": ["**LOADING DATA :**\n", "\n", "The first step is to read the data from the CSV file using pandas.\n", "\n", "The current data_type is data frame.\n", "Difference between data frame and matrix is that data frame can store strings, numbers etc, whereas matrices can only store numbers."], "metadata": {}, "cell_type": "markdown"}, {"source": ["train_data = pd.read_csv('../input/train.csv')\n", "test_data = pd.read_csv('../input/test.csv')\n", "print(train_data.shape)\n", "print(test_data.shape)"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "072537e605ea10da3c1680fac9310921b39810f0", "_cell_guid": "6755cd4b-f0ed-44d2-b1e5-5b0464ba065b"}, "outputs": []}, {"source": ["train_data.head(3)"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "d37d90dd712de2adee3460ba6dbb7fd690743c9b", "scrolled": false, "_cell_guid": "6a2f767a-315b-423c-b2b5-24dcd6ba6188"}, "outputs": []}, {"source": ["**FEATURE ENGINEERING:**\n", "\n", "Thanks to the author of  https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python\n", "\n", "Do check it out for more details on Feature Engineering.\n", "\n", "The author explains detailed description of how to extract features from the dataset.\n"], "metadata": {}, "cell_type": "markdown"}, {"source": ["full_data = [train_data, test_data]\n", "\n", "# Feature that tells whether a passenger had a cabin on the Titanic\n", "train_data['Has_Cabin'] = train_data[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n", "test_data['Has_Cabin'] = test_data[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n", "\n", "# Feature engineering steps taken from Sina\n", "# Create new feature FamilySize as a combination of SibSp and Parch\n", "for dataset in full_data:\n", "    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n", "# Create new feature IsAlone from FamilySize\n", "for dataset in full_data:\n", "    dataset['IsAlone'] = 0\n", "    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n", "# Remove all NULLS in the Embarked column\n", "for dataset in full_data:\n", "    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n", "# Remove all NULLS in the Fare column and create a new feature CategoricalFare\n", "for dataset in full_data:\n", "    dataset['Fare'] = dataset['Fare'].fillna(train_data['Fare'].median())\n", "train_data['CategoricalFare'] = pd.qcut(train_data['Fare'], 4,duplicates='drop')\n", "# Create a New feature CategoricalAge\n", "for dataset in full_data:\n", "    age_avg = dataset['Age'].mean()\n", "    age_std = dataset['Age'].std()\n", "    age_null_count = dataset['Age'].isnull().sum()\n", "    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n", "    dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\n", "    dataset['Age'] = dataset['Age'].astype(int)\n", "train_data['CategoricalAge'] = pd.cut(train_data['Age'], 5)\n", "# Define function to extract titles from passenger names\n", "def get_title(name):\n", "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n", "    # If the title exists, extract and return it.\n", "    if title_search:\n", "        return title_search.group(1)\n", "    return \"\"\n", "# Create a new feature Title, containing the titles of passenger names\n", "for dataset in full_data:\n", "    dataset['Title'] = dataset['Name'].apply(get_title)\n", "# Group all non-common titles into one single grouping \"Rare\"\n", "for dataset in full_data:\n", "    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n", "    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n", "    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n", "    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n", "\n", "for dataset in full_data:\n", "    # Mapping Sex\n", "    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n", "    # Mapping titles\n", "    title_mapping = {\"Mr\": 0, \"Miss\": 1, \"Mrs\": 2, \"Master\": 3, \"Rare\": 4}\n", "    dataset['Title'] = dataset['Title'].map(title_mapping)\n", "    dataset['Title'] = dataset['Title'].fillna(0)\n", "    # Mapping Embarked\n", "    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n", "    # Mapping Fare\n", "    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare']                               = 0\n", "    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n", "    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n", "    dataset.loc[ dataset['Fare'] > 31, 'Fare']                                  = 3\n", "    dataset['Fare'] = dataset['Fare'].astype(int)\n", "    \n", "    # Mapping Age\n", "    dataset.loc[ dataset['Age'] <= 16, 'Age']                          = 0\n", "    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n", "    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n", "    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n", "    dataset.loc[ dataset['Age'] > 64, 'Age']                           = 4"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "899844afef248a36b7b5fc587e61b75bba600bdf", "_cell_guid": "289c583b-016f-41d5-b9bd-8e2eaa3e9354"}, "outputs": []}, {"source": ["# Feature selection\n", "drop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']\n", "train_data = train_data.drop(drop_elements, axis = 1)\n", "train_data = train_data.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\n", "test_data  = test_data.drop(drop_elements, axis = 1)"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "0fd48952f41fc6272410cbb24ae3f15e954028b9", "collapsed": true, "_cell_guid": "b437048a-da4a-40ad-9ce6-c9fa024b2119"}, "outputs": []}, {"source": ["train_data.head(3)"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "098cc6f97cb86488c64a0c439f2f149c63910491", "_cell_guid": "a1389de7-5885-4afd-bd04-5100b4c4b33c"}, "outputs": []}, {"source": ["colormap = plt.cm.rainbow\n", "plt.figure(figsize=(12,12))\n", "plt.title('Pearson Correlation of Features', y=1.05, size=15)\n", "sns.heatmap(train_data.astype(float).corr(),linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True)"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "f5c5d451033f7a547d2e2edad45f4585aea7d353", "_cell_guid": "399cdfa5-ebe5-4751-b973-48edb2f14e22"}, "outputs": []}, {"source": ["**SIMPLE LOGISTIC REGRESSION:**"], "metadata": {}, "cell_type": "markdown"}, {"source": ["**TRAIN AND VALIDATION SET:**\n", "\n", "Convert the dataframe to matrix using pandas.\n", "\n", "Split first 100 entries for validation. \n", "\n", "And also slice the labels from the train_data."], "metadata": {}, "cell_type": "markdown"}, {"source": ["train_data = train_data.as_matrix()\n", "test_data = test_data.as_matrix()\n", "X_train = train_data[100:,1:]\n", "y_train = train_data[100:,:1]\n", "y_train = np.reshape(y_train,-1)\n", "X_val = train_data[:100,1:]\n", "y_val = train_data[:100,:1]\n", "y_val = np.reshape(y_val,-1)\n", "X_test = test_data\n", "print('Train data shape: ', X_train.shape)\n", "print('Train labels shape: ', y_train.shape)\n", "print('Validation data shape: ', X_val.shape)\n", "print('Validation labels shape: ', y_val.shape)\n", "print('Test data shape: ', X_test.shape)"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "11eb9fea07e8997b7a6f1e1ec33bf5a68225df3d", "_cell_guid": "e376cc00-43d8-44e9-8472-71e4c77d6d44"}, "outputs": []}, {"source": ["def run_model(session, predict, loss_val, Xd, yd,\n", "              epochs=1, batch_size=64, print_every=100,\n", "              training=None, plot_losses=False):\n", "    # have tensorflow compute accuracy\n", "    correct_prediction = tf.equal(tf.argmax(predict,1), y)\n", "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n", "    \n", "    # shuffle indicies\n", "    train_indicies = np.arange(Xd.shape[0])\n", "\n", "    training_now = training is not None\n", "    \n", "    # setting up variables we want to compute (and optimizing)\n", "    # if we have a training function, add that to things we compute\n", "    variables = [cost_op,correct_prediction,accuracy]\n", "    if training_now:\n", "        variables[-1] = training\n", "    \n", "    # counter \n", "    iter_cnt = 0\n", "    for e in np.arange(epochs):\n", "        # keep track of losses and accuracy\n", "        correct = 0\n", "        losses = []\n", "        # make sure we iterate over the dataset once\n", "        for i in np.arange(int(math.ceil(Xd.shape[0]/batch_size))):\n", "            # generate indicies for the batch\n", "            start_idx = (i*batch_size)%Xd.shape[0]\n", "            idx = train_indicies[start_idx:start_idx+batch_size]\n", "            \n", "            # create a feed dictionary for this batch\n", "            feed_dict = {x: Xd[idx,:],\n", "                         y: yd[idx],\n", "                         is_training: training_now }\n", "            # get batch size\n", "            actual_batch_size = yd[idx].shape[0]\n", "            # have tensorflow compute loss and correct predictions\n", "            # and (if given) perform a training step\n", "            loss, corr, _ = session.run(variables,feed_dict=feed_dict)\n", "            \n", "            # aggregate performance stats\n", "            losses.append(loss*actual_batch_size)\n", "            correct += np.sum(corr)\n", "            \n", "            # print every now and then\n", "            if training_now and (iter_cnt % print_every) == 0:\n", "                print(\"Iteration {0}: with minibatch training loss = {1:.3g} and accuracy of {2:.2g}\"\\\n", "                      .format(iter_cnt,loss,np.sum(corr)/actual_batch_size))\n", "            iter_cnt += 1\n", "        total_correct = correct/Xd.shape[0]\n", "        total_loss = np.sum(losses)/Xd.shape[0]\n", "        print(\"Epoch {2}, Overall loss = {0:.3g} and accuracy of {1:.3g}\"\n", "              .format(total_loss,total_correct,e+1))\n", "        if plot_losses and (e == epochs-1):\n", "            plt.plot(losses)\n", "            plt.grid(True)\n", "            plt.title('Epoch {} Loss'.format(e+1))\n", "            plt.xlabel('minibatch number')\n", "            plt.ylabel('minibatch loss')\n", "            plt.show()\n", "    return total_loss,total_correct"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "04fb74dfb8708b7b526d2518338f685c318ba340", "_cell_guid": "801eb4ab-ac68-49a5-bec9-9da505d71f5f"}, "outputs": []}, {"source": ["Here the number of features = 10\n", "\n", "And the number of classes/Labels we are predicting (here just 2: survived or not_survived)"], "metadata": {}, "cell_type": "markdown"}, {"source": ["numFeatures = X_train.shape[1]\n", "numLabels = 2"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "e2217d7ec76c2520d484c9aa041b23ca0c40b41a", "collapsed": true, "_cell_guid": "faf7bf71-8171-4b19-b4d9-8b02f5791a80"}, "outputs": []}, {"source": ["**CREATING PLACEHOLDERS AND INITIALIZATIONS:**\n", "\n", "Placeholders:\n", "    Here we need two placeholders for input X and output y respectively.\n", "    A graph can be parameterized to accept external inputs, known as placeholders. A placeholder is a promise to provide a value later.\n", "    \n", "Lambda:\n", "    Regularization Parameter. Avoids overfitting. Start with 0.001 value and increase or decrease it accordingly.\n", "\n", "Learning Rate:\n", "    In training deep networks, it is usually helpful to anneal the learning rate over time. Good intuition to have in mind is that with a high learning rate, the system contains too much kinetic energy and the parameter vector bounces around chaotically, unable to settle down into deeper, but narrower parts of the loss function.\n", "    \n", "    checkout : http://cs231n.github.io/neural-networks-3/ for more details.\n", "\n"], "metadata": {}, "cell_type": "markdown"}, {"source": ["# clear old variables\n", "tf.reset_default_graph()\n", "\n", "x = tf.placeholder(tf.float32, [None, numFeatures])\n", "y = tf.placeholder(tf.int64, [None])\n", "is_training = tf.placeholder(tf.bool)\n", "Lambda = 0.001 #Regularization Parameter\n", "learningRate = tf.train.exponential_decay(learning_rate=1e-2,\n", "                                          global_step= 1,\n", "                                          decay_steps=X_train.shape[0],\n", "                                          decay_rate= 0.95,\n", "                                          staircase=True)"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "93924dd107542b6bd3eb11fbc120f6cf39fee0b8", "collapsed": true, "_cell_guid": "9d46112a-f98e-4265-b75e-1d17b7b0c62a"}, "outputs": []}, {"source": ["**LOGISTIC REGRESSION MODEL:\n", "**\n", "\n", "This model uses Sigmoid as the activation function.\n", "L2 Regularization is used.\n", "\n", "**fully_connected_layer = x*weights + bias**\n", "\n", "**activation_layer = sigmoid(fully_connected_layer)**\n", "\n", "**Loss = cross_entropy(activation_layer)+Lambda*L2_loss**\n", "\n", "Using GradientDescent Optimizer\n"], "metadata": {}, "cell_type": "markdown"}, {"source": ["# Logistic Regression\n", "def Titanicmodel(x,y,is_training):   \n", "    weights=tf.get_variable(\"weights\",shape=[numFeatures,numLabels])\n", "    bias=tf.get_variable(\"bias\",shape=[numLabels])\n", "    y_out = tf.matmul(x,weights)+bias\n", "    return(y_out,weights)\n", "y_out,weights = Titanicmodel(x,y,is_training)"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "d7e503662e39038b4737b65a65c6cf0c51314b66", "_cell_guid": "f8eb6ef5-7295-4ba7-b303-f038aae8507d"}, "outputs": []}, {"source": ["loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.one_hot(y,2),logits=y_out))\n", "regularizer = tf.nn.l2_loss(weights)\n", "cost_op = tf.reduce_mean(loss + Lambda * regularizer)\n", "optimizer = tf.train.GradientDescentOptimizer(learningRate)\n", "train_step = optimizer.minimize(cost_op)"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "52a41199f02733a651acec95c3fe9d4271459cb5", "_cell_guid": "c161589a-f018-4d7c-9ce7-ee8353c67f0b"}, "outputs": []}, {"source": ["#Prediction\n", "prediction = tf.argmax(y_out,1)"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "1ebae57000d8f781684fef26183437b3e0446efc", "collapsed": true, "_cell_guid": "b297f58b-24ff-4bc4-af29-79203ab04bd2"}, "outputs": []}, {"source": ["#Lets strat a session\n", "sess = tf.Session()\n", "sess.run(tf.global_variables_initializer())"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "20b42d8bdda947d970bab7e5c57f7ae5d6228795", "collapsed": true, "_cell_guid": "91076ef6-c84a-4dc3-85af-6ada81d4b6c6"}, "outputs": []}, {"source": ["**TRAINING & VALIDATION**\n", "\n", " **Parameters: **\n", " \n", "     session, predict, loss_val, Xd, yd,epochs, batch_size, print_every,training, plot_losses\n", "     \n", "     Here I am using epochs = 100, batch_size = 100, print_every = 100.\n", "     \n", "     Tune this accordingly to get best results.\n", "     "], "metadata": {}, "cell_type": "markdown"}, {"source": ["print('Training')\n", "run_model(sess,y_out,cost_op,X_train,y_train,100,100,100,train_step,True)\n", "print('Validation')\n", "run_model(sess,y_out,cost_op,X_val,y_val,1,100)"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "5257078e5603e0755087b557fac249787ca00cfe", "scrolled": false, "_cell_guid": "e24622ad-f238-46a4-966d-ea0c81166e29"}, "outputs": []}, {"source": ["**RECEIVER OPERATING CHARACTERISTIC(ROC) CURVE:**\n", "\n", "ROC curve, is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied.\n", "\n", "The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. \n", "\n", "This can be used to analyse if the classifier is skewed or not. (i.e) To make sure that the classifier doesn't always predict the same output"], "metadata": {}, "cell_type": "markdown"}, {"source": ["predicted_vallabels = np.zeros(X_val.shape[0])\n", "for i in np.arange(0,X_val.shape[0]/50,dtype=np.int64):\n", "    start = i*50\n", "    end = (i+1)*50\n", "    predicted_vallabels[start:end] = sess.run(prediction,feed_dict={x: X_val[start:end,:],y: predicted_vallabels[start:end],is_training: False})\n", "# Compute ROC curve and ROC area for each class\n", "fpr = dict()\n", "tpr = dict()\n", "roc_auc = dict()\n", "fpr, tpr, _ = metrics.roc_curve(y_val, predicted_vallabels)\n", "roc_auc = metrics.auc(fpr, tpr)\n", "plt.figure()\n", "plt.plot(fpr, tpr, color='darkorange', label='ROC curve (area = %0.2f)' % roc_auc)\n", "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n", "plt.xlim([0.0, 1.0])\n", "plt.ylim([0.0, 1.05])\n", "plt.xlabel('False Positive Rate')\n", "plt.ylabel('True Positive Rate')\n", "plt.title('ROC_Curve')\n", "plt.legend(loc=\"lower right\")\n", "plt.show()"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "91664dd947333eec799482e5ca025e551789632d", "_cell_guid": "4c7c80c7-acd9-48e2-b9d5-193b7884ac3c"}, "outputs": []}, {"source": ["**PREDICTION:**\n", "\n", "Predict the test_labels using the trained model and weights"], "metadata": {}, "cell_type": "markdown"}, {"source": ["predicted_labels = np.zeros(X_test.shape[0])\n", "for i in np.arange(0,X_test.shape[0]/50,dtype=np.int64):\n", "    start = i*50\n", "    end = (i+1)*50\n", "    predicted_labels[start:end] = sess.run(prediction,feed_dict={x: X_test[start:end,:],y: predicted_labels[start:end],is_training: False})"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "0fec89339027da9708b52f36271ca79229d1d5fb", "collapsed": true, "_cell_guid": "8128ea4f-2695-41da-8d34-2e95ab50652a"}, "outputs": []}, {"source": ["print('predicted_labels:',predicted_labels[5])"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "c6d8f635daed4dcd7fffcb51e8b928e7b91adf6b", "_cell_guid": "79c43923-6f7e-4f73-9a9c-dbfb31ab2c7f"}, "outputs": []}, {"source": ["testID=pd.read_csv('../input/gendermodel.csv')\n", "print(testID.shape)\n", "PassengerId = testID['PassengerId']"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "a0e745fdc82ee1e6ffff7fd7e2c2197b3ce5c27c", "_cell_guid": "7a4e6f9e-9574-45a8-9f9f-28ccd8e2c37a"}, "outputs": []}, {"source": ["**MAKE SUBMISSION FILE:**"], "metadata": {}, "cell_type": "markdown"}, {"source": ["# save results\n", "np.savetxt('submission.csv', \n", "           np.c_[PassengerId,predicted_labels], \n", "           delimiter=',', \n", "           header = 'PassengerId,Survived', \n", "           comments = '', \n", "           fmt='%d')"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "d67600537fd84fc9e0c656186c4fad080d4e0798", "collapsed": true, "_cell_guid": "0c794b1d-5152-4746-a1d9-d6c6d837bdcc"}, "outputs": []}, {"source": ["sess.close()"], "execution_count": null, "cell_type": "code", "metadata": {"_uuid": "cff2ac89af48e49fa5cf0661021a15f8de8f917a", "collapsed": true, "_cell_guid": "0d15c121-ffbb-4619-b12a-86d7057e61cf"}, "outputs": []}, {"source": ["**Thank you! :)**"], "metadata": {}, "cell_type": "markdown"}], "metadata": {"kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}}, "nbformat": 4, "nbformat_minor": 1}