{"nbformat_minor": 1, "cells": [{"source": ["**INTRODUCTION :**\n", "\n", "This Notebook helps beginners visualize the Decision Boundaries for various classifiers.\n", "\n", "This Notebook covers Decision Boundary Visualization of below classifiers:\n", "\n", "**CLASSIFIERS :**\n", "1. Logistic_Regression\n", "2. K_Nearest_Neighbors\n", "3. Support_Vector_Machines\n", "4. Decision_Trees\n", "5. Random_Forest\n", "6. Extra_Trees\n", "7. Ada_Boost\n", "8. Gradient_Boost\n", "\n", "Please do upvote if you find this helpful.\n", "\n", "Suggestions are welcome :)"], "cell_type": "markdown", "metadata": {"_cell_guid": "4954371f-2e56-4a34-9df8-f909f29d01c5", "_uuid": "0608622e8c5d19c19775f8e6f563f323761dbd5a"}}, {"source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "\n", "import re\n", "import time\n", "import math\n", "import numpy as np \n", "import pandas as pd \n", "import seaborn as sns\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline\n", "#import classifiers from sklearn\n", "from sklearn.model_selection import train_test_split\n", "from sklearn import svm\n", "from sklearn.tree import DecisionTreeClassifier\n", "from sklearn.neighbors import KNeighborsClassifier\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.ensemble import RandomForestClassifier\n", "from sklearn.ensemble import GradientBoostingClassifier\n", "from sklearn.ensemble import AdaBoostClassifier\n", "from sklearn.ensemble import ExtraTreesClassifier\n", "\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "import warnings\n", "warnings.filterwarnings('ignore')\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "\n", "# Any results you write to the current directory are saved as output."], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "eb368eda-0bf3-45e9-b08d-db830bb4a25a", "_uuid": "33437ce7308b67e35d02f1d00bc664ecb96b232c"}}, {"source": ["**LOADING DATA :**\n", "\n", "The first step is to read the data from the CSV file using pandas.\n", "\n", "The current data_type is data frame. Difference between data frame and matrix is that data frame can store strings, numbers etc, whereas matrices can only store numbers."], "cell_type": "markdown", "metadata": {"_cell_guid": "992a25b5-1140-47bb-9520-5466545720a6", "_uuid": "cce713395a2c554bf947425cde1034f56448d467"}}, {"source": ["train_data = pd.read_csv('../input/train.csv')\n", "test_data = pd.read_csv('../input/test.csv')\n", "train_data.head(2)"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "d619fba7-694e-4a21-b538-462e0e9d3fbf", "_uuid": "146cde53ca1c8566adeb7bd02dbe61f2634fe2a0"}}, {"source": ["**FEATURE ENGINEERING:**\n", "\n", "Thanks to the author of https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python\n", "\n", "Do check it out for more details on Feature Engineering.\n", "\n", "The author explains detailed description of how to extract features from the dataset."], "cell_type": "markdown", "metadata": {"_cell_guid": "de1cf3bc-19e8-4f78-a0e5-b0b911a4c60b", "_uuid": "df74e77dca1b6f7853c1d53a5fa9243c8b3820a0"}}, {"source": ["full_data = [train_data, test_data]\n", "\n", "# Feature that tells whether a passenger had a cabin on the Titanic\n", "train_data['Has_Cabin'] = train_data[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n", "test_data['Has_Cabin'] = test_data[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n", "\n", "# Feature engineering steps taken from Sina\n", "# Create new feature FamilySize as a combination of SibSp and Parch\n", "for dataset in full_data:\n", "    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n", "# Create new feature IsAlone from FamilySize\n", "for dataset in full_data:\n", "    dataset['IsAlone'] = 0\n", "    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n", "# Remove all NULLS in the Embarked column\n", "for dataset in full_data:\n", "    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n", "# Remove all NULLS in the Fare column and create a new feature CategoricalFare\n", "for dataset in full_data:\n", "    dataset['Fare'] = dataset['Fare'].fillna(train_data['Fare'].median())\n", "train_data['CategoricalFare'] = pd.qcut(train_data['Fare'], 4,duplicates='drop')\n", "# Create a New feature CategoricalAge\n", "for dataset in full_data:\n", "    age_avg = dataset['Age'].mean()\n", "    age_std = dataset['Age'].std()\n", "    age_null_count = dataset['Age'].isnull().sum()\n", "    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n", "    dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\n", "    dataset['Age'] = dataset['Age'].astype(int)\n", "train_data['CategoricalAge'] = pd.cut(train_data['Age'], 5)\n", "# Define function to extract titles from passenger names\n", "def get_title(name):\n", "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n", "    # If the title exists, extract and return it.\n", "    if title_search:\n", "        return title_search.group(1)\n", "    return \"\"\n", "# Create a new feature Title, containing the titles of passenger names\n", "for dataset in full_data:\n", "    dataset['Title'] = dataset['Name'].apply(get_title)\n", "# Group all non-common titles into one single grouping \"Rare\"\n", "for dataset in full_data:\n", "    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n", "    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n", "    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n", "    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n", "\n", "for dataset in full_data:\n", "    # Mapping Sex\n", "    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n", "    # Mapping titles\n", "    title_mapping = {\"Mr\": 0, \"Miss\": 1, \"Mrs\": 2, \"Master\": 3, \"Rare\": 4}\n", "    dataset['Title'] = dataset['Title'].map(title_mapping)\n", "    dataset['Title'] = dataset['Title'].fillna(0)\n", "    # Mapping Embarked\n", "    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n", "    # Mapping Fare\n", "    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare']                               = 0\n", "    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n", "    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n", "    dataset.loc[ dataset['Fare'] > 31, 'Fare']                                  = 3\n", "    dataset['Fare'] = dataset['Fare'].astype(int)\n", "    \n", "    # Mapping Age\n", "    dataset.loc[ dataset['Age'] <= 16, 'Age']                          = 0\n", "    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n", "    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n", "    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n", "    dataset.loc[ dataset['Age'] > 64, 'Age']                           = 4\n", " \n", "# Feature selection\n", "drop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']#'Parch','Fare','Embarked','IsAlone']\n", "train_data = train_data.drop(drop_elements, axis = 1)\n", "train_data = train_data.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\n", "test_data  = test_data.drop(drop_elements, axis = 1)\n", "train_data.head(2)"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "cad97144-43d8-4ae0-a628-ece68622a198", "_uuid": "5e24bd613467483faa7b44a2df4fe19d0cb923ac"}}, {"source": ["plt.figure(figsize=(15,7)) \n", "plt.title('Pearson Correlation of Features', y=1.05, size=15)\n", "sns.heatmap(train_data.corr(),annot=True,cmap=plt.cm.winter) #draws  heatmap with input as the correlation matrix calculted by(iris.corr())\n", "plt.show()"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "c117b9f1-fc70-46d3-97e8-5dcfb5f8a03d", "_uuid": "31321fc01ad7980140c204b2c60e4b7cdd3b16b6"}}, {"source": ["def plot(X_reduced,y,y_predict,title,f,axs):\n", "    # create meshgrid\n", "    #plot the negative points and positive points\n", "    axs.set_title(title,fontsize=20)\n", "    neg_val1 = X_reduced[np.where(y == 0), 0]\n", "    neg_val2 = X_reduced[np.where(y == 0), 1]\n", "    pos_val1 = X_reduced[np.where(y == 1), 0]\n", "    pos_val2 = X_reduced[np.where(y == 1), 1]\n", "    resolution = 500 # 100x100 background pixels\n", "    X2d_xmin, X2d_xmax = np.min(X_reduced[:,0]), np.max(X_reduced[:,0])\n", "    X2d_ymin, X2d_ymax = np.min(X_reduced[:,1]), np.max(X_reduced[:,1])\n", "    xx, yy = np.meshgrid(np.linspace(X2d_xmin, X2d_xmax, resolution), np.linspace(X2d_ymin, X2d_ymax, resolution))\n", "\n", "    # approximate Voronoi tesselation on resolution x resolution grid using 1-NN\n", "    background_model = KNeighborsClassifier(n_neighbors=1).fit(X_reduced, y_predict) \n", "    voronoiBackground = background_model.predict(np.c_[xx.ravel(), yy.ravel()])\n", "    voronoiBackground = voronoiBackground.reshape((resolution, resolution))\n", "\n", "    #plot\n", "    axs.contourf(xx, yy, voronoiBackground)\n", "    l1 = axs.scatter(neg_val1, neg_val2, marker='o', c='red')\n", "    l2 = axs.scatter(pos_val1, pos_val2, marker='x', c='green')\n", "    f.legend((l1, l2), ('Not_Survived', 'Survived'), 'upper left',fontsize=15)"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "762688e0-6208-426b-a692-cafb81d6032f", "_uuid": "b8a328434be18eb3081c411cac1ac86ed3c63090", "collapsed": true}}, {"source": ["train_data = train_data.as_matrix()\n", "test_data = test_data.as_matrix()\n", "X = train_data[:,1:]\n", "y = train_data[:,:1]\n", "X_test = test_data\n", "X_train, X_val, y_train, y_val = train_test_split( X, y, test_size = 0.1)# in this our main data is split into train and test\n", "# the attribute test_size=0.1 splits the data into 90% and 10% ratio. train=90% and test=10%\n", "y_train = np.reshape(y_train,-1)\n", "y_val = np.reshape(y_val,-1)\n", "print('Train data shape: ', X_train.shape)\n", "print('Train labels shape: ', y_train.shape)\n", "print('Test data shape: ', X_val.shape)\n", "print('Test labels shape: ', y_val.shape)"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "e1b95114-8ee2-4551-a09a-afda94459da9", "_uuid": "fc200cbb95cab4ed305c9bb5e90ba700cb342648"}}, {"source": ["mean = np.mean(X_train, axis=0,dtype=np.int64)\n", "std = np.std(X_train, axis=0)\n", "X_train -= mean\n", "X_train = X_train/std\n", "X_val -= mean\n", "X_val = X_val/std\n", "X_test -= mean\n", "X_test = X_test/std"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "b5e53ba7-2699-4190-a554-990825ee5f35", "_uuid": "5bafc93475ecede17dac77d9d8f19a638377fd64", "collapsed": true}}, {"source": ["**DATA VISUALIZATION :**\n", "\n", "**Use PCA to reduce the dimensions to 2 features.**"], "cell_type": "markdown", "metadata": {"_cell_guid": "70ed32e8-cae2-454e-8e68-48f953875535", "_uuid": "2c686723530a74db87374d08c8d90c6ad18a57f0"}}, {"source": ["from sklearn.decomposition import TruncatedSVD\n", "X_train_reduced = TruncatedSVD(n_components=2, random_state=0).fit_transform(X_train)\n", "X_val_reduced = TruncatedSVD(n_components=2, random_state=0).fit_transform(X_val)"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "8626a28c-e68c-440a-9430-73bf47ae1602", "_uuid": "38d710d566d6f2ca9b2535c93cb04d4691b2fc87", "collapsed": true}}, {"source": ["**VISUALIZING TRAIN & VALIDATION DATA : **"], "cell_type": "markdown", "metadata": {"_cell_guid": "f587aae3-b4e2-4e00-aa31-f6a2875e8143", "_uuid": "f47b0c1e82183ce59fdc85c74f3e71265aca3149"}}, {"source": ["#plot the negative points and positive points\n", "f, axs = plt.subplots(1,2,figsize=(16,7))\n", "axs[0].set_title('Training_Data',fontsize=20)\n", "l1 = axs[0].scatter(X_train_reduced[np.where(y_train == 0), 0], X_train_reduced[np.where(y_train == 0), 1], marker='o', c='red')\n", "l2 = axs[0].scatter(X_train_reduced[np.where(y_train == 1), 0], X_train_reduced[np.where(y_train == 1), 1], marker='x', c='green')\n", "f.legend((l1, l2), ('Not_Survived', 'Survived'), 'upper left',fontsize=15)\n", "axs[1].set_title('Validation_Data',fontsize=20)\n", "l3 = axs[1].scatter(X_val_reduced[np.where(y_val == 0), 0], X_val_reduced[np.where(y_val == 0), 1], marker='o', c='red')\n", "l4 = axs[1].scatter(X_val_reduced[np.where(y_val == 1), 0], X_val_reduced[np.where(y_val == 1), 1], marker='x', c='green')\n", "f.legend((l3, l4), ('Not_Survived', 'Survived'), 'upper right',fontsize=15)"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "335b3bc9-ae8b-4855-953f-29a9a1571fd2", "_uuid": "fdbceb8d9f412e1481d093bd22e0cb1e5aefc9cc"}}, {"source": ["columns = ['Train_Accuracy','Validation_Accuracy']\n", "index = ['Logistic_Regression','KNN','SVM','Decision_Tree','Random_Forest','Extra_Trees','Ada_Boost','Gradient_Boost']\n", "data = np.zeros((8,2))"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "69430ad3-8af2-4568-8d53-a4b0341cc052", "_uuid": "634a9b7463ceee18ca97404727c4b08fcdedec7d", "collapsed": true}}, {"source": ["LR = LogisticRegression().fit(X_train,y_train)\n", "svm = svm.SVC(kernel='rbf', random_state=0, gamma=.10, C=1.0).fit(X_train, y_train)\n", "knn = KNeighborsClassifier().fit(X_train,y_train)\n", "DT = DecisionTreeClassifier().fit(X_train,y_train)\n", "RF = RandomForestClassifier(n_estimators=500, max_depth=None,min_samples_split=2, random_state=0).fit(X_train,y_train)\n", "ET = ExtraTreesClassifier(n_estimators=500, max_depth=None,min_samples_split=2, random_state=0).fit(X_train,y_train)\n", "AB = AdaBoostClassifier(n_estimators=500).fit(X_train,y_train)\n", "GB = GradientBoostingClassifier(n_estimators=500).fit(X_train,y_train)"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "4ece8095-4f1f-4293-b30e-4213d6ad9318", "_uuid": "d75325869d8ee88664aea4b00f32c42030ccd768", "collapsed": true}}, {"source": ["**DECISION_BOUNDARIES :**"], "cell_type": "markdown", "metadata": {"_cell_guid": "e5fd2235-576e-4c89-89a4-e484df8a7ee7", "_uuid": "07597eae247905ac9fd1b85de25110eba1aa79fc"}}, {"source": ["**LOGISTIC_REGRESSION :**"], "cell_type": "markdown", "metadata": {}}, {"source": ["f, axs = plt.subplots(1,2,figsize=(16,7))\n", "plot(X_train_reduced,y_train,LR.predict(X_train),'Logistic_Regression_Train',f,axs[0])\n", "plot(X_val_reduced,y_val,LR.predict(X_val),'Logistic_Regression_Validation',f,axs[1])"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {}}, {"source": ["**K_NEAREST_NEIGHBORS :**"], "cell_type": "markdown", "metadata": {}}, {"source": ["f, axs = plt.subplots(1,2,figsize=(16,7))\n", "plot(X_train_reduced,y_train,knn.predict(X_train),'KNN_Train',f,axs[0])\n", "plot(X_val_reduced,y_val,knn.predict(X_val),'KNN_Validation',f,axs[1])"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {}}, {"source": ["**SUPPORT_VECTOR_MACHINES : **"], "cell_type": "markdown", "metadata": {}}, {"source": ["f, axs = plt.subplots(1,2,figsize=(16,7))\n", "plot(X_train_reduced,y_train,svm.predict(X_train),'SVM_Train',f,axs[0])\n", "plot(X_val_reduced,y_val,svm.predict(X_val),'SVM_Validation',f,axs[1])"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "0f91bf18-9f40-43a7-9197-a83c768589cc", "_uuid": "05f72c94084b8106e7e079835b46942b4844fe5a"}}, {"source": ["**DECISION_TREES :**"], "cell_type": "markdown", "metadata": {}}, {"source": ["f, axs = plt.subplots(1,2,figsize=(16,7))\n", "plot(X_train_reduced,y_train,DT.predict(X_train),'Decision_Tree_Train',f,axs[0])\n", "plot(X_val_reduced,y_val,DT.predict(X_val),'Decision_Tree_Validation',f,axs[1])"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {}}, {"source": ["**RANDOM_FOREST :**"], "cell_type": "markdown", "metadata": {"_cell_guid": "0c53ebe3-5986-4a4a-9af8-1fca66e28952", "_uuid": "b8aabd84f5245b86e0dc2e62e2936600d2ee9e8b"}}, {"source": ["f, axs = plt.subplots(1,2,figsize=(16,7))\n", "plot(X_train_reduced,y_train,RF.predict(X_train),'Random_Forest_Train',f,axs[0])\n", "plot(X_val_reduced,y_val,RF.predict(X_val),'Random_Forest_Validation',f,axs[1])"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "8750e4df-949d-4991-8548-c5f2ccd919b6", "_uuid": "5103abeeae85279ff953dedf04c16196e7604b9c"}}, {"source": ["**EXTRA_TREES :**"], "cell_type": "markdown", "metadata": {}}, {"source": ["f, axs = plt.subplots(1,2,figsize=(16,7))\n", "plot(X_train_reduced,y_train,ET.predict(X_train),'Extra_Trees_Train',f,axs[0])\n", "plot(X_val_reduced,y_val,ET.predict(X_val),'Extra_Trees_Validation',f,axs[1])"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {}}, {"source": ["**ADA_BOOSTING :**"], "cell_type": "markdown", "metadata": {}}, {"source": ["f, axs = plt.subplots(1,2,figsize=(16,7))\n", "plot(X_train_reduced,y_train,AB.predict(X_train),'Ada_Boost_Train',f,axs[0])\n", "plot(X_val_reduced,y_val,AB.predict(X_val),'Ada_Boost_Validation',f,axs[1])"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {}}, {"source": ["**GRADIENT_BOOSTING :**"], "cell_type": "markdown", "metadata": {}}, {"source": ["f, axs = plt.subplots(1,2,figsize=(16,7))\n", "plot(X_train_reduced,y_train,GB.predict(X_train),'Gradient_Boost_Train',f,axs[0])\n", "plot(X_val_reduced,y_val,GB.predict(X_val),'Gradient_Boost_Validation',f,axs[1])"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {}}, {"source": ["**ACCURACIES OF TRAIN AND VALIDATION SET:**"], "cell_type": "markdown", "metadata": {"_cell_guid": "0b811889-72e8-4121-86e1-66b209d50561", "_uuid": "8f7bf9a3e8ca5f8db9d9a52280f68a25b6d4580d"}}, {"source": ["data[0,0] = LR.score(X_train,y_train)\n", "data[0,1] = LR.score(X_val,y_val)\n", "data[1,0] = knn.score(X_train,y_train)\n", "data[1,1] = knn.score(X_val,y_val)\n", "data[2,0] = svm.score(X_train,y_train)\n", "data[2,1] = svm.score(X_val,y_val)\n", "data[3,0] = DT.score(X_train,y_train)\n", "data[3,1] = DT.score(X_val,y_val)\n", "data[4,0] = RF.score(X_train,y_train)\n", "data[4,1] = RF.score(X_val,y_val)\n", "data[5,0] = ET.score(X_train,y_train)\n", "data[5,1] = ET.score(X_val,y_val)\n", "data[6,0] = AB.score(X_train,y_train)\n", "data[6,1] = AB.score(X_val,y_val)\n", "data[7,0] = GB.score(X_train,y_train)\n", "data[7,1] = GB.score(X_val,y_val)"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "198737fd-9207-4260-9cc0-84917d4d36a3", "_uuid": "884cb814dbccb9a43b6d4a5515dd9e191ef7322b", "collapsed": true}}, {"source": ["accuracy = pd.DataFrame(data, index=index, columns=columns)\n", "print(accuracy)"], "execution_count": null, "outputs": [], "cell_type": "code", "metadata": {"_cell_guid": "7e9aaab5-e251-4cb6-aff7-586de94059d8", "_uuid": "df7088edb19e1b31fe9dcf2d92bb567259d7fe86"}}, {"source": ["**Thank you!!!\n", "Have a Nice Day!!**"], "cell_type": "markdown", "metadata": {"_cell_guid": "44195bab-91ae-4782-ac85-b031101d5786", "_uuid": "39221b22006115be361c98bfee255630a43acb2b"}}], "nbformat": 4, "metadata": {"language_info": {"mimetype": "text/x-python", "file_extension": ".py", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "name": "python", "version": "3.6.1"}, "kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}}}