{"cells":[{"metadata":{"_uuid":"d0316f30ff8317a6f8a5f27f854ed4e4a00a4f4d"},"cell_type":"markdown","source":"# The preprocessing framework with Titanic"},{"metadata":{"_uuid":"924ea1b630e2d08bdaee334e734630e0dc708213"},"cell_type":"markdown","source":"*by Lana Samoilova (April, 2018)*"},{"metadata":{"_uuid":"259b59df19f1ce7f5aa00f8064bf0b80b91347ef"},"cell_type":"markdown","source":"Hi there,\n\nI want to introduce you my thoughts about what and how to clean in data sets.   \n(no modeling part in this notebook, data cleansing only)     \n\nI'm trying to create full routine framework to make that job easier.     \nFor beginners espesially.   \n\nHope, it will help.   \n\n\nTABLE OF CONTENTS:     \n[1. Imports](#1)  \n[2. First meeting with Data (load file and understand the goal)](#2)  \n[3. Duplicate values](#3)  \n[4. Outliers](#4)  \n[5. Data types (deep understanding the data)](#5)  \n[6. Missing values](#6)  \n[7. Feature engineering (and feature selection)](#7)  \n[8. Encoding](#8)  \n[9. Final check and final thoughts](#9)  \n\nI am new here and hoping to learn a lot, so any feedback is really welcome!"},{"metadata":{"_uuid":"d4183a51c111e6cd92dfc12dbb08139120be9bf8"},"cell_type":"markdown","source":"# Imports "},{"metadata":{"_uuid":"0c78e9181983051c77e812e3d05f519e1acfd39f"},"cell_type":"markdown","source":"The simplest part, isn't it?    \nBut do not forget that it can make you job a little bit easier if you do some customizing here."},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"77845d50e6c36032c0f286ba2346c6ac5ff94d93"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\npd.options.display.float_format = '{:,.3f}'.format # to format large numbers\n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style='whitegrid', color_codes=True)\npalette = 'cubehelix' # good for bw printer\n\n# show plots in Jupyter Notebook\n%matplotlib inline ","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"098513680cb8415bee81a5ed4c4ac945c30436fd"},"cell_type":"markdown","source":"# First meeting with Data (load file and understand the goal) "},{"metadata":{"_uuid":"51a0bfd6683d03f582fa1ef48db27775db536690"},"cell_type":"markdown","source":"Titanic.     \nWho doesn't know about this shipwrecks?    \nBut behind of the story and movie there are people.    \n\nSo, my first suggestion is: always to remember that data is not only data, it's real live and real lifes.\nLet's dive into..."},{"metadata":{"trusted":true,"_uuid":"f0006aa17eb67fd381a30a80160af8ab418194cd"},"cell_type":"code","source":"raw_train = pd.read_csv('../input/train.csv')\nraw_train.head()","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"f91e54c277b1325fac353c491a4ed0f678a957c5"},"cell_type":"markdown","source":"> *** Our goal is to predict survival for people in the test data set using train data as a base for machine learning***"},{"metadata":{"trusted":true,"_uuid":"5127f571dbdd79307dba862e032eef2c080232e1"},"cell_type":"code","source":"raw_test = pd.read_csv('../input/test.csv')\nraw_test.head()","execution_count":4,"outputs":[]},{"metadata":{"_uuid":"d1d4a49c86de3e4221af28fefb0f32478afa4211"},"cell_type":"markdown","source":"As we can see, the data sets are contain the same info, except 'Survived' column.   \n\nI'll save this column as 'target' and I'll concatinate data sets to exclude extra job during a data cleansing (I'll separate it back at the end of process)"},{"metadata":{"trusted":true,"_uuid":"c1a5d697942783cd3b607461dcdf2a1ec56bb588"},"cell_type":"code","source":"target = raw_train['Survived']\ntarget.head()","execution_count":5,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"499e505cbf07856bc0b9c55a63e197dc02c88e86"},"cell_type":"code","source":"df = pd.concat(objs=[raw_train, raw_test], axis=0).reset_index(drop=True)\ndel df['Survived']\ndf.head()","execution_count":6,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c1b8b87acf515d9b0263687f42dfae8584ffe0cd"},"cell_type":"code","source":"print('The shape of working data set is:', df.shape, sep='\\n')","execution_count":7,"outputs":[]},{"metadata":{"_uuid":"d917ecb1d86371d335df83ae8e8c01d03b5b3dca"},"cell_type":"markdown","source":"# Duplicate values "},{"metadata":{"_uuid":"556ba4bf412947288911226fc026b5082aa8fb73"},"cell_type":"markdown","source":"If any duplicate values in data sets?"},{"metadata":{"trusted":true,"_uuid":"d1e947cd971e2b7cf4cc6946aa591d7ac8189d14"},"cell_type":"code","source":"df.duplicated().sum()","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"600176ac0541c408b39a033bd6a45055a02c2f96"},"cell_type":"markdown","source":"# Outliers "},{"metadata":{"_uuid":"b37e8686741b1ea20ea223b44c7f27af102243fe"},"cell_type":"markdown","source":"If any outliers in data sets?   \nThis question is not so simple to answer as previous.  "},{"metadata":{"_uuid":"956012961d111dab7c95bfe6b3776831d536f965"},"cell_type":"markdown","source":"> MOST COMMON CAUSES OF OUTLIERS ON A DATA SET:\n1. Data entry errors\n2. Measurement errors\n3. Experimental errors\n4. Data computation errors\n5. Unusual data (not error actually)"},{"metadata":{"_uuid":"d552b45b9309f89f161faae4a524a38078c9787a"},"cell_type":"markdown","source":"There are a lot methods of outliers detection (Z-score, Probabilistic, Linear Reg., etc.)  \nIt Titanic case we can simply visualize the data to find it."},{"metadata":{"trusted":true,"_uuid":"e26136a0cd613486c42db6486edecc005d86cb17"},"cell_type":"code","source":"sns.PairGrid(df).map(plt.scatter)\nplt.show()","execution_count":9,"outputs":[]},{"metadata":{"_uuid":"612d78446945f2a9fb6979e458a78b9e160ac1c9"},"cell_type":"markdown","source":"We can definitely say that there are outliers in 'Fare' column and in 'SibSp'.   "},{"metadata":{"trusted":true,"_uuid":"3d3eff372342a725f136bff0b1e3d5d7d8820eb4"},"cell_type":"code","source":"# Outliers in 'Fare'\ndf[(df.Fare > 300)]","execution_count":10,"outputs":[]},{"metadata":{"_uuid":"294eedbe7dfcb222427163c2a3535f8b0e3fd182"},"cell_type":"markdown","source":"Was that mother, son and assistants?   "},{"metadata":{"trusted":true,"_uuid":"2d196d676f5deb1defd1de0dd748b363bb655128"},"cell_type":"code","source":"# Outliers in 'SibSp'\ndf[(df.SibSp > 7)]","execution_count":11,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9fce59518cb8aaa14fbe67f77152cdc5787da484"},"cell_type":"code","source":"# Let's get more information:\ndf[df['Name'].str.contains('Sage,')]","execution_count":12,"outputs":[]},{"metadata":{"_uuid":"cd6da26f2f458529308408aa8cdbd964e3c231e2"},"cell_type":"markdown","source":"So big family and nobody was survived..."},{"metadata":{"_uuid":"641ccbd8f80f023e98916727497bc8ae42fc8aff"},"cell_type":"markdown","source":"Usually, outliers were dropped.  \nBut I don't do it, because I already know that it will be hidden by binning and feature engineering."},{"metadata":{"_uuid":"744a694410fadff619451556121d3c7ed3ce4bc0"},"cell_type":"markdown","source":"# Data types (deep understanding the data) "},{"metadata":{"_uuid":"104e9673a4922cbb9979e06e9eb5c8270a05a4b7"},"cell_type":"markdown","source":"It wasn't very honest to tell what I'm going to do without explaining why, was it?  \nSorry for that.\nI'll fix it now by explaining my way to understanding the data."},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"c393c7effe1bea4cdb8b10a3052d7d4ae0a164bc"},"cell_type":"code","source":"# I've wrote this simple function to print all data types of data set by column names\ndef print_dtypes(df):\n    \"\"\"\n    :param df: data frame name\n    :return: print lists with column's names by data type\n    \"\"\"\n    cat, fl, integ, time = [], [], [], []\n    cat = df.dtypes[df.dtypes == 'object'].index.tolist()\n    fl = df.dtypes[df.dtypes == 'float64'].index.tolist()\n    integ = df.dtypes[df.dtypes == 'int64'].index.tolist()\n    time = df.dtypes[df.dtypes == 'datetime64'].index.tolist()\n    print('Categorical columns:', cat, '', 'Numerical columns:', 'Floats:', fl, \n          'Integers:', integ, '', 'Time series:', time, sep='\\n')","execution_count":13,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc429b1b26c8bd2a18567b1a0d5e8aa694bb87e4"},"cell_type":"code","source":"print_dtypes(df)","execution_count":14,"outputs":[]},{"metadata":{"_uuid":"d0911aa84718db0cd4c239ba758a3eb589290701"},"cell_type":"markdown","source":"'Integers' is the simpliest type, because it can be numbers only, no missing values etc."},{"metadata":{"trusted":true,"_uuid":"8c57f370ae0ac1237618d0726bf9732aa2df5e28"},"cell_type":"code","source":"for i in ['Parch', 'PassengerId', 'Pclass', 'SibSp']:\n    print(df[i].unique())","execution_count":15,"outputs":[]},{"metadata":{"_uuid":"0d618df6db6b7dda15b9cea30c4bba888938467f"},"cell_type":"markdown","source":"**'PassengerId'** is random unique identifier with we'll need to output data frame, but not needed to modeling."},{"metadata":{"trusted":true,"_uuid":"5e7feaa5d7d7f757180d763c4857cf2ecf9c9baf"},"cell_type":"code","source":"id = raw_test['PassengerId']\nid.shape","execution_count":16,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4afd464dc8711610afb6ba1c72d70f40da7c85b9"},"cell_type":"code","source":"df.drop(['PassengerId'], axis=1)\ndf.shape","execution_count":17,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"093844dfb885f4d2ce661f230dbd55ec26638e4a"},"cell_type":"code","source":"# and again...\ndf = df.drop(['PassengerId'], axis=1)\ndf.shape","execution_count":18,"outputs":[]},{"metadata":{"_uuid":"7bb767d6a84a4927f4385a4cfd09d405f7abbf1c"},"cell_type":"markdown","source":"So simple mistake, but so important to result...   \nI advise to everybody to check any important step - it's much faster than looking for mistakes at the end of work"},{"metadata":{"_uuid":"5dfc24344de23db75d405e7ad4b8915c461cbe06"},"cell_type":"markdown","source":"**'Pclass'** is identifier of ticket class (1 = 1st, 2 = 2nd, 3 = 3rd) => ordinal value (qualitative, not quantitative)\nAt this step we will do nothing with this."},{"metadata":{"_uuid":"7364b3577aa00a7ff7199ace94986c8755822598"},"cell_type":"markdown","source":"**'Parch' and 'SibSp'**  \n    'SibSp' - number of siblings / spouses aboard the Titanic = discrete quantitative  \n    'Parch' -  # of parents / children aboard the Titanic = discrete quantitative  \n\nI'll use this couple for the feature engineering..."},{"metadata":{"trusted":true,"_uuid":"8533fae4f66ceaffbbb8dc0e91fb0d8208b577ca"},"cell_type":"code","source":"for i in ['Age', 'Fare']:\n    print(df[i].describe(), '\\n')","execution_count":19,"outputs":[]},{"metadata":{"_uuid":"a5658fd25a7218041bbef932d2711b011f78f8f6"},"cell_type":"markdown","source":"It's 1309 rows in our data set == both columns have missing values   \n**Age** - passenger's age in years  \n**Fare** - passenger fare  \nBoth columns I'm going to bin"},{"metadata":{"trusted":true,"_uuid":"6493cb0ebaf3eb5fab3d062ae7f6f71766f46146"},"cell_type":"code","source":"for i in ['Cabin', 'Embarked', 'Name', 'Sex', 'Ticket']:\n    print(i, 'has', df[i].nunique(), 'unique values')","execution_count":20,"outputs":[]},{"metadata":{"_uuid":"76223db5ea52fa739cf52e3d7c3149c14f225f9c"},"cell_type":"markdown","source":"**'Cabin'** - cabin number    \n**'Embarked'** - port of embarkation    \n**'Name'** - passenger name    \n**'Sex'** - passenger sex    \n**'Ticket'** - ticket number\n\nDo I need 'Cabin' and 'Ticket' for modeling? It depends of how many missing values here."},{"metadata":{"_uuid":"99799161a39143039400127bfb2e1926f6d40093"},"cell_type":"markdown","source":"# Missing values "},{"metadata":{"_uuid":"23c150014bf7dec1e9ab4e6f082fd66a5f0b2ab1"},"cell_type":"markdown","source":"> There are 2 strategies to work with missing values:\n1. Drop it\n2. Compute and impute"},{"metadata":{"trusted":true,"_uuid":"332a9325c6b9561a3502dd625928fbb88f1f070a"},"cell_type":"code","source":"#\ndf.isnull().sum().sort_values(ascending=False)","execution_count":21,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7198dce5949ea0be301a2cab6bd75c509ea6a199"},"cell_type":"code","source":"print('\\n', round(df['Cabin'].isnull().sum() * 100 / len(df.index)), '% of Cabin values are missing')","execution_count":22,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"f4bb60b25fdd1aab9e17ae9db460ae4b13408a9f"},"cell_type":"code","source":"# So, we can drop it surely\ndf = df.drop(['Cabin'], axis=1)","execution_count":23,"outputs":[]},{"metadata":{"_uuid":"cff281273684480dc1c7a9207d075fbda2382da1"},"cell_type":"markdown","source":"I'm not such kind of person who like drop missing values even if it's 2 rows only - I want to fill it with most used value.   \nI can do it by 2 ways: \n\n** 1 way: use Imputer**  \nThe code would be:  \n    from sklearn.preprocessing import Imputer  \n    imputer = Imputer()  \n    imputed_data = imputer.fit_transform(df)  \nWhy 'would'? Because Imputer doesn't work with categorical values = we can't use it without encoding  \n  \n** 2nd way: hand made imputing **  "},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"1e6264dde354e30d6f092419a59739dd41082aca"},"cell_type":"code","source":"df = df.fillna({'Age': df['Age'].mode()[0],\n                'Embarked': df['Embarked'].mode()[0],\n                'Fare': df['Fare'].mode()[0]\n               })","execution_count":24,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"238e629c522902ad531bfa37d5ac0aac6727ae4e"},"cell_type":"code","source":"# and check, and check\nprint('\\n', 'There are', df.isnull().any().sum(), 'missing values in data frame now.')","execution_count":25,"outputs":[]},{"metadata":{"_uuid":"f355ef91509396c4d508a80bd186143c55f8c473"},"cell_type":"markdown","source":"But what about 'Ticket' - column has no missing values, but 929 unique values only.  \nLet's try to understand more during feature engineering"},{"metadata":{"_uuid":"6fb846ee2f7721496dd2d1a8d58c87a19a494118"},"cell_type":"markdown","source":"# Feature engineering (and feature selection, and...)"},{"metadata":{"trusted":true,"_uuid":"3d1df1e634b5a4679d5c855f3f8c5a1d09abb41b"},"cell_type":"code","source":"# take a look to the first 13 group tickets\ndf.Ticket.value_counts()[:13]","execution_count":26,"outputs":[]},{"metadata":{"_uuid":"28890c05772bf3830709c6158d7004e6bdf1674d"},"cell_type":"markdown","source":"We can recognize the first value immediately - CA. 2343 is ticket of unlucky Sage family."},{"metadata":{"trusted":true,"_uuid":"fc04bf0ea1bad5bebb9fab656bfcfbc56670609f"},"cell_type":"code","source":"# How many group tickets do we have?\n(df.Ticket.value_counts() > 1).value_counts()","execution_count":27,"outputs":[]},{"metadata":{"_uuid":"9e620c1ef63702e3b2c654971a138c6ba10701b0"},"cell_type":"markdown","source":"So, 216 tickets were group tickets.   \nAnd... does it mean that Fare values for this tickets are wrong and I should divide every value by numbers of person? Shoul we play here more?  \nWhat do you think?  "},{"metadata":{"_uuid":"7529d742dbd9a1730fbc094c34dd185547e60f37"},"cell_type":"markdown","source":"**'Parch' and 'SibSp'**  - what statistically important meanings are hidden here?  \nI'm going to create 'IsAlone' column and collect family size information as 'FSize'."},{"metadata":{"trusted":true,"_uuid":"9343033916a3a502f526f4fb32ab59406c56677a"},"cell_type":"code","source":"df['FSize'] = (df['SibSp'] + df['Parch'] + 1).astype(int)\ndf.FSize.unique()","execution_count":28,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"85529bf1662e4e06ae8e7fdd65fc4c3940aa1df1"},"cell_type":"code","source":"df['IsAlone'] = 1  # as 1 == yes, is alone\ndf['IsAlone'].loc[df['FSize'] > 1] = 0 \ndf.IsAlone.unique()","execution_count":29,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"0034506deb72a215c50c2a3742d4a116802bb45c"},"cell_type":"code","source":"# I think, we don't need SibSp and Parch for modeling any more, so...\ndf = df.drop(['SibSp', 'Parch'], axis=1)","execution_count":30,"outputs":[]},{"metadata":{"_uuid":"1ac4df340eaeb5ad1e268f01e91fa1b81882f51e"},"cell_type":"markdown","source":"Now I'm going to cut continuous quantitatives - **'Age' and 'Fare'**:"},{"metadata":{"trusted":true,"_uuid":"9eadf586be687f5308b54f6b3acba38bf2373a37"},"cell_type":"code","source":"df.Age.describe()","execution_count":31,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"50a4d42bdcd80c0969c8e944b42b16e9306f0c4d"},"cell_type":"code","source":"# I divide max by std and so decide about bins amount\n# more infomatio about pd.cut function here - https://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html\nlabels = [0, 1, 2, 3, 4, 5]\ndf['BinnedAge'] = pd.cut(df['Age'], bins=6, labels=labels, include_lowest=True)\ndf['Age'] = df['BinnedAge'].astype(int)\nprint('\\n', 'Age uniques:', df.Age.unique())","execution_count":32,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"40db0122a7c9adb70cb6e70471a7c88409a324b9"},"cell_type":"code","source":"# Do the same for 'Fare':\ndf.Fare.describe()","execution_count":33,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"650fa2d1a6c403db08462276aea7f40ce4b53bcf"},"cell_type":"code","source":"labels = [0, 1, 2, 3, 4, 5]\ndf['BinnedFare'] = pd.qcut(df['Fare'], 6, labels=labels)\ndf['Fare'] = df['BinnedFare'].astype(int)\nprint('\\n', 'Fare uniques:', df.Fare.unique())","execution_count":34,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"e79aa4f352ba51be344d6e8d241759095c641370"},"cell_type":"code","source":"# delete unneсeыsary columns\ndf = df.drop(['BinnedAge', 'BinnedFare'], axis=1)","execution_count":35,"outputs":[]},{"metadata":{"_uuid":"f4c9a47fc770a1d2b70a3fb81f9eed253fea05f1"},"cell_type":"markdown","source":"**'Name'**  \nFrom this column we can extract information about passenger title:"},{"metadata":{"trusted":true,"_uuid":"c2babf6630e8e6b96e46b55c7f2279ff52d659d7"},"cell_type":"code","source":"# extracting (title information goes after comma and before dot)\ndf['Title'] = df['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n# checking \ndf['Title'].value_counts()","execution_count":36,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3c2653caed863a684bc169218135af1174dbff3d"},"cell_type":"code","source":"# Let's create 'other' for rare values:\ntitles = (df['Title'].value_counts() < 10)\ndf['Title'] = df['Title'].apply(lambda x: 'other' if titles.loc[x] == True else x)\ndf['Title'].value_counts()","execution_count":37,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"1cf2c33c709cf07cb19f9d39d031587310cbd35f"},"cell_type":"code","source":"# We don't need 'Name' any more\ndf = df.drop(['Name'], axis=1)","execution_count":38,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"27bb1ada89df0e45e1afaa933eaaf4ca180dfff6"},"cell_type":"code","source":"# Let's check how is the data frame now:\ndf.head()","execution_count":39,"outputs":[]},{"metadata":{"_uuid":"295605e25349b7dc13ae406969683474b034c362"},"cell_type":"markdown","source":"That 'Ticket'!"},{"metadata":{"_uuid":"87c163041ea1d7053c3ea2264c0686e04cb84eb1"},"cell_type":"markdown","source":"# Encoding "},{"metadata":{"_uuid":"73f4e755b1c9e20ff731ef4dc7ab2db56ac0dc77"},"cell_type":"markdown","source":"What do you decide about 'Ticket'?  \n\nI still not sure for deleting...\nLet's label it by Label Encoder (convert text values to numbers)"},{"metadata":{"trusted":true,"_uuid":"ab0e522910c95a1e522923833a146731e86d4cbd"},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ndf['Ticket'] = le.fit_transform(df['Ticket'])\ndf['Ticket'].unique()","execution_count":40,"outputs":[]},{"metadata":{"_uuid":"70e9c4785aab89710d0e1f3a306155f3a426b22c"},"cell_type":"markdown","source":"Why we need to encode all qualitative data to numbers?  \nWe do it for modeling (some model work with numerical values only)\n\nI prefer to do it \"by hand\" in such simple data sets:"},{"metadata":{"trusted":true,"_uuid":"fa930080f3a128e3ca312589eb1c7a5d09116c55"},"cell_type":"code","source":"df.Title.unique()","execution_count":41,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"94accd8685fcc6dde444732e6b45cbe028dd0ea8"},"cell_type":"code","source":"df['Title'] = df.Title.map({'Mr': 0,  'Mrs': 1,  'Miss': 2,  'Master': 3,  'other': 4}).astype(int)","execution_count":42,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f35a5a8c4340eb334a48b61f009f8eb448fe759d"},"cell_type":"code","source":"df.Embarked.unique()","execution_count":43,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"aae94410daefab42aed7c6a0a49f6a198e3fe7d0"},"cell_type":"code","source":"df['Embarked'] = df.Embarked.map({'S': 0, 'C': 1, 'Q': 2}).astype(int)","execution_count":44,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"848ce30adb368043f7cb6097b905c629efe04e93"},"cell_type":"code","source":"df.Sex.unique()","execution_count":45,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"94be53b25d5a47896f33a579b3f2cbb925e4209f"},"cell_type":"code","source":"df['Sex'] = df.Sex.map({'male': 1, 'female': 0}).astype(int)","execution_count":46,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cc8e936d0b06555dc9d0b1b7506f0188951a35ce"},"cell_type":"code","source":"df.head()","execution_count":47,"outputs":[]},{"metadata":{"_uuid":"4085bae8c6faee784b86e5a41d690736b4421c82"},"cell_type":"markdown","source":"# Final check and final thoughts "},{"metadata":{"_uuid":"63edcbc188640ab147ed344f2b81495203d92d1d"},"cell_type":"markdown","source":"\\Looks like we finished, but we are not.  \nNow we should split data set back and prepare X and y for future modeling."},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"adcc388d4ed17eb68de7a851bd24255641afe4ef"},"cell_type":"code","source":"# get a raw data sets q-ty of rows:\nte, tr = raw_test.shape[0], raw_train.shape[0]","execution_count":48,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"47b3ba5b2225e24baa288e78f5ae8c4d15426757"},"cell_type":"code","source":"# splitting back:\ntrain = df[:tr]\ntest = df[tr:]","execution_count":49,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1fd06bb1d233342f83e17b3026c732a2a798cf8d"},"cell_type":"code","source":"X = train.values\ny = target\nprint('Train set for modeling shape is:', X.shape, 'Target shape is:', y.shape, sep='\\n')","execution_count":50,"outputs":[]},{"metadata":{"_uuid":"e1cecb166e927b11488e662bc76afb3c1d0274db"},"cell_type":"markdown","source":"Because I'll not show modeling part in this notebook, I'll prepare the data frame for submission now:"},{"metadata":{"trusted":true,"_uuid":"73440788bfd64b5f53680c793c788f4e4d30b14b"},"cell_type":"code","source":"result = pd.DataFrame()\nresult['PassengerId'] = raw_test['PassengerId'].astype('int')\nresult.shape","execution_count":51,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"7b05c1470c6f2db4f8a9d6dc154de07fb7a82357"},"cell_type":"code","source":"# After modeling you'll add column...\n#result['Survived'] = y_pred.astype('int')\n\n# ... and save file...\n# result.to_csv(\"Titanic_prediction.csv\", index=False)\n\n# ... and do not forget to check before submission\n# print(result.shape, result.head(), sep='\\n')","execution_count":52,"outputs":[]},{"metadata":{"_uuid":"26758b9af619b33281f1116773785fdac11cfaf7"},"cell_type":"markdown","source":"That's it!  \n\nRemember, I am new here, so any feedback is very, very welcome!  \nAnd english language is not my native - sorry me, if I use it in crazy way sometimes.\n\nBy the way, if you're an experienced Data Scientist and you don't have enough time for cleaning data for all your projects, I may help (my Upwork profile here - https://www.upwork.com/o/profiles/users/_~014dcb1a76dd7f7892/).\n\nThank you for your time!\n\nwbr,  \nLana"},{"metadata":{"_uuid":"925120020dcb3f29997476998a3b32b181d3f2e3"},"cell_type":"markdown","source":"P.S.  \nI didn't show any visualization here, because of the topic.  \nIt's coming soon as a separate notebook.   \nHere, on Kaggle."},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"6e48cb89ee794803b571b9b57b9395cb6b320696"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}