{"cells":[{"metadata":{"_cell_guid":"34873a1e-b89b-4c2b-aba5-2da94e26b33a","_uuid":"91526dfeb69024aebc06166d2c7fd2861c23d2f3"},"cell_type":"markdown","source":"# Minimalistic Titanic in Python LightGBM\n\nThis notebook investigates working with the Titanic set using only the **Sex, Embarked and Pclass** features for prediction. \n\nThis is a supporting kernel for this other [**kernel**](https://www.kaggle.com/pliptor/how-am-i-doing-with-my-score). \n\nYou can find a [**similar kernel in R using XGBoost**](https://www.kaggle.com/pliptor/minimalistic-xgb/notebook). We will use LightGBM. \n\n1. [Reading data](#read_data)\n2. [Label Encoding](#label_enc)\n3. [Modeling](#modeling)\n4. [Predicting and creating a submission file](#submission)\n\n[Conclusions](#conclusions)\n\n"},{"metadata":{"_cell_guid":"91605170-52e3-4c0b-8f31-ffd11a4238b5","_uuid":"64ecdfa33ca527b7846f032b1a5c32faddd0aacb"},"cell_type":"markdown","source":"# 1) Reading data <a class=\"anchor\" id=\"read_data\"></a>\n\nWe will only read the relevant columns of the data. It will keep the data clean and prevent any accidental leakage of other features into our setup."},{"metadata":{"_cell_guid":"589607b7-854a-44b0-8bc5-6b23192a0b1b","_uuid":"3c94428e0f3ad420deb240165888f0a1b1643978","trusted":false,"collapsed":true},"cell_type":"code","source":"import pandas as pd\nimport numpy  as np\n\nnp.random.seed(2018)\n\nfeature_names = ['Sex','Embarked','Pclass']\n\n# load data sets \ntrain = pd.read_csv('../input/train.csv', usecols =['Survived','PassengerId'] + feature_names)\ntest  = pd.read_csv('../input/test.csv', usecols =['PassengerId'] + feature_names )\n\n# combine train and test for joint processing \ntest['Survived'] = np.nan\ncomb = pd.concat([ train, test ])\ncomb.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0f9044fc-e3de-441c-b251-bdcec30c0e05","_uuid":"40cd0490107523a6c631ce07047b8c7bbadeef34"},"cell_type":"markdown","source":"Let's also fix two missing values in Embarked"},{"metadata":{"_cell_guid":"2161916c-2fff-47ca-9eac-40ca38b54a82","_uuid":"b9bc43a48ec0b288b4fe00222bb0beef2624f648","trusted":false,"collapsed":true},"cell_type":"code","source":"print('Number of missing Embarked values ',comb['Embarked'].isnull().sum())\ncomb['Embarked'] = comb['Embarked'].fillna('S')\ncomb['Embarked'].unique()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"40f5d07b-6877-4120-89da-890fa79142f5","_uuid":"7c0392af2ed2decdcb712fcc098cbdd3864ddf1a"},"cell_type":"markdown","source":"# 2) Label Encoding <a class=\"anchor\" id=\"label_enc\"></a>\n\nThe following steps encode non-numeric features to numeric ones. "},{"metadata":{"_cell_guid":"626d8237-1aba-4b92-9682-f6f05bb7858f","_uuid":"990f1b21f0300cfadf707bc2828236dcb919750e","trusted":false,"collapsed":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ncomb.Embarked = le.fit_transform(comb.Embarked)\ncomb.Sex      = le.fit_transform(comb.Sex)\ncomb.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"452dcbd6-7f94-488a-af02-bf841614c67a","_uuid":"0648803a252da233671479beadcaade4d863778f"},"cell_type":"markdown","source":"Now we split back comb2 as we are done pre-processing"},{"metadata":{"_cell_guid":"da2ebf0f-15d1-4025-821f-42609c02c356","_uuid":"70e931a92a991c6657f2ce6ab8f06f573af4d814","trusted":false,"collapsed":true},"cell_type":"code","source":"df_train = comb.loc[comb['Survived'].isin([np.nan]) == False]\ndf_test  = comb.loc[comb['Survived'].isin([np.nan]) == True]\n\nprint(df_train.shape)\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ce93fdfa-3d48-4f92-b22d-59ee0c274214","_uuid":"75d60b8163ecb62d7cf857d3e8a73388e32738b2","trusted":false,"collapsed":true},"cell_type":"code","source":"print(df_test.shape)\ndf_test.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7f38ca5a-847a-4ecb-9735-510c7d3c1201","_uuid":"a7ec3d1fc4824e6bb9085168784c72d362170f62"},"cell_type":"markdown","source":"We are now ready for modeling!"},{"metadata":{"_cell_guid":"1402fa11-d227-4241-825d-79c29add910a","_uuid":"a44800feb2e521f063bf69bd366d0cc4d7e54369"},"cell_type":"markdown","source":"# 3) Modeling <a class=\"anchor\" id=\"modeling\"></a>\n\nWe will use a LightGBM for the model and use GridSearchCV to tune it."},{"metadata":{"collapsed":true,"_cell_guid":"65a0a04d-b799-4860-9741-71503d98591f","_uuid":"c1545d6e83bb671c16e3fc5bf763719f25423229","trusted":false},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7ca96edc-5aba-419e-bb81-bf25d68b524f","_uuid":"c013280fdc14fcdd12a153d8104647ff15adccbd","trusted":false,"collapsed":true},"cell_type":"code","source":"from lightgbm import LGBMClassifier\nmodel = LGBMClassifier()\nparam_grid = {'n_estimators':[20],'max_depth':[2,3,4]} \ngrs = GridSearchCV(model, param_grid=param_grid, cv = 10, n_jobs=4, return_train_score = False)\ngrs.fit(np.array(df_train[feature_names]), np.array(df_train['Survived']))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"44fd1eb7-c903-4473-926f-1ec39ee8136f","_uuid":"da79be9cecf346cfbff933cb68c4614ac1b627fc"},"cell_type":"markdown","source":"Now that the tuning is completed, we print the best parameter found and also the estimated accuracy for the unseen data.  "},{"metadata":{"_cell_guid":"d4df18e9-67d6-4b6e-9acf-928de370af5b","_uuid":"16a9cce6e30725c3ec6cd3a476db8a78cb1a3ae8","trusted":false,"collapsed":true},"cell_type":"code","source":"print(\"Best parameters \" + str(grs.best_params_))\ngpd = pd.DataFrame(grs.cv_results_)\nprint(\"Estimated accuracy of this model for unseen data:{0:1.4f}\".format(gpd['mean_test_score'][grs.best_index_]))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f793fdbe-9181-437f-b568-9299ad8cb8e6","_uuid":"8df6f77c3d7fbf4012e2e9654c2635fc339bf8c9"},"cell_type":"markdown","source":"# 4) Predicting and creating a submission file<a class=\"anchor\" id=\"submission\"></a>"},{"metadata":{"_cell_guid":"2bcd8315-0635-4ff1-9897-51f1220ac21d","_uuid":"f282d29f2eccd44722894e82db84ccae50c89d8f","trusted":false,"collapsed":true},"cell_type":"code","source":"pred = grs.predict(np.array(df_test[feature_names]))\n\nsub = pd.DataFrame({'PassengerId':df_test['PassengerId'],'Survived':pred})\nsub.to_csv('minimalistic.csv', index = False, float_format='%1d')\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bf27d500-9148-4220-b2df-4d8282d6020a","_uuid":"97bc84ccb0fd600f42713bbfe930eece36fb00cb"},"cell_type":"markdown","source":"# Conclusions <a class=\"anchor\" id=\"conclusions\"></a>\n\nWe tackled the Titanic problem using only the **Sex, Embarked and Pclass** features. You should get a public score of **0.77990**. It is argued in [**this kernel**](https://www.kaggle.com/pliptor/how-am-i-doing-with-my-score) that the public score is about 2% lower than cross validation estimates for unseen data when the model heavily relies on the Gender and Pclass features. This kernel supports the findings. The cross validation for unseen data is 0.8114.\n\nThanks for reading this kernel and let me know if you have any questions or comments!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}