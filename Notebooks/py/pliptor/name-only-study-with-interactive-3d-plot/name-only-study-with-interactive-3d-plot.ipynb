{"cells":[{"metadata":{"_uuid":"5f99e7dae1d1c9769ae7df2ffff813f94853bd08"},"cell_type":"markdown","source":"# What score can we expect using just the Name feature?\n\nThis notebook investigates working with the Titanic set using only the **Name** feature for prediction. \n\nThis is motivated by an attempt to understand how each feature alone may contribute to the\nprediction of survival. Doing so for simple features such as gender (public score 0.76555) or passenger class (public score 0.6550) poses no problem and we know those scores are the optimum solutions under those restrictions. Other examples can be found [**here**](https://www.kaggle.com/pliptor/how-am-i-doing-with-my-score). Single feature prediction is not so straightforward for features such as **Name** or **Cabin** because or their more complex structure. \n\nInstead of the typical approach of finding *titles* in the Name field, identifying families and \nmanually creating engineered features, we will use a more \"machine learning\" automated approach of vectorizing the name field with the aid of sklearn.feature_extraction.text. By doing so, we will detach ourselves from the semantics of the words and use just word extraction and counting statistics. \n\nThe Name field is initially expanded into a sparse 600+ dimensional space. We will demonstrate how to reduce the dimension of the problem using principal component analsys **(PCA)** to a more manageable 6-dimensional problem. This would also allow us to [**interactively visualize**](#3dplot) the Name feature in 3 dimensions! \n\nFinally, we will apply the **KNeighbors** algorithm solve it. We will optimize the KNeighbors parameter with GridSearch and also compute the cross validation to estimate the expected public score. Those familiar with this data set know that the gender can be extracted from the Name field. We therefore expect this approach to perform no worse than the gender-only approach. Are we going to succeed?\n\nThe picture below is a snapshot of the [interactive visualization](#3dplot) in section 2. You may use your mouse to rotate the structure and change its perspective.You'll be able to see clusters of survival and death that have been extracted just from the Name feature!\n\n![PCA](https://kaggle2.blob.core.windows.net/forum-message-attachments/281004/8555/newplot.png)\n\n## Contents\n\n1. [Reading data](#read_data)\n2. [Vectorizing the Name feature and interactive 3D visualization](#vectorizing)\n    * [Using PCA to reduce dimension](#pca)\n    * [Interactive 3D Visualization](#3dplot)\n3. [Modeling](#modeling)\n4. [Predicting and creating a submission file](#submission)\n\n[Conclusions](#conclusions)\n\n","outputs":[],"execution_count":null},{"metadata":{"_uuid":"913ef5c0d9816b3ae180a77b72d255ecb9ffb804"},"cell_type":"markdown","source":"# 1) Reading data <a class=\"anchor\" id=\"read_data\"></a>\n\nWe will only read the relevant columns of the data. It will keep the data clean and prevent any accidental leakage of other features into our setup.","outputs":[],"execution_count":null},{"metadata":{"trusted":false,"_uuid":"de77ada431e456329d9c7a6ca31d74d45fdb16f1"},"cell_type":"code","source":"import pandas as pd\nimport numpy  as np\n\nnp.random.seed(2018)\n\n# load data sets \ntrain = pd.read_csv('../input/train.csv', usecols =['Survived','PassengerId','Name'])\ntest  = pd.read_csv('../input/test.csv', usecols =['PassengerId','Name'])\n\n# combine train and test for joint processing \ntest['Survived'] = np.nan\ncomb = pd.concat([ train, test ])\ncomb.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"061a2c943a31beae26ddb9fb1ba793eaecd1d418"},"cell_type":"markdown","source":"# 2) Vectorizing the Name feature and interactive 3D visualization <a class=\"anchor\" id=\"vectorizing\"></a>\n\nThe following steps vectorizes the Name feature using sklearn.feature_extraction.text. It transform a text \nfeature into numeric format.\n\nWe first create a pre-processing filter \"clean_name\" to remove punctuations out of the Name feature. ","outputs":[],"execution_count":null},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"82e2f549070282784b12bd9f0d34cfbad45aa96c"},"cell_type":"code","source":"# define a filter for the name field to remove punctuations and single letters\ndef clean_name(x):\n    x = x.replace(',',' ').replace('.',' ').replace('(',' ').replace(')',' ').replace('\"',' ').replace('-',' ')\n    return ' '.join([ w for w in x.split() if len(w)> 1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2c762761d8681de45883be0b331976bd4af80ed9"},"cell_type":"markdown","source":"## 2.1) Defining the vectorizer\n\nTwo parameters will be passed to the vectorizer; the clean_name function above and min_df=2. What the latter does is \nto make the vectorizer look at words that repeat at least twice in the entire data set. This way we remove all outliers that won't help with the statistics and will prevent model overfitting.","outputs":[],"execution_count":null},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"c84aa73c8c569ce6f0eb6e46390590e84874eb5d"},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n# setting punctuation filter and a minimum of terms to 2. \ncount_vect = CountVectorizer(preprocessor=clean_name, min_df=2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"71710b956ddd8d287441cc1bc21a146097b9ff49"},"cell_type":"markdown","source":"## 2.2) Fitting the vectorizer\n\nIt is fairly simple to fit the vectorizer into the Name feature! Note what it does is to build a dictionary of words (or tokens) and associate a unique number to it. Aaron corresponds to 0, Abbott to 1, etc. There are 638 distinct tokens. We print 6 terms in the dictionary for inspection.","outputs":[],"execution_count":null},{"metadata":{"trusted":false,"_uuid":"d5654d7314516ad65e89fb7d6c71a3da047625aa"},"cell_type":"code","source":"# the following assigns a unique number for each word in the Name feature\ncount_vect.fit(comb['Name'])\nfor i,k in enumerate(count_vect.vocabulary_):\n    if i>5:\n        break\n    else:\n        print(k, count_vect.vocabulary_[k])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6f0cc72ba2fb3c5b186347b97ff866b8cfd10e41"},"cell_type":"markdown","source":"## 2.3) Transform the Name feature\n\nNow we transform the Name feature to an array that indicates which word is included in each passenger's Name feature.","outputs":[],"execution_count":null},{"metadata":{"trusted":false,"_uuid":"816c98dc734b42def469e5b29d45859735886c1b"},"cell_type":"code","source":"# the following transforms the Name feature to a vector indicating which word they contain\nv = count_vect.transform(comb['Name'] )\nva = np.array(v.toarray())\nva.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bf5dfc5a76ca91208f282296448282b6deb9eefb"},"cell_type":"markdown","source":"## 2.4) Using PCA to reduce dimension <a class=\"anchor\" id=\"pca\"></a>\n\nAt this point we transformed the Name feature to a 638 dimension vector. While a previous version of this notebook posed no problem handling this size, we will demonstrate a dimension reduction technique using PCA to 6 dimensions. The reduced dimension will also alow us to inspect it in an interactive 3D plot (We will project only 3 dimensions out of 6 for this effect).","outputs":[],"execution_count":null},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"022334ef71fd4f2cd205740e4535ed690f86bec3"},"cell_type":"code","source":"from sklearn.decomposition import PCA\n\n#reducing va to 6 dimensions using PCA\npca = PCA(n_components = 6, random_state = 2018, whiten = True)\nva = pca.fit_transform(va)\n\n# we will plot only those that have a Survived label\nva_survived = va[comb.index[comb.loc[:,'Survived']==1],...] \nva_perished = va[comb.index[comb.loc[:,'Survived']==0], ...]\nva_na       = va[comb.index[comb.loc[:,'Survived']== np.nan],...]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b1327b95c9e9b971fb4abb53562699e486ad959b"},"cell_type":"markdown","source":"## 2.5) 3D visualization <a class=\"anchor\" id=\"3dplot\"></a>\nWe will use plotly for 3D visualization.","outputs":[],"execution_count":null},{"metadata":{"trusted":false,"_uuid":"6d35cd3afd1a0fad1a6f92fa2d1a6efc4d5d0928"},"cell_type":"code","source":"import plotly.offline as py\nimport plotly.graph_objs as go\npy.init_notebook_mode(connected=True)\n\ntrace1 = go.Scatter3d(\n    x=va_survived[:,1],\n    y=va_survived[:,2],\n    z=va_survived[:,3],\n    mode='markers',\n    marker=dict(\n        size=12,\n        line=dict(\n            color='rgba(217, 217, 217, 0.14)',\n            width=0.5\n        ),\n        opacity=1\n    ),\n    name = 'Survived'\n)\ntrace2 = go.Scatter3d(\n    x=va_perished[:,1],\n    y=va_perished[:,2],\n    z=va_perished[:,3],\n    mode='markers',\n    marker=dict(\n        size=6,\n        line=dict(\n            color='rgba(217, 217, 217, 0.14)',\n            width=0.5\n        ),\n        opacity=1\n    ),\n    name = 'Perished'\n)\ntrace3 = go.Scatter3d(\n    x=va_na[:,1],\n    y=va_na[:,2],\n    z=va_na[:,3],\n    mode='markers',\n    marker=dict(\n        size=6,\n        line=dict(\n            color='rgba(217, 217, 217, 0.14)',\n            width=0.5\n        ),\n        opacity=1\n    ),\n    name = 'Perished'\n)\ndata = [trace1, trace2, trace3]\nlayout = go.Layout(\n    margin=dict(\n        l=0,\n        r=0,\n        b=0,\n        t=0\n    )\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"08210bfa1e8a896de2893d415b50b401dd5ac0aa"},"cell_type":"markdown","source":"Clusters of survial and death can be seen. Rembember this is all being done without any semantic intepretation of names or titles. It is was just done identifying unique terms and counting them. We do see that some points in different classes are almost overlapping.","outputs":[],"execution_count":null},{"metadata":{"_uuid":"98cbfa4e8fd4909ed501469be5d585b0e0e6e234"},"cell_type":"markdown","source":"## 2.6) Build a Name dataframe\n\nLet's now build a dataframe from the above array and name each column from 0 to 5 (each PCA dimension after reduction).","outputs":[],"execution_count":null},{"metadata":{"trusted":false,"_uuid":"e2b478e3c69dcbfcf05f1645bc716551726075d8"},"cell_type":"code","source":"# build a DataFrame from the array\nfeature_names = ['Nv' + str(i) for i in range(va.shape[1])]\nNameVect = pd.DataFrame(data = va, index = comb.index, columns = feature_names)\nNameVect.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"813f6599f4be77d67891d76e117c4dd9fc28ec2a"},"cell_type":"markdown","source":"## 2.7) Build df_train and df_test data frames\n\nFirst we concatenate the survived target to the NameVect table and create a new dataframe","outputs":[],"execution_count":null},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"54291e953f44232664b6dca09ed57c62fc067408"},"cell_type":"code","source":"# comb2 now becomes the combined data in numeric form\ncomb2 = pd.concat([comb[['Survived']], NameVect],axis =1)\ncomb2.head()\ncomb2.to_csv('name_only_df.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"482adb6ad618a3f1fb674c7055838204a2847c17"},"cell_type":"markdown","source":"Now we split back comb2 as we are done pre-processing","outputs":[],"execution_count":null},{"metadata":{"trusted":false,"_uuid":"7161c5f7a7b4f860c6ce5665884eb0c68a2c2dca"},"cell_type":"code","source":"df_train = comb2.loc[comb2['Survived'].isin([np.nan]) == False]\ndf_test  = comb2.loc[comb2['Survived'].isin([np.nan]) == True]\n\nprint(df_train.shape)\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3febf86fe743a34b9092b12cef71143d774f7893"},"cell_type":"code","source":"print(df_test.shape)\ndf_test.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"14c65d6bae3c6db90bf78c3b3ca716c3fcd958f5"},"cell_type":"markdown","source":"We are now ready for modeling!","outputs":[],"execution_count":null},{"metadata":{"_uuid":"55d4bb0d2b2bf74dbaa6a1f8012a627e7754a128"},"cell_type":"markdown","source":"# 3) Modeling <a class=\"anchor\" id=\"modeling\"></a>\n\nWe will use a KNeighborsClassifier for the model and use GridSearchCV to tune it.","outputs":[],"execution_count":null},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"bbfa633457de00a54b4a7e0fbf8c1df42a786d0a"},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"43c0404d3d8a9340b97c6da14d088429e91c17d1"},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nknclass = KNeighborsClassifier(n_neighbors=11, metric = 'manhattan')\nparam_grid = ({'n_neighbors':[3,4,5,6,7,8,9,11,12,13],'metric':['manhattan','minkowski'],'p':[1,2]}) \ngrs = GridSearchCV(knclass, param_grid, cv = 28, n_jobs=1, return_train_score = True, iid = False)\ngrs.fit(np.array(df_train[feature_names]), np.array(df_train['Survived']))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2b6a93d8d5f0ab81f2bba9968b64c68b926d5b08"},"cell_type":"markdown","source":"Now that the tuning is completed, we print the best parameter found and also the estimated accuracy for the unseen data.  ","outputs":[],"execution_count":null},{"metadata":{"trusted":false,"_uuid":"beab84d5d1911d44877b341a9145aeaf7f0e096d"},"cell_type":"code","source":"print(\"Best parameters \" + str(grs.best_params_))\ngpd = pd.DataFrame(grs.cv_results_)\nprint(\"Estimated accuracy of this model for unseen data:{0:1.4f}\".format(gpd['mean_test_score'][grs.best_index_]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1b3634a994026812e4923574883b1107ef2a09c5"},"cell_type":"markdown","source":"# 4) Predicting and creating a submission file<a class=\"anchor\" id=\"submission\"></a>","outputs":[],"execution_count":null},{"metadata":{"trusted":false,"_uuid":"61fa92eadabf4ff61313f8e2151034dddffdce3f"},"cell_type":"code","source":"pred_knn = grs.predict(np.array(df_test[feature_names]))\n\nsub = pd.DataFrame({'PassengerId':test['PassengerId'],'Survived':pred_knn})\nsub.to_csv('name_only_knn.csv', index = False, float_format='%1d')\nsub.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7001898076d84bd4665819f148342d9ba5afdfd7"},"cell_type":"markdown","source":"# Conclusions <a class=\"anchor\" id=\"conclusions\"></a>\n\nWe tackled the Titanic problem using only Name feature processing. The Name feature was transformed to a 600+ dimensional binary matrix using a count vectorizer in sklearn.feature_extraction.text. Next we reduced the dimension of the data matrix to 6 by using PCA. The prediction was then made with a KNneighbor optimizer tuned with a cross-validated grid search. As a by-product, we found the estimated accuracy for unseen data to be 0.7820. The obtained public score is 0.78468, which is higher than the gender-only optimal solution (0.76555) as we were hoping for!\n\nFuture work: Unfortunately we can't guarantee this solution for Name-only is optimal. It might be still possible to improve it both from the perspective of vectorization procedure and from KNneighbor optimization.\nI hope to do it at some time. \n\nPlease let me know if you have questions or comments!\n","outputs":[],"execution_count":null}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":1}