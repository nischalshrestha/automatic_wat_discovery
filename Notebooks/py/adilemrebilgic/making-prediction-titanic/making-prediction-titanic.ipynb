{"cells":[{"metadata":{"_uuid":"fcd968beb2ee71a8efa0b01df97aac027288b311"},"cell_type":"markdown","source":"Titanic dataset is analyzed by following these steps below:\n1. Read Data\n1. Descriptive Stats\n1. Visualize\n1. Handle missing values\n1. Create Dummies\n1. Standardize\n1. Train Classifier\n"},{"metadata":{"_uuid":"a419e59aaf0e18eb5f5e82b832e017feeb8c13e8","_cell_guid":"f5fd05ce-7691-4a48-a4df-20928e722a1b","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom sklearn.metrics import confusion_matrix,precision_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression,SGDClassifier\nfrom sklearn import svm\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import RandomForestClassifier,VotingClassifier \n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3d089333526be5dfee6b126e9797bdb43c27a82a"},"cell_type":"markdown","source":"1-Reading Data"},{"metadata":{"trusted":true,"_uuid":"3da03e505f18ee49d4f925ede0f8ef90f2dd7caf"},"cell_type":"code","source":"trainData=pd.read_csv('../input/train.csv', index_col=0)\ntestData=pd.read_csv('../input/test.csv', index_col=0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d53c4d6987b084cffa97adbbd9eff178be333b5"},"cell_type":"markdown","source":"2-Descriptive Stats"},{"metadata":{"trusted":true,"_uuid":"f8777fedc8ad2ea11c000d1a76606387b00d55a9"},"cell_type":"code","source":"#Shape of data\nprint('Number of obs: ',trainData.shape[0])\nprint('Number of features: ',trainData.shape[1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"24f62d3adf85d8ab3414f93dbcb8aa97a66f56b2"},"cell_type":"markdown","source":"2-1-Printing first 5 rows"},{"metadata":{"trusted":true,"_uuid":"463b53f36a450faaf5efb2d2e6b2341d6c5ad0e2"},"cell_type":"code","source":"trainData.head().T","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f9435eb8ce65bb39bc0fe1be9cb295bfc8b16923"},"cell_type":"markdown","source":"2-2-Listing features, number of non-null observations and datatypes"},{"metadata":{"trusted":true,"_uuid":"2ce34ffb5ecda2d10bb935ecef97b2d02e962100"},"cell_type":"code","source":"trainData.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f6a2afc1a815ee5397fdfe40fe7b93360fb68ccc"},"cell_type":"markdown","source":"2-3-Calculating Descriptive Stats"},{"metadata":{"_uuid":"7e5763c3cf46f2075846086805f16956925827e2"},"cell_type":"markdown","source":"You can see the descriptive statistics for both numeric and categoric variables."},{"metadata":{"trusted":true,"_uuid":"dc95474751f975079f780947eb0461e56ed23186"},"cell_type":"code","source":"trainData.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8fa85b3e36c0d8f5b191e45d0379c7af88db387f","collapsed":true},"cell_type":"markdown","source":"Looks like there are some irrelavant variables in our dataset. Name, Ticket, Cabin variables can be removed and rest of the data can be grouped as  numeric, categoric and target.\n* Numeric Features:Age, Fare, SibSp, Parch\n* Categoric Features: Survived, Pclass, Sex, Embarked\n* Target:Survived"},{"metadata":{"_uuid":"50df084cbca98b52e1da2dd214a51c775ac61b31"},"cell_type":"markdown","source":"3-Visualizing Data"},{"metadata":{"_uuid":"a6ed0fd23dbeac50a7cfa67d384d985cba0f92ea"},"cell_type":"markdown","source":"3-1-Visualizing Numeric Values\n* You can create 3 different type of plot. You can see general data distribution on histogram plots. Scatter plots are generally used for relation between 2 variables but you can use it to have general view of data with comparing with percentile values. This scatter graph and box plot can also give you information about extreme values."},{"metadata":{"trusted":true,"_uuid":"937b9641f6b74efe94ca13e28ec6d2054a1bbab2"},"cell_type":"code","source":"#Visualize Numeric Vals\n##Age\nfig=plt.figure()\n\n###\nax1=fig.add_subplot(4,3,1)\nax1=trainData['Age'].plot(kind='hist',\n         title='Age',\n        \n         grid=True,\n        figsize=(25,20))\n\n###\nax2=fig.add_subplot(4,3,2)\nax2=trainData['Age'].plot(kind='line',\n         title='Age',\n         style='o',\n         grid=False,\n         alpha=0.5,figsize=(25,20)\n         )\nax2=plt.plot([trainData['Age'].mean()]*trainData.shape[0],color='red',linewidth=2,alpha=0.5)\nax2=plt.plot([trainData['Age'].quantile(q=0.01)]*trainData.shape[0],\n          linestyle='--',\n          color='red',\n          alpha=0.5)\nax2=plt.plot([trainData['Age'].quantile(q=0.99)]*trainData.shape[0],\n          linestyle='--',\n          color='red',\n          alpha=0.5)\n\n###\nax3=fig.add_subplot(4,3,3)\nax3=trainData['Age'].plot(kind='box',figsize=(26,18))\n\n##Fare\n###\nax4=fig.add_subplot(4,3,4)\nax4=trainData['Fare'].plot(kind='hist',\n         title='Fare',\n         grid=True,\n        figsize=(25,20))\n###\nax5=fig.add_subplot(4,3,5)\nax5=trainData['Fare'].plot(kind='line',\n         title='Fare',\n         style='o',\n         grid=False,\n         alpha=0.5,figsize=(25,20)\n         )\nax5=plt.plot([trainData['Fare'].mean()]*trainData.shape[0],color='red',linewidth=2,alpha=0.5)\nax5=plt.plot([trainData['Fare'].quantile(q=0.01)]*trainData.shape[0],\n          linestyle='--',\n          color='red',\n          alpha=0.5)\nax5=plt.plot([trainData['Fare'].quantile(q=0.99)]*trainData.shape[0],\n          linestyle='--',\n          color='red',\n          alpha=0.5)\n\n###\nax6=fig.add_subplot(4,3,6)\nax6=trainData['Fare'].plot(kind='box')\n\n##Sibsp\n###\n\nax7=fig.add_subplot(4,3,7)\nax7=trainData['SibSp'].plot(kind='hist',\n         title='SibSp',\n         grid=True,\n        figsize=(16,8))\n###\nax8=fig.add_subplot(4,3,8)\nax8=trainData['SibSp'].plot(kind='line',\n         title='SibSp',\n         style='o',\n         grid=False,\n         alpha=0.5,figsize=(25,20)\n         )\nax8=plt.plot([trainData['SibSp'].mean()]*trainData.shape[0],color='red',linewidth=2,alpha=0.5)\nax8=plt.plot([trainData['SibSp'].quantile(q=0.01)]*trainData.shape[0],\n          linestyle='--',\n          color='red',\n          alpha=0.5)\n\n\nax8=plt.plot([trainData['SibSp'].quantile(q=0.99)]*trainData.shape[0],\n          linestyle='--',\n          color='red',\n          alpha=0.5)\n\n###\nax9=fig.add_subplot(4,3,9)\nax9=trainData['SibSp'].plot(kind='box')\n\n\n##Parch\n###\n\nax10=fig.add_subplot(4,3,10)\nax10=trainData['Parch'].plot(kind='hist',\n         title='Parch',\n         grid=True,\n        figsize=(25,20))\n###\nax11=fig.add_subplot(4,3,11)\nax11=trainData['Parch'].plot(kind='line',\n         title='Parch',\n         style='o',\n         grid=False,\n         alpha=0.5,figsize=(25,20)\n         )\nax11=plt.plot([trainData['Parch'].mean()]*trainData.shape[0],color='red',linewidth=2,alpha=0.5)\nax11=plt.plot([trainData['Parch'].quantile(q=0.01)]*trainData.shape[0],\n          linestyle='--',\n          color='red',\n          alpha=0.5)\n\n\nax11=plt.plot([trainData['Parch'].quantile(q=0.99)]*trainData.shape[0],\n          linestyle='--',\n          color='red',\n          alpha=0.5)\n\n###\nax12=fig.add_subplot(4,3,12)\nax12=trainData['Parch'].plot(kind='box')\n\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"97500844148fbc1fc2b7f6d564b8b59ae6b6b505"},"cell_type":"markdown","source":"3-2-Visualize Categoric Variables\n* On this step, you can see general information about categoric variables by using pie charts. There are 2 or 3 categories in each variable. If there are more categories in the variables, It it better to use histograms."},{"metadata":{"trusted":true,"_uuid":"e2918ad2fd7708690f0fab2a037db10b5429e1d4"},"cell_type":"code","source":"feature_list=['Survived','Pclass','Sex','Embarked']\nfig_row_size=len(feature_list)\nfig_col_size=2\nfig=plt.figure()\n## Survived\nfor i in range(len(feature_list)):\n    feature=feature_list[i]\n    fig_name=('ax'+str(i))\n    fig_name=fig.add_subplot(fig_row_size,fig_col_size,int(i+1))\n    trainData[feature].value_counts().plot.pie(startangle=90,\n                                         autopct='%1.0f%%',\n                                         figsize=(8,14),\n                                         colormap='Pastel1',\n                                         ax=fig_name\n                                              )\n    fig_name.set_ylabel(feature)\n    \nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"de7e562ce3d7345007508ad38edacf441b2c6e73"},"cell_type":"markdown","source":"3-3-Visualizing interaction between categoric and target variables\n* As you are trying to predict the target variable, it is very usefull to visualize and analyze it with other variables. You can get some useful clues by doing this step.\n * It looks like class and target variable have relationship with eachother. As class gets higher, the survival probability. Survival probability in first class is higher than the survival probability in 3th class.\n * If you are older than 60, it was probabily your last trip.\n * Average ladies' survival rate is larger than men's .\n * If you were travelling with your family, you probably took care of yourself.\n    "},{"metadata":{"trusted":true,"_uuid":"0bfa9635b0b5fada4e7ccc722fcfd4d16ba90728"},"cell_type":"code","source":"#Features and survival rates\ndef ageGroup(age):\n    if age > 100: return '100+'\n    elif age > 90: return '90-99'\n    elif age > 80: return '80-89'\n    elif age > 70: return '70-79'\n    elif age > 60: return '60-69'\n    elif age > 50: return '50-59'\n    elif age > 40: return '40-49'\n    elif age > 30: return '30-39'\n    elif age > 20: return '20-29'\n    elif age > 10: return '10-19'\n    else: return '10-'\n\ntrainData['Age_Group']=trainData['Age'].map(ageGroup)\nfeature_list=['Pclass','Age_Group','Sex','SibSp','Parch','Embarked']\nfig_row_size=len(feature_list)\nfig_col_size=1\nfig=plt.figure()\nfor i in range(len(feature_list)):\n    feature=feature_list[i]\n    fig_name=fig.add_subplot(fig_row_size,fig_col_size,int(i+1))\n    df_for_plotting=trainData.groupby(feature).agg(['sum','count'])['Survived']\n    df_for_plotting['SurvivalRate']=df_for_plotting['sum']/df_for_plotting['count']\n    plot_title=('Survival Rate - '+feature)\n    df_for_plotting.SurvivalRate.plot(kind='bar',title=plot_title,ax=fig_name,figsize=(16,28))\n    fig_name.set_ylabel('Survival Rate')\n\nplt.tight_layout()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"742dceac55a88100a9492a6741bd7dd0fba0cec7"},"cell_type":"markdown","source":"4-Handling Missing Values"},{"metadata":{"_uuid":"5638cbaab9a9404278c29895e9f53f6c75bd4082"},"cell_type":"markdown","source":"First you scan the data for missing values by using a function."},{"metadata":{"trusted":true,"_uuid":"226451459fdef3e2eff4941d46166e33e9b80e97"},"cell_type":"code","source":"##Create a function to find variables with missing value\ndef anyMissingValueInDataFrame(df):\n    for column in df.columns:\n        if df[column].isnull().any():\n            numberOfMissingVals=df[column].isnull().sum()\n            print(' {} feature has {} missing values'.format(column,numberOfMissingVals));\n\nanyMissingValueInDataFrame(trainData)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9491af96ac301a41fb9703698ca510f8d7a9f595"},"cell_type":"code","source":"##You should also check the test data for missing values.\nanyMissingValueInDataFrame(testData)  ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1681758aaea5d824cd5859c60e102322b5518e75"},"cell_type":"markdown","source":"You should analyze the distribution of variable with missing values. After you analyze you have a few options:\n* Replacing missing values with mean values. This option makes no effect on the mean of the variable. \n* Replacing missing values with median values. This option makes no effect on the median of the variable. \n* Dropping the variable. This is an option when most of the data is missing.\n* Predicting missing values. You can create a classifier to predict the missing values by using other variables as input. This option can cause variance of the predicted values to inflate.\n* If your variable is categoric then you can replace missing values with most frequent value. Beside replacing them, you can drop the variable or predict the missing values."},{"metadata":{"trusted":true,"_uuid":"7d15f62a5bdaa71d43e7b157fb2f1f7ed7ddb90c"},"cell_type":"code","source":"#Replacing missing values\n##age\nprint('<--Missing Value Replacement-Age-->')\nprint(trainData.groupby(['Sex','Pclass']).agg(['mean','max','min','median'])['Age'])\nprint('Age feature differentiates between PClass and Sex groups. It is better to replace missing values with group median')\ntrainData['Age']=trainData.groupby(['Pclass','Sex'])['Age'].transform(lambda x: x.fillna(x.median()))\ntestData['Age']=testData.groupby(['Pclass','Sex'])['Age'].transform(lambda x: x.fillna(x.median()))\nprint('Missing {} values are replaced!!!'.format('Age'))\nprint('')\n##Cabin\nprint('<--Missing Value Replacement-Cabin-->')\nprint('Most of cabin values are missing. Droping this feature is better')\ntrainData=trainData.drop(labels='Cabin', axis=1,errors='ignore')\ntestData=testData.drop(labels='Cabin', axis=1,errors='ignore')\nprint('{} feature is dropped!!!'.format('Cabin'))\nprint('')\n##Embarked\nprint('<--Missing Value Replacement-Embarked-->')\ntrainData['Embarked']=trainData['Embarked'].fillna(trainData['Embarked'].value_counts().idxmax())\ntestData['Embarked']=testData['Embarked'].fillna(testData['Embarked'].value_counts().idxmax())\nprint('Missing {} values are replaced!!!'.format('Embarked'))\nprint('')\n##Fare\nprint('<--Missing Value Replacement-Fare-->')\ntrainData['Fare']=trainData.groupby(['Pclass','Sex'])['Fare'].transform(lambda x: x.fillna(x.median()))\ntestData['Fare']=testData.groupby(['Pclass','Sex'])['Fare'].transform(lambda x: x.fillna(x.median()))\nprint('Missing {} values are replaced!!!'.format('Fare'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c8a087eb5f6e52151f16ab129f8c3a6df703cc06"},"cell_type":"markdown","source":"5-Creating Dummy Variables"},{"metadata":{"_uuid":"c2cd9a5e1c358a083c49543aa78b6fd1d95f0f8e"},"cell_type":"markdown","source":"In this step, you create dummy variables for categoric inputs. "},{"metadata":{"trusted":true,"_uuid":"e6cb3b459ea69a6d711930c07afa96a5c58bc7da"},"cell_type":"code","source":"trainData=pd.get_dummies(trainData, columns=['Pclass','Sex','Embarked'])\ntestData=pd.get_dummies(testData, columns=['Pclass','Sex','Embarked'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"71f4efac8e0a1f9de60cc22da3edfd9990050b93"},"cell_type":"markdown","source":"5-2-Splitting Data as target and features"},{"metadata":{"_uuid":"8a164e2cf8a2739cd22ba2a77402a821dc8572f6"},"cell_type":"markdown","source":"In this step you split the data target(dependent variable)  and features(independent variables). You drop unneccessary features like Name, Ticket, Age Group. "},{"metadata":{"trusted":true,"_uuid":"a86e07fa28f7135378fd53ff783cefdfb882fe15"},"cell_type":"code","source":"trainDataTarget=trainData['Survived']\ntrainDataFeatures=trainData.drop(labels=['Survived','Name','Ticket','Age_Group'],axis=1)\ntestDataFeatures=testData.drop(labels=['Name','Ticket'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0dd977e50e6dca05caeb072fdd5a5902f51578a8"},"cell_type":"markdown","source":"6-Standardizing data"},{"metadata":{"_uuid":"40b167fb000511b92ee554fd77b986fa18f1a874"},"cell_type":"markdown","source":"Before classification, it is better to standardize your variables. You use min-max scaler to place your features distribution between 0 and 1."},{"metadata":{"trusted":true,"_uuid":"4ed839abb187a278685ab8c4445bdfd7f9ac2c7b"},"cell_type":"code","source":"## first create a scaler\nscaler=MinMaxScaler()\nscaler.fit(trainDataFeatures)\n## apply the scaler to the data\ntrainDataFeatures=pd.DataFrame(scaler.transform(trainDataFeatures), index=trainDataFeatures.index,columns=trainDataFeatures.columns)\ntestDataFeatures=pd.DataFrame(scaler.transform(testDataFeatures), index=testDataFeatures.index,columns=testDataFeatures.columns)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fc69f54b2dad36e781095ec5132d7abb323ca83c"},"cell_type":"markdown","source":"7-Training a classifier"},{"metadata":{"_uuid":"9b8b69186969de4d9012a86f814ac2aa57bb59c2"},"cell_type":"markdown","source":"Your data preparation is finished and now it's time to create few classifiers to predict if a passanger is alive after the accident.\nYou are going to use 4 different classifiers:\n* Decision Tree\n* K-nearest Neighbour\n* Logistic Regression\n* Random Forest\n\nAfter choosing best ones, you merge them and create a final classifier."},{"metadata":{"_uuid":"31f8be67a4a7793194beeb714480a34b299cd696"},"cell_type":"markdown","source":"First, you create a dataframe to save classifier's results. After you create all classifiers, you will use this dataframe to compare with each other and choose best ones for final classifier."},{"metadata":{"trusted":true,"_uuid":"19aa208018b4c7eb72c567a0f9aacc39cc0884b2"},"cell_type":"code","source":"models_columns=['Model_Name','Parameter','Model_Trained','Accuracy_Precision','Sensitivity','Lift']\nmodels=pd.DataFrame(columns=models_columns)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"04ebe1aebe6cf59f081f719c7fd63bea2652cdf8"},"cell_type":"markdown","source":"**Decision Tree**\n\nIn this step, you train decision tree classifiers with different parameters. Use a for loop to determine best \"max depth parameters\""},{"metadata":{"trusted":true,"_uuid":"c77a89deb1ae90ae35c920da9fc0f4c5f3473634"},"cell_type":"code","source":"for d in np.arange(2,15,1):\n    #Create classifier\n    dt=DecisionTreeClassifier(criterion='entropy',\n                           max_depth=d)\n    #do cross validation in 5 folds\n    scores = cross_val_score(dt, trainDataFeatures, trainDataTarget, cv=5)\n    print('Cross Val Scores-{:.2f}:'.format(d),scores)\n    print('Mean Accuracy-{}: {:.2f}'.format(d,scores.mean()))\n    \n    dt_model=dt.fit(trainDataFeatures,trainDataTarget)\n    dt_pred=dt_model.predict(trainDataFeatures)\n    \n    tn, fp, fn, tp=confusion_matrix(y_true=trainDataTarget, \n                                    y_pred= dt_pred\n                                   ).ravel()\n    print('tn: {}, fp:{}, fn:{}, tp:{}'.format(tn, fp, fn, tp))\n    \n    Sensitivity=tp/(tp+fn)\n    print('Sensitivity-{} : {:.2f}'.format(d,Sensitivity))\n    \n    Accuracy_Precision=precision_score(y_true=trainDataTarget, y_pred=dt_pred,average='binary')\n    print('Precision-{} :{:.2f}'.format(d,Accuracy_Precision))\n    \n    model_success=tp/(tp+fp)\n    random_selection=(tp+fn)/(tp+fp+tn+fn)\n    lift=model_success/random_selection\n    print('Lift-{} :{:.2f}'.format(d,lift))\n    \n    print('-'*50)\n    Model='dt'+str(d)\n    Parameter=d\n    Model_Trained=dt_model\n    lift=lift\n    new_row=[Model,Parameter,Model_Trained,Accuracy_Precision,Sensitivity,lift]\n    models.loc[-1]=new_row\n    models.index=models.index+1\n\n%matplotlib inline\nmodels.plot(x='Parameter',y=['Sensitivity','Accuracy_Precision','Lift'])\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b434bdb1217eb6ef8270655e53fd4dcb04f7b44e"},"cell_type":"markdown","source":"Decision tree with max depth-6 or 7 are better than the others. They are better to detect survivals. \n\nDecision tree with max depth parameter 6's sensitivity ratio is %82 and precision ratio is %84. That means it classifies %82 of survivals correctly. And %84 of passangers who are classified as survived are correct. \n\nDecision tree with max depth parameter 7's sensitivity ratio is %76 and precision ratio is %91. Sensitivity ratio is a bit lower than the decision tree with max depth parameter 6 but precision is better."},{"metadata":{"trusted":true,"_uuid":"9c94d537638aa52e08556cafcef44989841c2b27","collapsed":true},"cell_type":"markdown","source":"**K Nearest Neighbour-KNN**\n\nIn this step, you train KNN classifiers with different k parameters from 1 to 10. "},{"metadata":{"trusted":true,"_uuid":"547836b65963c7c828a1d789b19613436d8a86dc"},"cell_type":"code","source":"##knn\nfor n_neighbor in np.arange(1,10,1):\n    knn=KNeighborsClassifier(n_neighbors=n_neighbor,\n                    weights='uniform',\n                    algorithm='auto', \n                    )\n    print('-'*10,'Number Of Neighbors:{}'.format(n_neighbor),'-'*10)\n    scores_knn=cross_val_score(knn,trainDataFeatures,trainDataTarget,cv=5)\n    print('{} accuracy is:{:.2f}'.format('KNN',scores_knn.mean()))\n    \n    knn_model=knn.fit(trainDataFeatures,trainDataTarget)\n    trainDataTarget_pred=knn.predict(trainDataFeatures)\n    \n    tn, fp, fn, tp=confusion_matrix(y_true=trainDataTarget, y_pred=trainDataTarget_pred).ravel()\n    print('tn: {}, fp:{}, fn:{}, tp:{}'.format(tn, fp, fn, tp)) \n    \n    Sensitivity=tp/(tp+fn)\n    print('Sensitivity: {:.2f}'.format(Sensitivity))\n\n    Accuracy_Precision=precision_score(y_true=trainDataTarget, y_pred=trainDataTarget_pred,average='binary')\n    print('Precision:{:.2f}'.format(Accuracy_Precision))\n    plt.plot(n_neighbor,Sensitivity,'bo')\n\n    model_success=tp/(tp+fp)\n    random_selection=(tp+fn)/(tp+fp+tn+fn)\n    lift=model_success/random_selection\n    print('Lift:{:.2f}'.format(lift))\n    \n    Model='knn'+str(n_neighbor)\n    Parameter=n_neighbor\n    Model_Trained=knn_model\n    lift=lift\n    new_row=[Model,Parameter,Model_Trained,Accuracy_Precision,Sensitivity,lift]\n    models.loc[-1]=new_row\n    models.index=models.index+1\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3ac4cf4ffee1b3a1d7998c5e203578de69546ca3"},"cell_type":"markdown","source":"KNN classifier with parameter 1 is better than others. \n\nIt can correctly classifies 338 survivals out of 342."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f5b63bd9d679cb4c9f96d458c6ec7de9c0862e3b"},"cell_type":"markdown","source":"**Logistic Regression**\n\nIn this step you create a standard logistic regression."},{"metadata":{"trusted":true,"_uuid":"8efb03c4ab197c3f9e9254a971e58597c9af0d0b"},"cell_type":"code","source":"##log reg\n\n#log_reg=LogisticRegression(solver='liblinear',max_iter=100) \nlog_reg=LogisticRegression()\nscores_log_reg=cross_val_score(log_reg,\n                               trainDataFeatures,\n                               trainDataTarget,\n                               cv=5\n                              )\nprint('{} accuracy is:{:.2f}'.format('Logistic Regression',\n                                     scores_log_reg.mean()\n                                    )\n     )\n\nlog_reg_model=log_reg.fit(trainDataFeatures,\n            trainDataTarget\n           )\nlog_reg_pred=log_reg.predict(trainDataFeatures\n                            )\n\ntn, fp, fn, tp=confusion_matrix(y_true=trainDataTarget, \n                                y_pred=log_reg_pred\n                               ).ravel()\nprint('tn: {}, fp:{}, fn:{}, tp:{}'.format(tn, \n                                           fp, \n                                           fn, \n                                           tp)\n     ) \n\nSensitivity=tp/(tp+fn)\nprint('Sensivity: {:.2f}'.format(Sensitivity))\n\nAccuracy_Precision=precision_score(y_true=trainDataTarget, \n                                   y_pred=log_reg_pred,\n                                   average='binary'\n                                  )\nprint('Precision:{:.2f}'.format(Accuracy_Precision)\n     )\n\nmodel_success=tp/(tp+fp)\nrandom_selection=(tp+fn)/(tp+fp+tn+fn)\nlift=model_success/random_selection\nprint('Lift:{:.2f}'.format(lift))\n\n\nModel='logReg'+str(1)\nParameter=0\nModel_Trained=log_reg_model\nlift=lift\nnew_row=[Model,Parameter,Model_Trained,Accuracy_Precision,Sensitivity,lift]\nmodels.loc[-1]=new_row\nmodels.index=models.index+1\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"14293a91f91d5a5f397665fd2b32344742d13189"},"cell_type":"markdown","source":"The results are not as good as decision tree classifier or KNN."},{"metadata":{"_uuid":"975fb2529e169615e72bd630967b875d5f43c870"},"cell_type":"markdown","source":"**Random Forest**\n\nIn this step you use a random forest classifier. \n\nYou can picture this classifier like you create lots of decision tree classifier and union them to make final decision. "},{"metadata":{"trusted":true,"_uuid":"ae829de2b0696918bfef063acf21bfeb43116237","scrolled":true},"cell_type":"code","source":"RandomForest=RandomForestClassifier(n_estimators=10, #you can change this parameter to change number of tree in your forest.\n                                    criterion='gini' \n                                    )\n\n\nscores_RandomForest=cross_val_score(RandomForest,\n                             trainDataFeatures,\n                             trainDataTarget,cv=5)\n\n\n\n\nprint('{} accuracy is:{:.2f}'.format('Random Forest',scores_RandomForest.mean()))\n\nRandomForest_model=RandomForest.fit(trainDataFeatures,trainDataTarget)\nRandomForest_pred=RandomForest.predict(trainDataFeatures)\n\ntn, fp, fn, tp=confusion_matrix(y_true=trainDataTarget, y_pred=RandomForest_pred).ravel()\nprint('tn: {}, fp:{}, fn:{}, tp:{}'.format(tn, fp, fn, tp)) \n\nSensitivity=tp/(tp+fn)\nprint('Sensitivity: {:.2f}'.format(Sensitivity))\n\nAccuracy_Precision=precision_score(y_true=trainDataTarget, y_pred=RandomForest_pred,average='binary')\nprint('Precision:{:.2f}'.format(Accuracy_Precision))\n\nmodel_success=tp/(tp+fp)\nrandom_selection=(tp+fn)/(tp+fp+tn+fn)\nlift=model_success/random_selection\nprint('Lift:{:.2f}'.format(lift))\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"800a9681493cb3e44455f6422a3153f6d91a31d3"},"cell_type":"markdown","source":"**Create an ensembled model**\n\nAfter you trained 4 different classifiers with many different parameters, there are some classifiers are more successfull to predict the survivals than the others.\nYou can select one of them as the champion model or you can use them all and create an ensembled model for prediction.\n\nThere are 4 different classifiers for your ensembled model:\n* Decision tree-max depth=6\n* K Nearest Neighbours-k=1\n* Logistic Regression-standard\n* Random Forest-n estimators=10"},{"metadata":{"trusted":true,"_uuid":"5a7844ebb6e54f45d9bc0c303b70e197bbc621b4"},"cell_type":"code","source":"#Decision tree\ndt=DecisionTreeClassifier(criterion='entropy',\n                           max_depth=6)\n#K Nearest Neighbours\nknn=KNeighborsClassifier(n_neighbors=1,\n                weights='uniform',\n                algorithm='auto', \n                )\n#Logistic Regression\nlog_reg=LogisticRegression()\n\n#Random Forest\nRandomForest=RandomForestClassifier(n_estimators=10, \n                                    criterion='gini' \n                                    )\n\n\n\nclf1 = dt\nclf2 = knn\nclf3 = log_reg\nclf4 =RandomForest\n\n#Create your ensembled classifier\nVoting_Classifier = VotingClassifier(estimators=[('DecisionTree', clf1),\n                                    ('KNN_1', clf2), \n                                    ('LogReg', clf3),\n                                   ('RandomForest', clf4)\n                                   ], \n                        voting='hard')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5f23c149074aee48564e6ac9abef0231ee45ce66"},"cell_type":"code","source":"scores_Voting_Classifier=cross_val_score(Voting_Classifier,\n                             trainDataFeatures,\n                             trainDataTarget,cv=5)\n\n\n\n\nprint('{} accuracy is:{:.2f}'.format('Voting Classifier',scores_Voting_Classifier.mean()))\n\nVoting_Classifier_model=Voting_Classifier.fit(trainDataFeatures,trainDataTarget)\nVoting_Classifier_pred=Voting_Classifier.predict(trainDataFeatures)\n\ntn, fp, fn, tp=confusion_matrix(y_true=trainDataTarget, y_pred=Voting_Classifier_pred).ravel()\nprint('tn: {}, fp:{}, fn:{}, tp:{}'.format(tn, fp, fn, tp)) \n\nSensitivity=tp/(tp+fn)\nprint('Sensivity: {:.2f}'.format(Sensitivity))\n\nAccuracy_Precision=precision_score(y_true=trainDataTarget, y_pred=Voting_Classifier_pred,average='binary')\nprint('Precision:{:.2f}'.format(Accuracy_Precision))\n\nmodel_success=tp/(tp+fp)\nrandom_selection=(tp+fn)/(tp+fp+tn+fn)\nlift=model_success/random_selection\nprint('Lift:{:.2f}'.format(lift))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"de153e8224d3bbdb768f03f8db076531236561b6"},"cell_type":"code","source":"#Predict test data and create a new column, Survived\ntestDataFeatures['Survived']=Voting_Classifier.predict(testDataFeatures)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"407126b078a0afbbafe5c7c3543b21a4bd2b9aec"},"cell_type":"code","source":"#Check your prediction\ntestDataFeatures['Survived'].head(15)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3edd4cded0fc27b2254dd0f8037d7818299fc0b3"},"cell_type":"code","source":"#Write your prediction to gender_submission.csv\ntestDataFeatures['Survived'].to_csv('gender_submission.csv')","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}