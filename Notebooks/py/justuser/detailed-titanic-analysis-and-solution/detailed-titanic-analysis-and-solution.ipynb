{"cells":[{"metadata":{"_uuid":"9a8852e5c712ed9d9647a3d93db4b0a3f4bad3f9","_cell_guid":"ae9f290a-7647-4a0a-bad4-cdb0adf8509e"},"cell_type":"markdown","source":"Titanic is one of the classical problems in machine learning. There are many solutions with different approaches out there, so here is my take on this problem. I tried to explain every step as detailed as I could, too, so if you're new to ML, this notebook may be helpful for you.\n\nMy solution scored 0.79425. If you have noticed any mistakes or if you have any suggestions, you are more than welcome to leave a comment down below.\n\nWith that being said, let's start with importing libraries that we'll need and take a peek at the data:"},{"metadata":{"_uuid":"e329f1468dfd35fd67e5d10b62caac84b55c800a","_cell_guid":"94e06b7a-c143-4267-a80d-055ddf8ebd10","collapsed":true,"trusted":false},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e0fa91360085bfe92109e0bbf62ecc5ef1bda785","_cell_guid":"698f72c8-3cac-4bc4-94c9-1c62c9c21080","collapsed":true,"trusted":false},"cell_type":"code","source":"filePath = \"../input/train.csv\"\ntrain = pd.read_csv(filePath)\nfilePath = \"../input/test.csv\"\ntest = pd.read_csv(filePath)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"83d5bb65b7296305785abc32556dc8665ed8045f","_cell_guid":"d0667b07-3a23-448a-b33e-ae1f15ea30a5","collapsed":true,"trusted":false},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5d919f2b8ec7298364447de64f81533edde1ab10","_cell_guid":"58cc8585-ec5c-4a60-ad44-e6b15bda411a"},"cell_type":"markdown","source":"At the first glance we can already tell that some data is missing.\n\nFirst, let's see how much data do we actually miss:"},{"metadata":{"_uuid":"6f5e4ef684ecd86fe679804589a8724191e3839f","_cell_guid":"a3179721-d3d2-4d42-b1d6-ba27498e3424","collapsed":true,"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(14, 12))\n\n# don't forget to set titles\nplt.subplot(211)\nsns.heatmap(train.isnull(), yticklabels=False)\nplt.subplot(212)\nsns.heatmap(test.isnull(), yticklabels=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4895b4db132245d55733cbc8555f9bacd2713956","_cell_guid":"ec361c5f-19b0-4804-b60c-7bf685c42afa"},"cell_type":"markdown","source":"As we can see, both in both test and train datasets we miss quite a lot of values. Some data like Age and Embarked may be filled out, but the Cabin column misses so much values that it can't really be used as a feature. It can be transformed or substituted, but we will do that later.\n\nNow lets focus on the data in details and see if there are any noticeable correlations. "},{"metadata":{"_uuid":"408a68d617472869679c735cbf0ce0b6ac8edf72","_cell_guid":"0ac5107b-29f5-41f9-9f98-8fb250189cc6"},"cell_type":"markdown","source":"# Initial data exploration"},{"metadata":{"_uuid":"cfb82c7c25a8d8ee8c943071583cf0024c6824b5","_cell_guid":"df1faa7a-7a19-4be1-9907-cf908223dd68"},"cell_type":"markdown","source":"The first thing we need to explore how survivability depends on different factors, such as Sex, Age (younger people are more fit), Passenger Class (possible higher class priority), and Number of Spouses/Siblings\n\nLet's explore how survivability depends on these features and if there are any correlation between them."},{"metadata":{"_uuid":"b5d366f213967590d05e63af89ddecd0b3a54cbc","_cell_guid":"414146ad-d938-47ee-8f9f-d97db20897de","collapsed":true,"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(14, 12))\n\nplt.subplot(321)\nsns.countplot('Survived', data=train)\nplt.subplot(322)\nsns.countplot('Sex', data=train, hue='Survived')\nplt.subplot(323)\nsns.distplot(train['Age'].dropna(), bins=25)\nplt.subplot(324)\nsns.countplot('Pclass', data=train, hue='Survived')\nplt.subplot(325)\nsns.countplot('SibSp', data=train)\nplt.subplot(326)\nsns.countplot('Parch', data=train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ce96ee63aaa58cb40b6b1aa1d1d854230b235605","_cell_guid":"b8931848-1ed9-4fc0-8f4d-991e5c487092"},"cell_type":"markdown","source":"From these plots we can make several conclusions:\n\n* most people didn't survive the crash. \n* most passengers were males \n* survivability of women was much higher than of men. We will have to explore the Sex feature more later and see if there are any other interesting correlations.\n* most passengers were middle aged, but there were also quite a few children aboard\n* most passeners had the third class tickets\n* survivability of first and second class passengers were higher compared to the third class\n* most passengers traveled alone or with one sibling/spouse \n\nNow we can take a look at each fature specifically to see if it depends on something else or if there ..."},{"metadata":{"_uuid":"d78b00620549b003d3d86164f431a2ae4495b9d7","_cell_guid":"9e7c96ad-47ae-4c60-81c7-498ad820f656"},"cell_type":"markdown","source":"# Filling in the missing data"},{"metadata":{"_uuid":"eed6a355ae30dc05a0ce2d8cd7287efe42c9676a","_cell_guid":"1be66c76-8df1-4c75-a9ed-66fa6863a812"},"cell_type":"markdown","source":"Okay, we could jump into full exploration and maybe even transformation of the data, but as we saw before, we miss quite a lot of data. The easiest aproach would be simply dropping all the missing values, be in this case we risk to lose accuracy of our models or entire features. \n\nInstead, we will try to fill the missing values based on some logic. Let's take a look at the training data once again to see which values do we miss"},{"metadata":{"_uuid":"9f22014acbacffab521fc0ef9908cad1f7599939","_cell_guid":"48e8efde-abac-445e-85fb-65d0d035ad31","collapsed":true,"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nsns.heatmap(train.isnull(), yticklabels=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9e58cd000921fb2382c9e30c556456599358baea","_cell_guid":"71dc10d3-d757-4cf9-820b-e688c1c8a659"},"cell_type":"markdown","source":"In current state the train data misses Age, Cabin, and Embarked values. Unfortunatelly, the Cabin column is missing most of its data and we can't really use it as a feature. However, it is not entirely useless, but I'll leave it for later. \n\nAge column can be filled in many ways. For example, we could take a look at the mean age of every passenger class and fill it based on that information. But instead, if we take a look at the names of the passengers, we can notice a information that can help us:  "},{"metadata":{"_uuid":"55b214e4056adb8503f0978479e3c50f78fa2868","_cell_guid":"2a04c337-af2a-422b-bd87-cb9aa8361efe","collapsed":true,"trusted":false},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4e749a05c85a6ef3edc66f5fffe1f689ffce5bc3","_cell_guid":"fb135217-daac-4d19-a0cd-8123f7de62b8"},"cell_type":"markdown","source":"Every name has a title (such as Mr., Miss., ets.) and follows the following pattern: Last_Name, Title. First_Name. We can categorise passengers by their titles and set unknown age values to mean value of a corresponding title.\n\nWe will do so by adding a column called 'Title' to the data and fill it out with a new funciton."},{"metadata":{"_uuid":"d4fbd29cf7ebbfcb8747365d3cacb13ed23dcb32","_cell_guid":"9dc1c1c0-be87-4648-952a-540f646cfd6b","collapsed":true,"trusted":false},"cell_type":"code","source":"def get_title(pasngr_name):\n    \n    index_1 = pasngr_name.find(', ') + 2\n    index_2 = pasngr_name.find('. ') + 1\n    \n    return pasngr_name[index_1:index_2]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ca4c44f88c2d0054a6a2c1abc7280a56fcbbdf7","_cell_guid":"ef204dbb-dec1-40cb-a4e0-c20f327e1911","collapsed":true,"trusted":false},"cell_type":"code","source":"train['Title'] = train['Name'].apply(get_title)\ntest['Title'] = test['Name'].apply(get_title)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"91d08a3cf9423cb69e8c4fc74f9c5f834a415b4d","_cell_guid":"3d040ab6-47a1-4bd6-9502-bff4155cae55","collapsed":true,"scrolled":false,"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(16, 10))\nsns.boxplot('Title', 'Age', data=train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9ebc4c043c027d5ffc7e1f939574b7c146ab7d81","_cell_guid":"548ec3d7-25d5-4888-8e4b-cc66fdc23a1f"},"cell_type":"markdown","source":"Now that we have all the titles, we can find out a mean value for each of them and use it to fill the gaps in the data."},{"metadata":{"_uuid":"ff1a633611780e247bd0ff71e93b7e891ca49023","_cell_guid":"fd5f17cd-4c1f-4226-a8d9-985067679d2b","collapsed":true,"trusted":false},"cell_type":"code","source":"train.Title.unique()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"99ee69f120cf0b8d8309fc06da2afa4434e6067e","_cell_guid":"7b65d0a2-0b99-4d4d-b27a-5123ff2b11b5","collapsed":true,"trusted":false},"cell_type":"code","source":"age_by_title = train.groupby('Title')['Age'].mean()\nprint(age_by_title)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"065eeb4a9bd04820f33053017bc0a02bf920dac5","_cell_guid":"1f538da8-060c-44f0-8043-c21b8600841d","collapsed":true,"trusted":false},"cell_type":"code","source":"def fill_missing_ages(cols):\n    age = cols[0]\n    titles = cols[1]\n    \n    if pd.isnull(age):\n        return age_by_title[titles]\n    else:\n        return age","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bceb6f6cb5855c617dc3b24871d87a075771cf3c","_cell_guid":"9705685c-4157-4928-8325-6fceb3a763fa","collapsed":true,"trusted":false},"cell_type":"code","source":"train['Age'] = train[['Age', 'Title']].apply(fill_missing_ages, axis=1)\ntest['Age'] = test[['Age', 'Title']].apply(fill_missing_ages, axis=1)\n\n#and one Fare value in the test set\ntest['Fare'].fillna(test['Fare'].mean(), inplace = True)\n\nplt.figure(figsize=(14, 12))\n\nplt.subplot(211)\nsns.heatmap(train.isnull(), yticklabels=False)\nplt.subplot(212)\nsns.heatmap(test.isnull(), yticklabels=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"545e72e6b5359695eac4463399296e7e42eb11cb","_cell_guid":"eaa33216-7c8a-4268-8273-9c6f922f710f"},"cell_type":"markdown","source":"Okay, now we have the Age column filled entirely. There are still missing values in Cabin and Embarked columns. Unfortunatelly, we miss so much data in Cabin that it would be impossible to fill it as we did with Age, but we are not going to get rid of it for now, it will be usefull for us later.\n\nIn embarked column only one value is missing, so we can set it to the most common value."},{"metadata":{"_uuid":"c67439177a5a9caba90d580282596f2479f9a475","_cell_guid":"dc3d1c60-e8a4-4b49-92e9-a888344673f5","collapsed":true,"trusted":false},"cell_type":"code","source":"sns.countplot('Embarked', data=train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"20fe4366b5f3321f9f1e62585ba46c4221a7096b","_cell_guid":"f507ef91-7c0b-4d1f-bcfc-900746b61854","collapsed":true,"trusted":false},"cell_type":"code","source":"train['Embarked'].fillna('S', inplace=True)\nsns.heatmap(train.isnull(), yticklabels=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"969f6cf5091815966973516a426478b24100a63b","_cell_guid":"0887fafb-c5f4-4891-9244-9cfabe4c9c37"},"cell_type":"markdown","source":"Now we have patched the missing data and can explore the features and correlations between them without worrying that we may miss something."},{"metadata":{"_uuid":"4992378ce8d62f25c2ac82e64ac6e9469d6ff98e","_cell_guid":"d4dda0c6-6506-4542-8dee-2b03c39a4198"},"cell_type":"markdown","source":"# Detailed exploration "},{"metadata":{"_uuid":"c820e208314081b71210c848d32ac6fc0ddc53d7","_cell_guid":"d4499d08-9ea4-4dc1-ae95-4efb3fcf2304"},"cell_type":"markdown","source":"In this section we will try to explore every possible feature and correlations them. Also, ..."},{"metadata":{"_uuid":"90e62246e5e17ed7b397baf1e24d9a7e74a4b487","_cell_guid":"d9bde0ee-2d7f-41c3-b23c-d57852ce915d","collapsed":true,"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(10,8))\nsns.heatmap(train.drop('PassengerId', axis=1).corr(), annot=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b7c72918759fe8138d1427616e70c09806abe6c9","_cell_guid":"6794fc9e-e7d4-43f0-be5d-b323b8851ba7"},"cell_type":"markdown","source":"Here's a shortened plan that we will follow to evaluate each feature and ...:\n* Age \n* Sex\n* Passenger classes and Fares\n* **(...)**"},{"metadata":{"_uuid":"11776cd3cb5a6ce13283170d8ec0ad7252f58609","_cell_guid":"ee0e2e8f-9d98-41ed-9666-8e7364177d0d"},"cell_type":"markdown","source":"### Age\nThe first feature that comes to my mind is Age. The theory is simple: survivability depends on the age of a passenger, old passengers have less chance to survive, younger passengers are more fit, children either not fit enough to survive, or they have higher chances since adults help them"},{"metadata":{"_uuid":"a4adc5a687c358d32f81cd5a80a62a68df0eec66","_cell_guid":"a8028bac-4835-4436-8ab9-afc503cf4967","collapsed":true,"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(10, 8))\nsns.violinplot('Survived', 'Age', data=train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b6e6c497af9130f0104f46e94eb20a52eb564064","_cell_guid":"0801c1c7-7dee-4f3d-8c33-d40ca5e204d3"},"cell_type":"markdown","source":"We can already notice that children had better chance to survive, and the majority of casulties were middle aged passengers (which can be explained by the fact that most of the passengers were middle aged). \n\nLet's explore the age, but this time separated by the Sex column."},{"metadata":{"_uuid":"3108e90b081a68e5a49eeeae93fa7f7c9f9d52e9","_cell_guid":"66ce0e73-3c0b-44a4-8fea-640578af8a90","collapsed":true,"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(10, 8))\nsns.violinplot('Sex', 'Age', data=train, hue='Survived', split=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"07d4780ee020f75b1dfda931e767ca02f25ef2bc","_cell_guid":"c8ac501a-20fc-45fb-ac03-33342253d0d8"},"cell_type":"markdown","source":"The plot above confirmes our theory for the young boys, but it is rather opposite with young girls: most females under the age of 16 didn't survive. This looks weird at first glance, but maybe it is connected with some other feature. \n\nLet's see if the class had influence on survivability of females."},{"metadata":{"_uuid":"8af8e653b9a02b987a4b90839a1d4e8ae4c05c36","_cell_guid":"41d763cc-c53f-401f-bdba-f95739fc8e17","collapsed":true,"trusted":false},"cell_type":"code","source":"grid = sns.FacetGrid(train, col='Pclass', hue=\"Survived\", size=4)\ngrid = grid.map(sns.swarmplot, 'Sex', 'Age', order=[\"female\"])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d23ccf3bb6ddc7e00771bbbd44ea76fe5ffe87f2","_cell_guid":"b784d1e6-2989-424c-9f51-387072b83441"},"cell_type":"markdown","source":"### Pclass\nIdea here is pretty straightforward too: the higher the class, the better chance to survive. First, let's take a look at the overall situation:"},{"metadata":{"_uuid":"6bfbe49a5a6df963b4563662a518501e007177b9","_cell_guid":"8a6ed6f9-f9d4-4fba-921d-c4869e9dffbf","collapsed":true,"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(10, 8))\nsns.countplot('Pclass', data=train, hue='Survived')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c5e2123dc095fb6abc1d6dcce89fe18bf5de1d94","_cell_guid":"cd5f12f8-5f87-4b17-8f0f-f6ce878ab948"},"cell_type":"markdown","source":"We can already see that the class plays a big role in survivability. Most of third class passengers didn't survive the crash, second class had 50/50 chance, and most of first class passengers survived.\n\nLet's further explore Pclass and try to find any correlations with other features.\n\nIf we go back to the correlation heatmap, we will notice that Age and Fare are strongly correlated with Pclass, so they will be our main suspects."},{"metadata":{"_uuid":"de8a72e0aaeaa56df4af455e2b5221ac65561fe8","_cell_guid":"8ec93c39-6476-4b9e-864c-9432aae89f5f","collapsed":true,"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(14, 6))\n\nplt.subplot(121)\nsns.barplot('Pclass', 'Fare', data=train)\nplt.subplot(122)\nsns.barplot('Pclass', 'Age', data=train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"16e15d67db3e4af43d688f4b78b77cacd381c023","_cell_guid":"a8f67eba-3107-4244-b869-3ccc62404a53"},"cell_type":"markdown","source":"As expected, these two features indeed are connected with the class. The Fare was rather expected: the higher a class, the more expencive it is. \n\nAge can be explained by the fact that usually older people are wealthier than the younger ones. **(...)**\n\nHere's the overall picture of Fares depending on Ages separated by Classes:"},{"metadata":{"_uuid":"aa449c31b002eaabacc58e99fb2721ad417af19a","_cell_guid":"9e4d202f-e89f-4b5b-9369-ca9c7f762d71","collapsed":true,"trusted":false},"cell_type":"code","source":"sns.lmplot('Age', 'Fare', data=train, hue='Pclass', fit_reg=False, size=7)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3d6fa9b566434e59a4ec2a3872895642592ddaed","_cell_guid":"4bba7594-8064-4848-a0f8-e2479be51c13"},"cell_type":"markdown","source":"### Family size\n\nThis feature will represent the family size of a passenger. We have information about number of Siblings/Spouses (SibSp) and Parent/Children relationships (Parch). Although it might not be full information about families, we can use it to determine a family size of each passenger by summing these two features."},{"metadata":{"_uuid":"f895a0f7eaeed761fd483693eaa4496e79029a5a","_cell_guid":"e0e9a409-47cb-4e00-bc4a-7f3886a1b880","collapsed":true,"trusted":false},"cell_type":"code","source":"train[\"FamilySize\"] = train[\"SibSp\"] + train[\"Parch\"]\ntest[\"FamilySize\"] = test[\"SibSp\"] + test[\"Parch\"]\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fac2e1f3fc95ffa80ad6e565a35ec99f5dcef571","_cell_guid":"1cc6c3b9-921a-4707-a83b-fd96d14a1e75"},"cell_type":"markdown","source":"Now let's see how family size affected survivability of passengers:"},{"metadata":{"_uuid":"7e9d1c70028b588701df69d83a1f764b852d31b3","_cell_guid":"b390169a-1125-4cd4-9c44-b4d182f2ce6d","collapsed":true,"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(14, 6))\n\nplt.subplot(121)\nsns.barplot('FamilySize', 'Survived', data=train)\nplt.subplot(122)\nsns.countplot('FamilySize', data=train, hue='Survived')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7332de96e067faa6e5fecfb64babb19146cdaa54","_cell_guid":"86c9a1d7-026d-4b5e-b98f-69d39deeb0a5"},"cell_type":"markdown","source":"We can notice a curious trend with family size: **(...)**"},{"metadata":{"_uuid":"d3f2ade48bd72e1fe11b8fb1e23598b668779133","_cell_guid":"7c9fecc7-ee0a-494a-9ae2-ab1b9b20819e","collapsed":true,"trusted":false},"cell_type":"code","source":"grid = sns.FacetGrid(train, col='Sex', size=6)\ngrid = grid.map(sns.barplot, 'FamilySize', 'Survived')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"582c5e7ab2de9ad372dd142420148b61dca25b6d","_cell_guid":"558c7785-a8c0-4251-9bf1-901e40fe64df"},"cell_type":"markdown","source":"These two plots only confirm our theory. With family size more than 3 survivability drops severely for both women and men. We also should keep in mind while looking at the plots above that women had overall better chances to survive than men.\n\nLet's just check if this trend depends on something else, like Pclass, for example:"},{"metadata":{"_uuid":"174a9225ae6254fa674f04b1fcf19f7013fdf840","_cell_guid":"c8e1cb08-167a-4489-b352-6dc0aafcc858","collapsed":true,"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(10, 8))\nsns.countplot('FamilySize',  data=train, hue='Pclass')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"52bc5b917791be6ab52bbb543da3ee998a7e86a9","_cell_guid":"20d6161f-b7d6-4ec8-b4bf-37a8219e6f4a"},"cell_type":"markdown","source":"### Embarked\n"},{"metadata":{"_uuid":"4228d4a3ee2115c2733c800cd3948830e594a0d4","_cell_guid":"9f4a5b89-1ba2-4f72-834d-2ce543b755a8","collapsed":true,"trusted":false},"cell_type":"code","source":"sns.countplot('Embarked', data=train, hue='Survived')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b0e23a65cdc0379499e4069c99a241fd4485bb48","_cell_guid":"b47d42cb-5877-45f4-bb61-bac80239899b","collapsed":true,"trusted":false},"cell_type":"code","source":"sns.countplot('Embarked', data=train, hue='Pclass')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f7585b848f4ac82c9a6f50a8692a5704177ece23","_cell_guid":"c9af5618-d4c2-44ac-a72a-a49ff8615af2"},"cell_type":"markdown","source":"### Conclusion: "},{"metadata":{"_uuid":"2a07c1308c5ed9afd8dbbec2dc1e51da25c78bbd","_cell_guid":"5fc56122-3542-4c7e-b4bc-7b38843f2c7b"},"cell_type":"markdown","source":"# Additional features"},{"metadata":{"_uuid":"572b6212f8b9be41ed7aa673f87c59d9169c9148","_cell_guid":"bb309663-e2c3-4a11-803f-377025895a48"},"cell_type":"markdown","source":"Now we've analyzed the data and have an idea of what will be relevant. But before we start building our model, there is one thing we can do to improve it even further.\n\nSo far we've worked with features that came with the dataset, but we can also create our own custom features (so far we have FamilySize as a custom, or engineered feature)."},{"metadata":{"_uuid":"c9f2eeefb432574a72e875c3aaceb85a9a6d1e18","_cell_guid":"b1794d92-0318-440a-bade-a3cb89ad27e8"},"cell_type":"markdown","source":"### Cabin\nNow this is a tricky part. Cabin could be a really important feature, especially if we knew the distribution of cabins on the ship, but we miss so much data that there is almost no practical value in the feature itself. However, there is one trick we can do with it. \n\nLet's create a new feature called CabinKnown that represents if a cabin of a certain passenger is known or not. Our theory here is that if the cabin is known, then probably that passenger survived."},{"metadata":{"_uuid":"6754b2db54ac138a87acc18c27f38ecfba405f6a","_cell_guid":"d6e36560-a2d1-407b-ae9d-0c7ecf9dd3b9","collapsed":true,"trusted":false},"cell_type":"code","source":"def has_cabin(pasngr_cabin):\n    \n    if pd.isnull(pasngr_cabin):\n        return 0\n    else:\n        return 1\n    \ntrain['CabinKnown'] = train['Cabin'].apply(has_cabin)\ntest['CabinKnown'] = test['Cabin'].apply(has_cabin)\nsns.countplot('CabinKnown', data=train, hue='Survived')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fcc5bf07abbcb5946bca8243670dbef37477c368","_cell_guid":"9cbde7cc-b216-4200-9a3d-764940bdb3e6"},"cell_type":"markdown","source":"Clearly, the corelation here is strong: the survivability rate of those passengers, whose cabin is known is 2:1, while situation in case the cabin is unknown is opposite. This would be a very useful feature to have.\n\nBut there is one problem with this feature. In real life, we wouldn't know in advance whether a cabin would be known or not (we can't know an outcome before an event happened). That's why this feature is rather \"artificial\". Sure, it can improve the score of our model for this competition, but using it is kinda cheating.\n\n**(decide what u wanna do with that feature and finish the description)**"},{"metadata":{"_uuid":"5bbd986aab97f11711d6182953d52550703bed15","_cell_guid":"4297328d-290b-4e72-8b84-cc868357f025"},"cell_type":"markdown","source":"### Age categories\n\n** * (explain why categories) * **\n\nLet's start with Age. The most logical way is to devide age into age categories: young, adult, and elder. Let's say that passenger of the age of 16 and younger are children, older than 50 are elder, and anyone else is adult."},{"metadata":{"_uuid":"6364b58bfcfcccffda85ce4ce830a820a3aed64c","_cell_guid":"210204ef-3bce-4e3c-b2fe-67d7aa1fa2ac","collapsed":true,"trusted":false},"cell_type":"code","source":"def get_age_categories(age):\n    if(age <= 16):\n        return 'child'\n    elif(age > 16 and age <= 50):\n        return 'adult'\n    else:\n        return 'elder'\n    \ntrain['AgeCategory'] = train['Age'].apply(get_age_categories)\ntest['AgeCategory'] = test['Age'].apply(get_age_categories)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1b699339414b7d8bb13feaff1584e672d480536b","_cell_guid":"c2f6dbc6-8c5f-4634-b7a8-1640aa87c95a","collapsed":true,"trusted":false},"cell_type":"code","source":"sns.countplot('AgeCategory', data=train, hue='Survived')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"998cd34179afe136d8230caa95cbebe8de9bd211","_cell_guid":"41565a68-ca31-4cc0-9849-1620a5ab939b"},"cell_type":"markdown","source":"** (...) **"},{"metadata":{"_uuid":"97678f095b5e19455b82c08d6b1513c55bdeb1ae","_cell_guid":"2f9a26c3-78e4-4991-9b35-d73e84bfbf56"},"cell_type":"markdown","source":"### Family size category\n\nNow lets do the same for the family size: we will separate it into TraveledAlone, WithFamily, and WithLargeFamily (bigger than 3, where the survivability rate changes the most)"},{"metadata":{"_uuid":"15617c163dd8925dd93f82be640c77d28654921f","_cell_guid":"294d80cb-7cb6-43aa-929e-d273a5534e32","collapsed":true,"trusted":false},"cell_type":"code","source":"def get_family_category(family_size):\n    \n    if(family_size > 3):\n        return 'WithLargeFamily'\n    elif(family_size > 0 and family_size<= 3):\n        return 'WithFamily'\n    else:\n        return 'TraveledAlone'\n    \ntrain['FamilyCategory'] = train['FamilySize'].apply(get_family_category)\ntest['FamilyCategory'] = test['FamilySize'].apply(get_family_category)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e7e2972e30e180c6732c329b9ec68fe95626dc64","_cell_guid":"b76faf47-ca40-405f-8f8b-2956f3fae5a5"},"cell_type":"markdown","source":"** (needs a description depending on whether it will be included or not) ** "},{"metadata":{"_uuid":"25e3e69015c7b74818514a9717944365117e8510","_cell_guid":"14a0d43e-acdd-49a8-9962-2d1e0d366acf"},"cell_type":"markdown","source":"### Title category"},{"metadata":{"_uuid":"0df6957ce06d457f389c2b5d051c07d4cc1da1e8","_cell_guid":"5e618a62-b25f-40e0-beb0-9f5b8428def7","collapsed":true,"trusted":false},"cell_type":"code","source":"print(train.Title.unique())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"21860329ca0ffdccc5c78ad7991145bec6b6384e","_cell_guid":"c55c134b-205e-4850-8f5c-2a0b539ad7cf","collapsed":true,"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(12, 10))\nsns.countplot('Title', data=train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3ed29c3d2fa8ad3b568137d54e3242d724bfa8b0","_cell_guid":"f75f992b-cf82-49b5-9871-ec6e51795d43","collapsed":true,"trusted":false},"cell_type":"code","source":"titles_to_cats = {\n    'HighClass': ['Lady.', 'Sir.'],\n    'MiddleClass': ['Mr.', 'Mrs.'],\n    'LowClass': []\n}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"498ffd1303704958a9eea7c1cc4f7eae137e6e37","_cell_guid":"d30dfefe-3be9-4052-8102-b90438d4978c"},"cell_type":"markdown","source":"### Fare scaling\n\nIf we take a look at the Fare distribution, we will see that it is scattered a lot:"},{"metadata":{"_uuid":"9def8c60494d6fdba659ff96cbbd83ab36a8ba11","_cell_guid":"1dcb5a71-1d85-408b-810a-bbf0ca9196e8","collapsed":true,"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(10, 8))\nsns.distplot(train['Fare'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d1b59e0b7241fddc4a3de1c158b2df899efd7a55","_cell_guid":"9752d8e8-ba32-45be-8e57-5edef4aae9eb"},"cell_type":"markdown","source":"# Creating the model:"},{"metadata":{"_uuid":"594f1f939c5e9e8a1cbc96e14406a656145873f9","_cell_guid":"3ac4d934-de36-4c86-bf14-14ca08a3f88d","collapsed":true},"cell_type":"markdown","source":"Now that we have all the data we need, we can start building the model. \n\nFirst of all, we need to prepare the data for the actual model. Classification algorithms work only with numbers or True/False values. For example, model can't tell the difference in Sex at the moment because we have text in that field. What we can do is transform the values of this feature into True or False (IsMale = True for males and IsMale = False for women).\n\nFor this purpose we will use two methods: transofrmation data into numerical values and dummies.\n\nLets start with Sex and transformation:"},{"metadata":{"_uuid":"15673d146e91e9231103f160894beef38a9a6676","_cell_guid":"8b4eb17b-3426-4579-89a0-f077e95fdc7f","collapsed":true,"trusted":false},"cell_type":"code","source":"train['Sex'] = train['Sex'].astype('category').cat.codes\ntest['Sex'] = test['Sex'].astype('category').cat.codes\ntrain[['Name', 'Sex']].head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f4e37421fe77f8d50734755978ab357da4438cf","_cell_guid":"814e92c3-9888-4f1c-b345-a30ab1fb8833"},"cell_type":"markdown","source":"As we see, the Sex column is now binary and takes 1 for males and 0 for females. Now classifiers will be able to work with it. \n\nNow we will transform Embarked column, but with a different method:"},{"metadata":{"_uuid":"1a36d406071cb34b159ee094f94ab77513d45ea9","_cell_guid":"7ec3dadc-30d1-4fdc-9e67-0145c9aac30f","collapsed":true,"trusted":false},"cell_type":"code","source":"embarkedCat = pd.get_dummies(train['Embarked'])\ntrain = pd.concat([train, embarkedCat], axis=1)\ntrain.drop('Embarked', axis=1, inplace=True)\n\nembarkedCat = pd.get_dummies(test['Embarked'])\ntest = pd.concat([test, embarkedCat], axis=1)\ntest.drop('Embarked', axis=1, inplace=True)\n\ntrain[['Q', 'S', 'C']].head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"426f5a0b27704d5aa300a5de2feea95d5847a5a4","_cell_guid":"25a93632-ac54-4632-ada7-a37346433b8d"},"cell_type":"markdown","source":"We used dummies, which replaced the Embarked column with three new columns corresponding to the values in the old column. Lets do the same for family size and age categories:"},{"metadata":{"_uuid":"96e850c704a140be2e1471af5c88ebce4a905905","_cell_guid":"144a5213-fe88-4522-898e-2b1461b0a9a9","collapsed":true,"trusted":false},"cell_type":"code","source":"# for the train set\nfamilyCat = pd.get_dummies(train['FamilyCategory'])\ntrain = pd.concat([train, familyCat], axis=1)\ntrain.drop('FamilyCategory', axis=1, inplace=True)\n\nageCat = pd.get_dummies(train['AgeCategory'])\ntrain = pd.concat([train, ageCat], axis=1)\ntrain.drop('AgeCategory', axis=1, inplace=True)\n\n#and for the test\nfamilyCat = pd.get_dummies(test['FamilyCategory'])\ntest = pd.concat([test, familyCat], axis=1)\ntest.drop('FamilyCategory', axis=1, inplace=True)\n\nageCat = pd.get_dummies(test['AgeCategory'])\ntest = pd.concat([test, ageCat], axis=1)\ntest.drop('AgeCategory', axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"524d05080c32f5da6688d6097fbf69e74a4e9eef","_cell_guid":"68782b7c-4c47-4c20-a93a-e9ff34e007ad","collapsed":true,"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(14,12))\nsns.heatmap(train.drop('PassengerId', axis=1).corr(), annot=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"911a73257f4228f149c0735807ba65ebd89d2876","_cell_guid":"c97b7f5d-3327-48ec-8c83-4e28174a21d5"},"cell_type":"markdown","source":"# Modelling\nNow we need to select a classification algorithm for the model. There are plenty of decent classifiers, but which is the best for this task and which one should we choose? \n\n*Here's the idea:* we will take a bunch of classifiers, test them on the data, and choose the best one.\n\nIn order to do that, we will create a list of different classifiers and see how each of them performs on the training data. To select the best one, we will evaluate them using cross-validation and compare their accuracy scores (percentage of the right answers). I decided to use Random Forest, KNN, SVC, Decision Tree, AdaBoost, Gradient Boost, Extremely Randomized Trees, and Logistic Regression."},{"metadata":{"_uuid":"f509f2d4e343c9be5e6247f6504dc615d301f708","_cell_guid":"4af06f63-6053-49f8-90b9-1e0382552a96","collapsed":true,"trusted":false},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nclassifiers = [\n    RandomForestClassifier(),\n    KNeighborsClassifier(),\n    SVC(),\n    DecisionTreeClassifier(),\n    AdaBoostClassifier(),\n    GradientBoostingClassifier(),\n    ExtraTreesClassifier(),\n    LogisticRegression()\n]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7271d3934180bcebecd186c72aefc4b6ebcbc8b6","_cell_guid":"02eb1190-0005-41d1-9953-db007b55b57b"},"cell_type":"markdown","source":"Now we need to select the features that will be used in the model and drop everything else. Also, the training data has to be split in two parts: *X_train* is the data the classifiers will be trained on, and *y_train* are the answers."},{"metadata":{"_uuid":"a3dc71d2ffde56675ecc7eec40d2bbd5ea1369d5","_cell_guid":"dc68726a-30b5-4cb0-8a93-2d6789670815","collapsed":true,"trusted":false},"cell_type":"code","source":"X_train = train.drop(['PassengerId', 'Survived', 'SibSp', 'Parch', 'Ticket', 'Name', 'Cabin', 'Title', 'FamilySize'], axis=1)\ny_train = train['Survived']\n\nX_final = test.drop(['PassengerId', 'SibSp', 'Parch', 'Ticket', 'Name', 'Cabin', 'Title', 'FamilySize'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"41d182ee20d6580b4d65e38d809ecc6fabfe4965","_cell_guid":"86a4318e-93f2-40db-8b38-cbf3494e6508"},"cell_type":"markdown","source":"We will use K-Folds as cross-validation. It splits the data into \"folds\", ** (...) **"},{"metadata":{"_uuid":"348d80d8b005384465058a3a330a71c157768699","_cell_guid":"e79c2a6a-e611-416a-b269-a3f39cb25380","collapsed":true,"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import KFold\n\n# n_splits=5\ncv_kfold = KFold(n_splits=10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"16c7c4e8559eb1b98a7d7aed46209e3d0604fcca","_cell_guid":"fb8eb0c5-e008-45b1-a31e-953842e160c9"},"cell_type":"markdown","source":"Now we evaluate each of the classifiers from the list using K-Folds. The accuracy scores will be stored in a list.\n\nThe problem is that K-Folds evaluates each algorithm several times. As result, we will have a list of arrays with scores for each classifier, which is not great for comparison. \n\nTo fix it, we will create another list of means of scores for each classifier. That way it will be much easier to compare the algorithms and select the best one.  "},{"metadata":{"_uuid":"feb9b874c9c73cc9fc5b484d65e0fad9666b3e0b","_cell_guid":"4103caf8-aa83-42fc-b07d-91422e1f8777","collapsed":true,"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\nclass_scores = []\nfor classifier in classifiers:\n    class_scores.append(cross_val_score(classifier, X_train, y_train, scoring='accuracy', cv=cv_kfold))\n    \nclass_mean_scores = []\nfor score in class_scores:\n    class_mean_scores.append(score.mean())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d5039838cc2e23fea70241d645f675d6b03d5b7","_cell_guid":"4f3aee49-528a-4b22-bec0-1c82906c16df"},"cell_type":"markdown","source":"Now that we have the mean accuracy scores, we need to compare them somehow. But since it's just a list of numbers, we can easily plot them. First, let's create a data frame of classifiers names and their scores, and then plot it:"},{"metadata":{"_uuid":"2dfbb841537f0dcab02de07310433c8d02113133","_cell_guid":"bfc1d3b5-565f-4d09-bd05-8f3ab134e989","collapsed":true,"trusted":false},"cell_type":"code","source":"scores_df = pd.DataFrame({\n    'Classifier':['Random Forest', 'KNeighbors', 'SVC', 'DecisionTreeClassifier', 'AdaBoostClassifier', \n                  'GradientBoostingClassifier', 'ExtraTreesClassifier', 'LogisticRegression'], \n    'Scores': class_mean_scores\n})\n\nprint(scores_df)\nsns.factorplot('Scores', 'Classifier', data=scores_df, size=6)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9e1650c043da6910daa40ff530fd9ff8fa3c2f27","_cell_guid":"9b7b9ac9-6e9f-49b1-8097-c1b5f670556c"},"cell_type":"markdown","source":"Two best classifiers happened to be Gradient Boost and Logistic Regression. Since Logistic Regression got sligthly lower score and is rather easily overfitted, we will use Gradient Boost. "},{"metadata":{"_uuid":"4c0a9030d519658e2bb831e8127862b6d812576d","_cell_guid":"38506e6f-8d5f-47c3-845b-b24c1bf737f5"},"cell_type":"markdown","source":"### Selecting the parameters\nNow that we've chosen the algorithm, we need to select the best parameters for it. There are many options, and sometimes it's almost impossible to know the best set of parameters. That's why we will use Grid Search to test out different options and choose the best ones.\n\nBut first let's take a look at all the possible parameters of Gradient Boosting classifier:"},{"metadata":{"_uuid":"ae07617211aee7533d61e8a9a02b365dca407393","_cell_guid":"28d4b6c9-fe98-493d-a6db-43b2f81413b0","collapsed":true,"trusted":false},"cell_type":"code","source":"g_boost = GradientBoostingClassifier()\ng_boost.get_params().keys()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"04add4849834e885440ea436e975e2bfa5cdc773","_cell_guid":"df26ea9a-e9de-4287-b3e5-6f027e05091e"},"cell_type":"markdown","source":"We will test different options for min_samples_leaf, min_samples_split, max_depth, and loss parameters. I will set n_estimators to 100, but it can be increased since Gradient Boosting algorithms generally don't tend to overfit."},{"metadata":{"_uuid":"86fc3a87c9bde13420e1562f12798f3e9026c533","_cell_guid":"c372bf6c-fa04-4f17-914a-f336928911af","collapsed":true,"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    'loss': ['deviance', 'exponential'],\n    'min_samples_leaf': [2, 5, 10],\n    'min_samples_split': [2, 5, 10],\n    'n_estimators': [100],\n    'max_depth': [3, 5, 10, 20]\n}\n\ngrid_cv = GridSearchCV(g_boost, param_grid, scoring='accuracy', cv=cv_kfold)\ngrid_cv.fit(X_train, y_train)\ngrid_cv.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"13eafa6875fd0107678fe06d7b1a8c2938a6a590","_cell_guid":"60468b0b-5ed6-47b8-8dd4-2ef72cbd1f15","collapsed":true,"trusted":false},"cell_type":"code","source":"print(grid_cv.best_score_)\nprint(grid_cv.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3cffaf9f2c34db0c1c381d227eb64235b5b7d4cc","_cell_guid":"d615075c-3636-4de1-bd28-27d5d3babc13"},"cell_type":"markdown","source":"Now that we have the best parameters we could find, it's time to create and train the model on the training data."},{"metadata":{"_uuid":"e7305c9c8f6ea7f6c1d46e8434cbbf1b739e4489","_cell_guid":"191956ba-0830-48e9-bf5c-4ad98ca451e0","collapsed":true,"trusted":false},"cell_type":"code","source":"g_boost = GradientBoostingClassifier(min_samples_split=5, loss='deviance', n_estimators=1000, \n                                     max_depth=3, min_samples_leaf=2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7edd59271223024bf81a85c9acaaa08e3eb4e571","_cell_guid":"c4bdbb58-3a4f-438b-b8f4-d9a0ce8e600d","collapsed":true,"trusted":false},"cell_type":"code","source":"g_boost.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7dbd5ad24faa1b21cdf3d69bc137fea03431affa","_cell_guid":"6ac32eed-e56a-401b-b0cd-d5f0097a00ed","collapsed":true,"trusted":false},"cell_type":"code","source":"feature_values = pd.DataFrame({\n    'Feature': X_final.columns,\n    'Importance': g_boost.feature_importances_\n})\n\nprint(feature_values)\nsns.factorplot('Importance', 'Feature', data=feature_values, size=6)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9c70fc769f58ce5c88dbde548027954ce7908ba8","_cell_guid":"52515e11-8d7f-4993-9f14-087ef8e445b1"},"cell_type":"markdown","source":"### Prediction on the testing set and output\nNow our model is ready, and we can make a prediction on the testing set and create a .csv output for submission."},{"metadata":{"_uuid":"205d9f79a39229b0a72dfa3ca4ff46bd1a3b4bc9","_cell_guid":"24fca890-76d6-41ef-a18f-c1ad5b27b039","collapsed":true,"trusted":false},"cell_type":"code","source":"prediction = g_boost.predict(X_final)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"87d5245aaf69c84f637832a572369f86cc55fc99","_cell_guid":"efb85a53-ae34-43eb-8739-931b6540ec32","collapsed":true,"trusted":false},"cell_type":"code","source":"submission = pd.DataFrame({\n    'PassengerId': test['PassengerId'],\n    'Survived': prediction\n})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7ab2711e606d54ed8635032798e8359dcfe0df6b","_cell_guid":"3ae22ed2-d925-49bc-988f-3b73165e2ede","collapsed":true,"trusted":false},"cell_type":"code","source":"#submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}