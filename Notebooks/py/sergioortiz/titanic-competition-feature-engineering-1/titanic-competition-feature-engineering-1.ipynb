{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# Titanic Competition: Feature Engineering - Iteration 1\n\nWelcome! This kernel is part of the *Titatic competition learning series* which can be accessed from <a href=\"https://www.kaggle.com/sergioortiz/titanic-competition-a-learning-diary\">here</a>.  \n\nLet's start this section by loading data for both training and test data sets."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import pandas as pd\ninput_io_dir=\"../input/\"\noriginal_train_data=pd.read_csv(input_io_dir+\"train.csv\")\noriginal_test_data=pd.read_csv(input_io_dir+\"test.csv\")\nprint('PrepareDataSets:original_train_data',original_train_data.shape)\nprint('PrepareDataSets:original_test_data',original_test_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"33882f4f30aae8a3f6753304daef3e90c1fdf466"},"cell_type":"markdown","source":"Columns doesn't match as the training set currently includes the labels (Survived).  \nNow we will separate this in different variables."},{"metadata":{"trusted":true,"_uuid":"41eba162b7591617562d9542f3db65b4a5394b52"},"cell_type":"code","source":"passengerId = original_test_data['PassengerId']\nprint('PrepareDataSets:passengerId (%d)'%len(passengerId))\nsurvived=original_train_data['Survived']\nprint('PrepareDataSets:Survived (%d)'%len(survived))\n# Once stored, let's drop the column\noriginal_train_data=original_train_data.drop('Survived',axis=1)\n# This will let us build a combined dataset\noriginal_all_data=original_train_data.append(original_test_data)\nprint('PrepareDataSets:original_alldata',original_all_data.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ace3f6f8229c680a5e807db42065060c36592fc7"},"cell_type":"markdown","source":"### Adding new features\nFor adding new features we will use a pipeline and an extension class.  \nIn the future, this will facilitate exploring how including/excluding features affects model performance.  \nAt the moment we will keep it simple..."},{"metadata":{"trusted":true,"_uuid":"187e64984eb1fe9bf2b71225c7ce76ec0af650ab"},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n# Support functions\n################################################################################################\n# Replace texts based on a dictionary\ndef multipleReplace(text, wordDic):\n    for key in wordDic:\n        if text.lower()==key.lower():\n            text=wordDic[key]\n            break\n    return text\n\n# Normalise title names by grouping them\ndef normaliseTitle(title):\n    wordDic = {\n    'Mlle': 'Miss',\n    'Ms': 'Mrs',\n    'Mrs':'Mrs',\n    'Master':'Master',\n    'Mme': 'Mrs',\n    'Lady': 'Nobility',\n    'Countess': 'Nobility',\n    'Capt': 'Army',\n    'Col': 'Army',\n    'Dona': 'Other',\n    'Don': 'Other',\n    'Dr': 'Other',\n    'Major': 'Army',\n    'Rev': 'Other',\n    'Sir': 'Other',\n    'Jonkheer': 'Other',\n    }    \n    title=multipleReplace(title,wordDic)\n    return title\n\n# Extract Title feature from name\ndef extractTitleFromName(name):\n    pos_point=name.find('.')\n    if pos_point == -1: return \"\"\n    wordList=name[0:pos_point].split(\" \")\n    if len(wordList)<=0: return \"\"\n    title=wordList[len(wordList)-1]\n    normalisedTitle=normaliseTitle(title)\n    return normalisedTitle\n\n# Extract TicketType feature from name\ndef getTicketType(name, normalise=True):\n    item=name.split(' ')\n    itemLength=len(item)\n    if itemLength>1:\n        ticketType=\"\"\n        for i in range(0,itemLength-1):\n            ticketType+=item[i].upper()\n    else:\n        ticketType=\"NORMAL\"\n    if normalise==True:\n        ticketType= ticketType.translate(str.maketrans('','','./'))\n    return ticketType\n\n# Custom pipeline filter to add new features\n################################################################################################\nclass CustomFeatureExtender(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        X['Title']=X['Name'].apply(lambda x: extractTitleFromName(x)).astype('category')\n        X['NoCabin']=X['Cabin'].isnull().apply(lambda x: 1 if x is True else 0).astype('category')\n        X['TicketType']=X['Ticket'].apply(lambda x: getTicketType(x)).astype('category')\n        X['IsAlone']=(X[\"SibSp\"]+X[\"Parch\"]).apply(lambda x: 0 if x>0 else 1).astype('category')\n        X['FamilySize']=X[\"SibSp\"]+X[\"Parch\"]+1\n        return X\n################################################################################################\npreprocessor=Pipeline(steps=[\n        ('extender', CustomFeatureExtender()),\n    ])\nprint(\"PrepareDataSets:Features extended\")\nenriched_train_data=preprocessor.fit_transform(original_train_data)\nenriched_test_data=preprocessor.fit_transform(original_test_data)\nprint('PrepareDataSets:enriched_train_data',enriched_train_data.shape)\nprint('PrepareDataSets:enriched_test_data',enriched_test_data.shape)\nenriched_train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"068e774b1b3ba4980cff1ad5df1c302ecc6e4fd0"},"cell_type":"markdown","source":"Great! We have added planned features.  \n### Excluding features\nLet's get rid of those features we will not be using for training..."},{"metadata":{"trusted":true,"_uuid":"1581b1d934837482ad8d89c29f34c1b2d8c272b1"},"cell_type":"code","source":"exclude_features=['Name','SibSp','Parch','Ticket','Cabin']\nfiltered_train_data=enriched_train_data.drop(exclude_features,axis=1)\nfiltered_test_data=enriched_test_data.drop(exclude_features[1:],axis=1)\nprint('PrepareDataSets:filtered_train_data',filtered_train_data.shape)\nprint('PrepareDataSets:filtered_test_data',filtered_test_data.shape)\nfiltered_train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b1dceb1e00b008ea8881721ccf425d469d0694bb"},"cell_type":"markdown","source":"### Feature processing  \nNext, we have to process both numeric and categorical data.  \nFor this purpose, we will create different pipelines as the processing varies.   \n**Numeric data**  \nFirst, missing values should be solved with imputing strategies - e.g. filling with the median of existing values.\nAddionally, numeric data should be scaled as training is not as effective when different features present different value scales - fare, age...  \nFinally, considering the reduced amount of data, we will convert continuous variables into discrete. We assume the training will be more effective grouping values into ranges.\n\n**Categorical data**  \nOn the other hand, categorical data must be encoded in a different way to improve learning effectiveness.\nFor example, turn labels such as \"male\" into a number. However, this transformation can lead to training issues as the model may assume there is a linear relationship between feature values (as it happens in continuous variables such as Age) when this is not true (close values can be as unrelated as others as they only representa categories). This can be solved using one hot encoding, which turns a categorical feature into N-1 features. For example, Sex turns into Sex_Male which has two potential values (0 and 1).\n\nLet's start with the numeric transformations...applying each filter step by step."},{"metadata":{"trusted":true,"_uuid":"8a406507d61278bfe6b7d29dfa62e887b2a4d668"},"cell_type":"code","source":"from sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n# Helper functions\n#############################################################################\n\n# Column selection\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return X[self.attribute_names]\n\n# Assign names to columns\nclass ColumnLabeler(BaseEstimator, TransformerMixin):\n    def __init__(self, column_names):\n        self.column_names = column_names\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        X=pd.DataFrame(X,columns=self.column_names)\n        return X\n\n# Transform features - continuous to discrete\nclass CustomRangeTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        Xdf=pd.DataFrame(X)\n        Xdf.loc[Xdf['Age'] <= 16, 'Age'] = 0\n        Xdf.loc[(Xdf['Age'] > 16) & (Xdf['Age'] <= 32), 'Age'] = 1\n        Xdf.loc[(Xdf['Age'] > 32) & (Xdf['Age'] <= 48), 'Age'] = 2\n        Xdf.loc[(Xdf['Age'] > 48) & (Xdf['Age'] <= 64), 'Age'] = 3\n        Xdf.loc[ Xdf['Age'] > 64, 'Age'] = 4\n        Xdf.loc[Xdf['Fare'] <= 7.91, 'Fare'] = 0\n        Xdf.loc[(Xdf['Fare'] > 7.91) & (Xdf['Fare'] <= 14.454), 'Fare'] = 1\n        Xdf.loc[(Xdf['Fare'] > 14.454) & (Xdf['Fare'] <= 31), 'Fare']   = 2\n        Xdf.loc[ Xdf['Fare'] > 31, 'Fare'] = 3\n        return Xdf\n\n# Numeric pipeline into action!\n#############################################################################\nnumeric_features=['Age','Fare','FamilySize']\n# Let's first ensure there are no missing values\nnumeric_pipeline_step1 = Pipeline(steps=[\n    ('selector', DataFrameSelector(numeric_features)),\n    ('imputer', SimpleImputer(strategy='median')),\n    ('labeler',ColumnLabeler(numeric_features))\n    ])\nnum_encoded_train_data=pd.DataFrame(numeric_pipeline_step1.fit_transform(filtered_train_data))\nprint(num_encoded_train_data.head())\nprint('-----------------------------------------')\nprint('Notice below there are no missing values ')\nprint('-----------------------------------------')\nnum_encoded_train_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"94e52da256bffe81296a20f02ee5c31cb3bae220"},"cell_type":"code","source":"# Now, let's transform continuous into discrete\nnumeric_pipeline_step2 = Pipeline(steps=[\n    ('selector', DataFrameSelector(numeric_features)),\n    ('imputer', SimpleImputer(strategy='median')),\n    ('labeler',ColumnLabeler(numeric_features)),\n    ('range_transformer',CustomRangeTransformer()),\n    ])\nnum_encoded_train_data=pd.DataFrame(numeric_pipeline_step2.fit_transform(filtered_train_data))\nnum_encoded_train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7ec9207ba866f2e11eb188b4e88a9fdf5a691445"},"cell_type":"code","source":"# Let's apply the complete pipeline, including scaling\nnumeric_pipeline = Pipeline(steps=[\n    ('selector', DataFrameSelector(numeric_features)),\n    ('imputer', SimpleImputer(strategy='median')),\n    ('labeler',ColumnLabeler(numeric_features)),\n    ('range_transformer',CustomRangeTransformer()),\n    ('scaler', StandardScaler()),\n    ('labeler2',ColumnLabeler(numeric_features)),\n    ])\nnum_encoded_train_data=pd.DataFrame(numeric_pipeline.fit_transform(filtered_train_data))\nprint('PrepareDataSets:num_encoded_train_data',num_encoded_train_data.shape)\nnum_encoded_test_data=pd.DataFrame(numeric_pipeline.fit_transform(filtered_test_data))\nprint('PrepareDataSets:num_encoded_test_data',num_encoded_test_data.shape)\nnum_encoded_train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"939f9c51f0be50e67c22758ef1c9c03e9838f38e"},"cell_type":"markdown","source":"It is curious that some of the filters in the pipeline destroy the dataset columns - e.g. Imputer or StandardScaler. This is very annoying during  development and may even difficult subsequent processing.  \nTo prevent this, I created the simple ColumnLabeler to get column names back again whenever a filter destroys them.\n\nLet's continue now with categorical data..."},{"metadata":{"trusted":true,"_uuid":"869cead51cd22ea300cc7091095ffe3a29dd9297"},"cell_type":"code","source":"# Helper functions\n#############################################################################\n\n# Fill missing data - just set to most frequent value as there are only 2 missing values\nclass CustomFiller(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        X['Embarked'].fillna('S')\n        return X\n\n# One hot encoding using pandas's getdummies\nclass DummyTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, dummy_na):\n        self.dummy_na=dummy_na\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        df = pd.DataFrame()\n        # We iterate so that we set column name as prefix for newly created features\n        for col in sorted(X.columns):\n            dummies=pd.get_dummies(X[col],prefix=col, drop_first=True,dummy_na=self.dummy_na)\n            df[dummies.columns]=dummies\n        X=df\n        X=X.astype('category')\n        return X\n\n# Categorical pipeline in action!\n#############################################################################\ncategorical_features = ['Embarked', 'Sex','Pclass','Title','NoCabin','IsAlone']\ncategorical_pipeline = Pipeline(steps=[\n        ('selector',DataFrameSelector(categorical_features)),\n        ('filler', CustomFiller()),\n        ('dummy', DummyTransformer(dummy_na=False)),\n    ])\ncat_encoded_train_data=pd.DataFrame(categorical_pipeline.fit_transform(filtered_train_data))\nprint('PrepareDataSets:cat_encoded_train_data',cat_encoded_train_data.shape)\ncat_encoded_test_data=pd.DataFrame(categorical_pipeline.fit_transform(filtered_test_data))\nprint('PrepareDataSets:cat_encoded_test_data',cat_encoded_test_data.shape)\ncat_encoded_train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4b18337dadd6ed0020ee9967b360872b7ce495c5"},"cell_type":"markdown","source":"Great! Now, we will combine the two different data sets - numeric and categorical features.  \nInitially, I tried to use the FeatureUnion pipeline filter but it completely destroyed all columns. Consequently, I decided to do it manually."},{"metadata":{"trusted":true,"_uuid":"58aa8ce145fb32ac50d26b88611286b32d9c5a81"},"cell_type":"code","source":"encoded_train_data=pd.concat([num_encoded_train_data,cat_encoded_train_data],axis=1)\nprint('PrepareDataSets:encoded_train_data',encoded_train_data.shape)\nencoded_test_data=pd.concat([num_encoded_test_data,cat_encoded_test_data],axis=1)\nprint('PrepareDataSets:encoded_test_data',encoded_test_data.shape)\nencoded_train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2c6bb873895ab4da667931774bcb3503ef218442"},"cell_type":"code","source":"encoded_test_data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"009921c11db7523bbd068154c5108034efa1e963"},"cell_type":"markdown","source":"Ups...if you have a look at the two datasets you will notice that some of the columns may be absent in one of the datasets. For example, Title_Nobility is not in the test dataset as there are no samples inclusing this value.\nIt will be necessary to normalise columns among datasets to ensure both datasets have the same number of features."},{"metadata":{"trusted":true,"_uuid":"357b014f87d15c71253f04db35d2429f84ebf68a"},"cell_type":"code","source":"# Helper to normalise columns between training and test set\ndef NormaliseColumns(dataframeA,dataframeB):\n    for testCol in dataframeB.columns:\n        if testCol not in dataframeA.columns:\n            dataframeA[testCol]=0\n    for trainCol in dataframeA.columns:\n        if trainCol not in dataframeB.columns:\n            dataframeB[trainCol]=0\n    return dataframeA,dataframeB\n\nencoded_train_data,encoded_test_data=NormaliseColumns(encoded_train_data,encoded_test_data)\nprint('PrepareDataSets:Adjusted encoded_train_data',encoded_train_data.shape)\nprint('PrepareDataSets:Adjusted encoded_test_data',encoded_test_data.shape)\nencoded_train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"41ca1181ac7a4f3d8dc6758ecedfccb7dd193670"},"cell_type":"code","source":"encoded_test_data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"04f44cf3dd6b9f992ba0f4cd4b8f43226be0c685"},"cell_type":"markdown","source":"Watch out! The column order has varied...  \nLet's re-index to solve this problem."},{"metadata":{"trusted":true,"_uuid":"c969718f081cf16e5646ff9627fd74848006e6d0"},"cell_type":"code","source":"encoded_train_data=encoded_train_data.reindex(sorted(encoded_train_data.columns), axis=1)\nencoded_test_data=encoded_test_data.reindex(sorted(encoded_test_data.columns), axis=1)\nencoded_train_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"711c244e71959b818ea117a5e5b8b3611fcb4d72"},"cell_type":"code","source":"encoded_test_data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f651b7aa45cec898ec066bc06ada27cf9555e515"},"cell_type":"markdown","source":"Great!  Now the two datasets have the same features with the same order.\n\nFinally, let's provide nice names to processed data and save them for later processing"},{"metadata":{"trusted":true,"_uuid":"8c1d7fec5aeecdd2c6a590092cd5224cb990f843"},"cell_type":"code","source":"train_features=encoded_train_data\ntest_features=encoded_test_data\ntrain_labels=survived\npassengerId.to_csv(\"passengerId.csv\",index=False,header=False)\ntrain_features.to_csv(\"train_features.csv\",index=False,header=True)\ntest_features.to_csv(\"test_features.csv\",index=False,header=True)\ntrain_labels.to_csv(\"train_labels.csv\",index=False,header=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3788955c02539b5d319484b14ac559ad260ddef5"},"cell_type":"markdown","source":"That's all! We've finished with all processing and we're ready to start evaluating models.\nIn the solution script, I define a function called PrepareDataSets which includes all this processing and returns:\n* PassengerId list\n* Train features\n* Train labels\n* Test features\n\nIn future revisions I will improve this function so that I can prepare different datasets and evaluate their performance.\nFor example, setting parameters for including more or less features, using different scalers, using continuous or ranges in some features, etc...\n\nHope you enjoyed and found it useful!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}