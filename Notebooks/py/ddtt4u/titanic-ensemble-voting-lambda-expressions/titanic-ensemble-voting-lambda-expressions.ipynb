{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "299a3527-8609-20f8-cccf-617a3dfe1344"
      },
      "source": [
        "My shot at the Titanic using ensemble voting of several classifiers.\n",
        "\n",
        "* Data cleaning and preparation using lambda expressions\n",
        "* Grid search for best classifier parameters \n",
        "* Ensemble voting of 6 different classifiers\n",
        " - Adaboost \n",
        " - Bagging\n",
        " - Gradient Boosting\n",
        " - Random Forest\n",
        " - Extra Trees\n",
        "\n",
        " Read more on scikit ensemble methods : http://scikit-learn.org/stable/modules/ensemble.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "db9dcb61-c1f3-1259-1691-bed633162218"
      },
      "outputs": [],
      "source": [
        "%reset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.ensemble import ( AdaBoostClassifier,\n",
        "                              BaggingClassifier,\n",
        "                              GradientBoostingClassifier,\n",
        "                              RandomForestClassifier,\n",
        "                              ExtraTreesClassifier,                                                           \n",
        "                              VotingClassifier)\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "train_data = pd.read_csv(\"../input/train.csv\")\n",
        "test_data = pd.read_csv(\"../input/test.csv\")\n",
        "\n",
        "train_data.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "16a7c2b4-50f5-b8c3-9e93-7279ce9f8a2b"
      },
      "outputs": [],
      "source": [
        "def prepare_data(train_data, test_data):\n",
        "    \n",
        "    data = train_data.append(test_data)\n",
        "    data.index = range(0, len(data))    \n",
        "    \n",
        "    #SAME SURNAME SURVIVORS\n",
        "    data['Surname'] = data['Name'].map(lambda name:name.split(',')[0].strip())\n",
        "    survivors_by_surname = data.groupby(['Surname'])['Survived'].sum()\n",
        "    surname_count = data.groupby(['Surname'])['Survived'].count()\n",
        "    survivors_by_surname_dict = dict(zip(survivors_by_surname.index, survivors_by_surname.values))\n",
        "    surname_count_dict = dict(zip(surname_count.index, surname_count.values))\n",
        "    data['Surname_Survivors'] = data.apply( lambda x: max(0, survivors_by_surname_dict.get(x.Surname) - 1), axis = 1)\n",
        "    data['Surname_Count'] = data.apply( lambda x:  surname_count_dict.get(x.Surname), axis = 1)    \n",
        "    \n",
        "    #TITLE\n",
        "    data['Title'] = data['Name'].map(lambda name:name.split(',')[1].split('.')[0].strip())\n",
        "    titles = list(enumerate(np.unique(data.Title.dropna())))\n",
        "    title_dict = {\n",
        "                        \"Mr\" :        \"1\",\n",
        "                        \"Capt\":       \"2\",\n",
        "                        \"Col\":        \"2\",\n",
        "                        \"Major\":      \"2\",\n",
        "                        \"Jonkheer\":   \"2\",\n",
        "                        \"Don\":        \"2\",\n",
        "                        \"Rev\":        \"2\",\n",
        "                        \"Sir\" :       \"2\",\n",
        "                        \"Dr\":         \"2\",\n",
        "                        \"Master\" :    \"3\",\n",
        "                        \"Ms\":         \"4\",                        \n",
        "                        \"Miss\" :      \"4\",\n",
        "                        \"Mrs\" :       \"5\",\n",
        "                        \"Mlle\":       \"5\" ,\n",
        "                        \"the Countess\":\"6\",\n",
        "                        \"Dona\":       \"6\",\n",
        "                        \"Lady\" :      \"6\",\n",
        "                        \"Mme\":        \"6\"                                                                                                                     \n",
        "                        }\n",
        "    data['Title_Class'] = data.apply(lambda row: title_dict.get(row.Title), axis = 1).astype(int)\n",
        "            \n",
        "    #AGE\n",
        "    median_age_dict = {}    \n",
        "    for title in range(1,6):       \n",
        "        median_age_dict[title] = np.median(data.loc[data.Title_Class == title, ('Age')].dropna())\n",
        "    data.loc[data.Age.isnull(), ('Age')] = data[data.Age.isnull()].apply( lambda x: median_age_dict.get(x.Title_Class), axis = 1)    \n",
        "    data['Infant'] = data.apply( lambda row: int(row['Age'] <= 8), axis = 1)\n",
        "    data['Elderly'] = data.apply( lambda row: int(row['Age'] >= 65), axis = 1)\n",
        "    data['No_Parents'] = data['Parch'].map(lambda parch: int(parch == 0))\n",
        "    data['Infant_No_Parents'] = data['Infant'] & data['No_Parents']\n",
        "    \n",
        "    #SEX\n",
        "    data['Sex'] = data['Sex'].map({'female':0, 'male': 1}).astype(int)\n",
        "\n",
        "    #FAMILY SIZE\n",
        "    data['Family_Size'] = data['SibSp'] + data['Parch'] + 1 \n",
        "    data['Singleton'] = data.apply( lambda row: int(row['Family_Size'] <= 1), axis = 1)\n",
        "    data['Pair_Family'] = data.apply( lambda row: int(row['Family_Size'] == 2), axis = 1)\n",
        "    data['Big_Family'] = data.apply( lambda row: int(row['Family_Size'] >=3), axis = 1)\n",
        " \n",
        "    #TICKET\n",
        "    data.loc[data['Ticket'] == \"LINE\", ('Ticket')] = 'LINE 00000'\n",
        "    data['Ticket_Number'] = data.apply(lambda row: max(list(map(int, re.findall(r'\\d+', row['Ticket'])))) , axis = 1)\n",
        "    data['Ticket_String'] = data.apply(lambda row: re.sub(\"[0-9]\", \"\", row['Ticket']) , axis = 1)\n",
        "    data['Ticket_String'] = data.apply(lambda row: re.sub(\"/\", \"\", row['Ticket_String']) , axis = 1)\n",
        "    data['Ticket_String'] = data.apply(lambda row: re.sub(\"\\.\", \"\", row['Ticket_String']) , axis = 1)\n",
        "    Ticket_Strings = list(enumerate(np.unique(data.Ticket_String)))\n",
        "    Ticket_Strings_dict = { name : i for i, name in Ticket_Strings }\n",
        "    data['Ticket_String_Id'] = data.Ticket_String.map(lambda x: Ticket_Strings_dict[x]).astype(int)  \n",
        "    \n",
        "    #EMBARKED\n",
        "    data.loc[data.Embarked.isnull(), ('Embarked')] = train_data.Embarked.mode()[0]\n",
        "    Ports = list(enumerate(np.unique(data.Embarked)))\n",
        "    Ports_dict = { name : i for i, name in Ports }\n",
        "    data.Embarked = data.Embarked.map(lambda x: Ports_dict[x]).astype(int)  \n",
        "    \n",
        "    #FARE\n",
        "    Classes = np.unique(data.Pclass)\n",
        "    data.loc[data.Fare.isnull(), ('Fare')] = 0\n",
        "    median_fare_dict = {}\n",
        "    for embarked in range(0,3):\n",
        "        for pass_class in Classes:\n",
        "                median_fare_dict[(embarked, pass_class)] = np.median(data.Fare[(data.Embarked == embarked) & (data.Pclass == pass_class)].dropna())        \n",
        "    data.loc[data.Fare == 0, ('Fare')] = data.loc[data.Fare.isnull()].apply( lambda x: median_fare_dict.get(x.Embarked, x.Pclass), axis = 1)\n",
        "    data.loc[data.Fare.isnull(), ('Fare')] = data.loc[data.Fare.isnull()].apply( lambda x: median_fare_dict.get(x.Embarked, x.Pclass), axis = 1)\n",
        "    \n",
        "    #DECK\n",
        "    data['Deck'] = data.Cabin.dropna().apply(lambda row: row[0])\n",
        "    Decks = list(enumerate(np.unique(data.Deck.dropna())))\n",
        "    Decks_dict = { name : i for i, name in Decks }\n",
        "    data.Deck = data.Deck.dropna().map(lambda x: Decks_dict[x]).astype(int)\n",
        "    family_deck = data.groupby(['Surname'])['Deck'].median()\n",
        "    family_deck_dict = dict(zip(family_deck.index, family_deck.values))\n",
        "    data['Deck'] = data.apply( lambda x: family_deck_dict.get(x.Surname), axis = 1)\n",
        "    data.loc[data.Deck.isnull(), ('Deck')] = -1\n",
        "         \n",
        "    #CABINMATE SURVIVORS\n",
        "    cabinmates_survived = train_data.groupby(['Cabin'])['Survived'].sum() - 1\n",
        "    cabinmates_survived [ cabinmates_survived == -1.0] = 0.0\n",
        "    cabinmates_survived_dict = dict(zip(cabinmates_survived.index, cabinmates_survived.values))    \n",
        "    data['Cabinmates_Survived'] = data.apply( lambda x: cabinmates_survived_dict.get(x.Cabin), axis = 1)    \n",
        "    data.loc[data.Cabinmates_Survived.isnull(), ('Cabinmates_Survived')] = 0.0            \n",
        "    data['Family_Size_Surname'] = data.apply(lambda row: min(row.Surname_Count, row.Family_Size), axis = 1).astype(int)    \n",
        "    return data[:len(train_data)], data[len(train_data):]\n",
        "\n",
        "train_data, test_data = prepare_data(train_data, test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "163f96f3-4418-9b4f-e125-3642ee320da2"
      },
      "outputs": [],
      "source": [
        "#FEATURE SELECTION\n",
        "features = ['Pclass', \n",
        "            'Sex',\n",
        "            'Age',   \n",
        "            'Parch',\n",
        "            'SibSp',\n",
        "            'Ticket_Number', \n",
        "            'Ticket_String_Id',\n",
        "            'Fare',\n",
        "            'Embarked',            \n",
        "            'Family_Size',\n",
        "            'Deck',\n",
        "            'Cabinmates_Survived',                                             \n",
        "            'Surname_Survivors',\n",
        "            'Family_Size_Surname',\n",
        "            'Title_Class',\n",
        "            'Surname_Count',\n",
        "            'Infant',\n",
        "            'Elderly',\n",
        "            'Infant_No_Parents',\n",
        "            'Singleton',\n",
        "            'Pair_Family',\n",
        "            'Big_Family'\n",
        "            ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b96aff5f-4864-6ac8-e663-56d0e1854985"
      },
      "outputs": [],
      "source": [
        "#GRID SEARCH FOR HYPERPARAMETER OPTIMIZATION\n",
        "\n",
        "nontree_classifiers = (\n",
        "                AdaBoostClassifier(),\n",
        "                BaggingClassifier(),\n",
        "                GradientBoostingClassifier()               \n",
        "              )\n",
        "\n",
        "tree_classifiers = (\n",
        "                RandomForestClassifier(), \n",
        "                ExtraTreesClassifier()                                      \n",
        "            )\n",
        "\n",
        "nontree_parameter_grid = {                            \n",
        "                 'n_estimators': [50, 100, 150, 200, 400]                 \n",
        "                 }\n",
        "\n",
        "tree_parameter_grid = {            \n",
        "                 'max_depth' : [3, 5, 7, 9],\n",
        "                 'n_estimators': [50, 100, 150, 200, 400]                 \n",
        "                 }\n",
        "\n",
        "cross_validation = StratifiedKFold(n_splits = 5)\n",
        "\n",
        "for clf in nontree_classifiers:\n",
        "    grid_search = GridSearchCV(clf, param_grid=nontree_parameter_grid, cv = cross_validation)\n",
        "    grid_search.fit(train_data[features], train_data['Survived'])\n",
        "    print(clf)\n",
        "    print('Best score: {}'.format(grid_search.best_score_))\n",
        "    print('Best parameters: {}'.format(grid_search.best_params_))\n",
        "    \n",
        "for clf in tree_classifiers:\n",
        "    grid_search = GridSearchCV(clf, param_grid=tree_parameter_grid, cv = cross_validation)\n",
        "    grid_search.fit(train_data[features], train_data['Survived'])\n",
        "    print(clf)\n",
        "    print('Best score: {}'.format(grid_search.best_score_))\n",
        "    print('Best parameters: {}'.format(grid_search.best_params_))  \n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "788b73a4-bfed-0702-2702-a7202ecc00cd"
      },
      "outputs": [],
      "source": [
        "#VOTING CLASSIFIER\n",
        "\n",
        "adac = AdaBoostClassifier(n_estimators = 50).fit(train_data[features], train_data['Survived'])\n",
        "bagc = BaggingClassifier(n_estimators= 150).fit(train_data[features], train_data['Survived']) \n",
        "gbc = GradientBoostingClassifier(n_estimators = 50).fit(train_data[features], train_data['Survived'])   \n",
        "rfc = RandomForestClassifier(n_estimators = 50, max_depth = 9).fit(train_data[features], train_data['Survived']) \n",
        "etc = ExtraTreesClassifier(n_estimators = 100, max_depth = 9).fit(train_data[features], train_data['Survived'])                \n",
        "\n",
        "\n",
        "voting_clf = VotingClassifier(estimators = [('adac', adac),\n",
        "                                            ('bag', bagc),\n",
        "                                            ('gbc', gbc),\n",
        "                                            ('rfc', rfc),\n",
        "                                            ('etc', etc),                                           \n",
        "                                            ],\n",
        "                              voting = 'soft',\n",
        "                              weights=[1,1,1,1,1])\n",
        "\n",
        "voting_clf = voting_clf.fit(train_data[features], train_data['Survived'])    \n",
        "test_data[['Survived']] = voting_clf.predict(test_data[features])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1260d81f-e0e9-6b73-e04a-b09282ded0c4"
      },
      "outputs": [],
      "source": [
        "test_data[['Survived']] = test_data[['Survived']].astype(int)\n",
        "data_submission = test_data[['PassengerId', 'Survived']]\n",
        "data_submission.to_csv('submission.csv', index = False )\n",
        "print(features, \"\\n\", voting_clf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "52a522fe-d74f-25d0-553a-9c88a337c6f9"
      },
      "outputs": [],
      "source": ""
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}