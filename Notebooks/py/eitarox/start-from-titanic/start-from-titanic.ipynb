{"cells":[{"metadata":{"_uuid":"97bcd9add8ae438df7f87d1fc4e9ea881484be56","_cell_guid":"c100e696-5ded-40ae-be63-50bc388aecbe"},"cell_type":"markdown","source":"<h1>Start from Titanic: the first step for kaggle beginner</h1>"},{"metadata":{"_uuid":"2b61e69936524037de67a7d968b7ab165513b97c","_cell_guid":"1c0da03f-3784-49ff-8e13-e66f4451eeb9"},"cell_type":"markdown","source":"***This work is in progress. Comments and feedbacks are always welcome.***"},{"metadata":{"_uuid":"4a3eba9ec89158b09f59f23279c74cddb2c20cc4","_cell_guid":"60f49a9a-f927-4304-ac6e-3db251905bc9"},"cell_type":"markdown","source":"<h2>Outline</h2>\n1. Question and Problem Definition\n2. Load Data and Modules\n3. Initial Exploration\n4. Relation between Features\n5. Data Cleaning\n6. Split Training Data and Testing Data\n7.  Model Data\n8. Model Evaluation"},{"metadata":{"_uuid":"8d5cab2c10963478a4e94580d26b79dbc8c1e678","_cell_guid":"4e36a5dd-4518-4254-9072-2fa491d0d2c8"},"cell_type":"markdown","source":"<h2>1. Question and problem definition</h2>\nCompetition sites like Kaggle define the problem to solve or questions to ask while providing the datasets for training your data science model and testing the model results against a test dataset. The question or problem definition for Titanic Survival competition is described [here](https://www.kaggle.com/c/titanic) at Kaggle.\n\n> In this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy."},{"metadata":{"_uuid":"06d5de58a12d78e639ca5c4cb91f2a43d9e15839","_cell_guid":"b3ec915f-28da-49a0-8c41-f66c3849944b"},"cell_type":"markdown","source":"<h2>2. Load Data and Modules</h2>\nThe list of modules grows step by step by adding new functionality that is useful for this project.  I prefer to have them all in one place to keep an overview."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","collapsed":true,"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# data analysis and wrangling\nimport numpy as np \nimport pandas as pd\nimport random as rnd\nfrom scipy import stats\n\n#visualization\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom statsmodels.graphics.mosaicplot import mosaic\n\n# machine learning\nimport sklearn as sk","execution_count":16,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# loading data\ndf_train = pd.read_csv('../input/train.csv')\ndf_test = pd.read_csv('../input/test.csv')\ncombine = pd.concat([df_train.drop('Survived',1), df_test])","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"a383bb04e80980e945fa8528fdacc0c1e0b5ca6e","_cell_guid":"74a3e1b2-bd62-48ca-b0d1-4cc74363a1c1"},"cell_type":"markdown","source":"<h2>3. Initial Exploration</h2>"},{"metadata":{"_uuid":"c67bdcad1166bee119ce659af936707521878e99","collapsed":true,"_cell_guid":"2f359dd7-7a0b-4cf9-a095-2d4bb7e76043","trusted":false},"cell_type":"code","source":"print(df_train.columns.values)\ndf_train.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6dd33907cc643ed146cfef48e6024d0b776f8fa9","collapsed":true,"_cell_guid":"dc74d030-0eaa-4709-855c-c676cae68504","trusted":false},"cell_type":"code","source":"df_test.head(3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"30783db7563a0a2537f34b106cd1ae8b199731b9","collapsed":true,"_cell_guid":"8cf5a46a-ba58-411c-9452-756bca484451","trusted":false},"cell_type":"code","source":"df_train.info()\nprint('_'*30)\ndf_test.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a3ac6e3bdda611578b47643dbc8afd42304f348e","collapsed":true,"_cell_guid":"9df5c050-31c6-448a-af42-ca53686650b7","trusted":false},"cell_type":"code","source":"df_train.describe(include=['O'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2e92607f94764287f8a21621c742258366fdc0bf","collapsed":true,"_cell_guid":"53c05f02-2ce0-4e23-9c24-983b06b9c9e5","trusted":false},"cell_type":"code","source":"print(df_train[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False))\nprint('_'*20)\nprint(df_train[['Sex', 'Survived']].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False))\nprint('_'*20)\nprint(df_train[['SibSp', 'Survived']].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False))\nprint('_'*20)\nprint(df_train[['Parch', 'Survived']].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False))\nprint('_'*20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1d27f84d9299fd01a5e062f2d2ce7bf2f53dd078","collapsed":true,"_cell_guid":"034fe071-5af3-42a6-b91b-78ea40433e3d","trusted":false},"cell_type":"code","source":"g = sns.FacetGrid(df_train, col='Survived')\ng.map(plt.hist, 'Age', bins=20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2facf7601d8f4285a69c07bd1894d939156f9b1d","collapsed":true,"_cell_guid":"68baead5-bf65-4ec3-b08f-7b1ed316c822","trusted":false},"cell_type":"code","source":"surv = df_train[df_train['Survived']==1]\nnosurv = df_train[df_train['Survived']==0]\nsurv_col = \"blue\"\nnosurv_col = \"red\"\n\nprint(\"Survived: %i (%.1f percent), Not Survived: %i (%.1f percent), Total: %i\"\n     %(len(surv), 1.*len(surv)/len(df_train)*100, len(nosurv), len(nosurv)/len(df_train)*100, len(df_train)))\nprint(\"Median age survivors: %.1f, Median age non-survivors: %.1f\"%(np.median(surv['Age'].dropna()), np.median(nosurv['Age'].dropna())))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1eb412a96e72caf5d343bf0792e239778a2f5758","collapsed":true,"_cell_guid":"1160c056-b56a-416c-af73-a79c7bcc8e37","trusted":false},"cell_type":"code","source":"# sns.set_palette('deep') \n# sns.set_palette('muted')\nsns.set_palette('pastel')\n# sns.set_palette('bright')\n# sns.set_palette('dark')\n# sns.set_palette('colorblind')\n# warnings.filterwarnings(action='ignore')\nplt.figure(figsize=[12,10])\nplt.subplot(331)\nsns.distplot(surv['Age'].dropna().values, bins=range(0, 81, 1), kde=False, color=surv_col, label=\"surv\")\nsns.distplot(nosurv['Age'].dropna().values, bins=range(0, 81, 1), kde=False, color=nosurv_col, label=\"nosurv\", axlabel='Age')\nplt.legend()\nplt.subplot(332)\nsns.barplot('Sex', 'Survived', data=df_train)\nplt.subplot(333)\nsns.barplot('Pclass', 'Survived', data=df_train)\nplt.subplot(334)\nsns.barplot('Embarked', 'Survived', data=df_train)\nplt.subplot(335)\nsns.barplot('SibSp', 'Survived', data=df_train)\nplt.subplot(336)\nsns.barplot('Parch', 'Survived', data=df_train)\nplt.subplot(337)\nsns.distplot(np.log10(surv['Fare'].dropna().values+1), kde=False, color=surv_col, label=\"surv\")\nsns.distplot(np.log10(nosurv['Fare'].dropna().values+1), kde=False, color=nosurv_col, label=\"nosurv\")\nplt.legend()\nplt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.25, wspace=0.35)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d965f046ffaab6e060e09831ee8ad75c8dfffabb","collapsed":true,"_cell_guid":"21721098-4a7e-4011-95ef-732a12e3822b"},"cell_type":"markdown","source":"We learn the following things from studying the individual features:\n* *Age*:\n* *Sex*:\n* *Pclass*:\n* *Embarcked*:\n* *SibSp*:\n* *Fare*:"},{"metadata":{"_uuid":"472fb534c5655619c99ed95a8744b5eb75ed0852","_cell_guid":"a9eef0d9-c672-4add-8a67-a77061189c3b"},"cell_type":"markdown","source":"<h2>4. Relation between Features</h2>"},{"metadata":{"_uuid":"dfca7a89287bd57f4d1c08d949dc8d738cd9e2a2","collapsed":true,"_cell_guid":"7126dd7a-9574-403d-8851-6b6a5a480b62","trusted":false},"cell_type":"code","source":"plt.figure(figsize=(14, 12))\nfoo = sns.heatmap(df_train.drop('PassengerId', axis=1).corr(), vmax=0.6, square=True, annot=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d1ae3ec8fcf2deb1b6150332ff7e23b75c1f7474","collapsed":true,"_cell_guid":"ee6906c4-b53a-4009-87bc-e9bcdb6f1ada","trusted":false},"cell_type":"code","source":"cols = ['Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\ng = sns.pairplot(data=df_train.dropna(),vars=cols, size=1.5, hue='Survived',  palette=[nosurv_col, surv_col])\ng.set(xticklabels=[])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e181d96cde901a9a15e83166013d94443722ab30","collapsed":true,"_cell_guid":"a5451a8e-84d1-451a-9c1f-c99a71dd8c9b","trusted":false},"cell_type":"code","source":"msurv = df_train[(df_train['Survived']==1) & (df_train['Sex']==\"male\")]\nfsurv = df_train[(df_train['Survived']==1) & (df_train['Sex']==\"female\")]\nmnosurv = df_train[(df_train['Survived']==0) & (df_train['Sex']==\"male\")]\nfnosurv = df_train[(df_train['Survived']==0) & (df_train['Sex']==\"female\")]\n\nplt.figure(figsize=[13,5])\nplt.subplot(121)\nsns.distplot(fsurv['Age'].dropna().values, bins=range(0, 81, 1), kde=False, color=surv_col)\nsns.distplot(fnosurv['Age'].dropna().values, bins=range(0, 81, 1), kde=False, color=nosurv_col, axlabel='Female Age')\nplt.subplot(122)\nsns.distplot(msurv['Age'].dropna().values, bins=range(0, 81, 1), kde=False, color=surv_col)\nsns.distplot(mnosurv['Age'].dropna().values, bins=range(0, 81, 1), kde=False, color=nosurv_col, axlabel='Male Age')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c699734e03efb1ef8a3c3dfd441517dad8c5049d","collapsed":true,"_cell_guid":"6bd6f54a-e82b-459f-b036-581afc13c28e","trusted":false},"cell_type":"code","source":"sns.violinplot(x='Pclass', y='Age', hue='Survived',  data=df_train, palette='deep', split=True)\nplt.hlines([0,10], xmin=-1, xmax=3, linestyles=\"dotted\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"03a5c40c03e809ec4e409975121a6e06301e6c06","_cell_guid":"432ed5fd-d743-4860-9b87-3d68427eabcf","trusted":false,"collapsed":true},"cell_type":"code","source":"dummy = mosaic(df_train, [\"Survived\", \"Sex\", \"Pclass\"])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a1ba9ee36201b6352f8f7f84677c0333ebb3264b","_cell_guid":"aabee272-0755-4d57-92fd-d764c966fe97","trusted":false,"collapsed":true},"cell_type":"code","source":"g = sns.factorplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", col=\"Embarked\", data=df_train, aspect=0.9, size=3.5, ci=95.0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a3a05966bae22b632a1c5042b15b06131fd56aa0","_cell_guid":"d9ad4fef-b6c2-417d-8f1a-bb0f0317d3eb","trusted":false,"collapsed":true},"cell_type":"code","source":"tab = pd.crosstab(combine['Embarked'], combine['Pclass'])\nprint(tab)\ndummy = tab.div(tab.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=True)\ndummy = plt.xlabel('Port embarked')\ndummy = plt.ylabel('Percentage')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"405eeba47cc3fc2aa03fd35b7f11b44b531a5c43","_cell_guid":"64e29a2d-6cae-4e14-bbf9-9abbe0a77cd2","trusted":false,"collapsed":true},"cell_type":"code","source":"sns.barplot(x=\"Embarked\", y=\"Survived\", hue=\"Pclass\", data=df_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a9ee1202b8c7be233a5f6451207bfbc5e19afc85","_cell_guid":"bdcd8281-a7ad-40b1-94bf-4a50fa4f6695"},"cell_type":"markdown","source":"<h2>5. Data Cleaning</h2>"},{"metadata":{"_uuid":"fc0b5dcdb7d1ea5258bf51d6813ee47c0085033e","scrolled":true,"_cell_guid":"ba5c251f-b9d4-4a54-8ad2-855c754e5ccb","trusted":true},"cell_type":"code","source":"data1 = df_train.copy(deep = True)\ndata_cleaner = [data1, df_test]\n\nfor dataset in data_cleaner:\n    dataset['Age'].fillna(dataset['Age'].median(), inplace = True)\n    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace =True)\n    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)\n    \ndrop_column = ['PassengerId', 'Cabin', 'Ticket']\ndata1.drop(drop_column, axis=1, inplace = True)\n\nprint (data1.isnull().sum())\nprint(\"-\"*10)\nprint(df_test.isnull().sum())","execution_count":22,"outputs":[]},{"metadata":{"_uuid":"8f2278a3966c6668e72edd9e3b7012463b9c9601","_cell_guid":"264cb477-8de1-46c5-a369-c36040063135","trusted":true},"cell_type":"code","source":"for dataset in data_cleaner:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n    dataset['IsAlone'] = 1\n    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0\n    dataset['Title'] = dataset['Name'].str.split(\",\", expand=True)[1].str.split(\".\", expand=True)[0]\n    dataset['FareBin'] = pd.qcut(dataset['Fare'], 4)\n    dataset['AgeBin'] = pd.cut(dataset['Age'].astype(int), 5)\n    \n    stat_min = 10\n    title_names = (data1['Title'].value_counts() < stat_min)\n\n    data1['Title'] = data1['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)\n    print(data1['Title'].value_counts())\n    print(\"-\"*10)\n    \n    data1.info()\n    df_test.info()\n    data1.head(10)","execution_count":24,"outputs":[]},{"metadata":{"_uuid":"ee6b896cb89f934e89b5a1e7f9195f34c4d88aeb","_cell_guid":"739d2451-3e78-437d-84bb-6a33a5f2bd5e","trusted":true},"cell_type":"code","source":"#CONVERT: convert objects to category using Label Encoder for train and test/validation dataset\n\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\n\n#code categorical data\nlabel = LabelEncoder()\nfor dataset in data_cleaner:    \n    dataset['Sex_Code'] = label.fit_transform(dataset['Sex'])\n    dataset['Embarked_Code'] = label.fit_transform(dataset['Embarked'])\n    dataset['Title_Code'] = label.fit_transform(dataset['Title'])\n    dataset['AgeBin_Code'] = label.fit_transform(dataset['AgeBin'])\n    dataset['FareBin_Code'] = label.fit_transform(dataset['FareBin'])\n\n#define y variable\nTarget = ['Survived']\n\n#define x variables for original features\ndata1_x = ['Sex','Pclass', 'Embarked', 'Title','SibSp', 'Parch', 'Age', 'Fare', 'FamilySize', 'IsAlone'] \ndata1_x_calc = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code','SibSp', 'Parch', 'Age', 'Fare'] \ndata1_xy =  Target + data1_x\nprint('Original X Y: ', data1_xy, '\\n')\n\n\n#define x variables for original w/bin features to remove continuous variables\ndata1_x_bin = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code', 'FamilySize', 'AgeBin_Code', 'FareBin_Code']\ndata1_xy_bin = Target + data1_x_bin\nprint('Bin X Y: ', data1_xy_bin, '\\n')\n\n\n#define x and y variables for dummy features original\ndata1_dummy = pd.get_dummies(data1[data1_x])\ndata1_x_dummy = data1_dummy.columns.tolist()\ndata1_xy_dummy = Target + data1_x_dummy\nprint('Dummy X Y: ', data1_xy_dummy, '\\n')\n\ndata1_dummy.head()","execution_count":25,"outputs":[]},{"metadata":{"_uuid":"46ab5a8160bd3dbfebf0acdd775d9a6d72677fde","_cell_guid":"09f405e8-b6f4-4a99-b251-37da7d69bb1e"},"cell_type":"markdown","source":"<h2>6. Split Training Data and Testing Data</h2>"},{"metadata":{"trusted":true,"_uuid":"0cb7ea7cedfcee2d03e4111031be189c56495b75"},"cell_type":"code","source":"from sklearn import model_selection\ntrain1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data1[data1_x_calc], data1[Target], random_state = 0)\ntrain1_x_bin, test1_x_bin, train1_y_bin, test1_y_bin = model_selection.train_test_split(data1[data1_x_bin], data1[Target] , random_state = 0)\ntrain1_x_dummy, test1_x_dummy, train1_y_dummy, test1_y_dummy = model_selection.train_test_split(data1_dummy[data1_x_dummy], data1[Target], random_state = 0)\n\nprint(\"Data1 Shape: {}\".format(data1.shape))\nprint(\"Train1 Shape: {}\".format(train1_x.shape))\nprint(\"Test1 Shape: {}\".format(test1_x.shape))\n\ntrain1_x_bin.head()","execution_count":30,"outputs":[]},{"metadata":{"_uuid":"29df6f07a7fadb79dee7c45a31036e0cb851e671"},"cell_type":"markdown","source":"<h2>Model Data</h2>"},{"metadata":{"trusted":true,"_uuid":"ecfc1499a75a5ea78057b147137617461bc71670"},"cell_type":"code","source":"from sklearn import ensemble, gaussian_process, linear_model, naive_bayes, neighbors, svm, tree, discriminant_analysis\nfrom xgboost import XGBClassifier\n\n#Machine Learning Algorithm (MLA) Selection and Initialization\nMLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    #Gaussian Processes\n    gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    #Navies Bayes\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    #Nearest Neighbor\n    neighbors.KNeighborsClassifier(),\n    \n    #SVM\n    svm.SVC(probability=True),\n    svm.NuSVC(probability=True),\n    svm.LinearSVC(),\n    \n    #Trees    \n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(),\n    \n    #Discriminant Analysis\n    discriminant_analysis.LinearDiscriminantAnalysis(),\n    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n\n    #xgboost: http://xgboost.readthedocs.io/en/latest/model.html\n    XGBClassifier()    \n    ]\n\n#split dataset in cross-validation with this splitter class: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit\n#note: this is an alternative to train_test_split\ncv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) # run model 10x with 60/30 split intentionally leaving out 10%\n\n#create table to compare MLA metrics\nMLA_columns = ['MLA Name', 'MLA Parameters','MLA Train Accuracy Mean', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD' ,'MLA Time']\nMLA_compare = pd.DataFrame(columns = MLA_columns)\n\n#create table to compare MLA predictions\nMLA_predict = data1[Target]\n\n#index through MLA and save performance to table\nrow_index = 0\nfor alg in MLA:\n\n    #set name and parameters\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n    \n    #score model with cross validation: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n    cv_results = model_selection.cross_validate(alg, data1[data1_x_bin], data1[Target], cv  = cv_split)\n\n    MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n    MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean()\n    MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()   \n    #if this is a non-bias random sample, then +/-3 standard deviations (std) from the mean, should statistically capture 99.7% of the subsets\n    MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3   #let's know the worst that can happen!\n    \n\n    #save MLA predictions - see section 6 for usage\n    alg.fit(data1[data1_x_bin], data1[Target])\n    MLA_predict[MLA_name] = alg.predict(data1[data1_x_bin])\n    \n    row_index+=1\n\n    \n#print and sort table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_values.html\nMLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\nMLA_compare\n#MLA_predict","execution_count":44,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3914eb4e9e503268cd91a6eed008033dab06b402"},"cell_type":"code","source":"sns.barplot(x='MLA Test Accuracy Mean', y = 'MLA Name', data = MLA_compare, color = 'm')\n\nplt.title('Machine Learning Algorithm Accuracy Score \\n')\nplt.xlabel('Accuracy Score (%)')\nplt.ylabel('Algorithm')","execution_count":45,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8b2df20e085c06fd4a36b53fa27dec746413bc7d"},"cell_type":"code","source":"#base model\ndtree = tree.DecisionTreeClassifier(random_state = 0)\nbase_results = model_selection.cross_validate(dtree, data1[data1_x_bin], data1[Target], cv  = cv_split)\ndtree.fit(data1[data1_x_bin], data1[Target])\n\nprint('BEFORE DT Parameters: ', dtree.get_params())\nprint(\"BEFORE DT Training w/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \nprint(\"BEFORE DT Test w/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\nprint(\"BEFORE DT Test w/bin score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\nprint('-'*10)\n\n\n#tune hyper-parameters: http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier\nparam_grid = {'criterion': ['gini', 'entropy'],  #scoring methodology; two supported formulas for calculating information gain - default is gini\n              'max_depth': [2,4,6,8,10,None], #max depth tree can grow; default is none\n              'random_state': [0] #seed or control random number generator: https://www.quora.com/What-is-seed-in-random-number-generation\n             }\n\n#print(list(model_selection.ParameterGrid(param_grid)))\n\n#choose best model with grid_search: #http://scikit-learn.org/stable/modules/grid_search.html#grid-search\n#http://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html\ntune_model = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, scoring = 'roc_auc', cv = cv_split)\ntune_model.fit(data1[data1_x_bin], data1[Target])\n\nprint('AFTER DT Parameters: ', tune_model.best_params_)\nprint(\"AFTER DT Training w/bin score mean: {:.2f}\". format(tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100)) \nprint(\"AFTER DT Test w/bin score mean: {:.2f}\". format(tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))\nprint(\"AFTER DT Test w/bin score 3*std: +/- {:.2f}\". format(tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3))\nprint('-'*10)","execution_count":46,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4157342d5d9e81b22a4232fd8024c2dc71d5b2f3"},"cell_type":"code","source":"#Graph MLA version of Decision Tree: http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\nimport graphviz \ndot_data = tree.export_graphviz(dtree, out_file=None, \n                                feature_names = data1_x_bin, class_names = True,\n                                filled = True, rounded = True)\ngraph = graphviz.Source(dot_data) \ngraph","execution_count":47,"outputs":[]},{"metadata":{"_uuid":"a7074e59bbd10a6cf4509ed64856c5cdbf19a3e0"},"cell_type":"markdown","source":"<h2>8. Model Evaluation</h2>"},{"metadata":{"trusted":true,"_uuid":"b630c8b954f3d9a615363b084abd9c2fb5969246"},"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\n#why choose one model, when you can pick them all with voting classifier\n#http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\n#removed models w/o attribute 'predict_proba' required for vote classifier and models with a 1.0 correlation to another model\nvote_est = [\n    #Ensemble Methods: http://scikit-learn.org/stable/modules/ensemble.html\n    ('ada', ensemble.AdaBoostClassifier()),\n    ('bc', ensemble.BaggingClassifier()),\n    ('etc',ensemble.ExtraTreesClassifier()),\n    ('gbc', ensemble.GradientBoostingClassifier()),\n    ('rfc', ensemble.RandomForestClassifier()),\n\n    #Gaussian Processes: http://scikit-learn.org/stable/modules/gaussian_process.html#gaussian-process-classification-gpc\n    ('gpc', gaussian_process.GaussianProcessClassifier()),\n    \n    #GLM: http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n    ('lr', linear_model.LogisticRegressionCV()),\n    \n    #Navies Bayes: http://scikit-learn.org/stable/modules/naive_bayes.html\n    ('bnb', naive_bayes.BernoulliNB()),\n    ('gnb', naive_bayes.GaussianNB()),\n    \n    #Nearest Neighbor: http://scikit-learn.org/stable/modules/neighbors.html\n    ('knn', neighbors.KNeighborsClassifier()),\n    \n    #SVM: http://scikit-learn.org/stable/modules/svm.html\n    ('svc', svm.SVC(probability=True)),\n    \n    #xgboost: http://xgboost.readthedocs.io/en/latest/model.html\n   ('xgb', XGBClassifier())\n\n]\n\n\n#Hard Vote or majority rules\nvote_hard = VotingClassifier(estimators = vote_est , voting = 'hard')\nvote_hard_cv = model_selection.cross_validate(vote_hard, data1[data1_x_bin], data1[Target], cv  = cv_split)\nvote_hard.fit(data1[data1_x_bin], data1[Target])\n\nprint(\"Hard Voting Training w/bin score mean: {:.2f}\". format(vote_hard_cv['train_score'].mean()*100)) \nprint(\"Hard Voting Test w/bin score mean: {:.2f}\". format(vote_hard_cv['test_score'].mean()*100))\nprint(\"Hard Voting Test w/bin score 3*std: +/- {:.2f}\". format(vote_hard_cv['test_score'].std()*100*3))\nprint('-'*10)\n\n\n#Soft Vote or weighted probabilities\nvote_soft = VotingClassifier(estimators = vote_est , voting = 'soft')\nvote_soft_cv = model_selection.cross_validate(vote_soft, data1[data1_x_bin], data1[Target], cv  = cv_split)\nvote_soft.fit(data1[data1_x_bin], data1[Target])\n\nprint(\"Soft Voting Training w/bin score mean: {:.2f}\". format(vote_soft_cv['train_score'].mean()*100)) \nprint(\"Soft Voting Test w/bin score mean: {:.2f}\". format(vote_soft_cv['test_score'].mean()*100))\nprint(\"Soft Voting Test w/bin score 3*std: +/- {:.2f}\". format(vote_soft_cv['test_score'].std()*100*3))\nprint('-'*10)","execution_count":48,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"78791070016650a6d8b521248d7233275bce6ae4"},"cell_type":"code","source":"#prepare data for modeling\nprint(df_test.info())\nprint(\"-\"*10)\n\n#hard voting classifier w/full dataset modeling submission score: defaults= 0.75598, tuned = 0.77990\ndf_test['Survived'] = vote_hard.predict(df_test[data1_x_bin])\n\n#submit file\nsubmit = df_test[['PassengerId','Survived']]\nsubmit.to_csv(\"../working/submit.csv\", index=False)\n\nprint('Validation Data Distribution: \\n', df_test['Survived'].value_counts(normalize = True))\nsubmit.sample(10)","execution_count":50,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"90c932bb30d093434c34b3ba2c3e220f38605a86"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}