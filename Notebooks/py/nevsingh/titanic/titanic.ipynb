{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport sklearn\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"scrolled":true,"collapsed":true},"cell_type":"code","source":"train_file_path = '../input/train.csv'\ntest_file_path = '../input/test.csv'\n\n# create a pandas dataframe from the training and test data\ntrain_df = pd.read_csv(train_file_path)\ntest_df = pd.read_csv(test_file_path)\n# take a look at the training dataset\n'''\nprint(train_df.head(12))\nprint(train_df.describe())\nprint(train_df.shape)\nprint(train_df.columns)\n\nprint(train_df.Fare.head(10))\nprint(train_df.Sex.head(10))\nprint(train_df.Age.head(10))\nprint(train_df.SibSp.head(10))\nprint(train_df.Parch.head(10))\nprint(train_df.Ticket.head(10))\nprint(train_df.Cabin.head(10))\nprint(train_df.Embarked.head(10))\n'''\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"2867d926b5a2d7817fce89b4527002716e8eb130"},"cell_type":"markdown","source":"''' \n## remove the missing data\ntrain_df_clean = train_df.dropna()\n\n## \"survived' is the output column, the rest can be the predictors\ny = train_df_clean.Survived\npredictors = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\nx = train_df_clean[predictors]\n\n#print(x_clean.head())\n#print(x_clean.describe())\n#print(x_clean.shape)\n\n## some of these columns are categorical values\n#print(x.dtypes)\nx_one_hot_encoded = pd.get_dummies(x)\n#print(x_one_hot_encoded)\n## o\n'''"},{"metadata":{"trusted":true,"_uuid":"942e08c8d0a6297190c5950c8f4e4fff285f5fc1","scrolled":false,"collapsed":true},"cell_type":"code","source":"## dropping the rows wih missing data reduces the number of records significantly, so it might be worth seeing if an imputer will be better \nfrom sklearn.impute import SimpleImputer\nmy_imputer = SimpleImputer()\n\ny = train_df.Survived\n#predictors = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n#predictors = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\npredictors = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Embarked']\nx = train_df[predictors]\n'''\nprint(x.shape)\nprint(x.head(10))\nprint(x.columns)\nprint(x.dtypes)\nprint(y) \n\n'''\n\n''' \n## get the unique values of each column\nprint(x.Pclass.unique()) ## 1, 2, 3\nprint(x.Sex.unique()) ## 'male', 'female'\nprint(x.Age.unique()) ## numeric \nprint(x.SibSp.unique()) ## 0-8\nprint(x.Parch.unique()) ## 0-6\n#print(x.Ticket.unique()) \nprint(x.Fare.unique()) \n#print(x.Cabin.unique()) ## too many individual ones\nprint(x.Embarked.unique()) ## S, C, Q, nan\n'''\nx_one_hot_encoded = pd.get_dummies(x)\none_hot_encoded_test_predictors = pd.get_dummies(test_df[predictors])\nfinal_train, final_test = x_one_hot_encoded.align(one_hot_encoded_test_predictors,join='inner',  axis=1)\n#print(x_one_hot_encoded.shape)\nx_imputed = pd.DataFrame(my_imputer.fit_transform(x_one_hot_encoded))\nx_imputed.columns = x_one_hot_encoded.columns\n#print(x_imputed)\nprint(x_imputed.dtypes)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"613ff9259da711991487821b5ee9c843b41401c5","collapsed":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\n\n## build a model\ntitanic_model = RandomForestClassifier()\nprint(y.head())\n## split the dataset into training and validation for testing, so we are not testing on the same dataset\ntrain_x, val_x, train_y, val_y = train_test_split(x_imputed, y,random_state = 0)\n\nprint(train_x.shape)\nprint(train_y.shape)\nprint(train_y.unique())\ntitanic_model.fit(train_x, train_y)\npredictions = titanic_model.predict(val_x)\nprint(predictions)\nprint(pd.DataFrame(predictions).head())\nprint(predictions.shape)\nprint(mean_absolute_error(val_y, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4088e1605b5ffd38e6eaec34a4e63490f5c85953","scrolled":true,"_kg_hide-output":false,"_kg_hide-input":false,"collapsed":true},"cell_type":"code","source":"\nfinal_test = my_imputer.transform(final_test)\n#print(type(final_test))\n\n\n## build a model\n#titanic_test_model = RandomForestRegressor()\n#titanic_test_model.fit(x_test, y_test)\nrandom_forest_predictions = titanic_model.predict(final_test)\n\n#print(random_forest_predictions)\n## ready to submit\nmy_submission = pd.DataFrame({'PassengerId': test_df.PassengerId, 'Survived': random_forest_predictions})\nmy_submission.to_csv('titanic_survival2.csv', index = False)\nprint(my_submission)\nprint(my_submission.describe())","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}