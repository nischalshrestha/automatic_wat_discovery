{"cells": [{"execution_count": null, "cell_type": "markdown", "metadata": {"_uuid": "11811047f1918cd6e05c086db360ef0dbddfd035", "_cell_guid": "71340427-2197-480e-9238-f0588cc7064f"}, "outputs": [], "source": "In this notebook, the following models are to be tested. They are:\n\n1. Linear / Logistic\n2. Naive Bayes\n3. SVM\n4. Random Forest\n5. Neural network"}, {"execution_count": null, "cell_type": "code", "metadata": {"trusted": false, "_cell_guid": "e4cf8d46-558b-4467-ab33-2168f05412af", "_execution_state": "idle", "_uuid": "a0844d3a160701ef8eef11401d778fdc2af842d0"}, "outputs": [], "source": "import numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport pandas\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nimport math\nimport seaborn as sns\nfrom six.moves import cPickle as pickle\n\n%matplotlib inline"}, {"execution_count": null, "cell_type": "markdown", "metadata": {"_uuid": "736db3ba95c8fab69a3193dfa0b2f96640748e4c", "_cell_guid": "5693202f-f4a7-4039-bf8d-dc3795ba03b6"}, "outputs": [], "source": "## Convert train data to one-hot-encoding\n\nThis is to build a data model easily fit into different machine learning model below:"}, {"execution_count": null, "cell_type": "code", "metadata": {"trusted": false, "collapsed": true, "_cell_guid": "53a89ef8-4da7-4024-b21f-ea87b4268159", "_execution_state": "idle", "_uuid": "e67512bf8b55a7ba79a3ba9d891c1e50199a44e6"}, "outputs": [], "source": "train_ds_file = '../input/cleansedtitanicdataset/train_dataset.pickle'\ntrain_lb_file = '../input/cleansedtitanicdataset/train_label.pickle'\ntest_ds_file = '../input/cleansedtitanicdataset/test_dataset.pickle'\n\nwith open(train_ds_file, 'rb') as f:\n    train_dataset = pickle.load(f)\n    \nwith open(train_lb_file, 'rb') as f:\n    train_label = pickle.load(f)\n    \nwith open(test_ds_file, 'rb') as f:\n    test_dataset = pickle.load(f)"}, {"execution_count": null, "cell_type": "code", "metadata": {"trusted": false, "_cell_guid": "29f296ab-62ad-435c-9a2c-154bdd9d11bf", "_execution_state": "idle", "_uuid": "99eeff41ae163ca3710492ac9028d1d363b64693"}, "outputs": [], "source": "def transform_ds_to_input(dataset):\n    columns = [\"Pclass\", \"Embarked_enc\", \"Salutation_enc\", \"CabinArea_enc\"]\n    ds_onehot = dataset[[\"Pclass\", \"Sex_enc\", \"SibSp\", \"Parch\", \"Fare\", \"CabinArea_enc\",\\\n                                       \"Embarked_enc\", \"Salutation_enc\", \"FamilyMember\"]]\n    ds_onehot = pandas.get_dummies(ds_onehot, sparse=True, columns=columns)\n    scaler = StandardScaler().fit(ds_onehot)\n    ds_onehot_scaled = scaler.transform(ds_onehot) \n    return ds_onehot_scaled\n\nfull_dataset = pandas.concat([train_dataset, test_dataset])\nfull_dataset_onehot = transform_ds_to_input(full_dataset)\ntrain_dataset_onehot= full_dataset_onehot[:len(train_dataset)]\ntest_dataset_onehot = full_dataset_onehot[len(train_dataset):]"}, {"execution_count": null, "cell_type": "markdown", "metadata": {"_uuid": "0ed0a5c48443d0e09f07f73df9ce0aaa05244a91", "_cell_guid": "9bfae68a-7ce8-4533-ac3e-3a1586749496"}, "outputs": [], "source": "## Model selection"}, {"execution_count": null, "cell_type": "code", "metadata": {"trusted": false, "collapsed": true, "_cell_guid": "a3c5f19b-2ba4-4119-833c-c6e7b1514006", "_execution_state": "idle", "_uuid": "63ceb20a38293bd3234a7511b3522208c2a9d63b"}, "outputs": [], "source": "seed = 10\ntest_size = 0.3\nX_train, X_test, y_train, y_test = train_test_split(train_dataset_onehot, train_label, test_size=test_size, random_state=seed)"}, {"execution_count": null, "cell_type": "code", "metadata": {"trusted": false, "collapsed": true, "_cell_guid": "512f7c6a-7fcc-43f4-a549-1f44f7783000", "_execution_state": "idle", "_uuid": "457a0543c30e1192966a2504efbb644b3ad3d71a"}, "outputs": [], "source": "from sklearn.linear_model import LogisticRegression\n\ndef default_logistic(X_train, y_train, X_test):\n    clf = LogisticRegression()      \n    clf.fit(X_train, y_train)\n\n    # make predictions for test data\n    y_pred = clf.predict(X_test)\n    predictions = [round(value) for value in y_pred]\n    \n    return predictions"}, {"execution_count": null, "cell_type": "code", "metadata": {"trusted": false, "collapsed": true, "_cell_guid": "8e81a45a-3de7-406e-b2b2-f2c9173f9acd", "_execution_state": "idle", "_uuid": "e4edd076a0c31b151a8e69a92dd6c0a371e585bf"}, "outputs": [], "source": "# Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n\ndef default_gauss_nb(X_train, y_train, X_test):\n    gnb = GaussianNB()\n    gnb.fit(X_train, y_train)\n\n    y_pred = gnb.predict(X_test)\n    predictions = [round(value) for value in y_pred]\n    \n    return predictions\n\ndef default_multinom_nb(X_train, y_train, X_test):\n    gnb = MultinomialNB()\n    gnb.fit(X_train, y_train)\n\n    y_pred = gnb.predict(X_test)\n    predictions = [round(value) for value in y_pred]\n\n    return predictions\n\ndef default_bernoulli_nb(X_train, y_train, X_test):\n    gnb = BernoulliNB()\n    gnb.fit(X_train, y_train)\n\n    y_pred = gnb.predict(X_test)\n    predictions = [round(value) for value in y_pred]\n\n    return predictions"}, {"execution_count": null, "cell_type": "code", "metadata": {"trusted": false, "collapsed": true, "_cell_guid": "197950aa-cbdd-46f3-9e60-915079eed753", "_execution_state": "idle", "_uuid": "69b11d62843fb9226b8a2f9b9ae31272436a4318"}, "outputs": [], "source": "# SVM\nfrom sklearn.svm import SVC\n\ndef default_rbf_svm(X_train, y_train, X_test):\n    clf = SVC()\n    clf.fit(X_train, y_train) \n    y_pred = clf.predict(X_test)\n    predictions = [round(value) for value in y_pred]\n    \n    return predictions\n\ndef default_linear_svm(X_train, y_train, X_test):\n    clf = SVC(kernel='linear')\n    clf.fit(X_train, y_train) \n    y_pred = clf.predict(X_test)\n    predictions = [round(value) for value in y_pred]\n    \n    return predictions\n\ndef default_poly_svm(X_train, y_train, X_test):\n    clf = SVC(kernel='poly')\n    clf.fit(X_train, y_train) \n    y_pred = clf.predict(X_test)\n    predictions = [round(value) for value in y_pred]\n    \n    return predictions"}, {"execution_count": null, "cell_type": "code", "metadata": {"trusted": false, "collapsed": true, "_cell_guid": "02d3af23-72f3-4e75-861f-5d00b71aa7c6", "_execution_state": "idle", "_uuid": "8dbb010e2b42ab3975753e85de58e5ec7fb9f05e"}, "outputs": [], "source": "# Random forest\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\n\ndef default_rf(X_train, y_train, X_test):\n    clf = RandomForestClassifier()      \n    clf.fit(X_train, y_train)\n\n    # make predictions for test data\n    y_pred = clf.predict(X_test)\n    predictions = [round(value) for value in y_pred]\n    \n    return predictions\n\ndef default_extra_tree(X_train, y_train, X_test):\n    clf = ExtraTreesClassifier()      \n    clf.fit(X_train, y_train)\n\n    # make predictions for test data\n    y_pred = clf.predict(X_test)\n    predictions = [round(value) for value in y_pred]\n    \n    return predictions"}, {"execution_count": null, "cell_type": "code", "metadata": {"trusted": false, "collapsed": true, "_cell_guid": "cc1c073e-6d04-46dc-b11e-298d884c6036", "_execution_state": "idle", "_uuid": "b6503f710156b5ddb28f1e21da8ab06932d9af4f"}, "outputs": [], "source": "# neural network\nfrom sklearn.neural_network import MLPClassifier\n\ndef default_nn(X_train, y_train, X_test):\n    clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\n                         hidden_layer_sizes=(5, 2), random_state=1)\n    clf.fit(X_train, y_train) \n    y_pred = clf.predict(X_test)\n    predictions = [round(value) for value in y_pred]\n    \n    return predictions"}, {"execution_count": null, "cell_type": "code", "metadata": {"trusted": false, "collapsed": true, "_cell_guid": "a3765a3c-f1cd-4655-b598-bf74560cffb4", "_execution_state": "idle", "_uuid": "70a8c1fde8b7f6be1e541167bea0b7524881cf23"}, "outputs": [], "source": "# boosting \nfrom xgboost import XGBClassifier\n\ndef default_xgboost(X_train, y_train, X_test):\n    clf = XGBClassifier()\n    clf.fit(X_train, y_train) \n    y_pred = clf.predict(X_test)\n    predictions = [round(value) for value in y_pred]\n    \n    return predictions"}, {"execution_count": null, "cell_type": "code", "metadata": {"trusted": false, "collapsed": true, "_cell_guid": "e658731a-f363-479f-9ff8-6ba1ea5bf7c5", "_execution_state": "idle", "_uuid": "84934e59bbb46d06795a12e00d56d78e999855f8"}, "outputs": [], "source": "def calc_accuracy(y_test, predictions):\n    accuracy = accuracy_score(y_test, predictions)\n    return accuracy"}, {"execution_count": null, "cell_type": "markdown", "metadata": {"_uuid": "1f8d33be30ce53ccef876b29b3bac97a08ce9624", "_cell_guid": "b5ea9d7a-37e0-43bf-ab23-bcc8168d8d71"}, "outputs": [], "source": "## Applying different models"}, {"execution_count": null, "cell_type": "code", "metadata": {"trusted": false, "_cell_guid": "5bdd228c-20fa-4b59-bf50-813737465024", "_execution_state": "idle", "_uuid": "c2e9badeb6ef0b0b9787359fb0b1eef60ba92f81"}, "outputs": [], "source": "## now run it for ten times\nclassifiers = {\"Logistic\" : default_logistic, \n               \"Guassian Naive bayes\" : default_gauss_nb, \n               #\"Multinominal Naive bayes\" : default_multinom_nb,\n               \"Bernoulli Naive bayes\" : default_bernoulli_nb,\n               \"RBF SVM\" : default_rbf_svm,\n               \"Linear SVM\" : default_linear_svm,\n               \"Polynomial SVM\" : default_poly_svm,\n               \"Random forest\" : default_rf,\n               \"Extra tree\" : default_extra_tree,\n               \"Neural Network\" : default_nn,\n               \"XGboost\": default_xgboost\n              }\n\n\nincorrect = pandas.DataFrame(columns=classifiers.keys())\n## show in graph what mistake create error\nfor key, value in classifiers.items():\n    prediction = value(X_train, y_train, X_test)\n    accuracy = calc_accuracy(y_test, prediction)\n    incorrect[key] = (y_test != prediction)\n    \n    print(\"%s Accuracy: %.2f%%\" % (key, accuracy * 100.0))"}, {"execution_count": null, "cell_type": "code", "metadata": {"trusted": false, "_cell_guid": "0bc985d2-29a5-4114-b570-e1ed9bf5fbf7", "_execution_state": "idle", "_uuid": "5c71c3e8f7805062894af1d3b326726519bc842b"}, "outputs": [], "source": "#instance most classifed incorrectly\nincorrect_sum = incorrect.sum(axis = 1)\nincorrect_sum.sort_values(ascending=False)\n\nfor i in range(10, 0, -1):\n    tmp = incorrect_sum >= i\n    print(\"No of instances incorrectly classified by at least %d classifier: %d (accuracy rate: %f%%)\" % \\\n          (i, tmp.sum(), (tmp.sum()/len(prediction)) * 100))"}, {"execution_count": null, "cell_type": "markdown", "metadata": {"_uuid": "542aeca44233139a149f96725559f306fec10d12", "_cell_guid": "6b366b1d-fff2-4e7b-8b94-5eedec7165a7"}, "outputs": [], "source": "## Model analysis"}, {"execution_count": null, "cell_type": "code", "metadata": {"_execution_state": "idle", "trusted": false, "_uuid": "de4d4f4a3548ca66544014cdd7061835bf81cad9", "_cell_guid": "6b7b7f72-baf9-426f-b483-5e2ec2200edc", "scrolled": false}, "outputs": [], "source": "#remove 2 columns for low accuracy rate\nincorrect_p = incorrect.drop('Guassian Naive bayes', 1)\n\nincorrect_rel = pandas.DataFrame(index=incorrect_p.columns, columns=incorrect_p.columns)\n\nfor key1 in incorrect_p.columns:\n    for key2 in incorrect_p.columns:\n        tmp = incorrect_p[key1] != incorrect_p[key2]\n        incorrect_rel[key1][key2] = tmp.sum()\n\nincorrect_rel = incorrect_rel.astype(float)\nfig = plt.figure(figsize=(8,8))\nsns.heatmap(incorrect_rel, cmap='RdYlGn_r', linewidths=0.5, annot=True)"}, {"execution_count": null, "cell_type": "code", "metadata": {"trusted": false, "_cell_guid": "30758d15-4930-4bb7-9d71-7890b2047db6", "_execution_state": "idle", "_uuid": "7508ec1d27343791b3b50116c2cd396aafcb3367"}, "outputs": [], "source": "# In the following, we just try to use 3 top classifer to form a voting classifier\n#Polynomial SVM Accuracy: 84.33%\n#XGboost Accuracy: 83.58%\n#RBF SVM Accuracy: 82.84%\n#Bernoulli Naive bayes Accuracy: 82.09%\n#Logistic Accuracy: 81.72%\n#Neural Network Accuracy: 81.72%\n#Random forest Accuracy: 81.34%\n#Linear SVM Accuracy: 81.34%\n#Extra tree Accuracy: 81.34%\n#Guassian Naive bayes Accuracy: 60.45%\n\nfrom sklearn.ensemble import VotingClassifier\nclf1 = SVC(kernel='poly')\nclf2 = XGBClassifier()\nclf3 = SVC()\nclf4 = BernoulliNB()\nclf5 = LogisticRegression()\nclf6 = RandomForestClassifier()\n\neclf1 = VotingClassifier(estimators=[('polysvc', clf1), ('xgb', clf2), ('rbf svm', clf3)], voting='hard')\n\neclf1.fit(X_train, y_train) \ny_pred = eclf1.predict(X_test)\npredictions = [round(value) for value in y_pred]\naccuracy = calc_accuracy(y_test, predictions)\nprint(\"%s Accuracy: %.2f%%\" % (\"Voting(3) classifier\", accuracy * 100.0))\n\neclf2 = VotingClassifier(estimators=[('polysvc', clf1), ('xgb', clf2), ('rbf svm', clf3), ('bnb', clf4), ('lr', clf5)], voting='hard')\neclf2.fit(X_train, y_train) \ny_pred = eclf2.predict(X_test)\npredictions = [round(value) for value in y_pred]\naccuracy = calc_accuracy(y_test, predictions)\nprint(\"%s Accuracy: %.2f%%\" % (\"Voting(5) classifier\", accuracy * 100.0))"}, {"execution_count": null, "cell_type": "markdown", "metadata": {"_uuid": "6302e1cdaa2e5796931c166b311daf284c336381", "_cell_guid": "8343354d-a321-49d0-a5a8-1ff2fee16f8c"}, "outputs": [], "source": "## Error case analysis"}, {"execution_count": null, "cell_type": "markdown", "metadata": {"collapsed": true, "_uuid": "fac40e415245f20bc7c86109b5ad2abc0022be53", "_cell_guid": "c2b0c136-98ef-40ef-aeb6-0d906a8c5b34"}, "outputs": [], "source": "## Outcome, what have we learnt?"}, {"execution_count": null, "cell_type": "code", "metadata": {"trusted": false, "_cell_guid": "4c25852d-9dbd-4014-b9dc-f23e4c0a86a5", "_execution_state": "idle", "_uuid": "0fdb1de4bed677fd4b07acd7094a52477b0114e1"}, "outputs": [], "source": "# A sample submission of the best model\n\nclf = SVC(kernel='poly')\nclf.fit(X_train, y_train)\n\n# make predictions for test data\ny_pred = clf.predict(X_test)\npredictions = [round(value) for value in y_pred]\n# evaluate predictions\naccuracy = accuracy_score(y_test, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n\nr_pred = clf.predict(test_dataset_onehot)\nr_predictions = [int(round(value)) for value in r_pred]\n\nsubmission_df = pandas.DataFrame(index=test_dataset.index, columns=[\"Survived\"])\nsubmission_df[\"Survived\"] = r_predictions\nsubmission_df.to_csv(\"submission_poly.csv\", sep=',')"}, {"execution_count": null, "cell_type": "code", "metadata": {"trusted": false, "collapsed": true, "_cell_guid": "a7a25fc6-bfb1-4164-acd0-5f7eea702d7e", "_execution_state": "idle", "_uuid": "4161dfa91c962a886e3cc4f0f9b5abea5f34032c"}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}, "language_info": {"name": "python", "file_extension": ".py", "nbconvert_exporter": "python", "version": "3.6.1", "pygments_lexer": "ipython3", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}}}, "nbformat_minor": 2, "nbformat": 4}