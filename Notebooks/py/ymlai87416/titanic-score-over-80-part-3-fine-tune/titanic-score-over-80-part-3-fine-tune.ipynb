{"nbformat_minor": 2, "cells": [{"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"_cell_guid": "a67174b4-28ca-4934-b9c3-d70458bea962", "trusted": false, "_uuid": "3fdae34cc547a3cb16592aa4245c749b9f9f9425"}, "source": "import numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport pandas\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nimport math\nimport seaborn as sns\nfrom six.moves import cPickle as pickle\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.svm import SVC\n\n%matplotlib inline"}, {"cell_type": "markdown", "outputs": [], "execution_count": null, "metadata": {"_cell_guid": "ba39e273-e345-40e1-b187-5633a7fc7356", "_uuid": "bfcbb35d8b987fb0ccd4b539dd3f3813b6b858f3"}, "source": "## From previous result\n\nFrom previous result, we decide to use the following models to do submission. They are:\n1. Polynomial SVM Accuracy: 83.96%\n2. XGboost Accuracy: 83.58%\n3. RBF SVM Accuracy: 82.84%\n4. Bernoulli Naive bayes Accuracy: 82.46%\n5. Logistic Accuracy: 81.72%\n6. Random forest Accuracy: 81.34%\n7. Linear SVM Accuracy: 81.34%\n8. Neural Network Accuracy: 81.34%\n9. Extra tree Accuracy: 79.85%\n10. Guassian Naive bayes Accuracy: 60.45%\n\nIn this section, we will run a grid search to find the best parameter for the validation set."}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"_cell_guid": "9f061c9d-d0a3-4a24-988e-182e505b43ac", "trusted": false, "_uuid": "c65947c5f83b0e5e38f30c0a20ddfb191e449336"}, "source": "train_ds_file = '../input/cleansedtitanicdataset/train_dataset.pickle'\ntrain_lb_file = '../input/cleansedtitanicdataset/train_label.pickle'\ntest_ds_file = '../input/cleansedtitanicdataset/test_dataset.pickle'\n\nwith open(train_ds_file, 'rb') as f:\n    train_dataset = pickle.load(f)\n    \nwith open(train_lb_file, 'rb') as f:\n    train_label = pickle.load(f)\n    \nwith open(test_ds_file, 'rb') as f:\n    test_dataset = pickle.load(f)\n    \ndef transform_ds_to_input(dataset):\n    columns = [\"Pclass\", \"Embarked_enc\", \"Salutation_enc\", \"CabinArea_enc\"]\n    ds_onehot = dataset[[\"Pclass\", \"Sex_enc\", \"SibSp\", \"Parch\", \"Fare\", \"CabinArea_enc\",\\\n                                       \"Embarked_enc\", \"Salutation_enc\", \"FamilyMember\"]]\n    ds_onehot = pandas.get_dummies(ds_onehot, sparse=True, columns=columns)\n    scaler = StandardScaler().fit(ds_onehot)\n    ds_onehot_scaled = scaler.transform(ds_onehot) \n    return ds_onehot_scaled\n\nfull_dataset = pandas.concat([train_dataset, test_dataset])\nfull_dataset_onehot = transform_ds_to_input(full_dataset)\ntrain_dataset_onehot= full_dataset_onehot[:len(train_dataset)]\ntest_dataset_onehot = full_dataset_onehot[len(train_dataset):]\n\nprint(pandas.DataFrame(train_dataset_onehot[0:10]))"}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"_cell_guid": "4908b98a-a838-4685-a3ae-fb0d7a3cdd68", "trusted": false, "collapsed": true, "_uuid": "88e734c6809eaf9818f8367ed75110d7239b1a26"}, "source": "def get_train_test_set(test_size):\n    X_train, X_test, y_train, y_test = \\\n        train_test_split(train_dataset_onehot, train_label, test_size=test_size)\n    \n    return X_train, X_test, y_train, y_test"}, {"cell_type": "markdown", "outputs": [], "execution_count": null, "metadata": {"_cell_guid": "47b746bb-a3c9-493f-bf19-cf3e56597606", "_uuid": "a07dbc96ceebe9211c031e8f340fabaf8e4a9237"}, "source": "## Tuning logistic regression\n\nThe model parameter to optimized are\n1. C - The regularization term"}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"_cell_guid": "c32773de-99dc-4bc1-89f2-9faf2d92823b", "trusted": false, "_uuid": "5c94a0274e256cec8679918ee15ebb5484f35c3c"}, "source": "from sklearn.linear_model import LogisticRegression\n\n#parameters = {'C':[0.001, 0.0025, 0.005, 0.0075, 0.01, 0.025, 0.05, 0.075, 0.1, 0.25, 0.5, 0.75, 1, 2.5, 5, 7.5]}\nparameters = {'C':[0.001, 0.0025, 0.005, 0.0075, 0.01]}\nlr = LogisticRegression()\nclf = GridSearchCV(lr, parameters)\nclf.fit(train_dataset_onehot, train_label)\n\nprint(clf.best_params_)"}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"_cell_guid": "b401c0f1-8ae3-47b4-8b15-103d7a7faea7", "trusted": false, "_uuid": "b1c5eaf1001aa27592c0c2da191a79f18695cb81"}, "source": "for i in range(10):\n    X_train, X_test, y_train, y_test = get_train_test_set(0.3)\n    lr = LogisticRegression(C = 0.075)\n    lr.fit(X_train, y_train)\n    y_pred = lr.predict(X_test)\n    predictions = [round(value) for value in y_pred]\n    accuracy = accuracy_score(y_test, predictions)\n    print(\"%s Accuracy: %.2f%%\" % (\"Logistic regression, C:0.3\", accuracy * 100.0))"}, {"cell_type": "markdown", "outputs": [], "execution_count": null, "metadata": {"_cell_guid": "6e2e2556-33fb-49d6-adb7-4d02cb4b3e3e", "_uuid": "14c58de5cb0d30e24ef5e00a315c28975b105879"}, "source": "# Tuning RBF SVM\n\nThe model parameter to optimized are\n\n1. C - regularization term\n2. Gamma - the influence of a single training example reaches"}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"_cell_guid": "1ff1546d-a082-4aa3-b291-7fcaeac3001c", "trusted": false, "_uuid": "6396ca216a99d6b9eab65f5d839eea016565cd60"}, "source": "from sklearn.naive_bayes import MultinomialNB\n\n#parameters = {'C':[0.001, 0.0025, 0.005, 0.0075, 0.01, 0.025, 0.05, 0.075, 0.1, 0.25, 0.5, 0.75, 1, 2.5, 5, 7.5], \n#              'gamma':[0.00001, 0.000025, 0.00005, 0.000075, 0.0001, 0.00025, 0.0005, 0.00075, 0.001, 0.0025, \\\n#                   0.005, 0.0075, 0.01, 0.025, 0.05, 0.075]}\nparameters = {'C':[0.1, 0.25, 0.5, 0.75], \n              'gamma':[0.01, 0.025, 0.05, 0.075]}\n\nrbf = SVC()\nclf = GridSearchCV(rbf, parameters, n_jobs=8)\nclf.fit(train_dataset_onehot, train_label)\n\nprint(clf.best_params_)"}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"_cell_guid": "85b6c1ae-96cf-463c-af6f-bd5bdfc4099f", "trusted": false, "_uuid": "9b873f89ae495e304f5054ebafc020caab428c2a"}, "source": "for i in range(10):\n    X_train, X_test, y_train, y_test = get_train_test_set(0.3)\n    lr = SVC(gamma=0.05, C=0.75)\n    lr.fit(X_train, y_train)\n    y_pred = lr.predict(X_test)\n    predictions = [round(value) for value in y_pred]\n    accuracy = accuracy_score(y_test, predictions)\n    print(\"%s Accuracy: %.2f%%\" % (\"RBF SVM, gamma:0.05, C=0.75\", accuracy * 100.0))"}, {"cell_type": "markdown", "outputs": [], "execution_count": null, "metadata": {"_cell_guid": "2f98aaa9-5b40-4dbf-87f0-7064c809dcce", "_uuid": "e255e770328a184a86ad96ad6b3af88b514436ee"}, "source": "## Tuning XGBoost\n\nThe model will use tree instead of linear\n\nThe parameter to be tuned are:\n1. learning_rate\n2. max_depth\n3. min_child_weight\n4. gamma\n5. subsample\n6. colsample_bytree\n7. objective\n8. learning_rate"}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"_cell_guid": "42b8b5fb-7c3c-4da0-8773-9608177755f4", "trusted": false, "_uuid": "6d435823d2663e98a7cc52ac2896f7b6c3d2ba2e"}, "source": "parameters = {\n    'min_child_weight':range(2,6,1),\n    'max_depth':range(3,7,1),\n    'gamma':[i/10.0 for i in range(0,5)],\n    'subsample':[i/10.0 for i in range(6,10)],\n    'colsample_bytree':[i/10.0 for i in range(6,10)],\n    'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100],\n    'learning_rate':[0.01, 0.1, 1]\n}\n\n#n_iter=5000\nn_iter_search=10\nxgb = XGBClassifier()\nclf = RandomizedSearchCV(xgb, parameters, n_jobs=8, n_iter=n_iter_search)\nclf.fit(train_dataset_onehot, train_label)\n\nprint(clf.best_params_)"}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"_cell_guid": "18bfe0ac-8bc9-450f-80dd-c0a7fd4f95c9", "trusted": false, "_uuid": "ce26ac139889be34eda8cd72bb0ea9d663033342"}, "source": "for i in range(10):\n    X_train, X_test, y_train, y_test = get_train_test_set(0.3)\n    lr = XGBClassifier(learning_rate=0.1, subsample=0.9, colsample_bytree=0.8, gamma=0.2,\n                       max_depth=5, reg_alpha=0.01, min_child_weight=3, objective= 'binary:logistic')\n    lr.fit(X_train, y_train)\n    y_pred = lr.predict(X_test)\n    predictions = [round(value) for value in y_pred]\n    accuracy = accuracy_score(y_test, predictions)\n    print(\"%s Accuracy: %.2f%%\" % (\"XGBoost\", accuracy * 100.0))"}, {"cell_type": "markdown", "outputs": [], "execution_count": null, "metadata": {"_cell_guid": "36a1fe69-af26-4d9d-a237-62e6402bc74d", "_uuid": "5af792e41407297f97fdd6bd8879f744cb0d1359"}, "source": "## Tuning Random Forest\n\nThe parameter to be tuned are:\n1. max_depth\n2. max_features\n3. min_samples_split\n4. min_samples_leaf\n5. bootstrap\n6. criterion"}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"_cell_guid": "84ae60a7-f102-436b-8532-c83512019e4a", "trusted": false, "_uuid": "77b80d2c7fcf1eb1c6a1e8338f6701baa6f2fc3d"}, "source": "from sklearn.ensemble import RandomForestClassifier\nfrom scipy.stats import randint as sp_randint\n\nparam_dist = {\"n_estimators\" : sp_randint(3, 20),\n              \"max_depth\": [1, 2, 3, None],\n              \"max_features\": sp_randint(1, 11),\n              \"min_samples_split\": sp_randint(2, 11),\n              \"min_samples_leaf\": sp_randint(1, 11),\n              \"bootstrap\": [True, False],\n              \"criterion\": [\"gini\", \"entropy\"]}\n\n# run randomized search\n#n_iter_search = 5000\nn_iter_search = 10\n# build a classifier\nclf = RandomForestClassifier()\nrandom_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n                                   n_iter=n_iter_search, n_jobs=8)\nrandom_search.fit(train_dataset_onehot, train_label)\n\nprint(random_search.best_params_)"}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"_cell_guid": "e166144c-e97e-46f8-bb60-e28265253a2f", "trusted": false, "_uuid": "aa109dd35edd35fbf8c4fd0de56f0512d01f4fca"}, "source": "for i in range(10):\n    X_train, X_test, y_train, y_test = get_train_test_set(0.3)\n    lr = RandomForestClassifier(max_features=9, bootstrap=True, min_samples_split=9, n_estimators=16, criterion='gini',\n                       min_samples_leaf=4, max_depth=None)  \n    lr.fit(X_train, y_train)\n    y_pred = lr.predict(X_test)\n    predictions = [round(value) for value in y_pred]\n    accuracy = accuracy_score(y_test, predictions)\n    print(\"%s Accuracy: %.2f%%\" % (\"Random forest\", accuracy * 100.0))"}, {"cell_type": "markdown", "outputs": [], "execution_count": null, "metadata": {"_cell_guid": "ef35d02a-53dc-4642-a64d-8ace08d9021e", "_uuid": "ae2b9a3df3e3b9507ed8e8d15b882c472600d3c7"}, "source": "# Submission, select the best model for Kaggle"}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"_cell_guid": "25227952-d16d-466d-8e65-266546756352", "trusted": false, "collapsed": true, "_uuid": "2ba21c478f385649cff677673869ea2a21ff3e95"}, "source": "# Submission score is 0.77990, better than gender classifier 0.76555\n\nclf1 = RandomForestClassifier(max_features=9, bootstrap=True, min_samples_split=9, n_estimators=16, criterion='gini',\n                       min_samples_leaf=4, max_depth=None)  \nclf1.fit(train_dataset_onehot, train_label)\nr_pred = clf1.predict(test_dataset_onehot)\nr_predictions = [int(round(value)) for value in r_pred]\n\nsubmission_df = pandas.DataFrame(index=test_dataset.index, columns=[\"Survived\"])\nsubmission_df[\"Survived\"] = r_predictions\nsubmission_df.to_csv(\"submission_best_rf.csv\", sep=',')"}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"_cell_guid": "b834e738-1cca-41c4-8354-3a9fdada2683", "trusted": false, "collapsed": true, "_uuid": "6bd09b085b841100c6096deedbc4b66d7c1cc48e"}, "source": "# Submission score is 0.78469, better than gender classifier 0.76555\n\nclf2 = XGBClassifier(learning_rate=0.1, subsample=0.9, colsample_bytree=0.8, gamma=0.2,\n                       max_depth=5, reg_alpha=0.01, min_child_weight=3, objective= 'binary:logistic')\nclf2.fit(train_dataset_onehot, train_label)\nr_pred = clf2.predict(test_dataset_onehot)\nr_predictions = [int(round(value)) for value in r_pred]\n\nsubmission_df = pandas.DataFrame(index=test_dataset.index, columns=[\"Survived\"])\nsubmission_df[\"Survived\"] = r_predictions\nsubmission_df.to_csv(\"submission_best_xg.csv\", sep=',')"}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"_cell_guid": "2dee2816-fcbd-4eb3-b9ab-82f68c33c180", "trusted": false, "_uuid": "650924ba9d9f1774d6b387de6711702ae39ed502"}, "source": "# Submission score is 0.78469, better than gender classifier 0.76555\n\nclf3 = SVC(gamma=0.05, C=0.75)\nclf3.fit(train_dataset_onehot, train_label)\nr_pred = clf3.predict(test_dataset_onehot)\nr_predictions = [int(round(value)) for value in r_pred]\n\nsubmission_df = pandas.DataFrame(index=test_dataset.index, columns=[\"Survived\"])\nsubmission_df[\"Survived\"] = r_predictions\nsubmission_df.to_csv(\"submission_best_svc.csv\", sep=',')"}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"_cell_guid": "157f37c0-d2c3-4727-b74a-fa7d027665bc", "trusted": false, "collapsed": true, "_uuid": "222086e8345d008a52ea36e7859e869fa5e213bd"}, "source": "# Submission score is 0.77512, better than gender classifier 0.76555\n\nclf4 = LogisticRegression(C = 0.075)\nclf4.fit(train_dataset_onehot, train_label)\nr_pred = clf4.predict(test_dataset_onehot)\nr_predictions = [int(round(value)) for value in r_pred]\n\nsubmission_df = pandas.DataFrame(index=test_dataset.index, columns=[\"Survived\"])\nsubmission_df[\"Survived\"] = r_predictions\nsubmission_df.to_csv(\"submission_best_lr.csv\", sep=',')"}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"_cell_guid": "6e272aff-5e39-4bd6-af8f-807567d5d301", "trusted": false, "collapsed": true, "_uuid": "df952f0339b65b014474281ed419654548a6cd8f"}, "source": "# Submission score is 0.81818, better than gender classifier 0.76555\n\nfrom sklearn.ensemble import VotingClassifier\n\neclf2 = VotingClassifier(estimators=[('rf', clf1), ('xgb', clf2), ('svm', clf3), ('lr', clf4)], voting='hard')\neclf2.fit(train_dataset_onehot, train_label) \nr_pred = eclf2.predict(test_dataset_onehot)\nr_predictions = [int(round(value)) for value in r_pred]\n\nsubmission_df = pandas.DataFrame(index=test_dataset.index, columns=[\"Survived\"])\nsubmission_df[\"Survived\"] = r_predictions\nsubmission_df.to_csv(\"submission_best_voting.csv\", sep=',')"}, {"cell_type": "code", "outputs": [], "execution_count": null, "metadata": {"_cell_guid": "1f92b458-8c61-4bd3-9eef-74d96cd79b86", "trusted": false, "collapsed": true, "_uuid": "8c093eddff21fbce50c4830badaa6c2a41869da5"}, "source": ""}], "metadata": {"kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}, "language_info": {"pygments_lexer": "ipython3", "codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "name": "python", "nbconvert_exporter": "python", "version": "3.6.1", "mimetype": "text/x-python"}}, "nbformat": 4}