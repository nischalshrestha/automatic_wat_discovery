{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e09f3e86-e94b-b5a7-d051-586de4898f7d"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "009db3a9-6481-d7a3-9895-2827bdab5aed"
      },
      "outputs": [],
      "source": [
        "# load data\n",
        "train_data = pd.read_csv(r\"../input/train.csv\")\n",
        "test_data = pd.read_csv(r\"../input/test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8e070496-f890-da5b-819d-ecae41505416"
      },
      "outputs": [],
      "source": [
        "train_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4690d5f5-1d18-b9d0-7fca-31494a4132fd"
      },
      "outputs": [],
      "source": [
        "test_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0e2e2685-776a-3e19-f6f3-1188363752c3"
      },
      "outputs": [],
      "source": [
        "# Feature Engineering\n",
        "from sklearn.preprocessing import Imputer\n",
        "\n",
        "def nan_padding(data, columns):\n",
        "    for column in columns:\n",
        "        imputer=Imputer()\n",
        "        data[column]=imputer.fit_transform(data[column].values.reshape(-1,1))\n",
        "    return data\n",
        "\n",
        "\n",
        "nan_columns = [\"Age\", \"SibSp\", \"Parch\"]\n",
        "\n",
        "train_data = nan_padding(train_data, nan_columns)\n",
        "test_data = nan_padding(test_data, nan_columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "724e35ee-418d-0ef6-e3d6-25dd158a1203"
      },
      "outputs": [],
      "source": [
        "train_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d9e85bf0-a7eb-71f0-5102-ba0c66a9f36f"
      },
      "outputs": [],
      "source": [
        "#save PassengerId for evaluation\n",
        "test_passenger_id=test_data[\"PassengerId\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "69f17817-2a9e-9c5d-e460-559588c4d316"
      },
      "outputs": [],
      "source": [
        "def drop_not_concerned(data, columns):\n",
        "    return data.drop(columns, axis=1)\n",
        "\n",
        "not_concerned_columns = [\"PassengerId\",\"Name\", \"Ticket\", \"Fare\", \"Cabin\", \"Embarked\"]\n",
        "train_data = drop_not_concerned(train_data, not_concerned_columns)\n",
        "test_data = drop_not_concerned(test_data, not_concerned_columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b642846b-31a6-1619-5b88-17ca2c908d6a"
      },
      "outputs": [],
      "source": [
        "train_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "27ad4c12-bcde-78ff-032b-ea58ae86fed6"
      },
      "outputs": [],
      "source": [
        "test_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a50e8d60-454e-b402-6953-21cd95d82f33"
      },
      "outputs": [],
      "source": [
        "def dummy_data(data, columns):\n",
        "    for column in columns:\n",
        "        data = pd.concat([data, pd.get_dummies(data[column], prefix=column)], axis=1)\n",
        "        data = data.drop(column, axis=1)\n",
        "    return data\n",
        "\n",
        "\n",
        "dummy_columns = [\"Pclass\"]\n",
        "train_data=dummy_data(train_data, dummy_columns)\n",
        "test_data=dummy_data(test_data, dummy_columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3ce32d22-6e51-ce31-fe73-5b349aa6b894"
      },
      "outputs": [],
      "source": [
        "test_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "93cd9d05-6ab1-42c2-6222-f16ae21be706"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "def sex_to_int(data):\n",
        "    le = LabelEncoder()\n",
        "    le.fit([\"male\",\"female\"])\n",
        "    data[\"Sex\"]=le.transform(data[\"Sex\"]) \n",
        "    return data\n",
        "\n",
        "train_data = sex_to_int(train_data)\n",
        "test_data = sex_to_int(test_data)\n",
        "train_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a6d562ac-cfe1-9ce2-53fa-4b2789812d9f"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def normalize_age(data):\n",
        "    scaler = MinMaxScaler()\n",
        "    data[\"Age\"] = scaler.fit_transform(data[\"Age\"].values.reshape(-1,1))\n",
        "    return data\n",
        "train_data = normalize_age(train_data)\n",
        "test_data = normalize_age(test_data)\n",
        "train_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6de2351b-7efb-f681-7b7b-74a6e9d5883d"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def split_valid_test_data(data, fraction=(1 - 0.8)):\n",
        "    data_y = data[\"Survived\"]\n",
        "    lb = LabelBinarizer()\n",
        "    data_y = lb.fit_transform(data_y)\n",
        "\n",
        "    data_x = data.drop([\"Survived\"], axis=1)\n",
        "\n",
        "    train_x, valid_x, train_y, valid_y = train_test_split(data_x, data_y, test_size=fraction)\n",
        "\n",
        "    return train_x.values, train_y, valid_x, valid_y\n",
        "\n",
        "train_x, train_y, valid_x, valid_y = split_valid_test_data(train_data)\n",
        "print(\"train_x:{}\".format(train_x.shape))\n",
        "print(\"train_y:{}\".format(train_y.shape))\n",
        "print(\"train_y content:{}\".format(train_y[:3]))\n",
        "\n",
        "print(\"valid_x:{}\".format(valid_x.shape))\n",
        "print(\"valid_y:{}\".format(valid_y.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b18da52c-396a-466c-b13c-3d9c55ab7ea5"
      },
      "outputs": [],
      "source": [
        "# Build Neural Network\n",
        "from collections import namedtuple\n",
        "\n",
        "def build_neural_network(hidden_units=10):\n",
        "    tf.reset_default_graph()\n",
        "    inputs = tf.placeholder(tf.float32, shape=[None, train_x.shape[1]])\n",
        "    labels = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "    learning_rate = tf.placeholder(tf.float32)\n",
        "    is_training=tf.Variable(True,dtype=tf.bool)\n",
        "    \n",
        "    initializer = tf.contrib.layers.xavier_initializer()\n",
        "    fc = tf.layers.dense(inputs, hidden_units, activation=None,kernel_initializer=initializer)\n",
        "    fc=tf.layers.batch_normalization(fc, training=is_training)\n",
        "    fc=tf.nn.relu(fc)\n",
        "    \n",
        "    logits = tf.layers.dense(fc, 1, activation=None)\n",
        "    cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=labels, logits=logits)\n",
        "    cost = tf.reduce_mean(cross_entropy)\n",
        "    \n",
        "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "    predicted = tf.nn.sigmoid(logits)\n",
        "    correct_pred = tf.equal(tf.round(predicted), labels)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "    # Export the nodes \n",
        "    export_nodes = ['inputs', 'labels', 'learning_rate','is_training', 'logits',\n",
        "                    'cost', 'optimizer', 'predicted', 'accuracy']\n",
        "    Graph = namedtuple('Graph', export_nodes)\n",
        "    local_dict = locals()\n",
        "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
        "\n",
        "    return graph\n",
        "\n",
        "model = build_neural_network()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "55b103de-7ae0-389a-7b8a-0452fbef28c1"
      },
      "outputs": [],
      "source": [
        "def get_batch(data_x,data_y,batch_size=32):\n",
        "    batch_n=len(data_x)//batch_size\n",
        "    for i in range(batch_n):\n",
        "        batch_x=data_x[i*batch_size:(i+1)*batch_size]\n",
        "        batch_y=data_y[i*batch_size:(i+1)*batch_size]\n",
        "        \n",
        "        yield batch_x,batch_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "27030dd0-a904-b53e-6165-2d7d7e4ee608"
      },
      "outputs": [],
      "source": [
        "epochs = 200\n",
        "train_collect = 50\n",
        "train_print=train_collect*2\n",
        "\n",
        "learning_rate_value = 0.001\n",
        "batch_size=16\n",
        "\n",
        "x_collect = []\n",
        "train_loss_collect = []\n",
        "train_acc_collect = []\n",
        "valid_loss_collect = []\n",
        "valid_acc_collect = []\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    iteration=0\n",
        "    for e in range(epochs):\n",
        "        for batch_x,batch_y in get_batch(train_x,train_y,batch_size):\n",
        "            iteration+=1\n",
        "            feed = {model.inputs: train_x,\n",
        "                    model.labels: train_y,\n",
        "                    model.learning_rate: learning_rate_value,\n",
        "                    model.is_training:True\n",
        "                   }\n",
        "\n",
        "            train_loss, _, train_acc = sess.run([model.cost, model.optimizer, model.accuracy], feed_dict=feed)\n",
        "            \n",
        "            if iteration % train_collect == 0:\n",
        "                x_collect.append(e)\n",
        "                train_loss_collect.append(train_loss)\n",
        "                train_acc_collect.append(train_acc)\n",
        "\n",
        "                if iteration % train_print==0:\n",
        "                     print(\"Epoch: {}/{}\".format(e + 1, epochs),\n",
        "                      \"Train Loss: {:.4f}\".format(train_loss),\n",
        "                      \"Train Acc: {:.4f}\".format(train_acc))\n",
        "                        \n",
        "                feed = {model.inputs: valid_x,\n",
        "                        model.labels: valid_y,\n",
        "                        model.is_training:False\n",
        "                       }\n",
        "                val_loss, val_acc = sess.run([model.cost, model.accuracy], feed_dict=feed)\n",
        "                valid_loss_collect.append(val_loss)\n",
        "                valid_acc_collect.append(val_acc)\n",
        "                \n",
        "                if iteration % train_print==0:\n",
        "                    print(\"Epoch: {}/{}\".format(e + 1, epochs),\n",
        "                      \"Validation Loss: {:.4f}\".format(val_loss),\n",
        "                      \"Validation Acc: {:.4f}\".format(val_acc))\n",
        "                \n",
        "\n",
        "    saver.save(sess, \"./titanic.ckpt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8ed01203-e9b6-423d-045d-11e6194a59f9"
      },
      "outputs": [],
      "source": [
        "plt.plot(x_collect, train_loss_collect, \"r--\")\n",
        "plt.plot(x_collect, valid_loss_collect, \"g^\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "26749357-f1eb-8112-2f5f-7b15134f31e6"
      },
      "outputs": [],
      "source": [
        "plt.plot(x_collect, train_acc_collect, \"r--\")\n",
        "plt.plot(x_collect, valid_acc_collect, \"g^\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e1336e1c-7d5e-c200-bcbb-24a7bb8d6bb7"
      },
      "outputs": [],
      "source": [
        "model=build_neural_network()\n",
        "restorer=tf.train.Saver()\n",
        "with tf.Session() as sess:\n",
        "    restorer.restore(sess,\"./titanic.ckpt\")\n",
        "    feed={\n",
        "        model.inputs:test_data,\n",
        "        model.is_training:False\n",
        "    }\n",
        "    test_predict=sess.run(model.predicted,feed_dict=feed)\n",
        "    \n",
        "test_predict[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "cc7801f4-88e2-9140-5313-17c444506dde"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import Binarizer\n",
        "binarizer=Binarizer(0.5)\n",
        "test_predict_result=binarizer.fit_transform(test_predict)\n",
        "test_predict_result=test_predict_result.astype(np.int32)\n",
        "test_predict_result[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "461ee38a-4b8f-a1af-4448-d2503ccc9be3"
      },
      "outputs": [],
      "source": [
        "passenger_id=test_passenger_id.copy()\n",
        "evaluation=passenger_id.to_frame()\n",
        "evaluation[\"Survived\"]=test_predict_result\n",
        "evaluation[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "15867098-96d8-9878-83b1-9741711928cc"
      },
      "outputs": [],
      "source": [
        "evaluation.to_csv(\"evaluation_submission.csv\",index=False)"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}