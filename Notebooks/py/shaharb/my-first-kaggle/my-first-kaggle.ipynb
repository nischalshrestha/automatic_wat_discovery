{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a59f0ff9-30e1-3cb5-d520-4a04226c9841"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1991b5a7-d731-41ce-a961-063f3bc86b4a"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Author : Shahar Barak\n",
        "Date : 06 Janudary 2016\n",
        "Revised: 06 Janudary 2016\n",
        "\n",
        "some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n",
        "complete the analysis of what sorts of people were likely to survive.\n",
        "In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\n",
        "\"\"\"\n",
        "from IPython.core.display import HTML\n",
        "HTML(\"\"\"\n",
        "<style>\n",
        ".output_png {\n",
        "    display: table-cell;\n",
        "    text-align: center;\n",
        "    vertical-align: middle;\n",
        "}\n",
        "</style>\n",
        "\"\"\")\n",
        "\n",
        "# remove warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "# ---\n",
        "\n",
        "#Data analysis\n",
        "import itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas import Series,DataFrame\n",
        "from time import time\n",
        "#Graphics\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns # statistical data visualization\n",
        "sns.set_style('whitegrid')\n",
        "%matplotlib inline\n",
        "matplotlib.rcParams['figure.figsize'] = (20.0, 10.0)\n",
        "#Machine learning\n",
        "import sklearn\n",
        "from sklearn import metrics\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.cross_validation import StratifiedKFold\n",
        "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\n",
        "from sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.linear_model import LogisticRegression, RidgeClassifier, Perceptron, PassiveAggressiveClassifier, SGDClassifier\n",
        "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier, NearestCentroid\n",
        "from sklearn.utils.extmath import density\n",
        "from sklearn.svm import LinearSVC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5bf1babd-a0a7-966a-5859-49418ab4384e"
      },
      "outputs": [],
      "source": [
        "def ReadData():\n",
        "## Read Data\n",
        "    test_file = 'test.csv'\n",
        "    train_file  = 'train.csv'\n",
        "    test = pd.read_csv(test_file, index_col='PassengerId')\n",
        "    train = pd.read_csv(train_file, index_col='PassengerId')\n",
        "    return test, train\n",
        "\n",
        "def SurvivalStats(df):\n",
        "    # VARIABLE DESCRIPTIONS: survival Survival (0 = No; 1 = Yes) pclass Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd)\n",
        "    # name Name sex Sex age Age sibsp Number of Siblings/Spouses Aboard parch Number of Parents/Children Aboard \n",
        "    # ticket Ticket Number fare Passenger Fare cabin Cabin \n",
        "    # embarked Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)\n",
        "    \n",
        "    n_passengers = df.shape[0]\n",
        "    n_survivors = df['Survived'][df['Survived'] == 1].count()\n",
        "    n_female = df['Sex_female'][df['Sex_female'] == 1].count()\n",
        "    n_male = df['Sex_male'][df['Sex_male'] == 1].count()\n",
        "    n_pc1 = df['Pclass_1'][df['Pclass_1'] == 1].count()\n",
        "    n_pc2 = df['Pclass_2'][df['Pclass_2'] == 1].count()\n",
        "    n_pc3 = df['Pclass_3'][df['Pclass_3'] == 1].count()\n",
        "    n_female_srv = df['Sex_female'][df['Sex_female'] == 1][df['Survived'] == 1].count()\n",
        "    n_male_srv = df['Sex_male'][df['Sex_male'] == 1][df['Survived'] == 1].count()\n",
        "    n_pc1_srv = df['Pclass_1'][df['Pclass_1'] == 1][df['Survived'] == 1].count()\n",
        "    n_pc2_srv = df['Pclass_2'][df['Pclass_2'] == 1][df['Survived'] == 1].count()\n",
        "    n_pc3_srv = df['Pclass_3'][df['Pclass_3'] == 1][df['Survived'] == 1].count() \n",
        "    \n",
        "    print('Number of passengers:', n_passengers)\n",
        "    print('Number of survivors: %d (%.0f%%)' % (n_survivors, 100*(n_survivors/n_passengers)))\n",
        "    print('Number of female: %d (%.0f%%)' % (n_female, 100*(n_female/n_passengers)))\n",
        "    print('Number of male: %d (%.0f%%)' % (n_male, 100*(n_male/n_passengers)))\n",
        "    print('Number of Class 1: %d (%.0f%%)' % (n_pc1, 100*(n_pc1/n_passengers)))\n",
        "    print('Number of Class 2: %d (%.0f%%)' % (n_pc2, 100*(n_pc2/n_passengers)))\n",
        "    print('Number of Class 3: %d (%.0f%%)' % (n_pc3, 100*(n_pc3/n_passengers)))\n",
        "    print('Number of female survivors: %d (%.0f%%)' % (n_female_srv, 100*(n_female_srv/n_survivors)))\n",
        "    print('Number of male survivors: %d (%.0f%%)' % (n_male_srv, 100*(n_male_srv/n_survivors)))\n",
        "    print('Number of Class 1 survivors: %d (%.0f%%)' % (n_pc1_srv, 100*(n_pc1_srv/n_survivors)))\n",
        "    print('Number of Class 2 survivors: %d (%.0f%%)' % (n_pc2_srv, 100*(n_pc2_srv/n_survivors)))\n",
        "    print('Number of Class 3 survivors: %d (%.0f%%)' % (n_pc3_srv, 100*(n_pc3_srv/n_survivors)))\n",
        "    print('')\n",
        "    \n",
        "def PrepareData(df):\n",
        "## Prepare Data\n",
        "    \n",
        "    #df = get_titles(df)\n",
        "\n",
        "    # Fill missing values and bin\n",
        "    df = FillMissingAge(df)\n",
        "    df = BinData(df, 'Age', [0, 14, 80, 100], ['Child', 'Adult', 'Senior'])\n",
        "    df['Fare'].fillna(df['Fare'].median(), inplace=True)\n",
        "    #df = BinData(df, 'Fare', [df[\"Fare\"].min(), df[\"Fare\"].median(), df[\"Fare\"].max()], ['Low', 'High'])\n",
        "    df['Embarked'].fillna('S', inplace=True)\n",
        "    df.Cabin.fillna('T',inplace=True)    \n",
        "    # mapping each Cabin value with the cabin letter\n",
        "    df['Cabin'] = df['Cabin'].map(lambda c : c[0])\n",
        "     \n",
        "    # Convert categorical variables into indicator variables\n",
        "    df = pd.get_dummies(df, columns=['Sex'])\n",
        "    \n",
        "    #df = NewFeature(df)\n",
        "    #df.drop(['Fare_High', 'F_C1', 'Pclass_2', 'M_C3', 'F_0-10', 'F_60-100', 'Age_60-100', 'Fare_Low', 'Age_0-10', 'M_C1', 'M_C2', 'M_0-10', 'F_C2','M_60-100','Age_10-60','F_C3'], axis=1, inplace=True)\n",
        "    #df['Family_Size'] = df['Parch'] + df['SibSp']\n",
        "    #df['NOT_ADULT'] = df['Age_Child'] + df['Age_Senior']\n",
        "    #df['Not_Class1'] = df['Pclass_2'] + df['Pclass_3']\n",
        "    #df['Not_Class3'] = df['Pclass_2'] + df['Pclass_1']\n",
        "    # drop unnecessary columns, these columns won't be useful in analysis and prediction\n",
        "    df.drop(['Name', 'Cabin', 'Ticket', 'Name', 'Parch', 'SibSp', 'Embarked','Pclass'], axis=1, inplace=True)\n",
        "    return df\n",
        "\n",
        "def get_titles(df):\n",
        "    \n",
        "    # we extract the title from each name\n",
        "    df['Title'] = df['Name'].map(lambda name:name.split(',')[1].split('.')[0].strip())\n",
        "    \n",
        "    # a map of more aggregated titles\n",
        "    Title_Dictionary = {\n",
        "                        \"Capt\":       \"VIP\",\n",
        "                        \"Col\":        \"VIP\",\n",
        "                        \"Major\":      \"VIP\",\n",
        "                        \"Jonkheer\":   \"VIP\",\n",
        "                        \"Don\":        \"VIP\",\n",
        "                        \"Sir\" :       \"VIP\",\n",
        "                        \"Dr\":         \"VIP\",\n",
        "                        \"Rev\":        \"VIP\",\n",
        "                        \"the Countess\":\"VIP\",\n",
        "                        \"Dona\":       \"VIP\",\n",
        "                        \"Mme\":        \"Ms\",\n",
        "                        \"Mlle\":       \"Ms\",\n",
        "                        \"Ms\":         \"Ms\",\n",
        "                        \"Mr\" :        \"Mr\",\n",
        "                        \"Mrs\" :       \"Ms\",\n",
        "                        \"Miss\" :      \"Ms\",\n",
        "                        \"Master\" :    \"VIP\",\n",
        "                        \"Lady\" :      \"VIP\"\n",
        "\n",
        "                        }\n",
        "    \n",
        "    # we map each title\n",
        "    df['Title'] = df.Title.map(Title_Dictionary)\n",
        "    return df\n",
        "    \n",
        "def BinData(df, feature, bins, group_names):\n",
        "## Bin Data\n",
        " \n",
        "    # Bin data\n",
        "    df[feature] = pd.cut(df[feature], bins, labels=group_names)\n",
        "    # Convert categorical variable into indicator variables\n",
        "    df = pd.get_dummies(df, columns=[feature])\n",
        "    \n",
        "    return df\n",
        "\n",
        "def FillMissingAge(df):\n",
        "# Age \n",
        "# Fill missing \"Age\" values with random numbers that fall within the first standard deviation of the mean\n",
        "\n",
        "    # get average, std, and number of NaN values in titanic_df\n",
        "    average_age   = df[\"Age\"].mean()\n",
        "    std_age       = df[\"Age\"].std()\n",
        "    count_nan_age = df[\"Age\"].isnull().sum()\n",
        "\n",
        "    # generate random numbers between (mean - std) & (mean + std)\n",
        "    rand = np.random.randint(average_age - std_age, average_age + std_age, size = count_nan_age)\n",
        "\n",
        "    # fill NaN values in Age column with random values generated\n",
        "    df[\"Age\"][np.isnan(df[\"Age\"])] = rand\n",
        "\n",
        "    # convert from float to int\n",
        "    df['Age'] = df['Age'].astype(int)\n",
        "    \n",
        "    return df\n",
        "\n",
        "def NewFeature(df):\n",
        "    ## combine age and gender\n",
        "    ## combine class and gender\n",
        "    df['F_0-10'] = df['Sex_female'] * df['Age_0-10']\n",
        "    df['F_10-60'] = df['Sex_female'] * df['Age_10-60']\n",
        "    df['F_60-100'] = df['Sex_female'] * df['Age_60-100']\n",
        "    \n",
        "    df['M_0-10'] = df['Sex_male'] * df['Age_0-10']\n",
        "    df['M_10-60'] = df['Sex_male'] * df['Age_10-60']\n",
        "    df['M_60-100'] = df['Sex_male'] * df['Age_60-100']\n",
        "\n",
        "    df['F_C1'] = df['Sex_female'] * df['Pclass_1']\n",
        "    df['F_C2'] = df['Sex_female'] * df['Pclass_2']\n",
        "    df['F_C3'] = df['Sex_female'] * df['Pclass_3']\n",
        "    \n",
        "    df['M_C1'] = df['Sex_male'] * df['Pclass_1']\n",
        "    df['M_C2'] = df['Sex_male'] * df['Pclass_2']\n",
        "    df['M_C3'] = df['Sex_male'] * df['Pclass_3']\n",
        "    \n",
        "    df['Family_Size'] = df['Parch'] + df['SibSp']\n",
        "    return df\n",
        "\n",
        "\n",
        "def RunRandomForest(X_train, Y_train, X_test):\n",
        "## Random Forests\n",
        "    \n",
        "    forest = RandomForestClassifier(max_features='sqrt')\n",
        "       \n",
        "    # Tune Hyperparameters through a grid search\n",
        "    parameter_grid = {'n_estimators': [5,10,15,25,50,100,200,300],\n",
        "                     'criterion': ['gini','entropy']}\n",
        "\n",
        "    cross_validation = StratifiedKFold(Y_train, n_folds=5)\n",
        "\n",
        "    grid_search = GridSearchCV(forest,\n",
        "                               param_grid=parameter_grid,\n",
        "                               cv=cross_validation)\n",
        "\n",
        "    grid_search.fit(X_train, Y_train)\n",
        "\n",
        "    #print('Best score: {}'.format(grid_search.best_score_))\n",
        "    #print('Best parameters: {}'.format(grid_search.best_params_))\n",
        "    \n",
        "    forest = RandomForestClassifier(n_estimators=grid_search.best_params_['n_estimators'], criterion=grid_search.best_params_['criterion'])\n",
        "    \n",
        "    # Build a forest of trees from the training set (X, y)\n",
        "    forest.fit(X_train, Y_train)\n",
        "    # Predict class for X\n",
        "    Y_pred = forest.predict(X_test)    \n",
        "    # Returns the mean accuracy on the given test data and labels.\n",
        "    score = forest.score(X_train, Y_train)\n",
        "    print('Random Forest score:', score)\n",
        "    # Plot Feature Importance\n",
        "    PlotFeatureImportance(X_train, forest)\n",
        "    \n",
        "    return Y_pred, score, forest\n",
        "\n",
        "def PlotFeatureImportance(X, random_forest):\n",
        "    \n",
        "    importances = random_forest.feature_importances_\n",
        "    std = np.std([tree.feature_importances_ for tree in random_forest.estimators_],\n",
        "             axis=0)\n",
        "    indices = np.argsort(importances)[::-1]\n",
        "    features = np.array(list(X))\n",
        "    # Print the feature ranking\n",
        "    print(\"Feature ranking:\")\n",
        "\n",
        "    for f in range(X.shape[1]):\n",
        "        print(\"%d. %s (%f)\" % (f + 1, features[indices[f]], importances[indices[f]]))\n",
        "\n",
        "    # Plot the feature importances of the forest\n",
        "    plt.figure()\n",
        "    plt.title(\"Feature importances\")\n",
        "    plt.bar(range(X.shape[1]), importances[indices],\n",
        "           color=\"r\", yerr=std[indices], align=\"center\")\n",
        "    plt.xticks(range(X.shape[1]), features[indices])\n",
        "    plt.xlim([-1, X.shape[1]])\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "def ConfusionMatrix(Y_train, Y_pred, train):\n",
        "    cnf_matrix = confusion_matrix(Y_train, Y_pred)\n",
        "    np.set_printoptions(precision=2)\n",
        "\n",
        "    # Plot non-normalized confusion matrix\n",
        "    plt.figure()\n",
        "    plot_confusion_matrix(cnf_matrix, classes=list(train),\n",
        "                          title='Confusion matrix, without normalization')\n",
        "\n",
        "    # Plot normalized confusion matrix\n",
        "    plt.figure()\n",
        "    plot_confusion_matrix(cnf_matrix, classes=list(train), normalize=True,\n",
        "                          title='Normalized confusion matrix')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def benchmark(clf):\n",
        "## Benchmark classifiers\n",
        "## Modified after: http://scikit-learn.org/stable/auto_examples/text/document_classification_20newsgroups.html#sphx-glr-auto-examples-text-document-classification-20newsgroups-py\n",
        "    print('_' * 80)\n",
        "    print(\"Training: \")\n",
        "    print(clf)\n",
        "    t0 = time()\n",
        "    clf.fit(X_train, Y_train)\n",
        "    train_time = time() - t0\n",
        "    print(\"train time: %0.3fs\" % train_time)\n",
        "\n",
        "    t0 = time()\n",
        "    Y_pred = clf.predict(X_test)\n",
        "    test_time = time() - t0\n",
        "    print(\"test time:  %0.3fs\" % test_time)\n",
        "\n",
        "    score = clf.score(X_train, Y_train)\n",
        "    print(\"Training accuracy:   %0.3f\" % score)\n",
        "\n",
        "    if hasattr(clf, 'coef_'):\n",
        "        print(\"dimensionality: %d\" % clf.coef_.shape[1])\n",
        "        print(\"density: %f\" % density(clf.coef_))\n",
        "\n",
        "    print(\"classification report:\")\n",
        "    print(metrics.classification_report(Y_test, Y_pred, target_names=target_names))\n",
        "\n",
        "    print(\"confusion matrix:\")\n",
        "    print(metrics.confusion_matrix(Y_test, Y_pred))\n",
        "\n",
        "    print()\n",
        "    clf_descr = str(clf).split('(')[0]\n",
        "    return clf_descr, score, train_time, test_time\n",
        "\n",
        "\n",
        "def PlotClf(results):\n",
        "## make some plots\n",
        "\n",
        "    indices = np.arange(len(results))\n",
        "\n",
        "    results = [[x[i] for x in results] for i in range(4)]\n",
        "\n",
        "    clf_names, score, training_time, test_time = results\n",
        "    training_time = np.array(training_time) / np.max(training_time)\n",
        "    test_time = np.array(test_time) / np.max(test_time)\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.title(\"Score\")\n",
        "    plt.barh(indices, score, .2, label=\"score\", color='navy')\n",
        "    plt.barh(indices + .3, training_time, .2, label=\"training time\",\n",
        "             color='c')\n",
        "    plt.barh(indices + .6, test_time, .2, label=\"test time\", color='darkorange')\n",
        "    plt.yticks(())\n",
        "    plt.legend(loc='best')\n",
        "    plt.subplots_adjust(left=.25)\n",
        "    plt.subplots_adjust(top=.95)\n",
        "    plt.subplots_adjust(bottom=.05)\n",
        "\n",
        "    for i, c in zip(indices, clf_names):\n",
        "        plt.text(-.3, i, c)\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a64c8f12-7b80-e33e-f6cc-59a571a83669"
      },
      "outputs": [],
      "source": [
        "###################\n",
        "# Begin Execution #\n",
        "###################\n",
        "\n",
        "# Read Data\n",
        "test, train = ReadData()\n",
        "\n",
        "# Prepare Data\n",
        "train_prep = PrepareData(train.copy())\n",
        "test_prep = PrepareData(test.copy())\n",
        "target_names = train_prep.columns\n",
        "\n",
        "# Split traning set\n",
        "x = train_prep.drop(\"Survived\",axis=1)\n",
        "y = train_prep[\"Survived\"]\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.33, random_state=42)\n",
        "\n",
        "# FSurvival Stats\n",
        "#SurvivalStats(train_prep)\n",
        "\n",
        "results = []\n",
        "for clf, name in (\n",
        "        (RidgeClassifier(alpha=5.0, tol=1e-2, solver=\"lsqr\"), \"Ridge Classifier\"),\n",
        "        (Perceptron(n_iter=50), \"Perceptron\"),\n",
        "        (PassiveAggressiveClassifier(n_iter=50), \"Passive-Aggressive\"),\n",
        "        (KNeighborsClassifier(n_neighbors=10), \"kNN\"),\n",
        "        (RandomForestClassifier(n_estimators=100), \"Random forest\")):\n",
        "    print('=' * 80)\n",
        "    print(name)\n",
        "    results.append(benchmark(clf))\n",
        "\n",
        "for penalty in [\"l2\", \"l1\"]:\n",
        "    print('=' * 80)\n",
        "    print(\"%s penalty\" % penalty.upper())\n",
        "    # Train Liblinear model\n",
        "    results.append(benchmark(LinearSVC(loss='l2', penalty=penalty,\n",
        "                                            dual=False, tol=1e-3)))\n",
        "\n",
        "    # Train SGD model\n",
        "    results.append(benchmark(SGDClassifier(alpha=.0001, n_iter=50,\n",
        "                                           penalty=penalty)))\n",
        "\n",
        "# Train SGD with Elastic Net penalty\n",
        "print('=' * 80)\n",
        "print(\"Elastic-Net penalty\")\n",
        "results.append(benchmark(SGDClassifier(alpha=.0001, n_iter=50,\n",
        "                                       penalty=\"elasticnet\")))\n",
        "\n",
        "# Train NearestCentroid without threshold\n",
        "print('=' * 80)\n",
        "print(\"NearestCentroid (aka Rocchio classifier)\")\n",
        "results.append(benchmark(NearestCentroid()))\n",
        "\n",
        "# Train sparse Naive Bayes classifiers\n",
        "print('=' * 80)\n",
        "print(\"Naive Bayes\")\n",
        "results.append(benchmark(MultinomialNB(alpha=.01)))\n",
        "results.append(benchmark(BernoulliNB(alpha=.01)))\n",
        "\n",
        "print('=' * 80)\n",
        "print(\"LinearSVC with L1-based feature selection\")\n",
        "# The smaller C, the stronger the regularization.\n",
        "# The more regularization, the more sparsity.\n",
        "results.append(benchmark(Pipeline([\n",
        "  ('feature_selection', LinearSVC(penalty=\"l1\", dual=False, tol=1e-3)),\n",
        "  ('classification', LinearSVC())\n",
        "])))\n",
        "\n",
        "PlotClf(results)\n",
        "\n",
        "# Define training and testing sets\n",
        "#X_train = train_prep.drop(\"Survived\",axis=1)\n",
        "#Y_train = train_prep[\"Survived\"]\n",
        "X_test  = test_prep\n",
        "\n",
        "\n",
        "# Logistic Regression\n",
        "#Y_pred, score, coeff_df = RunLogReg(X_train, Y_train, X_test)\n",
        "\n",
        "# Ridge Regression\n",
        "#Y_pred, score, coeff_df = RunRidgeReg(X_train, Y_train, X_test)\n",
        "\n",
        "\n",
        "\n",
        "# Compute confusion matrix\n",
        "#ConfusionMatrix(Y_train, Y_pred, train)\n",
        "\n",
        "# Random Forest\n",
        "Y_pred, rf_score, random_forest = RunRandomForest(X_train, Y_train, X_test)\n",
        "\n",
        "# Write predictions\n",
        "test['Survived'] = Y_pred\n",
        "test.to_csv('output.csv',index=True, columns=['Survived'], header=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b7d1e2b2-a585-acfc-41c8-529aad4e7046"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "45422392-191d-109f-82b4-e01d084d7525",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}