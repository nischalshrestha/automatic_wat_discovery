{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport math\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\ntrain_file_path = '../input/train.csv'\ntest_file_path = '../input/test.csv'\n\ntrain_data = pd.read_csv(train_file_path)\ntest_data = pd.read_csv(test_file_path)\n\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"train_data.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c51261e6d2054631e46970e5d363b0713ae9bb92","collapsed":true},"cell_type":"code","source":"train_data.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c5ab615facf37b1947ea64b145f7c7021b52a004","collapsed":true},"cell_type":"code","source":"print(\"missing values: (train_data)\")\nprint(train_data.isnull().sum().sort_values(ascending=False))\n\nprint(\"\\nmissing values: (test_data)\")\nprint(test_data.isnull().sum().sort_values(ascending=False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"948ccafaba58fa158f4fd06476dcc8ec1c4fb14b","collapsed":true},"cell_type":"code","source":"def distplot(titanic_dataframe, cols, filter_zero=False):\n    num_col = 3\n    num_row = math.ceil(len(cols) / num_col)\n    plt.figure(figsize=(6 * num_col, 5 * num_row))\n    for i, col in enumerate(cols, 1):\n        plt.subplot(num_row, num_col, i)\n        if filter_zero:\n            sns.distplot(titanic_dataframe[titanic_dataframe[col] > 0][col], fit=norm);\n        else:\n            sns.distplot(titanic_dataframe[col], fit=norm);\n\ndistplot(train_data.fillna(0), ['Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dcb81ea613ff97c341829c658ad195e6c233e775","collapsed":true},"cell_type":"code","source":"def process_age(df,cut_points,label_names):\n    df[\"Age\"] = df[\"Age\"].fillna(-0.5)\n    df[\"Age_cat\"] = pd.cut(df[\"Age\"],cut_points,labels=label_names)\n    return df\n\ncut_points = [-1,0,5,16,22,35,60,100]\nlabel_names = [\"Missing\",\"Infant\",\"Child\",\"Teenager\",\"YoungAdult\",\"Adult\",\"Senior\"]\n\ntrain_data = process_age(train_data,cut_points,label_names)\ntest_data = process_age(test_data,cut_points,label_names)\n\n\ntrain_data['Fare'] = np.log1p(train_data['Fare'])\ntest_data['Fare'] = np.log1p(test_data['Fare'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"db5cd7e9fd31c8994c8866200dfa786507888c49","collapsed":true},"cell_type":"code","source":"train_x = train_data[['Pclass', 'Sex', 'Age_cat', 'SibSp', 'Parch', 'Fare','Embarked']]\ntrain_y = train_data[['Survived']]\ntest_x = test_data[['Pclass', 'Sex', 'Age_cat', 'SibSp', 'Parch', 'Fare','Embarked']]\n\ntrain_x = pd.get_dummies(train_x)\ntest_x = pd.get_dummies(test_x)\n\ntrain_x.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3e2bbbfccd528093e3902fa249d8d0f7d2ec7d0a","collapsed":true},"cell_type":"code","source":"num_sample = len(train_x)\nprint(num_sample)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2c58564e6cf1e32a5be27d64aae3d2b5e27861fa"},"cell_type":"code","source":"#split training samples v.s. validation samples\nnum_sample = len(train_x)\ntrain_num = int(num_sample * 0.7)\nvalidate_num = num_sample - train_num\n\ntrain_examples = train_x.head(train_num)\ntrain_targets = train_y.head(train_num)\n\n\nvalidate_examples = train_x.tail(validate_num)\nvalidate_targets = train_y.tail(validate_num)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c5c88f2f3ebdc5c5d8f4c9de5297eebaffa3f503","collapsed":true},"cell_type":"code","source":"from tensorflow.python.data import Dataset\ndef my_input_fn(features, targets, batch_size=1, shuffle=True, num_epochs=None):\n    # Convert pandas data into a dict of np arrays.\n    features = {key:np.array(value) for key,value in dict(features).items()}                                           \n \n    # Construct a dataset, and configure batching/repeating.\n    ds = Dataset.from_tensor_slices((features,targets)) # warning: 2GB limit\n    ds = ds.batch(batch_size).repeat(num_epochs)\n    \n    # Shuffle the data, if specified.\n    if shuffle:\n      ds = ds.shuffle(100)\n    \n    # Return the next batch of data.\n    features, labels = ds.make_one_shot_iterator().get_next()\n    return features, labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"078b6e26eeb8e463dcf9b022497191fbf06c62eb","collapsed":true},"cell_type":"code","source":"import tensorflow as tf\n\ndef construct_feature_columns(input_features):\n    return set([tf.feature_column.numeric_column(my_feature) for my_feature in input_features])\n\nmy_optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\nmy_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n\nfeature_columns = construct_feature_columns(train_examples)\n\nclassifier = tf.estimator.LinearClassifier(\n  feature_columns = feature_columns,\n  optimizer=my_optimizer,\n)\n\nclassifier.train(\n  input_fn=lambda: my_input_fn(train_examples,train_targets),\n  steps=1000)\n\nevaluation_metrics = classifier.evaluate(\n  input_fn=lambda: my_input_fn(train_examples,train_targets),\n  steps=1000)\nprint(\"Training set metrics:\")\nfor m in evaluation_metrics:\n  print(m, evaluation_metrics[m])\nprint(\"---\")\n\nevaluation_metrics = classifier.evaluate(\n  input_fn=lambda: my_input_fn(validate_examples,validate_targets),\n  steps=1000)\nprint(\"Validation set metrics:\")\nfor m in evaluation_metrics:\n  print(m, evaluation_metrics[m])\nprint(\"---\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2fd5418d5384b0b5f626822a51ac883caf3e7ffe","scrolled":false,"collapsed":true},"cell_type":"code","source":"pred_li = classifier.predict(lambda: my_input_fn(test_x,test_x[\"Fare\"],1,True,1))\npred_li = np.array([item['class_ids'][0] for item in pred_li])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7eb843d0ea131e38e789c8f35e46fb6d42d90447","collapsed":true},"cell_type":"code","source":"linear_result = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': pred_li})\nlinear_result.head(5)\n\nlinear_result.to_csv('Titanic_Linear.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0ef60c47378f929f1e7f7b129b59f8535dc412f6","collapsed":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import mean_absolute_error\n\nforest_model = RandomForestClassifier(max_depth=8,random_state=50)\nforest_model.fit(train_examples, train_targets)\nmelb_preds = forest_model.predict(validate_examples)\nprint(mean_absolute_error(validate_targets, melb_preds))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e5df5e3a257b40563fafb4fa7f0f7db1a82cf80d","collapsed":true},"cell_type":"code","source":"pred = forest_model.predict(test_x.fillna(0))\nsolution = pd.DataFrame({\"id\":test_data.PassengerId, \"Survived\":pred})\nsolution.head(5)\n\nsolution.to_csv(\"Titanic_Dicision.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c1055182e79c4d0161b03c036e4edc12a13c10de","collapsed":true},"cell_type":"code","source":"my_optimizer = tf.train.AdagradOptimizer(learning_rate=0.1)\nmy_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n\nestimator = tf.estimator.DNNClassifier(\n    feature_columns=feature_columns,\n    hidden_units=[20, 10],\n    optimizer=my_optimizer\n    )\n\nestimator.train(\n  input_fn=lambda: my_input_fn(train_examples,train_targets),\n  steps=1000)\n\nevaluation_metrics = estimator.evaluate(\n  input_fn=lambda: my_input_fn(train_examples,train_targets),\n  steps=1000)\nprint(\"Training set metrics:\")\nfor m in evaluation_metrics:\n  print(m, evaluation_metrics[m])\nprint(\"---\")\n\nevaluation_metrics = estimator.evaluate(\n  input_fn=lambda: my_input_fn(validate_examples,validate_targets),\n  steps=1000)\nprint(\"Validation set metrics:\")\nfor m in evaluation_metrics:\n  print(m, evaluation_metrics[m])\nprint(\"---\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0dc418d7399da07edb9bff3e6c39d89acfb4a0fc"},"cell_type":"code","source":"nn_result = estimator.predict(lambda: my_input_fn(test_x,test_x[\"Fare\"],1,True,1))\nnn_result = np.array([prediction[\"class_ids\"][0] for prediction in nn_result])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"455f110485f908c4008845367cbc752cbf69636e","collapsed":true},"cell_type":"code","source":"neural_result = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': nn_result})\nneural_result.head(5)\n\nneural_result.to_csv('Titanic_Neural.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}