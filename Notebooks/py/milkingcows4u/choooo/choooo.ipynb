{"cells":[
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "# Load packages\nimport numpy as np  \nimport pandas as pd\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport re\nimport seaborn as sns\nsns.set_style('whitegrid')\n\nfrom sklearn import cross_validation\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\n\nprint (\"Read in packages from numpy, pandas, sklearn, seaborn & matplotlib\")"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "# Load training data\ntrain_set = pd.read_csv(\"../input/train.csv\")\ntest_set  = pd.read_csv(\"../input/test.csv\")\nprint (\"Read in training, test data as Panda dataframes\")\n\n# Review input features - Part 1\nprint (\"\\n\\n---------------------\")\nprint (\"TRAIN SET INFORMATION\")\nprint (\"---------------------\")\nprint (\"Shape of training set:\", train_set.shape, \"\\n\")\nprint (\"Column Headers:\", list(train_set.columns.values), \"\\n\")\nprint (train_set.describe(), \"\\n\\n\")\nprint (train_set.dtypes)\n\nprint (\"\\n\\n--------------------\")\nprint (\"TEST SET INFORMATION\")\nprint (\"--------------------\")\nprint (\"Shape of test set:\", test_set.shape, \"\\n\")\nprint (\"Column Headers:\", list(test_set.columns.values), \"\\n\")\nprint (test_set.describe(), \"\\n\\n\")\nprint (test_set.dtypes)"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "# Review input features (train set) - Part 2A\nmissing_values = []\nnonumeric_values = []\n\nprint (\"TRAINING SET INFORMATION\")\nprint (\"========================\\n\")\n\nfor column in train_set:\n    # Find all the unique feature values\n    uniq = train_set[column].unique()\n    print (\"'{}' has {} unique values\" .format(column,uniq.size))\n    if (uniq.size > 25):\n        print(\"~~Listing up to 25 unique values~~\")\n    print (uniq[0:24])\n    print (\"\\n-----------------------------------------------------------------------\\n\")\n    \n    # Find features with missing values\n    if (True in pd.isnull(uniq)):\n        s = \"{} has {} missing\" .format(column, pd.isnull(train_set[column]).sum())\n        missing_values.append(s)\n    \n    # Find features with non-numeric values\n    for i in range (1, np.prod(uniq.shape)):\n        if (re.match('nan', str(uniq[i]))):\n            break\n        if not (re.search('(^\\d+\\.?\\d*$)|(^\\d*\\.?\\d+$)', str(uniq[i]))):\n            nonumeric_values.append(column)\n            break\n  \nprint (\"\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\")\nprint (\"Features with missing values:\\n{}\\n\\n\" .format(missing_values))\nprint (\"Features with non-numeric values:\\n{}\" .format(nonumeric_values))\nprint (\"\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\")\n# Review input features (test set) - Part 2B\nmissing_values = []\nnonumeric_values = []\n\nprint (\"TEST SET INFORMATION\")\nprint (\"====================\\n\")\n\nfor column in test_set:\n    # Find all the unique feature values\n    uniq = test_set[column].unique()\n    print (\"'{}' has {} unique values\" .format(column,uniq.size))\n    if (uniq.size > 25):\n        print(\"~~Listing up to 25 unique values~~\")\n    print (uniq[0:24])\n    print (\"\\n-----------------------------------------------------------------------\\n\")\n    \n    # Find features with missing values\n    if (True in pd.isnull(uniq)):\n        s = \"{} has {} missing\" .format(column, pd.isnull(test_set[column]).sum())\n        missing_values.append(s)\n    \n    # Find features with non-numeric values\n    for i in range (1, np.prod(uniq.shape)):\n        if (re.match('nan', str(uniq[i]))):\n            break\n        if not (re.search('(^\\d+\\.?\\d*$)|(^\\d*\\.?\\d+$)', str(uniq[i]))):\n            nonumeric_values.append(column)\n            break\n  \nprint (\"\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\")\nprint (\"Features with missing values:\\n{}\\n\\n\" .format(missing_values))\nprint (\"Features with non-numeric values:\\n{}\" .format(nonumeric_values))\nprint (\"\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\")"
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "Notes about input features\n--------------------------  \n\n###Size of training data\nShape of dataframe: (891, 11+1) \n \n###Size of test data\nShape of dataframe: (418, 11) \n\n###Output Feature (1)\nSurvived (0 | 1)\n \n###Input Features (11)  \nPassengerId [1 2 3 ... ]      \nPclass      [1 2 3]  \nName        ['Braund, Mr. Owen Harris' 'Cumings, Mrs. John Bradley (Florence Briggs Thayer)' 'Heikkinen, Miss. Laina' ...]   \nSex         ['male' 'female']  \nAge         [22. 38. 26. ...]   \nSibSp       [0 1 2 3 4 5 8]  \nParch       [0 1 2 3 4 5 6 (9)]  \nTicket   ['A/5 21171' 'PC 17599' 'STON/O2. 3101282' ... ]  \nFare     [7.25  71.2833  7.925 ... ]  \nCabin    [nan 'C85' 'C123' 'E46' ... ]  \nEmbarked ['S' 'C' 'Q' nan]\n\n###Features w/ missing values (3 train, 3 test)\nCabin (687, 327)  \nAge (177, 86)  \nEmbarked (2, 0)  \nFare (0, 1)\n\n###Features w/ non-numeric values (5)\nName  \nSex  \nTicket  \nCabin  \nEmbarked"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "# Feature Cleaning\n# Convert non-numeric values for Sex, Embarked\n# male=0, female=1\ntrain_set.loc[train_set[\"Sex\"] == \"male\", \"Sex\"]   = 0\ntrain_set.loc[train_set[\"Sex\"] == \"female\", \"Sex\"] = 1\n\ntest_set.loc[test_set[\"Sex\"] == \"male\", \"Sex\"]   = 0\ntest_set.loc[test_set[\"Sex\"] == \"female\", \"Sex\"] = 1\n\n# Handle Parch=9 found only in test\n# replace by value 6 which is the closest available in training data\ntest_set.loc[test_set[\"Parch\"] == 9, \"Parch\"] = 6\n\n# S=0, C=1, Q=2\ntrain_set.loc[train_set[\"Embarked\"] == \"S\", \"Embarked\"] = 0\ntrain_set.loc[train_set[\"Embarked\"] == \"C\", \"Embarked\"] = 1\ntrain_set.loc[train_set[\"Embarked\"] == \"Q\", \"Embarked\"] = 2\n\ntest_set.loc[test_set[\"Embarked\"] == \"S\", \"Embarked\"] = 0\ntest_set.loc[test_set[\"Embarked\"] == \"C\", \"Embarked\"] = 1\ntest_set.loc[test_set[\"Embarked\"] == \"Q\", \"Embarked\"] = 2\n\n# Substitute missing values for Age, Embarked & Fare\ntrain_set[\"Age\"]      = train_set[\"Age\"].fillna(train_set[\"Age\"].median())\ntrain_set[\"Fare\"]     = train_set[\"Fare\"].fillna(train_set[\"Fare\"].median())\ntrain_set[\"Embarked\"] = train_set[\"Embarked\"].fillna(train_set[\"Embarked\"].median())\n\ntest_set[\"Age\"] = test_set[\"Age\"].fillna(test_set[\"Age\"].median())\ntest_set[\"Fare\"] = test_set[\"Fare\"].fillna(test_set[\"Fare\"].median())\n\nprint (\"Converted non-numeric features for Sex & Embarked...\\nSubstituted missing values for Age, Embarked & Fare\")"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "# Pclass - Visualize the features and their impact on outcomes\n# Two subplots, the axes array is 1-d\nprint (\"VISUALIZING TRAINING DATA - FEATURES vs OUTCOME\")\nprint (\"===============================================\\n\\n\")\n\nf, ((axis1,axis2), (axis3,axis4), (axis5,axis6)) = plt.subplots(3,2, sharey=True, figsize=(20,30))\n\nsns.barplot(x='Pclass', y='Survived', data=train_set, order=[1, 2, 3], ax=axis1, palette=\"Blues_d\")\naxis1.set_xticklabels(['First', 'Second', 'Third'], rotation=0)\n\nsns.barplot(x='Sex', y='Survived', data=train_set, order=[0, 1], ax=axis2, palette=\"Blues_d\")\naxis2.set_xticklabels(['M', 'F'], rotation=0)\n\nsns.barplot(x='Embarked', y='Survived', data=train_set, order=[0, 1, 2], ax=axis3, palette=\"Blues_d\")\naxis3.set_xticklabels(['S', 'C', 'Q'], rotation=0)\n\nsns.barplot(x='SibSp', y='Survived', data=train_set, order=[0, 1, 2, 3, 4, 5, 8], ax=axis4, palette=\"Blues_d\")\n\nsns.barplot(x='Parch', y='Survived', data=train_set, order=[0, 1, 2, 3, 4, 5, 6], ax=axis5, palette=\"Blues_d\")\n\n\n# Print survival rate for each feature\nfor feature in ['Pclass', 'Sex', 'Embarked', 'SibSp', 'Parch']:\n    feature_survived = pd.crosstab(train_set[feature], train_set[\"Survived\"])\n    feature_survived_frac = feature_survived.apply(lambda r: r/r.sum(), axis=1)\n    print (\"Tables for {}\\n\\n{}\\n\\n{}\\n\" .format(feature, feature_survived, feature_survived_frac))\n    print (\"-----------------------------------\\n\")"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "# Longer X-Axis for age, fare\nf, (axis1,axis2,axis3,axis4) = plt.subplots(4,1, sharey=True, figsize=(20,15))\nsurvival_by_age = train_set[['Age', 'Survived']].groupby(['Age'],as_index=False).mean() \nsns.barplot(x='Age', y='Survived', data=survival_by_age, ax=axis1, palette=\"Blues_d\")\n\ntrain_set['age_group'] = train_set.apply(lambda r: int(r.Age/2.5), axis=1)\ntest_set['age_group']  = test_set.apply(lambda r: int(r.Age/2.5), axis=1)\nsurvival_by_agegroup = train_set[['age_group', 'Survived']].groupby(['age_group'],as_index=False).mean() \nsns.barplot(x='age_group', y='Survived', data=survival_by_agegroup, ax=axis2, palette=\"Blues_d\")\n\nsurvival_by_fare = train_set[['Fare', 'Survived']].groupby(['Fare'],as_index=False).mean() \nsns.barplot(x='Fare', y='Survived', data=survival_by_fare, ax=axis3, palette=\"Blues_d\")\n\ntrain_set['fare_group'] = train_set.apply(lambda r: int(r.Fare/6.0), axis=1)\ntest_set['fare_group']  = test_set.apply(lambda r: int(r.Fare/6.0), axis=1)\nsurvival_by_faregroup = train_set[['fare_group', 'Survived']].groupby(['fare_group'],as_index=False).mean() \nsns.barplot(x='fare_group', y='Survived', data=survival_by_faregroup, ax=axis4, palette=\"Blues_d\")"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "Notes about input features\n--------------------------  \n\n###Size of training data"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "# Features used for training\npredictors = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'age_group', 'fare_group']\n\n# Train / Test split for original training data\n# Withold 5% from train set for testing\nX_train, X_test, y_train, y_test = cross_validation.train_test_split(\n    train_set[predictors], train_set[\"Survived\"], test_size=0.05, random_state=0)\n\nprint (\"Original Training Set: {}\\nTraining Set: {}\\nTesting Set(witheld): {}\" .format(train_set.shape, X_train.shape,X_test.shape))\n\n\n# Normalize features - both training & test (withheld & final)\nscaler = StandardScaler().fit(X_train)\nX_train_transformed = scaler.transform(X_train)\nX_test_transformed = scaler.transform(X_test)\nfinal_test_transformed  = scaler.transform(test_set[predictors])\n\nprint (\"Transformed training, test sets (withheld & final)\")\n\n# Scoring Metric - Accuracy\nprint (\"Use accuracy as the score function\")"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "X_new = np.delete(X_train_transformed, [1,3], axis=1)\nX_new.shape\nX_train_transformed.shape"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "# Assess Feature importance\n# Initialize the algorithm\n# Defaults to mean accuracy as score\nfeature_labels = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\nX_train1 = np.delete(X_train_transformed, [7,8], axis=1)\n\nalg = RandomForestClassifier(random_state=1, n_estimators=10000, min_samples_split=50, min_samples_leaf=1)\nclf = alg.fit(X_train1, y_train)\n\nimportances = clf.feature_importances_\nindices = np.argsort(importances)[::-1]\nfor f in range(X_train1.shape[1]):\n    print(\"%2d) %-*s %f\" % (f + 1, 30, \n                             feature_labels[indices[f]], \n                             importances[indices[f]]))\n\nlabels_reordered = [ feature_labels[i] for i in indices]\n    \nplt.title('Feature Importances')\nplt.bar(range(X_train1.shape[1]), \n         importances[indices],\n         color='lightblue', \n         align='center')\nplt.xticks(range(X_train1.shape[1]), labels_reordered, rotation=90)\nplt.xlim([-1, X_train1.shape[1]])\nplt.tight_layout()\nplt.show()"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "# Assess Feature importance\n# Initialize the algorithm\n# Defaults to mean accuracy as score\nfeature_labels = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked', 'age_group', 'fare_group']\nX_train2 = np.delete(X_train_transformed, [2,5], axis=1)\n\nalg = RandomForestClassifier(random_state=1, n_estimators=10000, min_samples_split=50, min_samples_leaf=1)\nclf = alg.fit(X_train2, y_train)\n\nimportances = clf.feature_importances_\nindices = np.argsort(importances)[::-1]\nfor f in range(X_train2.shape[1]):\n    print(\"%2d) %-*s %f\" % (f + 1, 30, \n                             feature_labels[indices[f]], \n                             importances[indices[f]]))\n\nlabels_reordered = [ feature_labels[i] for i in indices]\n    \nplt.title('Feature Importances')\nplt.bar(range(X_train2.shape[1]), \n         importances[indices],\n         color='lightblue', \n         align='center')\nplt.xticks(range(X_train2.shape[1]), labels_reordered, rotation=90)\nplt.xlim([-1, X_train2.shape[1]])\nplt.tight_layout()\nplt.show()"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "# Use a simple model\n# Initialize the algorithm\n# Defaults to mean accuracy as score\nalg = RandomForestClassifier(random_state=1, n_estimators=200, min_samples_split=5, min_samples_leaf=3)\nclf = alg.fit(X_train_transformed, y_train)\n\n# Scores\ntrain_score = clf.score(X_train_transformed, y_train)\ntest_score  = clf.score(X_test_transformed, y_test)\nprint (\"Train Score: %0.3f\\nTest Score: %0.3f\" %(train_score, test_score))"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "# Use Cross Validation\nscores = cross_validation.cross_val_score(clf, X_train_transformed, y_train, cv=3)\nprint(\"Accuracy: %0.3f (+/- %0.3f)\" % (scores.mean(), scores.std() * 2))\n# Use GridSearchCV\n# Specify parameters\nparam_grid = {\"n_estimators\": [200, 300, 500],\n              \"max_depth\": [None],\n              \"max_features\": [5],\n              \"min_samples_split\": [9],\n              \"min_samples_leaf\": [6],\n              \"bootstrap\": [True],\n              \"criterion\": [\"gini\"]}\n             \nclf = RandomForestClassifier()\n\ngrid_search = GridSearchCV(clf, param_grid=param_grid)\ngrid_search.fit(X_train_transformed, y_train)\nprint (grid_search.best_estimator_) \n\n# Scores\ntrain_score = grid_search.score(X_train_transformed, y_train)\ntest_score  = grid_search.score(X_test_transformed, y_test)\nprint (\"Train Score: %0.3f\\nTest Score: %0.3f\" %(train_score, test_score))"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "# Use Random Forest with Best Parameters\nclf_final = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=None, max_features=5, max_leaf_nodes=None,\n            min_samples_leaf=6, min_samples_split=9,\n            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=1,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False)\nclf_final.fit(X_train_transformed, y_train)\n\n# Scores\ntrain_score = clf_final.score(X_train_transformed, y_train)\ntest_score  = clf_final.score(X_test_transformed, y_test)\nprint (\"Train Score: %0.3f\\nTest Score: %0.3f\" %(train_score, test_score))\n\n#CV\nscores = cross_validation.cross_val_score(clf_final, X_train_transformed, y_train, cv=3)\nprint(\"Accuracy: %0.3f (+/- %0.3f)\" % (scores.mean(), scores.std() * 2))"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "# Make Predictions using Test Set\npredictions = clf_final.predict(final_test_transformed)\n\n# Create a new dataframe with only the columns Kaggle wants from the dataset.\nsubmission = pd.DataFrame({\n        \"PassengerId\": test_set[\"PassengerId\"],\n        \"Survived\": predictions\n    })\nsubmission.to_csv('titanic_rf4.csv', index=False)\n\nsubmission.head(15)"
 }
],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}}, "nbformat": 4, "nbformat_minor": 0}