{"cells":[
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "# Load packages\nimport numpy as np  \nimport pandas as pd\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport re\n\nfrom sklearn import cross_validation\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\n\nprint (\"Read in packages from numpy, pandas, sklearn & matplotlib\")"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "# Load training data\ntrain_set = pd.read_csv(\"../input/train.csv\")\ntest_set  = pd.read_csv(\"../input/test.csv\")\nprint (\"Read in training, test data as Panda dataframes\")\n\n# Review input features - Part 1\nprint (\"\\n\\n---------------------\")\nprint (\"TRAIN SET INFORMATION\")\nprint (\"---------------------\")\nprint (\"Shape of training set:\", train_set.shape, \"\\n\")\nprint (\"Column Headers:\", list(train_set.columns.values), \"\\n\")\nprint (train_set.describe(), \"\\n\\n\")\nprint (train_set.dtypes)\n\nprint (\"\\n\\n--------------------\")\nprint (\"TEST SET INFORMATION\")\nprint (\"--------------------\")\nprint (\"Shape of test set:\", test_set.shape, \"\\n\")\nprint (\"Column Headers:\", list(test_set.columns.values), \"\\n\")\nprint (test_set.describe(), \"\\n\\n\")\nprint (test_set.dtypes)"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "\n\n\n# preview the data\ntrain_set.head()\n# Review input features (train set) - Part 2A\nmissing_values = []\nnonumeric_values = []\n\nprint (\"TRAINING SET INFORMATION\")\nprint (\"========================\\n\")\n\nfor column in train_set:\n    # Find all the unique feature values\n    uniq = train_set[column].unique()\n    print (\"'{}' has {} unique values\" .format(column,uniq.size))\n    if (uniq.size > 25):\n        print(\"~~Listing up to 25 unique values~~\")\n    print (uniq[0:24])\n    print (\"\\n-----------------------------------------------------------------------\\n\")\n    \n    # Find features with missing values\n    if (True in pd.isnull(uniq)):\n        s = \"{} has {} missing\" .format(column, pd.isnull(train_set[column]).sum())\n        missing_values.append(s)\n    \n    # Find features with non-numeric values\n    for i in range (1, np.prod(uniq.shape)):\n        if (re.match('nan', str(uniq[i]))):\n            break\n        if not (re.search('(^\\d+\\.?\\d*$)|(^\\d*\\.?\\d+$)', str(uniq[i]))):\n            nonumeric_values.append(column)\n            break\n  \nprint (\"\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\")\nprint (\"Features with missing values:\\n{}\\n\\n\" .format(missing_values))\nprint (\"Features with non-numeric values:\\n{}\" .format(nonumeric_values))\nprint (\"\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\")\n# Review input features (test set) - Part 2B\nmissing_values = []\nnonumeric_values = []\n\nprint (\"TEST SET INFORMATION\")\nprint (\"====================\\n\")\n\nfor column in test_set:\n    # Find all the unique feature values\n    uniq = test_set[column].unique()\n    print (\"'{}' has {} unique values\" .format(column,uniq.size))\n    if (uniq.size > 25):\n        print(\"~~Listing up to 25 unique values~~\")\n    print (uniq[0:24])\n    print (\"\\n-----------------------------------------------------------------------\\n\")\n    \n    # Find features with missing values\n    if (True in pd.isnull(uniq)):\n        s = \"{} has {} missing\" .format(column, pd.isnull(test_set[column]).sum())\n        missing_values.append(s)\n    \n    # Find features with non-numeric values\n    for i in range (1, np.prod(uniq.shape)):\n        if (re.match('nan', str(uniq[i]))):\n            break\n        if not (re.search('(^\\d+\\.?\\d*$)|(^\\d*\\.?\\d+$)', str(uniq[i]))):\n            nonumeric_values.append(column)\n            break\n  \nprint (\"\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\")\nprint (\"Features with missing values:\\n{}\\n\\n\" .format(missing_values))\nprint (\"Features with non-numeric values:\\n{}\" .format(nonumeric_values))\nprint (\"\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\")\n# Notes about input features\n# --------------------------\n# \n# ###Size of training data\n# Shape of dataframe: (891, 11+1) \n# \n# ###Size of test data\n# Shape of dataframe: (418, 11) \n# \n# ###Output Feature (1)\n# Survived (0 | 1)\n# \n# ###Input Features (11)  \n# PassengerId [1 2 3 ... ]      \n# Pclass      [1 2 3]  \n# Name        ['Braund, Mr. Owen Harris' 'Cumings, Mrs. John Bradley (Florence Briggs Thayer)' 'Heikkinen, Miss. Laina' ...]   \n# Sex         ['male' 'female']  \n# Age         [22. 38. 26. ...]   \n# SibSp       [0 1 2 3 4 5 8]  \n# Parch       [0 1 2 3 4 5 6 (9)]  \n# Ticket   ['A/5 21171' 'PC 17599' 'STON/O2. 3101282' ... ]  \n# Fare     [7.25  71.2833  7.925 ... ]  \n# Cabin    [nan 'C85' 'C123' 'E46' ... ]  \n# Embarked ['S' 'C' 'Q' nan]\n# \n# ###Features w/ missing values (3 train, 3 test)\n# Cabin (687, 327)  \n# Age (177, 86)  \n# Embarked (2, 0)  \n# Fare (0, 1)\n# \n# ###Features w/ non-numeric values (5)\n# Name  \n# Sex  \n# Ticket  \n# Cabin  \n# Embarked\n# Feature Cleaning\n# Convert non-numeric values for Sex, Embarked\n# male=0, female=1\ntrain_set.loc[train_set[\"Sex\"] == \"male\", \"Sex\"]   = 0\ntrain_set.loc[train_set[\"Sex\"] == \"female\", \"Sex\"] = 1\n\ntest_set.loc[test_set[\"Sex\"] == \"male\", \"Sex\"]   = 0\ntest_set.loc[test_set[\"Sex\"] == \"female\", \"Sex\"] = 1\n\n# Handle Parch=9 found only in test\n# replace by value 6 which is the closest available in training data\ntest_set.loc[test_set[\"Parch\"] == 9, \"Parch\"] = 6\n\n# S=0, C=1, Q=2\ntrain_set.loc[train_set[\"Embarked\"] == \"S\", \"Embarked\"] = 0\ntrain_set.loc[train_set[\"Embarked\"] == \"C\", \"Embarked\"] = 1\ntrain_set.loc[train_set[\"Embarked\"] == \"Q\", \"Embarked\"] = 2\n\ntest_set.loc[test_set[\"Embarked\"] == \"S\", \"Embarked\"] = 0\ntest_set.loc[test_set[\"Embarked\"] == \"C\", \"Embarked\"] = 1\ntest_set.loc[test_set[\"Embarked\"] == \"Q\", \"Embarked\"] = 2\n\n# Substitute missing values for Age, Embarked & Fare\ntrain_set[\"Age\"]      = train_set[\"Age\"].fillna(train_set[\"Age\"].median())\ntrain_set[\"Fare\"]     = train_set[\"Fare\"].fillna(train_set[\"Fare\"].median())\ntrain_set[\"Embarked\"] = train_set[\"Embarked\"].fillna(train_set[\"Embarked\"].median())\n\ntest_set[\"Age\"] = test_set[\"Age\"].fillna(test_set[\"Age\"].median())\ntest_set[\"Fare\"] = test_set[\"Fare\"].fillna(test_set[\"Fare\"].median())\n\nprint (\"Converted non-numeric features for Sex & Embarked...\\nSubstituted missing values for Age, Embarked & Fare\")\n# Pclass - Visualize the features and their impact on outcomes\nfeature_survived = pd.crosstab(train_set[\"Pclass\"], train_set[\"Survived\"])\nfeature_survived_frac = feature_survived.apply(lambda r: r/r.sum(), axis=1)\nprint (\"{}\\n\\n{}\\n\" .format(feature_survived, feature_survived_frac))\n    \nplt.figure()\nplt.bar([0,1,2], feature_survived_frac[1]+feature_survived_frac[0], color=\"lightsage\", label=\"Survived\")\nplt.bar([0,1,2], feature_survived_frac[0], color=\"lightskyblue\", label=\"Died\")\nplt.xticks([0.5, 1.5, 2.5], ['1st Class', '2nd Class', '3rd Class'], rotation='horizontal')\nplt.ylabel(\"Count\")\nplt.xlabel(\"\")\nplt.legend(loc=9, bbox_to_anchor=(0.5, -0.1), ncol=2)\nplt.show()\n# Sex - Visualize the features and their impact on outcomes\nfeature_survived = pd.crosstab(train_set[\"Sex\"], train_set[\"Survived\"])\nfeature_survived_frac = feature_survived.apply(lambda r: r/r.sum(), axis=1)\nprint (\"{}\\n\\n{}\\n\" .format(feature_survived, feature_survived_frac))\n    \nplt.figure()\nplt.bar([0,1], feature_survived_frac[1]+feature_survived_frac[0], color=\"lightsage\", label=\"Survived\")\nplt.bar([0,1], feature_survived_frac[0], color=\"lightskyblue\", label=\"Died\")\nplt.xticks([0.5, 1.5], ['Male', 'Female'], rotation='horizontal')\nplt.ylabel(\"Count\")\nplt.xlabel(\"\")\nplt.legend(loc=9, bbox_to_anchor=(0.5, -0.1), ncol=2)\nplt.show()\n# Embarked - Visualize the features and their impact on outcomes\nfeature_survived = pd.crosstab(train_set[\"Embarked\"], train_set[\"Survived\"])\nfeature_survived_frac = feature_survived.apply(lambda r: r/r.sum(), axis=1)\nprint (\"{}\\n\\n{}\\n\" .format(feature_survived, feature_survived_frac))\n    \nplt.figure()\nplt.bar([0,1,2], feature_survived_frac[1]+feature_survived_frac[0], color=\"lightsage\", label=\"Survived\")\nplt.bar([0,1,2], feature_survived_frac[0], color=\"lightskyblue\", label=\"Died\")\nplt.xticks([0.5, 1.5, 2.5], ['S', 'C', 'Q'], rotation='horizontal')\nplt.ylabel(\"Count\")\nplt.xlabel(\"\")\nplt.legend(loc=9, bbox_to_anchor=(0.5, -0.1), ncol=2)\nplt.show()\n# Features used for training\npredictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n\n# Train / Test split for original training data\n# Withold 5% from train set for testing\n\n\nX_train, X_test, y_train, y_test = cross_validation.train_test_split(\n    train_set[predictors], train_set[\"Survived\"], test_size=0.05, random_state=0)\n\nprint (\"Original Training Set: {}\\nTraining Set: {}\\nTesting Set(witheld): {}\" .format(train_set.shape, X_train.shape,X_test.shape))\n\n\n# Normalize features - both training & test (withheld & final)\n\n\nscaler = StandardScaler().fit(X_train)\nX_train_transformed = scaler.transform(X_train)\nX_test_transformed = scaler.transform(X_test)\nfinal_test_transformed  = scaler.transform(test_set[predictors])\n\nprint (\"Transformed training, test sets (withheld & final)\")\n\n# Scoring Metric - Accuracy\nprint (\"Use accuracy as the score function\")\n# Assess Feature importance\n# Initialize the algorithm\n# Defaults to mean accuracy as score\nalg = RandomForestClassifier(random_state=1, n_estimators=10000, min_samples_split=50, min_samples_leaf=1)\nclf = alg.fit(X_train_transformed, y_train)\n\nfeature_labels = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\nimportances = clf.feature_importances_\nindices = np.argsort(importances)[::-1]\nfor f in range(X_train_transformed.shape[1]):\n    print(\"%2d) %-*s %f\" % (f + 1, 30, \n                             feature_labels[indices[f]], \n                             importances[indices[f]]))\n\nlabels_reordered = [ feature_labels[i] for i in indices]\n    \nplt.title('Feature Importances')\nplt.bar(range(X_train_transformed.shape[1]), \n         importances[indices],\n         color='lightblue', \n         align='center')\nplt.xticks(range(X_train_transformed.shape[1]), labels_reordered, rotation=90)\nplt.xlim([-1, X_train_transformed.shape[1]])\nplt.tight_layout()\nplt.show()\n# Use a simple model\n# Initialize the algorithm\n# Defaults to mean accuracy as score\nalg = RandomForestClassifier(random_state=1, n_estimators=200, min_samples_split=5, min_samples_leaf=3)\nclf = alg.fit(X_train_transformed, y_train)\n\n# Scores\ntrain_score = clf.score(X_train_transformed, y_train)\ntest_score  = clf.score(X_test_transformed, y_test)\nprint (\"Train Score: {}\\nTest Score: {}\" .format(train_score, test_score))\n\n# Use Cross Validation\nscores = cross_validation.cross_val_score(clf, X_train_transformed, y_train, cv=3)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n# Use GridSearchCV\n# Specify parameters\nparam_grid = {\"n_estimators\": [200, 300, 500],\n              \"max_depth\": [None],\n              \"max_features\": [5],\n              \"min_samples_split\": [9],\n              \"min_samples_leaf\": [6],\n              \"bootstrap\": [True],\n              \"criterion\": [\"gini\"]}\n             \nclf = RandomForestClassifier()\n\ngrid_search = GridSearchCV(clf, param_grid=param_grid)\ngrid_search.fit(X_train_transformed, y_train)\nprint (grid_search.best_estimator_) \n\n# Scores\ntrain_score = grid_search.score(X_train_transformed, y_train)\ntest_score  = grid_search.score(X_test_transformed, y_test)\nprint (\"Train Score: {}\\nTest Score: {}\" .format(train_score, test_score))\n# Use Random Forest with Best Parameters\nclf_final = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=None, max_features=5, max_leaf_nodes=None,\n            min_samples_leaf=6, min_samples_split=9,\n            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=1,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False)\nclf_final.fit(X_train_transformed, y_train)\n\n# Scores\ntrain_score = clf_final.score(X_train_transformed, y_train)\ntest_score  = clf_final.score(X_test_transformed, y_test)\nprint (\"Train Score: {}\\nTest Score: {}\" .format(train_score, test_score))\n\n#CV\nscores = cross_validation.cross_val_score(clf_final, X_train_transformed, y_train, cv=3)\nprint(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n# Make Predictions using Test Set\n\n# Make predictions using the test set.\npredictions = clf_final.predict(final_test_transformed)\n\n# Create a new dataframe with only the columns Kaggle wants from the dataset.\nsubmission = pd.DataFrame({\n        \"PassengerId\": test_set[\"PassengerId\"],\n        \"Survived\": predictions\n    })\nsubmission.to_csv('titanic_rf4.csv', index=False)\n\nsubmission.head(15)\n"
 }
],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}}, "nbformat": 4, "nbformat_minor": 0}