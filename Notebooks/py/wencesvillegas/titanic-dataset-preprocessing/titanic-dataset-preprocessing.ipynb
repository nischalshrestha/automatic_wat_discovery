{"cells":[{"metadata":{"_uuid":"2bf7f8707d9d5cc0d01a690cb8722015bd0fd2ea"},"cell_type":"markdown","source":"# Titanic Dataset Preprocessing\n"},{"metadata":{"_uuid":"c10e11983f7015226cec51fd23976508d8cf2bca"},"cell_type":"markdown","source":"First of all let's import basic libraries to load, edit and visualize the dataset"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"d1496cd0a30a05908406aaa562f16f1b1177c325"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n","execution_count":43,"outputs":[]},{"metadata":{"_uuid":"c7d23df98ac0d40c7f0428082e22dd5fe0292e5d"},"cell_type":"markdown","source":"Now lets load the csv file for the training and test set."},{"metadata":{"trusted":false,"_uuid":"20e6b4042e3ad5bc9ce40fd0994c20055cc8429b"},"cell_type":"code","source":"training_set = pd.read_csv('../input/train.csv')\ntest_set = pd.read_csv('../input/test.csv')\ntraining_set.head()","execution_count":44,"outputs":[]},{"metadata":{"_uuid":"68479016bc23698ec20e32d14838a822b7250b59"},"cell_type":"markdown","source":"Here we can see that we have many categorical features and some numeric ones too. Before turning this dataset into vectors of numbers that our classification algorithms can use, we should deal with missing values.\nLet's check how many missing values has our dataset per feature."},{"metadata":{"trusted":false,"_uuid":"be221d0ed5af24c37e7992d7de2547434ae48309"},"cell_type":"code","source":"training_set.isnull().sum()","execution_count":45,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d98c2ba69e942d2e203b619e278292d0281a11f2"},"cell_type":"code","source":"test_set.isnull().sum()","execution_count":46,"outputs":[]},{"metadata":{"_uuid":"0c5c6feee89bf5af038a3cbefa8ff84542c0d2ec"},"cell_type":"markdown","source":"We could delete the training samples that have NaN values but in this case we dont have a huge dataset.\nFirst we are going to transform the Cabin feature into a Deck feature, each cabin starts with a letter that denotes the deck and we dont really need more information than that."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"605249223527af75d7d3c9841713f9841199a407"},"cell_type":"code","source":"# make a list of all the posible Decks, the last element is used when no cabin code is present\ncabin_list = ['A', 'B', 'C', 'D', 'E', 'F', 'T', 'G', 'Unknown']\n#define a function that replaces the cabin code with the deck character\ndef search_substring(big_string, substring_list):\n    for substring in substring_list:\n        if substring in big_string:\n            return substring\n    return substring_list[-1]","execution_count":47,"outputs":[]},{"metadata":{"_uuid":"d0e68c0b75424917a42390d514e8f52f07ca5e54"},"cell_type":"markdown","source":"We have a similar problem with the Name feature, we have too much information that is hard to encode and nt useful. So we can take only the title of the name for each person, lets define a function for that."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"2c0627778ebb18396a1a4abce196ea80041412b7"},"cell_type":"code","source":"# replace passenger's name with his/her title (Mr, Mrs, Miss, Master)\ndef get_title(string):\n    import re\n    regex = re.compile(r'Mr|Don|Major|Capt|Jonkheer|Rev|Col|Dr|Mrs|Countess|Dona|Mme|Ms|Miss|Mlle|Master', re.IGNORECASE)\n    results = regex.search(string)\n    if results != None:\n        return(results.group().lower())\n    else:\n        return(str(np.nan))","execution_count":48,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"6043cacfdfc8ad30288c26c208db88f0cf71deb9"},"cell_type":"code","source":"# dictionary to map to generate the new feature vector\ntitle_dictionary = {\n    \"capt\":\"Officer\", \n    \"col\":\"Officer\", \n    \"major\":\"Officer\", \n    \"dr\":\"Officer\",\n    \"jonkheer\":\"Royalty\",\n    \"rev\":\"Officer\",\n    \"countess\":\"Royalty\",\n    \"dona\":\"Royalty\",\n    \"lady\":\"Royalty\",\n    \"don\":\"Royalty\",\n    \"mr\":\"Mr\",\n    \"mme\":\"Mrs\",\n    \"ms\":\"Mrs\",\n    \"mrs\":\"Mrs\",\n    \"miss\":\"Miss\",\n    \"mlle\":\"Miss\",\n    \"master\":\"Master\",\n    \"nan\":\"Mr\"\n}","execution_count":49,"outputs":[]},{"metadata":{"_uuid":"688b70a102ec52663386944e46107b6e2c0fcf2d"},"cell_type":"markdown","source":"Now that we have the functions we need, lets apply them and create the features Title and Deck"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"40a678de5659fe017b7d6dcb9acc435a36c6ecec"},"cell_type":"code","source":"training_set['Deck'] = training_set['Cabin'].map(lambda x: search_substring(str(x), cabin_list))\ntest_set['Deck'] = test_set['Cabin'].map(lambda x: search_substring(str(x), cabin_list))\n# delete the Cabin feature\ntraining_set.drop('Cabin', 1, inplace=True)\ntest_set.drop('Cabin', 1, inplace=True)\n\ntraining_set['Title'] = training_set['Name'].apply(get_title)\ntest_set['Title'] = test_set['Name'].apply(get_title)\ntraining_set['Title'] = training_set['Title'].map(title_dictionary)\ntest_set['Title'] = test_set['Title'].map(title_dictionary)\n# delete the Name feature\ntraining_set.drop('Name', 1, inplace=True)\ntest_set.drop('Name', 1, inplace=True)","execution_count":50,"outputs":[]},{"metadata":{"_uuid":"b6b5d875a569ee7289929715ace7fb88e134da7d"},"cell_type":"markdown","source":"Let's take a look at the results we got"},{"metadata":{"trusted":false,"_uuid":"b135a71654806d50072bd5f7f5dd9fc6b3dd1606"},"cell_type":"code","source":"training_set.tail()","execution_count":51,"outputs":[]},{"metadata":{"_uuid":"d628c44ecb4e1adf3fad6de650cbace6fabfc08a"},"cell_type":"markdown","source":"Now we will drop the Ticket feature that does not really give much insight."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"378055e8e89f76f4f092183d3fe67bc6cdb676a9"},"cell_type":"code","source":"#dropping ticket column\ntraining_set.drop('Ticket', 1, inplace=True)\ntest_set.drop('Ticket', 1, inplace=True)","execution_count":52,"outputs":[]},{"metadata":{"_uuid":"79bea12a90f271d88e6203a511df41f5314d2a7a"},"cell_type":"markdown","source":"We have to do something about the NaN values in the Age column. We can replace them with the mean of the age, but that would mean that some kid (Master or Miss) would appear to be older than they are. So we will take the mean of the age from each Title, and then replace each NaN value with the mean of the age of the corresponding persons title."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"3f621a338925e11dd1783abf77ac22e370cfe4f7"},"cell_type":"code","source":"means_title = training_set.groupby('Title')['Age'].mean()","execution_count":53,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"77bab27eca7556ebf865406b18e72b9452e99ce6"},"cell_type":"code","source":"title_list = ['Mr','Miss','Mrs','Master', 'Royalty', 'Officer']\ndef age_nan_replace(means, dframe, title_list):\n    for title in title_list:\n        temp = dframe['Title'] == title #extract indices of samples with same title\n        dframe.loc[temp, 'Age'] = dframe.loc[temp, 'Age'].fillna(means[title]) # replace nan values for mean\n        \n\nage_nan_replace(means_title, training_set, title_list)\nage_nan_replace(means_title, test_set, title_list)","execution_count":54,"outputs":[]},{"metadata":{"_uuid":"4b9cad87c9b01500e8eb7aae1f6459a6b01cca01"},"cell_type":"markdown","source":"Now lets fill those two NaN cases in the Embarked column."},{"metadata":{"trusted":false,"_uuid":"7043264f2536b28b8741d80aef353c0fcc3f84b1"},"cell_type":"code","source":"training_set.groupby('Embarked').size().plot(kind='bar')","execution_count":55,"outputs":[]},{"metadata":{"_uuid":"9d7e92eb4ec6d48110eb716c99114b4088750771"},"cell_type":"markdown","source":"We will assgn them the letter S which is the most common case."},{"metadata":{"collapsed":true,"scrolled":false,"trusted":false,"_uuid":"dac68f0c923fa2b519812aa259baf994100c8b9b"},"cell_type":"code","source":"training_set['Embarked'].fillna('S', inplace=True)\ntest_set['Embarked'].fillna('S', inplace=True)\n#fill the fare column in the test set\ntest_set['Fare'].fillna(test_set['Fare'].mean(), inplace=True)","execution_count":56,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"beff48e7758eb9d1519b4459b67803c244e6b0aa"},"cell_type":"code","source":"training_set.head()","execution_count":57,"outputs":[]},{"metadata":{"_uuid":"871db334223f933404cd4534dad81cc67d0dcca0"},"cell_type":"markdown","source":"Let's visualize some aspects of the data so that we can understand what are the most important factors that determined survival."},{"metadata":{"trusted":false,"_uuid":"0ed0a03142b443564f9206360642ccf7497d3eab"},"cell_type":"code","source":"index = training_set['Survived'].unique() # get the number of bars\ngrouped_data = training_set.groupby(['Survived', 'Sex']) \ntemp = grouped_data.size().unstack() \nwomen_stats = (temp.iat[0,0], temp.iat[1,0])\nmen_stats = (temp.iat[0,1], temp.iat[1,1])\np1 = plt.bar(index, women_stats)\np2 = plt.bar(index, men_stats, bottom=women_stats)\nplt.xticks(index, ('No', 'Yes'))\nplt.ylabel('Number of People')\nplt.xlabel('Survival')\nplt.title('Survival of passengers')\nplt.legend((p1[0], p2[0]), ('Women', 'Men'))\nplt.tight_layout()\n","execution_count":58,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"0cceab105cad3d980abbac5960cd4d2babfd77aa"},"cell_type":"code","source":"training_set.pivot_table('Survived',index='Sex',columns='Pclass').plot(kind='bar')\n","execution_count":59,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d3a4aef018f5c9c458f95d64faec955bdcee598a"},"cell_type":"code","source":"training_set.pivot_table('Survived', index='Title', columns='Pclass').plot(kind='bar')","execution_count":60,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"17d3875f7128470b3bda56ffc0138ab9462a8a77"},"cell_type":"code","source":"age_intervals = pd.qcut(training_set['Age'], 3)\ntraining_set.pivot_table('Survived', ['Sex', age_intervals], 'Pclass').plot(kind='bar')","execution_count":61,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f670a9ab6628943a8b654f8d92dd3878f4a43784"},"cell_type":"code","source":"parch_intervals = pd.cut(training_set['Parch'], [0,1,2,3])\nsibsp_intervals = pd.cut(training_set['SibSp'], [0,1,2,3])\ntraining_set.pivot_table('Survived', parch_intervals, 'Sex').plot(kind='bar')","execution_count":62,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"68a5c520ff9c3a4babde641c3ff4a733e2145ec9"},"cell_type":"code","source":"training_set.pivot_table('Survived', sibsp_intervals, 'Sex').plot(kind='bar')","execution_count":63,"outputs":[]},{"metadata":{"_uuid":"c8c0c04e0139cd46802651409fbd3307e8f6117f"},"cell_type":"markdown","source":"What we can take from this analysis is that Passenger Class was relevant to survive, and that the features SibSp and Parch behave similarly. We can make a new feature called Family Size that is the sum of those 2 feature columns. "},{"metadata":{"trusted":false,"_uuid":"85d0ab37bf41275d0a86acfa8fb191ca4ebf879d"},"cell_type":"code","source":"training_set['Family Size'] = training_set['Parch'] + training_set['SibSp']\ntest_set['Family Size'] = test_set['Parch'] + test_set['SibSp']\ntraining_set.drop('Parch', axis=1, inplace=True)\ntraining_set.drop('SibSp', axis=1, inplace=True)\ntest_set.drop('Parch', axis=1, inplace=True)\ntest_set.drop('SibSp', axis=1, inplace=True)\ntraining_set.head()","execution_count":64,"outputs":[]},{"metadata":{"_uuid":"dd8c07626edbc55968193c5b2b54dc27bd10d473"},"cell_type":"markdown","source":"Let's standardize the numerical features Age and Fare"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"46e0692e25a02cf5b191ad08b5091edfcdd3d34c"},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nnumericals_list = ['Age','Fare']\nfor column in numericals_list:\n    sc = StandardScaler(with_mean=True, with_std=True)\n#    print(training_set[column].size)\n#    print(test_set[column].size)\n    sc.fit(training_set[column].values.reshape(-1,1))\n    training_set[column] = sc.transform(training_set[column].values.reshape(-1,1))\n    test_set[column] = sc.transform(test_set[column].values.reshape(-1,1))\n","execution_count":67,"outputs":[]},{"metadata":{"_uuid":"13eefb38289c341e5f6d6a1563001ede1363c70a"},"cell_type":"markdown","source":"Now let's encode categorical classes with sklearn's LabelEncoder"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"7bc72d4e2e358e3743c0cb9e5096272bde0a5a48"},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\ncategorical_classes_list = ['Sex','Embarked','Deck', 'Title'] #Pclass is already encoded\n#encode features that are cateorical classes\nencoding_list = []\nfor column in categorical_classes_list:\n    le = LabelEncoder()\n    le.fit(training_set[column])\n    encoding_list.append(training_set[column].unique())\n    encoding_list.append(list(le.transform(training_set[column].unique())))\n    training_set[column] = le.transform(training_set[column])\n    test_set[column] = le.transform(test_set[column])","execution_count":68,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d9228f178ad284affc72a674562a04ca993b456e"},"cell_type":"code","source":"# lets see the results\ntraining_set.head()","execution_count":69,"outputs":[]},{"metadata":{"_uuid":"c059700b629d3914dcd6e9a613009f996559a63c"},"cell_type":"markdown","source":"Now we are going to onehot encode categorical features such as Embarked, Title and Pclass"},{"metadata":{"trusted":false,"_uuid":"650d53ef093c23ab4dfaee6685aea0b0bb1e40ec"},"cell_type":"code","source":"training_set = pd.get_dummies(training_set, columns=['Embarked','Pclass','Title', 'Deck'])\ntest_set = pd.get_dummies(test_set, columns=['Embarked','Pclass','Title', 'Deck'])\n","execution_count":70,"outputs":[]},{"metadata":{"_uuid":"3facefe5bd78e3e4f999ec78760242b66a061486"},"cell_type":"markdown","source":"The test set lacks a sample where deck 7 is selected so we will have to align the dataframes to fill that column. "},{"metadata":{"trusted":false,"_uuid":"da361f57d407f9367670dd805db1a2eb2d1f4715"},"cell_type":"code","source":"training_set, test_set = training_set.align(test_set, axis=1)\ntest_set.drop('Survived', axis=1, inplace=True)\ntest_set.fillna(0, axis=1, inplace=True)","execution_count":73,"outputs":[]},{"metadata":{"_uuid":"73ffb3ead3377df35734afe02f5c866f31b63371"},"cell_type":"markdown","source":"Now that we have our dataset clean and ready, we need to transform it into a numpy matrix that a learning algorithm can use.\nWe will get a Y vector containing the labels for training, a matrix X that has all the features for training, and X_test that has all the samples for training from the test set."},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"c2e8aa7f2498bf4bfda11425217407bf693a59bf"},"cell_type":"code","source":"#test_set.fillna(0, inplace=True)\ny = training_set['Survived'].values\nX = training_set.drop(['Survived','PassengerId'], axis=1).values\nX_test = test_set.drop('PassengerId', axis=1).values","execution_count":74,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"84a5d8cc1909c23c183a7f160a59da50972a46f3"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}