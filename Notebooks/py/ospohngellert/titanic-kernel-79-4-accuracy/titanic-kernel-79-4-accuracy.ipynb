{"metadata": {"language_info": {"file_extension": ".py", "version": "3.6.3", "pygments_lexer": "ipython3", "name": "python", "mimetype": "text/x-python", "nbconvert_exporter": "python", "codemirror_mode": {"name": "ipython", "version": 3}}, "kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}}, "nbformat_minor": 1, "cells": [{"metadata": {"_uuid": "1b689a8865f7f781ec1d8283eb02c276a9709da5", "_cell_guid": "0e88f34f-46b7-4aa2-af55-ecfb76cfaabb"}, "cell_type": "markdown", "source": ["# Titanic Survival Solution\n", "\n", "## Table of Contents\n", "\n", "* [Abstract](#Abstract)\n", "* [Data Analysis](#Data-Analysis)\n", "    * [Initial Impressions](#Initial-Impressions)\n", "    * [Missing Ages](#Missing-Ages)\n", "        * [Conclusion on Age](#Conclusion-on-Age)\n", "    * [Fare](#Fare)\n", "* [Data Cleansing](#Data-Cleansing)\n", "    * [Initial Cleansing](#Initial-Cleansing)\n", "    * [Assessing Correlation](#Assessing-Correlation)\n", "    * [Feature Extraction](#Feature-Extraction)\n", "    * [Dealing with Colinearity](#Dealing-with-Colinearity)\n", "* [Running the Algorithms](#Running-the-Algorithms)\n", "    * [Algorithms](#Algorithms)\n", "    * [Testing Without Colinearity](#Testing-Without-Colinearity)\n", "    * [Testing With Colinearity](#Testing-With-Colinearity)\n", "    * [Dealing with Overfitting](#Dealing-with-Overfitting)\n", "    * [Final Modeling](#Final-Modeling)\n", "* [Conclusion and Final Thoughts](#Conclusion-and-Final-Thoughts)\n", "\n", "\n", "## Abstract\n", "\n", "The purpose of this notebook is to provide a solution to the titanic problem on kaggle. In order to do this, the data was imported, and then trends were analyzed in an attempt to extract the useful features. The most notable part of the data cleansing was using number of siblings, name, and number of parents to more accurately impute missing age values. Through this, 79.4% accuracy was achieved."]}, {"metadata": {"_uuid": "2c22ced80d98ec15bfc2ef7230cef1fc2ef3b0c6", "_cell_guid": "4372c960-484f-4bd3-8d81-315cbeee6613", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["import pandas as pd\n", "import random\n", "from sklearn.neural_network import MLPClassifier\n", "import numpy\n", "from sklearn.model_selection import train_test_split\n", "from sklearn import preprocessing\n", "import re\n", "import matplotlib.pyplot as plt\n", "import matplotlib.patches as patches\n", "import matplotlib.path as path\n", "import scipy.stats as stats\n", "import math\n", "from sklearn.ensemble import RandomForestClassifier\n", "from sklearn import svm"], "outputs": []}, {"metadata": {"_uuid": "55fc33db299b3a629c61fa4b7290260a5504d826", "_cell_guid": "5bac2dcd-e35a-4db1-b323-e8029a738ea1"}, "cell_type": "markdown", "source": ["Below we import train and test data into two separate data frames."]}, {"metadata": {"_uuid": "3e543d0632e7c76a31ef862871fbaa61fba8a7b2", "_cell_guid": "27c11993-3b24-4027-aca4-6cf86c748d75", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["train = pd.read_csv(\"../input/train.csv\")\n", "test = pd.read_csv(\"../input/test.csv\")"], "outputs": []}, {"metadata": {"_uuid": "460bc789797261f2c3da9efe411be7e0d9008352", "_cell_guid": "f96c6e90-cfc2-44b4-9f15-9d8fb8ac8198"}, "cell_type": "markdown", "source": ["## Data Analysis"]}, {"metadata": {"_uuid": "298d135d952d07719e6bbc43711f33b9d38833e4", "_cell_guid": "86c12919-fc19-45dd-bc13-3f17e9262034"}, "cell_type": "markdown", "source": ["First, let's take a look at a general summary of the data."]}, {"metadata": {"_uuid": "a23bf41ece2522239ac015da8ce0edb675861113", "_cell_guid": "e7a836d3-c7cf-4400-97a6-e58180739d04", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["train.describe()"], "outputs": []}, {"metadata": {"_uuid": "702552e5195d64e15c02c301efc505c5dd7837d2", "_cell_guid": "dd2fd350-b493-4fd3-988d-c3330d6f700b"}, "cell_type": "markdown", "source": ["### Initial Impressions\n", "\n", "* Age has missing values (714 is its count, meaning there are some na)\n", "* Fare has some values that are skewing the mean, as the mean is greater than the 75% value.\n", "* Only 38% of people in the data actually survived\n", "\n", "### Missing Ages\n", "The first thing we will look at is the missing ages. How does the age data look, and what should be our strategies for imputing this? First let's find what proportion of the ages are missing."]}, {"metadata": {"_uuid": "9c2dc4446792d5ac3d16124a3f388ab75400e65c", "_cell_guid": "c6df0ce2-c356-4180-a12d-792793e83793", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["sum(pd.isnull(train.Age))/len(train)"], "outputs": []}, {"metadata": {"_uuid": "fe72bab77ff1c976cfaa53ba97fead7ae53a3e95", "_cell_guid": "3058f9ff-a35c-4d56-ae69-565ef99b124a"}, "cell_type": "markdown", "source": ["This is a large chunk of the data, so we would like to impute these ages as accurately as possible. Let's analyze the ages we have to see if we can find significant trends to impute our unknown ages."]}, {"metadata": {"_uuid": "88658865b7ab875c3084eab38ab7bc82690b20a9", "_cell_guid": "3b6156c5-54e3-434f-b74d-c8054951cd49", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["ageNoNa = train.Age[pd.isnull(train.Age) == False]\n", "plt.hist(ageNoNa)\n", "plt.xlabel(\"Age\")\n", "plt.ylabel(\"Frequency\")\n", "plt.title(\"Age distribution for known training data.\")\n", "plt.show()\n", "print(\"Mean age: \" + str(numpy.mean(ageNoNa)))\n", "print(\"Median age: \" + str(numpy.median(ageNoNa)))\n", "print(\"Standard debiation: \" + str(numpy.std(ageNoNa)))\n", "print(\"Normality: \" + str(stats.normaltest(ageNoNa)))"], "outputs": []}, {"metadata": {"_uuid": "6865fe24036c115e67ed9ad2c196f64bf30e8495", "_cell_guid": "4edc496d-ccdc-4be6-82cd-c964a772a943"}, "cell_type": "markdown", "source": ["The data here is fairly normal, but the mean isn't exactly desirable as a fill in, as it does not encompass the range of the data. Next let's look into which other factors in the data can be used to make an educated guess on the missing ages. The first thing we will look at is using the number of siblings someone has, and the number of parents/children someone has."]}, {"metadata": {"_uuid": "2211af6875475e9e0c4a5658a70c95d8c29a43ee", "_cell_guid": "6a5103c9-57de-431c-a81f-bbd9b1b49749", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["ageHasSibs = train[(train.SibSp > 0) & (pd.isnull(train.Age) == False)].Age\n", "plt.hist(ageHasSibs)\n", "plt.xlabel(\"Age\")\n", "plt.ylabel(\"Frequency\")\n", "plt.title(\"Age distribution for people who have siblings.\")\n", "plt.show()\n", "print(\"Mean age: \" + str(numpy.mean(ageHasSibs)))\n", "print(\"Median age: \" + str(numpy.median(ageHasSibs)))"], "outputs": []}, {"metadata": {"_uuid": "6782a60b320b0be27a4dd4c184648be6f867948e", "_cell_guid": "47068d80-4df0-4fe7-9353-fe01776c2675", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["ageHasNoSibs = train[(train.SibSp == 0) & (pd.isnull(train.Age) == False)].Age\n", "plt.hist(ageHasNoSibs)\n", "plt.xlabel(\"Age\")\n", "plt.ylabel(\"Frequency\")\n", "plt.title(\"Age distribution for people who don't have siblings.\")\n", "plt.show()\n", "print(\"Mean age: \" + str(numpy.mean(ageHasNoSibs)))\n", "print(\"Median age: \" + str(numpy.median(ageHasNoSibs)))"], "outputs": []}, {"metadata": {"_uuid": "94b6151fff00fb5dd4493855c2b93442ecb8c7e9", "_cell_guid": "b1eacce6-20e8-4b4b-a476-5a296cbd9b86"}, "cell_type": "markdown", "source": ["Though the average age for people who have siblings was lower than for those who don't, it is not a significant enough difference to be noticeable. Though it is less likely to be indicative, let's look at people who have more than one parent/child, versus those who don't."]}, {"metadata": {"_uuid": "cb1aad87d8391dda4c443d8f478578b03a6f5e89", "_cell_guid": "b8d49cc4-2ba9-4146-82c9-27230821bd96", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["ageHasParch = train[(train.Parch > 0) & (pd.isnull(train.Age) == False)].Age\n", "plt.hist(ageHasParch)\n", "plt.xlabel(\"Age\")\n", "plt.ylabel(\"Frequency\")\n", "plt.title(\"Age distribution for people who have parents/children.\")\n", "plt.show()\n", "print(\"Mean age: \" + str(numpy.mean(ageHasParch)))\n", "print(\"Median age: \" + str(numpy.median(ageHasParch)))"], "outputs": []}, {"metadata": {"_uuid": "2ba73beb99b02c10c6f274996d403d7887f7fbab", "_cell_guid": "6ef97469-052a-4085-8584-5427a94cf8e8", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["ageHasNoParch = train[(train.Parch == 0) & (pd.isnull(train.Age) == False)].Age\n", "plt.hist(ageHasNoParch)\n", "plt.xlabel(\"Age\")\n", "plt.ylabel(\"Frequency\")\n", "plt.title(\"Age distribution for people who don't have siblings.\")\n", "plt.show()\n", "print(\"Mean age: \" + str(numpy.mean(ageHasNoParch)))\n", "print(\"Median age: \" + str(numpy.median(ageHasNoParch)))"], "outputs": []}, {"metadata": {"_uuid": "fb5d6c2c58a8b2a5260a25e65f682ba4855e5717", "_cell_guid": "750c6e4d-0ee0-4c89-b651-be808444ed64"}, "cell_type": "markdown", "source": ["Surprisingly, this is a much better indicator. However, because of the large distribution of ages of people with Parch, simply using Parch may not be the best indicator. One thing we can look into is using the names, specifically their title. Let's look at the ages of people who have the word \"Master\", \"Miss\", \"Mr.\", and \"Mrs.\" in their names."]}, {"metadata": {"_uuid": "408bdc35a70b48b281bd33a6556edb141faa38d5", "_cell_guid": "4218d62b-8f8b-4697-b4f8-3e40bf63d312", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["ageOfMaster = train[train.Name.str.contains('Master.') & (pd.isnull(train.Age) == False)].Age\n", "plt.hist(ageOfMaster)\n", "plt.xlabel(\"Age\")\n", "plt.ylabel(\"Frequency\")\n", "plt.title(\"Age distribution for people who have the title Master.\")\n", "plt.show()\n", "print(\"Mean age: \" + str(numpy.mean(ageOfMaster)))\n", "print(\"Median age: \" + str(numpy.median(ageOfMaster)))"], "outputs": []}, {"metadata": {"_uuid": "30aafc46e888f9598f2ebc06392bab58c3ed41bc", "_cell_guid": "4632a69c-0d19-4ac8-ade3-99874f18e452", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["ageOfMiss = train[train.Name.str.contains('Miss.') & (pd.isnull(train.Age) == False)].Age\n", "plt.hist(ageOfMiss)\n", "plt.xlabel(\"Age\")\n", "plt.ylabel(\"Frequency\")\n", "plt.title(\"Age distribution for people who have the title Miss.\")\n", "plt.show()\n", "print(\"Mean age: \" + str(numpy.mean(ageOfMiss)))\n", "print(\"Median age: \" + str(numpy.median(ageOfMiss)))"], "outputs": []}, {"metadata": {"_uuid": "e454a6bc862f899c002bf6f6fc22f99f588f7cb8", "_cell_guid": "95978a1a-0739-453d-96b1-305c13b9f448", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["ageOfMr = train[train.Name.str.contains('Mr.') & (pd.isnull(train.Age) == False)].Age\n", "plt.hist(ageOfMr)\n", "plt.xlabel(\"Age\")\n", "plt.ylabel(\"Frequency\")\n", "plt.title(\"Age distribution for people who have the title Mr.\")\n", "plt.show()\n", "print(\"Mean age: \" + str(numpy.mean(ageOfMr)))\n", "print(\"Median age: \" + str(numpy.median(ageOfMr)))"], "outputs": []}, {"metadata": {"_uuid": "853fb167e0e20f9ec649ca26101ead4b4936b55d", "_cell_guid": "59533f3e-4f31-46a6-9c99-b39628e11b62", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["ageOfMrs = train[train.Name.str.contains('Mrs.') & (pd.isnull(train.Age) == False)].Age\n", "plt.hist(ageOfMr)\n", "plt.xlabel(\"Age\")\n", "plt.ylabel(\"Frequency\")\n", "plt.title(\"Age distribution for people who have the title Mrs.\")\n", "plt.show()\n", "print(\"Mean age: \" + str(numpy.mean(ageOfMrs)))\n", "print(\"Median age: \" + str(numpy.median(ageOfMrs)))"], "outputs": []}, {"metadata": {"_uuid": "84ce8b68c4ece350ea290734601637c9edd4c621", "_cell_guid": "03d2b193-c93d-4667-a721-b36167ea52bf"}, "cell_type": "markdown", "source": ["There is a clear difference between people who have the title \"Miss\" or \"Master\", and those who have the titles \"Mr\" or \"Mrs\". Master has a much clearer difference than Miss though, so we should look to combine Miss with other factors to see if we can get some even better differences."]}, {"metadata": {"_uuid": "46f5b1575a4ce46648af86f4bd84f212ed2fa8c8", "_cell_guid": "fed707ca-3271-4e37-a896-f268b58bbd3a", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["ageOfMissWithParch = train[train.Name.str.contains('Miss.') & (train.Parch > 0) & (pd.isnull(train.Age) == False)].Age\n", "plt.hist(ageOfMissWithParch)\n", "plt.xlabel(\"Age\")\n", "plt.ylabel(\"Frequency\")\n", "plt.title(\"Age distribution for people who have the title Miss and have Parents/Children.\")\n", "plt.show()\n", "print(\"Mean age: \" + str(numpy.mean(ageOfMissWithParch)))\n", "print(\"Median age: \" + str(numpy.median(ageOfMissWithParch)))"], "outputs": []}, {"metadata": {"_uuid": "0b1a880c72d6eebbfbba0365d18434ec5658b92e", "_cell_guid": "e82ecdd2-3c83-42c8-af0d-5522b0dcd007", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["ageOfMissWithoutParch = train[train.Name.str.contains('Miss.') & (train.Parch == 0) & (pd.isnull(train.Age) == False)].Age\n", "plt.hist(ageOfMissWithoutParch)\n", "plt.xlabel(\"Age\")\n", "plt.ylabel(\"Frequency\")\n", "plt.title(\"Age distribution for people who have the title Miss and don't have Parents/Children.\")\n", "plt.show()\n", "print(\"Mean age: \" + str(numpy.mean(ageOfMissWithoutParch)))\n", "print(\"Median age: \" + str(numpy.median(ageOfMissWithoutParch)))"], "outputs": []}, {"metadata": {"_uuid": "b81a36fde6f4c4c202f649e7bf42bce6cf20e5dc", "_cell_guid": "4f9bd5ff-c841-469a-9bd2-ad12168da6fd", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["ageOfMrsWithParch = train[train.Name.str.contains('Mrs.') & (train.Parch > 0) & (pd.isnull(train.Age) == False)].Age\n", "plt.hist(ageOfMrsWithParch)\n", "plt.xlabel(\"Age\")\n", "plt.ylabel(\"Frequency\")\n", "plt.title(\"Age distribution for people who have the title Mrs and have Parents/Children.\")\n", "plt.show()\n", "print(\"Mean age: \" + str(numpy.mean(ageOfMrsWithParch)))\n", "print(\"Median age: \" + str(numpy.median(ageOfMrsWithParch)))"], "outputs": []}, {"metadata": {"_uuid": "900c6516951bceec63a24b15a6950c081a5d70ed", "_cell_guid": "35174abe-9e82-4317-b2e4-742bf1757242", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["ageOfMrsWithoutParch = train[train.Name.str.contains('Mrs.') & (train.Parch == 0) & (pd.isnull(train.Age) == False)].Age\n", "plt.hist(ageOfMrsWithoutParch)\n", "plt.xlabel(\"Age\")\n", "plt.ylabel(\"Frequency\")\n", "plt.title(\"Age distribution for people who have the title Mrs and don't have Parents/Children.\")\n", "plt.show()\n", "print(\"Mean age: \" + str(numpy.mean(ageOfMrsWithoutParch)))\n", "print(\"Median age: \" + str(numpy.median(ageOfMrsWithoutParch)))"], "outputs": []}, {"metadata": {"_uuid": "080f0da2d5390a71f1713126a4f80c73e405ab02", "_cell_guid": "65bf6490-a366-42a5-87d7-ab4026b6c1b9"}, "cell_type": "markdown", "source": ["It appears that the difference is extremely notable for Miss, but negligable for Mrs. One thing we have to note is that the number of samples when we get this specific is much lower, so we have to make sure that we are not overfitting to it. This is something we should be looking for if we get high variance on the test set. I won't be doing similar analysis on the test set now, as this would not emulate a real world scenario, where we may not have a test set on hand.\n", "\n", "#### Conclusion on Age\n", "\n", "Based on the previous analysis, we will use a function that takes into account both name and number of siblings in order to impute the age. The titles \"Master\" and \"Miss\" will be used to identify potential children, and the rest will be adults.\n", "\n", "### Fare\n", "\n", "The first thing we should look into with fare is its distribution."]}, {"metadata": {"_uuid": "5e66928d36752f2c12e42fa6402450eb5da3559f", "_cell_guid": "881bf145-c125-4fd3-9963-22969beeab5f", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["fare = train.Fare\n", "plt.hist(fare)\n", "plt.xlabel(\"Price\")\n", "plt.ylabel(\"Frequency\")\n", "plt.title(\"Price distribution for known training data.\")\n", "plt.show()\n", "print(\"Mean fare: \" + str(numpy.mean(fare)))\n", "print(\"Median fare: \" + str(numpy.median(fare)))\n", "print(\"Standard deviation: \" + str(numpy.std(fare)))\n", "print(\"Normality: \" + str(stats.normaltest(fare)))"], "outputs": []}, {"metadata": {"_uuid": "b740e4fc23ceaf1d14488678174b2c1128527a9c", "_cell_guid": "05e0a3bb-d29c-48bd-81e3-5fed2b1a50fc"}, "cell_type": "markdown", "source": ["It is clear here that the data is not even close to normally distributed, and that the majority of the values are under 50 dollars. So, we should look at the following:\n", "\n", "1. Do people who paid greater than 50 dollars survive at a higher rate?\n", "2. Is there a way to make the data more normally distributed?\n", "\n", "Based on the data above, data within 3 standard deviations of the mean is fares below 181 dollars. So, we will test to see if removing those data points helps, and if the fares above 181 dollars differ significantly to those near 181 dollars."]}, {"metadata": {"_uuid": "a7e3a5b52e1ce9568cecf1d5447bcd4da3446cf1", "_cell_guid": "03112da9-1fe0-47d5-9492-009ba0a7041e", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["fareOver181 = train[train.Fare > 181]\n", "fareBetween100And181 = train[(train.Fare > 100) & (train.Fare < 181)]\n", "fareUnder181 = train[(train.Fare < 181)].Fare\n", "plt.hist(fareUnder181)\n", "plt.xlabel(\"Price\")\n", "plt.ylabel(\"Frequency\")\n", "plt.title(\"Price distribution for fares under 181.\")\n", "plt.show()\n", "print(\"Survival rate for those whose fare is > 181: \" + str(sum(fareOver181.Survived) / len(fareOver181.Survived)))\n", "print(\"Survival rate for those whose fare is < 181 and > 100: \" + str(sum(fareBetween100And181.Survived) / len(fareBetween100And181.Survived)))\n", "\n", "print(\"Normality: \" + str(stats.normaltest(fareUnder181)))\n"], "outputs": []}, {"metadata": {"_uuid": "31bcd12e2fcd1616202821cf564ccab822dc1173", "_cell_guid": "929ccce1-87cb-4d60-a34b-d95a46b3fa35"}, "cell_type": "markdown", "source": ["It appears that those who pay a higher rate have a significantly higher survival rate, so we may not want to lose that data. However, data nearby it has a similar survival rate, so it appears fine to eliminate it as an outlier. Furthermore, though this is an improvement, it certainly does not make the data normal. Next, let's try applying some transformations to the data and checking the normality.\n", "\n", "#### Square Root"]}, {"metadata": {"_uuid": "da8519719690ad7c6f79fb733576e2c6f618cf9d", "_cell_guid": "d9d5e311-f4b1-408f-9d57-bca0814be589", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["rootFareUnder181 = fareUnder181.map(lambda x: math.sqrt(x))\n", "plt.hist(rootFareUnder181)\n", "plt.xlabel(\"Price\")\n", "plt.ylabel(\"Frequency\")\n", "plt.title(\"Price distribution for square roots of fares under 181.\")\n", "plt.show()\n", "print(\"Normality: \" + str(stats.normaltest(rootFareUnder181)))"], "outputs": []}, {"metadata": {"_uuid": "aa1c3f0b714743658fbebc5218f8214db465707a", "_cell_guid": "c68bbe54-f8b4-40ee-8bff-d337e1782fa2"}, "cell_type": "markdown", "source": ["#### Log"]}, {"metadata": {"_uuid": "9888d819248ecd521e2dca0cef768add4804c9cd", "_cell_guid": "3f0032e6-f095-476f-86a7-39a45573e1e9", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["logFareUnder181 = fareUnder181.map(lambda x: math.log(x, 10) if x != 0 else x)\n", "plt.hist(rootFareUnder181)\n", "plt.xlabel(\"Price\")\n", "plt.ylabel(\"Frequency\")\n", "plt.title(\"Price distribution for logs of fares under 181.\")\n", "plt.show()\n", "print(\"Normality: \" + str(stats.normaltest(logFareUnder181)))"], "outputs": []}, {"metadata": {"_uuid": "878c455a03d034500b7a9b6984cdd328cff277de", "_cell_guid": "cce97cb1-573d-4844-a1d3-2779b18d5f09"}, "cell_type": "markdown", "source": ["The log of the filtered data is a significant improvement in terms of normality, while potentially offering little performance hit. Therefore, this will be implemented below."]}, {"metadata": {"_uuid": "0b9b1ffbf39c3a6e3659e493df3c761ef2fcd515", "_cell_guid": "349916e4-e607-40fc-a49f-e27e8a47a5fe"}, "cell_type": "markdown", "source": ["## Data Cleansing\n", "\n", "The first step for cleansing the data is creating the functions we will use, most importantly the age function."]}, {"metadata": {"_uuid": "db146c723aacabd3134ec41d2508ef2b3fafe5d9", "_cell_guid": "7eea83a6-a2af-449c-87f9-b480483d7497", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["def ageFunc(x):\n", "    age = x['Age']\n", "    name = x['Name']\n", "    sibs = x['SibSp']\n", "    if math.isnan(age):\n", "        if \"Master.\" in name:\n", "            x['Age'] = 5\n", "        elif \"Miss.\" in name and sibs > 0:\n", "            x['Age'] = 11\n", "        elif \"Miss.\" in name and sibs == 0:\n", "            x['Age'] = 27\n", "        elif \"Mr.\" in name:\n", "            x['Age'] = 32\n", "        elif \"Mrs.\" in name:\n", "            x['Age'] = 36\n", "        else:\n", "            x['Age'] = 29\n", "    return x\n", "    \n", "def embarkedFunc(x):\n", "    vals = {'Q': 1, 'S': 2, 'C': 3}\n", "    return vals.get(x, 0)"], "outputs": []}, {"metadata": {"_uuid": "1875cf55ddd3851c9decaadfb25e1b70b14e503f", "_cell_guid": "724811fe-739c-4907-8475-c4da7b67acb2"}, "cell_type": "markdown", "source": ["### Initial Cleansing\n", "\n", "Next, we will create a function that will cleanse an input data frame, so it can be reused on the training and test sets. The most notable parts of this cleansing are the following:\n", "* Text data that has discrete values are converted to factors (0:n-1)\n", "* We use whether or not there is a letter (not number) in the ticket\n", "* We use whether or not a cabin was listed"]}, {"metadata": {"_uuid": "c60e45f7d5c9217db110b87d14e6dd23d8d78c9e", "_cell_guid": "47d26f15-7be1-442d-a0ca-f9b3c225f3a6", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["def cleanData(frame, isTest):\n", "    averageFare = numpy.mean(frame.Fare)\n", "    frame = frame.apply(ageFunc, axis='columns')\n", "    frame.Fare = frame.Fare.map(lambda x: x if not numpy.isnan(x) else averageFare)\n", "    out = frame.query('Fare < 181').copy() if not isTest else frame.copy()\n", "    out.Sex = out.Sex == 'female'\n", "    out.Fare = out.Fare.map(lambda y: math.log(y, 10) if y != 0 else y)\n", "    out.Ticket = out.Ticket.map(lambda x: not pd.isnull(re.search(\"[a-zA-Z]\", x)))\n", "    out['emS'] = out.Embarked == 'S'\n", "    out['emC'] = out.Embarked == 'C'\n", "    out['class1'] = out.Pclass == 1\n", "    out['class2'] = out.Pclass == 2\n", "    out.Embarked = out.Embarked.map(embarkedFunc)\n", "    out.Cabin = out.Cabin.map(lambda x: not pd.isnull(x))\n", "    return out.iloc[:,out.columns.get_level_values(0).isin({\"Survived\", \"PassengerId\", \"Fare\", 'class1', 'class2', \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Cabin\", \"Ticket\", 'emS', 'emC'})]\n", "\n", "train = cleanData(train, False)\n", "test = cleanData(test, True)"], "outputs": []}, {"metadata": {"_uuid": "67d2d96ebc6d824c72a3e2846cd88fa7ab45a3e3", "_cell_guid": "1ed94465-6409-42db-801e-8d61ef49da6e"}, "cell_type": "markdown", "source": ["### Assessing Correlation\n", "\n", "Now that we have cleaned the data, let's look into the colinearity of each of the variables, and see if we can eliminate some that are heavily correlated."]}, {"metadata": {"_uuid": "0942832177c36a52f53b2d19c44e34921bb7b2b3", "_cell_guid": "73e0f7cf-9aa6-4598-a5fc-17f4b37b083e", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["train.corr()"], "outputs": []}, {"metadata": {"_uuid": "392fce1662313c787c2988efa270f99aca78772e", "_cell_guid": "19a2ef71-e1de-4b15-b443-6dc95ae4d2c9"}, "cell_type": "markdown", "source": ["Based on this there are two major groups of colinear features: the group Cabin, Fare, and PClass, and the second group SibSp and Parch. These both make sense, and we will handle the two differently.\n", "\n", "The most suprising thing about this correlation is the lack of correlation between age and family factors in survival. We will also look further into this to see if we can use feature extraction to create a better correlation.\n", "\n", "### Feature Extraction\n", "\n", "We will first look into using bins of ages to create a better correlation between age and survival. First, let's look at the survival rates of different age ranges."]}, {"metadata": {"_uuid": "c7f52183fb32eec9fac5124ef32c54a292ff4846", "_cell_guid": "5fa6200c-26c2-4d84-bac4-dd13e8ba48bb", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["ageLessEq15 = train[(train.Age <= 15)].Survived\n", "ageBetween16and35 = train[(train.Age > 15) & (train.Age <= 35)].Survived\n", "ageBetween36and60 = train[(train.Age > 35) & (train.Age <= 60)].Survived\n", "ageGreater60 = train[(train.Age > 60)].Survived\n", "print(\"<= 15 survival: \" + str(sum(ageLessEq15) / len(ageLessEq15)))\n", "print(\"16 - 35 survival: \" + str(sum(ageBetween16and35) / len(ageBetween16and35)))\n", "print(\"36 - 60 survival: \" + str(sum(ageBetween36and60) / len(ageBetween36and60)))\n", "print(\"> 60 survival: \" + str(sum(ageGreater60) / len(ageGreater60)))"], "outputs": []}, {"metadata": {"_uuid": "7d627b5f5579d27bc8ac45e2c033475d17c51776", "_cell_guid": "28516bb7-76ab-4d68-8dfa-bfab6ea745c7"}, "cell_type": "markdown", "source": ["It seems like people below the age of 15 (children) had a significantly better than average chance of survival. Further, it seems the elderly had a low survival rate. So, let's see if adding an isChild and an isSenior variable has a good correlation with survival."]}, {"metadata": {"_uuid": "48b02dafd3d17f4b4171771964006674b8d9a819", "_cell_guid": "8b867e01-dd82-4fe7-9db9-6a84696a2adc", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["train['isChild'] = train.Age <= 15\n", "train['isSenior'] = train.Age > 60\n", "train.corr()"], "outputs": []}, {"metadata": {"_uuid": "a03e92ff7f012c80f44078a8a24a9bd3a0e32e39", "_cell_guid": "a1a27395-d4d4-4de0-907b-78d450cb2297"}, "cell_type": "markdown", "source": ["isChild provided a slightly better correlation, but isSenior did not. This is probably due to the relative sizes of the two groups as displayed below:"]}, {"metadata": {"_uuid": "a2cd04a0d6ed8c33760fec2072839097206e23a8", "_cell_guid": "82d574dc-7de7-4604-ab23-0b9323642f78", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["print(\"isChild Size: {}\".format(len(ageLessEq15)))\n", "print(\"isSenior Size: {}\".format(len(ageGreater60)))"], "outputs": []}, {"metadata": {"_uuid": "2b9a9a63bdb754f27fec5d4bd24deb33986ab68c", "_cell_guid": "6e8658a3-7f5e-434d-8511-e019d5122316"}, "cell_type": "markdown", "source": ["Based on this, the isSenior variable should not be used, so we will remove it."]}, {"metadata": {"_uuid": "9fb15d4b1b759983e6f1e80a497949b011bdaf97", "_cell_guid": "2e15bcff-dbff-4eb7-90e2-ddb0ea33e401", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["train = train.drop(\"isSenior\", axis=1, errors='ignore')\n", "test[\"isChild\"] = test.Age <= 15"], "outputs": []}, {"metadata": {"_uuid": "415af1938751d9d5ea626a6b4f35a08539e9fd0e", "_cell_guid": "952457cb-5899-469d-a25e-3e3554761ab3"}, "cell_type": "markdown", "source": ["Now let's look at Siblings and Parch"]}, {"metadata": {"_uuid": "966135c9b4ee6ee6d3045b5d42f91ea972453c08", "_cell_guid": "807b2091-87ad-4d98-8e7b-474ef89e012f", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["plt.hist(train.SibSp)\n", "plt.xlabel(\"Price\")\n", "plt.ylabel(\"Frequency\")\n", "plt.title(\"Distribution of Siblings\")\n", "plt.show()\n", "\n", "plt.hist(train.Parch)\n", "plt.xlabel(\"Price\")\n", "plt.ylabel(\"Frequency\")\n", "plt.title(\"Distributions of Parch\")\n", "plt.show()"], "outputs": []}, {"metadata": {"_uuid": "8ef96bad4150fcdb7d810a601d04dab0147df904", "_cell_guid": "521afa05-5f4f-4384-b4bf-0b52525d0ca3"}, "cell_type": "markdown", "source": ["The distributions of these two variables is very similar, which is further suported by the 0.4 correlation between the two variables. In another kernel, I saw the idea to sum these two together into a variable called family size. We will try that and see the distribution and correlation with survival."]}, {"metadata": {"_uuid": "8bcaba47b0e86759932a6fe784fb29c78f96a193", "_cell_guid": "0dafb6aa-cf75-49d3-9cbd-da99b430f1d6", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["train[\"FamilySize\"] = train.SibSp + train.Parch\n", "plt.hist(train.FamilySize)\n", "plt.xlabel(\"Price\")\n", "plt.ylabel(\"Frequency\")\n", "plt.title(\"Distributions of Parch\")\n", "plt.show()\n", "train.corr()"], "outputs": []}, {"metadata": {"_uuid": "e5a03cb42516586a7d175ba8552954f4c84e4c7d", "_cell_guid": "3d200f18-c6c5-42d6-8eed-6667d9bd536f"}, "cell_type": "markdown", "source": ["Unfortunately, family size is even less correlated than the other two. In the end, the best approach may be to just use Parch. For now, we will drop family size."]}, {"metadata": {"_uuid": "ffeb3359e967198cdba3bebb168c92a1b6e3074f", "_cell_guid": "e9c8fe26-2ab8-44f7-a263-548465e90929", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["train = train.drop(\"FamilySize\", axis=1, errors='ignore')"], "outputs": []}, {"metadata": {"_uuid": "047a97c08f6c93813f4ad61fb9455c626e33f33b", "_cell_guid": "6f7281c7-8476-494a-bd7c-712212bd8d30"}, "cell_type": "markdown", "source": ["### Dealing with Colinearity\n", "\n", "The way we will deal with colinearity is testing the algorithms. We will apply each algorithm to the data with and without colinear features. The features we will use regardless of colinearity are: Parch and Fare. isChild would be used instead of Age, however this is a derived feature."]}, {"metadata": {"_uuid": "2d01418d6cb175630c0aec04a88286ad96f86388", "_cell_guid": "7b2cbd7d-b68a-497b-9490-f2eda5abb856"}, "cell_type": "markdown", "source": ["## Running the Algorithms\n", "\n", "Next we will run a few different algorithms, splitting the training data into train and verify data so that we can have a preliminary way of seeing which algorithm is performing best.\n", "\n", "### Algorithms\n", "\n", "We will be trying four different algorithms on the data: an SVM, a Neural Network, a Random Forest, and a best two out of three vote of the three algorithms. Note that for the SVM we will use a Gaussian kernel, as we do not have many features. We will test it on 50 different 80/20 splits of the data for train/validation. We will also do this using the colinear version of the data and the non colinear version. "]}, {"metadata": {"_uuid": "0cc40f4075520a8bfc0bf7b1fe332b663d159cbe", "_cell_guid": "59fc2f1c-c80d-40d6-862f-860bb7b4f25f", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["def createSplitNoColinear(t):\n", "    train, cv = train_test_split(t, test_size = 0.2)\n", "    X_train = preprocessing.scale(numpy.transpose([train.isChild, train.Sex, train.Parch, train.Fare, train.emS, train.emC, train.Ticket]))\n", "    Y_train = numpy.transpose(train.Survived)\n", "    X_cv = preprocessing.scale(numpy.transpose([cv.isChild, cv.Sex, cv.Parch, cv.Fare, cv.emS, cv.emC, cv.Ticket]))\n", "    Y_cv = numpy.transpose(cv.Survived) \n", "    return {\n", "        \"X_train\": X_train,\n", "        \"Y_train\": Y_train,\n", "        \"X_cv\": X_cv,\n", "        \"Y_cv\": Y_cv\n", "    }\n", "\n", "def createSplitColinear(t):\n", "    train, cv = train_test_split(t, test_size = 0.2)\n", "    X_train = preprocessing.scale(numpy.transpose([train.isChild, train.Sex, train.Parch, train.Fare, train.emS, train.emC, train.Ticket, train.class1, train.class2, train.Age, train.SibSp, train.Cabin]))\n", "    Y_train = numpy.transpose(train.Survived)\n", "    X_cv = preprocessing.scale(numpy.transpose([cv.isChild, cv.Sex, cv.Parch, cv.Fare, cv.emS, cv.emC, cv.Ticket, cv.class1, cv.class2, cv.Age, cv.SibSp, cv.Cabin]))\n", "    Y_cv = numpy.transpose(cv.Survived) \n", "    return {\n", "        \"X_train\": X_train,\n", "        \"Y_train\": Y_train,\n", "        \"X_cv\": X_cv,\n", "        \"Y_cv\": Y_cv\n", "    }\n", "\n", "\n", "def testData(split, x):\n", "    layer_sizes = [len(split['X_train'][0]) * 3, len(split['X_train'][0])]\n", "    neural =  MLPClassifier(solver='lbfgs', alpha=1e-4, hidden_layer_sizes=(layer_sizes[0], layer_sizes[1], 1), random_state=1)\n", "    forest = RandomForestClassifier(max_depth=5, random_state=0)\n", "    machine = svm.SVC(kernel='rbf')\n", "    neural.fit(split['X_train'], split['Y_train'])\n", "    forest.fit(split['X_train'], split['Y_train'])\n", "    machine.fit(split['X_train'], split['Y_train'])\n", "    nSurvived = neural.predict(split['X_cv'])\n", "    fSurvived = forest.predict(split['X_cv'])\n", "    sSurvived = machine.predict(split['X_cv'])\n", "    vSurvived = list(map(lambda x: int(sum(x) > 2), zip(nSurvived, fSurvived, sSurvived)))\n", "    x['neural'] = sum(nSurvived == split['Y_cv'])/len(nSurvived)\n", "    x['forest'] = sum(fSurvived == split['Y_cv'])/len(fSurvived)\n", "    x['svm'] = sum(sSurvived == split['Y_cv'])/len(sSurvived)\n", "    x['vote'] = sum(vSurvived == split['Y_cv'])/len(vSurvived)\n", "    return x"], "outputs": []}, {"metadata": {"_uuid": "50764daee52ad543ebdaf5b1413999be9b33f76f", "_cell_guid": "353dd279-1b01-49a4-a9c2-bb0f40d43435"}, "cell_type": "markdown", "source": ["### Testing Without Colinearity"]}, {"metadata": {"_uuid": "1b06e5f429f721cdba1b890fc57ffc7eabc06c55", "_cell_guid": "db3b7409-80d4-4219-8442-04b05a464869", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["TRIALS = 50\n", "predictions = pd.DataFrame({'neural': range(TRIALS), 'forest': range(TRIALS), 'svm': range(TRIALS), 'vote': range(TRIALS)}, dtype=float)\n", "predictions = predictions.apply(lambda x: testData(createSplitNoColinear(train), x), axis=1)\n", "predictions.describe()"], "outputs": []}, {"metadata": {"_uuid": "498026522627a03785089f1c9bfe4b44b2ce167c", "_cell_guid": "c6868ee6-e26e-4d09-aeab-263d62c3b005"}, "cell_type": "markdown", "source": ["Interestingly enough, the neural network performed the worst out of all the algorithms in all cases, while the SVM performed the best. Next, we will try using all of the data, instead of eliminating data that is colinear.\n", "\n", "### Testing With Colinearity"]}, {"metadata": {"_uuid": "ad2b755cd6d827e7d3338e33f31261c92fcc02ce", "_cell_guid": "84d7c1b6-d3a5-4600-a1d9-57808d275e5f", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["TRIALS = 50\n", "predictions = pd.DataFrame({'neural': range(TRIALS), 'forest': range(TRIALS), 'svm': range(TRIALS), 'vote': range(TRIALS)}, dtype=float)\n", "predictions = predictions.apply(lambda x: testData(createSplitColinear(train), x), axis=1)\n", "predictions.describe()"], "outputs": []}, {"metadata": {"_uuid": "9dfe8f07e46e641b86aaa37ddf873d6dd2999323", "_cell_guid": "5c3ecd14-b522-4c53-8871-6cf0303010db"}, "cell_type": "markdown", "source": ["Again, the neural network perfored extremely poorly, and the SVM performed the best. The forest performed even better with the colinear data than it did without it, so we will include everything when training for the real test data.\n", "\n", "The neural network performed pretty poorly in general this could be because we are overfitting the data. We should look into adjusting the regularization parameter to find the optimal value.\n", "\n", "### Dealing with Overfitting\n", "\n", "We will now find the optimal regularization parameter for the colinear data, as the algorithms performed better on the colinear data in general."]}, {"metadata": {"_uuid": "c9090867bd5508eafcdd4d7a35c95ecf8fd0ace1", "_cell_guid": "6f0d16da-f7cc-468d-9264-74769d6eb79b", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["def testCVNeural(split, x, reg):\n", "    layer_sizes = [len(split['X_train'][0]) * 3, len(split['X_train'][0])]\n", "    for a in reg:\n", "        neural =  MLPClassifier(solver='lbfgs', alpha=a, hidden_layer_sizes=(layer_sizes[0], layer_sizes[1], 1), random_state=1)\n", "        neural.fit(split['X_train'], split['Y_train'])\n", "        nSurvived = neural.predict(split['X_cv'])\n", "        x[str(a)] = sum(nSurvived == split['Y_cv'])/len(nSurvived)\n", "    return x"], "outputs": []}, {"metadata": {"_uuid": "af62ac4f111ae59a6d6853a25fce2cbf72231a90", "_cell_guid": "f5d57f8c-ea4e-4ca7-a815-a25b98d24bd4", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["TRIALS = 50\n", "reg = [1e-4, 3e-4, 1e-3, 3e-3, 1e-2, 3e-2, 1e-1, 3e-1, 1, 3, 10, 30, 100]\n", "regDict = {}\n", "for a in reg:\n", "    regDict[str(a)] = range(TRIALS)\n", "predictions = pd.DataFrame(regDict, dtype=float)\n", "predictions = predictions.apply(lambda x: testCVNeural(createSplitColinear(train), x, reg), axis=1)\n", "predictions.describe()"], "outputs": []}, {"metadata": {"_uuid": "c31a73bfefdd47d8cb7dae9f89b94f25ecb0395a", "_cell_guid": "9e5b746c-ecb3-4e29-b388-7be36b58e0c9"}, "cell_type": "markdown", "source": ["Based on this, the optimal regularization term is 10, so we will use this in the final model. Now, let's do the same thing for the SVM."]}, {"metadata": {"_uuid": "3a596ef71d1d75ade6f4cc1570d543af37631c96", "_cell_guid": "adf00de3-70d0-4e30-a007-86de8204e7e1", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["def testCVSVM(split, x, reg):\n", "    layer_sizes = [len(split['X_train'][0]) * 3, len(split['X_train'][0])]\n", "    for a in reg:\n", "        machine = svm.SVC(kernel='rbf', C=a)\n", "        machine.fit(split['X_train'], split['Y_train'])\n", "        sSurvived = machine.predict(split['X_cv'])\n", "        x[str(a)] = sum(sSurvived == split['Y_cv'])/len(sSurvived)\n", "    return x"], "outputs": []}, {"metadata": {"_uuid": "a4a10b14962d96b5e5021a2deb1b2a4e176e22ed", "_cell_guid": "9d3bb3c0-4500-401a-b797-8c5c935dabf6", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["TRIALS = 50\n", "reg = [1e-3, 3e-3, 1e-1, 3e-1, 1, 3, 10]\n", "regDict = {}\n", "for a in reg:\n", "    regDict[str(a)] = range(TRIALS)\n", "predictions = pd.DataFrame(regDict, dtype=float)\n", "predictions = predictions.apply(lambda x: testCVSVM(createSplitColinear(train), x, reg), axis=1)\n", "predictions.describe()"], "outputs": []}, {"metadata": {"_uuid": "e8d60b856b8a6436fed01d20b0f4857247bea542", "_cell_guid": "5dab69cd-9e28-4c0f-bf9c-8c713270838e"}, "cell_type": "markdown", "source": ["Since 0.3 performed the best, we will use it.\n", "\n", "### Final Modeling"]}, {"metadata": {"_uuid": "fb77e09d52569acceeef2d4f2f489d04f21b4412", "_cell_guid": "87dcd4b5-d17d-4966-beaa-573e0e950dab", "collapsed": true}, "execution_count": null, "cell_type": "code", "source": ["X_train = preprocessing.scale(numpy.transpose([train.isChild, train.Sex, train.Parch, train.Fare, train.emS, train.emC, train.Ticket, train.class1, train.class2, train.Age, train.SibSp, train.Cabin]))\n", "Y_train = numpy.transpose(train.Survived)\n", "X_test = preprocessing.scale(numpy.transpose([test.isChild, test.Sex, test.Parch, test.Fare, test.emS, test.emC, test.Ticket, test.class1, test.class2, test.Age, test.SibSp, test.Cabin]))\n", "layer_sizes = [len(X_train[0]) * 3, len(X_train[0])]\n", "neural =  MLPClassifier(solver='lbfgs', alpha=10, hidden_layer_sizes=(layer_sizes[0], layer_sizes[1]), random_state=1)    \n", "forest = RandomForestClassifier(max_depth=5, random_state=0)\n", "machine = svm.SVC(kernel='rbf', C=0.3)\n", "neural.fit(X_train, Y_train)\n", "forest.fit(X_train, Y_train)\n", "machine.fit(X_train, Y_train)\n", "nSurvived = neural.predict(X_test)\n", "fSurvived = forest.predict(X_test)\n", "sSurvived = machine.predict(X_test)\n", "vSurvived = list(map(lambda x: int(sum(x) > 2), zip(nSurvived, fSurvived, sSurvived)))\n", "\n", "pd.DataFrame({\"PassengerId\": test.PassengerId, \"Survived\": nSurvived}).to_csv(\"neural.csv\", index=False)\n", "pd.DataFrame({\"PassengerId\": test.PassengerId, \"Survived\": fSurvived}).to_csv(\"forest.csv\", index=False)\n", "pd.DataFrame({\"PassengerId\": test.PassengerId, \"Survived\": sSurvived}).to_csv(\"svm.csv\", index=False)\n", "pd.DataFrame({\"PassengerId\": test.PassengerId, \"Survived\": vSurvived}).to_csv(\"vote.csv\", index=False)"], "outputs": []}, {"metadata": {"_uuid": "335e6f060347ba34a57d230be8233a9ca59437b4", "_cell_guid": "acced733-23d0-4d4f-9a12-dc29ba40ef1a"}, "cell_type": "markdown", "source": ["## Conclusion and Final Thoughts\n", "\n", "The overal success rates after uploading these to the kaggle website were as follows:\n", "\n", "* forest: 0.78468\n", "* neural: 0.77990\n", "* svm: 0.79425\n", "* vote: 0.77990\n", "\n", "I would like to thank anyone who took the time out to read this document. This was my first time trying a data science problem like this, and I would love to hear any feedback about how I can improve, and if it was a clear read through. Thank you!"]}], "nbformat": 4}