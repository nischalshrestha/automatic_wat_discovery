{"cells":[{"metadata":{"_uuid":"4d4fb71db6508c190dbf9a4b5f6d708277a1bb22"},"cell_type":"markdown","source":"# titanic"},{"metadata":{"_uuid":"06b211e23dcec65eb9f0772b1cf74d8c575ce51d"},"cell_type":"markdown","source":"My notebook contains three models.\n1.  xgboost  \nI executed this model on my PC.\n    * ubuntu 18.04\n    * xgboost==0.80\n    * \"submit2.csv\" of the png attached below   \n    \n    Fortunately this model on kaggle produces the same result."},{"metadata":{"trusted":true,"_uuid":"4ff373ab2b3d92ecde387aee4b5f4685fb4c2568","scrolled":true},"cell_type":"code","source":"from IPython.display import Image\nImage(\"../input/mypng1/Screenshot-xgb.png\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"11d96e6d6cfdf7830a73b3bd93a549f63990e111"},"cell_type":"markdown","source":"2.  keras neural network  #1  \nI executed this model on google colab.\n    * tensorflow 1.12.0\n    * keras 2.2.4\n    * \"submission.2018-11-16v1-0-4_4.csv\" of the png attached below       \n    \n    Unortunately this model on kaggle does not produce the same result.\n\n3.  keras neural network  #2  \nI executed this model on my PC.\n    * ubuntu 18.04\n    * tensorflow 1.11.0\n    * keras 2.2.2\n    * \"submission.2018-11-19v1-0-4_4.csv\" of the png attached below       \n    \n    Unortunately this model on kaggle does not produce the same result."},{"metadata":{"trusted":true,"_uuid":"c92031ff0e31b744a50e7b19db292eb02a5eb76a"},"cell_type":"code","source":"from IPython.display import Image\nImage(\"../input/mypngs2/Screenshot-nn.png\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"59e58d835131b0337768fd7c2f003c958625edde"},"cell_type":"markdown","source":"As stated above, I run my model on colab and my pc. Kaggle has tensorflow 1.11.0-rc1 and keras 2.2.4 and this is the third environment for me. Enough...  \nAfter uploading my notebook to kaggle, I changed it to make it run on kaggle but I don't tune it because I want to finish titanic trial ASAP.  \n\nMy three predictions have the accuracy of 0.80861 but they are different from each other.\n* keras on colab and on my pc : 2 different predictions\n* keras on colab and xgb on my pc : 29 different predictions\n* keras on my pc and xgb on my pc : 27 different predictions  \n\nI checked the results on kaggle.  Two models, xgb and \"call model on colab\", produced the same result."},{"metadata":{"_uuid":"90e0e627bccba331bce7edeaf9f00ebe79ed20e3"},"cell_type":"markdown","source":"以下のカーネルを参照。\n- [Titanic Data Science Solutions by Manav Sehgal][1]\n- [Titanic Deep Net [0.82296] by Chris Deotte][2]\n- [Titanic WCG+XGBoost [0.84688] by Chris Deotte][3]  \n\nまずManavさんのカーネルを参考にデータ処理を行い、その後Chrisさんのカーネルを参照して追加のデータ処理、フィーチャーエンジニアリングを実施。  \nフィーチャーエンジニアリングとしては目新しことはなにもしていない。Chrisさんの知恵を自分なりにインプリしただけのためフィーチャーエンジニアリングに関するコードは含めていない。上記カーネルを参照してほしい。\n\nI referred to the following kernels.\n- [Titanic Data Science Solutions by Manav Sehgal][1]\n- [Titanic Deep Net [0.82296] by Chris Deotte][2]\n- [Titanic WCG+XGBoost [0.84688] by Chris Deotte][3]  \n\nFirst I referred to Manav-san's kernel for basic data processing and next Chris-san's for additional data processing, feature engineering.\nI have nothing new to say about data insights. I just implement their insights in my own way. This is why I don't include feature engineering related codes in my kernel. Please refer to their kernels about Titanic's data insights.  \n\n[1]:https://www.kaggle.com/startupsci/titanic-data-science-solutions\n[2]:https://www.kaggle.com/cdeotte/titanic-deep-net-0-82296\n[3]:https://www.kaggle.com/cdeotte/titanic-wcg-xgboost-0-84688"},{"metadata":{"_uuid":"ba8caa8b5c7be690ea2bff624831c2f93fb33ee5"},"cell_type":"markdown","source":"# 初期化 / initialize"},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T09:52:03.826664Z","start_time":"2018-11-24T09:52:03.821863Z"},"_cell_guid":"e7319668-86fe-8adc-438d-0eef3fd0a982","_uuid":"13f38775c12ad6f914254a08f0d1ef948a2bd453","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport random as rnd\nimport math\nimport collections\nimport pickle\n\n\nimport xgboost\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.preprocessing import MinMaxScaler\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"571961217eefbd9df5c6eadf98d05957a4070e37"},"cell_type":"code","source":"import os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a682ab6f75d513942770a9886c430a00d28b49f5"},"cell_type":"markdown","source":"# データ処理 / data processing\n当初はトレーニング・データとテスト・データを別々に処理していたが途中で一緒に処理する形態に変更。\ntrain-test.csvは両データをマージしたcsvファイル。\n\nAt first I processed training data and test data separately. At last I processed them together. \nI merged them into train-test.csv.  \n\nI executed this part of the code on my PC only."},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T09:23:27.396786Z","start_time":"2018-11-24T09:23:27.361457Z"},"trusted":true,"_uuid":"5b4848b217c05156cf530872aca3bc59d02a9b46"},"cell_type":"code","source":"# train_df = pd.read_csv('./kaggle.titanic/train-test.csv' )\ntrain_df = pd.read_csv('../input/mycsvdata/train-test.csv' )\nprint( train_df.shape )\nprint( train_df.head(3) )    \nprint( train_df.tail(3) )    \n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"09e0d910724fc94dcf8593f7d9b0c9eaa6360188"},"cell_type":"markdown","source":"## 名前とタイトル / name and title"},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T09:23:29.953916Z","start_time":"2018-11-24T09:23:29.938518Z"},"_cell_guid":"df7f0cd4-992c-4a79-fb19-bf6f0c024d4b","_uuid":"c916644bd151f3dc8fca900f656d415b4c55e2bc","trusted":true},"cell_type":"code","source":"train_df['Title'] = train_df.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T09:24:34.478421Z","start_time":"2018-11-24T09:24:34.474850Z"},"trusted":true,"_uuid":"90b8b2ab8ca5decf86c1bc19db90b60db6896d5e"},"cell_type":"code","source":"train_df['Title'].unique()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T09:24:47.693685Z","start_time":"2018-11-24T09:24:47.678654Z"},"_cell_guid":"553f56d7-002a-ee63-21a4-c0efad10cfe9","_uuid":"b8cd938fba61fb4e226c77521b012f4bb8aa01d0","trusted":true},"cell_type":"code","source":"train_df['Title'] = train_df['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\ntrain_df['Title'] = train_df['Title'].replace('Mlle', 'Miss')\ntrain_df['Title'] = train_df['Title'].replace('Ms', 'Miss')\ntrain_df['Title'] = train_df['Title'].replace('Mme', 'Mrs')\ntrain_df[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T09:25:54.788248Z","start_time":"2018-11-24T09:25:54.750260Z"},"_cell_guid":"67444ebc-4d11-bac1-74a6-059133b6e2e8","_uuid":"e805ad52f0514497b67c3726104ba46d361eb92c","trusted":true},"cell_type":"code","source":"title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\ntrain_df['Title'] = train_df['Title'].map(title_mapping)\ntrain_df['Title'] = train_df['Title'].fillna(0)\n\nprint( train_df.shape )\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"001d48de29652e7a3384f05fefd76ed2f40f2cfd"},"cell_type":"markdown","source":" ## 性別 / Sex"},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T09:26:05.337755Z","start_time":"2018-11-24T09:26:05.254239Z"},"_cell_guid":"c20c1df2-157c-e5a0-3e24-15a828095c96","_uuid":"840498eaee7baaca228499b0a5652da9d4edaf37","trusted":true},"cell_type":"code","source":"train_df['Sex'] = train_df['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ce01fff0cb6eb0aab3bb0d1f516b958b4e3b7cd1"},"cell_type":"markdown","source":"## 年齢推測 / estimating missing age values\n年齢データの欠損についての対応。年齢のバンド化は行わない。\n\nEstimating missing age values. No pd.cut for age."},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T09:26:21.573188Z","start_time":"2018-11-24T09:26:21.438993Z"},"_cell_guid":"a4015dfa-a0ab-65bc-0cbe-efecf1eb2569","_uuid":"31198f0ad0dbbb74290ebe135abffa994b8f58f3","scrolled":false,"trusted":true},"cell_type":"code","source":"# based on Manav-san's kernel\nguess_ages = np.zeros((2,3))\n# guess_ages\nfor i in range(0, 2):\n    for j in range(0, 3):\n        guess_df = train_df[(train_df['Sex'] == i) & \\\n                              (train_df['Pclass'] == j+1)]['Age'].dropna()\n\n        # age_mean = guess_df.mean()\n        # age_std = guess_df.std()\n        # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)\n\n        age_guess = guess_df.median()\n\n        # Convert random age float to nearest .5 age\n        guess_ages[i,j] = int( age_guess/0.5 + 0.5 ) * 0.5\n\nfor i in range(0, 2):\n    for j in range(0, 3):\n        train_df.loc[ (train_df.Age.isnull()) & (train_df.Sex == i) & (train_df.Pclass == j+1),\\\n                'Age'] = guess_ages[i,j]\n\ntrain_df['Age'] = train_df['Age'].astype(int)\n\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"75569b3cc7d45a39ab8301fcee37197ed00f96f5"},"cell_type":"markdown","source":"## ファミリー・サイズ / family size \nfamily size = sibsp + parch + 1"},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T09:26:32.182175Z","start_time":"2018-11-24T09:26:32.115724Z"},"_cell_guid":"7e6c04ed-cfaa-3139-4378-574fd095d6ba","_uuid":"33d1236ce4a8ab888b9fac2d5af1c78d174b32c7","scrolled":true,"trusted":true},"cell_type":"code","source":"train_df['FamilySize'] = train_df['SibSp'] + train_df['Parch'] + 1\n\ntrain_df[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b13d7f79058265cd7e2767e225ca0bb4667e7733"},"cell_type":"markdown","source":"## 単独乗船 / IsAlone\nif family size == 1:\n    IsAlone = 1"},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T09:26:43.628512Z","start_time":"2018-11-24T09:26:43.552741Z"},"_cell_guid":"5c778c69-a9ae-1b6b-44fe-a0898d07be7a","_uuid":"3b8db81cc3513b088c6bcd9cd1938156fe77992f","trusted":true},"cell_type":"code","source":"train_df['IsAlone'] = 0\ntrain_df.loc[train_df['FamilySize'] == 1, 'IsAlone'] = 1\n\ntrain_df[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c3e93ab5e9e1316ae7f699e24ef567a2229054a7"},"cell_type":"markdown","source":"## Age*Class\n結局使わないけど、age*class = age * pclass\n\nAlthough this data is not used."},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T09:26:54.156139Z","start_time":"2018-11-24T09:26:54.098567Z"},"_cell_guid":"305402aa-1ea1-c245-c367-056eef8fe453","_uuid":"aac2c5340c06210a8b0199e15461e9049fbf2cff","scrolled":false,"trusted":true},"cell_type":"code","source":"train_df['Age*Class'] = train_df.Age * train_df.Pclass\n\ntrain_df.loc[:, ['Age*Class', 'Age', 'Pclass']].head(10)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T09:27:01.895490Z","start_time":"2018-11-24T09:27:01.872966Z"},"scrolled":true,"trusted":true,"_uuid":"3e81996f7335884ce785158921467e4190d7bdcb"},"cell_type":"code","source":"train_df.columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"219b274ae75f617a8afdfa13d91542919c251511"},"cell_type":"markdown","source":"## 乗降地推測 / estimating missing embarked values"},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T09:27:08.151568Z","start_time":"2018-11-24T09:27:08.119558Z"},"_cell_guid":"bf351113-9b7f-ef56-7211-e8dd00665b18","_uuid":"1e3f8af166f60a1b3125a6b046eff5fff02d63cf","trusted":true},"cell_type":"code","source":"freq_port = train_df.Embarked.dropna().mode()[0]\nfreq_port","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T09:27:12.128707Z","start_time":"2018-11-24T09:27:12.081276Z"},"_cell_guid":"51c21fcc-f066-cd80-18c8-3d140be6cbae","_uuid":"d85b5575fb45f25749298641f6a0a38803e1ff22","scrolled":true,"trusted":true},"cell_type":"code","source":"train_df['Embarked'] = train_df['Embarked'].fillna(freq_port)\n    \ntrain_df[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T09:27:16.890562Z","start_time":"2018-11-24T09:27:16.804220Z"},"_cell_guid":"89a91d76-2cc0-9bbb-c5c5-3c9ecae33c66","_uuid":"e480a1ef145de0b023821134896391d568a6f4f9","scrolled":true,"trusted":true},"cell_type":"code","source":"train_df['Embarked'] = train_df['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"caee081782ea81ab38a29d445873d33a927ee0e2"},"cell_type":"markdown","source":"## 乗船料推測 / estimating missing fare values\n乗船料についても欠損対応を行うだけでバンド化は行っていない。\n\nEstimating missing fare values and pd.cut is not used as same as age."},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T09:27:38.905965Z","start_time":"2018-11-24T09:27:38.864910Z"},"trusted":true,"_uuid":"5d860936248774c13632005471d0370842068674"},"cell_type":"code","source":"print( train_df['Fare'].describe() )\nprint( train_df['Fare'].unique().shape )","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T09:27:44.192016Z","start_time":"2018-11-24T09:27:44.118998Z"},"trusted":true,"_uuid":"d1aaabaa1959524a13deb7aa724bb9eb784b1f9e"},"cell_type":"code","source":"train_df['Fare'].fillna(train_df['Fare'].dropna().median(), inplace=True)\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T09:27:49.695114Z","start_time":"2018-11-24T09:27:49.552226Z"},"scrolled":true,"trusted":true,"_uuid":"658dedcce1a3144782cf005c9b179b69f655503a"},"cell_type":"code","source":"train_df.describe(include=\"all\")","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T09:27:52.855106Z","start_time":"2018-11-24T09:27:52.800864Z"},"trusted":true,"_uuid":"9a65e1ac208b165ff7734ca55fe93fe213d2a44c"},"cell_type":"code","source":"train_df[\"Fare\"].unique()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T09:27:57.600325Z","start_time":"2018-11-24T09:27:57.574402Z"},"trusted":true,"_uuid":"e25eba946b9a36eba62f26a76feb3947b6fa3ddf"},"cell_type":"code","source":"train_df[[\"Ticket\"]].values[0:10]","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T09:28:10.530307Z","start_time":"2018-11-24T09:28:10.526668Z"},"scrolled":false,"trusted":true,"_uuid":"b302096baa6e113c0f34f54b77f85cbcd633c240"},"cell_type":"code","source":"train_df.columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"06605686af13acaed93a667c93a248d9bf8b08a1"},"cell_type":"markdown","source":"## 一人あたりの乗船料 / fare per person\n同じチケットの乗船者の数をカウントし、その後、乗船料 / カウントでFareAdjを算出。\n- count the number of passengers who have same ticket (\"Ticket C\")   \n- fare per person(FareAdj) = fare / the count"},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T09:29:31.407262Z","start_time":"2018-11-24T09:29:31.219221Z"},"scrolled":true,"trusted":true,"_uuid":"d281f38ac9dd92011cb61ee0019e7611a70a9a0a"},"cell_type":"code","source":"ticket_list = train_df[[\"Ticket\"]].values.tolist()\n# 上のtolist()だとリスト化されるがその要素もリストとなる。そのため\n# 文字列とするために以下の１行を追加。\nticket_list = [_i[0] for _i in ticket_list]\nticket_count = [0 for _i in range(len(ticket_list))]\nc = collections.Counter(ticket_list)\nfor _i in c.keys():\n    # 辞書のキーを順に処理。ticket_listの要素と同じならインデックスを抽出。\n    # 抽出されたインデックスに関し出現回数を設定\n    for _l in [_j for _j, x in enumerate(ticket_list) if x == _i]: \n#             print(_i,_l,c[_i])\n        ticket_count[_l] = c[_i]\ntrain_df = pd.concat([train_df, pd.DataFrame(data=ticket_count, columns=[\"Ticket C\"], dtype='int')], axis=1)\ntrain_df['FareAdj'] = train_df['Fare'] / train_df['Ticket C']\n","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T09:29:41.369924Z","start_time":"2018-11-24T09:29:41.291830Z"},"trusted":true,"_uuid":"396a846dfc9338b3c81644703c51d03cb2c3186b"},"cell_type":"code","source":"train_df.head(3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2fb0b9973f8096b9e8efc0c4799fb3148bb47861"},"cell_type":"markdown","source":"## 家族生存数 / Family Survived Count\n1) 同じチケットを持つ人の乗船IDをリスト化  \n2) リストを処理  \n  2-1) 同じチケットを持つ人が一人だとcontinue    \n  2-2) 同じチケットを持つ人のSurvivedがnanでなければ家族生存数としてカウント    \n  \n\n1) make a list of passengers who have same ticket  \n2) process the list  \n  2-1) if the len of the list==1, continue (person who has no family on the board)   \n  2-2) count the number of persons who survived, excluding herself/himself   \n"},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T09:32:52.874335Z","start_time":"2018-11-24T09:32:46.735728Z"},"trusted":true,"_uuid":"9299a8f17630b43b48f4fb9331d1b10f4f9145b9"},"cell_type":"code","source":"ticket_list = train_df[[\"Ticket\"]].values.tolist()\nticket_list = [_i[0] for _i in ticket_list]\nfamily_survived_count = [0 for _i in range(len(ticket_list))]\nc = collections.Counter(ticket_list)\n\nfor _i, _t in enumerate(ticket_list):\n    same_g = train_df.query('Ticket==@_t')[[\"PassengerId\"]].values.tolist( )\n    same_g = [_i[0] for _i in same_g]\n    if len(same_g) == 1:\n        continue\n#         print(_i,same_g,\" \", end=\"\")\n    tmpcounter = 0\n    for _l in [_x for  _x in same_g if _x!= (_i+1) ]: \n#             print(\"_l :\",_l, end=\"\")\n        if (not math.isnan(train_df.loc[_l-1,\"Survived\"])) :\n            tmpcounter += train_df.loc[_l-1,\"Survived\"]\n#             print( \" tmpcounter :\",tmpcounter)\n#         for _l in [_j for _j, _x in enumerate(ticket_list) if _x == _i]:\n#             family_survived_count[_l]=tmpcounter\n    family_survived_count[_i]=tmpcounter\ntrain_df = pd.concat([train_df, pd.DataFrame(data=family_survived_count, columns=[\"Family_S_C\"], dtype='int')], axis=1)\n\n","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T09:33:12.543712Z","start_time":"2018-11-24T09:33:12.448413Z"},"trusted":true,"_uuid":"05d5c01a761e854abe2d6088012d7a7d1a586d09"},"cell_type":"code","source":"train_df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fb3ef9a745420f5f052149d175d0f0babbcdf83a"},"cell_type":"markdown","source":"## binary 家族生存状況 / Simple Family Survival Status\nif Family_S_C > 0:  Simple_S_C = 1  \nelse:  Simple_S_C = 0  "},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T09:35:38.217539Z","start_time":"2018-11-24T09:35:38.210141Z"},"trusted":true,"_uuid":"ed070e45857b5004db0bfa6da9347a0d1828bf22"},"cell_type":"code","source":"train_df['Simple_S_C']=train_df['Family_S_C'].apply(lambda x: 1 if x >0 else 0)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T09:35:39.838671Z","start_time":"2018-11-24T09:35:39.810840Z"},"trusted":true,"_uuid":"b5afac4d6f5c23f253c21888860cbf87ca42cb89"},"cell_type":"code","source":"train_df[['Family_S_C', 'Simple_S_C']]","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T09:35:53.041683Z","start_time":"2018-11-24T09:35:53.037652Z"},"trusted":true,"_uuid":"c675729752782325e7f99935df784b126511f03d"},"cell_type":"code","source":"train_df.columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5a8bcf7f2e5be52256ff638333ad59d7afde3216"},"cell_type":"markdown","source":"## 幾つかのカラムをドロップ / drop some columns"},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T09:44:20.052849Z","start_time":"2018-11-24T09:44:20.047349Z"},"trusted":true,"_uuid":"aefa83798ea8f21d97b133ede5e455041e1f1e33"},"cell_type":"code","source":"train_df = train_df.drop([\"Ticket\", \"Fare\", \"Age*Class\",\"Ticket C\"], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T09:44:30.367051Z","start_time":"2018-11-24T09:44:30.363190Z"},"scrolled":true,"trusted":true,"_uuid":"e6ea36a54107e7657bb5f8b34f6036c616f896c6"},"cell_type":"code","source":"train_df.columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"759176a727f0ad0d5cbecd1fd16c2718342e778e"},"cell_type":"markdown","source":"## キャビン情報 / CabinInfo \nCabinデータの最初の一文字を抜き出して数値化。  \nTaking the first letter of Cabin data and convert it into numerical value\n\n"},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T09:45:08.588988Z","start_time":"2018-11-24T09:45:08.583881Z"},"trusted":true,"_uuid":"afaf7b00feaa9496401fce67e4c5d0e30478c57b"},"cell_type":"code","source":"train_df['Cabin'].unique()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T09:45:19.713332Z","start_time":"2018-11-24T09:45:19.697924Z"},"scrolled":true,"trusted":true,"_uuid":"b669fa9bf586dab7f8da20f945735fdf47a0d594"},"cell_type":"code","source":"train_df['CabinInfo'] = train_df['Cabin'].apply(lambda _x: str(_x)[0:1] if type(_x) == str else 'noinfo' ) \n\ntrain_df['CabinInfo'].unique()\n\ntitle_mapping = {\"noinfo\":0, \"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\":6, \"G\":7, \"T\":8 }\ntrain_df['CabinInfo'] = train_df['CabinInfo'].map(title_mapping)\n\nprint( train_df.columns )\ntrain_df['CabinInfo'].unique()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"877f1146d2686639458a0a1227ecb85e285277cc"},"cell_type":"markdown","source":"## データ分割 / divide data into train and test "},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T09:46:13.484197Z","start_time":"2018-11-24T09:46:13.481232Z"},"trusted":true,"_uuid":"b942cf0a54c9a0532b0c3086ee472c0586fdb97f"},"cell_type":"code","source":"train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T09:46:39.798710Z","start_time":"2018-11-24T09:46:39.745173Z"},"trusted":true,"_uuid":"48f3d89c4da883147beb5816ef8f99f2b43f754f"},"cell_type":"code","source":"test_df = train_df.query('PassengerId>=892')\ntest_df = test_df.drop(\"Survived\", axis=1)\ntrain_df = train_df.query('PassengerId<=891')","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T09:46:57.731569Z","start_time":"2018-11-24T09:46:57.727920Z"},"trusted":true,"_uuid":"a71e145f0377f4e8c5bbde10e006b5a0d966634a"},"cell_type":"code","source":"print( train_df.shape )\nprint( test_df.shape )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"579615e4da502805bee82ee9bda79cd46b831c33"},"cell_type":"markdown","source":"## X_train, Y_train, X_test作成 / making X_Train, Y_train  and X_test"},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T09:48:10.543319Z","start_time":"2018-11-24T09:48:10.534911Z"},"_cell_guid":"0acf54f9-6cf5-24b5-72d9-29b30052823a","_uuid":"04d2235855f40cffd81f76b977a500fceaae87ad","scrolled":true,"trusted":true},"cell_type":"code","source":"X_train = train_df.drop(\"Survived\", axis=1)\nY_train = train_df[\"Survived\"]\nX_test  = test_df.drop(\"PassengerId\", axis=1).copy()\nX_train.shape, Y_train.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T09:48:16.599531Z","start_time":"2018-11-24T09:48:16.590525Z"},"scrolled":true,"trusted":true,"_uuid":"450c12d8d033876f83b58462a2722a0067db4a2c"},"cell_type":"code","source":"X_train.columns","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T09:48:29.755486Z","start_time":"2018-11-24T09:48:29.751568Z"},"trusted":true,"_uuid":"5218a01a16f8a3e29a6629bd2ae34c2078a45eef"},"cell_type":"code","source":"X_test.columns","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T09:48:19.549877Z","start_time":"2018-11-24T09:48:19.496155Z"},"scrolled":false,"trusted":true,"_uuid":"11a66defd47bfcd22c2898c5537ba4bdad80f3d9"},"cell_type":"code","source":"X_test","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-09-28T13:17:58.911014Z","start_time":"2018-09-28T13:17:58.908897Z"},"_uuid":"ab14150beb8cb24c66881e9cb057476a79c4a236"},"cell_type":"markdown","source":"## 正規化 / Normalization\n先の通り、最初はTrainとTestを個別に処理していた。正規化処理についてはその際コード化していた。その後一体処理に変えたので、Train/Test分割を経て正規化処理となっている。正規化からの分割の方が良い気もするが、、、\n\nAs stated above, I processed train and test separately at first. Normalization codes was made at the time. After that, I changed to process them together so that normalization was processed after splitting them. Although I should change the sequence,..."},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T09:51:34.088591Z","start_time":"2018-11-24T09:51:34.057814Z"},"scrolled":false,"trusted":true,"_uuid":"f59fc913b2509fac3ff9387670dca920e71a3e7b"},"cell_type":"code","source":"# print( type(X_train) )\n# print( X_train.describe() )\nX_train.mean()\nprint(\"Pclass: \", pd.unique( X_train[\"Pclass\"] ))\nprint(\"Sex: \", pd.unique( X_train[\"Sex\"] ))\nprint(\"Age: \", pd.unique( X_train[\"Age\"] ))\n# print(\"Fare: \", pd.unique( X_train[\"Fare\"] ))\nprint(\"FareAdj: \", pd.unique( X_train[\"FareAdj\"] ))\nprint(\"Embarked: \", pd.unique( X_train[\"Embarked\"] ))\nprint(\"Title: \", pd.unique( X_train[\"Title\"] ))\n# print(\"IsAlone: \", pd.unique( X_train[\"IsAlone\"] ))\nprint(\"FamilySize: \", pd.unique( X_train[\"FamilySize\"] ))\nprint(\"Family_S_C: \", pd.unique( X_train[\"Family_S_C\"] ))","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T09:51:41.267503Z","start_time":"2018-11-24T09:51:41.251609Z"},"scrolled":true,"trusted":true,"_uuid":"222c5d4b8aea9e8ffb9b5d73464aea2cc4ab9dbc"},"cell_type":"code","source":"print( X_train.columns )","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T09:51:44.409839Z","start_time":"2018-11-24T09:51:44.382260Z"},"trusted":true,"_uuid":"264ece615c15fe122aaf9050eb0f05a3a79fb38f"},"cell_type":"code","source":"print( X_train[\"FamilySize\"].unique() )\nprint( X_test[\"FamilySize\"].unique() )\nprint( X_train[\"Family_S_C\"].unique() )\nprint( X_test[\"Family_S_C\"].unique() )","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T09:52:15.554114Z","start_time":"2018-11-24T09:52:15.529664Z"},"trusted":true,"_uuid":"cf50de648ffe8444789d87bda94e52bca753c1c2"},"cell_type":"code","source":"scaler_for_family = MinMaxScaler()\n# scaler_for_family.fit( X_train[['FamilySize' ]] )\n# xxx = X_train[[\"FamilySize\",\"Family_S_C\"]]\nscaler_for_family.fit( [[0],[11]] )\n\nprint( X_train[[\"FamilySize\",\"Family_S_C\"]].head(12) )\nX_train[[\"FamilySize\",\"Family_S_C\"]] =  scaler_for_family.transform(X_train[[\"FamilySize\",\"Family_S_C\" ]])\nprint( X_train[[\"FamilySize\",\"Family_S_C\"]].head(12) )\n\nprint( X_test[[\"FamilySize\",\"Family_S_C\"]].head(12) )\nX_test[[\"FamilySize\",\"Family_S_C\"]] =  scaler_for_family.transform(X_test[[\"FamilySize\",\"Family_S_C\" ]])\nprint( X_test[[\"FamilySize\",\"Family_S_C\"]].head(12) )","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T09:52:45.521015Z","start_time":"2018-11-24T09:52:45.481778Z"},"trusted":true,"_uuid":"ef3e03fec8cba31e3b7f11b73157ab0712e3315a"},"cell_type":"code","source":"columns_to_transform = X_train.columns\nscaler = MinMaxScaler()\nX_train[[\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Embarked\", \"Title\", \"IsAlone\", \"FareAdj\", \"Simple_S_C\", \"CabinInfo\" ] ] =  scaler.fit_transform(X_train[ [ \"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Embarked\", \"Title\", \"IsAlone\", \"FareAdj\", \"Simple_S_C\", \"CabinInfo\" ]])\n\nprint(\"Pclass: \", pd.unique( X_train[\"Pclass\"] ))\nprint(\"Sex: \", pd.unique( X_train[\"Sex\"] ))\nprint(\"Age: \", pd.unique( X_train[\"Age\"] ))\n# print(\"Fare: \", pd.unique( X_train[\"Fare\"] ))\nprint(\"FareAdj: \", pd.unique( X_train[\"FareAdj\"] ))\nprint(\"Embarked: \", pd.unique( X_train[\"Embarked\"] ))\nprint(\"Title: \", pd.unique( X_train[\"Title\"] ))\n# print(\"IsAlone: \", pd.unique( X_train[\"IsAlone\"] ))\nprint(\"FamilySize: \", pd.unique( X_train[\"FamilySize\"] ))\nprint(\"Family_S_C: \", pd.unique( X_train[\"Family_S_C\"] ))","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T09:53:04.024055Z","start_time":"2018-11-24T09:53:03.894205Z"},"scrolled":false,"trusted":true,"_uuid":"5e138e5c84208dc7378b5a97e70632a24fd6f768"},"cell_type":"code","source":"print( type(X_test) )\nprint( X_test.describe() )\nX_test.mean()\nprint(\"Pclass: \", pd.unique( X_test[\"Pclass\"] ))\nprint(\"Sex: \", pd.unique( X_test[\"Sex\"] ))\nprint(\"Age: \", pd.unique( X_test[\"Age\"] ))\n# print(\"Fare: \", pd.unique( X_test[\"Fare\"] ))\nprint(\"FareAdj: \", pd.unique( X_test[\"FareAdj\"] ))\nprint(\"Embarked: \", pd.unique( X_test[\"Embarked\"] ))\nprint(\"Title: \", pd.unique( X_test[\"Title\"] ))\n# print(\"IsAlone: \", pd.unique( X_test[\"IsAlone\"] ))\nprint(\"FamilySize: \", pd.unique( X_test[\"FamilySize\"] ))\nprint(\"Family_S_C: \", pd.unique( X_test[\"Family_S_C\"] ))","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T09:53:40.864317Z","start_time":"2018-11-24T09:53:40.783252Z"},"scrolled":true,"trusted":true,"_uuid":"071fb98003d8bffac286079535e58ef08fb26777"},"cell_type":"code","source":"X_test[[\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Embarked\",  \"Title\", \"IsAlone\", \"FareAdj\", \"Simple_S_C\", \"CabinInfo\" ] ] =  scaler.fit_transform(X_test[ [  \"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Embarked\",  \"Title\", \"IsAlone\", \"FareAdj\", \"Simple_S_C\", \"CabinInfo\" ]] )\n\n\nprint(\"Pclass: \", pd.unique( X_test[\"Pclass\"] ))\nprint(\"Sex: \", pd.unique( X_test[\"Sex\"] ))\nprint(\"Age: \", pd.unique( X_test[\"Age\"] ))\n# print(\"Fare: \", pd.unique( X_test[\"Fare\"] ))\nprint(\"FareAdj: \", pd.unique( X_test[\"FareAdj\"] ))\nprint(\"Embarked: \", pd.unique( X_test[\"Embarked\"] ))\nprint(\"Title: \", pd.unique( X_test[\"Title\"] ))\n# print(\"IsAlone: \", pd.unique( X_test[\"IsAlone\"] ))\nprint(\"FamilySize: \", pd.unique( X_test[\"FamilySize\"] ))\nprint(\"Family_S_C: \", pd.unique( X_test[\"Family_S_C\"] ))","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T09:53:45.332346Z","start_time":"2018-11-24T09:53:45.285379Z"},"scrolled":true,"trusted":true,"_uuid":"c2e7b4ddb357775ec110fa8d077ef286359e7984"},"cell_type":"code","source":"print(\"type of Y_train: \",type(Y_train) )\nprint( Y_train.describe() )\nprint( Y_train.unique() )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c0e204e96fb562ed235d59817564f6ec028dad84"},"cell_type":"markdown","source":"## データ保存 / Save data"},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T09:56:08.627120Z","start_time":"2018-11-24T09:56:08.598444Z"},"trusted":true,"_uuid":"62caa029fff30e1809a96d278a6a89ad1eacc05c"},"cell_type":"code","source":"pickle_file = \"./titanic.2018-11-24.pickle\"\n\nwith open(pickle_file, 'wb') as f:\n    pickle.dump(X_train, f)\n    pickle.dump(X_test, f)\n    pickle.dump(Y_train, f)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"31659b663b3e95519a64c11d1200893decc35b94"},"cell_type":"markdown","source":"### データ確認 / check data\nI check data before/after cleaning this notebook. My models use \"*11-03.pickle\".\nI dont know why Cabin data have differences. But I left it because it wouldnt be used later."},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T09:56:56.208262Z","start_time":"2018-11-24T09:56:56.189770Z"},"trusted":true,"_uuid":"50e1c9be6ad936e7392befa0f47d6b62a2fcc51c"},"cell_type":"code","source":"# pickle_file03 = \"/home/hiroshisakuma/ml/machine-learning/titanic.2018-11-03.pickle\"\n# pickle_file10 = \"/home/hiroshisakuma/ml/machine-learning/titanic.2018-11-10.pickle\"\n\n# with open(pickle_file03, 'rb') as f:\n#     X_train03 = pickle.load(f)\n#     X_test03 = pickle.load(f)\n#     Y_train03 = pickle.load(f)\n\n# with open(pickle_file10, 'rb') as f:\n#     X_train10 = pickle.load(f)\n#     X_test10 = pickle.load(f)\n#     Y_train10 = pickle.load(f)\n\n# print( X_train03.columns )\n# print( X_train10.columns )\n","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T09:57:01.764500Z","start_time":"2018-11-24T09:57:01.739571Z"},"trusted":true,"_uuid":"b6b050f6eadb4ee943d9ca9426aa26a4d37a230f"},"cell_type":"code","source":"# # lst = ['PassengerId', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Cabin', 'Embarked', 'Title', 'FamilySize', 'IsAlone', 'FareAdj', 'Family_S_C', 'Simple_S_C', 'CabinInfo']\n# lst = X_train03.columns\n# print(lst)\n# for _i, _l in enumerate(lst):\n#     print( _i, (X_train03[_l] == X_train10[_l]).sum() )","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T09:57:12.737923Z","start_time":"2018-11-24T09:57:12.720329Z"},"scrolled":true,"trusted":true,"_uuid":"1daab1532a983de52cdbbfe3fb4b6993d9ac2a68"},"cell_type":"code","source":"# lst = ['Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Cabin', 'Embarked', 'Title', 'IsAlone', 'FamilySize', 'FareAdj', 'Family_S_C', 'Simple_S_C', 'CabinInfo']\n# print(lst)\n# for _i,_l in enumerate(lst):\n#     print( _i, (X_test03[_l] == X_test10[_l]).sum() )\n\n# print( (Y_train03 == Y_train10).sum() )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6341855d32a31c418a1294ed5ced5a874e3e43f9"},"cell_type":"markdown","source":"# keras neural network\nI spent a lot of time but cant archive the accuracy over 0.8 with keras neural network. The best score was 0.79904. I lost my way. I dont know what I could do and what I should do..."},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"bbb747b3af08e417845cd7f462bb5c019fd01d6d"},"cell_type":"code","source":"import tensorflow as tf\nimport keras\nprint(tf.__version__)\nprint(keras.__version__) # 2.1.5","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f7ad358895af5752a4f61502cca1cd33c4bd2e13"},"cell_type":"markdown","source":"# xgboost\nMy cat, his name is Michel, sits on my lap. Michel is one of major cat-names in Japan. Two years ago, he came to my home from an animal shelter with his name Michel when he was 4 years old. I commnunicated with him about my situation. At the time, xgboost came to my mind although I havent used it before. I accept it as his advice.\n\nI refered to the follwoing pages as for xgboost.  \n[KAZ log TechMemo][1] : how to use xgb.cv and choose the best model  \n[Titanic Kaggle Competition – Exploration and XGBoost][2] : as for xgb parameters  \n[Titanic Starter with XGBoost, 173/209 LB][3] : as for xgb parameters   \n\nI executed this part of the code on my PC.  \n- xgboost==0.80\n\n[1]:http://tkzs.hatenablog.com/entry/2015/10/02/092236\n[2]:http://aplunket.com/titanic-kaggle-xgboost/\n[3]:https://www.kaggle.com/numbersareuseful/titanic-starter-with-xgboost-173-209-lb\n"},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T10:01:02.746806Z","start_time":"2018-11-24T10:01:02.740421Z"},"trusted":true,"_uuid":"df0ee5c188e93f7797b9840b4c94f7a1dbcb5bb6"},"cell_type":"code","source":"pickle_file = \"./titanic.2018-11-24.pickle\"\n\nwith open(pickle_file, 'rb') as f:\n    X_train = pickle.load(f)\n    X_test = pickle.load(f)\n    Y_train = pickle.load(f)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T10:01:09.188022Z","start_time":"2018-11-24T10:01:09.175362Z"},"trusted":true,"_uuid":"08d246f4a9543e6e0597c7eca8305406659fd4b8"},"cell_type":"code","source":"data_col= [ \"Sex\", \"Age\", \"FamilySize\", \"FareAdj\", \"Simple_S_C\" , \"CabinInfo\"]\nX_train = X_train[ data_col ]\nX_test = X_test[ data_col ]\n\n# test = pd.read_csv(\"./kaggle.titanic/test.csv\",header=0)\ntest = pd.read_csv(\"../input/titanic/test.csv\",header=0)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T10:02:57.210631Z","start_time":"2018-11-24T10:02:51.407659Z"},"trusted":true,"_uuid":"beb96f43086b7fabe0b671ded64bfa51df752e14"},"cell_type":"code","source":"params={\n#         \"eta\":0.005,\n#         \"learning_rate\" :0.01, \n#         \"n_estimators\":5000, \n# #         \"max_depth\":4,\n#         \"max_depth\":5,\n#         \"min_child_weight\":5, \n#         \"gamma\":0, \n#         \"subsample\":0.8, \n#         \"colsample_bytree\":0.95,\n#         \"reg_alpha\":1e-05,\n#         \"objective\": 'binary:logistic', \n#         \"nthread\":4, \n#         \"scale_pos_weight\":1, \n#         \"seed\":29\n            \"learning_rate\":0.01, \n            \"n_estimators\":5000, \n            \"max_depth\":5,\n            \"min_child_weight\":5, \n            \"gamma\":0, \n            \"subsample\":0.8, \n            \"colsample_bytree\":0.95,\n            \"reg_alpha\":1e-05,\n            \"objective\": 'binary:logistic', \n            \"nthread\":4, \n            \"scale_pos_weight\":1, \n            \"seed\":29\n    }\n\n            \ndtrain = xgb.DMatrix(X_train, label=Y_train)\ncv=xgb.cv(params,dtrain,num_boost_round=200,nfold=10)","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T10:02:58.024213Z","start_time":"2018-11-24T10:02:57.985268Z"},"trusted":true,"_uuid":"0c1a501ae7f831fd095c1f7d681e96908bb35bba"},"cell_type":"code","source":"minid=cv[[\"test-error-mean\"]].idxmin()\ncv[[\"test-error-mean\"]].min()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T10:03:00.663997Z","start_time":"2018-11-24T10:03:00.594698Z"},"trusted":true,"_uuid":"d2e22d6f40071d7059826cb0827db72a94b34695"},"cell_type":"code","source":"cv","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-19T14:47:07.330456Z","start_time":"2018-11-19T14:47:07.321592Z"},"trusted":true,"_uuid":"251c552f216551c22c4356fc6804fa53d4c989ef"},"cell_type":"code","source":"minid[[\"test-error-mean\"]].values[0]","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T10:03:11.796668Z","start_time":"2018-11-24T10:03:11.273218Z"},"scrolled":true,"trusted":true,"_uuid":"8ab610bfb66bbbebea7dccf11fb6cf2a7c5d0cbf"},"cell_type":"code","source":"# bst=xgb.train(params,dtrain,num_boost_round=minid)\nbst=xgb.train(params,dtrain,num_boost_round=minid[[\"test-error-mean\"]].values[0])\ndtest = xgb.DMatrix(X_test)\nypred = bst.predict(dtest)\nanswer = ypred.round().astype(int)\nanswer\n\n# df_test_origin = pd.read_csv('./kaggle.titanic/test.csv')\ndf_test_origin = pd.read_csv('../input/titanic/test.csv')\nsubmit_data =  pd.Series(answer, name='Survived', index=df_test_origin['PassengerId'])\n# submit_data.to_csv('./kaggle.titanic/submit4.csv', header=True)\nsubmit_data.to_csv('./submit.xgboost.csv', header=True)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fcd3c4f97a8aa2bb0b9d7d98b2dc87b6847dfdb4"},"cell_type":"markdown","source":"# keras neural network again"},{"metadata":{"_uuid":"babe02769609fe0c908eadc6fa0d27b3df34a602"},"cell_type":"markdown","source":"## initialization"},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T13:12:56.778436Z","start_time":"2018-11-24T13:12:53.330221Z"},"trusted":true,"_uuid":"5cc26ed95838e5b9dcc28b09af54e05602408c67"},"cell_type":"code","source":"import subprocess\nimport sys\nimport datetime\nimport os\nimport random as rn\n\nimport numpy as np\nimport pandas as pd\nimport pickle\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nimport keras\nfrom keras import regularizers\nfrom keras import backend as K\nfrom keras.callbacks import LearningRateScheduler\nfrom keras.layers import Input, Embedding, LSTM, Dense, concatenate, Dropout\nfrom keras.models import Model, load_model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"25a1407b80fe84568a7223a2f2aa488f2ca47460"},"cell_type":"markdown","source":"## data "},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T13:12:56.875950Z","start_time":"2018-11-24T13:12:56.781217Z"},"trusted":true,"_uuid":"298fb05af83cf89264ee33e8262801f3805a876d"},"cell_type":"code","source":"# pickle_file = \"titanic.2018-11-03.pickle\"\npickle_file = \"./titanic.2018-11-24.pickle\"\n# mygdrive = \"/gdrive/My Drive/\"\n\n# res = subprocess.run([\"uname\",  \"-a\"], stdout=subprocess.PIPE)\n# myenv = res.stdout.split()\n    \n# if myenv[1] != b'hiroshisakuma-ThinkPad-E480':\n#     from google.colab import drive\n#     drive.mount('/gdrive')\n\n\n# if myenv[1] != b'hiroshisakuma-ThinkPad-E480':\n#     print(\"Data Processing in Colab env.\")\n#     from google.colab import drive\n#     drive.mount('/gdrive')\n#     pickle_file = mygdrive + pickle_file\n    \n\n# else:\n    \n#     pickle_file = './' + pickle_file\n\nwith open(pickle_file, 'rb') as f:\n    X_train = pickle.load(f)\n    X_test = pickle.load(f)\n    Y_train = pickle.load(f)\n    ","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T13:12:56.947256Z","start_time":"2018-11-24T13:12:56.880913Z"},"trusted":true,"_uuid":"7bc43aa8800335b4a8721b0cbe916907f660f2ed"},"cell_type":"code","source":"X_train.info()\nprint('_'*40)\nX_test.info()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T13:12:59.790146Z","start_time":"2018-11-24T13:12:59.775775Z"},"trusted":true,"_uuid":"4340fdd055057e80c3a1921ce21afcea502740b6"},"cell_type":"code","source":"X_train = pd.concat([X_train, Y_train], axis=1)\n\nprint( X_train.columns )\nprint( X_test.columns )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d11339f00a64e22c8c037fc410160519fbe9c390"},"cell_type":"markdown","source":"### choosing columns\n[Chris][1] uses six data listed below with his neural network model and archived the accuracy over 0.8\n- Sex + Age + FamilySize + FareAdj + FamilyOneSurvived + FamilyAllDied  \n\n\nI chose the following five data expecting they have the similar effect with Chris' data. \n- \"Sex\", \"Age\", \"FamilySize\", \"FareAdj\", \"Simple_S_C\" \n\nBut I added \"CabinInfo\" for my greediness and spent a lot of time without success. I can not archive accuracy over 0.8.  \n\nI tried various combination of parameters, number of out units, dropout ratio, weights for regularizer, regularizer it selft, number of layers, epochs and learning rate without success. \n\nThere is nothing to do for me furthermore when I asked Michel for an advice.  \"Get back to basics\" came to my mind. I accept it as his adivce for me. \n\n- removed \"CabinInfo\"\n- set 0 to dropout ratio and weights\n- many units and layers  \n\nAnd at last I could archive the accuracy over 0.8 with this simple model.\n\n[1]:https://www.kaggle.com/cdeotte/titanic-deep-net-0-82296"},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T13:13:05.618755Z","start_time":"2018-11-24T13:13:05.592363Z"},"trusted":true,"_uuid":"530e64782021be833a8de54518cd64cb8a5b3ab5"},"cell_type":"code","source":"data_col= [ \"Sex\", \"Age\", \"FamilySize\", \"FareAdj\", \"Simple_S_C\" ]\n\ntrain_targets = Y_train.values\ntrain_data = X_train[ data_col ].values\ntest_data   = X_test[ data_col ].values","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T13:13:07.027080Z","start_time":"2018-11-24T13:13:07.002275Z"},"trusted":true,"_uuid":"7a1a5cd1c021a031704d7b36bcd4dfaefcb640bb"},"cell_type":"code","source":"train_data","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"41df676a63451c7a95b8e402b6949ec29ab43361"},"cell_type":"markdown","source":"## model\nAs for neural network model, I refered to the following page, [fchollet's Predicting house prices: a regression example][1]  \n\n\n\n[1]:https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/3.7-predicting-house-prices.ipynb"},{"metadata":{"_uuid":"6f05665a76d1d39ff94a8328f9eaaeabf3581c92"},"cell_type":"markdown","source":"### build_model()"},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T13:13:35.839463Z","start_time":"2018-11-24T13:13:35.803880Z"},"trusted":true,"_uuid":"12b6befd59ab3f47f63c5e44505f43cfaf2b9e4b"},"cell_type":"code","source":"fn = 0           # number of units of fhe first  dense\ndim_of_hlayer=[] # number of units of for hidden layers\n\n# _id : paramter for ft.set_random_seed\n# _dr : dropout ratio\n# _ac : activation function\n# _kreg : value for regularizers\ndef build_model(_id, _dr, _ac, _kreg): \n    # https://keras.io/getting-started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development\n    os.environ['PYTHONHASHSEED'] = '0'\n    np.random.seed(42)\n    rn.seed(12345)\n    session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n    tf.set_random_seed(_id)\n    sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n    K.set_session(sess)\n    \n    \n    # first layer\n    inputs2 = Input(shape=(train_data.shape[1],))\n    x2 = Dense(fn, activation=_ac, kernel_regularizer=regularizers.l1(_kreg), bias_regularizer=regularizers.l1(_kreg) )(inputs2) \n#     x2 = Dense(fn, activation=_ac, kernel_regularizer=regularizers.l1_l2(_kreg,_kreg) )(inputs2) \n    x2 = Dropout(_dr)(x2)\n\n    # hidden layers if dim_of_hlayer contains values\n    if len(dim_of_hlayer):\n        for _l in dim_of_hlayer:\n            x2 = Dense(_l, activation=_ac, kernel_regularizer=regularizers.l1(_kreg), bias_regularizer=regularizers.l1(_kreg) )(x2)\n#             x2 = Dense(_l, activation=_ac, kernel_regularizer=regularizers.l1_l2(_kreg,_kreg))(x2)\n            x2 = Dropout(_dr)(x2)\n        \n    main_output = Dense(1, activation='sigmoid', name='main_output')(x2)\n    \n    model = Model(inputs=inputs2, outputs=main_output)\n    model.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['accuracy'])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T13:13:37.139101Z","start_time":"2018-11-24T13:13:37.111522Z"},"trusted":true,"_uuid":"0d5a6b0a80b3d48269d0ecdcc2e807b12051a26c"},"cell_type":"code","source":"print( X_train.columns.values )\nprint( X_test.columns.values )\nprint( train_data.shape, test_data.shape )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"37ba7a15f211f7fae34c27b41d2ede362ee0b03e"},"cell_type":"markdown","source":"### check_result"},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T13:13:41.381602Z","start_time":"2018-11-24T13:13:41.300813Z"},"trusted":true,"_uuid":"8f5c56c0725807e4870668a7289ac7bad84b69be"},"cell_type":"code","source":"def check_result(_model,_id):\n  y_test_nn = _model.predict( X_test[ data_col ].values )\n  y_pred = np.round(y_test_nn)\n  result_df = X_test\n  result_df = result_df.reset_index()############################################################\n  result_df = result_df.drop(['FareAdj','Embarked'],  axis=1) \n  result_df = pd.concat([result_df, pd.DataFrame(data=y_pred, columns=[\"Survived\"], dtype='int')], axis=1)\n  print( \"result at \",_id,\" \",result_df.query('Sex==0 & Survived==1').shape, result_df.query('Sex==1 & Survived==1').shape    ,#) #######################\n    ((result_df['Sex']==0) & (result_df['Pclass']==0) & (result_df['Survived']==1)).sum(),\n    ((result_df['Sex']==0) & (result_df['Pclass']==0.5) & (result_df['Survived']==1)).sum(),\n    ((result_df['Sex']==0) & (result_df['Pclass']==1) & (result_df['Survived']==1)).sum(),\n    ((result_df['Sex']==1) & (result_df['Pclass']==0) & (result_df['Survived']==1)).sum(),\n    ((result_df['Sex']==1) & (result_df['Pclass']==0.5) & (result_df['Survived']==1)).sum(),\n    ((result_df['Sex']==1) & (result_df['Pclass']==1) & (result_df['Survived']==1)).sum()\n    )    \n\n  return y_pred, result_df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d5dbef1fea89676ddbc736d8e258834e583c2b07"},"cell_type":"markdown","source":"### run_model\nFive models will be created. Each corresponding to an iteration of cross validation. The last one will be used to predict."},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T13:13:47.504681Z","start_time":"2018-11-24T13:13:47.439205Z"},"trusted":true,"_uuid":"523175b54cb5347f2b000d9f990b00417051d7d7"},"cell_type":"code","source":"k = 5\nnum_epochs = 75\nbatch_size=32\nmodels = []\n\n# _id : paramter for ft.set_random_seed\n# _dr : dropout ratio\n# _ac : activation function\n# _kreg : value for regularizers\n#  they are paraters for build_model()\ndef run_model(_id, _dr, _ac, _kreg): \n    \n    num_val_samples = len(train_data) // k\n    all_scores = []\n    for i in range(k):\n        val_data    = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n        val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n        \n        partial_train_data = np.concatenate( [train_data[:i * num_val_samples], train_data[(i + 1) * num_val_samples:]], axis=0)\n        partial_train_targets = np.concatenate( [train_targets[:i * num_val_samples], train_targets[(i + 1) * num_val_samples:]], axis=0)\n        \n        model = build_model(_id, _dr, _ac, _kreg)\n#         if not models: \n#             model = build_model(_id, _dr, _ac, _kreg)\n        \n        if i == 0:\n            dt_now = datetime.datetime.now()\n            print(dt_now)\n            print( \"part train data & target   val data & target\")\n            print( len(partial_train_data), len(partial_train_targets), len(val_data), len(val_targets) )\n            print( \"# of folds: \", k, \"num_epochs: \", num_epochs, \"batch_size: \", batch_size )\n            model.summary()\n            \n        def step_decay(epoch):\n          x = 0.0001 \n          if epoch >= 25: x = 0.00001\n          if epoch >= 50: x = 0.000001\n          return x\n        \n        lr_decay = LearningRateScheduler(step_decay,verbose=0)\n        history = model.fit(partial_train_data, \n              partial_train_targets,\n              epochs=num_epochs, batch_size=batch_size, verbose=0,\n              validation_data=(val_data, val_targets) ,\n              callbacks=[lr_decay])\n        \n#         if myenv[1] != b'hiroshisakuma-ThinkPad-E480':\n#             model.save('/gdrive/My Drive/tatanic.models/my_model'+str(_id)+\"-\"+str(i)+'.h5')\n#         else:\n#             model.save('./titanic.models/my_model'+str(_id)+\"-\"+str(i)+'.h5')\n        model.save('./my_model'+str(_id)+\"-\"+str(i)+'.h5')\n        models.append(model)\n        ypred, r_df = check_result(model,_id)\n        all_scores.append(history)\n        \n    return all_scores, history","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3b65a851a6d879c17f7edfa85917923fe3681c11"},"cell_type":"markdown","source":"## call model"},{"metadata":{"_uuid":"126b980fe73361f912509c25dbe22420a774732c"},"cell_type":"markdown","source":"### call model on colab\n- google colab \n- tensorflow  1.12.0  \n- keras 2.2.4\n- public score 0.80861"},{"metadata":{"trusted":true,"_uuid":"6177bc132adbae6b4d3d62b06d6768e51d3d2772"},"cell_type":"code","source":"# data_col= [ \"Sex\", \"Age\", \"FamilySize\", \"FareAdj\", \"IsAlone\", \"Simple_S_C\" , \"CabinInfo\"]\n# data_col= [ \"Sex\", \"Age\", \"FamilySize\", \"FareAdj\", \"Simple_S_C\" , \"CabinInfo\"]\ndata_col= [ \"Sex\", \"Age\", \"FamilySize\", \"FareAdj\", \"Simple_S_C\" ]\n\n\nmodels=[];  all_scores=[]; num_epochs=100; fn=50; dim_of_hlayer=[50,50,50,50,50,50]; print(\"I set fn=\",fn) \nfor _i in  [ 0 ]:\n  all_scores, history = run_model(_i, 0, 'relu', 0 ) # random_seed, dropr, activation, weight\n  for _d in all_scores:\n      print( \"acc {:.2}  val_acc {:.2}\".format( np.mean(_d.history['acc']), np.mean(_d.history['val_acc'])  ))\n  models=[]\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8638641cbbc7f0eac0f05732a786d8425201b55d"},"cell_type":"markdown","source":"### call model on my PC\nWhen I executed the above model on my PC, I got different predict.  \nI added one more layer with same output units. \n- ubuntu 18.04 \n- tensorflow 1.11.0  \n- keras 2.2.2\n- public score 0.80861"},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T13:15:04.799221Z","start_time":"2018-11-24T13:13:56.240360Z"},"trusted":true,"_uuid":"37b3473bdad5719b4d1688e6987409992d87b3d7"},"cell_type":"code","source":"# # data_col= [ \"Sex\", \"Age\", \"FamilySize\", \"FareAdj\", \"IsAlone\", \"Simple_S_C\" , \"CabinInfo\"]\n# # data_col= [ \"Sex\", \"Age\", \"FamilySize\", \"FareAdj\", \"Simple_S_C\" , \"CabinInfo\"]\n# data_col= [ \"Sex\", \"Age\", \"FamilySize\", \"FareAdj\", \"Simple_S_C\" ]\n\n# models=[];  all_scores=[]; num_epochs=100; fn=50; dim_of_hlayer=[50,50,50,50,50,50,50]; print(\"I set fn=\",fn) \n# for _i in  [ 0 ]:\n#   all_scores, history = run_model(_i, 0, 'relu', 0 ) # random_seed, dropr, activation, weight\n#   for _d in all_scores:\n#       print( \"acc {:.2}  val_acc {:.2}\".format( np.mean(_d.history['acc']), np.mean(_d.history['val_acc'])  ))\n#   models=[]\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a4dae688b9826a0d242eed540eb7a95425faf6d5"},"cell_type":"markdown","source":"## Post Processing\nTwo keras nn models shares this part of the code."},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T13:15:10.669229Z","start_time":"2018-11-24T13:15:10.364125Z"},"trusted":true,"_uuid":"189109f10647f1ad3f397a8cf85bca1bad62526b"},"cell_type":"code","source":"_w = \"0\"            # paramter for ft.set_random_seed\nmodel_file_id=\"4\"   # last model\n\nhistory = all_scores[int(model_file_id)]\nhistory_dict = history.history\nhistory_dict.keys()\n\nfor _d in all_scores:\n    print( \"acc {:.2}  val_acc {:.2}\".format(\n            np.mean(_d.history['acc']),\n            np.mean(_d.history['val_acc']) \n        ))\n    \nplt.style.use('dark_background')\n\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\n# \"bo\" is for \"blue dot\"\nplt.plot(epochs, loss, 'bo', label='Training loss')\n# b is for \"solid blue line\"\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T13:15:12.052888Z","start_time":"2018-11-24T13:15:11.784670Z"},"trusted":true,"_uuid":"4cfa9aaf86ab0ac407ed6073693f1dea733dcc70"},"cell_type":"code","source":"plt.clf()   # clear figure\nacc_values = history_dict['acc']\nval_acc_values = history_dict['val_acc']\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fa0976e74b2239899a4a2bd10e8f22512d519e77"},"cell_type":"markdown","source":"## read model from file"},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T13:15:22.989810Z","start_time":"2018-11-24T13:15:18.317454Z"},"trusted":true,"_uuid":"4f31bf14588bbe9e88a0128443c9ebb54f1f3788"},"cell_type":"code","source":"# if myenv[1] != b'hiroshisakuma-ThinkPad-E480':\n#     model = load_model('/gdrive/My Drive/tatanic.models/my_model'+_w+'-'+model_file_id+'.h5')\n#     print('/gdrive/My Drive/tatanic.models/my_model'+_w+'-'+model_file_id+'.h5')\n    \n# else:\n#     model = load_model('./titanic.models/my_model'+_w+'-'+model_file_id+'.h5')\n#     print('./titanic.models//my_model'+_w+'-'+model_file_id+'.h5')\n\nmodel = load_model('./my_model'+_w+'-'+model_file_id+'.h5')\nprint('./my_model'+_w+'-'+model_file_id+'.h5')\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"57313140cee88fbd4b8ca85376f3544e790d48df"},"cell_type":"markdown","source":"## Predict"},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T13:15:37.251029Z","start_time":"2018-11-24T13:15:36.955534Z"},"trusted":true,"_uuid":"340de354070cb82bb5af099ba8b1879304930ab2","scrolled":true},"cell_type":"code","source":"y_test_nn = model.predict(  X_test[ data_col ].values, verbose=1 )\ny_pred = np.round(y_test_nn)\n\n# if myenv[1] != b'hiroshisakuma-ThinkPad-E480':\n#     result_df = pd.read_csv(\"/gdrive/My Drive/kaggle.titanic/test.csv\")\n# else:\n#     result_df = pd.read_csv(\"./kaggle.titanic/test.csv\")\n\nresult_df = pd.read_csv(\"../input/titanic/test.csv\")\n\nresult_df = pd.concat([result_df, pd.DataFrame(data=y_pred, columns=[\"Survived\"], dtype='int')], axis=1)\nresult_df = result_df.drop(['Pclass', 'Name', 'Sex', 'Age','SibSp', 'Parch','Ticket','Fare','Cabin','Embarked'],  axis=1) \nresult_df.columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"286dab89478b67064070bed44d75c1a64b953090"},"cell_type":"markdown","source":"## to_csv"},{"metadata":{"ExecuteTime":{"end_time":"2018-11-24T13:16:01.086301Z","start_time":"2018-11-24T13:16:01.063834Z"},"trusted":true,"_uuid":"0d6678518f96b6f210951f52ce8d2472867b362d"},"cell_type":"code","source":"# _t = '2018-11-24v1-0-4'\n\n# if myenv[1] != b'hiroshisakuma-ThinkPad-E480':\n#     result_df.to_csv('/gdrive/My Drive/kaggle.titanic/submission.'+_t+'_'+model_file_id+'.csv', index=False)\n# else:\n#     result_df.to_csv('./kaggle.titanic/submission.'+_t+'_'+model_file_id+'.csv', index=False)\n\nsubmit_data.to_csv('./submit.keras-nn.csv', header=True)","execution_count":null,"outputs":[]}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{"height":"calc(100% - 180px)","left":"10px","top":"150px","width":"375px"},"toc_section_display":true,"toc_window_display":true}},"nbformat":4,"nbformat_minor":1}