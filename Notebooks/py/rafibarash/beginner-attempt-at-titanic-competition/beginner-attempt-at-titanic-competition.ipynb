{"cells":[{"metadata":{"_uuid":"00a2b8c1f3749edca823e90921e4ed67cb9531ba"},"cell_type":"markdown","source":"# Results so far\n\n* reaching around 80% accuracy on training and validation data\n* reaching between 76-77% accuracy on test data"},{"metadata":{"_uuid":"e8409cd54eb124d6acdf4254ed4e4f18a2d27d1d"},"cell_type":"markdown","source":"# TODO\n\n* Seperate train data into train and test partitions\n* Visualize train partition to find patterns\n* Add median age to members with missing age\n* Visualize training process with matplotlib to more efficiently find training values"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n# import libraries\nimport math\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport sklearn\nfrom sklearn import metrics\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom IPython import display\nfrom matplotlib import cm\nfrom matplotlib import gridspec\nfrom matplotlib import pyplot as plt\n\n# Get data\ntrain = pd.read_csv('../input/train.csv')\ntrain = train.reindex(\n    np.random.permutation(train.index)\n)\ntest = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"489beff778ff20947f0b62cf0ab6756f74769649"},"cell_type":"markdown","source":"# Data Visualizations"},{"metadata":{"trusted":true,"_uuid":"bbe17b7e383807f20057b663dcbcc36747070762","_kg_hide-output":false,"_kg_hide-input":false,"collapsed":true},"cell_type":"code","source":"# display.display(train)\n# display.display(train.info())\n# display.display(test.info())\n# display.display(train.head())\n# display.display(train.corr())\n# _ = display.display(train.hist())\n# display.display(train.Pclass)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"# Steps in TF Project\n\nTo write a TensorFlow program based on pre-made Estimators, you must perform the following tasks:\n\n* Create one or more input functions.\n* Define the model's feature columns.\n* Instantiate an Estimator, specifying the feature columns and various hyperparameters.\n* Call one or more methods on the Estimator object, passing the appropriate input function as the source of the data."},{"metadata":{"_uuid":"cd1f46df09530fe063f0721a45de7f39fbc45ddf"},"cell_type":"markdown","source":"### Create input functions"},{"metadata":{"trusted":true,"_uuid":"33507f600ba102d5200b388d4faf8031473dee47","collapsed":true},"cell_type":"code","source":"def eval_input_fn(features, labels=None, batch_size=1):\n    \"\"\"An input function for evaluation or prediction\"\"\"\n    features=dict(features)\n    if labels is None:\n        # No labels, use only features.\n        inputs = features\n    else:\n        inputs = (features, labels)\n    # convert inputs to DataSet\n    dataset = tf.data.Dataset.from_tensor_slices(inputs)\n    # shuffle, repeat, and batch examples\n    return dataset.repeat(1).batch(batch_size)\n\ndef my_input_fn(features, targets=None, batch_size=1, shuffle=True, num_epochs=None):\n    \"\"\"Trains a neural network model.\n  \n    Args:\n      features: pandas DataFrame of features\n      targets: pandas DataFrame of targets\n      batch_size: Size of batches to be passed to the model\n      shuffle: True or False. Whether to shuffle the data.\n      num_epochs: Number of epochs for which data should be repeated. None = repeat indefinitely\n    Returns:\n      Tuple of (features, labels) for next data batch\n    \"\"\"\n    \n    # Convert pandas data into a dict of np arrays.\n    features = dict(features)\n    if targets is None:\n        inputs = features\n    else:\n        inputs = (features, targets)\n\n    # Construct a dataset, and configure batching/repeating.\n    ds = tf.data.Dataset.from_tensor_slices(inputs) # warning: 2GB limit\n    ds = ds.batch(batch_size).repeat(num_epochs)\n    \n    # Shuffle the data, if specified.\n    if shuffle:\n        ds = ds.shuffle(10000)\n    \n    # Return the next batch of data.\n    features, labels = ds.make_one_shot_iterator().get_next()\n    return features, labels","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"275c9338f593f46ec1310e70301ead15a7ed9796"},"cell_type":"markdown","source":"### Define model's feature columns"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2261938a2834437cf155e2b301ac9ebac3eebd87"},"cell_type":"code","source":"def linear_scale(series):\n    min_val = series.min()\n    max_val = series.max()\n    scale = (max_val - min_val) / 2.0\n    return series.apply(lambda x:((x - min_val) / scale) - 1.0)\n\ndef log_normalize(series):\n    return series.apply(lambda x:math.log(x+1.0))\n\ndef clip(series, clip_to_min, clip_to_max):\n    return series.apply(lambda x:(\n        min(max(x, clip_to_min), clip_to_max)))\n\ndef z_score_normalize(series):\n    mean = series.mean()\n    std_dv = series.std()\n    return series.apply(lambda x:(x - mean) / std_dv)\n\ndef binary_threshold(series, threshold):\n    return series.apply(lambda x:(1 if x > threshold else 0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2a2f38d25c555c61333953bb179947266e823654","collapsed":true},"cell_type":"code","source":"def preprocess_features(df):\n    x = pd.DataFrame(df, columns=['Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked'])\n    # Fill missing values\n    x['Age'].fillna(x['Age'].median(), inplace=True)\n    x['Fare'].fillna(x['Fare'].median(), inplace=True)\n    x['Embarked'].fillna(x['Embarked'].mode()[0], inplace=True)\n    # Add some synthetic features\n    x['FamilySize'] = x['Parch'] + x['SibSp']\n    x['IsAlone'] = x['FamilySize'].apply(lambda n: 1 if n == 0 else 0)\n    x['Title'] = x['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0] # get prefix title from name\n    x['Title'] = x['Title'].apply(lambda s: 'Misc' if x.Title.value_counts()[s] < 10 else s) # make prefix = 'Misc' if >10\n    # Bin a couple features too\n    bucket_labels=['0', '1', '2', '3', '4', '5']\n    x['AgeBin'] = pd.cut(x['Age'].astype(int), 6, labels=bucket_labels)\n    x['FareBin'] = pd.qcut(x['Fare'].astype(int), 6, labels=bucket_labels)\n    x['FamilySizeBin'] = pd.cut(x['FamilySize'].astype(int), 6, labels=bucket_labels)\n    # One-hot encode some features\n    x = pd.get_dummies(x, columns=['Sex', 'Embarked', 'AgeBin', 'FareBin', 'FamilySizeBin'])\n    # Drop bad features\n    x.drop(['Name', 'SibSp', 'Parch', 'Age', 'Fare', 'Title'], axis=1, inplace=True)\n    return x\n\ndef preprocess_targets(df):\n    return df['Survived']\n\ndef construct_feature_columns(input_features):\n    return set([tf.feature_column.numeric_column(my_feature) for my_feature in input_features])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"630fdc08bf5ac9c8c9cade798926b6da86e554cc"},"cell_type":"markdown","source":"### Instantiate an estimator"},{"metadata":{"trusted":true,"_uuid":"f8955be5941cf360b918e167022d66b502748b14","collapsed":true},"cell_type":"code","source":"def train_dnn_classifier(\n    steps,\n    batch_size,\n    hidden_units,\n    train_x,\n    train_y,\n    validate_x=None,\n    validate_y=None,\n    validate=False,\n    dropout=0):\n    '''Trains neural network regression model'''\n    \n    periods = 10\n    steps_per_period = steps / periods\n    \n    # Create DNN classifier object\n#     my_optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n    my_optimizer = tf.train.AdamOptimizer(learning_rate=0.0007)\n    my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n    classifier = tf.estimator.DNNClassifier(\n        feature_columns = construct_feature_columns(train_x),\n        hidden_units = hidden_units,\n        optimizer = my_optimizer,\n        dropout = dropout\n    )\n    \n    # Create input functions\n    train_input_fn = lambda: my_input_fn(train_x, train_y, batch_size)\n    predict_train_input_fn = lambda: my_input_fn(train_x, train_y, num_epochs=1, shuffle=False)\n    predict_validate_input_fn = lambda: my_input_fn(validate_x, validate_y, num_epochs=1, shuffle=False)\n    \n    # Train model in loop to periodically assess\n    print(\"Training model...\")\n    print(\"LogLoss error (on train/validation data):\")\n    training_log_losses = []\n    validation_log_losses = []\n    for period in range (0, periods):\n        # Train the model, starting from the prior state.\n        classifier.train(\n            input_fn=train_input_fn,\n            steps=steps_per_period\n        )\n\n        # Take a break and compute probabilities.\n        training_probabilities = classifier.predict(input_fn=predict_train_input_fn)\n        training_probabilities = np.array([item['probabilities'] for item in training_probabilities])\n        training_log_loss = metrics.log_loss(train_y, training_probabilities)\n        training_log_losses.append(training_log_loss)\n        print(\"  train period %02d : %0.3f\" % (period, training_log_loss))\n        \n        if validate:\n            validation_probabilities = classifier.predict(input_fn=predict_validate_input_fn)\n            validation_probabilities = np.array([item['probabilities'] for item in validation_probabilities])\n            validation_log_loss = metrics.log_loss(validate_y, validation_probabilities)\n            validation_log_losses.append(validation_log_loss)\n            print(\"  validation period %02d : %0.3f\" % (period, validation_log_loss))\n    print(\"Model training finished.\")\n    \n    # Output a graph of loss metrics over periods.\n    plt.ylabel(\"LogLoss\")\n    plt.xlabel(\"Periods\")\n    plt.title(\"LogLoss vs. Periods\")\n    plt.tight_layout()\n    plt.plot(training_log_losses, label=\"training\")\n    plt.plot(validation_log_losses, label=\"validation\")\n    plt.legend()\n\n    return classifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"604a9da1ede0f19b1b8ffd3d07b574b79becbde6","collapsed":true},"cell_type":"code","source":"n = len(train.index)\nclassifier = train_dnn_classifier(1000, 20, [10, 10], preprocess_features(train.head(int(n*0.8))), preprocess_targets(train.head(int(n*0.8))), preprocess_features(train.tail(int(n*0.2))), preprocess_targets(train.tail(int(n*0.2))), validate=True, dropout=0.1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aa92ab311f545a16f9d6eab4dd1eba27e4d1ee58","collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0880c8e644021568ca59f4f9626f9956b7ffcdc1"},"cell_type":"markdown","source":"# Train model (evaluation)"},{"metadata":{"trusted":true,"_uuid":"55cb397ec8bb65bad12ac0a3a2f938f4667f0057","collapsed":true},"cell_type":"code","source":"def main_evaluation(hidden=[80, 80], drop=0.2):\n    # Choose the first 12000 (out of 17000) examples for training.\n    n = len(train.index)\n    train_x = preprocess_features(train.head(int(n*0.8)))\n    train_y = preprocess_targets(train.head(int(n*0.8)))\n\n    # Choose the last 5000 (out of 17000) examples for validation.\n    validate_x = preprocess_features(train.tail(int(n*0.2)))\n    validate_y = preprocess_targets(train.tail(int(n*0.2)))\n\n    # instantiate classifier\n    classifier = train_dnn_classifier(\n        steps=1300,\n        batch_size=50,\n        hidden_units=hidden,\n        train_x=train_x,\n        train_y=train_y,\n        validate_x=validate_x,\n        validate_y=validate_y,\n        validate=True,\n        dropout=drop)\n    \n    # evaluate model\n    train_eval_metrics = classifier.evaluate(input_fn = lambda: my_input_fn(train_x, train_y, num_epochs=1, shuffle=False))\n    validate_eval_metrics = classifier.evaluate(input_fn = lambda: my_input_fn(validate_x, validate_y, num_epochs=1, shuffle=False))\n    print(\"AUC on the training set: %0.5f\" % train_eval_metrics['auc'])\n    print(\"Accuracy on the training set: %0.5f\" % train_eval_metrics['accuracy'])\n    print(\"AUC on the validation set: %0.2f\" % validate_eval_metrics['auc'])\n    print(\"Accuracy on the validation set: %0.2f\" % validate_eval_metrics['accuracy'])\n    \n    # plot AUC\n    validation_probabilities = classifier.predict(input_fn = lambda: my_input_fn(validate_x, validate_y, num_epochs=1, shuffle=False))\n    # Get just the probabilities for the positive class.\n    validation_probabilities = np.array([item['probabilities'][1] for item in validation_probabilities])\n    false_positive_rate, true_positive_rate, thresholds = metrics.roc_curve(\n        validate_y, validation_probabilities)\n    plt.figure()\n    plt.plot(false_positive_rate, true_positive_rate, label=\"our model\")\n    plt.plot([0, 1], [0, 1], label=\"random classifier\")\n    _ = plt.legend(loc=2)\n    \n    return [hidden, drop, train_eval_metrics['auc'], train_eval_metrics['accuracy'], validate_eval_metrics['auc'], validate_eval_metrics['accuracy']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d2a9abf2cc70bad3f731cecef54cdbfafba57363","collapsed":true},"cell_type":"code","source":"# hidden = [80, 80]\n# drop = .1\n# configs = []\n# for i in range(6):\n#     for j in range(4):\n#         x = main_evaluation(hidden, drop)\n#         print(x)\n#         configs += x\n#         drop += 0.1\n#     hidden[0] += 10\n#     hidden[1] += 10\n#     drop = 0.1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"12e999c124c76494ee3c8ffa0395f807cafc8584","collapsed":true},"cell_type":"code","source":"# def sort_config(configs):\n#     new_config = []\n#     for config in configs:\n#         new_config += tuple(config)\n#     return sorted(new_config, key = lambda config: config[3] + config[5])\n\n# s = sort_config(configs)\n# for config in s:\n#     print(config)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e79a513df6e510cf5140334c4fbcd33b19c49426"},"cell_type":"markdown","source":"# Train Model (main)"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7ca4b45400a12a1742af35ba71af91257e24f1c5"},"cell_type":"code","source":"def get_submission(classifier, test_x):\n    '''Export predictions on test features to csv'''\n#     predictions = classifier.predict(\n#         input_fn = lambda: my_input_fn(test_x, num_epochs=1, shuffle=False)\n#     )\n    predictions = classifier.predict(\n        input_fn = lambda: eval_input_fn(test_x)\n    )\n    # group survive prediction with ids in submission dataframe\n    submission = pd.DataFrame()\n    ids = test['PassengerId']\n    for pred, p_id in zip(predictions, ids):\n        cur = pd.Series()\n        class_id = pred['class_ids'][0]\n        cur['Survived'] = class_id\n        cur['PassengerId'] = p_id\n        submission = submission.append(cur, ignore_index=True)\n\n    submission['PassengerId'] = submission['PassengerId'].astype(int)\n    submission['Survived'] = submission['Survived'].astype(int)\n\n    return submission\n\ndef main():\n    train_x = preprocess_features(train)\n    train_y = preprocess_targets(train)\n\n    # instantiate classifier\n    classifier = train_dnn_classifier(\n        steps=1300,\n        batch_size=50,\n        hidden_units=[80, 80],\n        train_x=train_x,\n        train_y=train_y,\n        dropout=0.20)\n    \n    test_x = preprocess_features(test)\n    submission = get_submission(classifier, test_x)\n    # download csv\n    submission.to_csv('titanic_submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a8213c450b5a4306ac9e3a56cbca4d2e6e15a48b","collapsed":true},"cell_type":"code","source":"main()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}