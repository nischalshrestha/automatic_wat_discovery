{"cells": [{"source": ["# **Titanic: Machine Learning from Disaster**\n", "## **Objective**"], "metadata": {"_uuid": "c53b28d4cdfcb15cf3c69e9d41824e1658260f69", "_cell_guid": "770a7c8f-4d0d-4db0-af58-01b779282f67"}, "cell_type": "markdown"}, {"source": ["This notebook aims to give an overview about the training dataset from the competition \"Titanic: Machine Learning from Disaster\". Parts of the code I based on the excelent notebook by Sergei Neviadomski (https://www.kaggle.com/neviadomski/titanic-data-exploration-starter).\n", "Now, I included a new part which is an basic introduction to predicting modeling, where I explore some models trying to predict who will survive in this tragic event.\n", "\n", "This notebook is structured as follow:\n", "1. Loading data\n", "2. Basic dataset statistics\n", "3. Checking for missing values\n", "4. Data visualization\n", "        4.1 Label feature\n", "        4.2 Numeric features\n", "        4.3 Ordinal features\n", "        4.4 Nominal features\n", "        4.5 Correlation between attributes\n", "5. Dealing with missing values\n", "6. Encoding\n", "7. Predictive Modeling\n", "8. Analysis of Models\n", "        8.1 Feature Importance\n", "        8.2 Fine Tuning XGBoost\n", "        8.3 Fine Tuning SVM rbf\n", "        8.4 Fine Tuning Knn\n", "        8.5 Emsembling Models\n", "9. Predicting Test Dataset\n", "        9.1 Preparing Test Dataset\n", "        9.2 Predicting and Submitting"], "metadata": {"_uuid": "33ad48243ab0e4a3c976f1d14f3cca2e062ad8bf", "_cell_guid": "4cbb2e84-a99b-4af6-9b82-759e3b7dd86a"}, "cell_type": "markdown"}, {"outputs": [], "source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "import seaborn as sns\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline\n", "\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "\n", "# Any results you write to the current directory are saved as output."], "metadata": {"_uuid": "f9782098733a161e500ecadca272c9a22cba9162", "_cell_guid": "740a8d04-1b72-4995-abc6-d476020ca1ad"}, "execution_count": null, "cell_type": "code"}, {"source": ["# **1. Loading data**"], "metadata": {"_uuid": "d30e8f12b5c0df32dce0dac0f7aeeedb08278ced", "_cell_guid": "6b5f5d1e-861e-4c0a-a0bf-42c67a3eb101"}, "cell_type": "markdown"}, {"outputs": [], "source": ["#Reading data from CSV file\n", "df_train = pd.read_csv('../input/train.csv')\n", "df_test = pd.read_csv('../input/test.csv')"], "metadata": {"collapsed": true, "_uuid": "e08ed963a0a809d9e1629f18a787111dec60fecf", "_cell_guid": "41113da6-e47a-4cab-a1b4-1bb1d0a0bcc6"}, "execution_count": null, "cell_type": "code"}, {"source": ["# **2. Basic dataset statistics**"], "metadata": {"_uuid": "16b5d1b70d04f02143e09fbae8a57682256abeb3", "_cell_guid": "17038da4-72f0-4041-868e-a9c2c28845ed"}, "cell_type": "markdown"}, {"source": ["Let's take a peek in the first lines and some basic statistics of train dataset."], "metadata": {"_uuid": "8ec909bbfbbb7ee1060505c181ebe477df28c6e5", "_cell_guid": "05f52664-51f3-4a76-8f84-82ccd4956fd4"}, "cell_type": "markdown"}, {"outputs": [], "source": ["print(\"The train dataset contains %d examples and %d features\" %(df_train.shape[0], df_train.shape[1]))\n", "print(df_train.head())"], "metadata": {"_uuid": "3d44ef5b0b0b3911870f13a3c886385eb9375c25", "_cell_guid": "c86a03d3-1a2d-4e0f-b715-80ea61e3d461"}, "execution_count": null, "cell_type": "code"}, {"source": ["There are 12 variables: 5 categorical (\"Survived\" - our label; \"Pclass\"; \"Sex\"; \"Cabin\"; and \"Embarked\"); 3 Id (\"PassengerId\"; and \"Name\"; Ticket); 4 numerical (\"Age\"; \"SibSp\"; \"Parch\"; and \"Fare\"). A brief description of these variables: \n", " \n", " - Survived: if the passanger survived or not (0 = No; 1 = Yes)\n", " - Pclass: ticket class (1 = 1st; 2 = 2nd; 3 = 3rd)\n", " - sex: sex of passanger (male or female)\n", " - Age: age in years\n", " - sibsp: number of siblings / spouses aboard the Titanic\n", " - parch: number of parents / children aboud the Titanic\n", " - ticket: ticket number\n", " - fare: passanger fare\n", " - cabin: cabin number\n", " - Embarked: port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)\n", " \n", "There are a total of 891 examples to train our model."], "metadata": {"_uuid": "dcaadc6e2ea38a0f7fc4909f168aa93bb20677bf", "_cell_guid": "cd35459f-5742-4f00-9252-0c7d6ed72545"}, "cell_type": "markdown"}, {"outputs": [], "source": ["df_train.describe()"], "metadata": {"_uuid": "f844a89ea02e06b16e24540000d34b24b2ed13eb", "_cell_guid": "1466cbbf-18ed-4bd5-a745-6654b0053557"}, "execution_count": null, "cell_type": "code"}, {"source": ["# **3. Checking for missing values**"], "metadata": {"_uuid": "92a84c18f446f151c6ec76e3212edb35fc4acc5c", "_cell_guid": "ed63890f-a138-4515-8b8a-6ba605c11eaf"}, "cell_type": "markdown"}, {"source": ["Let's check for missing values in the train dataset."], "metadata": {"_uuid": "b7329af0c8c0634b06e9e360c64dd01c75f4c831", "_cell_guid": "dbfa1987-47e7-44a9-ab1a-cd8cfd75b038"}, "cell_type": "markdown"}, {"outputs": [], "source": ["missing_df = pd.DataFrame(columns=[\"# missing\", \"% missing\"])     #create two new columns\n", "missing_df[\"# missing\"] = df_train.isnull().sum()                 #compute number of missing values for each att\n", "missing_df[\"% missing\"] = missing_df[\"# missing\"]/df_train.shape[0] * 100      #compute percentage of missing values\n", "print(missing_df)"], "metadata": {"_uuid": "f8821f956131077e56a3b299f7813f85350695ab", "_cell_guid": "cd855006-5876-4cf7-b516-a6d4a608ab72"}, "execution_count": null, "cell_type": "code"}, {"source": ["There are a large number of missing values in the age (~20%) and in cabin (~77%). Some features don't affect the prediction, such as \"PassengerId\", \"Name\" and \"Ticket\". Together with \"Cabin\", which has too many missing values, we'll drop those attributes."], "metadata": {"_uuid": "6a316d8ed5a9c7b8e4b1e731a5c08dd19978af4d", "_cell_guid": "bd949296-57d3-4f13-a5bb-9e5b6d6e9491"}, "cell_type": "markdown"}, {"outputs": [], "source": ["to_drop = [\"Name\", \"Ticket\", \"Cabin\"]     #columns to be dropped\n", "    \n", "df_train.drop(to_drop, axis=1, inplace=True);            #drop columns from train dataset\n", "df_test.drop(to_drop, axis=1, inplace=True);             #drop columns from test dataset"], "metadata": {"collapsed": true, "_uuid": "72e985c2ba848ec1377881a08f8afbaaef64259a", "_cell_guid": "78d34846-cd17-49f1-a05f-7b03df1e9ce4"}, "execution_count": null, "cell_type": "code"}, {"source": ["# **4. Data visualization**"], "metadata": {"_uuid": "418e176883ef0f7d35b214dacc844ebf252ac9fd", "_cell_guid": "4c15b15c-25d8-4878-9c11-082650d0afec"}, "cell_type": "markdown"}, {"outputs": [], "source": ["# Separating features by type\n", "\n", "# Id feature\n", "ID_feat = [\"PassengerId\"]\n", "\n", "# Target feature\n", "target_feat = [\"Survived\"]\n", "\n", "# Numeric features\n", "num_feat = [\"Age\", \"Fare\"]\n", "\n", "# Categorical Features\n", "ordinal_feat = [\"Pclass\", \"SibSp\", \"Parch\"]\n", "nominal_feat = [\"Sex\", \"Embarked\"]\n", "\n", "df_train[\"target_name\"] = df_train[\"Survived\"].map({0: \"Not Survived\", 1: \"Survived\"})\n", "print(\"The following features were separated by their types: \")\n", "print(\"\\tId: \" + str(ID_feat))\n", "print(\"\\tTarget: \" + str(target_feat))\n", "print(\"\\tNumeric features: \" + str(num_feat) + \"\\n\", end=\"\")\n", "print(\"\\tOrdinal features: \" + str(ordinal_feat) + \"\\n\", end=\"\")\n", "print(\"\\tNominal features: \" + str(nominal_feat) + \"\\n\", end=\"\")"], "metadata": {"_uuid": "14a8d06372122006c0d9b0bb3c6e50e51d5cafc1", "_cell_guid": "60fc6e41-e0d3-4bf2-a979-d879f1d592fa"}, "execution_count": null, "cell_type": "code"}, {"source": ["> ## **4.1 Label feature**"], "metadata": {"_uuid": "76d1fa45dc2f63ce3ead0df87e12f1556100fef4", "_cell_guid": "2edab3bf-1226-44cb-8250-91f51e87a443"}, "cell_type": "markdown"}, {"source": ["Learning about the label: Survived. Let's plot the distribution for \"Survived\"."], "metadata": {"_uuid": "500553a14468cce67c5eeb054b5bb250d113969b", "_cell_guid": "9074f889-648f-49b6-8d07-9d66bcb3e45d"}, "cell_type": "markdown"}, {"outputs": [], "source": ["ax = sns.countplot(df_train[target_feat[0]]);\n", "\n", "###A dding percents over bars\n", "# Getting heights of our bars\n", "height = [p.get_height() if p.get_height()==p.get_height() else 0 for p in ax.patches]\n", "\n", "# Counting number of bar groups\n", "ncol = int(len(height)/2)\n", "\n", "# Counting total height of groups\n", "total = [height[i] + height[i + ncol] for i in range(ncol)] * 2\n", "\n", "# Looping through bars\n", "for i, p in enumerate(ax.patches):    \n", "    # Adding percentages   \n", "    ax.text(p.get_x()+p.get_width()/2, height[i]*1.01 + 10,\n", "                '{:1.0%}'.format(height[i]/total[i]), ha=\"center\", size=14) \n", "\n", "plt.xlabel(\"Survived\");\n", "plt.ylabel(\"Occurrences\");"], "metadata": {"_uuid": "a0017c38c17379aa719c0a96814fc15df64ee0b4", "_cell_guid": "7a9efd67-8db5-4a27-9d72-8690b56c6cc6"}, "execution_count": null, "cell_type": "code"}, {"source": ["The majority of people didn't survived. By only guessing, we could get an accuracy of 62% (guessing that everyone had died). Therefore, our model should perform better than this"], "metadata": {"_uuid": "b7eb602221f38c4c7abc3b872efdb83506daa2c4", "_cell_guid": "f4519d05-3214-4abf-944f-04a29b8cc66e"}, "cell_type": "markdown"}, {"source": ["## **4.2 Numeric features**"], "metadata": {"_uuid": "845e29572796421eafd6dce243d506e0a9a4ee2d", "_cell_guid": "11e593a8-c6f4-4d3c-80f1-1ea845a39768"}, "cell_type": "markdown"}, {"outputs": [], "source": ["#Plotting numeric features\n", "\n", "for column in num_feat:\n", "    fig = plt.figure(figsize=(18, 12));\n", "\n", "    sns.distplot(df_train[column].dropna(), ax = plt.subplot(221));\n", "    plt.xlabel(column, fontsize=14);\n", "    plt.ylabel(\"Density\", fontsize=14);\n", "    plt.suptitle(\"Plots for \" + column, fontsize=18);\n", "    \n", "    sns.distplot(df_train.loc[df_train.Survived==0, column].dropna(),\n", "                color = \"red\", label=\"Not survived\", ax = plt.subplot(222))\n", "    sns.distplot(df_train.loc[df_train.Survived==1, column].dropna(),\n", "                color = \"blue\", label=\"Survived\", ax = plt.subplot(222))\n", "    plt.legend(loc=\"best\")\n", "    plt.xlabel(column, fontsize=14)\n", "    plt.ylabel(\"Density\", fontsize=14)\n", "    \n", "    sns.barplot(x=\"target_name\", y=column, data=df_train, ax=plt.subplot(223))\n", "    plt.xlabel(\"Survived\", fontsize=14)\n", "    plt.ylabel(\"Average \" + column, fontsize=14)\n", "    \n", "    sns.boxplot(x=\"target_name\", y=column, data=df_train, ax=plt.subplot(224))\n", "    plt.xlabel(\"Survived\", fontsize=14)\n", "    plt.ylabel(column, fontsize=14)\n", "    plt.show()"], "metadata": {"_uuid": "80eaffb91c2b680a95957b2168a8942e4d20f483", "_cell_guid": "20e610b4-9fdc-498f-ae07-3142b78b332e"}, "execution_count": null, "cell_type": "code"}, {"source": ["Children have higher change of surviving. The average age of one that survived is slightly lower than the one who didn't. People who paid higher fares also have more chance of surviving. The average fare for the ones who survived are more than the double compared to the ones who didn't."], "metadata": {"_uuid": "74b730e4406a2ebf57b180b52aa7287edd40e2c1", "_cell_guid": "bd2e8d71-93ad-4a96-8e3a-76579dbb12c5"}, "cell_type": "markdown"}, {"source": ["##  **4.3 Ordinal features**"], "metadata": {"_uuid": "4efe397a58400364b4b2b506b6df3aa3983a9695", "_cell_guid": "70d839e9-40d1-4e68-bc8d-78b251401bfa"}, "cell_type": "markdown"}, {"outputs": [], "source": ["###Plotting Ordinal Features\n", "for column in ordinal_feat:\n", "    #Figure initiation\n", "    fig = plt.figure(figsize=(18, 18));\n", "    \n", "    sns.barplot(x=\"target_name\", y = column, data = df_train, ax=plt.subplot(321))\n", "    plt.xlabel(\"Survived\", fontsize = 14)\n", "    plt.ylabel(\"Average \" + column, fontsize = 14)\n", "    plt.suptitle(\"Plots for \" + column, fontsize = 18)\n", "    \n", "    sns.boxplot(x=\"target_name\", y=column, data=df_train, ax=plt.subplot(322))\n", "    plt.xlabel(\"Survived\", fontsize=14)\n", "    plt.ylabel(column, fontsize=14)\n", "    \n", "    ax = sns.countplot(x=column, hue=\"target_name\", data=df_train, ax=plt.subplot(312))\n", "    plt.xlabel(column, fontsize=14)\n", "    plt.ylabel(\"Number of occurences\", fontsize=14)\n", "    plt.legend(loc =1)\n", "    \n", "    ### Adding percents over bars\n", "    # Getting heights of our bars\n", "    height = [p.get_height() if p.get_height()==p.get_height() else 0 for p in ax.patches]\n", "    # Counting number of bar groups \n", "    ncol = int(len(height)/2)\n", "    # Counting total height of groups\n", "    total = [height[i] + height[i + ncol] for i in range(ncol)] * 2\n", "    # Looping through bars\n", "    for i, p in enumerate(ax.patches):    \n", "        # Adding percentages   \n", "        ax.text(p.get_x()+p.get_width()/2, height[i]*1.01 + 10,\n", "                '{:1.0%}'.format(height[i]/total[i]), ha=\"center\", size=14)\n", "        \n", "    sns.pointplot(x=column, y=\"Survived\", data=df_train, ax=plt.subplot(313));\n", "    plt.xlabel(column, fontsize=14)\n", "    plt.ylabel(\"Survived Percentage\", fontsize=14)\n", "    plt.show()\n"], "metadata": {"_uuid": "83ab75f0de4387fedf449d7a38be056f3e10e560", "_cell_guid": "e4601a8a-d720-48cf-a4fc-d9c530327fda"}, "execution_count": null, "cell_type": "code"}, {"source": ["People from 1st class have a higher change of surviving, while people from the lowest class have more chance of not surviving. The class with highest percentage of surviving is 1st.\n", "\n", "People with fewer siblings aboard have higher change of surviving. Above 3 siblings aboard, the chances of surviving drop significantly. The only number of siblings that have higher chance of surviving is 1 sibling (yet not very substantial).\n", "\n", "People with higher number of parents/children aboad have better chances of surviving. Above 3 parents/children aboard, the changes of surviving is almost none. There is no much difference in surviving or not in the range of 0 to 4 parents/children aboard."], "metadata": {"_uuid": "ca6c485e60f42594b3da1f2516d1fe14a1482bd0", "_cell_guid": "6f74755d-1a84-44e9-812e-465407ab352f"}, "cell_type": "markdown"}, {"source": ["## **4.4 Nominal features**"], "metadata": {"_uuid": "8f3ec91fa440c34003eaa0b32e9876058cb0e7b2", "_cell_guid": "9fbf02c5-5040-4654-ac0b-f0e7dc0efa5c"}, "cell_type": "markdown"}, {"outputs": [], "source": ["#Plotting nominal features\n", "\n", "for column in nominal_feat:\n", "    #Figure initiation\n", "    fig = plt.figure(figsize=(18, 10));\n", "    \n", "    ax = sns.countplot(x=column, hue=\"target_name\", data=df_train, ax=plt.subplot(221))\n", "    plt.xlabel(column, fontsize=14)\n", "    plt.ylabel(\"Number of occurrences\", fontsize=14)\n", "    plt.suptitle(\"Plots for \" + column, fontsize=18)\n", "    plt.legend(loc=1)\n", "    \n", "    ### Adding percents over bars\n", "    # Getting heights of our bars\n", "    height = [p.get_height() for p in ax.patches]\n", "    # Counting number of bar groups \n", "    ncol = int(len(height)/2)\n", "    # Counting total height of groups\n", "    total = [height[i] + height[i + ncol] for i in range(ncol)] * 2\n", "    # Looping through bars\n", "    for i, p in enumerate(ax.patches):    \n", "        # Adding percentages\n", "        ax.text(p.get_x()+p.get_width()/2, height[i]*1.01 + 10,\n", "                '{:1.0%}'.format(height[i]/total[i]), ha=\"center\", size=14) \n", "    \n", "    sns.pointplot(x=column, y=\"Survived\", data=df_train, ax=plt.subplot(222))\n", "    plt.xlabel(column, fontsize=14)\n", "    plt.ylabel(\"Survived Percentage\", fontsize=14)\n", "    plt.show()\n"], "metadata": {"_uuid": "985b425d1d940d2e7cdc210798a36f1490858754", "_cell_guid": "9636a625-ed66-4958-b377-1d69d1b65f50"}, "execution_count": null, "cell_type": "code"}, {"source": ["Among the sex, female seems to have more chances in surviving (74% for women vs only 19% for men). People who embarked at \"C\" por have higher chance of surviving, followed by \"Q\" and \"S\". I don't know how the port of embarkation can influence the chance of survival. Maybe there is something related with the location inside the ship."], "metadata": {"_uuid": "6e872c94d7bdc5a79f2c1ff22103c9d3582d4ea9", "_cell_guid": "b7e4ebf1-cf5b-44c5-9031-629fdb565583"}, "cell_type": "markdown"}, {"source": ["## **4.5 Correlation between attributes**"], "metadata": {"_uuid": "182dc4de2b8718984dd7bbc3a378ab621400af45", "_cell_guid": "20587998-1889-40bf-836f-1540a40e022a"}, "cell_type": "markdown"}, {"outputs": [], "source": ["plt.figure(figsize=(10, 10))\n", "sns.heatmap(df_train.corr(), vmin=-1, vmax=1, annot=True, square=True);"], "metadata": {"_uuid": "4ad947ddbbb199465b33c6a50201882b12cac3a2", "_cell_guid": "7a509089-1a0a-4787-b695-f73872812cf5"}, "execution_count": null, "cell_type": "code"}, {"source": ["From above, \"Survived\" is negative correlated with \"Pclass\": higher class (therefore lower number: 1 is higher class than 2) tends to survived; also is positive correlated with \"Fare\": who paid more for the fare has more chance of surviving. Obviously, the ones who paid more for the fare are the ones who are from higher class.\n", "\n", "\"Patch\" and \"SibSp\" are correlated: who has more parents/children aboard also tends to have more spouse/siblings aboard. Maybe we can create a variable called \"Family size\" = \"Patch\" + \"SibSp\"."], "metadata": {"_uuid": "ed3468c59ac39e2025e30e8dc7861c123335e7ca", "_cell_guid": "953115d2-5e9f-41bc-847e-75aa00ba0253"}, "cell_type": "markdown"}, {"source": ["# **5. Dealing with missing values**"], "metadata": {"_uuid": "7bc0b3ae0d9c529a1258023fe83b2d63b437d772", "_cell_guid": "d0d6de8b-937c-4466-9b4b-d987422dbf92"}, "cell_type": "markdown"}, {"source": ["Let's performe the replacement of missing values only after data visualization, so we don't let be mislead from it."], "metadata": {"_uuid": "8d4d8b776f54b1561778625d14217e71385adcc3", "_cell_guid": "80a2d009-5d79-403a-91d8-2acd6ab79f0a"}, "cell_type": "markdown"}, {"outputs": [], "source": ["#Replace categorical missing value with most frequent category\n", "df_train[\"Embarked\"] = df_train[\"Embarked\"].fillna(df_train[\"Embarked\"].value_counts().index[0])\n", "#Replace numerical missing values with mean\n", "df_train[\"Age\"] = df_train[\"Age\"].fillna(df_train[\"Age\"].mean())"], "metadata": {"collapsed": true, "_uuid": "4f88f3c2851d9931a5ac96c9f5c694f203f82932", "_cell_guid": "d7576ac5-cb11-4195-a74f-43dc667a3c03"}, "execution_count": null, "cell_type": "code"}, {"source": ["Now, we have a dataset without missing values!"], "metadata": {"_uuid": "3bdea0ab2902dbed8c5f276415775a834bc9f777", "_cell_guid": "55967002-369b-4ae2-bc8b-21e8dcd65405"}, "cell_type": "markdown"}, {"source": ["# **6. Encoding**"], "metadata": {"_uuid": "d899a0ba1713b19f0ae217d50c5df2a9f3799d28", "_cell_guid": "2b25346e-6ce8-42e5-b722-6129c68a6909"}, "cell_type": "markdown"}, {"source": ["Let's replace nominal features encoding them.'"], "metadata": {"_uuid": "ec28fb179bba3ffd9997925be39700d99ddf80aa", "_cell_guid": "3a5580db-b980-4ed6-959d-60d216fb2830"}, "cell_type": "markdown"}, {"outputs": [], "source": ["df_train['Sex'].replace(['male', 'female'], [0, 1], inplace = True)\n", "df_train['Embarked'].replace(['S', 'C', 'Q'], [0, 1, 2], inplace=True)"], "metadata": {"collapsed": true, "_uuid": "121b927db838ded5a667fa82322c3f1f4bd03429", "_cell_guid": "faa41930-f01d-4bd0-8eae-b487b8adc83c"}, "execution_count": null, "cell_type": "code"}, {"source": ["Also, I will create a new variable called \"Family Size\", which is the sum of \"Parch\" and \"SibSp\"."], "metadata": {"_uuid": "7567aa65f54a8ec8f95fb8572ef5fcdd5c6d2f37", "_cell_guid": "3388b3ac-7e92-4a1a-95ba-8ddaf28b5c46"}, "cell_type": "markdown"}, {"outputs": [], "source": ["df_train[\"Family_Size\"] = 0\n", "df_train[\"Family_Size\"] = df_train[\"Parch\"] + df_train[\"SibSp\"]"], "metadata": {"collapsed": true, "_uuid": "56a9d21749a47076c4fad3779dee0f3f56d4be4c", "_cell_guid": "1bb886ce-9a03-44a4-881b-5d048af065c0"}, "execution_count": null, "cell_type": "code"}, {"source": ["# 7. Predictive Modeling"], "metadata": {"_uuid": "be6ed87e5c1a66cbac19810ae215ca3a755b1866", "_cell_guid": "993faaeb-fb68-43f4-8d69-7ca156d45df2"}, "cell_type": "markdown"}, {"outputs": [], "source": ["# Importing algorithms\n", "from sklearn.linear_model import LogisticRegression     # Logistic regression\n", "from sklearn.naive_bayes import GaussianNB              # Naive Bayes\n", "from sklearn import svm                                 # Support Vector Classification\n", "from sklearn.tree import DecisionTreeClassifier         # Decision Tree\n", "from sklearn.ensemble import RandomForestClassifier     # Random Forest\n", "import xgboost as xgb                                   # Extreme Gradient Boost\n", "from sklearn.neighbors import KNeighborsClassifier      #Knn\n", "\n", "# Importing preprocessing modules\n", "from sklearn.model_selection import cross_val_score    # Cross validation\n", "from sklearn.model_selection import ShuffleSplit       # Shuffle Split\n", "from sklearn.pipeline import make_pipeline             # Pipeline\n", "from sklearn import preprocessing"], "metadata": {"collapsed": true, "_uuid": "94be7e3d0de6d307b9cf3e07fd7a51f79acd59d5", "_cell_guid": "b9c38c4f-b4c2-4579-90f9-539dc5548c17"}, "execution_count": null, "cell_type": "code"}, {"outputs": [], "source": ["# Separating train dataset into features and label\n", "df_train_X = df_train.drop([\"PassengerId\", \"Survived\", \"target_name\"], axis=1);   # Selecting coluns of features\n", "df_train_Y = df_train[df_train.columns[1]];                       # Selecting label\n", "\n", "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)     # Dividing train dataset into folds\n", "\n", "# List with models to be tested\n", "models = [LogisticRegression(), \n", "          GaussianNB(), \n", "          svm.SVC(kernel=\"linear\"),\n", "          svm.SVC(kernel=\"rbf\"),\n", "          DecisionTreeClassifier(),\n", "          RandomForestClassifier(),\n", "          xgb.XGBClassifier(),\n", "          KNeighborsClassifier()\n", "         ]\n", "\n", "# Create dataframe to store models and their performances\n", "accuracy_models = pd.DataFrame(columns=[\"Model\", \"Accuracy\", \"Std\"])\n", "accuracy_models[\"Model\"] = [\"Logistic Regression\", \n", "                            \"Naive Bayes\", \n", "                            \"SVM linear\", \n", "                            \"SVM rbf\", \n", "                            \"Decision Tree\",\n", "                            \"Randon Forest\", \n", "                            \"XGBoost\", \"Knn\"]\n", "\n", "\n", "j = 0\n", "\n", "# Loop over the models\n", "for model in models:\n", "    clf_model = make_pipeline(preprocessing.StandardScaler(), model);\n", "    scores_model = cross_val_score(clf_model, df_train_X, list(np.ravel(df_train_Y)), cv = cv);\n", "    accuracy_models[\"Accuracy\"][j] = scores_model.mean();\n", "    accuracy_models[\"Std\"][j] = scores_model.std();    \n", "    j += 1\n", "    "], "metadata": {"collapsed": true, "_uuid": "a27771ddaeda8551c2d3d48e5212c4c1e9f14252", "_cell_guid": "e5f19e62-33bb-41c2-8176-ab2e23de40ca"}, "execution_count": null, "cell_type": "code"}, {"outputs": [], "source": ["# Print table with models and performance sorting from the highest accuracy to the lowest\n", "accuracy_models.sort_values(by=\"Accuracy\", ascending=False)"], "metadata": {"_uuid": "3321e7a6f7bac23ae44a412fda743c768a44f0a3", "_cell_guid": "388f8788-c23b-4283-8df5-55a3809601d4"}, "execution_count": null, "cell_type": "code"}, {"source": ["The best 3 models are XGBoost, SVM with kbr kernel and Knn. Remember that Knn performance is highly dependent of the k paramenter. Therefore, further analysis should compute how its performance develops according with k. "], "metadata": {"_uuid": "77b894398b30faf0cd42266a39c8562eb0893679", "_cell_guid": "c7f48c07-3724-44cd-a87a-1d1ab3e66af1"}, "cell_type": "markdown"}, {"source": ["# 8. Analysis of Models"], "metadata": {"_uuid": "2e29df4fcce4faf405f1783b2b1ba4909158aae0", "_cell_guid": "102852df-f95b-4676-8d1c-349b3aae9787"}, "cell_type": "markdown"}, {"source": ["## 8.1 Feature importance"], "metadata": {"_uuid": "741a83d71550a247c018609eff6e2f2aef8791de", "_cell_guid": "772ee7e4-49e7-4451-aa85-4a189e2a7867"}, "cell_type": "markdown"}, {"source": ["From the GXBoost and the Random Forest models, let's check what are the most important features."], "metadata": {"_uuid": "d07dea49b6825a22a2ea3571a2ecd2168015a937", "_cell_guid": "5bffd464-5202-4211-8b69-f23ddaeec123"}, "cell_type": "markdown"}, {"outputs": [], "source": ["fig, ax = plt.subplots(ncols=2, nrows=1, figsize = (15, 5))\n", "\n", "model = RandomForestClassifier()\n", "model.fit(df_train_X, df_train_Y)\n", "pd.Series(model.feature_importances_, df_train_X.columns).sort_values(ascending = True).plot.barh(width = 0.8, ax=ax[0], color = \"b\");\n", "ax[0].set_title(\"Feature importance in Random Forest\");\n", "\n", "model = xgb.XGBClassifier()\n", "model.fit(df_train_X, df_train_Y)\n", "pd.Series(model.feature_importances_, df_train_X.columns).sort_values(ascending = True).plot.barh(width = 0.8, ax=ax[1], color = \"b\");\n", "ax[1].set_title(\"Feature importance in XGBoost\");"], "metadata": {"_uuid": "c88971ccf9a18c4642b5c7cc599874969357bd6b", "_cell_guid": "3de71be5-fce6-49c2-bfcb-652642df631d"}, "execution_count": null, "cell_type": "code"}, {"source": ["In both models, Fare and Age have high importance. If we think about them, we remember that both features have large values if we compare with the range of other attributes. Let's scale only those features and run the feature importance again."], "metadata": {"_uuid": "17dd94a16a5a647e763fb9a5ca6abc32c060d683", "_cell_guid": "544679a2-8afe-4f03-a9ab-103cd5b09ad5"}, "cell_type": "markdown"}, {"outputs": [], "source": ["fig, ax = plt.subplots(ncols=2, nrows=1, figsize = (15, 5))\n", "\n", "# Scaling Age and Fare\n", "df_train[[\"Age\", \"Fare\"]]\n", "scaler = preprocessing.StandardScaler()\n", "scaler.fit(df_train[[\"Age\", \"Fare\"]])\n", "pd.DataFrame(scaler.transform(df_train[[\"Age\", \"Fare\"]]), columns = [\"Age\", \"Fare\"])\n", "df_train_X[[\"Age\", \"Fare\"]] = pd.DataFrame(scaler.transform(df_train[[\"Age\", \"Fare\"]]), columns = [\"Age\", \"Fare\"])\n", "\n", "model = RandomForestClassifier()\n", "model.fit(df_train_X, df_train_Y)\n", "pd.Series(model.feature_importances_, df_train_X.columns).sort_values(ascending = True).plot.barh(width = 0.8, ax=ax[0], color = \"b\");\n", "ax[0].set_title(\"Feature importance in Random Forest\");\n", "\n", "model = xgb.XGBClassifier()\n", "model.fit(df_train_X, df_train_Y)\n", "pd.Series(model.feature_importances_, df_train_X.columns).sort_values(ascending = True).plot.barh(width = 0.8, ax=ax[1], color = \"b\");\n", "ax[1].set_title(\"Feature importance in XGBoost\");"], "metadata": {"_uuid": "89a268dba049dd456be51e92ac4d8fb6eb4354ea", "_cell_guid": "e5d2fa88-3bb0-4daf-bd81-36068296e53c"}, "execution_count": null, "cell_type": "code"}, {"source": ["Even scaling, both features have high importance. While in the Randon Forest the order of importance changed, in XGBoost it doesn't. I believe we can conclude that scaling features in handed internally by XGBoost while for Random Forest doesn't."], "metadata": {"_uuid": "2366ef9df333046183a94f004e9f8166f5ee3d11", "_cell_guid": "f64e4551-a76e-4cf6-9fbd-65ca77d171f9"}, "cell_type": "markdown"}, {"source": ["## 8.2 Fine Tuning XGBoost"], "metadata": {}, "cell_type": "markdown"}, {"outputs": [], "source": ["from sklearn.grid_search import GridSearchCV\n", "\n", "df_train_X = df_train.drop([\"PassengerId\", \"Survived\", \"target_name\"], axis=1);\n", "df_train_Y = df_train[df_train.columns[1]];\n", "\n", "\n", "max_depth_range = list(range(1, 10))\n", "min_child_weight_range = list(range(1, 6))\n", "gamma_range = [i/10 for i in range(0, 5)]\n", "\n", "param_grid = {\"max_depth\": max_depth_range,\n", "              \"min_child_weight\": min_child_weight_range,\n", "              \"gamma\": gamma_range\n", "              }\n", "\n", "grid_xgb = GridSearchCV(estimator=xgb.XGBClassifier(), \n", "                    param_grid=param_grid, \n", "                    cv = 5, \n", "                    scoring='accuracy', \n", "                    refit=True\n", "                   )     #setting grid with estimator\n", "\n", "xgb_model = make_pipeline(preprocessing.StandardScaler(), grid_xgb)    #creating preprocessing\n", "xgb_model.fit(df_train_X, df_train_Y)      #fitting data\n", "\n", "print(\"Accuracy of the tuned model: %.4f\" %grid_xgb.best_score_)\n", "print(grid_xgb.best_params_)"], "metadata": {}, "execution_count": null, "cell_type": "code"}, {"source": ["## 8.3 Fine Tuning SVM rbf"], "metadata": {}, "cell_type": "markdown"}, {"outputs": [], "source": ["C_range = [0.01, 0.1, 1, 10, 100, 1000]       \n", "gamma_range = [0.001, 0.01, 0.1, 1, 10, 100]\n", "\n", "param_grid = {\"C\": C_range,\n", "              \"gamma\": gamma_range\n", "              }         #setting grid of parameters\n", "\n", "grid_svm = GridSearchCV(estimator = svm.SVC(), \n", "                    param_grid = param_grid, \n", "                    cv = 5, \n", "                    scoring = 'accuracy', \n", "                    refit = True)   #setting grid with estimator\n", "\n", "svm_model = make_pipeline(preprocessing.StandardScaler(), grid_svm)     #creating preprocessing\n", "svm_model.fit(df_train_X, df_train_Y)       #fitting data\n", "\n", "print(\"Accuracy of the tuned model: %.4f\" %grid_svm.best_score_)\n", "print(grid_svm.best_params_)"], "metadata": {}, "execution_count": null, "cell_type": "code"}, {"source": ["## 8.4 Fine Tuning Knn"], "metadata": {}, "cell_type": "markdown"}, {"outputs": [], "source": ["weight_functions = [\"uniform\", \"distance\"]\n", "p_values = [1, 2]\n", "n_range = list(range(1, 51))\n", "\n", "param_grid = {\"n_neighbors\": n_range,\n", "              \"weights\": weight_functions,\n", "              \"p\": p_values\n", "              }         #setting grid of parameters\n", "\n", "grid_knn = GridSearchCV(estimator = KNeighborsClassifier(), \n", "                    param_grid = param_grid, \n", "                    cv = 5, \n", "                    scoring = 'accuracy', \n", "                    refit = True)   #setting grid with estimator\n", "\n", "knn_model = make_pipeline(preprocessing.StandardScaler(), grid_knn)     #creating preprocessing\n", "knn_model.fit(df_train_X, df_train_Y)       #fitting data\n", "\n", "print(\"Accuracy of the tuned model: %.4f\" %grid_knn.best_score_)\n", "print(grid_knn.best_params_)"], "metadata": {}, "execution_count": null, "cell_type": "code"}, {"source": ["## 8.5 Emsembling Models"], "metadata": {}, "cell_type": "markdown"}, {"source": ["The only ensemble model I will try is the Voting Classifier."], "metadata": {}, "cell_type": "markdown"}, {"outputs": [], "source": ["from sklearn.ensemble import VotingClassifier\n", "\n", "voting = make_pipeline(preprocessing.StandardScaler(), VotingClassifier(estimators=[(\"XGb\", grid_xgb.best_estimator_),\n", "                                                                                    (\"SVM\", grid_svm.best_estimator_),\n", "                                                                                    (\"knn\", grid_knn.best_estimator_)]));\n", "#scores_model = cross_val_score(clf_model, df_train_X, list(np.ravel(df_train_Y)), cv = cv);\n", "\n", "#voting = VotingClassifier(estimators=[(\"XGb\", grid_xgb.best_estimator_),\n", "#                                      (\"SVM\", grid_svm.best_estimator_),\n", "#                                      (\"knn\", grid_knn.best_estimator_)])\n", "\n", "voting.fit(df_train_X, df_train_Y);\n", "\n", "scores_model_ensemble = cross_val_score(voting, df_train_X, list(np.ravel(df_train_Y)), cv = cv);\n", "print(\"Accuracy of ensemble model: %.3f (+/- %.2f)\" %(scores_model_ensemble.mean(), scores_model_ensemble.std()))"], "metadata": {}, "execution_count": null, "cell_type": "code"}, {"source": ["# 9. Predicting test dataset"], "metadata": {"_uuid": "967db9ba50defc309798292eeb6e5d9a8d57bf4f", "_cell_guid": "19b707bc-49f1-43bc-81ce-621078a19d8d"}, "cell_type": "markdown"}, {"source": ["To run the same model I developed previously, I need to prepare the test dataset similarly. Thus, let's run the same operations we did for the train dataset: \n", "1. Checking for missing values\n", "2. Drop same features we dropped from the train dataset (done in step 3)\n", "3. Encoding variables"], "metadata": {"_uuid": "b5f1da6a3ae95dccaa9c9e1d1bb0e64fe941ac4e", "_cell_guid": "6d7d1445-0451-42ff-9148-749d0f49d4cc"}, "cell_type": "markdown"}, {"outputs": [], "source": ["# Checking missing values\n", "missing_df = pd.DataFrame(columns=[\"# missing\", \"% missing\"])     #create two new columns\n", "missing_df[\"# missing\"] = df_test.isnull().sum()                 #compute number of missing values for each att\n", "missing_df[\"% missing\"] = missing_df[\"# missing\"]/df_test.shape[0] * 100      #compute percentage of missing values\n", "print(missing_df)"], "metadata": {"_uuid": "c6d273c7c1395805b5e9179799f025b726ca2702", "_cell_guid": "a87d970a-9597-4914-8bf5-0008c9fbbd82"}, "execution_count": null, "cell_type": "code"}, {"outputs": [], "source": ["#Replace numerical missing values with mean from train dataset\n", "df_test[\"Age\"] = df_test[\"Age\"].fillna(df_train[\"Age\"].mean())\n", "df_test[\"Fare\"] = df_test[\"Fare\"].fillna(df_train[\"Fare\"].mean())"], "metadata": {"collapsed": true, "_uuid": "449b4b0e129b4e1cc8a9443f3c915584a2007803", "_cell_guid": "7dc09419-c0fd-4ea5-a53e-da1f2ad2f56a"}, "execution_count": null, "cell_type": "code"}, {"outputs": [], "source": ["# Checking missing values\n", "missing_df = pd.DataFrame(columns=[\"# missing\", \"% missing\"])     #create two new columns\n", "missing_df[\"# missing\"] = df_test.isnull().sum()                 #compute number of missing values for each att\n", "missing_df[\"% missing\"] = missing_df[\"# missing\"]/df_test.shape[0] * 100      #compute percentage of missing values\n", "print(missing_df)"], "metadata": {"_uuid": "0085136dbd28196e19f2c66655f03e249f190e83", "_cell_guid": "da91a9f0-9816-421a-998a-d54cb81eddf5"}, "execution_count": null, "cell_type": "code"}, {"source": ["No more missing values! Let's encode categorical features as the training dataset."], "metadata": {"_uuid": "365c87fc7493a424a5fa3f0074c7bacaf9b64756", "_cell_guid": "bbc362bc-e90d-4916-a935-5ca007e67129"}, "cell_type": "markdown"}, {"outputs": [], "source": ["# Encoding features\n", "df_test['Sex'].replace(['male', 'female'], [0, 1], inplace = True)\n", "df_test['Embarked'].replace(['S', 'C', 'Q'], [0, 1, 2], inplace=True)\n", "\n", "# Creating new features\n", "df_test[\"Family_Size\"] = 0\n", "df_test[\"Family_Size\"] = df_test[\"Parch\"] + df_test[\"SibSp\"]"], "metadata": {"collapsed": true, "_uuid": "8b3202dd4656e77124e41c71dd879fa7f9e988db", "_cell_guid": "8812aa60-f32f-4795-a072-49d26cb18c1b"}, "execution_count": null, "cell_type": "code"}, {"source": ["## 9.2 Predicting and Submitting"], "metadata": {"_uuid": "f380fefec901f0fa8837a9e586843d745d724812", "_cell_guid": "3962fbbf-2eea-4f60-9d64-6e8ad45c6879"}, "cell_type": "markdown"}, {"source": ["I will use the ensemble model as the final model. It ensembles XGBoost, Knn and SVM with rbf kernel."], "metadata": {}, "cell_type": "markdown"}, {"outputs": [], "source": ["# Predicting\n", "predict = voting.predict(df_test.drop([\"PassengerId\"], axis=1))\n", "\n", "# Saving prediction data to dataframe\n", "submission = pd.DataFrame(columns=[\"PassengerId\", \"Survived\"])\n", "submission[\"PassengerId\"] = df_test.PassengerId\n", "submission[\"Survived\"] = predict\n", "\n", "# Checking prediction\n", "submission.head()"], "metadata": {"_uuid": "ea043e17fc015bba66170d32c6bd93f4eb16ace9", "_cell_guid": "a29f3ca9-d324-4fd2-b5fe-6930b3c0f5e0"}, "execution_count": null, "cell_type": "code"}, {"outputs": [], "source": ["# Saving CSV\n", "submission.to_csv(\"../working/submit.csv\", index=False)\n", "#submission.to_csv(\"Submission_XGBoost.csv\", index=False)"], "metadata": {"collapsed": true, "_uuid": "5246cedc2ff13872b48bc089f1d2aa5251a8475b", "_cell_guid": "ae18c705-c20b-475c-be83-ee509f7dfda2"}, "execution_count": null, "cell_type": "code"}, {"outputs": [], "source": [], "metadata": {"collapsed": true, "_uuid": "f01d34fe58acc2f9a5a8028da2c93401a086a16c", "_cell_guid": "1eb59a67-82b8-44fa-8c86-add31a4106e9"}, "execution_count": null, "cell_type": "code"}], "nbformat": 4, "metadata": {"language_info": {"nbconvert_exporter": "python", "name": "python", "pygments_lexer": "ipython3", "version": "3.6.3", "file_extension": ".py", "codemirror_mode": {"name": "ipython", "version": 3}, "mimetype": "text/x-python"}, "kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}}, "nbformat_minor": 1}