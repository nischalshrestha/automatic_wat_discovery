{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "1cc52783-01d6-d737-6070-e0acea8cf5f6"
      },
      "source": [
        "The aim of this notebook is to go through all the commonest techniques in participating in a Kaggle competition. The dataset used in this notebook is from the Kaggle competition [Titanic: Machine Learning from Disaster]( https://www.kaggle.com/c/titanic).\n",
        "\n",
        "I'm still a beginner, so any advice or suggestion is well accepted!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "176da240-96ac-3590-31d5-b609a14a2051"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import linear_model\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn import ensemble\n",
        "from sklearn.metrics import make_scorer, accuracy_score, f1_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn import preprocessing\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import xgboost as xgb\n",
        "%matplotlib inline\n",
        "\n",
        "RANDOM_SEED = 4321\n",
        "np.random.seed = RANDOM_SEED"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "e41ad11b-5cc6-580e-b29f-38cf24e08154"
      },
      "source": [
        "Importing the dataset\n",
        "--\n",
        "\n",
        "We import the train and test csv file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b9eb59c6-6393-4b27-46ac-7bc3183e1c98"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('../input/train.csv')\n",
        "test = pd.read_csv('../input/test.csv')\n",
        "\n",
        "train.info()\n",
        "train.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "dde0abdf-1efc-060b-7716-5da6de44441e"
      },
      "source": [
        "As for the training, we can see that there are some people with an empty age. The Cabin column is at most empty and only 2 entries have a missing Embarked feature.\n",
        "\n",
        "Let us consider the test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "89449908-19ee-e8b0-93a1-796c4c87e3ff"
      },
      "outputs": [],
      "source": [
        "test.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "0e08d870-b782-8d17-9ded-ba120c417adf"
      },
      "source": [
        "Also the test set has similar properties of the training test. We will deal with missing values later.\n",
        "\n",
        "Let us explore the dataset a bit:\n",
        "\n",
        "## Survivals\n",
        "\n",
        "As we can expect, most of the people abroad did not survive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3151e54b-d819-1f94-efea-cf9c8a748745"
      },
      "outputs": [],
      "source": [
        "plt.title('Number of people survived the Titanic', y=1.1, size=15)\n",
        "sns.countplot('Survived', data=train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "062c2012-acba-dd02-6747-e4fb4b2c221c"
      },
      "source": [
        "## Sex:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "15a8e3d5-a567-8822-526a-c1a803f3e147"
      },
      "outputs": [],
      "source": [
        "plt.title('Survival count between sex', size=20, y=1.1)\n",
        "sns.countplot(x = 'Survived', hue='Sex', data=train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "195c30fe-1dda-a556-a19a-e74e949adf81"
      },
      "source": [
        "So we can see that most of the survived people were women. Let's also analyze the survival rate:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "38f26b04-65e8-6b53-69ac-145ec30f4dc5"
      },
      "outputs": [],
      "source": [
        "plt.title('Survival rate between sex', size=20, y=1.1)\n",
        "sns.barplot(x='Sex', y='Survived', data=train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "7ae5037c-a49f-4d6f-d5f7-2f195e17e90d"
      },
      "source": [
        "Since we must deal with numerical feature, we should convert male/female in a binary vector 0/1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "96228b5f-cecf-dc6e-3db7-40e4ff14f637"
      },
      "outputs": [],
      "source": [
        "for df in [train, test]:\n",
        "    df['Sex'] = df['Sex'].apply(lambda x : 1 if x == 'male' else 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "f38440eb-0c61-feaa-adb3-0bc84a5fb93f"
      },
      "source": [
        "So females has an overall higher chance to survive.\n",
        "\n",
        "## PClass:\n",
        "\n",
        "Now let's analyze the Pclass feature:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0a11966f-10aa-4d6e-fde2-623caf1888ad"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 12))\n",
        "plt.subplot(2,2,1)\n",
        "plt.title('Survival rate / Pclass', size=15, y=1.1)\n",
        "sns.barplot(x='Pclass', y = 'Survived', data=train, palette='muted')\n",
        "plt.subplot(2,2,2)\n",
        "plt.title('Count survival / Pclass', size=15, y=1.1)\n",
        "sns.countplot(x='Pclass', hue = 'Survived', data=train, palette='muted')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "0ee68fe0-5b80-69c3-0380-3867b21ca872"
      },
      "source": [
        "## Embarked\n",
        "\n",
        "Now let's dive into the embarked feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9e241887-22a2-1f4d-7f11-4d1faf7debd2"
      },
      "outputs": [],
      "source": [
        "train.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "261b15af-f1d4-82fe-03a9-0f9cf26c570a"
      },
      "outputs": [],
      "source": [
        "sns.countplot(train['Embarked'])\n",
        "train['Embarked'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "93a9a90f-eccb-fe79-c99c-e808266a3104"
      },
      "source": [
        "It seems that most of the people embarked in Southampton. Since it is the most frequent, we will fill the two missing values with 'S'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "18d635dc-7db7-db1c-0b31-79d96398e2ea"
      },
      "outputs": [],
      "source": [
        "train['Embarked'] = train['Embarked'].fillna('S')\n",
        "plt.figure(figsize=(12, 12))\n",
        "plt.subplot(2,2,1)\n",
        "sns.barplot(y='Survived', x='Embarked', data=train)\n",
        "plt.subplot(2,2,2)\n",
        "sns.countplot(x='Survived', hue='Embarked', data=train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d7c8c77e-dd13-d110-105c-552e143f4f2d"
      },
      "outputs": [],
      "source": [
        "sns.boxplot(x='Embarked', y='Fare', data=train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e81a571f-c5dc-177d-8f67-4c53b682f64d"
      },
      "outputs": [],
      "source": [
        "sns.countplot(hue='Pclass', x='Embarked', data=train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a66e66f1-7717-c8bb-a217-f8976997b6e8"
      },
      "source": [
        "There is a little correlation between the embarkment harbor and the survival rate. \n",
        "\n",
        "Since we want a numerical feature, we will convert the embarked in one hot vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2456715e-3530-50e6-cb5c-53c99b169b83"
      },
      "outputs": [],
      "source": [
        "train = pd.get_dummies(train, columns=['Embarked'])\n",
        "test = pd.get_dummies(test, columns=['Embarked'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "5b70374d-77c4-a48a-e519-73ac62cb76ec"
      },
      "source": [
        "## Fare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c565b0e1-bb80-0431-7b93-33fd5a9bdbc9"
      },
      "outputs": [],
      "source": [
        "sns.distplot(train['Fare'])\n",
        "train['Fare'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "7b25d7a5-2e06-db5b-b67e-2420e15a5441"
      },
      "source": [
        "Fare is really right skewed (few people paying a very high fare and most people paying a low fare). We can see that in the train we have all the values, but one value is missing in test set. Fill it with the median:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f8d49013-8aa6-a9a5-47c8-4c395f6509c4",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "test['Fare'] = test['Fare'].fillna(test['Fare'].median())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "5a2ee2c9-0572-331a-ff34-1f06672cba6b"
      },
      "source": [
        "Since the fare is so oddly distributed, is seems reasonable to introduce categories:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7d77ee00-4c4c-8482-5e12-ecd9fe186e52"
      },
      "outputs": [],
      "source": [
        "for df in [train, test]:\n",
        "    df['Fare'] = pd.qcut(df['Fare'], 4, labels=[0, 1, 2, 3])\n",
        "\n",
        "train.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "de30075b-20d2-5a43-175d-893405e188e9"
      },
      "source": [
        "## Cabin\n",
        "\n",
        "Cabin is a feature that has very few non null entries. Thus, we can drop this column and only take into account whether the value was present or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8751b1cc-9c15-119a-16fc-360be22e00e7"
      },
      "outputs": [],
      "source": [
        "for df in [train, test]:\n",
        "    df['Cabin'] = df['Cabin'].fillna('NaN')\n",
        "    df['HasCabin'] = df['Cabin'].apply(lambda x : 0 if x == 'NaN' else 1)\n",
        "\n",
        "train, test = train.drop(['Cabin'], axis=1), test.drop(['Cabin'], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a9586c29-22e5-25a3-df88-6d34de945dc1"
      },
      "source": [
        "## Parch and SibSp\n",
        "\n",
        "**Parch** is the abbreviation of 'parent/children' and represent the sum of parents and children.\n",
        "\n",
        "**SibSp** is the abbreviation of 'sibling/spouse' and represent the sum of brothers/sisters/wife/husband.\n",
        "\n",
        "This said, we can compute the family size as the sum of the two above + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f5e9b3ed-a224-35af-07a2-3df1ff6cdc3e"
      },
      "outputs": [],
      "source": [
        "for df in [train, test]:\n",
        "    df['FamilySize'] = df['Parch'] + df['SibSp'] + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "95b0ef77-aa9f-9718-3a38-ac0f832129d3"
      },
      "source": [
        "We have also added a feature TravelAlone, this is to say if the passenger has some family member on board or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5065a6d0-8944-37da-3e96-b1afe7e28dfb"
      },
      "outputs": [],
      "source": [
        "sns.barplot(x='FamilySize', y='Survived' , data=train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "8d90a8bd-c410-18d0-71c5-b8c65f29f726"
      },
      "source": [
        "Those with 2-4 family members had a slightly more chance to survive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f59aa89d-2f69-459f-5abb-f7d2fffc80b0"
      },
      "outputs": [],
      "source": [
        "def filter_family_size(x):\n",
        "    if x == 1:\n",
        "        return 'Solo'\n",
        "    elif x < 4:\n",
        "        return 'Small'\n",
        "    else:\n",
        "        return 'Big'\n",
        "\n",
        "for df in [train, test]:\n",
        "    df['FamilySize'] = df['FamilySize'].apply(filter_family_size)\n",
        "\n",
        "train = pd.get_dummies(train, columns=['FamilySize'])\n",
        "test = pd.get_dummies(test, columns=['FamilySize'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "8dc84060-c287-0f44-8661-26b4dc18cb5e"
      },
      "source": [
        "## Name\n",
        "\n",
        "Let's see if we can extract something from the name. Here is a list of names from the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "fda27a81-408b-e0d7-9de4-9a0a500b308f"
      },
      "outputs": [],
      "source": [
        "train['Name'].head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "56e82a42-5acf-9824-4e28-2701cad46b0a"
      },
      "source": [
        "One thing that can be useful is the title, which can be found soon after the first comma. Since there are a lot of rare titles, we will join the uncommon ones (such as countess, dona, jonkeer, ...) in 6 labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "651bdfcb-76d9-90d6-1a40-6f84dcf5ca75"
      },
      "outputs": [],
      "source": [
        "# Filter the name\n",
        "def get_title(x):\n",
        "    y = x[x.find(',')+1:].replace('.', '').replace(',', '').strip().split(' ')\n",
        "    if y[0] == 'the':    # Search for the countess\n",
        "        title = y[1]\n",
        "    else:\n",
        "        title = y[0]\n",
        "    return title\n",
        "\n",
        "def filter_title(title, sex):\n",
        "    if title in ['Countess', 'Dona', 'Lady', 'Jonkheer', 'Mme', 'Mlle', 'Ms', 'Capt', 'Col', 'Don', 'Sir', 'Major', 'Rev', 'Dr']:\n",
        "        if sex:\n",
        "            return 'Rare_male'\n",
        "        else:\n",
        "            return 'Rare_female'\n",
        "    else:\n",
        "        return title\n",
        "\n",
        "for df in [train, test]:\n",
        "    df['NameLength'] = df['Name'].apply(lambda x : len(x))\n",
        "    df['Title'] = df['Name'].apply(get_title)\n",
        "    \n",
        "train.groupby('Title')['PassengerId'].count().sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "acd6988e-96b9-bb6c-36aa-965d42d982c1"
      },
      "outputs": [],
      "source": [
        "for df in [train, test]:\n",
        "    df['Title'] = df.apply(lambda x: filter_title(x['Title'], x['Sex']), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d37a8bc8-efe9-8dbb-971b-8ef1ff16a793"
      },
      "outputs": [],
      "source": [
        "sns.countplot(y=train['Title'])\n",
        "train.groupby('Title')['PassengerId'].count().sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "70441960-476a-5e22-e78d-414623d8444d"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 8))\n",
        "sns.barplot(x='Sex', y='Survived', hue='Title', data=train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "78020ebc-d634-67bb-ed74-91a64b52ffcd"
      },
      "source": [
        "We can see that all the ladies with some rare title have all survived.\n",
        "\n",
        "As we did before, we must convert the name column in a one hot vector:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8213433c-b0c7-460c-5687-b585335d3dae"
      },
      "outputs": [],
      "source": [
        "train = pd.get_dummies(train, columns=['Title'])\n",
        "test = pd.get_dummies(test, columns=['Title'])\n",
        "\n",
        "train = train.drop(['Name', 'Sex'], axis=1)\n",
        "test = test.drop(['Name', 'Sex'], axis=1)\n",
        "\n",
        "#name_mapping = {'Mr' : 0, 'Mrs' : 1, 'Miss' : 2, 'Master' : 3, 'Rare' : 4}\n",
        "#for df in [train, test]:\n",
        "#    df['Name'] = df['Name'].apply(lambda x : name_mapping[x])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "36e708fa-132c-05ba-5ee7-556e49045bc6"
      },
      "source": [
        "## Ticket\n",
        "\n",
        "Have a look at the ticket column:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d7cc9881-edd0-6cc0-9630-18fb8f852186"
      },
      "outputs": [],
      "source": [
        "train['Ticket'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0f5de295-64db-7a5a-53c9-9d36554d0579"
      },
      "outputs": [],
      "source": [
        "train['Ticket'].head(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "7def6bc7-b4e1-ea17-1880-6a00481680b8"
      },
      "source": [
        "There are 681 unique tickets among 891 tickets, so there is some repetition. Let us add a column DuplicatedTicket that take into account that a certain ticket was duplicated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "85af5327-9e97-1ac6-75b5-5e44ad3800b6",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "for df in [train, test]:\n",
        "    df['TicketLetter'] = df['Ticket'].apply(lambda x : str(x)[0])\n",
        "    #df['TicketLength'] = df['Ticket'].apply(lambda x : len(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5c5190df-51ad-2a7b-6f8d-fcb6c2a8738d"
      },
      "outputs": [],
      "source": [
        "train.groupby(['TicketLetter'])['Survived'].mean().sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d0226aec-7cc0-e82a-42a0-ab720e1a5373"
      },
      "outputs": [],
      "source": [
        "sns.barplot(x = 'TicketLetter', y='Survived', data=train)\n",
        "df_count = train.groupby(['TicketLetter'],as_index=True)['PassengerId'].count().sort_values(ascending=False)\n",
        "print(df_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "716f9933-c971-7fdb-6c1f-2b599c51f1ca"
      },
      "outputs": [],
      "source": [
        "def filter_ticket(x):\n",
        "    if x in ['9', '8', '5', 'L', '6', 'F', '7', '4', 'W', 'A']:\n",
        "        return 'Rare'\n",
        "    elif x in ['C', 'P', 'S', '1']:\n",
        "        return 'Frequent'\n",
        "    elif x == '2':\n",
        "        return 'Common'\n",
        "    elif x == '3':\n",
        "        return 'Commonest'\n",
        "\n",
        "for df in [train, test]:\n",
        "    df['TicketCategory'] = df['TicketLetter'].apply(filter_ticket)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1ecbf2d8-e93f-6779-a16c-1e27136fe673"
      },
      "outputs": [],
      "source": [
        "sns.barplot(x='TicketCategory', y='Survived', data=train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6053bce3-be0d-90da-5ebb-ce6518af69d4"
      },
      "outputs": [],
      "source": [
        "def simplify_ticket(df):\n",
        "    df = pd.get_dummies(df, columns=['TicketCategory'])\n",
        "    df = df.drop(['TicketLetter', 'Ticket'], axis=1)\n",
        "    return df\n",
        "\n",
        "train,test = simplify_ticket(train), simplify_ticket(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5c60d9e3-17da-2ff8-6bf4-d2f5b002ecad"
      },
      "outputs": [],
      "source": [
        "train.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d8efffc1-7d9c-6440-2624-7211b0a5974d"
      },
      "source": [
        "## Age:\n",
        "\n",
        "We have seen that there are some missing values for the age. It is worthy to study its distribution:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "72296096-9020-d16c-4643-99b77dc2f74d"
      },
      "outputs": [],
      "source": [
        "plt.title('Age distribution', size=20, y=1.1)\n",
        "sns.distplot(train['Age'].dropna())\n",
        "train['Age'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "3c466b5d-25a5-63b2-7992-7c654c53c820"
      },
      "source": [
        "So the age is not that far from a Gaussian distribution. The median is 28 years, while the average is more like 29-30 years. \n",
        "\n",
        "Let's keep track of whether the age was missing or not with a binary value, and then we will fill the missing values.\n",
        "We will fill the missing values generating a random number according to a Gaussian distribution.\n",
        "\n",
        "**Note:** we must reflect the changes to the test data as well!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "55814b2f-f896-8e30-b2e4-5e73f0aff391",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "for df in [train, test]:\n",
        "    df['MissingAge'] = df['Age'].apply(lambda x : 1 if np.isnan(x) else 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8323e279-e4b4-7c7a-a8c3-322ecb3e1771"
      },
      "outputs": [],
      "source": [
        "col_to_drop = ['PassengerId', 'Age', 'Survived', 'MissingAge']\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "x_train_not_age_nan = train.loc[train['MissingAge'] == 0, :]\n",
        "y_train_not_age_nan = x_train_not_age_nan['Age']\n",
        "\n",
        "x_train_not_age_nan = x_train_not_age_nan.drop(col_to_drop, axis=1, errors='ignore')\n",
        "\n",
        "\n",
        "rpart_params = {\n",
        "    'criterion' : ['mse'],\n",
        "    'splitter' : ['best'],\n",
        "    'max_features' : ['auto', 'sqrt', 'log2', None],\n",
        "    'max_depth' : [2, 3, 4],\n",
        "    'min_samples_split' : [2, 3],\n",
        "    'min_samples_leaf' : [1, 2],\n",
        "    'max_leaf_nodes' : [3, 4, None],\n",
        "    'random_state' : [RANDOM_SEED],\n",
        "    'presort' : [True]\n",
        "}\n",
        "\n",
        "model = DecisionTreeRegressor()\n",
        "age_grid = GridSearchCV(model, rpart_params, n_jobs=-1).fit(x_train_not_age_nan, y_train_not_age_nan.values.ravel())\n",
        "print('Best model CV score: ', age_grid.best_score_)\n",
        "age_estimator = age_grid.best_estimator_.fit(x_train_not_age_nan, y_train_not_age_nan.values.ravel())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d5134423-7410-fade-c702-8c2b87db77dc"
      },
      "outputs": [],
      "source": [
        "for df in [train, test]:\n",
        "    age_fill = age_estimator.predict(df.loc[df['MissingAge'] == 1, :].drop(col_to_drop, axis=1, errors='ignore'))\n",
        "\n",
        "    df.loc[df['MissingAge'] == 1, 'Age'] = age_fill\n",
        "\n",
        "plt.title('Age distribution after filling NaN', size=20, y=1.1)\n",
        "sns.distplot(train['Age'])\n",
        "train['Age'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "1119907e-914e-b60d-79d5-04868b7b9001"
      },
      "source": [
        "Ok, the mean is slightly changed but the distribution is quite the same.\n",
        "\n",
        "To simplify the feature space, we will subdivide the age in 5 slots:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "54fb91b6-2e36-aead-f15f-295961010c50",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "for df in [train, test]:\n",
        "    df['Age'] = pd.cut(df['Age'], 5, labels=[0, 1, 2, 3, 4])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "08e71f2a-860d-1c62-6a29-de5ceaba3c3f"
      },
      "source": [
        "## Final refinements\n",
        "\n",
        "The only final refinement is to drop the unused columns and convert the Sex column in a binary digit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "94ebabf9-6dd6-980d-6939-7a2a748c17e6"
      },
      "outputs": [],
      "source": [
        "train_y = train['Survived'].ravel()\n",
        "train_x = train.drop(['Survived', 'PassengerId'], axis=1)\n",
        "\n",
        "test_x = test.drop(['PassengerId'], axis=1)\n",
        "\n",
        "\n",
        "for df in [train_x, test_x]:\n",
        "    for col in df.columns:\n",
        "        df[col] = df[col].astype('int')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "0100c925-2eae-b980-4cc1-834501bfd5c7"
      },
      "source": [
        "We have all numerical features and hence we are ready to start with the model selection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "56e3e725-db38-9584-3c90-cb4b399dab28"
      },
      "source": [
        "# Correlation between all features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "54d72af4-09f1-64bb-1e45-1e5e03d48b78"
      },
      "outputs": [],
      "source": [
        "colormap = plt.cm.viridis\n",
        "plt.figure(figsize=(12, 12))\n",
        "plt.title(\"Feature correlation\", y=1.05, size=15)\n",
        "sns.heatmap(train_x.corr(), linewidths=0.1, square=True, vmax=1.0, annot=True, cmap=colormap)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9e28a5b2-8a8b-ef1f-fcd0-154d23882550"
      },
      "outputs": [],
      "source": [
        "var_correlations = {c: np.abs(train['Survived'].corr(train_x[c])) for c in train_x.columns}\n",
        "\n",
        "corr_dataframe = pd.DataFrame(var_correlations, index=['Correlation']).T.sort_values(by='Correlation')\n",
        "plt.title('Correlation between feature and survival rate', y=1.1, size=15)\n",
        "plt.barh(range(corr_dataframe.shape[0]), corr_dataframe['Correlation'].values, tick_label=train_x.columns.values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9d8f98db-b572-367b-e414-f74b1e4a8ee9"
      },
      "source": [
        "# Creating the validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "df3f32e9-e629-633e-0ef0-0d987be5090e"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_validation, y_train, y_validation = train_test_split(train_x, train_y, test_size=0.3, random_state=RANDOM_SEED)\n",
        "x_test = test_x.copy()\n",
        "\n",
        "x_train.index = np.arange(len(x_train))\n",
        "x_validation.index = np.arange(len(x_validation))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "31fee41b-2f14-e9f2-25fb-8519ed0327cb"
      },
      "source": [
        "# Model selection\n",
        "\n",
        "## Logistic regression\n",
        "\n",
        "Logistic regression is the simplest classification method. To train the logistic regression, we will use a bunch of parameters and perform a grid search using a crossvalidation with the default 3 folds.\n",
        "\n",
        "Original parameters are:\n",
        "\n",
        "    lr_params = {\n",
        "        'C' : [0.01, 0.03, 0.1, 0.3, 1, 2, 3],\n",
        "        'fit_intercept' : [True, False],\n",
        "        'max_iter' : [5000],\n",
        "        'solver' : ['newton-cg', 'lbfgs', 'liblinear', 'sag'],\n",
        "        'tol' : [1e-4],\n",
        "        'random_state' : [RANDOM_SEED]\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "835c118e-1ad6-5dd0-248c-0f5fdaf3877b"
      },
      "outputs": [],
      "source": [
        "lr_params = {\n",
        "    'C' : [1],\n",
        "    'fit_intercept' : [True],\n",
        "    'max_iter' : [5000],\n",
        "    'solver' : ['newton-cg'],\n",
        "    'tol' : [1e-4],\n",
        "    'random_state' : [RANDOM_SEED]\n",
        "}\n",
        "\n",
        "log_regression = linear_model.LogisticRegression()\n",
        "acc_scorer = make_scorer(accuracy_score)\n",
        "log_reg_models = GridSearchCV(log_regression, lr_params, scoring=acc_scorer, n_jobs=-1)\n",
        "log_reg_models = log_reg_models.fit(x_train, y_train)\n",
        "\n",
        "lr_best = log_reg_models.best_estimator_\n",
        "lr_best = lr_best.fit(x_train, y_train)\n",
        "\n",
        "lr_model = {\n",
        "    'Name' : 'Logistic regression', \n",
        "    'CVScore' : log_reg_models.best_score_, \n",
        "    'CVStd' : log_reg_models.cv_results_['std_test_score'][log_reg_models.best_index_],\n",
        "    'Result_train' : lr_best.predict(x_train),\n",
        "    'Result_test' : lr_best.predict(x_test),\n",
        "    'Model' : lr_best\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "99fcea48-a052-56e1-dda1-6ab9a45b1041"
      },
      "outputs": [],
      "source": [
        "print('Best model - avg:', \n",
        "      lr_model['CVScore'],\n",
        "      '+/-', \n",
        "      lr_model['CVStd'])\n",
        "print()\n",
        "print(log_reg_models.best_estimator_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "560624dd-27b3-6997-3a81-d71efc220dd4"
      },
      "source": [
        "## Random forest classifier\n",
        "\n",
        "Random forests are a powerful classification method, but require a discrete time to train. We will do the same thing as before, using cross validation to find the best model with a grid search over a bunch of different parameters\n",
        "\n",
        "Originally written with these parameters:\n",
        "\n",
        "    rf_params = {\n",
        "        'n_estimators' :  [50, 100, 400, 700, 1000],\n",
        "        'max_features' : ['log2', 'sqrt', 'auto'],\n",
        "        'criterion' : ['entropy', 'gini'],\n",
        "        'min_samples_split' :  [2, 4, 10, 12, 16],\n",
        "        'min_samples_leaf' : [1, 5, 10],\n",
        "        'random_state' : [RANDOM_SEED]\n",
        "    }\n",
        "\n",
        "\n",
        "Then shorted out to save time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6232c6b7-d9ac-ad11-e387-1aa781ca1a0d"
      },
      "outputs": [],
      "source": [
        "rf_params = {\n",
        "    'n_estimators' :  [50],\n",
        "    'max_features' : ['log2'],\n",
        "    'criterion' : ['gini'],\n",
        "    'min_samples_split' :  [16],\n",
        "    'min_samples_leaf' : [1],\n",
        "    'random_state' : [RANDOM_SEED]\n",
        "}\n",
        "\n",
        "random_forest = ensemble.RandomForestClassifier()\n",
        "acc_scorer = make_scorer(accuracy_score)\n",
        "rf_models = GridSearchCV(random_forest, rf_params, scoring=acc_scorer, n_jobs=-1)\n",
        "rf_models = rf_models.fit(x_train, y_train)\n",
        "\n",
        "rf_best = rf_models.best_estimator_\n",
        "rf_best = rf_best.fit(x_train, y_train)\n",
        "\n",
        "rf_model = {\n",
        "    'Name' : 'Random forest', \n",
        "    'CVScore' : rf_models.best_score_, \n",
        "    'CVStd' : rf_models.cv_results_['std_test_score'][rf_models.best_index_],\n",
        "    'Result_train' : rf_best.predict(x_train),\n",
        "    'Result_test' : rf_best.predict(x_test),\n",
        "    'Model' : rf_best\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6b424d89-d1d1-993d-a67f-0419ee0698d6"
      },
      "outputs": [],
      "source": [
        "best_idx = rf_models.best_index_\n",
        "print('Best model - avg:', \n",
        "      rf_model['CVScore'],\n",
        "      '+/-', \n",
        "      rf_model['CVStd'])\n",
        "print()\n",
        "print(rf_models.best_estimator_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5bd993dd-c7b5-0563-098b-8fcf42880055"
      },
      "outputs": [],
      "source": [
        "feature_importances = [(x, y) for x,y in zip(rf_best.feature_importances_, x_train.columns.values)]\n",
        "\n",
        "feature_importances.sort(key = lambda x : x[0])\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.barh(range(len(feature_importances)), [x[0] for x in feature_importances], tick_label = [x[1] for x in feature_importances])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "4558b18a-6004-b16f-d06a-5e4d51705777"
      },
      "source": [
        "XGBoost classifier\n",
        "--\n",
        "\n",
        "This is a very good general purpose classifier. The training process will take about 1 minute.\n",
        "\n",
        "Originally with this parameters:\n",
        "\n",
        "    xgb_params = {\n",
        "        'max_depth' : [2, 3, 4, 5, 6],\n",
        "        'learning_rate' : [0.05, 0.01, 0.005],\n",
        "        'n_estimators' : [100, 300, 600],\n",
        "        'seed' : [RANDOM_SEED]\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b4320edd-2fb6-3e28-f66d-a14e9b03af16"
      },
      "outputs": [],
      "source": [
        "xgb_params = {\n",
        "    'max_depth' : [5],\n",
        "    'learning_rate' : [0.05],\n",
        "    'n_estimators' : [100],\n",
        "    'seed' : [RANDOM_SEED]\n",
        "}\n",
        "\n",
        "xgb_model = xgb.XGBClassifier()\n",
        "acc_scorer = make_scorer(accuracy_score)\n",
        "xgb_grid = GridSearchCV(xgb_model, xgb_params, scoring=acc_scorer)\n",
        "xgb_grid = xgb_grid.fit(x_train, y_train)\n",
        "\n",
        "xgb_best = xgb_grid.best_estimator_\n",
        "xgb_best = xgb_best.fit(x_train, y_train)\n",
        "\n",
        "xgb_model = {\n",
        "    'Name' : 'XGBoost', \n",
        "    'CVScore' : xgb_grid.best_score_, \n",
        "    'CVStd' : xgb_grid.cv_results_['std_test_score'][xgb_grid.best_index_],\n",
        "    'Result_train' : xgb_best.predict(x_train),\n",
        "    'Result_test' : xgb_best.predict(x_test),\n",
        "    'Model' : xgb_best\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "93d0769e-3d9c-13e3-f0c2-eecad5554f63"
      },
      "outputs": [],
      "source": [
        "best_idx = xgb_grid.best_index_\n",
        "print('Best model - avg:', \n",
        "      xgb_model['CVScore'],\n",
        "      '+/-', \n",
        "      xgb_model['CVStd'])\n",
        "print()\n",
        "print(xgb_grid.best_estimator_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "21012a8e-d9b5-c64e-e7c1-69ba4012c45e"
      },
      "outputs": [],
      "source": [
        "xgb.plot_importance(xgb_best)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "4fbedadc-804a-3ecb-7320-c26db984c869"
      },
      "source": [
        "## Support vector classifier\n",
        "\n",
        "SVC are also very good classifier, but requires a normalization phase in order to converge."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a31cfaae-708e-6746-8780-4546084527ad"
      },
      "outputs": [],
      "source": [
        "train_test = pd.concat([x_train, x_test, x_validation], ignore_index=True)\n",
        "train_test_normalized = preprocessing.scale(train_test)\n",
        "x_train_normalized = train_test_normalized[:len(x_train), :]\n",
        "x_test_normalized = train_test_normalized[len(x_train):len(x_train) + len(x_test), :]\n",
        "x_validation_normalized = train_test_normalized[len(x_train) + len(x_test):, :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "4a129136-ab7d-2cd7-cc52-767a73757d91"
      },
      "source": [
        "And now for the training\n",
        "\n",
        "    svm_params = {\n",
        "        'C' : [0.1, 0.3, 0.8, 0.9, 1.0, 2.0],\n",
        "        'kernel' : ['linear', 'poly', 'rbf', 'sigmoid'],\n",
        "        'tol' : [1e-3, 1e-4],\n",
        "        'degree' : [2, 3, 4, 5],\n",
        "        'random_state' : [RANDOM_SEED]\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4b82b728-7c8c-56d3-8aed-5e12ddbda945"
      },
      "outputs": [],
      "source": [
        "svm_params = {\n",
        "    'C' : [0.3],\n",
        "    'kernel' : ['rbf'],\n",
        "    'tol' : [1e-3],\n",
        "    'degree' : [2],\n",
        "    'random_state' : [RANDOM_SEED]\n",
        "}\n",
        "acc_scorer = make_scorer(accuracy_score)\n",
        "svc = SVC()\n",
        "svc_classifiers = GridSearchCV(svc, svm_params, scoring=acc_scorer, n_jobs=-1)\n",
        "svc_classifiers = svc_classifiers.fit(x_train_normalized, y_train)\n",
        "\n",
        "svc_best = svc_classifiers.best_estimator_\n",
        "svc_best = svc_best.fit(x_train_normalized, y_train)\n",
        "\n",
        "svc_model = {\n",
        "    'Name' : 'SVC', \n",
        "    'CVScore' : svc_classifiers.best_score_, \n",
        "    'CVStd' : svc_classifiers.cv_results_['std_test_score'][svc_classifiers.best_index_],\n",
        "    'Result_train' : svc_best.predict(x_train_normalized),\n",
        "    'Result_test' : svc_best.predict(x_test_normalized),\n",
        "    'Model' : svc_best\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "75802172-85fb-c7ce-e141-217937a06253"
      },
      "outputs": [],
      "source": [
        "best_idx = svc_classifiers.best_index_\n",
        "print('Best model - avg:', \n",
        "      svc_model['CVScore'], \n",
        "      '+/-', \n",
        "      svc_model['CVStd'])\n",
        "print()\n",
        "print(svc_classifiers.best_estimator_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "ef664eb4-b8f4-e79f-56db-d719990f0e79"
      },
      "source": [
        "# Ada boost\n",
        "\n",
        "Also here, the parameters were:\n",
        "\n",
        "    ada_params = {\n",
        "        'n_estimators' : [20, 50, 100, 500, 1000],\n",
        "        'learning_rate' : [0.1, 0.4, 0.3, 0.9, 1],\n",
        "        'algorithm' : ['SAMME', 'SAMME.R'],\n",
        "        'random_state' : [RANDOM_SEED]\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9445697f-24f2-fda1-1112-5c94bc54fad0"
      },
      "outputs": [],
      "source": [
        "ada_params = {\n",
        "    'n_estimators' : [100],\n",
        "    'learning_rate' : [0.1],\n",
        "    'algorithm' : ['SAMME.R'],\n",
        "    'random_state' : [RANDOM_SEED]\n",
        "}\n",
        "\n",
        "acc_scorer = make_scorer(accuracy_score)\n",
        "ada_class = ensemble.AdaBoostClassifier()\n",
        "ada_classifiers = GridSearchCV(ada_class, ada_params, scoring=acc_scorer, n_jobs=-1)\n",
        "ada_classifiers = ada_classifiers.fit(x_train, y_train)\n",
        "\n",
        "ada_best = ada_classifiers.best_estimator_\n",
        "ada_best = ada_best.fit(x_train, y_train)\n",
        "\n",
        "ada_model = {\n",
        "    'Name' : 'Ada boost', \n",
        "    'CVScore' : ada_classifiers.best_score_, \n",
        "    'CVStd' : ada_classifiers.cv_results_['std_test_score'][ada_classifiers.best_index_],\n",
        "    'Result_train' : ada_best.predict(x_train),\n",
        "    'Result_test' : ada_best.predict(x_test),\n",
        "    'Model' : ada_best\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5d48628b-bac7-c151-4a4e-6cc0cf453911"
      },
      "outputs": [],
      "source": [
        "best_idx = ada_classifiers.best_index_\n",
        "print('Best model - avg:', \n",
        "      ada_model['CVScore'], \n",
        "      '+/-', \n",
        "      ada_model['CVStd'])\n",
        "print()\n",
        "print(ada_classifiers.best_estimator_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "4ab405c0-1581-57f4-7631-24d1fa80f7cb"
      },
      "source": [
        "# Ensemble\n",
        "\n",
        "We have seen that overall the XGBClassifier is the one that performs the best."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c7ef49c3-f4fc-65b3-e8b1-7b8de21b6ad2",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "class Ensemble:\n",
        "    def __init__(self, models, svc_last=True):\n",
        "        self.models = models[:]\n",
        "        self.svc_last = svc_last\n",
        "    \n",
        "    def fit(self, X_train, y_train, X_train_normalized):\n",
        "        fitted_models = [m['Model'].fit(X_train, y_train) for m in self.models]\n",
        "        for i in range(len(fitted_models)):\n",
        "            self.models[i]['Model'] = fitted_models[i]\n",
        "    \n",
        "    def predict(self, X_test, X_test_normalized):\n",
        "        predictions = []\n",
        "        for m in self.models:\n",
        "            if m['Name'] == 'SVC':\n",
        "                predictions.append(m['Model'].predict(X_test_normalized))\n",
        "            else:\n",
        "                predictions.append(m['Model'].predict(X_test))\n",
        "        \n",
        "        df = pd.DataFrame(np.array(predictions), index=[m['Name'] for m in self.models])\n",
        "        \n",
        "        return df.apply(lambda x : 0 if np.sum(x) <= 2 else 1)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"Ensemble(\" + ', '.join([m['Name'] for m in self.models]) + \")\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b37484eb-a8cf-01fa-1161-a507f42d22af"
      },
      "outputs": [],
      "source": [
        "ens = Ensemble([rf_model, ada_model, xgb_model, lr_model, svc_model])\n",
        "predictions = ens.predict(x_test, x_test_normalized).values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "fad2a92e-8af6-1ef4-ab04-6af652e0fbc5"
      },
      "outputs": [],
      "source": [
        "ensemble_model = {\n",
        "    'Name' : 'Ensemble', \n",
        "    'CVScore' : 0, \n",
        "    'CVStd' : 0,\n",
        "    'Result_train' : [],\n",
        "    'Result_test' : predictions,\n",
        "    'Model' : ens\n",
        "}\n",
        "\n",
        "answer_df = pd.DataFrame()\n",
        "answer_df['PassengerId'] = test['PassengerId']\n",
        "answer_df['Survived'] = predictions\n",
        "\n",
        "answer_df.to_csv('results_ensemble.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "4395625b-84ca-b1ed-5057-4a381987b184"
      },
      "source": [
        "# Stacking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "17661812-6117-e233-c09b-56c08ebc985a"
      },
      "outputs": [],
      "source": [
        "def get_stacked(x, x_normalized, models):\n",
        "    predictions = []\n",
        "    for m in models:\n",
        "        if m['Name'] == 'SVC':\n",
        "            predictions.append(m['Model'].predict(x_normalized))\n",
        "        else:\n",
        "            predictions.append(m['Model'].predict(x))\n",
        "    stack = pd.DataFrame(np.array(predictions).T, columns=[m['Name'] for m in models])\n",
        "    return pd.concat([x, stack], axis=1)\n",
        "\n",
        "stacking_models = [rf_model, ada_model, svc_model]\n",
        "train_stacked = get_stacked(x_train, x_train_normalized, stacking_models)\n",
        "test_stacked = get_stacked(x_test, x_test_normalized, stacking_models)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "f15679fd-0f85-425c-901d-6ee9177949dd"
      },
      "source": [
        "These are the initial parameters I used for GridSearch:\n",
        "\n",
        "    xgb_params = {\n",
        "        'max_depth' : [2, 3, 4, 5, 6],\n",
        "        'learning_rate' : [0.05, 0.01],\n",
        "        'n_estimators' : [30, 100, 300, 600],\n",
        "        'seed' : [RANDOM_SEED]\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f5052492-40ef-3030-4b8e-c31fd209ffab"
      },
      "outputs": [],
      "source": [
        "xgb_params = {\n",
        "    'max_depth' : [2],\n",
        "    'learning_rate' : [0.05],\n",
        "    'n_estimators' : [30],\n",
        "    'seed' : [RANDOM_SEED]\n",
        "}\n",
        "\n",
        "xgb_stacked = xgb.XGBClassifier()\n",
        "acc_scorer = make_scorer(accuracy_score)\n",
        "xgb_stacked_grid = GridSearchCV(xgb_stacked, xgb_params, scoring=acc_scorer)\n",
        "xgb_stacked_grid = xgb_stacked_grid.fit(train_stacked, y_train)\n",
        "\n",
        "stacked_best = xgb_stacked_grid.best_estimator_.fit(train_stacked, y_train)\n",
        "\n",
        "stacked_model = {\n",
        "    'Name' : 'Stacking', \n",
        "    'CVScore' : xgb_stacked_grid.best_score_, \n",
        "    'CVStd' : xgb_stacked_grid.cv_results_['std_test_score'][xgb_stacked_grid.best_index_],\n",
        "    'Result_train' : xgb_stacked_grid.predict(train_stacked),\n",
        "    'Result_test' : xgb_stacked_grid.predict(test_stacked),\n",
        "    'Model' : stacked_best\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a4dade9e-7de0-7d52-a2d7-419adc984643"
      },
      "outputs": [],
      "source": [
        "print('Best model - avg:', \n",
        "      stacked_model['CVScore'], \n",
        "      '+/-', \n",
        "      stacked_model['CVStd'])\n",
        "print()\n",
        "print(stacked_best)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "2f79ba05-3580-6c0d-1ce3-5489b137b9a1"
      },
      "source": [
        "# Check the validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0df3336a-52f8-7996-d6dc-0ac251043265"
      },
      "outputs": [],
      "source": [
        "models = [lr_model, rf_model, ada_model, xgb_model, svc_model, ensemble_model, stacked_model]\n",
        "models_df = pd.DataFrame(models, index=[m['Name'] for m in models])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5cc21afe-c6c4-81ea-7bb5-d357d3099f46"
      },
      "outputs": [],
      "source": [
        "x_validation_stacked = get_stacked(x_validation, x_validation_normalized, stacking_models)\n",
        "\n",
        "def get_validation_predictions(x):\n",
        "    if x['Name'] == 'SVC':\n",
        "        return x['Model'].predict(x_validation_normalized)\n",
        "    elif x['Name'] == 'Ensemble':\n",
        "        return x['Model'].predict(x_validation, x_validation_normalized)\n",
        "    elif x['Name'] == 'Stacking':\n",
        "        return x['Model'].predict(x_validation_stacked)\n",
        "    else:\n",
        "        return x['Model'].predict(x_validation)\n",
        "\n",
        "models_df['ValidationScore'] = models_df.apply(lambda x : accuracy_score(get_validation_predictions(x), y_validation), axis=1)\n",
        "\n",
        "models_df['ValidationScore']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "4a4490d3-66c4-bc48-bc8a-80aba3eaa1b0"
      },
      "source": [
        "# Result output\n",
        "\n",
        "Overall, it seems that the SVC generalizes better, so we will use that SVC to output the result. \n",
        "\n",
        "Note that here a statistical test should be carried out, because the accuracy alone is usually not enough. One should use cross validation instead of holdout and see if there is a statistical significance of a given model over the others."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "736248a5-8593-5ecd-0b0e-774eb8506b14"
      },
      "outputs": [],
      "source": [
        "best_model = svc_model['Model']\n",
        "best_model = best_model.fit(np.concatenate([x_train_normalized, x_validation_normalized]), np.concatenate([y_train, y_validation]))\n",
        "\n",
        "predictions = best_model.predict(x_test_normalized)\n",
        "\n",
        "result = pd.DataFrame()\n",
        "result['PassengerId'] = test['PassengerId']\n",
        "result['Survived'] = predictions\n",
        "\n",
        "result.to_csv('results.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "1a6793ea-5f44-80ac-6c44-6dd5020aeb66"
      },
      "source": [
        "# References\n",
        "\n",
        "- [Titanic Random Forest: 82.78%](https://www.kaggle.com/zlatankr/titanic/titanic-random-forest-82-78/run/806902)\n",
        "- [Scikit-Learn ML from Start to Finish](https://www.kaggle.com/jeffd23/titanic/scikit-learn-ml-from-start-to-finish/run/320209)\n",
        "- [A Journey through Titanic](https://www.kaggle.com/omarelgabry/titanic/a-journey-through-titanic/run/447794)"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}