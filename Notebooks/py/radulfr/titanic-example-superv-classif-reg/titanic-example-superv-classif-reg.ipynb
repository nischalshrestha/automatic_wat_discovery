{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"523fc45fc15cccd3dec87d6127c5332ff4f0d005"},"cell_type":"markdown","source":"**RRC**: First of all, load our dataset."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"traindf = pd.read_csv(\"../input/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f894ed1cfb2dc48c229a2dd918e3e2e8876d8ac8"},"cell_type":"markdown","source":"**RRC**: And see what we have at the tail"},{"metadata":{"trusted":true,"_uuid":"f48ff5e103b52d4ae600f44f182a90e929c8569b"},"cell_type":"code","source":"traindf.tail(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4c05e384e62a1111c93a6b6ca43872c6a8f6dadc"},"cell_type":"markdown","source":"## **Preprocessing**"},{"metadata":{"_uuid":"31bb7e59a62ef5471dfea89e5dc3e353a248a0bf"},"cell_type":"markdown","source":"## Identification of missings\n"},{"metadata":{"trusted":true,"_uuid":"42c6a0847029e16061c020693f5341716b2bc828"},"cell_type":"code","source":"#traindf.describe()\ntraindf.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"804ba78ee9ab07cb9f7247d0545d14ee27c8c263"},"cell_type":"markdown","source":"**RRC**: Surprise! Age feature has nulls. Let's work on that. "},{"metadata":{"_uuid":"c758cd997080de7e4e3407ab6e4afb814ac7c66f"},"cell_type":"markdown","source":"**WARNING**: number 0 is not a missing value. Also, \" \" sometimes is not a missing value. It depends on the context! Big mistake impute 'null' values as 0. **Reminder** see number 0 as any other value that could bias a prediction. "},{"metadata":{"_uuid":"c503174445648427076c0c8906f2c35b48e602c8"},"cell_type":"markdown","source":"* Identifying missing values (Typically NaN (Not a Number), null, empty cell in excel spreadsheet, etc.)\n* Eliminating samples or features with missing\n* Imputing missing values"},{"metadata":{"_uuid":"f1e5aabe51684e726bd2163404f9bfce77ee0963"},"cell_type":"markdown","source":"## Record dropping"},{"metadata":{"trusted":true,"_uuid":"0e983b0b307855129ebfe80da9f6910c9b89a499"},"cell_type":"code","source":"traindf.dropna().describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"221b286ebe0a01c79968f43704a53dc3adc4471d"},"cell_type":"markdown","source":"**RRC**: Uh oh! Wait! 183!? That's too much. Why?\nLet's try just removing those features with nulls. "},{"metadata":{"_uuid":"dbc4bd421b20a4dd40512d81d02b561b97311727"},"cell_type":"markdown","source":"## Feature dropping"},{"metadata":{"trusted":true,"_uuid":"b43ac8d878451e63f2e7f1ea7140d073d0b68cb2"},"cell_type":"code","source":"traindf.dropna(axis=1).describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8e692554df4f47123ab41944c7359b86514c5b47"},"cell_type":"markdown","source":"## Imputer"},{"metadata":{"_uuid":"6fe47f5c15e217f3d48be777f96cf59bbb4ad4b1"},"cell_type":"markdown","source":"**RRC**: Ok, let's fill (impute) null numerical values in Age. Our strategy is using the 'mean'. Naive, but secure. "},{"metadata":{"trusted":true,"_uuid":"a69ca723475bfefc2af345842921b1ab598945e0"},"cell_type":"code","source":"traindf['Age'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1039b4659881e591e9989df580269d694e82f501"},"cell_type":"code","source":"from sklearn.impute import SimpleImputer\nimp = SimpleImputer(missing_values=np.nan, strategy='mean')\ntraindf['Age'] = imp.fit_transform(traindf[['Age']])\n\ntraindf['Age'].describe()\n\n#imr = Imputer(missing_values=np.nan, strategy='mean', axis=1)\n#traindf['Age'] = imr.fit_transform(traindf['Age']).T\n\n#traindf.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f87bc4d3671a36a94652ea15a231de48efe1f3fa"},"cell_type":"markdown","source":"## Scaling ordinal features\n"},{"metadata":{"_uuid":"2236d5e1c372bd432cd4bf6c004b6ac205e0409f"},"cell_type":"markdown","source":"**RRC**: Our Fare and Age feautres are completely different. Check Max, quartiles, mins, and distribution are not so homogeneous. Let's fix that. "},{"metadata":{"trusted":true,"_uuid":"52790bbe57d279898df6cf189087109e258e663f"},"cell_type":"code","source":"traindf[['Fare', 'Age']].describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9ecd5c05e684f05e9b4975d83b620c945c9519b2"},"cell_type":"markdown","source":"**RRC**: Let's normalize that. "},{"metadata":{"trusted":true,"_uuid":"cfd2d47c9c91a43ac2538b7c7be3ba978adf41f7"},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nmms = MinMaxScaler()\ntraindfScaled = mms.fit_transform(traindf[['Fare', 'Age']])\ntraindf[['Fare', 'Age']] = traindfScaled\ntraindf[['Fare', 'Age']].describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"02d6ea3463242c7f1c7c792d722c154478a8be3a"},"cell_type":"markdown","source":"**RRC**: Check! min and max once scalled. Now, Fare and Age are numbers between 0 and 1. This is nice for a good training. "},{"metadata":{"_uuid":"424bc9d56a696b251435ca91370cd2568ff3f451"},"cell_type":"markdown","source":"## Encoding of categorical variables (some examples)"},{"metadata":{"_uuid":"6ccc97ff61fcac0d565eaa74a11cc3af3d86a3fe"},"cell_type":"markdown","source":"**RRC**: What would happen if we try to encode the Ticket column? "},{"metadata":{"trusted":true,"_uuid":"5f6ad7c58d99164a0797e7f6364cb7d8687edf4f"},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ny = le.fit_transform(traindf['Ticket'].values)\ny","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a0d22e2baf37fcd61f3e29f48088914194a9b086"},"cell_type":"markdown","source":"**RRC**: just kidding. Plenty of values. But a good example ;D"},{"metadata":{"_uuid":"ed923ec6674e139893de50696fa3af7b5c1f3ffe"},"cell_type":"markdown","source":"## One hot encoding\n\n**RRC**: Onehot encoding is a nice technique to encode variables. But warning! with this code this does not encode missing values. Try to fix that before you use this. \n\nCheck variables like Sex and Embarked. Let's one-hot encode them. "},{"metadata":{"trusted":true,"_uuid":"1066ddfb3d5069e348b0d24f4bf6d63196fa8bdb"},"cell_type":"code","source":"traindf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"04f3cb553e654ec428e09edafa9f11d894bf1285"},"cell_type":"code","source":"traindf = pd.get_dummies(traindf, columns = ['Embarked', 'Sex'])\n#Once encoded let's see. \ntraindf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"338fee59fda24668aa7dfb47d5265db73925a712"},"cell_type":"markdown","source":"## Transformations"},{"metadata":{"trusted":true,"_uuid":"81a287b71848f43d286cdc5edeaa8ac11ad920bb"},"cell_type":"code","source":"#The most simple one :D\ntraindf['ratioFareClass'] = traindf['Fare']/traindf['Pclass']\n\ntraindf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4841d515bc2b1fe3721a4c4dd87e2bddbf209974"},"cell_type":"markdown","source":"## Partitioning a dataset into separate training and test sets"},{"metadata":{"trusted":true,"_uuid":"276c75b54f3b7db4cc885f48246d4ba534c65bee"},"cell_type":"code","source":"features = ['Age', 'Fare', 'Pclass', 'Embarked_C', 'Embarked_S', 'Embarked_Q', 'Sex_female', 'Sex_male', 'ratioFareClass']\ntraindf['Survived'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0307432715c45b4b6995f67323766f4faa52f0c0"},"cell_type":"markdown","source":"**RRC**: Target variable is well balanced. Let's split dataset with a test size as 30% stratifying the target class"},{"metadata":{"trusted":true,"_uuid":"f2740845eb689be49e7e1f57debe90cc5feb288f"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(traindf[features],\n                                                    traindf[\"Survived\"],\n                                                    test_size=0.3,\n                                                    stratify=traindf['Survived'])\n\n# test size as a 30%\n# stratify on survived","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d1bdca7866c8a0a097ca3367e113fc27867c6847"},"cell_type":"code","source":"y_test.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8a1666dc8da102517b9fa5e2ab71bdb9d7b80288"},"cell_type":"markdown","source":"**RRC**: Check survived=1 is exactly a 30% of the total on y_test\n\nGood. Let's see our final dataset and some statistics about it. "},{"metadata":{"trusted":true,"_uuid":"eee3ae0f82977d014100f0841276caabeb95eefa"},"cell_type":"code","source":"traindf[features].head()\n# Features is the final feature list we are going to use. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aaeb6a82cecc60b35c2407c8e272d83ccbbb70c1"},"cell_type":"code","source":"traindf.describe()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8a0c462cf4f8a02424dd16876755a2f82f7afe6d"},"cell_type":"markdown","source":"**RRC**: It seems everything is Ok! so we can start playing with our data training different models. "},{"metadata":{"_uuid":"3743ce342fcdcd1acaf5e5ca4f24411f3356a304"},"cell_type":"markdown","source":"## Training "},{"metadata":{"_uuid":"737a7a7e065d30d02b0a75174e9849e4cec8f98f"},"cell_type":"markdown","source":"### Logistic Regression Classifier"},{"metadata":{"trusted":true,"_uuid":"facf345c78399691d3ace27665c190ffcc2cd97e"},"cell_type":"code","source":"#http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(random_state=0, solver='lbfgs',\n                         multi_class='multinomial').fit(X_train, y_train)\n\n\nprint(\"Logistic Regression score (Train): {0:.2}\".format(lr.score(X_train, y_train)))\nprint(\"Logistic Regression score (Test): {0:.2}\".format(lr.score(X_test, y_test)))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b448a71d759315d90f32d645ace84cf4f2d58927"},"cell_type":"markdown","source":"### KNN Classifier"},{"metadata":{"trusted":true,"_uuid":"15dbfa0eaad090ec7f8f3afe0d9c85dfc3db0a94"},"cell_type":"code","source":"#http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\nfrom sklearn.neighbors import KNeighborsClassifier\nneigh = KNeighborsClassifier(n_neighbors=3)\nneigh.fit(X_train, y_train)\nprint(\"KNN score (Train): {0:.2}\".format(neigh.score(X_train, y_train)))\nprint(\"KNN score (Test): {0:.2}\".format(neigh.score(X_test, y_test)))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"342867421733e28e4bf03a500db6e088bd0a8630"},"cell_type":"markdown","source":"### Support Vector Machines (Support Vector Classifier)"},{"metadata":{"trusted":true,"_uuid":"383dd07bac16255a09ee36e95b798f84675fc088"},"cell_type":"code","source":"#http://scikit-learn.org/stable/modules/svm.html\nfrom sklearn import svm\nsvclass = svm.SVC(gamma='scale')\nsvclass.fit(X_train, y_train) \nprint(\"SVM score (Train): {0:.2}\".format(svclass.score(X_train, y_train)))\nprint(\"SVM score (Test): {0:.2}\".format(svclass.score(X_test, y_test)))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"563efee643f677bc390009ce5817c39c02394aba"},"cell_type":"markdown","source":"### Decision tree"},{"metadata":{"trusted":true,"_uuid":"c3893b36d3b94e9a699a9683bc87388adf6f5625"},"cell_type":"code","source":"#http://scikit-learn.org/stable/modules/tree.html\nfrom sklearn import tree\ndt = tree.DecisionTreeClassifier()\ndt = dt.fit(X_train, y_train)\nprint(\"Decision Tree score (Train): {0:.2}\".format(dt.score(X_train, y_train)))\nprint(\"Decision Tree score (Test): {0:.2}\".format(dt.score(X_test, y_test)))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"56114b54c083de56a5948ebcb588e4d26852cf81"},"cell_type":"markdown","source":"### Random Forest Classifier"},{"metadata":{"trusted":true,"_uuid":"a5a2979a5d2081547e730b5a37662bc3c06a5b82"},"cell_type":"code","source":"# http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\nfrom sklearn.ensemble import RandomForestClassifier\n\nforest = RandomForestClassifier(n_estimators=100,\n                                criterion='gini',\n                                max_depth=5,\n                                min_samples_split=10,\n                                min_samples_leaf=5,\n                                random_state=0)\nX_train.head()\nforest.fit(X_train, y_train)\nprint(\"Random Forest score (Train): {0:.2}\".format(forest.score(X_train, y_train)))\nprint(\"Random Forest score (Test): {0:.2}\".format(forest.score(X_test, y_test)))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1bb1858830f33bd09bba0c43db3586ac2b8b86c2"},"cell_type":"markdown","source":"### Model Evaluation"},{"metadata":{"trusted":true,"_uuid":"132ee6ae31c7c8bfa14ec72adf7b31d6ec1523f0"},"cell_type":"code","source":"model = forest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d2480d465c2b5854e8b9dcc98c674c18c7b90121"},"cell_type":"code","source":"#WAAAAARNING: not all models have the \"feature_importances_\" functions\nplt.bar(np.arange(len(features)), model.feature_importances_)\nplt.xticks(np.arange(len(features)), features, rotation='vertical', ha='left')\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"491d77c9b9f9515d2eebf4efd9ca0156346d68f3"},"cell_type":"code","source":"X_test\n\nmodel = forest\n# This is an example! Also a bad practise :D\n#AGE, SEX, AGE_NAN, PClass, FARE\ntestcase = np.array([[25, 12, 3, 0, 1, 0, 1, 0, 1]])\nprediction = model.predict(testcase)[0]\npproba = model.predict_proba(testcase)[0]\nprint(\"Prediction for test case: %s (perish -> %.2f, surv -> %.2f)\" %\n      ('PERISH' if prediction == 0 else 'SURVIVED!', pproba[0], pproba[1]))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fb8a90c4d3796747c550269d7ce750b848900448"},"cell_type":"markdown","source":"## Some regression examples"},{"metadata":{"_uuid":"e6d8d67caebda23ed5ce701cdb8c99f22f9581ec"},"cell_type":"markdown","source":"### Ex 1: Diabetes dataset"},{"metadata":{"trusted":true,"_uuid":"65b67a1b6fc5028134d3720420286206e3e49502"},"cell_type":"code","source":"# http://scikit-learn.org/stable/auto_examples/linear_model/plot_ols.html\n\n# Code source: Jaques Grobler\n# License: BSD 3 clause\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn import datasets, linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Load the diabetes dataset\ndiabetes = datasets.load_diabetes()\n\n# Use only one feature\ndiabetes_X = diabetes.data[:, np.newaxis, 2]\n\n# Split the data into training/testing sets\ndiabetes_X_train = diabetes_X[:-20]\ndiabetes_X_test = diabetes_X[-20:]\n\n# Split the targets into training/testing sets\ndiabetes_y_train = diabetes.target[:-20]\ndiabetes_y_test = diabetes.target[-20:]\n# -----------------------------------------------\n# Create linear regression object\nregr = linear_model.LinearRegression()\n\n# Train the model using the training sets\nregr.fit(diabetes_X_train, diabetes_y_train)\n\n# Make predictions using the testing set\ndiabetes_y_pred = regr.predict(diabetes_X_test)\n\n# The coefficients\nprint('Coefficients: \\n', regr.coef_)\n# The mean squared error\nprint(\"Mean squared error: %.2f\"\n      % mean_squared_error(diabetes_y_test, diabetes_y_pred))\n# Explained variance score: 1 is perfect prediction\nprint('Variance score: %.2f' % r2_score(diabetes_y_test, diabetes_y_pred))\n\n# Plot outputs\nplt.scatter(diabetes_X_test, diabetes_y_test,  color='black')\nplt.plot(diabetes_X_test, diabetes_y_pred, color='blue', linewidth=3)\n\nplt.xticks(())\nplt.yticks(())\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5683fef4f030f86ff9c4880f775e36694e291142"},"cell_type":"markdown","source":"### Ex 2: Polynomial interpolation"},{"metadata":{"trusted":true,"_uuid":"638ff80c1a4c631552cc505fa09cf72c67d28386"},"cell_type":"code","source":"#http://scikit-learn.org/stable/auto_examples/linear_model/plot_polynomial_interpolation.html\n# Author: Mathieu Blondel\n#         Jake Vanderplas\n# License: BSD 3 clause\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\n\n\ndef f(x):\n    \"\"\" function to approximate by polynomial interpolation\"\"\"\n    return x * np.sin(x)\n\n\n# generate points used to plot\nx_plot = np.linspace(0, 10, 100)\n\n# generate points and keep a subset of them\nx = np.linspace(0, 10, 100)\nrng = np.random.RandomState(0)\nrng.shuffle(x)\nx = np.sort(x[:20])\ny = f(x)\n\n# create matrix versions of these arrays\nX = x[:, np.newaxis]\nX_plot = x_plot[:, np.newaxis]\n\ncolors = ['teal', 'yellowgreen', 'gold']\nlw = 2\nplt.plot(x_plot, f(x_plot), color='cornflowerblue', linewidth=lw,\n         label=\"ground truth\")\nplt.scatter(x, y, color='navy', s=30, marker='o', label=\"training points\")\n\nfor count, degree in enumerate([3, 4, 5]):\n    model = make_pipeline(PolynomialFeatures(degree), Ridge())\n    model.fit(X, y)\n    y_plot = model.predict(X_plot)\n    plt.plot(x_plot, y_plot, color=colors[count], linewidth=lw,\n             label=\"degree %d\" % degree)\n\nplt.legend(loc='lower left')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"48b56985fe0cd5a8507cb1ef1251c6b792b81247"},"cell_type":"markdown","source":"### Ex 3: Isotonic Regression"},{"metadata":{"trusted":true,"_uuid":"f0043a099fbda48f82de9ae0c7fd27ffb4de5e07"},"cell_type":"code","source":"#http://scikit-learn.org/stable/auto_examples/plot_isotonic_regression.html#sphx-glr-auto-examples-plot-isotonic-regression-py\n# Author: Nelle Varoquaux <nelle.varoquaux@gmail.com>\n#         Alexandre Gramfort <alexandre.gramfort@inria.fr>\n# License: BSD\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.collections import LineCollection\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.isotonic import IsotonicRegression\nfrom sklearn.utils import check_random_state\n\n# Data is genereted randomly\nn = 100\nx = np.arange(n)\nrs = check_random_state(0)\ny = rs.randint(-50, 50, size=(n,)) + 50. * np.log1p(np.arange(n))\n\n# #############################################################################\n# Fit IsotonicRegression and LinearRegression models\n\nir = IsotonicRegression()\n\ny_ = ir.fit_transform(x, y)\n\nlr = LinearRegression()\nlr.fit(x[:, np.newaxis], y)  # x needs to be 2d for LinearRegression\n\n# #############################################################################\n# Plot result\n\nsegments = [[[i, y[i]], [i, y_[i]]] for i in range(n)]\nlc = LineCollection(segments, zorder=0)\nlc.set_array(np.ones(len(y)))\nlc.set_linewidths(np.full(n, 0.5))\n\nfig = plt.figure()\nplt.plot(x, y, 'r.', markersize=12)\nplt.plot(x, y_, 'g.-', markersize=12)\nplt.plot(x, lr.predict(x[:, np.newaxis]), 'b-')\nplt.gca().add_collection(lc)\nplt.legend(('Data', 'Isotonic Fit', 'Linear Fit'), loc='lower right')\nplt.title('Isotonic regression')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"73a6bdbdf4cf978c835249ede22915c8e8992efb"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}