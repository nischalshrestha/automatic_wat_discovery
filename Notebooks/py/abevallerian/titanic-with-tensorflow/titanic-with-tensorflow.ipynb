{"cells": [{"metadata": {"_uuid": "304d862d352d8ead504f586ce8210c130a35602f", "_cell_guid": "54a3948b-713d-4a07-9ba6-ac718c06a132"}, "cell_type": "markdown", "source": ["# Logistic Regression with Tensorflow\n", "\n", "I know it is quite overkill to use Tensorflow for this task, but I just learned using Tensorflow and I want to apply what I've learned in this task. Basically, I'm going to build Logistic Regression using Tensorflow. So, let's begin!\n", "\n", "First, I start importing the libraries and loading the data."]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "d477307493c72c7655bdb0fff66ace25fe355a94", "_cell_guid": "1cfbe74d-4e3a-4f90-8f60-ffa71f32d8fa"}, "source": ["import numpy as np\n", "import pandas as pd\n", "from sklearn.model_selection import train_test_split\n", "import tensorflow as tf"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "2df18247179fb7ef4e8742d04fa4807c5ffd8a2f", "_cell_guid": "3057de14-8cf2-4893-9314-80dba16984ce"}, "source": ["train = pd.read_csv('../input/train.csv')\n", "test = pd.read_csv('../input/test.csv')"]}, {"metadata": {"_uuid": "276c2b5d79cc5f941e70be9978f5866b1c21926d", "_cell_guid": "34b2700e-4ee4-4a26-b5a7-add24f566635"}, "cell_type": "markdown", "source": ["## Preprocessing the Data\n", "Let's just take a quick view of the data."]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"scrolled": true, "collapsed": true, "_uuid": "cf7c94315f3922587035f20cbc8329ae9053b65e", "_cell_guid": "23fc8036-3ce3-4ac7-a429-07485d05471f"}, "source": ["train.head()"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"scrolled": true, "collapsed": true, "_uuid": "959e5403851711e14b7c99ff41b9c900d31c2f73", "_cell_guid": "f6fccb06-8451-4d26-b0be-90562ca14693"}, "source": ["train.describe()"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"scrolled": true, "collapsed": true, "_uuid": "4f9eb7022143c110b7bd0e52603ad06094974c8c", "_cell_guid": "72b78491-a105-41d9-a43c-b6e317ff4295"}, "source": ["test.head()"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "ba7464fe14bcef110c4afce8877ff56be2dbe6c8", "_cell_guid": "c24ece66-2214-4c4e-8f09-981b918cf898"}, "source": ["test.describe()"]}, {"metadata": {"_uuid": "de8258e33b507a4756ef45ee716872b601bfec45", "_cell_guid": "8a10b0e4-c5c2-4791-91c9-b6de78a77c9a"}, "cell_type": "markdown", "source": ["The goal of this project is to predict whether a passenger survives. Therefore, I don't think that *Name*, *Ticket*, *Fare*, and *Embarkment* are related to survival. Just delete those columns from the table. Moreover, there are also several *NaN* in the table. Replace those *NaN*s with 0."]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "77cd356fe0c660bbe7407b4f6df2b1e02a6d5d7f", "_cell_guid": "49096527-4af1-46ce-a9fe-5492715205e0"}, "source": ["del train['Name']\n", "del train['Ticket']\n", "del train['Fare']\n", "del train['Embarked']"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"scrolled": true, "collapsed": true, "_uuid": "bec8739154048a12886275e79718cb0bb4b0dd9c", "_cell_guid": "8d8f6b88-68e1-45db-9753-cb36932a2ffb"}, "source": ["train = train.fillna(value=0.0)"]}, {"metadata": {"_uuid": "65746702bc6c3760a577f1bc7278a620439fa613", "_cell_guid": "0da2fb10-53d8-4ab2-8c24-5d3bfd6eb6f9"}, "cell_type": "markdown", "source": ["1. First, let's preprocess the *Sex*. Just replace it with 0 (Female) or 1 (Male).\n", "2. Then, let's handle the *Age*. Since the age is categorical data, I group the age 8 groups: *NaN*, 0-10, 10-20, ..., 70-80. From the desribe above, it's shown that the maximum age is 80.\n", "3. *Cabin* is quite interesting. It is stored in string. I think the format is written as *Cabin Section + Cabin Number*. I'm only interested in obtaining the *Cabin Section*."]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "0272d2e7b8042932288ab27d1a447a1cb27bafe4", "_cell_guid": "12478a25-42a6-470b-938c-957af41052de"}, "source": ["for i in range(train.shape[0]):\n", "    if train.at[i, 'Sex'] == 'male':\n", "        train.at[i, 'Sex'] = 1\n", "    else:\n", "        train.at[i, 'Sex'] = 0"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "e8851ca16628e4a38d6871c4d1517b9d31fffbd2", "_cell_guid": "f912af27-ab75-40c8-91d4-d69ec126157c"}, "source": ["train['Age_group'] = 0\n", "for i in range(train.shape[0]):\n", "    for j in range(70, 0, -10):\n", "        if train.at[i, 'Age'] > j:\n", "            train.at[i, 'Age_group'] = int(j/10)\n", "            break\n", "del train['Age'] # it's unnecessary anymore"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "dc0e27980e29ab6f9b362b091a6d03cd0f470e38", "_cell_guid": "c202285e-f184-471d-9783-8cd79c484ee2"}, "source": ["print(list(set(train['Cabin'].values))[:10]) # sample of 'Cabin' values\n", "train['Cabin_section'] = '0'\n", "for i in range(train.shape[0]):\n", "    if train.at[i, 'Cabin'] != 0:\n", "        train.at[i, 'Cabin_section'] = train.at[i, 'Cabin'][0]\n", "CABIN_SECTION = list(set(train['Cabin_section'].values)) # will be reused for test data\n", "print(CABIN_SECTION) # 'Cabin_Section' values\n", "for i in range(train.shape[0]):\n", "    train.at[i, 'Cabin_section'] = CABIN_SECTION.index(train.at[i, 'Cabin_section'])\n", "del train['Cabin'] # it's unnecessary anymore"]}, {"metadata": {"_uuid": "de567fb6a6985f2d093a3e6355112d102869492d", "_cell_guid": "13ba756d-db72-43cb-9a95-e5c48258b250"}, "cell_type": "markdown", "source": ["I've done with the preprocessing. Here is the result."]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "3ebd8d1b2fdafcf9edbceedd2e914ce2e33b6e50", "_cell_guid": "aa0825ff-6691-409e-9bbf-a5802cf88bb0"}, "source": ["train.head()"]}, {"metadata": {"_uuid": "7e7e405dd8535e35862b8f505c0bd3ef4485e7c9", "_cell_guid": "c6de35c9-52ba-4817-adba-b6455f7aeb70"}, "cell_type": "markdown", "source": ["What's next is preparing the numpy array for the input of Tensorflow. I need to convert the categorical data (*Pclass*, *Age_group*, and *Cabin_section*) into *one hot* array using np.eye. Then, divide the data into training and dev set."]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "19be0a2fa6cc9bfdd3f31d13ee98183c86f0f10c", "_cell_guid": "1efe0a95-2f2e-4706-bc02-ab8d39817878"}, "source": ["pclass = np.eye(train['Pclass'].values.max()+1)[train['Pclass'].values]\n", "age_group = np.eye(train['Age_group'].values.max()+1)[train['Age_group'].values]\n", "cabin_section = np.eye(train['Cabin_section'].values.max()+1) \\\n", "                    [train['Cabin_section'].values.astype(int)] # prevent IndexError"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "1908ef641d0038dff9a273e9053c3fc8235b5ffb", "_cell_guid": "deccf367-69fc-4048-be97-2c71d3188d09"}, "source": ["X = train[['Sex', 'SibSp', 'Parch']].values\n", "X = np.concatenate([X, age_group], axis=1)\n", "X = np.concatenate([X, pclass], axis=1)\n", "X = np.concatenate([X, cabin_section], axis=1)\n", "X = X.astype(float)\n", "\n", "y = train['Survived'].values\n", "y = y.astype(float).reshape(-1, 1)"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "9e930094d7790d619bf443186b7fc99b0745f4f4", "_cell_guid": "291a631f-bb72-4cf6-8fac-58e1a01c8881"}, "source": ["X_train, X_dev, y_train, y_dev = train_test_split(X, y, test_size=0.1, random_state=0)"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "087d8d30992f7634c060de39b237272f8582a2c4", "_cell_guid": "b2ca280c-ef22-463d-9c01-20f459db9b44"}, "source": ["print(X_train.shape, y_train.shape)"]}, {"metadata": {"_uuid": "16c960e2a1c09efb722d2e2a39cb0b3768aa798e", "_cell_guid": "cbb219f9-5078-421e-a561-396948e0351a"}, "cell_type": "markdown", "source": ["Repeat the preprocessing for the test data as well."]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "baebb61f7665a6e19e5f922448339285d2fa98a8", "_cell_guid": "4f9cf1d5-cf23-4898-8942-07eb2f6277fe"}, "source": ["del test['Name']\n", "del test['Ticket']\n", "del test['Fare']\n", "del test['Embarked']\n", "\n", "test = test.fillna(value=0.0)\n", "\n", "test['Age_group'] = 0\n", "test['Cabin_section'] = '0'\n", "for i in range(test.shape[0]):\n", "    if test.at[i, 'Sex'] == 'male':\n", "        test.at[i, 'Sex'] = 1\n", "    else:\n", "        test.at[i, 'Sex'] = 0\n", "\n", "    for j in range(70, 0, -10):\n", "        if test.at[i, 'Age'] > j:\n", "            test.at[i, 'Age_group'] = int(j/10)\n", "            break\n", "\n", "    if test.at[i, 'Cabin'] != 0:\n", "        test.at[i, 'Cabin_section'] = test.at[i, 'Cabin'][0]\n", "    test.at[i, 'Cabin_section'] = CABIN_SECTION.index(test.at[i, 'Cabin_section'])\n", "\n", "del test['Cabin'] # it's unnecessary anymore\n", "del test['Age'] # it's unnecessary anymore"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "182bbfc7540b9e2f04f6f2739f321097676313d2", "_cell_guid": "c60b7624-41a3-4cc0-bac4-7c96da57c4a1"}, "source": ["test.head()"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "f4bba6a4daeba396a21b89ca7b0b278c78ba828b", "_cell_guid": "31deadf3-1b9b-4dd3-ac74-aa9af894424a"}, "source": ["pclass_test = np.eye(test['Pclass'].values.max()+1)[test['Pclass'].values]\n", "age_group_test = np.eye(test['Age_group'].values.max()+1)[test['Age_group'].values]\n", "cabin_section_test = np.eye(test['Cabin_section'].values.max()+1) \\\n", "                    [test['Cabin_section'].values.astype(int)] # prevent IndexError\n", "\n", "X_test = test[['Sex', 'SibSp', 'Parch']].values\n", "X_test = np.concatenate([X_test, age_group_test], axis=1)\n", "X_test = np.concatenate([X_test, pclass_test], axis=1)\n", "X_test = np.concatenate([X_test, cabin_section_test], axis=1)\n", "X_test = X_test.astype(float)\n", "\n", "id_test = test['PassengerId'].values\n", "id_test = id_test.reshape(-1, 1)"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"scrolled": true, "collapsed": true, "_uuid": "d11b1333617d3aed45149173717ee38f33e50ad7", "_cell_guid": "bfa30d78-d3e3-4f73-922a-0ac4d9a8d03f"}, "source": ["print(X_test.shape, id_test.shape)"]}, {"metadata": {"_uuid": "89d27c6d5b4109fb5e7170760dfdb1fa9f150a27", "_cell_guid": "1947257e-cc0d-4c27-9b13-7076ee081d52"}, "cell_type": "markdown", "source": ["## Building the Neural Network\n", "Let's start by defining the hyperparameters"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "5e82f7ba1a23be1501d33dad6b462a904d608709", "_cell_guid": "9baac1a4-9bf5-40c4-be17-ba8b1b02570c"}, "source": ["seed = 7 # for reproducible purpose\n", "input_size = X_train.shape[1] # number of features\n", "learning_rate = 0.001 # most common value for Adam\n", "epochs = 8500 # I've tested previously that this is the best epochs to avoid overfitting"]}, {"metadata": {"_uuid": "1d1359e39b68cc470076b1aaa0352fa20add31b1", "_cell_guid": "44b85950-8dc4-42eb-905e-2d19dab3686d"}, "cell_type": "markdown", "source": ["The Logistic Regression looks like this: W1\\*X + b1 = pred, where \\* is the matrix multiplication and sigmoid is used as activation function at the output layer. *Cross Entropy* and *Adam Optimizer* are used as the loss function and optimizer."]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"scrolled": true, "collapsed": true, "_uuid": "ba18a2b4bfc98cb1978876895a88a049b060a6d1", "_cell_guid": "ceea2310-e4d4-48d3-b1e1-04fd8d6389cc"}, "source": ["graph = tf.Graph()\n", "with graph.as_default():\n", "    tf.set_random_seed(seed)\n", "    np.random.seed(seed)\n", "\n", "    X_input = tf.placeholder(dtype=tf.float32, shape=[None, input_size], name='X_input')\n", "    y_input = tf.placeholder(dtype=tf.float32, shape=[None, 1], name='y_input')\n", "    \n", "    W1 = tf.Variable(tf.random_normal(shape=[input_size, 1], seed=seed), name='W1')\n", "    b1 = tf.Variable(tf.random_normal(shape=[1], seed=seed), name='b1')\n", "    sigm = tf.nn.sigmoid(tf.add(tf.matmul(X_input, W1), b1), name='pred')\n", "    \n", "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y_input,\n", "                                                                  logits=sigm, name='loss'))\n", "    train_steps = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n", "\n", "    pred = tf.cast(tf.greater_equal(sigm, 0.5), tf.float32, name='pred') # 1 if >= 0.5\n", "    acc = tf.reduce_mean(tf.cast(tf.equal(pred, y_input), tf.float32), name='acc')\n", "    \n", "    init_var = tf.global_variables_initializer()"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "a12f3e95eb67c34abc237f7d072f2bccc4af3f43", "_cell_guid": "878191e3-5d61-46dd-bc04-cf301e05475d"}, "source": ["train_feed_dict = {X_input: X_train, y_input: y_train}\n", "dev_feed_dict = {X_input: X_dev, y_input: y_dev}\n", "test_feed_dict = {X_input: X_test} # no y_input since the goal is to predict it"]}, {"metadata": {"_uuid": "138a11e1d0f4d4bdee48c20cf63ba37675919b37", "_cell_guid": "13ccf448-b160-48aa-bb95-5cad170f0d7c"}, "cell_type": "markdown", "source": ["## Training the Network\n", "Let's start the training. I initialize the session and variables first and start the training. During training, the loss and accuracy are printed."]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "5aa9f28ab236a2553d24c5baf540bf452416d7f3", "_cell_guid": "830a610d-65a1-49c3-b567-37066beeff9d"}, "source": ["sess = tf.Session(graph=graph)\n", "sess.run(init_var)"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"scrolled": true, "collapsed": true, "_uuid": "88eec1b222eba807e0975ceaee55b268d6fc9043", "_cell_guid": "52bb4657-3d99-4d2c-9b13-650a7953097b"}, "source": ["cur_loss = sess.run(loss, feed_dict=train_feed_dict)\n", "train_acc = sess.run(acc, feed_dict=train_feed_dict)\n", "test_acc = sess.run(acc, feed_dict=dev_feed_dict)\n", "print('step 0: loss {0:.5f}, train_acc {1:.2f}%, test_acc {2:.2f}%'.format(\n", "                       cur_loss, 100*train_acc, 100*test_acc))\n", "for step in range(1, epochs+1):\n", "    sess.run(train_steps, feed_dict=train_feed_dict)\n", "    cur_loss = sess.run(loss, feed_dict=train_feed_dict)\n", "    train_acc = sess.run(acc, feed_dict=train_feed_dict)\n", "    test_acc = sess.run(acc, feed_dict=dev_feed_dict)\n", "    if step%100 != 0: # print result every 100 steps\n", "        continue\n", "    print('step {3}: loss {0:.5f}, train_acc {1:.2f}%, test_acc {2:.2f}%'.format(\n", "                       cur_loss, 100*train_acc, 100*test_acc, step))"]}, {"metadata": {"_uuid": "c8adc7882329ccd6d43ec1507effcc8ee363c28b", "_cell_guid": "f90ec9e2-dfda-4662-a01a-770d57b3fa18"}, "cell_type": "markdown", "source": ["## Evaluating the Network\n", "Actually the network performance is not very good (only around 80%). Finally, I need to prepare the prediction."]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "19a93c8ba9198c49fbfa19e652c24ba359a5d517", "_cell_guid": "1814d9ca-894f-4bdb-a5f4-bef94bd260ef"}, "source": ["y_pred = sess.run(pred, feed_dict=test_feed_dict).astype(int)\n", "prediction = pd.DataFrame(np.concatenate([id_test, y_pred], axis=1),\n", "                          columns=['PassengerId', 'Survived'])"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"scrolled": true, "collapsed": true, "_uuid": "0673bdfe031f40ac969b065f2cf765f43fa4a882", "_cell_guid": "139859b0-017a-49bf-9640-4b7d7b8a6d8e"}, "source": ["prediction.head()"]}, {"metadata": {"_uuid": "877c1c7a2eb90fa5d8b7292807fc10fe7a152157", "_cell_guid": "8839a202-8b9f-4282-b5c7-5eb127adc061"}, "cell_type": "markdown", "source": ["## Takeaways\n", "1. I think I'm not doing enough Exploratory Data Analysis, which I think very crucial in beginning the project.\n", "2.  80% accuracy in train and dev set is not very good actually. I think other models such as Random Forest will produce better accuracy.\n", "3. Even if Logistic Regression should be used, using Tensorflow is not very efficient. There are many build-in libraries for Logistic Regression (e.g. Scikit-Learn)."]}, {"metadata": {"collapsed": true, "_uuid": "6578fdcce4e3c968f29036fc3fc59ff7f53b2bba", "_cell_guid": "a95e5ec0-ac6a-4a38-9edd-88064d59d8cc"}, "cell_type": "markdown", "source": ["Any feedbacks are very welcomed!"]}, {"outputs": [], "execution_count": null, "cell_type": "code", "metadata": {"collapsed": true}, "source": []}], "nbformat": 4, "metadata": {"language_info": {"pygments_lexer": "ipython3", "version": "3.6.3", "mimetype": "text/x-python", "codemirror_mode": {"version": 3, "name": "ipython"}, "file_extension": ".py", "name": "python", "nbconvert_exporter": "python"}, "kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}}, "nbformat_minor": 1}