{"cells":[{"metadata":{"_uuid":"967c83a7cf13052659f1212d7327b10800a4ea83"},"cell_type":"markdown","source":"* [2. EDA](#eda)\n* [3. Feature engineering](#engineering)\n* [4. Train model](#train)\n* [5. Submit prediction](#submit)\n* [6. Final score and position](#score)\n* [7. Reference kernel](#reference)"},{"metadata":{"_uuid":"adac806e439b6d561e035e972ebaeb73c39e30c2"},"cell_type":"markdown","source":"## 0. Import necessary utilities"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom collections import Counter\nimport math\nimport scipy.stats as stats\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nimport time\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import SGDClassifier\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, BatchNormalization\nfrom keras.optimizers import Adam\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b6d503d58b15dd14d9c2b50d049a8ec312cdae04"},"cell_type":"code","source":"# turn off warning: SettingWithCopyWarning\npd.set_option('chained_assignment', None)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c913ee06ed99bccda7533bee152cf87e741733b5"},"cell_type":"markdown","source":"## 1. Load data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"df_csv_train = pd.read_csv(\"../input/train.csv\") # 891 samples\ndf_csv_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ac3e17e471af519b9d98f156ac6f1ac5cc86348","collapsed":true},"cell_type":"code","source":"df_csv_test = pd.read_csv(\"../input/test.csv\")  # 418 samples\ndf_csv_test.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"050cc9950342ad8d3b82cf37bdfdd9a97c06d172"},"cell_type":"markdown","source":"<a id='eda'></a>\n## 2. EDA\n[Refer to my kernel for EDA](https://www.kaggle.com/vincentman0403/titanic-survival-prediction-eda)"},{"metadata":{"_uuid":"99cb7973a594917a6ec337e6bd1c10a20bb7d5d9"},"cell_type":"markdown","source":"<a id='engineering'></a>\n## 3. Feature engineering"},{"metadata":{"trusted":true,"_uuid":"6c0674563c90c76692173be2c055b45d67c44f3d","collapsed":true},"cell_type":"code","source":"dataset = pd.concat(objs=[df_csv_train, df_csv_test], axis=0, sort=True).reset_index(drop=True)\ndataset.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1325f7ad97372f3646727c3ec0f28b3b8fb5e36e"},"cell_type":"markdown","source":"### 3.1 \"Fare\" value processing "},{"metadata":{"trusted":true,"_uuid":"7258db44e8eed40eb48bbb348befe53679776709","collapsed":true},"cell_type":"code","source":"# Fill Fare missing values with the median value\nprint('Null count of Fare before fillna: ', dataset[\"Fare\"].isnull().sum())\ndataset[\"Fare\"] = dataset[\"Fare\"].fillna(dataset[\"Fare\"].median())\nprint('Null count of Fare after fillna: ', dataset[\"Fare\"].isnull().sum())\n# Apply log to Fare to reduce skewness distribution\nprint('Skewness of Fare before log: ', stats.skew(dataset[\"Fare\"]))\ndataset[\"Fare\"] = dataset[\"Fare\"].map(lambda i: np.log(i) if i > 0 else 0)\nprint('Skewness of Fare after log: ', stats.skew(dataset[\"Fare\"]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fadf9f6648a792dbfd2bfd00d72deedf04daf49e"},"cell_type":"markdown","source":"### 3.2 \"Embarked\" value processing "},{"metadata":{"trusted":true,"_uuid":"26d3187b53db690d21be93fe18a093edf7d201e9","collapsed":true},"cell_type":"code","source":"# Fill Embarked null values of dataset set with 'S' most frequent value\nprint('Null count of Embarked before fillna: ', dataset[\"Embarked\"].isnull().sum())\ndataset[\"Embarked\"] = dataset[\"Embarked\"].fillna(\"S\")\nprint('Null count of Embarked after fillna: ', dataset[\"Embarked\"].isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"481b7ca16b60ea25ee37eb10fb7e8ae7f4c12ea8"},"cell_type":"markdown","source":"### 3.3 \"Sex\" value processing"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d954d99a962f47f005055c08bd4d15c92cd2b4d8"},"cell_type":"code","source":"# convert Sex into categorical value 0 for male and 1 for female\ndataset[\"Sex\"] = dataset[\"Sex\"].map({\"male\": 0, \"female\": 1})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c22c1cee9bd8e9f5b6fe16c30291c636e829c33b"},"cell_type":"markdown","source":"### 3.4 \"Age\" value processing\nIn order to fill Age null value, I pick out samples whose Age value is null. Then I pick out samples(Samples_A) whose SibSp, Parch, Pclass values are the same as these values of samples whose Age value is null. Finally I  use median of Age value of Samples_A to fill  Age null value."},{"metadata":{"trusted":true,"_uuid":"547786ffacf79457451fb72d488f3edc044bd6bd","collapsed":true},"cell_type":"code","source":"print('NaN value count of Age before fillna: ', dataset[\"Age\"].isnull().sum())\nindex_NaN_age = list(dataset[\"Age\"][dataset[\"Age\"].isnull()].index)\nfor i in index_NaN_age:\n    age_med = dataset[\"Age\"].median()\n    age_pred = dataset[\"Age\"][(\n            (dataset['SibSp'] == dataset.iloc[i][\"SibSp\"]) & (\n            dataset['Parch'] == dataset.iloc[i][\"Parch\"]) & (\n                    dataset['Pclass'] == dataset.iloc[i][\"Pclass\"]))].median()\n    if not np.isnan(age_pred):\n        dataset['Age'].iloc[i] = age_pred\n    else:\n        dataset['Age'].iloc[i] = age_med\nprint('NaN value count of Age after fillna: ', dataset[\"Age\"].isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0715f7e8c66a5bf16f549b49b4e0a5705bb89a25"},"cell_type":"markdown","source":"### 3.5 Extract \"Name\" and create a new \"Title\" feature\nBecause \"Name\" value contains \"Title\" information such as \"Mr\", \"Mrs\", \"Master\", etc..., I try to extract \"Title\" from \"Name\", and classify \"Title\" into fewer classes. "},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"caa3611133b0a3ffbf78ef1a16059ce16619ae78"},"cell_type":"code","source":"# Get Title from Name\ndataset_title = [i.split(\",\")[1].split(\".\")[0].strip() for i in dataset[\"Name\"]]\ndataset[\"Title\"] = pd.Series(dataset_title)\n# Convert to categorical values Title\ndataset[\"Title\"] = dataset[\"Title\"].replace(['Lady', 'the Countess', 'Countess', 'Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ndataset[\"Title\"] = dataset[\"Title\"].map({\"Master\": 0, \"Miss\": 1, \"Ms\": 1, \"Mme\": 1, \"Mlle\": 1, \"Mrs\": 1, \"Mr\": 2, \"Rare\": 3})\ndataset[\"Title\"] = dataset[\"Title\"].astype(int)\n# Drop Name variable\ndataset.drop(labels=[\"Name\"], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"345776acd4d801ccfda80eb9339b54852dfdd610"},"cell_type":"markdown","source":"### 3.6 Combine \"SibSp\" and \"Parch\" to a new \"Fsize\" feature\n I try to create a \"Fize\" (family size) feature which is the sum of SibSp , Parch and 1 (including the passenger)."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0c5de9d0e143bede2107f327b1489bd5d6874113"},"cell_type":"code","source":"dataset[\"Fsize\"] = dataset[\"SibSp\"] + dataset[\"Parch\"] + 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"eccf04ce2336010c764e4edab704191ef1bf565d"},"cell_type":"code","source":"# Create new features for family size\ndataset['Single'] = dataset['Fsize'].map(lambda s: 1 if s == 1 else 0)\ndataset['SmallF'] = dataset['Fsize'].map(lambda s: 1 if s == 2 else 0)\ndataset['MedF'] = dataset['Fsize'].map(lambda s: 1 if 3 <= s <= 4 else 0)\ndataset['LargeF'] = dataset['Fsize'].map(lambda s: 1 if s >= 5 else 0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d3d5dbc2a50d076f40c14ebe876574bcfee24aaa"},"cell_type":"markdown","source":"### 3.7 \"Cabin\" value processing\nIf \"Cabin\" value is null I replace null with \"X\", otherwise I replace value with first character."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e0a6c926143b80a4469c35caa7352bad2d060bfb"},"cell_type":"code","source":"dataset[\"Cabin\"] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in dataset['Cabin']])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0606a97dcc2589b828b730544aa20f32e92b20f5"},"cell_type":"markdown","source":"### 3.8 \"Ticket\" value processing\nI try to replace \"Ticket\" value with its prefix. If there is no prefix, I replace it with \"X\".  "},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"51dcac3fab46db5e1098acdb9e09b2fd82652ba4"},"cell_type":"code","source":"Ticket = []\nfor i in list(dataset.Ticket):\n    if not i.isdigit():\n        Ticket.append(i.replace(\".\", \"\").replace(\"/\", \"\").strip().split(' ')[0])  # Take prefix\n    else:\n        Ticket.append(\"X\")\ndataset[\"Ticket\"] = Ticket","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"799404677ce2660e32b9d390ce32b875f7954f56"},"cell_type":"markdown","source":"### 3.9 Convert categorical variables into dummy variables"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"fc1fb7ba244802e85055d934a0816f295f997605"},"cell_type":"code","source":"dataset = pd.get_dummies(dataset, columns=[\"Title\"])\ndataset = pd.get_dummies(dataset, columns=[\"Embarked\"], prefix=\"Em\")\ndataset = pd.get_dummies(dataset, columns=[\"Cabin\"], prefix=\"Cabin\")\ndataset = pd.get_dummies(dataset, columns=[\"Ticket\"], prefix=\"T\")\ndataset = pd.get_dummies(dataset, columns=[\"Pclass\"], prefix=\"Pc\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9d229f7058fbe8180a1ab045cf2893682451d8d5"},"cell_type":"code","source":"# Drop useless variables\ndataset.drop(labels=[\"PassengerId\"], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"95c26da48f80ac799c32ad307c9f9206a8fce840"},"cell_type":"markdown","source":"### 3.10 Split train and test data"},{"metadata":{"trusted":true,"_uuid":"f67f9fc853c3466b343e6abd3a40d35320ce6c53","collapsed":true},"cell_type":"code","source":"df_test = dataset[dataset['Survived'].isnull()]\nprint('test data shape: ', df_test.shape)\ndf_train = dataset[dataset['Survived'].notnull()]\nprint('train data shape: ', df_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cc1d6bc89f0a29271d79c9f5227a1443f81b234c"},"cell_type":"markdown","source":"### 3.11 Drop outliers of train data\nI select \"Age\", \"SibSp\", \"Parch\", \"Fare\" features to detect outlier samples. First, I try to find samples whose feature value is larger than *[Q3 + (1.5 x IQR)] * or less than *[Q1 - (1.5 x IQR)] *. Then I drop samples which have more than two outlier features."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"51fceb75a119386f4bf7cc4844ed1c1f9138c823"},"cell_type":"code","source":"def drop_outliers(df, n, features):\n    outlier_indices = []\n\n    # iterate over features(columns)\n    for col in features:\n        # 1st quartile (25%)\n        Q1 = np.percentile(df[col], 25)\n        # 3rd quartile (75%)\n        Q3 = np.percentile(df[col], 75)\n        # Interquartile range (IQR)\n        IQR = Q3 - Q1\n\n        # outlier step\n        outlier_step = 1.5 * IQR\n\n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step)].index\n\n        # append the found outlier indices for col to the list of outlier indices\n        outlier_indices.extend(outlier_list_col)\n\n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)\n    multiple_outliers = list(k for k, v in outlier_indices.items() if v > n)\n    print('drop outlier samples id: ', multiple_outliers)\n\n    df = df.drop(multiple_outliers, axis=0)\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4546e2aaee7d23e463342a287c41d9a54ca6166f","collapsed":true},"cell_type":"code","source":"df_train = drop_outliers(df_train, 2, [\"Age\", \"SibSp\", \"Parch\", \"Fare\"])\nprint('After drop outlier samples, train data shape: ', df_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3d85984dd526131d36e9b8f16337d774560e6e17"},"cell_type":"code","source":"# Convert Survived dtype as int\ndf_train['Survived'] = df_train['Survived'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"daa6909e2cd2bcff59cee414dc0ae08118006eac"},"cell_type":"markdown","source":"<a id='train'></a>\n## 4. Train model"},{"metadata":{"trusted":true,"_uuid":"a6fe5b85a7036f907b183c6759183700cd7f0d49","collapsed":true},"cell_type":"code","source":"# Split train data into x(features) and y(labels)\ny_train = df_train.Survived\nx_train = df_train.drop(['Survived'], axis=1)\n# Standardize train data\nx_train_std = StandardScaler().fit_transform(x_train.values)\ny_train = y_train.values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"463b57396340890fe6cb4ff4e69b7e3678d98626"},"cell_type":"markdown","source":"### 4.1 sklearn's RandomForestClassifier with GridSearch"},{"metadata":{"trusted":true,"_uuid":"60d2c65931a28176e61f65830a011075bbd99ae4","collapsed":true},"cell_type":"code","source":"clf = RandomForestClassifier(n_estimators=1000, random_state=0, n_jobs=-1, bootstrap=False)\nkfold = StratifiedKFold(n_splits=10)\nparam_max_depth = [None]\nparam_min_samples_split = [2, 3, 10]\nparam_min_samples_leaf = [1, 3, 10]\nparam_max_features = [1, 3, 10]\nparam_n_estimators = [1100]\nparam_grid = {\"max_depth\": param_max_depth,\n              \"max_features\": param_max_features,\n              \"min_samples_split\": param_min_samples_split,\n              \"min_samples_leaf\": param_min_samples_leaf,\n              \"n_estimators\": param_n_estimators,\n              }\ngs = GridSearchCV(estimator=clf,\n                  param_grid=param_grid,\n                  scoring='accuracy',\n                  cv=kfold, iid=False)\nstart = time.time()\ngs.fit(x_train, y_train)\nend = time.time()\nelapsed_train_time = 'Random Forest, elapsed training time: {} min, {} sec '.format(int((end - start) / 60),\n                                                                                    int((end - start) % 60))\nprint(elapsed_train_time)\nprint('--------------------------------------------')\nprint(gs.best_estimator_)\nprint('--------------------------------------------')\nprint('Random Forest, train best score: {}'.format(gs.best_score_))\nprint('Random Forest, train best param: {}'.format(gs.best_params_))\nrandom_forest_clf = gs.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6d5eed5b61ddbab8471ac73049d02951b78c8278"},"cell_type":"markdown","source":"### 4.2 sklearn's SVC with GridSearch"},{"metadata":{"trusted":true,"_uuid":"522bed3ceb1f03f73b1552889186ab8822e47925","collapsed":true},"cell_type":"code","source":"param_C = [1, 10, 50, 100, 200, 300, 1000]\nparam_gamma = [0.0001, 0.001, 0.01, 0.1, 1.0]\nparam_grid = {'C': param_C, 'gamma': param_gamma, 'kernel': ['rbf']}\nsvm = SVC(random_state=0, verbose=False)\nkfold = StratifiedKFold(n_splits=10)\ngs = GridSearchCV(estimator=svm,\n                  param_grid=param_grid,\n                  scoring='accuracy',\n                  cv=kfold, iid=False)\nstart = time.time()\ngs.fit(x_train_std, y_train)\nend = time.time()\nelapsed_train_time = 'SVM, elapsed training time: {} min, {} sec '.format(int((end - start) / 60),\n                                                                          int((end - start) % 60))\nprint(elapsed_train_time)\nprint('--------------------------------------------')\nprint(gs.best_estimator_)\nprint('--------------------------------------------')\nprint('SVM, train best score: {}'.format(gs.best_score_))\nprint('SVM, train best param: {}'.format(gs.best_params_))\nsvm_clf = gs.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"18d3edcf45a4c024599993877d57c450e6da67b9"},"cell_type":"markdown","source":"### 4.3 sklearn's SGDClassifier with GridSearch"},{"metadata":{"trusted":true,"_uuid":"09200d15437161630ef6f475d465f2bd33c3828d","collapsed":true},"cell_type":"code","source":"param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\nparam_l1_ratio = np.arange(0.1, 1, 0.1)\nparam_grid = {'loss': ['hinge'], 'alpha': param_range, 'l1_ratio': param_l1_ratio}\nkfold = StratifiedKFold(n_splits=5)\nsgd = SGDClassifier(loss='hinge', verbose=0, max_iter=None, penalty='elasticnet', tol=1e-3)\ngs = GridSearchCV(estimator=sgd,\n                  param_grid=param_grid,\n                  scoring='accuracy',\n                  cv=kfold, iid=False)\nstart = time.time()\ngs.fit(x_train_std, y_train)\nend = time.time()\nelapsed_train_time = 'SGD with SVM, elapsed training time: {} min, {} sec '.format(int((end - start) / 60),\n                                                                                   int((end - start) % 60))\nprint(elapsed_train_time)\nprint('--------------------------------------------')\nprint(gs.best_estimator_)\nprint('--------------------------------------------')\nprint('SGD with SVM at GridSearch, train best score: {}'.format(gs.best_score_))\nprint('SGD with SVM at GridSearch, train best param: {}'.format(gs.best_params_))\nsgd_clf = gs.best_estimator_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"9058177b6b1f9a1f288af319ce70bfca39431bd1"},"cell_type":"markdown","source":"### 4.4 Keras's model: MLP(Multiple Layer Perceptron )"},{"metadata":{"_uuid":"02b3f3c3ec8f900d4fa612e97023c1e2f248eff2"},"cell_type":"markdown","source":"#### 4.4.1 Create model "},{"metadata":{"trusted":true,"_uuid":"34b318425ee31dda5151a1fbcccad0471963365e","collapsed":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Dense(units=40, input_dim=x_train.shape[1], kernel_initializer='uniform'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\n\nmodel.add(Dense(units=30, kernel_initializer='uniform'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\n\nmodel.add(Dense(units=30, kernel_initializer='uniform'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\n\nmodel.add(Dense(units=1, activation='sigmoid'))\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eb07845d94af830188554642f1a412e4292d0c3c"},"cell_type":"markdown","source":"#### 4.4.2 Compile model "},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1f9b06e21bbea5d07d20057b3b3d5a4b6c8848ab"},"cell_type":"code","source":"adam = Adam(lr=0.01, decay=0.001, beta_1=0.9, beta_2=0.9)\nmodel.compile(loss='binary_crossentropy',\n              optimizer=adam, metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9dc5351af7be5a16ee8a182610f1dd4d3b8bfa6e"},"cell_type":"markdown","source":"#### 4.4.3 Fit model"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"bf534263d9a8d151774f2c59568827ec7b0c0888"},"cell_type":"code","source":"def show_train_history(train_history, train_acc, validation_acc, ylabel):\n    plt.plot(train_history.history[train_acc])\n    plt.plot(train_history.history[validation_acc])\n    epoch_num = len(train_history.epoch)\n    final_epoch_train_acc = train_history.history[train_acc][epoch_num - 1]\n    final_epoch_validation_acc = train_history.history[validation_acc][epoch_num - 1]\n    plt.text(epoch_num, final_epoch_train_acc, 'train = {:.3f}'.format(final_epoch_train_acc))\n    plt.text(epoch_num, final_epoch_validation_acc-0.01, 'valid = {:.3f}'.format(final_epoch_validation_acc))\n    plt.title('Train History')\n    plt.ylabel(ylabel)\n    plt.xlabel('Epoch')\n    plt.xlim(xmax=epoch_num+1)\n    plt.legend(['train', 'validation'], loc='upper left')\n    plt.show()\n    return final_epoch_train_acc, final_epoch_validation_acc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f338795404f987a96942fc3d27589efd231f0dec","collapsed":true},"cell_type":"code","source":"start = time.time()\ntrain_history = model.fit(x=x_train_std,\n                          y=y_train,\n                          validation_split=0.1,\n                          epochs=30,\n                          shuffle=True,\n                          batch_size=20, verbose=0)\nend = time.time()\ntrain_acc, validation_acc = show_train_history(train_history, 'acc', 'val_acc', 'accuracy')\ntrain_loss, validation_loss = show_train_history(train_history, 'loss', 'val_loss', 'loss')\nprint('elapsed training time: {} min, {} sec '.format(int((end - start) / 60), int((end - start) % 60)))\nprint('train accuracy = {}, validation accuracy = {}'.format(train_acc, validation_acc))\nprint('train loss = {}, validation loss = {}'.format(train_loss, validation_loss))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fd5a8a1390d1d93f8fb6b7718b63d908828ef6c5"},"cell_type":"markdown","source":"<a id='submit'></a>\n## 5. Submit prediction"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"913cfdcf0a4b9a94588c283b6688d710208a0b9d"},"cell_type":"code","source":"# drop Survived of test data\nx_test = df_test.drop('Survived', axis=1)\n# standardize test data\nx_test_std = StandardScaler().fit_transform(x_test.values)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f45623635e3e692158f5ecface2c606edd1fff1f"},"cell_type":"markdown","source":"### 5.1 Keras's model: MLP"},{"metadata":{"trusted":true,"_uuid":"ca9a6c024a63016b7d43f35837e54ed7e5649b44","collapsed":true},"cell_type":"code","source":"pd.DataFrame({\"PassengerId\": np.arange(892, 1310), \"Survived\": model.predict_classes(x_test_std).ravel()}).to_csv(\n    'submission_mlp.csv', header=True, index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"31697383251ff60e73cb65ed97df84388796dca7"},"cell_type":"markdown","source":"### 5.2 sklearn's model: RandomForestClassifier"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a0ed8787fa3c19dedbeb6d3ae3bb5d3ec3fd8f26"},"cell_type":"code","source":"pd.DataFrame({\"PassengerId\": np.arange(892, 1310), \"Survived\": random_forest_clf.predict(x_test).astype(int)}).to_csv(\n    'submission_rf.csv', header=True, index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b552e47635249ef41cd03cbd3de37621c27988a9"},"cell_type":"markdown","source":"### 5.3 sklearn's model: SVC"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"da49b98e5cb624f41b946a3bbfa5023cce4de3a9"},"cell_type":"code","source":"pd.DataFrame({\"PassengerId\": np.arange(892, 1310), \"Survived\": svm_clf.predict(x_test_std).astype(int)}).to_csv(\n    'submission_svm.csv', header=True, index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"102524bfaedff7b850a315225d24ff13c765085b"},"cell_type":"markdown","source":"### 5.2 sklearn's model: SGDClassifier"},{"metadata":{"trusted":true,"_uuid":"aef99c2448617964aaeebe889d9692b35a9d6dcc","collapsed":true},"cell_type":"code","source":"pd.DataFrame({\"PassengerId\": np.arange(892, 1310), \"Survived\": sgd_clf.predict(x_test_std).astype(int)}).to_csv(\n    'submission_sgd.csv', header=True, index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6c1204c1eb535ac7b43a41a579fa7155ef592f2b"},"cell_type":"markdown","source":"<a id='score'></a>\n## 6. Final score and position (2018/06/28)"},{"metadata":{"_uuid":"03444fde46042d07da02fe8a69bdf4ddf7b94d34"},"cell_type":"markdown","source":"1. MLP, score = 0.76076\n2. RandomForestClassifier, score = 0.79904\n3. SVC, score = 0.78947\n4. SGDClassifier, score = 0.75119\n> Best model is **RandomForestClassifier**, postition is Top **16%**. \n\nSummary: I think this benefits from ensemble method."},{"metadata":{"_uuid":"17539441fbd9febaa3e76a44e0589d687cdc3f7f"},"cell_type":"markdown","source":"<a id='reference'></a>\n## 7. Reference kernel\n[Yassine Ghouzam: Titanic Top 4% with ensemble modeling](https://www.kaggle.com/yassineghouzam/titanic-top-4-with-ensemble-modeling)"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"965e6ea5c97b866ea3a2ed69450555d2fe5eb25d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}