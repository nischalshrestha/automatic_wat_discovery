{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "43c338e5-638d-593e-5c53-7bbb1052a8a9"
      },
      "source": [
        "**This kernel shows:**\n",
        "\n",
        " - Data exploration with some simple yet useful graphs.\n",
        " - Data cleaning.\n",
        " - Feature creation.\n",
        " - Feature selection.\n",
        " - Model score benchmark.\n",
        " - Train and test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d206a1ee-63b3-72a1-051e-525c892f848f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import tree\n",
        "from sklearn import svm\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import fbeta_score\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "49f0563f-5246-df8a-2029-8e99c456ce5a"
      },
      "source": [
        "**I will work with three datasets:**\n",
        "\n",
        " - train: contains the information from train.csv. This one will be used to get statistics and graphs.\n",
        " - test: contains the information from test.csv. This one will be used to get the predicted labels.\n",
        " - data: contains both train and test. This is the one where all data manipulation will be done."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "89a47abc-cfb3-70c2-318d-0352cafc025d"
      },
      "outputs": [],
      "source": [
        "# Read the data:\n",
        "train = pd.read_csv('../input/train.csv')\n",
        "test = pd.read_csv('../input/test.csv')\n",
        "data = pd.concat([train,test],ignore_index=True)\n",
        "labels=train[\"Survived\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0aac8b15-10bb-3bbb-69e7-5fba155da4e0"
      },
      "outputs": [],
      "source": [
        "#\u00a0Functions used in the kernel\n",
        "\n",
        "# Create a graph that groups, counts and check survivors per group\n",
        "def survival_rate(column,t):\n",
        "    df=pd.DataFrame()\n",
        "    df['total']=train.groupby(column).size()\n",
        "    df['survived'] = train.groupby(column).sum()['Survived']\n",
        "    df['percentage'] = round(df['survived']/df['total']*100,2)\n",
        "    print(df)\n",
        "\n",
        "    df['survived'].plot(kind=t)\n",
        "    df['total'].plot(kind=t,alpha=0.5,title=\"Survivors per \"+str(column))\n",
        "    plt.show()\n",
        "\n",
        "# If age is less than 1, we return 1. Else, we return the original age.\n",
        "def normalize_age_below_one(age):\n",
        "    if age < 1:\n",
        "        return 1\n",
        "    else:\n",
        "        return age\n",
        "\n",
        "# Group ages in buckets\n",
        "def group_age(value):\n",
        "    if value <= 10:\n",
        "        return \"0-10\"\n",
        "    elif value <= 20:\n",
        "        return \"10-20\"\n",
        "    elif value <= 30:\n",
        "        return \"20-30\"\n",
        "    elif value <= 40:\n",
        "        return \"30-40\"\n",
        "    elif value <= 50:\n",
        "        return \"40-50\"\n",
        "    elif value <= 60:\n",
        "        return \"50-60\"\n",
        "    elif value <= 70:\n",
        "        return \"60-70\"\n",
        "    elif value <= 80:\n",
        "        return \"70-80\"\n",
        "    elif value <= 90:\n",
        "        return \"80-90\"\n",
        "    else:\n",
        "        return \"No data\"\n",
        "\n",
        "# Change sex type to integers\n",
        "def sex(value):\n",
        "    if value == \"male\":\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "\n",
        "# Change embarked type to integers\n",
        "def embarked(value):\n",
        "    if value == \"C\":\n",
        "        return 0\n",
        "    elif value ==\"Q\":\n",
        "        return 1\n",
        "    else:\n",
        "        return 2\n",
        "\n",
        "# Clean title and convert to numeric.\n",
        "data[\"TitleClean\"] = data[\"Name\"].str.extract('(\\w*\\.)', expand=True)\n",
        "def title_to_int(value):\n",
        "    if value == \"Capt.\":\n",
        "        return 0\n",
        "    elif value == \"Col.\":\n",
        "        return 1\n",
        "    elif value == \"Countess.\":\n",
        "        return 2\n",
        "    elif value == \"Don.\":\n",
        "        return 3\n",
        "    elif value == \"Dr.\":\n",
        "        return 4\n",
        "    elif value == \"Jonkheer.\":\n",
        "        return 5\n",
        "    elif value == \"Lady.\":\n",
        "        return 6\n",
        "    elif value == \"Major.\":\n",
        "        return 7\n",
        "    elif value == \"Master.\":\n",
        "        return 8\n",
        "    elif value == \"Miss.\":\n",
        "        return 9\n",
        "    elif value == \"Mlle.\": #Same as miss\n",
        "        return 9\n",
        "    elif value == \"Mme.\":\n",
        "        return 11\n",
        "    elif value == \"Mr.\":\n",
        "        return 12\n",
        "    elif value == \"Mrs.\":\n",
        "        return 13\n",
        "    elif value == \"Ms.\":\n",
        "        return 14\n",
        "    elif value == \"Rev.\":\n",
        "        return 15\n",
        "    elif value == \"Sir.\":\n",
        "        return 16\n",
        "    elif value == \"Dona.\": # Same as Mrs\n",
        "        return 13\n",
        "    else:\n",
        "        return np.nan\n",
        "    \n",
        "# Test a bunch of models. If NL is false, Neural Networks are not tested (they are pretty slow)\n",
        "def lets_try(NL):\n",
        "    results={}\n",
        "    def test_model(clf):\n",
        "        \n",
        "        cv = KFold(n_splits=10)\n",
        "        fbeta_scorer = make_scorer(fbeta_score, beta=1)\n",
        "        cohen_scorer = make_scorer(cohen_kappa_score)\n",
        "        accu = cross_val_score(clf, features, labels, cv=cv)\n",
        "        fbeta = cross_val_score(clf, features, labels, cv=cv,scoring=fbeta_scorer)\n",
        "        cohen = cross_val_score(clf, features, labels, cv=cv,scoring=cohen_scorer)\n",
        "        scores=[accu.mean(),fbeta.mean(),cohen.mean()]\n",
        "        return scores\n",
        "\n",
        "    # Decision Tree\n",
        "    clf = tree.DecisionTreeClassifier()\n",
        "    results[\"Decision Tree\"]=test_model(clf)\n",
        "    # Logistic Regression\n",
        "    clf = LogisticRegression()\n",
        "    results[\"Logistic Regression\"]=test_model(clf)\n",
        "    # SVM Linear\n",
        "    clf = svm.LinearSVC()\n",
        "    results[\"Linear SVM\"]=test_model(clf)\n",
        "    # SVM RBF\n",
        "    clf = svm.SVC()\n",
        "    results[\"RBF SVM\"]=test_model(clf)\n",
        "    # Gaussian Bayes\n",
        "    clf = GaussianNB()\n",
        "    results[\"Gaussian Naive Bayes\"]=test_model(clf)\n",
        "    # Random Forest\n",
        "    clf=RandomForestClassifier()\n",
        "    results[\"Random Forest\"]=test_model(clf)\n",
        "    # AdaBoost with Decision Trees\n",
        "    clf=AdaBoostClassifier()\n",
        "    results[\"AdaBoost\"]=test_model(clf)\n",
        "    # SGDC\n",
        "    clf=SGDClassifier()\n",
        "    results[\"SGDC\"]=test_model(clf)\n",
        "    # Bagging\n",
        "    clf=BaggingClassifier()\n",
        "    results[\"Bagging\"]=test_model(clf)\n",
        "    # Neural Networks\n",
        "    if NL:\n",
        "        clf=MLPClassifier()\n",
        "        results[\"Neural Network\"]=test_model(clf)\n",
        "    \n",
        "    results = pd.DataFrame.from_dict(results,orient='index')\n",
        "    results.columns=[\"Accuracy\",\"F-Score\", \"Cohen Kappa\"] \n",
        "    results=results.sort(columns=[\"Accuracy\",\"F-Score\", \"Cohen Kappa\"],ascending=False)\n",
        "    results.plot(kind=\"bar\",title=\"Model Scores\")\n",
        "    axes = plt.gca()\n",
        "    axes.set_ylim([0,1])\n",
        "    return plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "da50efae-6062-f4ca-2012-fd9a644d91a7"
      },
      "source": [
        "# Data Exploration #\n",
        "\n",
        "In this section I will do some data exploration to try to find some relations between the survival changes and some of the most important features present in the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d2a999a3-a431-ff67-3a8b-a5c256074ca9"
      },
      "source": [
        "###Data Set's Characteristics###\n",
        "\n",
        "The data to analyse includes information from passengers of Titanic, that collided with an iceberg on 15 April 1912. \n",
        "\n",
        "First we get some basic information from the datasets. Things I am interesting in are **rows**, **columns** and **cells without data**. This will help us to have some interesting information like:\n",
        "\n",
        " 1. The size of our dataset.\n",
        " 2. The different features we have.\n",
        " 3. How much data is missing from each feature.\n",
        "\n",
        "For future reference, these are column's descriptions:\n",
        "\n",
        "    survival        Survival\n",
        "                    (0 = No; 1 = Yes)\n",
        "    pclass          Passenger Class\n",
        "                    (1 = 1st; 2 = 2nd; 3 = 3rd)\n",
        "    name            Name\n",
        "    sex             Sex\n",
        "    age             Age\n",
        "    sibsp           Number of Siblings/Spouses Aboard\n",
        "    parch           Number of Parents/Children Aboard\n",
        "    ticket          Ticket Number\n",
        "    fare            Passenger Fare\n",
        "    cabin           Cabin\n",
        "    embarked        Port of Embarkation\n",
        "                    (C = Cherbourg; Q = Queenstown; S = Southampton)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "6dc7eef5-1627-2c23-679f-055c11bd08b2"
      },
      "source": [
        "This is the information we have in the training data set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7bc29c3c-e3cd-b8f1-28b8-ba2714aab2ea"
      },
      "outputs": [],
      "source": [
        "# Count the number of rows\n",
        "print(\"*** Number of rows: \" + str(train.shape[0]))\n",
        "total = train.shape[0]\n",
        "print(\"\\n\")\n",
        "\n",
        "# List all the columns\n",
        "print(\"*** Columns: \" + str(train.columns.values), end=\"\\n\")\n",
        "\n",
        "# Count the number of NaNs each column has.\n",
        "print(\"\\n*** NaNs per column:\")\n",
        "print(pd.isnull(train).sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "914638dd-0a78-b544-b18f-08ab6c358b24"
      },
      "source": [
        "###Passenger's Gender###\n",
        "\n",
        "The following section checks the **gender distribution** and the **survival percentage**. Data output:\n",
        "\n",
        " 1. Passengers per gender and the percentage over total number of passengers\n",
        " 2. Passengers that survived per gender and the percentage over the total number of passengers per gender"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5c927efa-f17e-8134-510d-419106fd90a2"
      },
      "outputs": [],
      "source": [
        "# Change gender's text to integers\n",
        "data[\"Sex\"] = data[\"Sex\"].apply(sex)\n",
        "\n",
        "# Draw survival per sex\n",
        "survival_rate(\"Sex\",\"barh\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a986ae66-9f67-e6de-1545-5130051c9ee3"
      },
      "source": [
        "###Passengers' Class###\n",
        "\n",
        "The following section checks the **class distribution** and the **survival percentage**.  Data output:\n",
        "\n",
        " 1. Passengers per class and the percentage over total number of passengers.\n",
        " 2. Passengers that survived per class and the percentage over the total number of passengers per class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5d10082d-05e7-1ae6-2062-f2d1a8ba33d7"
      },
      "outputs": [],
      "source": [
        "#\u00a0Draw survival per Class\n",
        "survival_rate(\"Pclass\",\"barh\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "392cff4e-3cee-44bf-72f1-91ed746b879d"
      },
      "source": [
        "###Passengers' Age###\n",
        "\n",
        "The following section checks the **age distribution**. First some data manipulation will be done:\n",
        "\n",
        " 1. The data set contain ages less than 1 for those people with only months of life. Those will be changed to 1.\n",
        " 2. Create new column with buckets of ages, in 10 years ranges.\n",
        "\n",
        "Then, the usual graph representation showing the ages in groups and the survival rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4015590f-9f34-4720-b386-924b2cf50f5b"
      },
      "outputs": [],
      "source": [
        "print(\"*** Number of people with age less than 1 (months):\")\n",
        "print(train[train[\"Age\"] < 1.0].shape[0])\n",
        "\n",
        "#\u00a0Those with age <1, changed to 1\n",
        "data['Age'] = data['Age'].apply(normalize_age_below_one)\n",
        "\n",
        "# Create new feature with data in buckets\n",
        "data[\"AgeGroup\"] = data[\"Age\"].apply(group_age)\n",
        "train[\"AgeGroup\"] = train[\"Age\"].apply(group_age)\n",
        "\n",
        "# Draw survival per age group\n",
        "survival_rate(\"AgeGroup\",\"bar\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "7bb8debe-8601-a4d1-a7db-5d6dd80fceb7"
      },
      "source": [
        "###Passengers' Fare###\n",
        "\n",
        "The following section checks the **fare distribution** and the **average fare per class**. The analysis shows that fare is mostly related with \"Class\", so no need to check the survival rate since survival per class has been already analysed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2b239722-69d4-0fbe-330d-a94dbfad2cd2"
      },
      "outputs": [],
      "source": [
        "#\u00a0Get Fare statistics\n",
        "print(\"*** Fare statistics:\")\n",
        "print(train[\"Fare\"].describe())\n",
        "\n",
        "# Seems that some people paid nothing:\n",
        "print(\"\\n*** People with fare 0:\")\n",
        "nothing = train[train[\"Fare\"] == 0]\n",
        "print(nothing[[\"Name\",\"Sex\",\"Age\",\"Pclass\",\"Survived\"]])\n",
        "\n",
        "# Graph average Fare per Class\n",
        "train.groupby(\"Pclass\").mean()['Fare'].plot(kind=\"bar\",title=\"Average Fare per Class\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "299a34f2-f835-51b7-1797-376ec182ac59"
      },
      "source": [
        "## Passengers' Port of Embarkation ##\n",
        "\n",
        "The following section checks the **port distribution** and the **survival percentage**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3c5175bd-7632-9763-bdbc-da450bdab87d"
      },
      "outputs": [],
      "source": [
        "# Change embarkation data type to integers\n",
        "data[\"Embarked\"] = data[\"Embarked\"].apply(embarked)\n",
        "\n",
        "# Graph survived per port of embarkation\n",
        "survival_rate(\"Embarked\",\"bar\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "299e0096-dcd2-4d84-dc83-2b5634cdf8d2"
      },
      "source": [
        "## Family members ##\n",
        "\n",
        "The following section checks the **family distribution**. First some data manipulation will be done:\n",
        "\n",
        " - New column, **FamilyMembers** will be added. It counts the number of family members of that particular passenger.\n",
        "\n",
        "I will also check large families (more than 5 members) and see if there are families where no member survived at all.\n",
        "\n",
        "Then, the usual graph representation showing the family members and the survival rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b43c6e4d-461a-ce01-9ab8-67135e4f20b7"
      },
      "outputs": [],
      "source": [
        "data[\"FamilyMembers\"]=data[\"SibSp\"]+data[\"Parch\"]\n",
        "train[\"FamilyMembers\"]=train[\"SibSp\"]+data[\"Parch\"]\n",
        "\n",
        "print(\"*** Family statistics, members:\")\n",
        "print(\"Min: \" + str(train[\"FamilyMembers\"].min()))\n",
        "print(\"Average: \" + str(round(train[\"FamilyMembers\"].mean(),2)))\n",
        "print(\"Max: \" + str(train[\"FamilyMembers\"].max()), end=\"\\n\\n\")\n",
        "\n",
        "print(\"*** Average family members per Class:\")\n",
        "print(train.groupby(\"Pclass\").mean()['FamilyMembers'], end=\"\\n\\n\")\n",
        "\n",
        "# Families with more than 5 members\n",
        "large_families=train[train[\"FamilyMembers\"]>= 5]\n",
        "large_families_by_ticket=large_families.groupby(\"Ticket\").sum()['Survived']\n",
        "print(\"*** Large families by ticket. Did all family die?:\")\n",
        "print(large_families_by_ticket==0, end=\"\\n\\n\")\n",
        "\n",
        "# Largest family where all members died\n",
        "largest_family_ticket=train[\"Ticket\"][train[\"FamilyMembers\"]==10].iloc[0]\n",
        "name=train[\"Name\"][train[\"Ticket\"]==largest_family_ticket].iloc[0]\n",
        "print(\"*** Largest family, all members died: \"+ name.split(\",\")[0], end=\"\\n\\n\")\n",
        "# More info: http://www.bbc.com/news/uk-england-cambridgeshire-17596264\n",
        "\n",
        "survival_rate(\"FamilyMembers\",\"bar\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "cbfeb677-35ca-3899-e138-01aae0da704f"
      },
      "source": [
        "## Passenger's Tickets ##\n",
        "\n",
        "As we have seen in previous section, tickets hide some information that could be valuable. Every member of the same family has the same ticket number. That means that we can use that information to in some way group people by family.\n",
        "\n",
        "There is no single ticket identification, but the most common one are numbers. Therefore, a regex will help is to get those and store in a new column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0c930f55-d56e-c153-b517-f4346926b905"
      },
      "outputs": [],
      "source": [
        "train[\"Ticket\"].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6e6d569e-afa7-1015-f248-a5a588cb924e"
      },
      "outputs": [],
      "source": [
        "data[\"TicketClean\"] = data[\"Ticket\"].str.extract('(\\d{2,})', expand=True)\n",
        "data[\"TicketClean\"].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "4d0def15-4a73-c878-3c53-a56df5f55107"
      },
      "source": [
        "There are 8 NaN tickets because they didn't have a number in them. We are going to manually assign those."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ddad82f2-b4b0-5503-0953-e6ea141b2a5b"
      },
      "outputs": [],
      "source": [
        "print(\"Rows with NaN: \" + str(pd.isnull(data[\"TicketClean\"]).nonzero()[0]))\n",
        "print(\"Ticket number: \")\n",
        "print(str(data[\"Ticket\"].ix[179]))\n",
        "print(str(data[\"Ticket\"].ix[271]))\n",
        "print(str(data[\"Ticket\"].ix[302]))\n",
        "print(str(data[\"Ticket\"].ix[597]))\n",
        "print(str(data[\"Ticket\"].ix[772]))\n",
        "print(str(data[\"Ticket\"].ix[841]))\n",
        "print(str(data[\"Ticket\"].ix[1077]))\n",
        "print(str(data[\"Ticket\"].ix[1193]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "99ae536d-6531-6502-c3b9-e9dd49c5f54a"
      },
      "source": [
        "To cleanup the data I am going to:\n",
        "\n",
        " - Convert \"TicketClean\" to number.\n",
        " - Assign median value to the first group.\n",
        " - Assign median+std() to the second group.\n",
        " - Assign median-std() to the third group.\n",
        " - Then manually update the values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "55aed2b1-0b44-0dc5-6f4d-af153841b35a"
      },
      "outputs": [],
      "source": [
        "data[\"TicketClean\"] = data[\"Ticket\"].str.extract('(\\d{3,})', expand=True)\n",
        "data[\"TicketClean\"] = data[\"TicketClean\"].apply(pd.to_numeric)\n",
        "med1=data[\"TicketClean\"].median()\n",
        "med2=data[\"TicketClean\"].median()+data[\"TicketClean\"].std()\n",
        "med3=data[\"TicketClean\"].median()-data[\"TicketClean\"].std()\n",
        "data.set_value(179, 'TicketClean', int(med1))\n",
        "data.set_value(271, 'TicketClean', int(med1))\n",
        "data.set_value(302, 'TicketClean', int(med1))\n",
        "data.set_value(597, 'TicketClean', int(med1))\n",
        "data.set_value(772, 'TicketClean', int(med2))\n",
        "data.set_value(841, 'TicketClean', int(med2))\n",
        "data.set_value(1077, 'TicketClean', int(med2))\n",
        "data.set_value(1193, 'TicketClean', int(med2))\n",
        "data[\"TicketClean\"].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a8fa882c-eede-2622-597a-e7921568c9d2"
      },
      "source": [
        "## Passenger's Name ##\n",
        "\n",
        "Names also hide some valuable information. Like family name, and also tittle abbreviations like Mr., Miss. and so on. I am going to extract those titles and examine them to see if there is something useful we can do with it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a9a295e3-4108-e46c-0121-857f662eac30"
      },
      "outputs": [],
      "source": [
        "data[\"TitleClean\"] = data[\"Name\"].str.extract('(\\w*\\.)', expand=True)\n",
        "data.groupby(data[\"TitleClean\"]).size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "abff5c1c-6356-d3af-d3d4-36e7249d0ce1"
      },
      "outputs": [],
      "source": [
        "data[\"TitleClean\"] = data[\"TitleClean\"].apply(title_to_int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "6170bb9a-f1df-2491-8550-1c915fbf7b94"
      },
      "source": [
        "## Machine Learning ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "353db6e3-46db-91aa-4087-a5c0963d8014"
      },
      "source": [
        "### Data Balance ###\n",
        "\n",
        "It is important to check if our data is balanced. That means that in this binary classification problem we should have the same number of rows for each possible outcome. If the data is imbalanced, our accuracy score could be wrong and the model could have problems to generalise. \n",
        "\n",
        "The graph shows that our data is imbalanced, so the things we can do are:\n",
        "\n",
        " - Use a different score. Apart from Accuracy and F-Score, I will also check Cohen's Kappa.\n",
        "\n",
        "**Cohen\u2019s kappa**: Classification accuracy normalized by the imbalance of the classes in the data.\n",
        "http://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/\n",
        "\n",
        " - **TODO**. Resample the data adding copies of instances from the under-represented class ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7799566f-1196-171c-fb18-61c9f9aa3aa1"
      },
      "outputs": [],
      "source": [
        "df=pd.DataFrame()\n",
        "df['total']=train.groupby(\"Survived\").size()\n",
        "df=df['total']/train.shape[0]\n",
        "df.plot(kind=\"bar\",title=\"Label's Balance\")\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([0,1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "1a4003e5-5fb5-93a6-2555-90ab998b2684"
      },
      "source": [
        "### Data Preparation ###\n",
        "\n",
        "First some data cleaning:\n",
        "\n",
        " - Drop useless columns.\n",
        " - Fill NaN ages and Fare with average from \"Title\" and \"Pclass\" groups.\n",
        " - Separate features and labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "aa8eb27c-95b5-2f1d-a6ed-b2e80fe1a686"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c73ed4af-ad93-f09f-7cba-5264138bb076"
      },
      "outputs": [],
      "source": [
        "remove=['Name','Cabin','Ticket', 'AgeGroup']\n",
        "for column in remove:\n",
        "    data = data.drop(column, 1)\n",
        "\n",
        "# Add missing ages. If there is a NaN, change it with the average for that title group.\n",
        "list_nan=pd.isnull(data[\"Age\"]).nonzero()\n",
        "# Get a pd with the mean age for each title\n",
        "means = data.groupby(\"TitleClean\").mean()['Age']\n",
        "# for each row with NaN, we write the average\n",
        "for i in list_nan[0]:\n",
        "    temp_title = data[\"TitleClean\"].ix[i]\n",
        "    data.set_value(i, 'Age', int(means[temp_title]))\n",
        "\n",
        "# Add missing fare. If there is a NaN, change it with the average for that Pclass.\n",
        "list_nan=pd.isnull(data[\"Fare\"]).nonzero()\n",
        "# Get a pd with the mean age for each title\n",
        "means = data.groupby(\"Pclass\").mean()['Fare']\n",
        "# for each row with NaN, we write the average\n",
        "for i in list_nan[0]:\n",
        "    temp_class = data[\"Pclass\"].ix[i]\n",
        "    data.set_value(i, 'Fare', int(means[temp_class]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "41a65867-58d1-91b7-ca38-89b39e5bf32c"
      },
      "outputs": [],
      "source": [
        "#\u00a0Prepare features\n",
        "train=data[data['Survived'].isin([0, 1])]\n",
        "#labels=train[\"Survived\"]\n",
        "train=train.drop(\"Survived\", 1)\n",
        "train=train.drop('PassengerId', 1)\n",
        "features=train\n",
        "\n",
        "# Prepare testing data\n",
        "test=data[~data['Survived'].isin([0, 1])]\n",
        "test=test.drop(\"Survived\", 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "45c4c218-dfc0-4602-c558-13bf053142cf"
      },
      "outputs": [],
      "source": [
        "lets_try(NL=False).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9ac69345-7b5c-4cf7-d0cd-9ba8dcef29ab"
      },
      "source": [
        "Seems that Random Forest and AdaBoost perform better. Both allow us to extract information about what are the most important features to take a decision. In the next graph we see which are the most important features for RandomForest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "061a3cea-37e2-4709-ad1c-feef6c8f5608"
      },
      "outputs": [],
      "source": [
        "def draw_best_features():\n",
        "    clf=RandomForestClassifier()\n",
        "    clf.fit(features,labels)\n",
        "    importances = clf.feature_importances_\n",
        "    names=features.columns.values\n",
        "\n",
        "    pd.Series(importances*100, index=names).plot(kind=\"bar\")\n",
        "    plt.show()\n",
        "    \n",
        "draw_best_features()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c06a3460-a8ec-df7c-be40-ad0a28f563ad"
      },
      "outputs": [],
      "source": [
        "# Now let's test only with relevant features\n",
        "#best_features=[\"Pclass\",\"Sex\",\"Age\",\"Fare\",\"FamilyMembers\", \"TicketClean\", \"TitleClean\"]\n",
        "best_features=[\"Pclass\",\"Sex\",\"Age\",\"Fare\", \"TicketClean\", \"TitleClean\"]\n",
        "features=features[best_features]\n",
        "features.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "04d9e55a-8233-15e4-ba21-1ea6589785cf"
      },
      "source": [
        "Most of the models require the data to be standardised, so I am going to use a scaler and then check the scores again. There should be a huge difference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6de46884-17af-39de-162d-93d87ab926b1"
      },
      "outputs": [],
      "source": [
        "scaler = MinMaxScaler()\n",
        "features_backup=features\n",
        "features = scaler.fit_transform(features)\n",
        "pd.DataFrame(features).head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "da62d333-fd5a-47a3-8d94-ce4e397851b4"
      },
      "outputs": [],
      "source": [
        "lets_try(NL=False).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7c6170df-f20b-ca3c-e496-412394e572ef"
      },
      "outputs": [],
      "source": [
        "features=features_backup\n",
        "cv = KFold(n_splits=5)\n",
        "\n",
        "parameters = {'n_estimators': [10,20,30,40,50],\n",
        "               'min_samples_split' :[2,3,4,5],\n",
        "               'min_samples_leaf' : [1,2,3]\n",
        "             }\n",
        "\n",
        "clf = RandomForestClassifier()\n",
        "grid_obj = GridSearchCV(clf, parameters, cv=cv)\n",
        "grid_fit = grid_obj.fit(features, labels)\n",
        "best_clf = grid_fit.best_estimator_ \n",
        "\n",
        "best_clf.fit(features,labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "02a94a89-dcbe-3d39-f27c-f0df298e23ca"
      },
      "outputs": [],
      "source": [
        "PassengerId=test[\"PassengerId\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1dc5fb5a-d00e-2107-f288-230ccffc83f9"
      },
      "outputs": [],
      "source": [
        "#remove=['PassengerId','SibSp', 'Parch', 'Embarked']\n",
        "#for column in remove:\n",
        "#    test = test.drop(column, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b6cc3196-b06b-412d-f825-247f3f7b2aed"
      },
      "outputs": [],
      "source": [
        "test=test[best_features]\n",
        "test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "594ae932-4abe-31bc-d8dd-4c8a9442f321"
      },
      "outputs": [],
      "source": [
        "predictions=best_clf.predict(test)\n",
        "\n",
        "sub = pd.DataFrame({\n",
        "        \"PassengerId\": PassengerId,\n",
        "        \"Survived\": predictions\n",
        "    })\n",
        "sub.to_csv(\"titanic_submission.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ed58d59d-a5d1-9050-5998-874f618d9194"
      },
      "outputs": [],
      "source": ""
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b3fc7065-8e03-8a66-24d0-9af08c4c103f"
      },
      "outputs": [],
      "source": ""
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}