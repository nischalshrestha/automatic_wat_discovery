{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# Support Vector Machines and Neural Networks \n\n[Iaroslav Shcherbatyi](http://iaroslav-ai.github.io/), [ED3S 2018](http://iss.uni-saarland.de/de/ds-summerschool/)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# Synopsis\n\nKernel Support Vector Machines are a popular class of learning algorithms. While such algorithms have strong theoretical underpinning and empirically perform well, they do not scale well to \"large\" datasets, due to computational and memory requirements. In contrast, Artificial Neural Networks scale better, and can be applied to datasets of size in millions. However, training procedure of ANN is less rigorous, and requires setting more parameters than that of SVM. In this notebook, we will look at examples on how both learning algorithms can be used. \n\nA few general aspects of machine learning will also be discussed, such data preprocessing, and scaling of feature ranges, which often leads to improved outcomes.\n\nNote: if you are viewing this notebook on Kaggle, you can download the notebook by clicking the cloud with arrow pointing down in the upper panel, and the necessary data from the panel to the right. "},{"metadata":{"_uuid":"778d58aed5338762fb4ba51ea758dff1f1d3c763"},"cell_type":"markdown","source":"# SVM usage in sklearn\n\nSVM learning algorithm is available in `scikit-learn`, and can be used similar as the other algorithms you have seen.  For this we will use a \"Titanic\" dataset, which contains records of which passengers survived Titanic crash, and their features."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\ntitanic = pd.read_csv('../input/train.csv')\n\n# some preprocessing is applied for simplicity\ntitanic = titanic[['Sex', 'Pclass', 'Age', 'SibSp', 'Fare', 'Survived']]  # use subset of columns\ntitanic = titanic.dropna()  # drop rows with missing values\n\ndisplay(titanic.head())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8549dce5111f448a5f2f4ac7efaa4f787ee3e649"},"cell_type":"markdown","source":"For some initial prototyping, we will use only numerical coumns, which can be provided directly into the SVM."},{"metadata":{"trusted":true,"_uuid":"d143af0eee93456aa43c091fda32f0d224d31c55"},"cell_type":"code","source":"# use only numerical values\nXy = titanic[['Pclass', 'Age', 'SibSp', 'Fare', 'Survived']].values\n\n# separate inputs and outputs\nX = Xy[:, :-1]\ny = Xy[:, -1]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e5460fafce8cc0a1d3f2477c587dc550e3c8f42b"},"cell_type":"markdown","source":"Now, lets use the Kernel SVM for prediction of whether a person would survive Titanic crash or not. A few possibilities are to be considered. \n\n* Firstly, the Gaussian Kernel used by default does not account for different ranges of features. Scaling feature values to standard range can be done using `StandardScaler` class from `sklearn`. \n* It is useful to keep the whole model, including scaling class, in a single variable. This can be done using `make_pipeline` function, which can \"chain\" multiple data transformation classes and one final learning algorithm. This typically reduces this size of codebase, and makes it easier to share model as a single variable [serialized](https://docs.python.org/3/library/pickle.html) to a file.\n* It is important to not use testing set for hyperparameter (`C`, `gamma`) selection, as it can lead to overfitting; Validation set should be used instead. Furthermore, it is good to avoid bias due to dataset split into training and validation partitions. One way is to use [cross-validation](https://en.wikipedia.org/wiki/Cross-validation)."},{"metadata":{"trusted":true,"_uuid":"cea27f5087e5505c7b5d73249211b629d89e698e"},"cell_type":"code","source":"# using Kernel SVM is easy in sklearn\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.model_selection import train_test_split, cross_val_score\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n# do the usual splitting into training / testing dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\nfor C in [0.01, 0.1, 1, 10, 100]:\n    for gamma in [0.01, 0.1, 1.0, 10, 100]:\n        score = 0\n        # Task: fill in here proper training and scoring of SVC.        \n        print(C, gamma, score)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"90ee6952cc662794ba92d802b068d5d6ca753c4f"},"cell_type":"markdown","source":"# Simple interface to hyperparameter grid search \n\nAbove loop can be simplified through the `GridSearchCV` class from `sklearn`. This class performs search for parameters of the model which result in highest cross-validation score, and fits the model to the dataset with such best parameters."},{"metadata":{"trusted":true,"_uuid":"e330b60a42d0e9db8215c4e758b26020f2d1d1a1","scrolled":true},"cell_type":"code","source":"# using Kernel SVM is easy in sklearn\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n# do the usual splitting into training / testing dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n\n# two arguments are necessary: sklearn \"estimator\" class,\n# and range of parameters for this estimator. \nmodel = GridSearchCV(\n    estimator=make_pipeline(StandardScaler(), SVC()),\n    param_grid={  # format: lowercase_estimator_name__param_name\n        'svc__C': [0.01, 1, 100], \n        'svc__gamma': [0.01, 1, 100],\n    },\n    cv=5,  # number of validation folds\n    n_jobs=1  # number of parallel jobs\n)\n# Q: how does the grid search scale computationally?\n\nmodel.fit(X_train, y_train)\nprint(model.score(X_test, y_test))\n\neg_inputs = X_test[:5]\nprint(eg_inputs)\nprint(model.predict(eg_inputs))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5a79ba0f833b2213930d179ce1688912f180d839"},"cell_type":"markdown","source":"Check out [scikit-optimize](https://github.com/scikit-optimize/scikit-optimize) for efficient parameter selection; For instance, `sklearn` compatible `BayesSearchCV` class."},{"metadata":{"trusted":true,"_uuid":"d0011bb1dba652e4486d4aa8e1a2d3d134d85b8b"},"cell_type":"code","source":"from skopt import BayesSearchCV\n\n# include below until https://github.com/scikit-optimize/scikit-optimize/issues/718 is resolved\nclass BayesSearchCV(BayesSearchCV):\n    def _run_search(self, x): raise BaseException('Use newer skopt')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"3b3c8bfce5ffde13dc0e35836d632020598ec1ef"},"cell_type":"code","source":"model = BayesSearchCV(\n    estimator=make_pipeline(StandardScaler(), SVC()),\n    search_spaces={\n        'svc__C': (0.01, 100.0, 'log-uniform'),  # specify ranges instead of discrete values\n        'svc__gamma': (0.01, 100.0, 'log-uniform'),\n    },\n    cv=5,\n    n_iter=16,  # fixed number of parameter configuration trials!\n    n_jobs=4,  # it runs evaluations in parallel too!\n    verbose=1\n)\n\nmodel.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"d724a34c81bec2ada73afe582e4e2bbf62feaccf"},"cell_type":"code","source":"print(model.best_params_)\nprint(model.score(X_test, y_test))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"68f815ff4020503dfb0d4944a4028813a90c7d2a"},"cell_type":"markdown","source":"# ANN in Keras\n\nKeras is a popular python package, that provides hight level apis to work with ANN. It is based on a popular `TensorFlow` library, and is supported by Google."},{"metadata":{"trusted":true,"_uuid":"56a176e17df6ceb49daa688811ec9828aa3a058c"},"cell_type":"code","source":"from keras.layers import Input, Dense, LeakyReLU, Softmax\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom keras.losses import sparse_categorical_crossentropy\n\n# define single input\ninp = Input(shape=(4,))\nh = inp\n\n# Task: add a layer with 1 neuron\nh = Dense(64)(h)  # linear transformation\nh = LeakyReLU()(h)  # activation\nh = Dense(2)(h)  # final linear layer\nh = Softmax()(h)  # softmax activation\n\n# create an ANN model definition\nmodel = Model(inputs=[inp], outputs=[h])\n\n# Task: set learning rate to 100.0, 0.0000001\n# this creates a C program that is called from python\nmodel.compile(Adam(), sparse_categorical_crossentropy, ['accuracy'])\n\n# fit the model\nmodel.fit(X_train, y_train, epochs=10)\n\n# evaluate the model\nloss, score = model.evaluate(X_test, y_test)\nprint(score)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f629204bb5bd711c9a0c471b3ce5d70224bd0fce"},"cell_type":"markdown","source":"It is also possible to use Keras models as part of your `sklearn` pipeline. This can be achieved with `keras` sklearn wrappers."},{"metadata":{"trusted":true,"_uuid":"f3a234b6b0d82b56016cb0bf0ca197572795a79f"},"cell_type":"code","source":"from keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\ndef make_net(n_neurons=128):\n    \"\"\"Defines architecture of ANN and compiles it.\"\"\"\n    inp = Input(shape=(4,))\n    h = Dense(n_neurons)(inp)  # linear transformation\n    h = LeakyReLU()(h)  # activation\n    h = Dense(2)(h)  # final linear layer\n    h = Softmax()(h)  # softmax activation\n\n    model = Model(inputs=[inp], outputs=[h])\n    model.compile(Adam(), sparse_categorical_crossentropy, ['accuracy'])\n    return model    \n\n# can be used as part of pipeline, and in *SearchCV\n# Task: wrap in pipeline, and add scaling of feature ranges\nmodel = KerasClassifier(make_net, n_neurons=256)\nmodel.fit(X_train, y_train)\nscore = model.score(X_test, y_test)\nprint(score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af98b742b71530f56892a6833f4fc97135f54015"},"cell_type":"code","source":"# Obtain example outputs. Remember, columns are:\n# Pclass, Age, SibSp, Fare\n# Task: what leads to increase of survival likelihood?\nmy_input= np.array([\n    [3, 22.0, 1, 7.2500],\n    [3, 20.0, 0, 10.0],\n])\nprint(model.predict_proba(my_input))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}