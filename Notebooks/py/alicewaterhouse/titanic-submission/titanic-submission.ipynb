{"cells":[{"metadata":{"_uuid":"c7be859e7d2e211b883e7b8bc038f64d368a0fd1"},"cell_type":"markdown","source":"This is my first Kaggle competition! I am just using it to experiment with some models and techniques. A rough plan is as follows:\n\n* Overview, data cleaning and some feature engineering\n* Correlation matrix and missing values\n* Basic visualisation\n* More feature engineering and pre-modelling\n* Modelling and submission\n\n**1. Overview, data cleaning and some feature engineering**\n\nFirst I import some relevant libraries along with the data, and take a look at where there are missing values."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#Import relevant modules\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, (e.g. pd.read_csv)\n\n#Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#Read the data and take a look at the first few lines\ntrain = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\ntrain.head(10)\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#Find the number of missing values in each column of the training data and print if it is non-zero\nmissing_vals_train = (train.isnull().sum())\nprint('Missing values in training data:\\n')\nprint(missing_vals_train[missing_vals_train > 0])\n\n#Do the same for test data\nprint('\\nMissing values in test data:\\n')\nmissing_vals_test = (test.isnull().sum())\nprint(missing_vals_test[missing_vals_test > 0])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"665af2f1067f9990d9fde0efbc8e9cc77f627bf6"},"cell_type":"markdown","source":"A lot of cabin data is missing, but it seems a shame to drop the data that is there because it may contain useful information. Also, it might have a reasonable effect on survival if it determines where passengers were on the ship when the iceberg hit. I will drop the numeric part and add an extra category 'N' for those passengers without cabin data, and remove the numeric part for those with cabin data.\n\nAge, fare and embarkation point can be imputed. I will decide how to do this a little later on. I drop passenger ID and ticket number as I don't expect these to affect survival."},{"metadata":{"trusted":true,"_uuid":"5f36536057308c8eb84ac2c43ffae8579219c072"},"cell_type":"code","source":"#Keep PassengerId for submission later\npassengerid = test.PassengerId\n\n#Drop irrelevant columns from both test and training data\nColumns_to_drop = ['Ticket','PassengerId']\ntrain.drop(Columns_to_drop, axis=1, inplace=True)\ntest.drop(Columns_to_drop, axis=1, inplace=True)\n\n#Add 'N' for passengers without cabin data\ntrain.Cabin.fillna(\"N\", inplace=True)\ntest.Cabin.fillna(\"N\", inplace=True)\n\n#Take the first character of the string to remove the numeric part of the cabin data\ntrain.Cabin = [i[0] for i in train.Cabin]\ntest.Cabin = [i[0] for i in test.Cabin]\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"694842926ad4183be2de867edea6293cc3bac7f9"},"cell_type":"markdown","source":"To investigate whether useful information can be obtained from the name column I will take the title and length. This will tell me, for example, whether the women on the ship are married, which might be relevant. Then I can drop the Name feature."},{"metadata":{"trusted":true,"_uuid":"a88c362cf782983546907ccf4ff31462076538bf"},"cell_type":"code","source":"#Add a column for name length\ntrain['NameLength'] = [len(i) for i in train.Name]\ntest['NameLength'] = [len(i) for i in test.Name]\n\n#Add a column for title\ntrain['Title'] = [i.split('.')[0] for i in train.Name]\ntrain['Title'] = [i.split(', ')[1] for i in train.Title]\ntest['Title'] = [i.split('.')[0] for i in test.Name]\ntest['Title'] = [i.split(', ')[1] for i in test.Title]\n\n#Drop Name\ntrain.drop('Name', axis=1, inplace=True)\ntest.drop('Name', axis=1, inplace=True)\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4ec343a9d25784d75e00fb3a0cbc6651aae88f21"},"cell_type":"markdown","source":"I'm now going to look at the titles that appear to (a) check that this worked as I expected and (b) see which are the main ones and whether any are obscure ones that only appear once and hence aren't very helpful."},{"metadata":{"trusted":true,"_uuid":"55b5f3cab09045a6f3742fd5b9c81083eb0747ff"},"cell_type":"code","source":"print('Title value counts in training data: \\n')\nprint(train.Title.value_counts())\nprint('\\nTitle counts in test data: \\n')\nprint(test.Title.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ddf2814e626b33303affe7df3456a2f855019fad"},"cell_type":"markdown","source":"I will assume Mlle and Ms mean the same as Miss and Mme means Mrs so these are easily replaced. I will also group military titles and titles indicating nobility. Maybe this is still too many categories. I don't know much about the pros and cons of having a lot of categories to one-hot encode. Something to look into another time. I do know that the fact that number of passengers is still much greater than number of features, which is encouraging."},{"metadata":{"trusted":true,"_uuid":"3322b9a6c92520caf7c569ef7fdf788335fe7659"},"cell_type":"code","source":"#Simplify titles in training data\ntrain[\"Title\"] = [i.replace('Ms', 'Miss') for i in train.Title]\ntrain[\"Title\"] = [i.replace('Mlle', 'Miss') for i in train.Title]\ntrain[\"Title\"] = [i.replace('Mme', 'Mrs') for i in train.Title]\ntrain[\"Title\"] = [i.replace('Col', 'Military') for i in train.Title]\ntrain[\"Title\"] = [i.replace('Major', 'Military') for i in train.Title]\ntrain[\"Title\"] = [i.replace('Don', 'Military') for i in train.Title]\ntrain[\"Title\"] = [i.replace('Jonkheer', 'Nobility') for i in train.Title]\ntrain[\"Title\"] = [i.replace('Sir', 'Nobility') for i in train.Title]\ntrain[\"Title\"] = [i.replace('Lady', 'Nobility') for i in train.Title]\ntrain[\"Title\"] = [i.replace('Capt', 'Military') for i in train.Title]\ntrain[\"Title\"] = [i.replace('the Countess', 'Nobility') for i in train.Title]\n\n#Simplify titles in test data\ntest[\"Title\"] = [i.replace('Ms', 'Miss') for i in test.Title]\ntest[\"Title\"] = [i.replace('Col', 'Military') for i in test.Title]\ntest[\"Title\"] = [i.replace('Dona', 'Nobility') for i in test.Title]\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"efe51fd0ad7e0f06414874530efed2891d4c83da"},"cell_type":"markdown","source":"Finally, I reformat Sex so that 1 is male and 0 is female:"},{"metadata":{"trusted":true,"_uuid":"d6fba908dca274a200c2761652e1051d75519680"},"cell_type":"code","source":"train[\"Sex\"] = train[\"Sex\"].replace({\"female\":0, \"male\":1})\ntest[\"Sex\"] = test[\"Sex\"].replace({\"female\":0, \"male\":1})\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c8d4211f35a53cb4be32d50961bd8cb87d9d6a80"},"cell_type":"markdown","source":"Summary so far: we dropped irrelevant columns and extracted name length and title from the Name column. We also engineered the cabin data, dropping the numerical part and adding 'N' where no cabin data was available.\n\nStill to do: decide on the best way to impute age, fare and embarkation point. But first, I want to gain some basic statistical understanding of the data.\n\n**2. Correlation matrix and missing values**\n\nThe first thing I want to do is look at the correlations between different features. Note: this will ignore categorical features that have not yet been one-hot-encoded."},{"metadata":{"trusted":true,"_uuid":"5c01c99263bd4ac7472e2c14ffc336d6b0505683"},"cell_type":"code","source":"#Change options to make pandas display all the columns\npd.options.display.max_columns = 99\n\n#Print the correlation matrix\nprint(train.corr())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"85351f39cf2bf4c350a346a1a82dea90c218f4e5"},"cell_type":"markdown","source":"Survival correlations that stand out:\n* Survival is positively correlated with name length (!)\n* Survival is negatively correlated with class\n* Being male is negatively correlated with survival\n* I am surprised how small the correlation is between age and survival. Perhaps this is because both children and the elderly had preferential access to lifeboats.\n\nOther correlations that stand out:\n* Class and age\n* Class and fare (makes sense)\n* Sex and name length (interesting??)\n* Age and number of siblings/spouses you are travelling with\n* Number of parents/children and number of siblings/spouses you are travelling with\n\nNow that I have this information, my first priority is imputation of age, fare and embarkation point. I waited until now because I want to use some knowledge of the correlations to inform my imputation.\n\nWe can impute using both the training and test data, since this gives us more information. In the real world this would be cheating because we would be unlikely to have access to test data at the time of training. Also, imputing based on all the data causes a so-called leaky pipeline. Technically when cross-validation later I should do the imputation for each fold. This will result in my accuracy on the test set being lower than expected from cross-validation scores in the training set. But that's okay since I still expect an increase in cross-validation score to reflect a better model fit.\n\nI create a new dataframe combine train and test data.\n"},{"metadata":{"trusted":true,"_uuid":"9ba2e4031680c8a1a0998bc49d7bc57ae0c6e6c9"},"cell_type":"code","source":"#Make a copy of the training data\ntrain1 = train.copy(deep=True)\n#Add a new column stating that \ntrain1['Dataset'] = 'Training Data'\n\n#Do the same for the test data. Add 'Unknown' for Survived\ntest1 = test.copy(deep=True)\ntest1['Dataset'] = 'Test Data'\ntest1['Survived'] = 'Unknown'\n\n#Combine the training and test data\ncombined_data = pd.concat([train1,test1],ignore_index=True)\ncombined_data.tail()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b9fd84a6195256356480c2b8435e1840ad5f452c"},"cell_type":"markdown","source":"Now, embarkation. We might expect this to be related to fare. Firstly, who are the two passengers whose embarkation point we don't know about? And what is the modal embarkation point?"},{"metadata":{"trusted":true,"_uuid":"69125f74b1e14988357bd9cba9376d460fc96a75"},"cell_type":"code","source":"#Examine the modal value for embarked in both training and test data\nprint('Embarked value counts in training data: \\n')\nprint(train.Embarked.value_counts())\nprint('\\nEmbarked value counts in test data: \\n')\nprint(test.Embarked.value_counts())\n\n#Extract passengers whose Embarkation data is missing\ntrain[train.Embarked.isnull()]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5e6c3a196bf16e0ebfc51731db9eea75c71cdf54"},"cell_type":"markdown","source":"Most people board at Southampton, and both passengers for whom the data is missing paid 80.0. Let's look at how Fare is distributed with Embarked."},{"metadata":{"trusted":true,"_uuid":"57ee4553120136871fbfe09fbe8eac72b64000b7"},"cell_type":"code","source":"#Import plotting libraries\nimport seaborn as sns #Statistical visualisation\nimport matplotlib.pyplot as plt #Plotting\n\n#Make a boxplot of Fare against embarked for each dataset\nplt.figure(figsize=(15,9))\nsns.set(style=\"whitegrid\")\nax = sns.boxplot(y=\"Embarked\", x=\"Fare\", hue=\"Dataset\", data=combined_data);\nplt.title('Fare Boxplots for each Embarkation Point', fontsize=18)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"898f2f9f1de450c4a40ec5dd957f864b9b42fdec"},"cell_type":"markdown","source":"The first thing we notice is there are some massive outliers in fare. Possibly these should be removed. I will think more about that later. The second thing we notice is that passengers embarking at C in general paid higher fares, meaning it makes the most sense to set Embarked to C for our two passengers whose fares were 80.0."},{"metadata":{"trusted":true,"_uuid":"7f98a75042ddd05ee3336e82ef1ed60920c08779"},"cell_type":"code","source":"#Fill in missing Embarked values with 'C'\ntrain.Embarked.fillna(\"C\", inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"97a5c1f2a4f9e3c6d5485dd807aba31817d1e2a9"},"cell_type":"markdown","source":"Now for Fare. Whose fare value is missing?"},{"metadata":{"trusted":true,"_uuid":"ca3f88654f56191323ad145d54a9e5039d0019b6"},"cell_type":"code","source":"test[test.Fare.isnull()]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6c76f60e26c14db9e970c95ba29ccfe03d1a58e7"},"cell_type":"markdown","source":"We already saw that Fare is correlated with Pclass, so it would be silly not to use the fact that this passenger's Pclass is 3. I take the mean fare of those passengers who embarked at Southampton and have Pclass 3."},{"metadata":{"trusted":true,"_uuid":"3feab750b07a27d122cbacb8f2fce0569e1903cf"},"cell_type":"code","source":"#Impute Fare with the mean for the subset described above\nmissing_fare = combined_data[(combined_data.Pclass == 3) & (combined_data.Embarked == \"S\")].Fare.mean()\n#Replace the test.Fare null values with missing_fare\ntest.Fare.fillna(missing_fare, inplace=True)\ntest.iloc[152]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b8511e8fc13908bf372d71991ef12f5013aaed59"},"cell_type":"markdown","source":"Cool so his fare was filled in as 14.4354.\n\nLast to be imputed is age. I wait until after my visualisation to do this, so that it doesn't mess up the plots involving age."},{"metadata":{"_uuid":"5cf7ba1cbc5c9c37c36568049687ca9f45a94918"},"cell_type":"markdown","source":"**3. Basic visualisation**\n\nTime to look in more detail at the qualitative effects of the features on survival.\n\n**Survival vs Sex**\n\nI examine this using a simple bar chart of percentage survival for each Sex.\n"},{"metadata":{"trusted":true,"_uuid":"9941bd68aca4af98bf0e2e51d5c3431eda2c1c3e"},"cell_type":"code","source":"plt.subplots(figsize = (15,8))\nax = sns.barplot(x = \"Sex\", y = \"Survived\", data=train)\nplt.title(\"Fraction of Passengers that Survived by Sex\", fontsize = 18)\nplt.ylabel(\"Fraction of Passengers that Survived\", fontsize = 15)\nplt.xlabel(\"Sex\",fontsize = 15);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"682aa690e4ce023873be07da097ec880c9900166"},"cell_type":"markdown","source":"Really I should change the labels for Sex back to male and female but recall that 0 is female and 1 is male. Women (as one might expect) were more likely to survive.\n\n**Survival vs Pclass**\n\nThis can be investigated using a similar chart."},{"metadata":{"trusted":true,"_uuid":"f867379b6d8af6f088c5b2e2b912842cd29a608b"},"cell_type":"code","source":"plt.subplots(figsize = (15,8))\nax = sns.barplot(x = \"Pclass\", y = \"Survived\", data=train)\nplt.title(\"Fraction of Passengers that Survived by Pclass\", fontsize = 18)\nplt.ylabel(\"Fraction of Passengers that Survived\", fontsize = 15)\nplt.xlabel(\"Pclass\",fontsize = 15);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"526e3aea21e8c28d48dd7f3487064cc0376a28aa"},"cell_type":"markdown","source":"First class passengers were more likely to survive than second, and second class passengers were more likely to survive than third.\n\n**Survival vs Age**\n\nI am using a KDE plot to examine differences in the age distribution between passengers who survived and passengers who didn't."},{"metadata":{"trusted":true,"_uuid":"7a19d5e3d38f770b5a77e2a7ca29bb767d5d9dc5","scrolled":true},"cell_type":"code","source":"plt.figure(figsize=(15,8))\nax = sns.kdeplot(train[(train['Survived'] == 1)].Age , shade=True, label='Survived')\nax = sns.kdeplot(train[(train['Survived'] == 0)].Age , shade=True, label='Did not survive')\nplt.title('Estimated probability density of age given survival', fontsize = 18)\nplt.xlabel('Age', fontsize = 15)\nplt.ylabel('Probability density', fontsize = 15);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6cb9f6cda82fd9596e1335f7f13f236d5c1ff7c3"},"cell_type":"markdown","source":"This is clearly a bit ridiculous because it has a non-zero probability of survival for negative ages. This is because of the smoothing the KDE plot does. But anyway, the small peak below 10 in the survived data which is not present in the data of those who did not survive suggests that children were prioritised over adults for lifeboat space, as we might expect. When feature engineering, I might categorise age according to this distinction.\n\n**Survival and Fare**"},{"metadata":{"trusted":true,"_uuid":"a8f9a8fbf9e8c807a6d8180778147fdf926a38f0"},"cell_type":"code","source":"plt.figure(figsize=(15,8))\nax = sns.kdeplot(train[(train['Survived'] == 1)].Fare, label='Survived', shade=True)\nax = sns.kdeplot(train[(train['Survived'] == 0)].Fare, label='Did not survive', shade=True)\nplt.title('Estimated probability density of fare given survival', fontsize = 18)\nplt.xlabel('Fare', fontsize = 15)\nplt.ylabel('Probability density', fontsize = 15);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"adb669817b23bb07138bcd34fa8ef1b0238dce80"},"cell_type":"markdown","source":"The sharp peak at low fares for those who didn't survive, because it is not present in the data for those who did survive, suggests those with lower fares were more likely to die. Similarly, the larger tail on the RHS for survivors suggests paying a higher fare made you more likely to survive.\n\nNow that this is done, I impute age."},{"metadata":{"trusted":true,"_uuid":"41324a7c5d2bb87ecff30aca7ab454e7a6fd1f88"},"cell_type":"code","source":"#Use the mean age from the combined data to fill in unknown ages\ntrain.Age.fillna(combined_data.Age.mean(), inplace=True)\ntest.Age.fillna(combined_data.Age.mean(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ccd927fd35f388b14f9a927199e79748bb6d54df"},"cell_type":"markdown","source":"**4. More feature engineering and pre-modelling**\n\nI want to create an extra feature for whether or not a passenger is a child. Since the peak in the KDE plot above is below age 10,  my definition of child will be the same."},{"metadata":{"trusted":true,"_uuid":"1d3d2e1f2a83e8d1292323941543854cd29f362a"},"cell_type":"code","source":"train['Child'] = [1 if i<10 else 0 for i in train.Age]\ntest['Child'] = [1 if i<10 else 0 for i in test.Age]\ntrain.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2a0ab7ec324444952ab788261c2942cd405a6259"},"cell_type":"markdown","source":"The last thing I need to do before modelling is add dummy variables for the categorical data and split into dependent and independent variables."},{"metadata":{"trusted":true,"_uuid":"4f743a6ba517c06f8f406d3de044d1b41589baca"},"cell_type":"code","source":"#Get dummies for one-hot-encoding of categorical features\ntrain = pd.get_dummies(train, drop_first=True)\ntest = pd.get_dummies(test, drop_first=True)\n\n#Drop the T Cabin...\ntrain.drop(['Cabin_T'], axis=1, inplace=True)\n\n#Split training data into dependent and independent variables\nX = train.drop(['Survived'], axis=1)\ny = train[\"Survived\"]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"26642471122bf13c6cc192aaa6863d3429a3902c"},"cell_type":"markdown","source":"**5. Modelling and submission**"},{"metadata":{"_uuid":"f91eec9c2a116310c2a97e6daf214fc77a8cf5b5"},"cell_type":"markdown","source":"Now to implement cross-validation in order to better compare different models."},{"metadata":{"trusted":true,"_uuid":"689a6c81499f6f5fc60e4f26ea1f17d4ff81e626"},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5350658bda4ef9e2bdf32ce46b4988501bb643c2"},"cell_type":"markdown","source":"Okay I want to test a few models against eachother, then tune the best one. I am going to use logistic regression, support vector classifier random forest classifier, and XGBoost. For each one I calculate its cross-validation score."},{"metadata":{"trusted":true,"_uuid":"d08d557479f201760e01fbbd5c79feb4ea3493de"},"cell_type":"code","source":"#Import the models and then construct a pipeline for each including imputation\nfrom sklearn.linear_model import LogisticRegression\nLR = LogisticRegression(random_state=1)\nfrom sklearn.svm import SVC\nSVC = SVC(random_state=1)\nfrom sklearn.ensemble import RandomForestClassifier\nRF = RandomForestClassifier(random_state=1)\nfrom xgboost import XGBClassifier\nXGB = XGBClassifier(random_state=1)\n\n#Function that takes a model and returns its cross-validation score\ndef cv_score(model):\n    scores = cross_val_score(model, X, y, scoring='accuracy', cv=10)\n    return scores.mean()\n\n#Calculate cross-validation scores using this function\nCV_scores = pd.DataFrame({'Cross-validation score':[cv_score(LR),cv_score(SVC),cv_score(RF),cv_score(XGB)]})\nCV_scores.index = ['LR','SVC','RF','XGB']\nprint(CV_scores)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8884a220f965a224ed0bd3931dfe8e37a47bc59d"},"cell_type":"markdown","source":"Let's tune the random forest classifier, since it did well and I understand it better than XGB. I use a grid search."},{"metadata":{"trusted":true,"_uuid":"2f894b90b7e551e9c56bf97bace68d95c58097de"},"cell_type":"code","source":"#Import GridSearchCV\nfrom sklearn.model_selection import GridSearchCV\n\n#Define parameter grid on the number of decision trees used and the maximum depth of the trees\nparameters = {'n_estimators':[100,120,150], 'max_depth':[5,10,15,20,25,30]}\n#Peform gridsearch\nRF_grid = GridSearchCV(RF, param_grid=parameters)\nRF_grid.fit(X,y)\n\n#Print the best parameters and what they scored\nprint('Best parameters:'+str(RF_grid.best_params_))\nprint('Best score:'+str(RF_grid.best_score_))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"34328b716bf830e0d201100c07690ac760ad20f5"},"cell_type":"markdown","source":"Now I use the optimised random forest to predict based on the test data."},{"metadata":{"trusted":true,"_uuid":"77f7e8e47459a49948c9ed1d977674b101e480d1"},"cell_type":"code","source":"#Make prediction and create a dataframe for submission\npredictions = RF_grid.predict(test)\nsubmission = pd.DataFrame({'PassengerId': passengerid, 'Survived': predictions})\nsubmission.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8d3eec97a80aadc5b3cd06ae8321007f46f1b48d"},"cell_type":"markdown","source":"Now to write these predictions to csv."},{"metadata":{"trusted":true,"_uuid":"a58b1cfaa7ad33f64ce5b4e672c6172cff8691e0"},"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}