{"nbformat": 4, "cells": [{"source": ["# This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "\n", "# Any results you write to the current directory are saved as output."], "cell_type": "code", "outputs": [], "metadata": {"_uuid": "1c3955f237585c53ed2f3e4b0efd7093fbeeba9d", "_cell_guid": "ef4c2900-3432-4b20-a3cf-6c6a696402f8"}, "execution_count": 4}, {"source": ["train_df =pd.read_csv('../input/train.csv')\n", "test_df = pd.read_csv('../input/test.csv')\n", "train_df.info()\n", "test_df.info()"], "cell_type": "code", "outputs": [], "metadata": {"_uuid": "cafde47d7266d240319897376803c917af4b013a", "_cell_guid": "b9790385-e31d-4090-bee5-23f2212885cb"}, "execution_count": 5}, {"source": ["train_df.describe()"], "cell_type": "code", "outputs": [], "metadata": {"_uuid": "a2edb0b11bd70cdc47710d640d55db74ce7d3370", "_cell_guid": "214e08de-beb3-476e-a6da-dbbb5e8c9890"}, "execution_count": 6}, {"source": ["train_df.head()"], "cell_type": "code", "outputs": [], "metadata": {"_uuid": "8d6c6dc238f355805efc6e1e52315c8d2538fc75", "_cell_guid": "e2ba77ef-a206-40e2-bb34-5b14a3d5e8fc"}, "execution_count": 7}, {"source": ["train_df.describe(include=['O'])"], "cell_type": "code", "outputs": [], "metadata": {"_uuid": "2495064d1bdafdd2fffaffd491b5d2257ed145c7", "_cell_guid": "734f1dc4-7503-4b93-a197-ce03521cee1e"}, "execution_count": 8}, {"cell_type": "markdown", "metadata": {"_uuid": "e4ceb044ffec4d73fb557db93430b7118faa2f48", "_cell_guid": "8aad1e8d-4c8c-40ce-88f9-6dfdcbe5d97f"}, "source": ["Column Info\n", "1. PassengerId, Name : Unique\n", "2. Ticket: Almost unique\n", "3. Cabin: It have many missing value.\n", "4. So, I select these feature (Pclass, Sex, Age, SibSp, Parch, Fare, Embarked)"]}, {"source": ["selected_feature = ['Pclass','Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n", "parameters = {}\n", "parameters['selected_feature'] = selected_feature"], "cell_type": "code", "outputs": [], "metadata": {"collapsed": true, "_uuid": "bf29a1fb466d9c54f370267000f4fc0aa0cf3cd8", "_cell_guid": "6199296d-7cf3-4a42-b6ec-9f8bce16c7ec"}, "execution_count": 9}, {"source": ["def cleanup_data(train_df, test_df):\n", "    age_mean = pd.concat([train_df['Age'], test_df['Age']], ignore_index=True).mean()\n", "    fare_mean = pd.concat([train_df['Fare'], test_df['Fare']], ignore_index=True).mean()\n", "    \n", "    train = train_df[['Survived'] + selected_feature].copy()\n", "    \n", "    train['Sex'] = train['Sex'].map({'male': 1, 'female': 0}).astype(int)\n", "    train['Age'] = train['Age'].fillna(age_mean)\n", "    train = train.dropna()\n", "    train['Embarked'] = train['Embarked'].map({'S': 0, 'C': 1, 'Q': 2}).astype(int)\n", "    \n", "    test = test_df[selected_feature].copy()\n", "    test['Sex'] = test['Sex'].map({'male': 1, 'female': 0}).astype(int)\n", "    test['Age'] = test['Age'].fillna(age_mean)\n", "    test['Fare'] = test['Fare'].fillna(fare_mean)\n", "    test['Embarked'] = test['Embarked'].map({'S': 0, 'C': 1, 'Q': 2}).astype(int)\n", "    \n", "    return train, test\n", "\n", "train, test = cleanup_data(train_df, test_df)"], "cell_type": "code", "outputs": [], "metadata": {"collapsed": true, "_uuid": "9fa8193754c8bc96a1c45d29d75618051ba88089", "_cell_guid": "5365e352-3e52-4add-a49e-437587af8395"}, "execution_count": 10}, {"source": ["train.describe()"], "cell_type": "code", "outputs": [], "metadata": {"_uuid": "ec70ec8bfff02299d916e4a05112f46fecba5b34", "_cell_guid": "e0fba21c-3bbe-4f8e-88f5-ba57d1ff66f9"}, "execution_count": 11}, {"source": ["test.describe()"], "cell_type": "code", "outputs": [], "metadata": {"_uuid": "7d88c82a92e29e9f710e361eb273d9ac92130ca9", "_cell_guid": "966c7372-9965-4ebd-9d51-8d7952d6c30f"}, "execution_count": 12}, {"source": ["def feature_scaling(parmeters):\n", "    def get_mean(data_list):\n", "        return pd.concat(data_list, ignore_index=True).mean()\n", "    \n", "    def get_std(data_list):\n", "        return pd.concat(data_list, ignore_index=True).std()\n", "\n", "    def get_min(data_list):\n", "        return pd.concat(data_list, ignore_index=True).min()\n", "\n", "    def get_max(data_list):\n", "        return pd.concat(data_list, ignore_index=True).max()\n", "\n", "    for feature in parameters['selected_feature']:\n", "        if parameters['feature_scaling'] == 'rescaling':\n", "            data_list = [train[feature], test[feature]]\n", "            min_ = get_min(data_list)\n", "            max_ = get_max(data_list)\n", "            train[feature] = (train[feature] - min_) / (max_ - min_)\n", "            test[feature] = (test[feature] - min_) / (max_ - min_)\n", "        elif parameters['feature_scaling'] == 'mean_normalization':\n", "            data_list = [train[feature], test[feature]]\n", "            mean = get_mean(data_list)\n", "            min_ = get_min(data_list)\n", "            max_ = get_max(data_list)\n", "            train[feature] = (train[feature] - mean) / (max_ - min_)\n", "            test[feature] = (test[feature] - mean) / (max_ - min_)\n", "        else:\n", "            data_list = [train[feature], test[feature]]\n", "            mean = get_mean(data_list)\n", "            std = get_std(data_list)\n", "            train[feature] = (train[feature] - mean) / std\n", "            test[feature] = (test[feature] - mean) / std"], "cell_type": "code", "outputs": [], "metadata": {"collapsed": true, "_uuid": "20cde30b11ab4ba52b4f75372ca1d5e6a59ef8cc", "_cell_guid": "a5cd8177-52fb-42ac-b7af-9cd0cd746cee"}, "execution_count": 13}, {"source": ["parameters['feature_scaling'] = 'standardization'\n", "feature_scaling(parameters)"], "cell_type": "code", "outputs": [], "metadata": {"collapsed": true, "_uuid": "3e33bd2e73d9af324523956c07adcec0def2108e", "_cell_guid": "383fd575-08a9-4109-8992-51ee4143a6db"}, "execution_count": 14}, {"source": ["train.describe()"], "cell_type": "code", "outputs": [], "metadata": {"_uuid": "1ac120927e4e642f4a150b703ef3f1c0a1fde01e", "_cell_guid": "6d2b63dd-6757-4e24-aede-f4c022ca9942"}, "execution_count": 15}, {"source": ["test.describe()"], "cell_type": "code", "outputs": [], "metadata": {"_uuid": "809a824d7c7a676e7ecf2372f20da59cac913022", "_cell_guid": "5631a1b7-6d85-407d-bcef-fd3d69d43b56"}, "execution_count": 16}, {"source": ["m = int(train.values.shape[0] * 0.7)\n", "train_X = train[selected_feature].values[:m, :]\n", "train_Y = train['Survived'].values.reshape(-1, 1)[:m, :]\n", "valid_X = train[selected_feature].values[m:, :]\n", "valid_Y = train['Survived'].values.reshape(-1, 1)[m:, :]\n", "test_X = test[selected_feature].values\n", "print(train_X.shape, train_Y.shape)\n", "print(valid_X.shape, valid_Y.shape)\n", "print(test_X.shape)"], "cell_type": "code", "outputs": [], "metadata": {"_uuid": "82a94e44426d39b3791b0efa37c6584e06d1028c", "_cell_guid": "f432c91d-3122-4bb6-85a2-695d22e55345"}, "execution_count": 17}, {"source": ["import math\n", "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n", "    \"\"\"\n", "    Creates a list of random minibatches from (X, Y)\n", "    \n", "    Arguments:\n", "    X -- input data, of shape (input size, number of examples)\n", "    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n", "    mini_batch_size -- size of the mini-batches, integer\n", "    \n", "    Returns:\n", "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n", "    \"\"\"\n", "    \n", "    np.random.seed(seed)            # To make your \"random\" minibatches the same as ours\n", "    m = X.shape[0]                  # number of training examples\n", "    mini_batches = []\n", "        \n", "    # Step 1: Shuffle (X, Y)\n", "    permutation = list(np.random.permutation(m))\n", "    shuffled_X = X[permutation, :]\n", "    shuffled_Y = Y[permutation, :].reshape((m,1))\n", "\n", "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n", "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n", "    for k in range(0, num_complete_minibatches):\n", "        ### START CODE HERE ### (approx. 2 lines)\n", "        mini_batch_X = shuffled_X[k * mini_batch_size : (k + 1) * mini_batch_size, :]\n", "        mini_batch_Y = shuffled_Y[k * mini_batch_size : (k + 1) * mini_batch_size, :]\n", "        ### END CODE HERE ###\n", "        mini_batch = (mini_batch_X, mini_batch_Y)\n", "        mini_batches.append(mini_batch)\n", "    \n", "    # Handling the end case (last mini-batch < mini_batch_size)\n", "    if m % mini_batch_size != 0:\n", "        ### START CODE HERE ### (approx. 2 lines)\n", "        mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size:, :]\n", "        mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size:, :]\n", "        ### END CODE HERE ###\n", "        mini_batch = (mini_batch_X, mini_batch_Y)\n", "        mini_batches.append(mini_batch)\n", "    \n", "    return mini_batches"], "cell_type": "code", "outputs": [], "metadata": {"collapsed": true, "_uuid": "b094bc376ec4964bd65bebbb3ebd2c37bd328a4e", "_cell_guid": "26e9e46e-4540-49d3-9829-b6f70fcb63d5"}, "execution_count": 18}, {"source": ["import tensorflow as tf\n", "def make_model(paramters):\n", "    num_feature = len(parameters['selected_feature'])\n", "    X = tf.placeholder(tf.float32, [None, num_feature])\n", "    Y = tf.placeholder(tf.float32, [None, 1])\n", "\n", "    layers_dim = paramters['layers_dim']\n", "    fc = tf.contrib.layers.stack(X, tf.contrib.layers.fully_connected, layers_dim)\n", "    hypothesis = tf.contrib.layers.fully_connected(fc, 1, activation_fn=None)\n", "    loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=hypothesis, labels=Y)\n", "    cost = tf.reduce_mean(loss)\n", "    \n", "    learning_rate = parameters['learning_rate']\n", "    train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n", "    \n", "    prediction = tf.round(tf.sigmoid(hypothesis))\n", "    correct_prediction = tf.equal(prediction, Y)\n", "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n", "    \n", "    model = {'X': X, 'Y': Y, 'hypothesis': hypothesis, 'cost': cost,\n", "             'train_op': train_op, 'prediction': prediction, 'accuracy': accuracy}\n", "    \n", "    return model"], "cell_type": "code", "outputs": [], "metadata": {"collapsed": true, "_uuid": "f457deead6cecd215c42dd4a3711e27a46bb51f1", "_cell_guid": "d0082405-7111-418a-a08a-106f8a42ca2e"}, "execution_count": 19}, {"source": ["def train(parameters, model):\n", "    num_epochs = parameters['num_epochs']\n", "    minibatch_size = parameters['minibatch_size']\n", "    train_size = train_X.shape[0]\n", "    saver = tf.train.Saver()\n", "    epoch_list = []\n", "    cost_list = []\n", "    with tf.Session() as sess:\n", "        init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n", "        sess.run(init_op)\n", "        for epoch in range(num_epochs):\n", "            epoch_cost = 0.\n", "            num_minibatches = int(train_size / minibatch_size)\n", "            minibatches = random_mini_batches(train_X, train_Y, minibatch_size)\n", "            for minibatch in minibatches:\n", "                (minibatch_X, minibatch_Y) = minibatch\n", "                feed_dict = {model['X'] : minibatch_X, model['Y'] : minibatch_Y}\n", "                _ ,minibatch_cost = sess.run([model['train_op'], model['cost']], feed_dict= feed_dict)\n", "                epoch_cost += minibatch_cost / num_minibatches\n", "            if parameters['print'] and (epoch % parameters['print_freq'] == 0):\n", "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n", "            if parameters['save_cost'] and (epoch % parameters['save_cost_freq'] == 0):\n", "                epoch_list.append(epoch)\n", "                cost_list.append(epoch_cost)\n", "        saver.save(sess, parameters['model_name'])\n", "    return {'epoch_list': epoch_list, 'cost_list' : cost_list}"], "cell_type": "code", "outputs": [], "metadata": {"collapsed": true, "_uuid": "697b934803740f09af3d4ec8b40fd2f27ec28d8f", "_cell_guid": "9a0c0bc9-d644-4ea1-9c1e-2b31b6e04125"}, "execution_count": 20}, {"source": ["# set model parameters\n", "parameters['layers_dim'] = [14]\n", "parameters['learning_rate'] = 0.01\n", "# set train parameters (hyper parameter)\n", "parameters['num_epochs'] = 2000\n", "parameters['minibatch_size'] = 16\n", "# set option parameters\n", "parameters['model_name'] = 'titanic'\n", "parameters['print'] = True\n", "parameters['print_freq'] = 100\n", "parameters['save_cost'] = True\n", "parameters['save_cost_freq'] = 10\n", "\n", "for k, v in parameters.items():\n", "    print(k, '=', v)"], "cell_type": "code", "outputs": [], "metadata": {"_uuid": "acbcd6269544e8cdd28de6590b439b1049ee502f", "_cell_guid": "15a52828-9627-4b87-abb6-e4b3cbb4eef9"}, "execution_count": 21}, {"source": ["with tf.Graph().as_default():\n", "    model = make_model(parameters)\n", "    plot_data = train(parameters, model)"], "cell_type": "code", "outputs": [], "metadata": {"_uuid": "e4bbdd6a4155c85f4c6c2a573a9a2b1b82d297a7", "_cell_guid": "a4effb81-d8a7-467d-80b4-9199e16004f7"}, "execution_count": 22}, {"source": ["import matplotlib.pyplot as plt\n", "%matplotlib inline\n", "print\n", "if parameters['save_cost']:\n", "    plt.plot(plot_data['epoch_list'], plot_data['cost_list'])"], "cell_type": "code", "outputs": [], "metadata": {"_uuid": "1799b249807f3fbe059d6751c77f415fedfb7beb", "_cell_guid": "5893a07a-d830-4b0d-96ee-6b599333918a"}, "execution_count": 23}, {"source": ["def evaluate(parameters, model):\n", "    saver = tf.train.Saver()\n", "    with tf.Session() as sess:\n", "        init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n", "        sess.run(init_op)\n", "        saver.restore(sess, parameters['model_name'])\n", "        print (\"Train Accuracy:\", model['accuracy'].eval({model['X']: train_X, model['Y']: train_Y}))\n", "        print (\"Valid Accuracy:\", model['accuracy'].eval({model['X']: valid_X, model['Y']: valid_Y}))"], "cell_type": "code", "outputs": [], "metadata": {"collapsed": true, "_uuid": "9ad580b551f59cf120fdc9610a372f9d85c27bd9", "_cell_guid": "41f346cf-c0f4-4f43-ac55-5ce767d26b3f"}, "execution_count": 24}, {"source": ["with tf.Graph().as_default():\n", "    model = make_model(parameters)\n", "    evaluate(parameters, model)"], "cell_type": "code", "outputs": [], "metadata": {"_uuid": "fd9564d23ed057b9219b54107d7dff524047b6a3", "_cell_guid": "0809622b-3ab1-4163-934a-d9e7a4ec8441"}, "execution_count": 25}, {"source": ["def predict(parameters, model):\n", "    saver = tf.train.Saver()\n", "    with tf.Session() as sess:\n", "        init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n", "        sess.run(init_op)\n", "        saver.restore(sess, parameters['model_name']) \n", "        return model['prediction'].eval({model['X']: test_X})"], "cell_type": "code", "outputs": [], "metadata": {"collapsed": true, "_uuid": "9a871efdf1457410f09822894c9f79a71298db81", "_cell_guid": "db1b4131-bd0a-49d4-bf92-5d305a8ff9cb"}, "execution_count": 26}, {"source": ["answer = pd.DataFrame(test_df['PassengerId'], columns=['PassengerId'])\n", "with tf.Graph().as_default():\n", "    model = make_model(parameters)\n", "    test_Y = predict(parameters, model)\n", "    answer['Survived'] = test_Y.astype(int)\n", "answer.to_csv('answer.csv', index=False)"], "cell_type": "code", "outputs": [], "metadata": {"_uuid": "be570798008a34e6bbe242fedb844c652b437e47", "_cell_guid": "478bbe9c-1c88-4f5a-be3d-79e97360cc6d"}, "execution_count": 27}, {"source": ["from sklearn import linear_model\n", "regr = linear_model.LinearRegression()\n", "regr.fit(train_X, train_Y)\n", "match = np.sum(test_Y == np.round(regr.predict(test_X)))\n", "print('match ratio with linear_model of scikit-learn: ', match / test_Y.shape[0])"], "cell_type": "code", "outputs": [], "metadata": {"_uuid": "e48ad3356c58383717e4d76b39b6c7ad0ccbe395", "_cell_guid": "ffdf3912-43ed-499b-a8a3-d64ef4686d78"}, "execution_count": 28}], "metadata": {"language_info": {"codemirror_mode": {"version": 3, "name": "ipython"}, "nbconvert_exporter": "python", "version": "3.6.3", "pygments_lexer": "ipython3", "file_extension": ".py", "mimetype": "text/x-python", "name": "python"}, "kernelspec": {"name": "python3", "language": "python", "display_name": "Python 3"}}, "nbformat_minor": 1}