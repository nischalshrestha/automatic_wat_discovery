{"cells":[{"metadata":{"_uuid":"708d8ca8227bfe81e1717d6e6bf2a383d7680739"},"cell_type":"markdown","source":"### 0. DESCRIPTION\n\nHi! I concieved of the ___Under the Hood___ series of Kaggle kernels as part of my machine learning training. The idea is to implement popular machine learning models in Python/Numpy and compare their performance against a benchmark, i.e. a similar model in the machine learning libraries like Scikit-learn or Keras. I agree with Andrej Karpathy who wrote in his [blog](http://karpathy.github.io/neuralnets/) : \"...everything became much clearer when I started writing code.\" \n\nThis kernel is dedicated to the ___Dense Neural Network___ (aka fully connected network) applied to the ___Titanic___ dataset. We have the following __challenges__: \nfor an adjustable network architecture implement model initialization, forward propagation with ReLU activation in the hidden layers and Sigmoid activation in the output layer, cross-entropy cost function with L2 regularization, backpropagation and parameters update with stochastic gradient descent and Adam optimizer, and finally prediction of binary labels after learning the model for various hyperparameters.\n \nAny feedback or ideas are welcome."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np                # to build the model\nimport pandas as pd               # to load, review and wrangle data\nimport matplotlib.pyplot as plt   # to plot the learning curve\nfrom keras.models import Sequential   # to compare with my model\nfrom keras.layers import Dense\nfrom keras.optimizers import Adam\nfrom sklearn.model_selection import train_test_split\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.regularizers import l2\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ec35c580b80877335c63b21dcd1d9c67947affed"},"cell_type":"markdown","source":"### **1. TITANIC DATA** "},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Import train and test data:\ntrain_data = pd.read_csv('../input/train.csv')\ntest_data = pd.read_csv('../input/test.csv')\nfull_data = train_data.append(test_data, ignore_index=True, sort=False)\n# review examples of data \nfull_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"11edae47912c1b6fffe1fc1060a929df4662f7c3"},"cell_type":"code","source":"# review information on data, pay attention to missing values and dtypes\ntrain_data.info()\nprint(\"_\" * 40)\ntest_data.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2731d5f06774eba64d3bbfe55059ec8998406833"},"cell_type":"markdown","source":"### 1.1. Pclass: this feature has a strong correlation with survival! Transform it into dummy variables"},{"metadata":{"trusted":true,"_uuid":"b2287a682d61e23f6f4fcf4e5047526694d1b751"},"cell_type":"code","source":"full_data[['Pclass', 'Survived']].groupby(['Pclass'], as_index=True).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8c118955562d2367cf6a7eaccf8ab0686f1b05c8"},"cell_type":"code","source":"# Create a new variable for every unique value of Pclass\npclass = pd.get_dummies(data=full_data.Pclass, prefix='Pclass', drop_first=False)\npclass.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7a3e4eb1d3c4cd594545aef7195a69a0476381fd"},"cell_type":"markdown","source":"### 1.2. Sex: it also has a strong correlation with survival! We transform it into a binary variable"},{"metadata":{"trusted":true,"_uuid":"1a1227b0bb528bdd1371119f67516eb4e2e0e1ae"},"cell_type":"code","source":"full_data[['Sex', 'Survived']].groupby(['Sex'], as_index=True).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3303325ce3c1b9cb756b9f87eb19388ad636483d"},"cell_type":"code","source":"# Transform Sex into binary values 0 and 1\nsex = pd.DataFrame(data=full_data.Sex.map({'female': '1', 'male': '0'}).astype('int'), columns=['Sex'])\nsex.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7839e136c253677719a2ffba206d38c0a04bd03c"},"cell_type":"markdown","source":"### 1.3. Embarked: it shows some correlation with survival, in particular @Cherbourg port. Get dummies:"},{"metadata":{"trusted":true,"_uuid":"c1edf5531d98ca1238a0d05f0bb702d237a3042f"},"cell_type":"code","source":"full_data[['Embarked', 'Survived']].groupby(['Embarked'], as_index=True).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"541791e13a5655912f327517df3f1963efeae301"},"cell_type":"code","source":"# look up missing values:\nfull_data.loc[full_data.Embarked.isnull(), :]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"19ff682d4dc76786daee5db33134517795ab30d8"},"cell_type":"code","source":"# As Sex and Pclass are much stronger indicators of survival, it would be reasonable\n# to assume that the 2 survived females from 1 class embarked in the port of highest survivig.\n# Create a new variable for every unique value of Embarked\nembarked = pd.get_dummies(data=full_data.Embarked.fillna('C'), prefix='Embarked')\nembarked.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bce4028eed0442a6f792274da82ffd79a7c540f7"},"cell_type":"markdown","source":"### 1.4. Name: Let's extract  a new 'Title' feature from 'Name', as it's likely to correlate with survival"},{"metadata":{"trusted":true,"_uuid":"268216a5b17ab4e3a4f8e9bbaef4d7673d44f7a1"},"cell_type":"code","source":"full_data['Title'] = full_data.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\nfull_data[['Title', 'Sex', 'Survived']].groupby(['Sex','Title'], as_index=True).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6af1f16fa1437eba4c1d9d9c84b316829d280209"},"cell_type":"code","source":"# Replacing rare titles with common would mean to reduce info to Mr/Mrs/Ms == Sex.\n# Therefore I group the rare titles into the groups of higher than average survival:\nTitle_Dictionary = {\"Capt\":       \"Mr\",\n                    \"Col\":        \"Officer\",\n                    \"Major\":      \"Officer\",\n                    \"Jonkheer\":   \"Mr\",\n                    \"Don\":        \"Mr\",\n                    \"Sir\" :       \"Royalty\",\n                    \"Dr\":         \"Officer\",\n                    \"Rev\":        \"Mr\",\n                    \"Countess\":   \"Royalty\",\n                    \"Dona\":       \"Mrs\",\n                    \"Mme\":        \"Mrs\",\n                    \"Mlle\":       \"Miss\",\n                    \"Ms\":         \"Mrs\",\n                    \"Mr\" :        \"Mr\",\n                    \"Mrs\" :       \"Mrs\",\n                    \"Miss\" :      \"Miss\",\n                    \"Master\" :    \"Officer\",\n                    \"Lady\" :      \"Royalty\"}\n\n# we map each title\nfull_data['Title'] = full_data.Title.map(Title_Dictionary)\nfull_data[['Title', 'Sex', 'Survived']].groupby(['Sex', 'Title'], as_index=True).mean()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e2604a7b1347d14a4daca45b818bc8a6fe7ad7d1"},"cell_type":"code","source":"# Map titles to categories (numbers) for fitting the model\ntitle = pd.get_dummies(full_data.Title)\ntitle.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b8ac8746647e8f98dd79a512153610e940ecd60"},"cell_type":"markdown","source":"### 1.5. Fare: look up and fix the missing value as it's a promising feature"},{"metadata":{"trusted":true,"_uuid":"95c407367d8ca8b94fa0aabe1f3a38d19168a5b6"},"cell_type":"code","source":"# look up the passenger with missing Fare value in the test_data: \nfull_data[full_data.Fare.isnull()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"05366d3e55bd7e009ea3b72c9fcceea9d32cc9ee"},"cell_type":"code","source":"# make an estimate of the missing Fare value for the above passenger in 3rd class embarked at S\ntest_data[[\"Pclass\", \"Fare\", \"Embarked\"]].groupby([\"Pclass\", \"Embarked\"]).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"201d923e86432d2ca1b4d243b7b1d881477b2d28"},"cell_type":"code","source":"# Fill in the missing value with 13.9 \nfare = pd.DataFrame(data=full_data.Fare.fillna(13.9))\n# normalize these comparatively high values:\nmu_fare = fare.Fare.mean()\nsigma_fare = (((fare.Fare-mu_fare)**2).mean())**0.5\nfare.Fare = (fare.Fare - mu_fare) / sigma_fare","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9ab11fc3e1db3c8260cd9867175a7d82be23066a"},"cell_type":"markdown","source":"### 1.6. Age: fill in missing vallues with median values over target groups"},{"metadata":{"trusted":true,"_uuid":"d954efe36e342fb5edcafd9f53b2c3325af32c3d"},"cell_type":"code","source":"# group Age by Sex, Pclass, and Title \ngrouped = full_data.groupby(['Sex', 'Pclass', 'Title'])  \n# view the median Age by the grouped features \ngrouped.Age.median()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b7ade16bb1999f82bfd64234c8742f50a6872819"},"cell_type":"code","source":"# apply the grouped median value on the Age NaN\nage = pd.DataFrame(data=grouped.Age.apply(lambda x: x.fillna(x.median())))\n# normalize these comparatively high values:\nmu_a = age.Age.mean()\nsigma_a = (((age.Age-mu_a)**2).mean())**0.5\nage.Age = (age.Age - mu_a) / sigma_a","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"305f27e5ca7280983021f12fe08492151b7309e1"},"cell_type":"markdown","source":"### 1.7. Cabin: availability of Cabin and its location can relate to survival"},{"metadata":{"trusted":true,"_uuid":"0a16d1d97d21e2ac16282ecc4ab354780ba9c575"},"cell_type":"code","source":"cabin = pd.DataFrame()\ncabin['Cabin'] = full_data.Cabin.fillna('U')\n# map each Cabin value with the cabin letter\ncabin['Cabin'] = cabin.Cabin.map(lambda x: x[0])\n# get dummies\ncabin = pd.get_dummies(cabin, drop_first=False)\ncabin.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a714afde9a6490499556912b275212a9d0099602"},"cell_type":"markdown","source":"### 1.8. Ticket: it correlates with other features like Pclass, Cabin, Fare, etc. Let's drop it as little new info."},{"metadata":{"_uuid":"8fc99318e5679d3519788a8879ba9d773ada3557"},"cell_type":"markdown","source":"### 1.9. Parch & SibSp: create family size and category for family size"},{"metadata":{"trusted":true,"_uuid":"f259e3255bdb730f626b3a6e066b57f99b80c2c9"},"cell_type":"code","source":"family = pd.DataFrame()\n# introducing a new feature: the size of families (including the passenger)\nfamily['FamilySize'] = full_data['Parch'] + full_data['SibSp'] + 1\n# introducing other features based on the family size\nfamily['Family_Single'] = family['FamilySize'].map(lambda s: 1 if s == 1 else 0)\nfamily['Family_Small'] = family['FamilySize'].map(lambda s: 1 if 2 <= s <= 4 else 0)\nfamily['Family_Large'] = family['FamilySize'].map(lambda s: 1 if 5 <= s else 0)\n# normalize the family size feature:\nmu_size = family.FamilySize.mean()\nsigma_size = (((family.FamilySize-mu_size)**2).mean())**0.5\nfamily.FamilySize = (family.FamilySize - mu_size) / sigma_size\nfamily.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f435f8664a007a445c5c697776d993a9fddb32f3"},"cell_type":"markdown","source":"### 1.10. Select data features for training the model"},{"metadata":{"trusted":true,"_uuid":"6fe30c9cc819d837294a3f0347c27286673cbb1c"},"cell_type":"code","source":"features = pd.concat([pclass, sex, embarked, title, fare, age, cabin, family] , axis=1 )\nfeatures.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"514d87ca4d37c7aec9c4e6e4d25e1eeabe605200"},"cell_type":"markdown","source":"### 1.11. Split the training and validation sets"},{"metadata":{"trusted":true,"_uuid":"711262d1b3ee3e48b6f4caf8931c986d53567c8a","scrolled":true},"cell_type":"code","source":"labels = train_data.Survived\ntrain_features, val_features, train_labels, val_labels = train_test_split(features[:891],\n                                                                          labels,\n                                                                          test_size = 0.2)\ntest_features = features[891:]\n\ntrain_features.shape, train_labels.shape, val_features.shape, val_labels.shape, test_features.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"27b2d81e233b893af91a0df0dfcbeb97d87fb91f"},"cell_type":"code","source":"# let's fix the important numbers for further modeling:\nm_train = train_features.shape[0]   # number of examples in the training set\nm_val = val_features.shape[0]       # number of examples in the validation set\nm_test = test_features.shape[0]     # number of examples in the test set\nn = train_features.shape[1]         # number of features in the model\nprint(f\" m_train = {m_train} / m_val = {m_val} / m_test = {m_test}, n = {n}\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"09c5a0c6fffc7db0b713393273826f3d00d97767"},"cell_type":"markdown","source":"### ** 2. A DENSE NEURAL NETWORK IN KERAS (binary classification)**"},{"metadata":{"trusted":true,"_uuid":"625db4a6878c155fa5a721303a36ad67e953a83f"},"cell_type":"code","source":"# Decide on the model architecture: [n_features, hidden_layers, binary_classifier]\nlayer_dims = [n, 7, 7, 1]  # the model architecture is adjustable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"770b58ba50ab4b5566c6797b3826ce4cf6c43719"},"cell_type":"code","source":"# create an instance of a neural network:\nk_model = Sequential()\n# the first hidden layer must have input dimensions:\nk_model.add(Dense(layer_dims[1], activation='relu',\n                  kernel_regularizer=l2(0.01),\n                  input_dim=n))\n# additional hidden layers are optional:\nk_model.add(Dense(layer_dims[2], activation='relu',\n                  kernel_regularizer=l2(0.01)))\n# the output layer- a binary classifier w/sigmoid activation:\nk_model.add(Dense(layer_dims[3], activation='sigmoid',\n                 kernel_regularizer=l2(0.01)))\n\n# Compile the model with Adam optimizer:\nk_model.compile(optimizer=Adam(lr=1e-1),\n                loss='binary_crossentropy',\n                metrics=['accuracy'])\n\n# Define a learning rate decay method:\nlr_decay = ReduceLROnPlateau(monitor='loss',\n                             patience=1,\n                             verbose=0,\n                             factor=0.5,\n                             min_lr=1e-7)\n# Train the model:\nk_model.fit(train_features, train_labels, epochs=300,\n            callbacks=[lr_decay], verbose=0)\n\n# Evaluate the model:\nk_train_loss, k_train_acc = k_model.evaluate(train_features, train_labels)\nk_val_loss, k_val_acc = k_model.evaluate(val_features, val_labels)\n\nprint(f'k_model: train accuracy = {round(k_train_acc * 100, 4)}%')\nprint(f'k_model: val accuracy = {round(k_val_acc * 100, 4)}%')\nprint(f'k_model: val error = {round((1 - k_val_acc) * m_val)} examples')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"921d4b93834b84d97a38b7851b0d1aa0084dab4f"},"cell_type":"markdown","source":"### 3. A CUSTOM NEURAL NETWORK IN PYTHON/NUMPY (binary classification)\n#### *Let's have a look \"under the hood\" at a similar model in Python/Numpy and appreciate the greatness of Keras :)*"},{"metadata":{"_uuid":"b5f70bd6c86b9924086087a25ec6db7f72151959"},"cell_type":"markdown","source":"### 3.1 Reshape data and define mini-batches"},{"metadata":{"trusted":true,"_uuid":"4d3a32f9473f322011bc6efd3ef2ee7bfb64960f"},"cell_type":"code","source":"# Reshape data to fit the Custom Model architecture: (n_features, n_examples)\nX_train = train_features.T.values\nY_train = train_labels.T.values.reshape(1, train_labels.shape[0])\nX_val = val_features.T.values\nY_val = val_labels.T.values.reshape(1, val_labels.shape[0])\nX_test = test_features.T.values\n\nX_train.shape, Y_train.shape, X_val.shape, Y_val.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"572989947a62c92e165f83b6d752ff276dd622b1"},"cell_type":"code","source":"# Define a function to create random mini-batches for sgd:\ndef random_mini_batches(X, Y, batch_size):\n    \"\"\"\n    This funcion creates a list of random minibatches from (X, Y)\n    Arguments:\n        X -- input data, of shape (input size, number of examples)\n        Y -- true \"label\" vector (1 / 0), of shape (1, number of examples)\n        batch_size -- size of the mini-batches, integer\n    Returns:\n        mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n    \"\"\"\n    m_train = X.shape[1]        # number of training examples\n    mini_batches = []\n        \n    # Step 1: Shuffle (X, Y)\n    permutation = list(np.random.permutation(m_train))\n    shuffled_X = X[:, permutation]\n    shuffled_Y = Y[:, permutation].reshape((1,m_train))\n\n    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n    num_complete_minibatches = m_train // batch_size # number of mini batches of size batch_size\n    for k in range(0, num_complete_minibatches):\n        mini_batch_X = shuffled_X[:, k*batch_size:(k+1)*batch_size]\n        mini_batch_Y = shuffled_Y[:, k*batch_size:(k+1)*batch_size]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    # Handling the end case (last mini-batch < mini_batch_size)\n    if m_train % batch_size != 0:\n        mini_batch_X = shuffled_X[:, num_complete_minibatches*batch_size:]\n        mini_batch_Y = shuffled_Y[:, num_complete_minibatches*batch_size:]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    return mini_batches","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e62cdaa8b8a50a0e4e84d9efb474e0a0b2965f99"},"cell_type":"markdown","source":"### 3.2 Define a neural network model with fully connected layers"},{"metadata":{"trusted":true,"_uuid":"a858034bfffa040daf2be78903e93325434ee903"},"cell_type":"code","source":"class Custom_model(object):\n    \n    def __init__(self, layer_dims):\n        \"\"\" \n        The model consists of an input layer (features), a number of hidden layers and\n        an output layer (binary classifier). To initialize an instance of the model we set\n        dimensions of its layers and initiaize parameters for the hidden/ output layers.\n        Arguments: \n            layer_dims -- list containing the input size and each layer size\n        Returns: \n            parameters -- python dictionary containing parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n                Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n                bl -- bias vector of shape (layer_dims[l], 1) \n        \"\"\"\n        self.layer_dims = layer_dims\n        self.num_layers = len(layer_dims)  # number of layers with weights (with input layer)\n        self.parameters = {}      # a dictionary with weight and bias parameters of the model\n        # Initializing weights randomly and biases to zeros\n        #np.random.seed(1)  # use for consistent initialization of params when tuning the model\n        for l in range(1, len(layer_dims)):\n            self.parameters[f\"W{l}\"] = np.random.randn(layer_dims[l], layer_dims[l-1])*0.01\n            self.parameters[f\"b{l}\"] = np.zeros((layer_dims[l], 1))\n    \n    # define getters and setters for accessing the class attributes:        \n    def get_layer_dims(self):\n        return self.layer_dims\n    def get_num_layers(self):\n        return self.num_layers\n    def get_params(self, key):\n        return self.parameters.get(key)\n    def set_params(self, key, value):\n        self.parameters[key] = value\n    \n        \n    def forward_propagation(self, X):\n        \"\"\" \n        Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n        Arguments:\n            X -- data, numpy array of shape (input size, number of examples)\n        Returns:\n            AL -- last post-activation value\n            caches -- list of caches containing:\n               every cache of layers w/ReLU activation (there are L-1 of them, indexed from 0 to L-2)\n               the cache of the output layer with Sigmoid activation (there is one, indexed L-1)\n        \"\"\"\n        caches = []\n        L = self.get_num_layers()-1    # number of layers with weights (hidden + output)\n        A = X                          # set input as the first hidden layer activation\n        # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n        for l in range(1, L):\n            A_prev = A                # initialize activation of the previous layer\n            W, b = self.get_params(f'W{l}'), self.get_params(f'b{l}') # get weights and biases\n            Z = W.dot(A_prev) + b     # linear activation for the hidden layers\n            A = np.maximum(0,Z)       # ReLU activation for the hidden layers\n            assert(A.shape == Z.shape)\n            cache = (A_prev, Z)       # useful during backpropagation\n            caches.append(cache)\n        # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n        W, b = self.get_params(f'W{L}'), self.get_params(f'b{L}')\n        Z = W.dot(A) + b              # Linear activation of the output layer\n        AL = 1 / (1 + np.exp(-Z))     # Sigmoid activation of the output layer\n        assert(AL.shape == (1,X.shape[1]))\n        cache = (A, Z)          # useful during backpropagation\n        caches.append(cache)\n        return AL, caches\n        \n    \n    def compute_cost(self, AL, Y, lambd):\n        \"\"\"\n        Implement the cost function with L2 regularization.\n        Arguments:\n            AL -- post-activation, output of forward propagation,\n                                    of shape (output size, number of examples)\n            Y -- \"true\" labels vector, of shape (output size, number of examples)\n            lambd -- regularization hyperparameter, scalar\n        Returns:\n            cost - value of the regularized loss function\n        \"\"\"\n        m = AL.shape[1]               # number of training examples\n        L = self.get_num_layers()-1   # number of layers with weights (hidden and output)\n        assert(Y.shape == AL.shape)\n        # Compute the cross-entropy part of the cost:\n        cross_entropy_cost = (1./m)*(-Y.dot(np.log(AL).T) - (1-Y).dot(np.log(1-AL).T))\n\n        # Compute L2 regularization cost\n        L2_reg_cost = 0\n        if lambd == 0:\n            pass\n        else:\n            for l in range(1, L+1):     # sum of all squared weights\n                L2_reg_cost += (1./m)*(lambd/2)*(np.sum(np.square(self.get_params(f'W{l}'))))\n\n        # Total cost:\n        cost = cross_entropy_cost + L2_reg_cost\n        # To make sure cost's shape is what we expect (e.g. this turns [[17]] into 17).\n        cost = np.squeeze(cost)   \n        return cost\n    \n            \n    def backward_propagation(self, AL, Y, caches, lambd):\n        \"\"\" \n        Implement the backward propagation for the [LINEAR->RELU]*(L-1)->[LINEAR->SIGMOID]\n        Arguments:\n            AL -- probability vector, output of the forward propagation\n            Y -- true \"label\" vector (0 or 1)\n            caches -- list of caches containing:\n                every cache of forward propagation with \"relu\" \n                                        (there are (L-1) or them, indexes from 0 to L-2)\n                the cache of forward propagation with \"sigmoid\" (there is one, index L-1)\n            lambd -- lambda, an L2 regularization parameter, scalar\n        Returns: grads -- a dictionary with updated gradients\n        \"\"\"\n        L = self.get_num_layers()-1   # number of layers with weights (hidden and output)\n        m = AL.shape[1]              # number of training examples\n        assert(Y.shape == AL.shape)\n        grads = {}     # a dict for the gradients of the cost function\n\n        # Initializing the backpropagation (derivative of cost w.r.t. AL)\n        dAL = -(np.divide(Y, AL) - np.divide(1-Y, 1-AL))\n\n        # Lth layer (SIGMOID -> LINEAR) gradients.\n        A_prev, Z = caches[L-1]   # get sigmoid and linear activations for the output layer\n        s = 1./(1. + np.exp(-Z))  # Sigmoid function of Z\n        dZ = dAL * s * (1-s)      # Gradient of the cost w.r.t. Z\n\n        W = self.get_params(f\"W{L}\")   # get weights for the output layer\n        dW = (1./m) * dZ.dot(A_prev.T) + (lambd/m) * W # Gradient of cost w.r.t. W\n        db = (1./m) * np.sum(dZ, axis=1, keepdims=True)  # Gradient of cost w.r.t. b\n        dA_prev = np.dot(W.T,dZ)            # Gradient of cost w.r.t. dA_prev\n\n        # Update the grads dictionary for the output layer\n        grads[f\"dA{L-1}\"] = dA_prev\n        grads[f\"dW{L}\"], grads[f\"db{L}\"] = dW, db\n\n        # l-th hidden layer: (RELU -> LINEAR) gradients.\n        for l in reversed(range(1, L)):   \n            A_prev, Z = caches[l-1]  # get ReLU and linear activations for l-th hidden layer\n            dZ = np.array(dA_prev, copy=True) # just converting dz to a correct object.\n            # When z <= 0, you should set dz to 0 as well. \n            dZ[Z <= 0] = 0\n            \n            W = self.get_params(f\"W{l}\")   # get weights for the l-th hidden layer\n            dW = (1./m) * dZ.dot(A_prev.T) + (lambd/m) * W # Gradient of cost w.r.t. W\n            db = (1./m) * np.sum(dZ, axis=1, keepdims=True) # Gradient of cost w.r.t. b\n            dA_prev = np.dot(W.T,dZ)            # Gradient of cost w.r.t. dA_prev\n            \n            # Update the grads dictionary for the hidden layers\n            grads[f\"dA{l-1}\"] = dA_prev \n            grads[f\"dW{l}\"], grads[f\"db{l}\"] = dW, db\n        \n        return grads\n    \n        \n    def update_parameters_with_gd(self, grads, lr):\n        \"\"\" \n        Update parameters using method gradient descent\n        Arguments:\n            grads -- python dictionary containing gradients, output of backprop\n            lr -- the learning rate, scalar\n        Returns: \n            updated parameters (weithts and biases of the model)\n        \"\"\"\n        L = self.get_num_layers()-1  # get number of layers with weights (hidden + output)\n        for l in range(1, L+1):\n            self.set_params(f\"W{l}\", self.get_params(f\"W{l}\") - lr * grads[f\"dW{l}\"])\n            self.set_params(f\"b{l}\", self.get_params(f\"b{l}\") - lr * grads[f\"db{l}\"])\n               \n            \n    def initialize_adam(self):\n        \"\"\"\n        Initializes v and s for the Adam optimizer as two python dictionaries with:\n            keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n            values: numpy arrays of zeros of the same shape as the corresponding gradients.\n        Returns: \n            v -- python dict that will contain the exponentially weighted average of the gradient.\n            s -- python dict that will contain the exponentially weighted average of the squared gradient.\n        \"\"\"\n        v = {}   \n        s = {}\n        L = self.get_num_layers()-1   # get number of layers with weights (hidden and output)\n        for l in range(1, L+1):\n            v[f\"dW{l}\"] = np.zeros((self.get_params(f\"W{l}\").shape))\n            v[f\"db{l}\"] = np.zeros((self.get_params(f\"b{l}\").shape))\n            s[f\"dW{l}\"] = np.zeros((self.get_params(f\"W{l}\").shape))\n            s[f\"db{l}\"] = np.zeros((self.get_params(f\"b{l}\").shape))    \n        return v,s\n    \n    \n    def update_parameters_with_adam(self, grads, v, s, t, lr, beta1, beta2, epsilon):\n        \"\"\"\n        Update parameters using Adam optimization algorithm\n        Arguments:\n            v -- Adam variable, moving average of the first gradient, python dict\n            s -- Adam variable, moving average of the squared gradient, python dict\n            t -- current timestep (minibatch)\n            lr -- the learning rate, scalar.\n            beta1 -- Exponential decay hyperparameter for the first moment estimates \n            beta2 -- Exponential decay hyperparameter for the second moment estimates \n            epsilon -- hyperparameter preventing division by zero in Adam updates\n        Returns:\n            updated parameters (model attributes)\n            v -- Adam variable, moving average of the first gradient, python dict\n            s -- Adam variable, moving average of the squared gradient, python dict\n        \"\"\"\n        v_corr = {}       # Initializing a bias-corrected first moment\n        s_corr = {}       # Initializing a bias corrected second moment estimate\n        L = self.get_num_layers()-1   # get number of layers with weights (hidden+output)\n        \n        # Perform Adam update on all parameters\n        for l in range(1, L+1):\n            # Moving average of the gradients\n            v[f\"dW{l}\"] = beta1*v[f\"dW{l}\"] + (1-beta1)*grads[f\"dW{l}\"]\n            v[f\"db{l}\"] = beta1*v[f\"db{l}\"] + (1-beta1)*grads[f\"db{l}\"]\n\n            # Compute bias-corrected first moment estimate\n            v_corr[f\"dW{l}\"] = v[f\"dW{l}\"]/(1-beta1**t)\n            v_corr[f\"db{l}\"] = v[f\"db{l}\"]/(1-beta1**t)\n\n            # Moving average of the squared gradients\n            s[f\"dW{l}\"] = beta2*s[f\"dW{l}\"] + (1-beta2)*grads[f\"dW{l}\"]**2\n            s[f\"db{l}\"] = beta2*s[f\"db{l}\"] + (1-beta2)*grads[f\"db{l}\"]**2\n\n            # Compute bias-corrected second raw moment estimate\n            s_corr[f\"dW{l}\"] = s[f\"dW{l}\"]/(1-beta2**t)\n            s_corr[f\"db{l}\"] = s[f\"db{l}\"]/(1-beta2**t)\n\n            # Update parameters\n            temp_W = v_corr[f\"dW{l}\"]/(s_corr[f\"dW{l}\"]**0.5 + epsilon)\n            temp_b = v_corr[f\"db{l}\"]/(s_corr[f\"db{l}\"]**0.5 + epsilon)\n            self.set_params(f\"W{l}\", self.get_params(f\"W{l}\") - lr*temp_W)\n            self.set_params(f\"b{l}\", self.get_params(f\"b{l}\") - lr*temp_b)\n\n        return v, s\n        \n        \n    def predict(self, X):\n        \"\"\" \n        This method is used to predict the results of a L-layer neural network.\n        Arguments:\n            X -- data set of examples you would like to label\n        Returns:\n            p -- predictions for the given dataset X\n        \"\"\"\n        m = X.shape[1]    # number of examples\n        p = np.zeros((1, m))\n        ### Forward propagation\n        AL, _ = self.forward_propagation(X)\n        ### convert probabilities AL to 0/1 predictions\n        for i in range(AL.shape[1]):\n            if AL[0,i] > 0.5:\n                p[0,i] = 1\n            else:\n                p[0,i] = 0\n        return np.int64(p)\n    \n        \n    def fit(self, X, Y, batch_size, num_epochs, lr, min_lr=1e-5, lambd=0,\n            beta1=0.9, beta2=0.999, epsilon=1e-8, optimizer='adam', print_cost=False):\n        \"\"\" \n        Implements an L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n        Arguments:\n            X -- data, numpy array of shape (# of features, # of examples)\n            Y -- true \"label\" vector, of shape (1, number of examples)\n            batch_size -- the size of a mini batch on which parameters are get updated\n            num_epochs -- number of epochs, i.e. passes through the training set\n            lr -- learning rate of the gradient descent update rule\n            min_lr -- the lower threshold of the learning rate decay\n            lambd -- lambda, L2 regularization hyperparameter\n            optimizer -- optimization metod [\"gd\"=gradient_descent or \"adam\"]\n            beta1 -- exp decay hyperparameter for the past gradients estimates in 'adam'\n            beta2 -- exp decay hyperparameter for the past squared gradients estimates in 'adam'\n            epsilon -- hyperparameter preventing division by zero in 'adam' updates\n            print_cost -- if True, it prints the cost every 100 steps\n        Returns:\n            parameters -- parameters learnt by the model. They can then be used to predict.\n        \"\"\"\n        costs = []                # to keep track of the cost\n        lr_0 = lr                 # to fix the initial learning rate before decay\n        t = 0                     # initializing the minibatch counter (for Adam update)\n        cost_prev = 1000          # initialize cost to a big number\n        # Initialize the optimizer\n        if optimizer == \"gd\":\n            pass                  # no initialization required for gradient descent\n        elif optimizer == \"adam\":\n            v, s = self.initialize_adam()\n        # Optimization loop\n        for epoch in range(num_epochs):\n            # Define the random minibatches. Reshuffle the dataset after each epoch\n            minibatches = random_mini_batches(X, Y, batch_size)\n            for minibatch in minibatches:\n                # Select a minibatch\n                (minibatch_X, minibatch_Y) = minibatch\n                # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n                AL, caches = self.forward_propagation(minibatch_X)\n                # Compute cost.\n                cost = self.compute_cost(AL, minibatch_Y, lambd)\n                # Backward propagation.\n                grads = self.backward_propagation(AL, minibatch_Y, caches, lambd)\n                # Update parameters\n                t = t + 1 # minibatch counter\n                if optimizer == \"gd\":\n                    self.update_parameters_with_gd(grads, lr)\n                elif optimizer == \"adam\":\n                    v, s = self.update_parameters_with_adam(grads, v, s, t, lr, \n                                                            beta1, beta2, epsilon)\n                # Define learning rate decay:\n                if cost > cost_prev and lr > min_lr:\n                    lr = lr / 2   # reduce lr, but not below the min value\n                cost_prev = cost  # save cost value for the next iteration\n                \n            # Print the cost every 20 epoch\n            if print_cost and epoch % 20 == 0:\n                print(f\"Cost after epoch {epoch}: {cost}\")\n            if print_cost and epoch % 1 == 0:    \n                costs.append(cost)\n        # plot the Learning curve\n        if print_cost:\n            plt.plot(np.squeeze(costs))\n            plt.ylabel(\"cost\")\n            plt.xlabel(\"epochs (x 1)\")\n            plt.title(f\"Learning rate = {lr_0} / {lr}\")\n            plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"433ee3eddee3490e413cefe49ed7a916592f9875"},"cell_type":"markdown","source":"### **4. MODEL TRAINING  **"},{"metadata":{"trusted":true,"_uuid":"0645c96fad9346239305cb8fa8587a003e085792"},"cell_type":"code","source":"# Create list to save journal records of the current session\nrecords_list = []","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2d9f3c85bab6025dc87686da176ef3d75e055318"},"cell_type":"code","source":"# Create an instance of the Custom_model class with 'layer_dims' architecture:\nc_model = Custom_model(layer_dims)\n\n# Set hyperparameters that we want to tune for the custom model:\nlr              = 1e-1       # the learning rate for the gradient descent\nmin_lr          = 1e-7       # the lower threshold of the learning rate decay\noptimizer       = 'adam'\nbatch_size      = m_train   \nnum_epochs      = 300          \nlambd           = 5e+0          # lambda - regularization hyperparameter, scalar\n\n# Train the model at various hyperparameters settings:\nc_model.fit(X_train, Y_train, batch_size=batch_size, num_epochs=num_epochs, \n            lr=lr, lambd=lambd, optimizer=optimizer, print_cost=True)       \n\n# Evaluation on the train data:\npredict_train = c_model.predict(X_train)\nc_train_acc = round(np.sum(predict_train == Y_train)/m_train, 2)\n\n# Evaluation on the validation data:\npredict_val = c_model.predict(X_val)\nc_val_acc = round(np.sum(predict_val == Y_val)/m_val, 2)\n\nprint(f\"c_model: train accuracy: {c_train_acc * 100}%\")\nprint(f\"c_model: val accuracy = {c_val_acc * 100}%\")\nprint(f'c_model: val error = {round((1 - c_val_acc) * m_val)} examples')\n\n# Update the journal of hyperparameters tuning records\nrecord = {'layer_dims'   : c_model.get_layer_dims(), \n          'acc_train'    : c_train_acc, \n          'acc_val'      : c_val_acc,\n          'val_error'    : round((1 - c_val_acc) * m_val),\n          'batch_size'   : batch_size,\n          'num_epochs'   : num_epochs,\n          'learning_rate': lr,\n          'min_lr'       : min_lr,\n          'lambda'       : lambd}\n\nrecords_list.append(record)\njournal = pd.DataFrame(records_list)   # saves records only when repeatedly running the current cell\njournal","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c338a8365e6dcf5f0210b4179e47118fc5f36861"},"cell_type":"markdown","source":"## Conclusion: we have looked \"under the hood\" of a fully connected network using a custom model with adjustable numbers of layers and their dimensions, L2 regularization and Adam optimization algotithm. Its performance is comparable to the Keras models.\n\n## Great! We know how it works!"},{"metadata":{"_uuid":"af5a5b2a6d6bde87d18c60be6e9ef4d13c331786"},"cell_type":"markdown","source":"### 5. PREDICTION ON TEST DATA AND SUBMISSION FILE"},{"metadata":{"trusted":true,"_uuid":"3fe6f88719522c0e6352b392345c3c10c6d23d70"},"cell_type":"code","source":"# Train the final model on all the data available with pretrained hyperparameters:\nc_model_final = Custom_model(layer_dims)\nX = np.concatenate((X_train, X_val), axis=1)\nY = np.concatenate((Y_train, Y_val), axis=1)\nc_model_final.fit(X, Y, batch_size=(m_train + m_val), num_epochs=num_epochs, \n                  lr=lr, min_lr=min_lr, lambd=lambd, \n                  optimizer=optimizer, print_cost=True)\n\n# Predict labels on the test dataset\nprediction = c_model_final.predict(X_test)\n\n# Save a submittion file with the predicted labels\nsubmission = pd.DataFrame({\"PassengerId\": test_data[\"PassengerId\"],\n                           \"Survived\": np.squeeze(prediction)}).sort_values(by=\"PassengerId\")\nsubmission.to_csv(\"submission.csv\", index=False)\nsubmission.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}