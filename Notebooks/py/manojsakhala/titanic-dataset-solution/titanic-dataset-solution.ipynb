{"cells": [{"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "8466f497-98e5-42dd-85e8-c6d8e5a92fcd", "collapsed": true, "_uuid": "f9a67b9ef8b861c01b92674eeb079258b1145bbd"}, "source": ["import pandas as pd\n", "import seaborn as sns\n", "import matplotlib.pyplot as plt\n", "import numpy as np\n", "%matplotlib inline\n", "sns.set_style(\"whitegrid\")\n", "from sklearn import preprocessing\n", "import warnings\n", "warnings.filterwarnings('ignore')"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "8aa54c26-118b-44c2-9ba5-355232cf0ebc", "collapsed": true, "_uuid": "aaf73b846b06e4ec3b1b41593dfb42d0a134daf6"}, "source": ["#Load Titanic Dataset\n", "train = pd.read_csv(\"../input/train.csv\")\n", "test = pd.read_csv(\"../input/test.csv\")"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "495dda92-62f5-43af-a603-a1456fcf61cc", "_uuid": "a36f8a1b8e411f836cef7b94325b754788639eaa"}, "source": ["#Get detailed information about datasets\n", "train.info()\n", "print('=' * 40)\n", "test.info()"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "767fe19a-cd51-4df8-9252-11b9f8453f69", "_uuid": "4f80b1da553d156cb2e14e1d1213661264b5fd47"}, "source": ["# Feature Engineering"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "378fe4b5-3b3c-48ce-9ca6-02ba58d7ba20", "_uuid": "564641aa444f3bb1dfbafa03328af845d871f31c"}, "source": ["#Get a view of data\n", "train.head(10)"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "e01ea350-da62-41ea-8165-0b69cfc1ec93", "_uuid": "4c80cb4d97e95fe64f49359947ab2d406d64963c"}, "source": ["#Get other description\n", "train.describe()"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "2a553a33-8e20-4730-abd6-5ffcd6bf07cf", "collapsed": true, "_uuid": "e38ae5fbb12ddf9ec28064a8bd1c9b9caf131983"}, "source": ["#Remove unnecessary columns\n", "train.drop(['PassengerId', 'Ticket'], axis=1, inplace=True)\n", "test.drop(['Ticket'], axis=1, inplace=True)"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "42a5039a-bfe1-4e95-a401-15d061508426", "_uuid": "8c59f0e652772d298832208bb4a95839cc174a0b"}, "source": ["#Feature - Pclass\n", "\n", "#Number of null values\n", "print(\"Total Null Entries in training samples :\", train['Pclass'].isnull().sum())\n", "print(\"Total Null Entries in testing samples  :\", test['Pclass'].isnull().sum())\n", "\n", "#Data Visualization\n", "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(12,5))\n", "sns.pointplot(data=train, x=\"Pclass\", y=\"Survived\", ax=ax1)\n", "sns.countplot(data=train, x=\"Pclass\", hue=\"Survived\", ax=ax2)"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "0658a643-def1-42b4-9933-afe224f11387", "_uuid": "a5fcd5ab76117399c6b056dfd5a9fc71a95685e5"}, "source": ["#Feature - Name\n", "\n", "#Number of null values\n", "print(\"Total Null Entries in training samples :\", train['Name'].isnull().sum())\n", "print(\"Total Null Entries in testing samples  :\", test['Name'].isnull().sum())\n", "\n", "#Names doesn't matter much, but the title associated with it may help.\n", "#Extract titles from names\n", "train['Title'] = train[\"Name\"].map(lambda name : name.split(\".\")[0].split(\" \")[-1])\n", "test['Title'] = test[\"Name\"].map(lambda name : name.split(\".\")[0].split(\" \")[-1])\n", "train[\"Title\"] = train[\"Title\"].map({\"Mr\" : \"Mr\", \"Mrs\" : \"Mrs\", \"Miss\" : \"Miss\", \"Master\" : \"Master\"})\n", "train[\"Title\"].fillna(\"Others\", inplace=True)\n", "test[\"Title\"] = test[\"Title\"].map({\"Mr\" : \"Mr\", \"Mrs\" : \"Mrs\", \"Miss\" : \"Miss\", \"Master\" : \"Master\"})\n", "test[\"Title\"].fillna(\"Others\", inplace=True)\n", "\n", "#Data Visualization\n", "fig, ax1 = plt.subplots(nrows=1, ncols=1, figsize=(15,8))\n", "sns.countplot(data=train, x=\"Title\", hue=\"Survived\", ax=ax1)\n", "\n", "#Map titles to nominal values\n", "train[\"Title\"] = train[\"Title\"].map({\"Mr\" : 0, \"Mrs\" : 1, \"Miss\" : 2, \"Master\" : 3, \"Others\" : 4})\n", "test[\"Title\"] = test[\"Title\"].map({\"Mr\" : 0, \"Mrs\" : 1, \"Miss\" : 2, \"Master\" : 3, \"Others\" : 4})\n", "\n", "#Drop names\n", "train.drop('Name', axis=1, inplace=True)\n", "test.drop('Name', axis=1, inplace=True)"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "cf1db460-20c0-434f-bcd1-97c63e289d99", "_uuid": "8b055c31d8cc3606b27f192fd1b84af036975e51"}, "source": ["#Feature - Sex\n", "\n", "#Number of null values\n", "print(\"Total Null Entries in training samples :\", train['Sex'].isnull().sum())\n", "print(\"Total Null Entries in testing samples  :\", test['Sex'].isnull().sum())\n", "\n", "#Data Visualization\n", "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,5))\n", "sns.pointplot(data=train, x=\"Sex\", y=\"Survived\", ax=ax1)\n", "sns.countplot(data=train, x=\"Sex\", hue=\"Survived\", ax=ax2)\n", "\n", "#Map to integer data\n", "train[\"Sex\"] = train[\"Sex\"].map({\"male\" : 0, \"female\" : 1})\n", "test[\"Sex\"] = test[\"Sex\"].map({\"male\" : 0, \"female\" : 1})"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "6f966d13-0fc5-4f1f-88a3-37b905a8c0ea", "scrolled": true, "_uuid": "1c5add7a3591bce3f4a8950a74db4f4d6fbafd50"}, "source": ["#Feature - Age\n", "\n", "#Number of null values\n", "print(\"Total Null Entries in training samples :\", train['Age'].isnull().sum())\n", "print(\"Total Null Entries in testing samples  :\", test['Age'].isnull().sum())\n", "\n", "#One of the way to deal with missing values is to replace them with mean of that feature values, \n", "#but since there are too many null entries, i will be categorizing these entries into \n", "#labels - [child, adult, old, missingdata]\n", "\n", "train[\"AgeCategory\"] = \"Adult\"\n", "train[\"AgeCategory\"].loc[train[\"Age\"] < 18 ] = \"Child\"\n", "train[\"AgeCategory\"].loc[train[\"Age\"] > 50 ] = \"Old\"\n", "train[\"AgeCategory\"].loc[train[\"Age\"].isnull()] = \"MissingData\"\n", "\n", "test[\"AgeCategory\"] = \"Adult\"\n", "test[\"AgeCategory\"].loc[train[\"Age\"] < 18 ] = \"Child\"\n", "test[\"AgeCategory\"].loc[train[\"Age\"] > 50 ] = \"Old\"\n", "test[\"AgeCategory\"].loc[train[\"Age\"].isnull()] = \"MissingData\"\n", "\n", "#Data Visualization\n", "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,5))\n", "sns.barplot(data=train, x=\"AgeCategory\", y=\"Survived\", ax=ax1, order=[\"Child\", \"Adult\", \"Old\", \"MissingData\"])\n", "sns.countplot(data=train, x=\"AgeCategory\", hue=\"Survived\", ax=ax2, order=[\"Child\", \"Adult\", \"Old\", \"MissingData\"])\n", "\n", "#Map to integer data\n", "train[\"AgeCategory\"] = train[\"AgeCategory\"].map({\"Child\" : 0, \"Adult\" : 1, \"Old\" : 2, \"MissingData\" : 3})\n", "test[\"AgeCategory\"] = test[\"AgeCategory\"].map({\"Child\" : 0, \"Adult\" : 1, \"Old\" : 2, \"MissingData\" : 3})\n", "\n", "#Drop Age\n", "train.drop(\"Age\", axis=1, inplace=True)\n", "test.drop(\"Age\", axis=1, inplace=True)"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "b546b533-9c89-4569-a65a-29e7d2f2a80c", "_uuid": "8d4533e94b98b79d94aae4c626ea77c8a768882c"}, "source": ["#Feature - Family\n", "\n", "#From sibSp and Parch, we can add whether a person has a family or not.\n", "#Also we will create another feature determining number of family members onboard.\n", "\n", "train['Family'] = train['SibSp'] + train['Parch'] + 1\n", "train['FamilySize'] = train['Family']\n", "train['FamilySize'].loc[train['Family'] == 1] = \"Small\"\n", "train['FamilySize'].loc[train['Family'] > 1] = \"Medium\"\n", "train['FamilySize'].loc[train['Family'] > 5] = \"Large\"\n", "train['Family'].loc[train['Family'] > 1] = 'withFamily'\n", "train['Family'].loc[train['Family'] == 1] = 'Alone'\n", "\n", "test['Family'] = test['SibSp'] + test['Parch'] + 1\n", "test['FamilySize'] = test['Family']\n", "test['FamilySize'].loc[test['Family'] == 1] = \"Small\"\n", "test['FamilySize'].loc[test['Family'] > 1] = \"Medium\"\n", "test['FamilySize'].loc[test['Family'] > 5] = \"Large\"\n", "test['Family'].loc[test['Family'] > 1] = 'withFamily'\n", "test['Family'].loc[test['Family'] == 1] = 'Alone'\n", "\n", "#Data Visualization\n", "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,5))\n", "sns.barplot(data=train, x=\"Family\", y=\"Survived\", ax=ax1)\n", "sns.countplot(data=train, x=\"Family\", hue=\"Survived\", ax=ax2)\n", "fig, (ax3, ax4) = plt.subplots(1, 2, figsize=(12,5))\n", "sns.barplot(data=train, x=\"FamilySize\", y=\"Survived\", ax=ax3, order=[\"Small\", \"Medium\", \"Large\"])\n", "sns.countplot(data=train, x=\"FamilySize\", hue=\"Survived\", ax=ax4, order=[\"Small\", \"Medium\", \"Large\"])\n", "\n", "#Map to integral values\n", "train[\"Family\"] = train[\"Family\"].map({\"Alone\" : 0, \"withFamily\" : 1})\n", "train[\"FamilySize\"] = train[\"FamilySize\"].map({\"Small\" : 0, \"Medium\" : 1, \"Large\" : 2})\n", "test[\"Family\"] = test[\"Family\"].map({\"Alone\" : 0, \"withFamily\" : 1})\n", "test[\"FamilySize\"] = test[\"FamilySize\"].map({\"Small\" : 0, \"Medium\" : 1, \"Large\" : 2})\n", "\n", "#Drop SibSp and Parch Columns\n", "train.drop([\"Parch\", \"SibSp\"], axis=1, inplace=True)\n", "test.drop([\"Parch\", \"SibSp\"], axis=1, inplace=True)"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "e61a65ab-4099-4f29-aca5-41644eb9a5aa", "_uuid": "e693bac1db347b2187235ef736a5e3b1dfdb07b4"}, "source": ["#Feature - Fare\n", "\n", "#Number of null values\n", "print(\"Total Null Entries in training samples :\", train['Fare'].isnull().sum())\n", "print(\"Total Null Entries in testing samples  :\", test['Fare'].isnull().sum())\n", "\n", "#Fill null values with mean fare\n", "test['Fare'].fillna(train['Fare'].mean(), inplace=True)\n", "\n", "#Data Visualization\n", "fig = plt.figure(figsize=(15,8))\n", "plt.hist([train[train[\"Survived\"]==0][\"Fare\"], train[train[\"Survived\"]==1][\"Fare\"]], stacked=True, bins=20, label=['Dead', 'Survived'])\n", "plt.xlabel(\"Fare Range\")\n", "plt.ylabel(\"Count\")\n", "plt.legend()\n", "\n", "#Fare data contains some extreme values which can be normalized\n", "scale = preprocessing.MinMaxScaler()\n", "train['normalizedFare'] = scale.fit_transform(train['Fare'].reshape(-1,1))\n", "\n", "#Data Visualization\n", "fig = plt.figure(figsize=(15,8))\n", "plt.hist([train[train[\"Survived\"]==0][\"normalizedFare\"], train[train[\"Survived\"]==1][\"normalizedFare\"]], stacked=True, bins=10, label=['Dead', 'Survived'])\n", "plt.xlabel(\"Normalized Fare Range\")\n", "plt.ylabel(\"Count\")\n", "plt.legend()\n", "\n", "test[\"normalizedFare\"] = scale.transform(test['Fare'].reshape(-1,1))\n", "train.drop(\"Fare\", axis=1, inplace=True)\n", "test.drop(\"Fare\", axis=1, inplace=True)"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "5fb9dc4f-2ce3-463e-baeb-0bc0fbf86d08", "_uuid": "6a395bd488a58700dc69358a7c04b47b73aa176a"}, "source": ["#Feature - Embarked\n", "\n", "#Number of null values\n", "print(\"Total Null Entries in training samples :\", train['Embarked'].isnull().sum())\n", "print(\"Total Null Entries in testing samples  :\", test['Embarked'].isnull().sum())\n", "\n", "#Fill missing values with maximum occurence of embarked category\n", "print(\"Maximum Occurrence :\", train['Embarked'].describe()['top'])\n", "train['Embarked'].fillna('S', inplace=True)\n", "test['Embarked'].fillna('S', inplace=True)\n", "\n", "#Data Visualization\n", "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,5))\n", "sns.pointplot(data=train, x=\"Embarked\", y=\"Survived\", ax=ax1)\n", "sns.countplot(data=train, x=\"Embarked\", hue=\"Survived\", ax=ax2)\n", "\n", "#Map Ports to integral values\n", "train[\"Embarked\"] = train[\"Embarked\"].map({\"S\" : 0, \"C\" : 1, \"Q\" : 2})\n", "test[\"Embarked\"] = test[\"Embarked\"].map({\"S\" : 0, \"C\" : 1, \"Q\" : 2})"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "be869540-831c-45f1-939a-0db01cf4eca5", "_uuid": "6f808380b49db20ee08c043c4bd4e573cefd24fb"}, "source": ["#Feature - Cabin\n", "\n", "#Number of null values\n", "print(\"Total Null Entries in training samples :\", train['Cabin'].isnull().sum())\n", "print(\"Total Null Entries in testing samples  :\", test['Cabin'].isnull().sum())\n", "\n", "#We are also dropping Cabin feature, since it contains a lot of missing values and cannot be used as such.\n", "train.drop(\"Cabin\", axis=1, inplace=True)\n", "test.drop(\"Cabin\", axis=1, inplace=True)"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "ea448322-b1ed-45db-bf59-652114e46e44", "_uuid": "f4c661c150066b6ba14ec6a7b96bd40fff79e85d"}, "source": ["# Machine Learning Algorithm Analysis"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "2f78f68e-a010-480e-9442-ad0446080880", "collapsed": true, "_uuid": "032f8cb98c1706ac9d702e4d3c0530c29eaee6da"}, "source": ["from sklearn.model_selection import cross_val_score\n", "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB \n", "from sklearn.neighbors import KNeighborsClassifier\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn import svm\n", "from sklearn.tree import DecisionTreeClassifier\n", "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "22ebb2d1-6382-439c-8c6d-bb8f7c3e5e59", "collapsed": true, "_uuid": "dd8d58a82f33e1289d2abd1f1a251c777f4115ee"}, "source": ["#Divide data into input and target values\n", "X_train = train.drop(\"Survived\", axis=1)\n", "y_train = train[\"Survived\"]\n", "\n", "#create dictionary to keep track of accuracies\n", "Accuracy = {}"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "6ccb8051-de88-4537-baad-9df5e30e3bcf", "_uuid": "adc26571ca63fd9f68e8ac79112ea4c0e46b2fb0"}, "source": ["X_train.head(10)"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "af0bd7b3-5b64-4a25-aab6-96ee35d7d97c", "_uuid": "f26f0cff6692f3aaf32fbd0cd2b5c24713b8f0e3"}, "source": ["X_train.describe()"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "b1631385-ef7f-4164-b41f-e67e58ae0aa5", "_uuid": "c78d4568e1780f995c0bfec4f8c116eab77ec819"}, "source": ["#Method - Naive Bayes \n", "\n", "#Titanic Dataset is evaluated against all methods in Naive Bayes to decide on best approach.\n", "\n", "#Gaussian\n", "GNB = GaussianNB()\n", "scores = cross_val_score(GNB, X_train, y_train, cv=5)\n", "accuracyGNB = scores.mean()\n", "print(\"Gaussian Naive Bayes Accuracy :\", accuracyGNB)\n", "\n", "#Multinomial\n", "MNB = MultinomialNB()\n", "scores = cross_val_score(MNB, X_train, y_train, cv=5)\n", "accuracyMNB = scores.mean()\n", "print(\"Multinomial Naive Bayes Accuracy :\", accuracyMNB)\n", "\n", "#Bernoulli\n", "BNB = BernoulliNB()\n", "scores = cross_val_score(BNB, X_train, y_train, cv=5)\n", "accuracyBNB = scores.mean()\n", "print(\"Bernoulli Naive Bayes Accuracy :\", accuracyBNB)\n", "\n", "#Plot accuracies corresponding to various naive bayes methods\n", "fig, ax1 = plt.subplots(1, 1, figsize=(8,5))\n", "fig = sns.barplot(y=[accuracyGNB, accuracyMNB, accuracyBNB], x=[\"GaussianNB\", \"MultinomialNB\", \"BernoulliNB\"], ax=ax1)\n", "fig.set(xlabel=\"Naive Bayes Methods\", ylabel=\"Accuracy\")\n", "\n", "#Finally we decide to use Bernoulli Naive Bayes\n", "Accuracy[\"naiveBayes\"] = accuracyBNB"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "0b2e92cf-7fde-4dc5-b937-c97927e44ccb", "_uuid": "88a7f07d0de337d797d551009d70cf861c49c917"}, "source": ["#Method - K Nearest Neighbors\n", "\n", "#Deciding K is very important factor for KNN.\n", "#Since target values are either 0 or 1. We have to set K value at least (possible outcomes + 1)3 \n", "#and maximum square root of number of training samples. We will use cross-validation technique to find optimal value of K.\n", "\n", "#Cross-Validation\n", "accuracy = []\n", "k_value = []\n", "for k in range(3,int(np.sqrt(len(train)))):\n", "    kNN = KNeighborsClassifier(n_neighbors=k)\n", "    scores = cross_val_score(kNN, X_train, y_train, cv=5)\n", "    accuracy.append(scores.mean())\n", "    k_value.append(k)\n", "\n", "#Plot accuracies corresponding to value of K\n", "fig, ax1 = plt.subplots(1, 1, figsize=(15,8))\n", "fig = sns.pointplot(y=accuracy, x=k_value, ax=ax1)\n", "fig.set(xlabel=\"K Value\", ylabel=\"Accuracy\")\n", "\n", "print(\"Maximum Accuracy :\", max(accuracy))\n", "print(\"Value of K corresponding to maximum accuracy :\", k_value[accuracy.index(max(accuracy))])\n", "\n", "#Final value of K must correspond to maximum accuracy.\n", "kNN = KNeighborsClassifier(n_neighbors=k_value[accuracy.index(max(accuracy))])\n", "scores = cross_val_score(kNN, X_train, y_train, cv=5)\n", "accuracy = scores.mean()\n", "print(\"K Nearest Neighbor Accuracy :\", accuracy)\n", "Accuracy[\"kNN\"] = accuracy"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "e90f6a1c-fa61-42bc-aa3a-137bf304a384", "_uuid": "4eaad98ae9ccc816f0f6e03129577942cb0dff1e"}, "source": ["#Method - Logistic Regreesion\n", "\n", "logisticRegression = LogisticRegression()\n", "scores = cross_val_score(logisticRegression, X_train, y_train, cv=5)\n", "accuracy = scores.mean()\n", "print(\"Logistic Regreesion Accuracy :\", accuracy)\n", "Accuracy[\"logisticRegression\"] = accuracy"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "3da3e9a7-b7d8-441f-824f-b2be1a025c56", "_uuid": "030821765d46efee5d75cd4660a79045b18b02de"}, "source": ["#Method - Support Vector Machine\n", "\n", "#We will be evaluating SVM with respect to linear kernel, RBF kernel and Polynomial kernel.\n", "\n", "#Linear Kernel\n", "SVML = svm.SVC(kernel=\"linear\")\n", "scores = cross_val_score(SVML, X_train, y_train, cv=5)\n", "accuracySVML = scores.mean()\n", "print(\"Support Vector Machine Accuracy (Linear Kernel) :\", accuracySVML)\n", "\n", "#RBF Kernel\n", "SVMR = svm.SVC(kernel=\"rbf\")\n", "scores = cross_val_score(SVMR, X_train, y_train, cv=5)\n", "accuracySVMR = scores.mean()\n", "print(\"Support Vector Machine Accuracy (RBF Kernel) :\", accuracySVMR)\n", "\n", "#Polynomial Kernel\n", "SVMP = svm.SVC(kernel=\"poly\")\n", "scores = cross_val_score(SVMP, X_train, y_train, cv=5)\n", "accuracySVMP = scores.mean()\n", "print(\"Support Vector Machine Accuracy (Poly Kernel) :\", accuracySVMP)\n", "\n", "#Plot accuracies corresponding to various naive bayes methods\n", "fig, ax1 = plt.subplots(1, 1, figsize=(8,5))\n", "fig = sns.barplot(y=[accuracySVML, accuracySVMR, accuracySVMP], x=[\"Linear\", \"RBF\", \"Polynomial\"], ax=ax1)\n", "fig.set(xlabel=\"Kernels\", ylabel=\"Accuracy\")\n", "\n", "#Finally we decide to use Polynomial Kernel\n", "Accuracy[\"supportVectorMachine\"] = accuracySVMP"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "d1045f89-68bd-4bae-a2aa-2ba6a454387f", "scrolled": false, "_uuid": "e567d97b8129193bd4a531ed00991331f3b0312a"}, "source": ["#Method - Decision Tree Classifier\n", "\n", "decisionTree = DecisionTreeClassifier()\n", "scores = cross_val_score(decisionTree, X_train, y_train, cv=5)\n", "accuracy = scores.mean()\n", "print(\"Decision Tree Classifier Accuracy :\", accuracy)\n", "Accuracy[\"decisionTree\"] = accuracy\n", "\n", "#Feature Weightage\n", "decisionTree.fit(X_train, y_train)\n", "fig = plt.figure(figsize=(15,8))\n", "fig = sns.barplot(y=list(decisionTree.feature_importances_), x=list(X_train.columns), color=\"orange\")\n", "fig.set(xlabel=\"features\", ylabel=\"Weights\")"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "a65f07d4-c49a-4f1b-919b-41addd2369a7", "_uuid": "46fb9c07a0d90df4814ba6e1dca0ac3e89e3c95a"}, "source": ["#Method - Random Forest Classifier\n", "\n", "randomForest = RandomForestClassifier(n_estimators=100)\n", "scores = cross_val_score(randomForest, X_train, y_train, cv=5)\n", "RFaccuracy = scores.mean()\n", "print(\"Random Forest Classifier Accuracy :\", RFaccuracy)\n", "Accuracy[\"randomForest\"] = RFaccuracy\n", "\n", "#Feature Weightage\n", "randomForest.fit(X_train, y_train)\n", "fig = plt.figure(figsize=(15,8))\n", "fig = sns.barplot(y=list(randomForest.feature_importances_), x=list(X_train.columns), color=\"orange\")\n", "fig.set(xlabel=\"features\", ylabel=\"Weights\")"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "2aae4a09-8dcc-4984-8435-49bac8277b81", "_uuid": "7d42871d94fff2ff6b23318dce244fed39d203ab"}, "source": ["#Method - AdaBoost Classifier\n", "\n", "adaBoost = AdaBoostClassifier(n_estimators=100)\n", "scores = cross_val_score(adaBoost, X_train, y_train, cv=5)\n", "accuracy = scores.mean()\n", "print(\"AdaBoost Classifier Accuracy :\", accuracy)\n", "Accuracy[\"adaBoost\"] = accuracy\n", "\n", "#Feature Weightage\n", "adaBoost.fit(X_train, y_train)\n", "fig = plt.figure(figsize=(15,8))\n", "fig = sns.barplot(y=list(adaBoost.feature_importances_), x=list(X_train.columns), color=\"orange\")\n", "fig.set(xlabel=\"features\", ylabel=\"Weights\")"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "e8829c97-e9d9-4282-99dd-be86bb17c825", "_uuid": "da54b94ad251fb30e07365aed04eab9f931e3dfe"}, "source": ["#Method - Gradient Boosting Classifier\n", "\n", "gradientBoosting = GradientBoostingClassifier(n_estimators=100)\n", "scores = cross_val_score(gradientBoosting, X_train, y_train, cv=5)\n", "accuracy = scores.mean()\n", "print(\"Gradient Boosting Classifier Accuracy :\", accuracy)\n", "Accuracy[\"gradientBoosting\"] = accuracy\n", "\n", "#Feature Weightage\n", "gradientBoosting.fit(X_train, y_train)\n", "fig = plt.figure(figsize=(15,8))\n", "fig = sns.barplot(y=list(gradientBoosting.feature_importances_), x=list(X_train.columns), color=\"orange\")\n", "fig.set(xlabel=\"features\", ylabel=\"Weights\")"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "4f4107dc-e7ff-46b8-b84b-0b9711ce2ff3", "_uuid": "bf21efca68a693377b32a1e3030fc08140cb9a3b"}, "source": ["#Method - Voting Classifier\n", "\n", "#I have tried many combinations for voting classifiers, here is what i observed-\n", "# 1) Both gradient boosting and adaboost has similar weightage for features. Hence, I think adding\n", "#    Only one classifier is fine.\n", "# 2) Naive Baeyes has least accuracy and is not much effective.\n", "# 3) Adding SVM somehow leads to decrease in accuracy(Maybe due to overfitting). Hence, removed.\n", "# 4) Even though decision tree and random forest uses same tree based algorithms, feature weightage is\n", "#    totally different and hence effective.\n", "\n", "votingClasssifier = VotingClassifier(estimators=[('KNN', kNN), ('LR', logisticRegression), (\"DT\", decisionTree), ('RF', randomForest), ('GB', gradientBoosting)], voting=\"hard\")\n", "scores = cross_val_score(votingClasssifier, X_train, y_train, cv=5)\n", "accuracy = scores.mean()\n", "print(\"Voting Classifier Accuracy :\", accuracy)\n", "Accuracy[\"votingClasssifier\"] = accuracy"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "4f29f427-39a3-44d0-8b7a-81e7ba7f4e35", "_uuid": "5c783eb269efc7a2fc71cdb7afd04c746f397739"}, "source": ["print(Accuracy)"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "f92fb1e6-6aec-45a3-8c85-f29e63683260", "_uuid": "4f5341db50b44c3acf47597edc315c5d8b56e5cb"}, "source": ["#Plot accuracies with repect to multiple methods\n", "fig = plt.figure(figsize=(15,8))\n", "fig = sns.pointplot(y=list(Accuracy.values()), x=list(Accuracy.keys()))\n", "fig.set(ylabel=\"Accuracy\", xlabel=\"Maching Learning Methods\")"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "6b887d4d-348b-46bf-8815-cb29fc75e10e", "collapsed": true, "_uuid": "b39b8cc460ba040f63adb52a4b82656cc9db0cc4"}, "source": ["#Create test data\n", "X_test = test.drop(\"PassengerId\", axis=1)\n", "\n", "#Run Voting Classifier for better results\n", "votingClasssifier.fit(X_train, y_train)\n", "y_test = votingClasssifier.predict(X_test)"]}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"_cell_guid": "8f7ffe66-c0ee-4a38-a7f3-82e16f7775ed", "collapsed": true, "_uuid": "52c01a1db02ac3bc838bc60ec18103000300bf0c"}, "source": ["submission = pd.DataFrame({\n", "        \"PassengerId\": test[\"PassengerId\"],\n", "        \"Survived\": y_test\n", "    })\n", "submission.to_csv('submission.csv', index=False)"]}, {"cell_type": "markdown", "metadata": {"_cell_guid": "8c5a530d-23d6-4ed5-ad40-3607303fabec", "_uuid": "3919e3475413afef8535467b9f53aad6c5f3fd5e"}, "source": ["***Final Result against test set - 0.8134* "]}], "nbformat_minor": 1, "nbformat": 4, "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}, "language_info": {"mimetype": "text/x-python", "name": "python", "codemirror_mode": {"version": 3, "name": "ipython"}, "version": "3.6.1", "file_extension": ".py", "pygments_lexer": "ipython3", "nbconvert_exporter": "python"}}}