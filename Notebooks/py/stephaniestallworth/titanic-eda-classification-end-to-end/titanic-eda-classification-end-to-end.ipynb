{"nbformat": 4, "metadata": {"language_info": {"name": "python", "codemirror_mode": {"version": 3, "name": "ipython"}, "file_extension": ".py", "version": "3.6.1", "mimetype": "text/x-python", "pygments_lexer": "ipython3", "nbconvert_exporter": "python"}, "kernelspec": {"name": "python3", "language": "python", "display_name": "Python 3"}}, "nbformat_minor": 1, "cells": [{"metadata": {"_cell_guid": "6793d21d-3c03-44d5-90da-47ad2b34f891", "_uuid": "03858050d08f513ca7436fbc1b3f31628e00d296"}, "cell_type": "markdown", "source": ["# **TITANIC EDA AND CLASSIFICATION **\n", "This kernel consists of three objectives:\n", "1. Perform exploratory analysis  to extract meaningful insight from the data and identify the best features to be used for  modeling.  \n", "\n", "2. Train, test, and refine various classification models to best predict which passengers survived.  The predictive algorithms to be trained are:\n", "    - Logistic Regression  \n", "    - Linear Support Vector Classifier (SVC)  \n", "    - Kernel Support Vector Classifier (Kernel SVC)\n", "    - K-Nearest Neighbors (KNN)\n", "    - Decision Tree \n", "    - Random Forest\n", "3.  Apply the best performing model to the test set for contest submission."]}, {"metadata": {"_cell_guid": "1364327d-2982-459a-a85d-a03d3845f51e", "_uuid": "a5414e25ab52e1f3d9a509ebfcf0eb2c46c3dcb1"}, "cell_type": "markdown", "source": ["### **TRAINING DATA PRE-PROCESSING** \n", "The first step in the machine learning pipeline is to clean and transform the training data into a useable format for analysis and modeling.   \n", "\n", "As such, data pre-processing addresses:\n", "- Assumptions about data shape\n", "- Incorrect data types\n", "- Outliers or errors\n", "- Missing values\n", "- Categorical variables "]}, {"metadata": {"_cell_guid": "a531a985-5663-4bb1-9288-11ee03f70019", "collapsed": true, "_execution_state": "idle", "_uuid": "f99f2ce1482ecf2244dfaac3fc2fa56f94afaaf2"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Import libraries\n", "import numpy as np\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline\n", "import sklearn\n", "import seaborn as sns\n", "import warnings\n", "warnings.filterwarnings('ignore')\n", "plt.rcParams[\"figure.figsize\"] = [10,5]"]}, {"metadata": {"_cell_guid": "3da9587e-c331-464b-85c1-38eaaf616123", "collapsed": true, "_execution_state": "idle", "_uuid": "386fdf4605156e4c83f1f58da6837b61d214ee24"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Read data\n", "train_data = pd.read_csv('../input/train.csv')"]}, {"metadata": {"_cell_guid": "1ffee743-5749-44d3-ba44-86d142b19159", "_uuid": "09bc2c97215d9f8470180850f97e13b8d984971d"}, "cell_type": "markdown", "source": ["**Data Shape**  \n", "After loading the dataset, I examine its shape to get a better sense of the data and the information it contains.  "]}, {"metadata": {"_cell_guid": "f4b943a8-3c76-4374-b6f6-3effe7fafec1", "_execution_state": "idle", "_uuid": "c322866d6c67eb7b79cda1c579f8db4beddec100"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Data shape\n", "print('train data:',train_data.shape)"]}, {"metadata": {"_cell_guid": "532fc41a-6992-44a7-a2cf-fc4b3c4a672d", "_execution_state": "idle", "_uuid": "23ffc78f6db97f2caf626dae0b8c0e4f64986f44"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# View first few rows\n", "train_data.head(3)"]}, {"metadata": {"_cell_guid": "2f9a64dc-6461-43da-8923-c4b27aff4733", "_execution_state": "idle", "_uuid": "356061202e4ed8be1674acd664fd66c02c55e980"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Data Info\n", "train_data.info()"]}, {"metadata": {"_cell_guid": "5e61ecf8-1405-4610-b9ac-7041c9764fc6", "_uuid": "8ce7a919390677204fac9e03363a4141ac8bf2b7"}, "cell_type": "markdown", "source": ["**Missing Data**  \n", "From the entry totals above, there appears to be missing data.  A heatmap will help better visualize what features as missing the most information."]}, {"metadata": {"_cell_guid": "abbd18c9-e271-49e4-8c7c-ddbd237ba3b8", "_execution_state": "idle", "_uuid": "9f5faca2de176273206a60a58fd5b7bd7e176663"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Heatmap\n", "sns.heatmap(train_data.isnull(),yticklabels = False, cbar = False,cmap = 'tab20c_r')\n", "plt.title('Missing Data: Training Set')\n", "plt.show()"]}, {"metadata": {"_cell_guid": "cfb1a51e-7f90-4198-aa12-28ca2671b032", "_uuid": "cbd98e84669f2c524e8c5c5c518e5082cba749e1"}, "cell_type": "markdown", "source": ["The 'Age' variable is missing roughly 20% of its data. This proportion is likely small enough for reasonable replacements using some form of imputation as well (using the knowledge of the other columns to fill in reasonable values).\n", "However, too much data from the 'Cabin' column is missing to do anything useful with it at a basic level. This column may need to be dropped from the data set altogether or change to another feature such as 'Cabin Known: 1 or 0'.  \n", "\n", "We want to fill in missing age data instead of just dropping the missing age data rows. One way to do this is by filling in the mean age of all the passengers (imputation).\n", "However, we can be smarter about this and check the average age by passenger class. \n"]}, {"metadata": {"_cell_guid": "57272498-8e36-4834-9828-fe544015a452", "_execution_state": "idle", "_uuid": "7ec334b048f55cd1db0297f4b2f6dfc943a5c7d8"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["plt.figure(figsize = (10,7))\n", "sns.boxplot(x = 'Pclass', y = 'Age', data = train_data, palette= 'GnBu_d').set_title('Age by Passenger Class')\n", "plt.show()"]}, {"metadata": {"_cell_guid": "174038cd-c170-420b-8c3f-d4bf3a2e9e17", "_uuid": "6e5cc498dd5fa7c73e958e93c30fdc1f48e2cf78"}, "cell_type": "markdown", "source": ["Naturally, the wealthier passengers in the higher classes tend to be older . We'll use these average age values to impute based on Pclass for Age.\n"]}, {"metadata": {"_cell_guid": "a994970d-0f02-4569-9b69-97aa9d35fbcc", "collapsed": true, "_execution_state": "idle", "_uuid": "27b9b892eb06facde863db38007915bb2d56e8a2"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Imputation function\n", "def impute_age(cols):\n", "    Age = cols[0]\n", "    Pclass = cols[1]\n", "    \n", "    if pd.isnull(Age):\n", "\n", "        if Pclass == 1:\n", "            return 37\n", "\n", "        elif Pclass == 2:\n", "            \n", "            return 29\n", "\n", "        else:\n", "            return 24\n", "\n", "    else:\n", "        return Age\n", "    \n", "# Apply the function to the Age column\n", "train_data['Age']=train_data[['Age','Pclass']].apply(impute_age, axis =1 )    "]}, {"metadata": {"_cell_guid": "db8bd3b2-3b82-42cb-bfee-c319fb4650c8", "_uuid": "7a02f0c507e7589f173880585455cd45616c0429"}, "cell_type": "markdown", "source": ["\n", "The Cabin column has too many missing values to do anything useful with, so it would be best to remove it from the data frame altogether."]}, {"metadata": {"_cell_guid": "dbb9de5c-665f-4f46-bec8-e727f974e4b8", "collapsed": true, "_execution_state": "idle", "_uuid": "59d84d7bcf4c24a18f5ad9cdbe4fa04d83c861e6"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Remove Cabin feature\n", "train_data.drop('Cabin', axis = 1, inplace = True)"]}, {"metadata": {"_cell_guid": "41186bdc-ec2c-48d5-9e2d-45ab6920c108", "_uuid": "40b4493b4fdcf01419884863c3c3e47416935129"}, "cell_type": "markdown", "source": ["Since there is only one missing value in Embarked, that observation can just be removed."]}, {"metadata": {"_cell_guid": "c9a39738-4ea7-49d5-b312-01031ab6863e", "collapsed": true, "_execution_state": "idle", "_uuid": "2b8c1506a72f92d86b8edc0c2d9457e90884d851"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Remove rows with missing data\n", "train_data.dropna(inplace = True)"]}, {"metadata": {"_cell_guid": "b0ab6c89-8326-4630-bf56-a5166408c57f", "_uuid": "fa95ccee48ae5599582b9005998a34e1c7330c10"}, "cell_type": "markdown", "source": ["** Data Types**  \n", "Next, I'll need to confirm that the variables are being assigned the correct data type to allow for easier analysis later on."]}, {"metadata": {"_cell_guid": "822d8b59-2768-4ec7-9392-4995f86fbedc", "_uuid": "53cb945c30cfb1604e0d9a11dadf5058211a300d"}, "cell_type": "markdown", "source": ["**Categorical Features**"]}, {"metadata": {"_cell_guid": "932482a7-6ffa-4441-badc-cc7b5c2f3e18", "_execution_state": "idle", "_uuid": "5865230fe5cdc796d2a949a3f09cbc790b7309ac"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Data types\n", "print(train_data.info())\n", "\n", "# Identify non-null objects\n", "print('\\n')\n", "print('Non-Null Objects to Be Converted to Category')\n", "print(train_data.select_dtypes(['object']).columns)"]}, {"metadata": {"_cell_guid": "ad1377ac-4877-4b3f-af51-2547fb2b4f13", "_uuid": "f61491193644a4f3eef0c5dfebf67957f0b8a686"}, "cell_type": "markdown", "source": ["Name and Ticket can be removed from the dataset as these features do not provide additional information about a passenger's liklihood of survival.    \n", "\n", "The remaining non-null objects, Sex and Embarked, will need to be specified as categories for better analysis results downstream.  "]}, {"metadata": {"_cell_guid": "c1eb25e1-9f8a-40dc-a5cd-6bf30b657fc4", "collapsed": true, "_execution_state": "idle", "_uuid": "88790bf7b11192d7c9b17deb992572d55fbceeeb"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Remove unnecessary columns  \n", "train_data.drop(['Name','Ticket'], axis = 1, inplace = True)\n", "\n", "# Convert objects to category data type\n", "objcat = ['Sex','Embarked']\n", "\n", "for colname in objcat:\n", "    train_data[colname] = train_data[colname].astype('category')"]}, {"metadata": {"_cell_guid": "980f0652-74c7-46af-bd86-c9d23685ce37", "_uuid": "a7e3112dd406e4d00e68ce6e8e9b46b80d67670c"}, "cell_type": "markdown", "source": ["**Numeric Features**"]}, {"metadata": {"_cell_guid": "98e330ac-b11a-41d6-987a-b6b8ff217f6c", "_execution_state": "idle", "_uuid": "4673ddf9fd922782060a54d30dff685269cbf4ec"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Numeric summary\n", "train_data.describe().transpose()"]}, {"metadata": {"_cell_guid": "32ee4dbd-e82b-4d8b-9e88-3f0d8391b6e9", "_uuid": "a6e86e6d89e6a061f886d350a509ab979a4e9eea"}, "cell_type": "markdown", "source": ["PassengerId can be removed from the dataset because it does not add any useful information in predicting a passenger's survival.  The remaining variables are the correct data type. "]}, {"metadata": {"_cell_guid": "dd54a358-7423-4092-8b73-fccb845f9902", "collapsed": true, "_execution_state": "idle", "_uuid": "d88a00f612c37434650b0f22220ed145d8fccb02"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Remove PassengerId\n", "train_data.drop('PassengerId', inplace = True, axis = 1)"]}, {"metadata": {"_cell_guid": "ab88987e-230f-4976-b37d-e8a3f3ea62a8", "collapsed": true, "_uuid": "61a33f8e620a381ba078bde4658440c89ca4da92"}, "cell_type": "markdown", "source": ["## **OBJECTIVE 1: EXPLORATORY DATA ANALYSIS**  "]}, {"metadata": {"_cell_guid": "769abf4c-7dd4-4250-91cf-e84e06d28c41", "collapsed": true, "_uuid": "e8d6639841d4fe299ed72e5eec707f110bf4440b"}, "cell_type": "markdown", "source": ["### **Target Variable**\n", "'Survived' is our target as that is the dependent variable we are trying to predict."]}, {"metadata": {"_cell_guid": "3aa5e954-50e2-4da0-9d9c-456d543af255", "_execution_state": "idle", "_uuid": "3310f94bb526b76bc1cdd3f4d208ac8cc1a1aaa3"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Survival Count\n", "print('Target Variable')\n", "print(train_data.groupby(['Survived']).Survived.count())\n", "\n", "# Target Variable Countplot\n", "sns.set_style('darkgrid')\n", "plt.figure(figsize = (10,5))\n", "sns.countplot(train_data['Survived'], alpha =.80, palette= ['grey','lightgreen'])\n", "plt.title('Survivors vs Non-Survivors')\n", "plt.ylabel('# Passengers')\n", "plt.show()\n"]}, {"metadata": {"_cell_guid": "23a4d387-86e7-434e-9d2a-3f9059fd27ea", "collapsed": true, "_uuid": "cf0b4ae6f5dda9c431d57e958fbe012ea8fcaf57"}, "cell_type": "markdown", "source": ["> **Target Variable Insight**    \n", "- The majority of passengers onboard did not survive."]}, {"metadata": {"_cell_guid": "548f558b-9152-4790-b1d7-dc2dcbd0a03a", "_uuid": "6141a6580b73a2f477c71298d1195eeb0dd72616"}, "cell_type": "markdown", "source": ["### **Numeric Features**"]}, {"metadata": {"_cell_guid": "33a0c640-3d81-49be-a277-91739d669278", "_execution_state": "idle", "_uuid": "3003db4b3275f7396765a6ba63933abcc8328151"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Identify numeric features\n", "print('Continuous Variables')\n", "print(train_data[['Age','Fare']].describe().transpose())\n", "print('--'*40)\n", "print('Discrete Variables')\n", "print(train_data.groupby('Pclass').Pclass.count())\n", "print(train_data.groupby('SibSp').SibSp.count())\n", "print(train_data.groupby('Parch').Parch.count())\n", "\n", "# Subplots of Numeric Features\n", "sns.set_style('darkgrid')\n", "fig = plt.figure(figsize = (20,16))\n", "fig.subplots_adjust(hspace = .30)\n", "\n", "ax1 = fig.add_subplot(321)\n", "ax1.hist(train_data['Pclass'], bins = 20, alpha = .50,edgecolor= 'black',color ='teal')\n", "ax1.set_xlabel('Pclass', fontsize = 15)\n", "ax1.set_ylabel('# Passengers',fontsize = 15)\n", "ax1.set_title('Passenger Class',fontsize = 15)\n", "\n", "ax2 = fig.add_subplot(323)\n", "ax2.hist(train_data['Age'], bins = 20, alpha = .50,edgecolor= 'black',color ='teal')\n", "ax2.set_xlabel('Age',fontsize = 15)\n", "ax2.set_ylabel('# Passengers',fontsize = 15)\n", "ax2.set_title('Age of Passengers',fontsize = 15)\n", "\n", "ax3 = fig.add_subplot(325)\n", "ax3.hist(train_data['SibSp'], bins = 20, alpha = .50,edgecolor= 'black',color ='teal')\n", "ax3.set_xlabel('SibSp',fontsize = 15)\n", "ax3.set_ylabel('# Passengers',fontsize = 15)\n", "ax3.set_title('Passengers with Spouses or Siblings',fontsize = 15)\n", "\n", "ax4 = fig.add_subplot(222)\n", "ax4.hist(train_data['Parch'], bins = 20, alpha = .50,edgecolor= 'black',color ='teal')\n", "ax4.set_xlabel('Parch',fontsize = 15)\n", "ax4.set_ylabel('# Passengers',fontsize = 15)\n", "ax4.set_title('Passengers with Children',fontsize = 15)\n", "\n", "ax5 = fig.add_subplot(224)\n", "ax5.hist(train_data['Fare'], bins = 20, alpha = .50,edgecolor= 'black',color ='teal')\n", "ax5.set_xlabel('Fare',fontsize = 15)\n", "ax5.set_ylabel('# Passengers',fontsize = 15)\n", "ax5.set_title('Ticket Fare',fontsize = 15)\n", "\n", "plt.show()"]}, {"metadata": {"_cell_guid": "c92c9e45-ebcb-4c56-8ee1-b72d226fda49", "collapsed": true, "_uuid": "9427ff7880c4f17de645c87a9644ad71e5f3892e"}, "cell_type": "markdown", "source": [">**Numeric Features Insights**    \n", "- The majority of passengers aboard were third class  \n", "- Most were single travelors with no spouses or children\n", "- Passenger age appears to be bi-modal, with a small peak around 5 years of age and a larger peak around 25 years old.   This is an indication that the passenger age on board is distributed towards younger individuals with the median age being 26.\n", "- The bulk of fare prices are under \\$25 with a median fare of \\$14.  However, the data is skewed to the right with outliers up to  \\$500.  "]}, {"metadata": {"_cell_guid": "684dcd7c-76cd-4880-a9cc-3a7bc060ee93", "_uuid": "8319fc48c63e5efd5412da9dd0177cb827643f6a"}, "cell_type": "markdown", "source": ["### **Target vs Numeric Features**"]}, {"metadata": {"_cell_guid": "1194ee3d-07a6-4ceb-aff2-7ae68125d230", "_uuid": "038677ae7a57500b952c254d0f43c76c4b77a314"}, "cell_type": "markdown", "source": ["### **Passenger Class**"]}, {"metadata": {"_cell_guid": "cd8180c6-91b4-40fa-b011-790d822a6c78", "_execution_state": "idle", "_uuid": "e2ab697d610f4af1eece32714fc7ae20ab959ad2"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Passenger class summary\n", "print('Passenger Class Summary')\n", "\n", "print('\\n')\n", "print(train_data.groupby(['Pclass','Survived']).Pclass.count().unstack())\n", "\n", "# Passenger class visualization\n", "pclass = train_data.groupby(['Pclass','Survived']).Pclass.count().unstack()\n", "p1 = pclass.plot(kind = 'bar', stacked = True, \n", "                   title = 'Passengers by Class: Survivors vs Non-Survivors', \n", "                   color = ['grey','lightgreen'], alpha = .70)\n", "p1.set_xlabel('Pclass')\n", "p1.set_ylabel('# Passengers')\n", "p1.legend(['Did Not Survive','Survived'])\n", "plt.show()"]}, {"metadata": {"_cell_guid": "807687f0-12dc-43ff-84a3-b46a13538b9e", "_uuid": "fb298b6075ba82ed3c3f761531cdbf99e188735d"}, "cell_type": "markdown", "source": [">**Passenger Class Insight**  \n", "- The majority of first-class passengers survived.  \n", "- Most of the passengers in second and third class did not survive the sinking.  \n"]}, {"metadata": {"_cell_guid": "170bd643-d732-4190-9890-77ba1a5d87ba", "_uuid": "97b41063255e26acc79ec970827706cf4032f95b"}, "cell_type": "markdown", "source": ["### **Solo Passengers**"]}, {"metadata": {"_cell_guid": "cbe466f7-4432-45ef-a389-25cf60b563fb", "_execution_state": "idle", "_uuid": "7ec0ae1b1c315d22b214808cb037d6d5f838e31c"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# SibSp Summary\n", "print('Passengers with Siblings or Spouse')\n", "print('\\n')\n", "print(train_data.groupby(['SibSp','Survived']).SibSp.count().unstack())\n", "\n", "sibsp = train_data.groupby(['SibSp','Survived']).SibSp.count().unstack()\n", "p2 = sibsp.plot(kind = 'bar', stacked = True,\n", "                   color = ['grey','lightgreen'], alpha = .70)\n", "p2.set_title('Passengers with Siblings or Spouse: Survivors vs Non-Survivors')\n", "p2.set_xlabel('Sibsp')\n", "p2.set_ylabel('# Passengers')\n", "p2.legend(['Did Not Survive','Survived'])\n", "plt.show()"]}, {"metadata": {"_cell_guid": "1c07b780-6421-4c33-b421-723d801d3ac6", "_uuid": "00ba13c202b93b2e51ab0f8c7b14a5ab3e02fae9"}, "cell_type": "markdown", "source": ["### **Passengers with Children**"]}, {"metadata": {"_cell_guid": "75f7ebd3-3312-452f-bab1-18717416f59d", "_execution_state": "idle", "_uuid": "be2d0993946887bde745cc4c09ea22dac095d949"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["print(train_data.groupby(['Parch','Survived']).Parch.count().unstack())\n", "\n", "parch = train_data.groupby(['Parch','Survived']).Parch.count().unstack()\n", "p3 = parch.plot(kind = 'bar', stacked = True,\n", "                   color = ['grey','lightgreen'], alpha = .70)\n", "p3.set_title('Passengers with Children: Survivors vs Non-Survivors')\n", "p3.set_xlabel('Parch')\n", "p3.set_ylabel('# Passengers')\n", "p3.legend(['Did Not Survive','Survived'])\n", "plt.show()"]}, {"metadata": {"_cell_guid": "bc6242a8-b7a2-4ef2-9071-7756da4333c2", "_uuid": "726066ac08475401da35a86d5b481c873e6f652c"}, "cell_type": "markdown", "source": [">**Passengers with Children Insight**\n", "- The majority of passengers aboard did not have children\n", "- Most passengers without children did not survive.   \n", "- Passengers with one or two children survived half of the time "]}, {"metadata": {"_cell_guid": "e0b9f041-f426-4790-9d60-298cee6e3bd0", "_uuid": "62e79ebeb83423970aced79881b025c417253008"}, "cell_type": "markdown", "source": ["### **Ticket Fare and Age of Passengers**"]}, {"metadata": {"_cell_guid": "09c4674b-6526-4c59-8ad1-a6fcfdd1fcb6", "_execution_state": "idle", "_uuid": "b3d7d5142351238c3a5d8673cc534b7d27fb0397"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# titanic.hist(bins=10,figsize=(9,7),grid=False)\n", "# Statistical summary of continuous variables \n", "print('Statistical Summary of Age and Fare')\n", "print('\\n')\n", "print('Did Not Survive')\n", "print(train_data[train_data['Survived']==0][['Age','Fare']].describe().transpose())\n", "print('--'*40)\n", "print('Survived')\n", "print(train_data[train_data['Survived']==1][['Age','Fare']].describe().transpose())\n", "# Subplots of Numeric Features\n", "sns.set_style('darkgrid')\n", "fig = plt.figure(figsize = (16,10))\n", "fig.subplots_adjust(hspace = .30)\n", "\n", "ax1 = fig.add_subplot(221)\n", "ax1.hist(train_data[train_data['Survived'] ==0].Age, bins = 25, label ='Did Not Survive', alpha = .50,edgecolor= 'black',color ='grey')\n", "ax1.hist(train_data[train_data['Survived']==1].Age, bins = 25, label = 'Survive', alpha = .50, edgecolor = 'black',color = 'lightgreen')\n", "ax1.set_title('Passenger Age: Survivors vs Non-Survivors')\n", "ax1.set_xlabel('Age')\n", "ax1.set_ylabel('# Passengers')\n", "ax1.legend(loc = 'upper right')\n", "\n", "ax2 = fig.add_subplot(223)\n", "ax2.hist(train_data[train_data['Survived']==0].Fare, bins = 25, label = 'Did Not Survive', alpha = .50, edgecolor ='black', color = 'grey')\n", "ax2.hist(train_data[train_data['Survived']==1].Fare, bins = 25, label = 'Survive', alpha = .50, edgecolor = 'black',color ='lightgreen')\n", "ax2.set_title('Ticket Fare: Suvivors vs Non-Survivors')\n", "ax2.set_xlabel('Fare')\n", "ax2.set_ylabel('# Passenger')\n", "ax2.legend(loc = 'upper right')\n", "\n", "ax3 = fig.add_subplot(122)\n", "ax3.scatter(x = train_data[train_data['Survived']==0].Age, y = train_data[train_data['Survived']==0].Fare,\n", "                        alpha = .50,edgecolor= 'black',  c = 'grey', s= 75, label = 'Did Not Survive')\n", "ax3.scatter(x = train_data[train_data['Survived']==1].Age, y = train_data[train_data['Survived']==1].Fare,\n", "                        alpha = .50,edgecolors= 'black',  c = 'lightgreen', s= 75, label = 'Survived')\n", "ax3.set_xlabel('Age')\n", "ax3.set_ylabel('Fare')\n", "ax3.set_title('Age of Passengers vs Fare')\n", "ax3.legend()\n", "\n", "plt.show()"]}, {"metadata": {"_cell_guid": "fc9bbfbf-8187-4f2b-b9be-8a5c2b5c7bf2", "_uuid": "dba925b231add85c6435e7c7fd070b21d922b1fa"}, "cell_type": "markdown", "source": [">**Numeric Features Insight**  \n", "- The majority of passengers under 10 years of age survived.     \n", "- Most people that paid over \\$100 for their ticket survived.  \n", "- There were a fewer number of people that survived over the age of 40. "]}, {"metadata": {"_cell_guid": "fe356de0-524c-4f8b-949c-426aee8fbc53", "_uuid": "0ec57733f2d71d87fd5cf3162481164d41263589"}, "cell_type": "markdown", "source": ["### **Categorical Features**"]}, {"metadata": {"_cell_guid": "3ca5e11e-dc99-408f-b181-ed96bbca6ea3", "_execution_state": "idle", "_uuid": "25fb8a0867ee065a3883c81381a92111ac347360"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Identify categorical features\n", "train_data.select_dtypes(['category']).columns"]}, {"metadata": {"_cell_guid": "6ef9fff4-84a2-430a-8551-d566357017d4", "_execution_state": "idle", "_uuid": "d21130d5e2bc686235984cb592972ba62c76c4cb"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Suplots of categorical features v price\n", "sns.set_style('darkgrid')\n", "f, axes = plt.subplots(1,2, figsize = (15,5))\n", "\n", "# Plot [0]\n", "sns.countplot(x = 'Sex', data = train_data, palette = 'GnBu_d', ax = axes[0])\n", "axes[0].set_xlabel('Sex')\n", "axes[0].set_ylabel('# Passengers')\n", "axes[0].set_title('Gender of Passengers')\n", "\n", "# Plot [1]\n", "sns.countplot(x = 'Embarked', data = train_data, palette = 'GnBu_d',ax = axes[1])\n", "axes[1].set_xlabel('Embarked')\n", "axes[1].set_ylabel('# Passengers')\n", "axes[1].set_title('Embarked')\n", "\n", "plt.show()"]}, {"metadata": {"_cell_guid": "8b130716-04e3-495a-969b-251298b0327d", "_uuid": "8c9401120d622f44a361a52f777886c9536bb21a"}, "cell_type": "markdown", "source": [">**Categorical Features Insight**\n", "- Passengers were primarily male\n", "- Most passengers embarked from Southhampton\n"]}, {"metadata": {"_cell_guid": "3512bf4c-7ea6-4518-a829-c1ef682ef3b6", "_uuid": "85b2400194dcea13bbfbea4557e04afd3f0e95cd"}, "cell_type": "markdown", "source": ["### **Target vs Categorical Features**"]}, {"metadata": {"_cell_guid": "caf8e25a-5026-43f5-bfd7-9ad9fbe89744", "_execution_state": "idle", "scrolled": true, "_uuid": "6110d9006d2fcc478b6021d9aefd9b4dc6fab2d9"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Suplots of categorical features v price\n", "sns.set_style('darkgrid')\n", "f, axes = plt.subplots(1,2, figsize = (20,7))\n", "\n", "gender = train_data.groupby(['Sex','Survived']).Sex.count().unstack()\n", "p1 = gender.plot(kind = 'bar', stacked = True, \n", "                   title = 'Gender: Survivers vs Non-Survivors', \n", "                   color = ['grey','lightgreen'], alpha = .70, ax = axes[0])\n", "p1.set_xlabel('Sex')\n", "p1.set_ylabel('# Passengers')\n", "p1.legend(['Did Not Survive','Survived'])\n", "\n", "\n", "embarked = train_data.groupby(['Embarked','Survived']).Embarked.count().unstack()\n", "p2 = embarked.plot(kind = 'bar', stacked = True, \n", "                    title = 'Embarked: Survivers vs Non-Survivors', \n", "                    color = ['grey','lightgreen'], alpha = .70, ax = axes[1])\n", "p2.set_xlabel('Embarked')\n", "p2.set_ylabel('# Passengers')\n", "p2.legend(['Did Not Survive','Survived'])\n", "\n", "plt.show()"]}, {"metadata": {"_cell_guid": "a94dc519-0210-4e41-ad51-7ac7012a83c0", "_uuid": "78975ddae4a1ac84d5638dc4d25e1401fc70dbd4"}, "cell_type": "markdown", "source": [">**Categorical Variable Insights**  \n", "- Most of the female passengers on board survived."]}, {"metadata": {"_cell_guid": "30f8782b-5e75-4f8a-8273-c3b7a9568a02", "_uuid": "10fe26385d363592a2913da3c62be7a5ebbb48e9"}, "cell_type": "markdown", "source": ["### **GETTING MODEL READY**\n", "\n", "Now that we've explored the data, it is time to get these features 'model ready'. Categorial features will need to be converted into 'dummy variables', otherwise a machine learning algorithm will not be able to take in those features as inputs."]}, {"metadata": {"_cell_guid": "78473719-3c51-489a-9ae4-363ddf505fa4", "_execution_state": "idle", "_uuid": "91434b325c8c885b7ab65ea67e2da3408cce606b"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Shape of train data\n", "train_data.shape"]}, {"metadata": {"_cell_guid": "3ee6e87f-a96b-4fc4-b75d-950bcd5c2183", "_execution_state": "idle", "_uuid": "e5521caeaf416bb46985cc1a1efd57ef96acc61b"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Identify categorical features\n", "train_data.select_dtypes(['category']).columns"]}, {"metadata": {"_cell_guid": "d238113d-4d19-4c1d-82ae-90c47e2a5e51", "collapsed": true, "_execution_state": "idle", "_uuid": "db81fb805384c1995ccd6fb9ea2cff88bba0c959"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Convert categorical variables into 'dummy' or indicator variables\n", "sex = pd.get_dummies(train_data['Sex'], drop_first = True) # drop_first prevents multi-collinearity\n", "embarked = pd.get_dummies(train_data['Embarked'], drop_first = True)"]}, {"metadata": {"_cell_guid": "f454ea53-c745-4b67-88c1-79b11adfac2c", "_execution_state": "idle", "_uuid": "7adb7e764ebb1200691e2aae88fd02a1a036cc90"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Add new dummy columns to data frame\n", "train_data = pd.concat([train_data, sex, embarked], axis = 1)\n", "train_data.head(2)"]}, {"metadata": {"_cell_guid": "7ab6ffb3-c4b1-4139-9dab-b93ac73bb053", "_execution_state": "idle", "_uuid": "5ae67ddc0f876d0e6e36d1e2c2787a3f22eea15e"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Drop unecessary columns\n", "train_data.drop(['Sex', 'Embarked'], axis = 1, inplace = True)\n", "\n", "# Shape of train data\n", "print('train_data shape',train_data.shape)\n", "\n", "# Confirm changes\n", "train_data.head()"]}, {"metadata": {"_cell_guid": "72b1ef4d-8cf3-436f-b0cd-d91535097e5d", "_uuid": "8edbe4f181839cc1d94f07e73c02e4f6d454f1c4"}, "cell_type": "markdown", "source": ["\n", ">Now the train data is perfect for a machine learning algorithm:  \n", "- all the data is numeric\n", "- everything is concatenated together"]}, {"metadata": {"_cell_guid": "88f5f1e8-b368-436c-bb1b-6b5142de1f2b", "_uuid": "63477c1eee0ec7bd8213688a0d59d59f9fa1f3de"}, "cell_type": "markdown", "source": ["## **OBJECTIVE 2: MACHINE LEARNING**\n", "Next, I will feed these features into various classification algorithms to determine the best performance using a simple framework: **Split, Fit, Predict, Score It.**"]}, {"metadata": {"_cell_guid": "39119653-123e-4557-a86c-e2fe98a8a20f", "collapsed": true, "_execution_state": "idle", "_uuid": "330997bde139b14782d0892bbe62d538bd738cb5"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Split data to be used in the models\n", "# Create matrix of features\n", "x = train_data.drop('Survived', axis = 1) # grabs everything else but 'Survived'\n", "\n", "# Create target variable\n", "y = train_data['Survived'] # y is the column we're trying to predict\n", "\n", "# Use x and y variables to split the training data into train and test set\n", "from sklearn.model_selection import train_test_split\n", "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = .20, random_state = 101)"]}, {"metadata": {"_cell_guid": "6f62eb54-b804-438f-85b5-263133e1cb82", "_uuid": "340a8b3a7ccc3ae1f4277f38a636078f2530eeb2"}, "cell_type": "markdown", "source": ["### ** LOGISTIC REGRESSION**"]}, {"metadata": {"_cell_guid": "aff974e1-7869-4ed0-b092-df80abc1dbc7", "_execution_state": "idle", "_uuid": "9dde9fd9ec41ccf1ad6db5baf9bf6a6659002bab"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Fit \n", "# Import model\n", "from sklearn.linear_model import LogisticRegression\n", "\n", "# Create instance of model\n", "lreg = LogisticRegression()\n", "\n", "# Pass training data into model\n", "lreg.fit(x_train, y_train)"]}, {"metadata": {"_cell_guid": "fe99fd6f-d2ef-47a2-8685-25166cf642ad", "collapsed": true, "_execution_state": "idle", "_uuid": "41366ef28e3ae4b1698f4fff9f6ebc7e60b4ba56"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Predict\n", "y_pred_lreg = lreg.predict(x_test)"]}, {"metadata": {"_cell_guid": "39815ee3-1b3b-4fd5-b2c4-7b94bb4367e5", "_execution_state": "idle", "_uuid": "769313a3308587825fdd38cd0f68caed4091817a"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Score It\n", "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score,f1_score, precision_score, recall_score\n", "\n", "# Confusion Matrix\n", "print('Logistic Regression')\n", "print('\\n')\n", "print('Confusion Matrix')\n", "print(confusion_matrix(y_test, y_pred_lreg))\n", "print('--'*40)\n", "\n", "# Classification Report\n", "print('Classification Report')\n", "print(classification_report(y_test,y_pred_lreg))\n", "\n", "# Accuracy\n", "print('--'*40)\n", "logreg_accuracy = round(accuracy_score(y_test, y_pred_lreg) * 100,2)\n", "print('Accuracy', logreg_accuracy,'%')"]}, {"metadata": {"_cell_guid": "78852ca4-80db-4103-a84e-796d65780e7c", "collapsed": true, "_uuid": "289d1f2ab87a2fa58d040294a882232ad7a5da30"}, "cell_type": "markdown", "source": ["### **Interpretation**  \n", "**Accuracy**  \n", "80% of the model's predictions are correct.\n", "\n", "**Precision**  \n", "Precision is  measure of how precise the model's predictions are.  When the model predicts a passenger survived, that person actually *did* survive 81% of the time.\n", "\n", "**Recall (Sensitivity)**  \n", "If there a passenger that survived is present in the test set, the model is able to identify (recall) it 81% of the time.    \n", "\n", "**F1 Score**  \n", "F1 Score is the best of both worlds as it is a weighted average of precision and recall. An F1 Score of 80% means that 80% of the time:\n", "\n", "- when the model predicts someone survived, you can be confident that person actually did survive and it is not a false alarm.\n", "- when there is a actual surivior in the dataset, the model is able to detect it\n", "In classification problems where there are more than two labels that apply, accuracy is less intuitive and the F1 Score is a better measure of a model's performance.\n", "\n"]}, {"metadata": {"_cell_guid": "195ed1ba-7978-4b7b-a55a-146247f94acc", "_uuid": "4294b69b621e332621351fa6e0450cb39b04695a"}, "cell_type": "markdown", "source": ["## ** LINEAR SUPPORT VECTOR CLASSIFIER**"]}, {"metadata": {"_cell_guid": "e46d4bd9-ebb3-4564-8e50-a2668f2fe642", "_execution_state": "idle", "_uuid": "9b8b1b5c475433d1bd0574717100b5705ad44235"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Fit\n", "# Import model\n", "from sklearn.svm import SVC\n", "\n", "# Instantiate the model\n", "svc = SVC()\n", "\n", "# Fit the model on training data\n", "svc.fit(x_train, y_train)"]}, {"metadata": {"_cell_guid": "0a3f8714-2bc4-49b0-81ae-34bf03485aa9", "collapsed": true, "_execution_state": "idle", "_uuid": "800509544d242e1e10732b289cb666b79d0124d2"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Predict\n", "y_pred_svc = svc.predict(x_test)"]}, {"metadata": {"_cell_guid": "a14ff1a6-384b-4d86-b323-0ffddb9c3089", "_execution_state": "idle", "_uuid": "8c6ccb66982659136d29cd4febc71d5cba5e13e4"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Score It\n", "print('Support Vector Classifier')\n", "print('\\n')\n", "# Confusion matrix\n", "print('Confusion Matrix')\n", "print(confusion_matrix(y_test, y_pred_svc))\n", "print('--'*40)\n", "\n", "# Classification report\n", "print('Classification Report')\n", "print(classification_report(y_test, y_pred_svc))\n", "\n", "# Accuracy\n", "print('--'*40)\n", "svc_accuracy = round(accuracy_score(y_test, y_pred_svc)*100,2)\n", "print('Accuracy', svc_accuracy,'%')"]}, {"metadata": {"_cell_guid": "b3d3e303-bcf8-4af2-b29a-3c34dc2e5ebd", "_uuid": "2ce7102658c412b686d7a8311f3156733b4c7d55"}, "cell_type": "markdown", "source": ["** SVC Parameter Tuning with GridSearch**  \n", "The SVC did not perform as well as the Logistic Regression, indicating that this model's parameters need to be tuned for better performance. Finding the right parameters (spefically, what C or gamma values to use) is a tricky task. But luckily, we just try different combinations and see what works best. This idea of creating a 'grid' of parameters and just trying out all the possible combinations is called a Gridsearch, this method is common enough that Scikit-learn has this functionality built in with GridSearchCV (CV stands for cross-validation).  \n", "\n", "GridSearchCV takes a dictionary that describes the parameters that should be tried and a model to train. The grid of parameters is defined as a dictionary, where the keys are the parameters and the values are the settings to be tested.       \n", "\n", "**Parameters**  \n", "**C**  \n", "controls the cost of missclassification on the training data.  \n", "High C: low bias (because you penalized the cost of misclassification alot) and high variance.  \n", "Low C: high bias (not penalizing the cost of missclassficiation as much) and low variance.  \n", "\n", "**Gamma**  \n", "Low gamma: means Gaussian with a large variance  \n", "High gamma: high bias and low variance  \n", "\n", "You would treat the GridSearchCV object just like you would a normal model.  "]}, {"metadata": {"_cell_guid": "b5533b82-4dc9-4034-9b68-a87c907bd4b0", "collapsed": true, "_execution_state": "idle", "_uuid": "6b1979bd2da5ddcdfc1bf8cd99e57e2016f5df3f"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Create parameter grid\n", "param_grid = {'C': [0.1,1, 10, 100, 1000], 'gamma': [1,0.1,0.01,0.001,0.0001], 'kernel': ['rbf']}"]}, {"metadata": {"_cell_guid": "6b15cfaf-1c80-45cc-b396-4f9ea70f568e", "_execution_state": "idle", "_uuid": "7fd7f33cb1a66f101485c886b62f6e6f040f62fe"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Fit\n", "# Import\n", "from sklearn.model_selection import GridSearchCV\n", "\n", "# Instantiate grid object\n", "grid = GridSearchCV(SVC(),param_grid, refit = True, verbose = 1)#verbose is the text output describing the process\n", "\n", "# Fit to training data\n", "grid.fit(x_train,y_train)"]}, {"metadata": {"_cell_guid": "b260ec0f-d0f7-436f-b2d0-c35ff6945a25", "_uuid": "fe9023b03d5c9cecd20ccc532e7c53020134beca"}, "cell_type": "markdown", "source": ["'Fitting' here is a bit more involved then usual:\n", "- First, it runs the same loop with cross-validation to find the best parameter combination.   \n", "- Once it has the best combination, it runs fit again on all data passed to fit (without cross-validation) to build a single new model using the best parameter setting.  \n", "\n", "We can then inspect the best parameters found by GridSearchCV in the bestparams attribute, and the best estimator in the best estimator attribute.  "]}, {"metadata": {"_cell_guid": "42670a2d-bebf-4ace-b230-cf565d831081", "_execution_state": "idle", "_uuid": "22de61ffc2e9da9bc1942f148abf6e66442102ca"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Call best_params attribute\n", "print(grid.best_params_)\n", "print('\\n')\n", "# Call best_estimators attribute\n", "print(grid.best_estimator_)"]}, {"metadata": {"_cell_guid": "faead7c9-d440-411e-bce1-2ade8ce98e55", "collapsed": true, "_execution_state": "idle", "_uuid": "8c8494b3da3175f434e6051482ce63a5fa18d1d9"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Predict using best parameters\n", "y_pred_grid = grid.predict(x_test)"]}, {"metadata": {"_cell_guid": "bd5417a4-8869-4137-85ce-f0eb10255342", "_execution_state": "idle", "_uuid": "40236857400efe1aebe4816181673875b8b3040a"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Score It\n", "# Confusion Matrix\n", "print('SVC with GridSearchCV')\n", "print('\\n')\n", "print('Confusion Matrix')\n", "print(confusion_matrix(y_test, y_pred_grid))\n", "print('--'*40)\n", "# Classification Report\n", "print('Classification Report')\n", "print(classification_report(y_test, y_pred_grid))\n", "\n", "# Accuracy\n", "print('--'*40)\n", "svc_grid_accuracy = round(accuracy_score(y_test, y_pred_grid)*100,2)\n", "print('Accuracy',svc_grid_accuracy,'%')"]}, {"metadata": {"_cell_guid": "364b28cd-13d8-4865-b30d-62db4c87a5a6", "_uuid": "668551ae1331a13fdaa1cf2c70fdb1562e09f16e"}, "cell_type": "markdown", "source": ["By adjusting the parameters with GridSearchCV, the model performed much better in precision, recall, and accuracy than the original SVC (an increase of 8%).  However, it still did not outperform the Logistic Regression which is 1% higher in precision."]}, {"metadata": {"_cell_guid": "7e5a47cb-b5b3-4e06-8b6d-08c8f667acd7", "_uuid": "b992049da34c59b8babdf78c51561566a2dae84c"}, "cell_type": "markdown", "source": ["### **KERNEL SUPPORT VECTOR CLASSIFIER**  \n", "There are functions, called kernels, that can take a low dimensional feature space and map it to a very high dimensional space so as to transform a non-linearably separable problem into a seperable one. \n", "\n", "Thus, by finding the best linear separator between the different classes and applying the 'kernel trick' in a high dimensional space, you get a very powerful system to set data sets apart where the division line might be non-linear."]}, {"metadata": {"_cell_guid": "de9ffbc2-c4aa-47cb-853b-042fe8fa48bf", "collapsed": true, "_execution_state": "idle", "_uuid": "13e66e7264c452591ccc0341fc253bb0af61414f"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Feature Scaling\n", "from sklearn.preprocessing import StandardScaler\n", "sc = StandardScaler()\n", "x_train_sc = sc.fit_transform(x_train)\n", "x_test_sc = sc.transform(x_test)"]}, {"metadata": {"_cell_guid": "21a0f8e4-4d39-49da-8456-dfd2e2d7abea", "_execution_state": "idle", "_uuid": "a844d2b0f5856d07b612df5280e7325fee31073f"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Fit\n", "# Import model\n", "from sklearn.svm import SVC\n", "\n", "# Instantiate model object\n", "ksvc= SVC(kernel = 'rbf', random_state = 0)\n", "\n", "# Fit on training data\n", "ksvc.fit(x_train_sc, y_train)"]}, {"metadata": {"_cell_guid": "28dd9133-e442-490b-8acb-0b6facb46a26", "collapsed": true, "_execution_state": "idle", "_uuid": "437099824a912f112f344e5a49424c43ba450a72"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Predict\n", "y_pred_ksvc = ksvc.predict(x_test_sc)"]}, {"metadata": {"_cell_guid": "170a8292-3038-4975-9c50-246765e7b994", "_execution_state": "idle", "_uuid": "eb1df638619599c9cea62ff8fc5b71fa53138f09"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Score it\n", "print('Kernel SVC')\n", "# Confusion Matrix\n", "print('\\n')\n", "print('Confusion Matrix')\n", "print(confusion_matrix(y_test, y_pred_ksvc))\n", "\n", "# Classification Report\n", "print('--'*40)\n", "print('Classification Report')\n", "print(classification_report(y_test, y_pred_ksvc))\n", "\n", "# Accuracy\n", "print('--'*40)\n", "ksvc_accuracy = round(accuracy_score(y_test,y_pred_ksvc)*100,1)\n", "print('Accuracy',ksvc_accuracy,'%')"]}, {"metadata": {"_cell_guid": "f76ca0f1-8a98-4e0e-861f-64ff357afbeb", "_uuid": "6dfa1748caf845b9d8d207024518f156f37ce375"}, "cell_type": "markdown", "source": ["### **K-NEAREST NEIGHBORS**   \n", "\n", "K-Nearest Neighbors (KNN) is used to classify new data points based on \u2018distance\u2019 to known data on a scatter plot with a two step process: \n", "1.  Find K nearest neighbors based on your distance metric\n", "2.  Let them all vote on the classification \n", "\n", "\n", "KNN is different from the previous classifiers in that it requires variables to be standardized before the model is trained.  This is because the KNN classifier predicts the class of a given test observation by identifying the observations nearest to it.  Thus, the scale of the variables matters. \n", "\n", "Any variables that are on a large scale will have a much larger effect on the distance between the observations (and hence, on the KNN classifier)  than variables that are on a small scale. Therefore, everything should be standardized to the same scale when using k-nearest neighbors to classify."]}, {"metadata": {"_cell_guid": "ef8c48b0-a758-4ea4-835a-8dbcec27091a", "_execution_state": "idle", "_uuid": "1db9f1227414e9c2dd56433f2bb3554466193fcb"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Standardize the Variables\n", "\n", "# Import StandardScaler\n", "from sklearn.preprocessing import StandardScaler\n", "\n", "# Create instance of standard scaler\n", "scaler = StandardScaler()\n", "\n", "# Fit scaler object to feature columns\n", "scaler.fit(train_data.drop('Survived', axis = 1)) # Everything but target variable \n", "\n", "# Use scaler object to do a transform columns\n", "scaled_features = scaler.transform(train_data.drop('Survived', axis = 1)) # performs the standardization by centering and scaling\n", "scaled_features"]}, {"metadata": {"_cell_guid": "c239ca77-3425-4572-bf33-c23b4bac033e", "collapsed": true, "_execution_state": "idle", "_uuid": "ff6698ff1be60c9be1784bad138825f069d6d6ca"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Use scaled features variable to re-create a features dataframe\n", "df_feat = pd.DataFrame(scaled_features, columns = train_data.columns[:-1])"]}, {"metadata": {"_cell_guid": "af033b53-41e6-452d-9cf1-0f7183345456", "collapsed": true, "_execution_state": "idle", "_uuid": "d870a80ec9120e6a503be1e2ddb00c45583a6e8a"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Split\n", "# Import\n", "from sklearn.model_selection import train_test_split\n", "\n", "# Create matrix of features\n", "x = df_feat\n", "\n", "# Create target variable\n", "y = train_data['Survived']\n", "\n", "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = .20, random_state = 101)"]}, {"metadata": {"_cell_guid": "84893607-d88e-44db-a119-d764ed3757fb", "_execution_state": "idle", "_uuid": "480585c9e4fe8d533d96d75a2d379775527d1f16"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Fit\n", "# Import model\n", "from sklearn.neighbors import KNeighborsClassifier\n", "\n", "# Create instance of model\n", "knn = KNeighborsClassifier(n_neighbors = 1)\n", "\n", "# Fit to training data\n", "knn.fit(x_train,y_train)"]}, {"metadata": {"_cell_guid": "4b54adc8-0a3a-4e2d-8e22-bc6f108e4070", "collapsed": true, "_execution_state": "idle", "_uuid": "a6d8c5045908f7e3456ce9f87ff05cff1d18cae0"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Predict\n", "y_pred_knn = knn.predict(x_test)"]}, {"metadata": {"_cell_guid": "30166b94-0db2-4772-88ee-5d3031ef31a3", "_execution_state": "idle", "_uuid": "540816bdc3fcfbce4f0f72f5e5a95e60a722b294"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Score it\n", "print('K-Nearest Neighbors (KNN)')\n", "print('k = 1')\n", "print('\\n')\n", "# Confusion Matrix\n", "print('Confusion Matrix')\n", "print(confusion_matrix(y_test,y_pred_knn))\n", "\n", "# Classification Report\n", "print('--'*40)\n", "print('Classification Report')\n", "print(classification_report(y_test, y_pred_knn))\n", "\n", "# Accuracy\n", "print('--'*40)\n", "knn_accuracy = round(accuracy_score(y_test, y_pred_knn)*100,1)\n", "print('Accuracy',knn_accuracy,'%')"]}, {"metadata": {"_cell_guid": "bcb78dcb-437e-42a7-bb0c-0cd782bc968a", "_uuid": "3188fded92ad9d790bedcc50adf06ea1ad10e079"}, "cell_type": "markdown", "source": ["**KNN Parameter Tuning**  \n", "With a precision of 72.5%, the model's performance is accepatable using k = 1, but could be improved by choosing a better k value.  \n", "\n", "The best way to identify the optimal k value is with the 'Elbow' method, which interates many models using different k values and plots their error rates. The k with the lowest error rate is the optimal value we want to use.  "]}, {"metadata": {"_cell_guid": "89e077a6-7caf-4195-aace-1771e27952a9", "_execution_state": "idle", "_uuid": "02e34f41881abb68840be0490a89c318f1b5eaa2"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Function\n", "error_rate = []\n", "\n", "for i in range (1,40):\n", "    knn = KNeighborsClassifier(n_neighbors = i)\n", "    knn.fit(x_train, y_train)\n", "    pred_i = knn.predict(x_test)\n", "    error_rate.append(np.mean(pred_i != y_test))\n", "\n", "# Plot error rate\n", "plt.figure(figsize = (10,6))\n", "plt.plot(range(1,40), error_rate, color = 'blue', linestyle = '--', marker = 'o', \n", "        markerfacecolor = 'green', markersize = 10)\n", "\n", "plt.title('Error Rate vs K Value')\n", "plt.xlabel('K')\n", "plt.ylabel('Error Rate')\n", "plt.show()"]}, {"metadata": {"_cell_guid": "f4374a40-8e73-435b-8cac-e32c69763c17", "collapsed": true, "_uuid": "b3b7d5c22c739e92bebe296b9b28dff7bd9d12b5"}, "cell_type": "markdown", "source": ["\n", "At k=13, the error rate is at it's lowest, making 13 the optimal value for k.  I will retrain and test the model accordingly.  "]}, {"metadata": {"_cell_guid": "cadb273e-6028-4e5a-8b75-d5cc6fbf62d7", "_execution_state": "idle", "_uuid": "8facca5e6218bd0dfd14ec3d18c0c4ba4dc4b3f0"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Fit new KNN\n", "# Create model object\n", "knn = KNeighborsClassifier(n_neighbors = 13)\n", "\n", "# Fit new KNN on training data\n", "knn.fit(x_train, y_train)"]}, {"metadata": {"_cell_guid": "fe8d8455-7325-417e-9ec8-540c200c53c8", "collapsed": true, "_execution_state": "idle", "_uuid": "579312add945aed3959be93ad040898ea60adbce"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Predict new KNN\n", "y_pred_knn_op = knn.predict(x_test)"]}, {"metadata": {"_cell_guid": "9ab9c42e-e2fb-4d84-9c35-c1c60e1cf92a", "_execution_state": "idle", "_uuid": "5e667f2ac32ef805a8785b3449689534d91c3b08"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Score it with new KNN\n", "print('K-Nearest Neighbors(KNN)')\n", "print('k = 13')\n", "\n", "# Confusion Matrix\n", "print('\\n')\n", "print(confusion_matrix(y_test, y_pred_knn_op))\n", "\n", "# Classification Report\n", "print('--'*40)\n", "print('Classfication Report',classification_report(y_test, y_pred_knn_op))\n", "\n", "# Accuracy\n", "print('--'*40)\n", "knn_op_accuracy =round(accuracy_score(y_test, y_pred_knn_op)*100,2)\n", "print('Accuracy',knn_op_accuracy,'%')"]}, {"metadata": {"_cell_guid": "197f79d6-e5e3-4a8a-add9-63bfe9f63927", "collapsed": true, "_uuid": "19f27a9a4102e20d0483305ad934e4b67e1ab079"}, "cell_type": "markdown", "source": [" By using the elbow method to find the optimal k value, the model's accuracy and precision improved to 83% and f1-score jumped to 82%. This performance also beats out that of the other models trained so far (Logistic Regression and SVC)."]}, {"metadata": {"_cell_guid": "a0a2bd14-1827-4078-9a2f-4462944ba4d4", "_uuid": "029d28eef7010127314014e2fb12c1573f6d1722"}, "cell_type": "markdown", "source": ["### ** DECISION TREE**"]}, {"metadata": {"_cell_guid": "f2249a26-0543-4941-8000-ff533207a868", "_execution_state": "idle", "_uuid": "0b5ec4c4b0673c0ce7e34a3c2695bf0556ca7a87"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Fit\n", "# Import model\n", "from sklearn.tree import DecisionTreeClassifier\n", "\n", "# Create model object\n", "dtree = DecisionTreeClassifier()\n", "\n", "# Fit to training sets\n", "dtree.fit(x_train,y_train)"]}, {"metadata": {"_cell_guid": "b819963f-b926-440b-8fd5-4c154e5e1b9c", "collapsed": true, "_execution_state": "idle", "_uuid": "fe7a821d53c1950795bd4c87ff9a35f7df350783"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Predict\n", "y_pred_dtree = dtree.predict(x_test)"]}, {"metadata": {"_cell_guid": "b965a991-d35a-444d-8faa-3c87401a8a62", "_execution_state": "idle", "_uuid": "a97a8906baee7c23e5de2e4a51275b35582f2584"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Score It\n", "print('Decision Tree')\n", "# Confusion Matrix\n", "print('\\n')\n", "print('Confusion Matrix')\n", "print(confusion_matrix(y_test, y_pred_dtree))\n", "\n", "# Classification Report\n", "print('--'*40)\n", "print('Classification Report',classification_report(y_test, y_pred_dtree))\n", "\n", "# Accuracy\n", "print('--'*40)\n", "dtree_accuracy = round(accuracy_score(y_test, y_pred_dtree)*100,2)\n", "print('Accuracy',dtree_accuracy,'%')"]}, {"metadata": {"_cell_guid": "413c0c2d-8b4a-4c4c-bc8a-f08263cd2a33", "_uuid": "bb23d22f71af4213a33c0bee31a0ebc785f32849"}, "cell_type": "markdown", "source": ["With an accuracy of just 79.2%, the decision tree performed much worse than the logistic regression.  In fact, the primary weakness of a single decision tree is that it doesn't tend to have the best predictive accuracy in general. This is partially due to the high variance - different splits in the training data can lead to very different trees. To improve performance, we can create an ensemble of decision trees with bootstrapped samples of the training set (sampling from the training set with replacement) and using random splits on the features.  "]}, {"metadata": {"_cell_guid": "18501a0c-f650-4691-a7da-bfc847d33fe8", "_uuid": "1e0b0027a193e536d65fefb65fbe66a0514ce984"}, "cell_type": "markdown", "source": ["### **RANDOM FOREST**"]}, {"metadata": {"_cell_guid": "a5b5e68a-9bb3-45b7-991d-9da483c2a09a", "_execution_state": "idle", "_uuid": "259d5d6d919f84d89b808d8e0441da2a2631fd78"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Fit\n", "# Import model object\n", "from sklearn.ensemble import RandomForestClassifier\n", "\n", "# Create model object\n", "rfc = RandomForestClassifier(n_estimators = 200)\n", "\n", "# Fit model to training data\n", "rfc.fit(x_train,y_train)"]}, {"metadata": {"_cell_guid": "a39f6c63-d44a-49cd-8476-141ee7ec0c7c", "collapsed": true, "_execution_state": "idle", "_uuid": "74ab96f58d84f435472cc13e1b6f535e339b5120"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Predict\n", "y_pred_rfc = rfc.predict(x_test)"]}, {"metadata": {"_cell_guid": "f6961459-6932-494d-92ff-72eda5140e56", "_execution_state": "idle", "_uuid": "aaaf531ae119e3eb63a0b6b5ed54f71469540533"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Score It\n", "print('Random Forest')\n", "# Confusion matrix\n", "print('\\n')\n", "print('Confusion Matrix')\n", "print(confusion_matrix(y_test, y_pred_rfc))\n", "\n", "# Classification report\n", "print('--'*40)\n", "print('Classification Report')\n", "print(classification_report(y_test, y_pred_rfc))\n", "\n", "# Accuracy\n", "print('--'*40)\n", "rf_accuracy = round(accuracy_score(y_test, y_pred_rfc)*100,2)\n", "print('Accuracy', rf_accuracy,'%')"]}, {"metadata": {"_cell_guid": "3d7a8608-dabf-4974-b614-70532bee733a", "_uuid": "5ab0ae76c2ac6cb66be860a5003b5fedba7ac36c"}, "cell_type": "markdown", "source": [" ### **AND THE WINNER IS...**\n", "K-Nearest Neighbors with K = 13 with the highest overall performance in accuracy, recall, and precision (~ 83% across the board).  The other classification models also performed well, but were a few percentage points lower in their evaluation metrics.  Additional parameter tuning could improve each model's performance even more, but this initial analysis proved to be a good starting point.  "]}, {"metadata": {"_cell_guid": "919842bb-c083-4de5-9167-54ca58aef56c", "_execution_state": "idle", "_uuid": "b7e2302b2fe89635d7c9f45913f73dcdf9da9292"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["models = pd.DataFrame({\n", "     'Model': ['Logistic Regression', 'Linear SVC', 'Kernel SVC', \n", "               'K-Nearest Neighbors', 'Decision Tree', 'Random Forest'],\n", "    'Score': [logreg_accuracy, svc_grid_accuracy, ksvc_accuracy, \n", "               knn_op_accuracy, dtree_accuracy, rf_accuracy]})\n", "models.sort_values(by='Score', ascending=False)"]}, {"metadata": {"_cell_guid": "2992e67c-f750-48d5-8d34-6e9691559ead", "_uuid": "2c750a673dba58d95567f85508643de41143baf5"}, "cell_type": "markdown", "source": [" ## **OBJECTIVE 3: CONTEST SUBMISSION**  \n", "Now that I've determined the best performing models for this data problem, I will apply KNN to the test data for the Kaggle submission.  \n", "\n", "But before I can use the test data, I need to perform the same pre-processing procedures used on the training data above. Again, I'll need to address missing data and encoding categorical variables to the correct data type.\n"]}, {"metadata": {"_cell_guid": "cc21614b-452d-457f-8656-281a4aba6b4a", "_uuid": "6ad5d70114083419c0fd9cb98cd9fb4f16143725"}, "cell_type": "markdown", "source": ["** TEST DATA PRE-PROCESSING**"]}, {"metadata": {"_cell_guid": "7ed9ba65-a4bc-4d41-b732-2c33839fcaa2", "_execution_state": "idle", "_uuid": "cf9f5beb193cf727237b84b32aed99c9fe98141a"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Load test data\n", "test_data = pd.read_csv('../input/test.csv')\n", "\n", "# Test data info\n", "test_data.info()\n", "\n", "# Test data shape\n", "print('shape',test_data.shape)"]}, {"metadata": {"_cell_guid": "328fc1a5-bfe8-4b81-ad8b-5c474727455b", "_execution_state": "idle", "_uuid": "6002d8147b37edaac8abc87521b571e5377cc577"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Heatmap\n", "sns.heatmap(test_data.isnull(),yticklabels = False, cbar = False,cmap = 'tab20c_r')\n", "plt.title('Missing Data Test Set')\n", "plt.show()"]}, {"metadata": {"_cell_guid": "148d1974-e9c9-428c-80dd-a53bf6a51e7d", "_execution_state": "idle", "_uuid": "b0425e7e640c4b5134693e897976bb7fa02b5995"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Missing Data\n", "# Impute Age\n", "def impute_age(cols):\n", "    Age = cols[0]\n", "    Pclass = cols[1]\n", "    \n", "    if pd.isnull(Age):\n", "\n", "        if Pclass == 1:\n", "            return 37\n", "\n", "        elif Pclass == 2:\n", "            \n", "            return 29\n", "\n", "        else:\n", "            return 24\n", "\n", "    else:\n", "        return Age\n", "    \n", "# Apply the function to the Age column\n", "test_data['Age']=test_data[['Age','Pclass']].apply(impute_age, axis =1 )    \n", "\n", "# Drop cabin feature\n", "test_data = test_data.drop(['Cabin'], axis = 1)\n", "\n", "# Impute Fare\n", "test_data['Fare'].fillna(test_data['Fare'].mean(), inplace=True)\n", "\n", "# Confirm changes\n", "test_data.info()"]}, {"metadata": {"_cell_guid": "acd17162-d055-482f-9333-fc312abf3e23", "_execution_state": "idle", "_uuid": "c1914e72a459651ff754a064a4916ae66bfbc412"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Remove unecessary columns\n", "test_data = test_data.drop(['Name','Ticket'],axis = 1)\n", "test_data.columns"]}, {"metadata": {"_cell_guid": "7e1801c5-d306-4aa1-bfa6-bd41bff23cbc", "_execution_state": "idle", "_uuid": "10ec068b11b4f2fafe4a2cf11a92f0667bb0a705"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Identify categorical variables\n", "test_data.select_dtypes(['object']).columns"]}, {"metadata": {"_cell_guid": "9c5953da-8636-40f7-90a6-9125ca22be1a", "collapsed": true, "_execution_state": "idle", "_uuid": "f5010f7fc73a73cc0909ed046427f96f38880699"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Convert categorical variables into 'dummy' or indicator variables\n", "testsex = pd.get_dummies(test_data['Sex'], drop_first = True) # drop_first prevents multi-collinearity\n", "testembarked = pd.get_dummies(test_data['Embarked'], drop_first = True)\n", "\n", "# Add new dummy columns to data frame\n", "test_data = pd.concat([test_data, testsex, testembarked], axis = 1)\n", "\n", "# Drop unecessary columns\n", "test_data.drop(['Sex', 'Embarked'], axis = 1, inplace = True)"]}, {"metadata": {"_cell_guid": "0b6a75f8-8d67-42cd-b3e8-017541ea9806", "_execution_state": "idle", "_uuid": "3c011d39f407a39ea29c658eb73e34a419be92b0"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Test data shape\n", "print(test_data.shape)\n", "\n", "# Confirm changes\n", "test_data.head()"]}, {"metadata": {"_cell_guid": "4b3e0e62-cb49-48f5-945f-5df8baa70616", "_execution_state": "idle", "_uuid": "28088e5fa2fef6f566147fca27400a3daadefbb4"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Split\n", "x_train2 = train_data.drop(\"Survived\", axis=1)\n", "y_train2 = train_data[\"Survived\"]\n", "x_test2  = test_data.drop(\"PassengerId\", axis=1).copy()\n", "print('x_train shape', x_train2.shape)\n", "print('y_train shape',y_train2.shape)\n", "print('x_test shape', x_test2.shape)"]}, {"metadata": {"_cell_guid": "e53edd09-fb6c-4e17-b47e-ac3e5be1909d", "_execution_state": "idle", "_uuid": "100008023da17e021dd1fbe12aef5781e699269d"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Fit new KNN\n", "# Create model object\n", "knn2 = KNeighborsClassifier(n_neighbors = 13)\n", "\n", "# Fit new KNN on training data\n", "knn2.fit(x_train2, y_train2)"]}, {"metadata": {"_cell_guid": "7c682588-6d69-4985-a378-3b2462422966", "collapsed": true, "_execution_state": "idle", "_uuid": "c7344766f8c51cc1442d91f6e566866512750e77"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Predict \n", "y_pred_knn_op2 = knn2.predict(x_test2)"]}, {"metadata": {"_cell_guid": "5a373169-408d-4d09-90ca-0256435e2953", "collapsed": true, "_execution_state": "idle", "_uuid": "5745ea88c61e2b7b6ce6636322cc62fd3234a840"}, "cell_type": "code", "execution_count": null, "outputs": [], "source": ["# Create contest submission\n", "submission = pd.DataFrame({\n", "        \"PassengerId\": test_data[\"PassengerId\"],\n", "        \"Survived\": y_pred_knn_op2\n", "    })\n", "\n", "submission.to_csv('mySubmission.csv', index=False)"]}]}