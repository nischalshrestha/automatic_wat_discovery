{"cells":[{"metadata":{"_uuid":"d42072e74df253e4deb44cfd4ab4fa5142a721d0"},"cell_type":"markdown","source":"# Kaggle Competition | Titanic: Machine Learning From a Disaster\n\n### Steps taken in this Notebook\n1. Data Wrangling\n    * Loading the datasets with Pandas\n    * Visualizing missing data with MissingNo\n    * Checking correlation between survival and individual factors  \n<br> \n2. Feature Engineering\n    * Feature engineering of new columns\n        * Passenger Title\n        * Family Size\n        * Travelling Alone?  \n<br> \n3. Imputing Missing Data Values\n    * Randomizing the two missing Embarked values\n    * Imputing the missing fare value using the average fare paid by each Pclass\n    * Testing various ML Regression algorithms to predict age\n        * Linear Regression\n        * Bayesian Ridge\n        * Multilayer Perceptron (MLP)\n        * Decision Tree\n        * Bagging\n    * Using Multilayer Perceptron (MLP) to predict the missing age values\n<br> <br>\n4. Predict Survival\n    * Test various ML Classifier algorithms to predict Survival\n        * Logistic Regression\n        * Support Vector \n        * Decision Tree \n        * Random Forest \n        * Adaptive Boosting \n        * Multilayer Perceptron (MLP) \n        * K-Nearest Neighbours (KNN) \n\n    * Using MLP to predict Survival\n    * Formatting output for submission\n\n<br> \n\n### Required Libraries\n* Numpy\n* Pandas\n* Missingno\n* Seaborn\n* Matplotlib\n* Random\n* SciKit-Learn\n"},{"metadata":{"_uuid":"571f685ee8bcb1e64a2ea6eef1f4a9d6713d53ed"},"cell_type":"markdown","source":"## Data Wrangling"},{"metadata":{"_uuid":"bde0f6a110db487bc1296035dc98fbc0563b8379"},"cell_type":"markdown","source":"#### Loading the datasets with Pandas"},{"metadata":{"trusted":true,"_uuid":"945d072bb7ce1ac46c004c605654490d34ac59f4","collapsed":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport random\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import ks_2samp, randint\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.neural_network import MLPRegressor, MLPClassifier\nfrom sklearn.linear_model import LinearRegression, BayesianRidge, LogisticRegression\nfrom sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\nfrom sklearn.ensemble import BaggingRegressor, RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\n\ntrain_dat = pd.read_csv('../input/train.csv')\ntest_dat = pd.read_csv('../input/test.csv')\nall_dat = [train_dat, test_dat]","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"af9f1225fde1b25a695c53454af8e3591b5a5101"},"cell_type":"markdown","source":"##### Training Dataset"},{"metadata":{"trusted":true,"_uuid":"44ab2f4242ebdda8361afdbc962caeb770b5ac6f"},"cell_type":"code","source":"display(train_dat.head(3))\ndisplay(train_dat.info())\ndisplay(train_dat.describe())","execution_count":4,"outputs":[]},{"metadata":{"_uuid":"5b2a9a20bc82b8dce021e506895b714cddff4951"},"cell_type":"markdown","source":"##### Testing Dataset"},{"metadata":{"trusted":false,"_uuid":"f4f8f2795239aa3543be04de8d4e19dbf8fb118c","collapsed":true},"cell_type":"code","source":"display(test_dat.head(3))\ndisplay(test_dat.info())\ndisplay(test_dat.describe())","execution_count":652,"outputs":[]},{"metadata":{"_uuid":"822126a807c3dfa76a7c16020bc8ea9347e4a7df"},"cell_type":"markdown","source":"#### Let's examine the data briefly:\nThe Name, Sex, Ticket, Cabin, and Embarked are listed as strings\nThe remaining lists (PassengerId, Age, SibSp, Parch, and Fare) are reported as integers, but some of them represent categories of values. These nominal values must be converted to dummy variables when creating models\n\nMany of the columns have some missing values, and are difficult to conceptualize when reported in a table form (using `info()`). We will use the `missingno` package to make it easier to visualize where the missing values lie, and what proportions are the data are missing"},{"metadata":{"_uuid":"7c4683826159903918a2bb7a2a97e7bac18a6e7f"},"cell_type":"markdown","source":"### Visualizing missing data with missingno"},{"metadata":{"trusted":false,"_uuid":"aac74996d6d2e7ef70c08676307efa96e5a26470","collapsed":true},"cell_type":"code","source":"import missingno as msno\ndisplay(msno.bar(train_dat))\ndisplay(msno.bar(test_dat))\ntrain_dat.drop(['Cabin', 'Ticket'], axis=1, inplace=True)\ntest_dat.drop(['Cabin', 'Ticket'], axis=1, inplace=True)","execution_count":653,"outputs":[]},{"metadata":{"_uuid":"97f61bde18cc46877d496113e00508fd00e6f23f"},"cell_type":"markdown","source":"In the Cabin column, we only have approximately 20% of values present in both datasets. There are too many missing values to attempt to impute the values. As such, we will drop it from the analysis\n\nThe Age column is missing approximately 20% of the values, but there are enough values present to impute the missing values. We will create an entire machine learning regression model for the missing age values later on\n\nThe training dataset contains two missing values for the Embarked data. Since there are only three possible options, we will randomly select values to fill the missing values\n\nThe test dataset contains one missing value for the Fare, which we will impute later based on the Pclass\n\n"},{"metadata":{"_uuid":"bdd2e6e223f0e89d3e9982933107428066bd783d"},"cell_type":"markdown","source":"### Checking correlation between survival and categorical factors"},{"metadata":{"trusted":false,"_uuid":"0d0d6a4f29c92a159a2434efbb595887b9fea7b2","collapsed":true},"cell_type":"code","source":"train_dat.corr()","execution_count":654,"outputs":[]},{"metadata":{"_uuid":"2689d92f55f06c80516eceefc67cb2cda1a82643"},"cell_type":"markdown","source":"#### Pclass"},{"metadata":{"trusted":false,"_uuid":"e3815129c75dd8802a3528a7a2a8c93b2b810c8d","collapsed":true},"cell_type":"code","source":"train_dat[['Pclass', 'Survived']].groupby('Pclass').mean()","execution_count":655,"outputs":[]},{"metadata":{"_uuid":"5c72acd8ed07f09b7fe442ca986a37ba456b132c"},"cell_type":"markdown","source":"#### Sex"},{"metadata":{"trusted":false,"_uuid":"900f70ca92572071bafdbdf017e37ae1ee1acf4d","collapsed":true},"cell_type":"code","source":"train_dat[['Sex', 'Survived']].groupby('Sex').mean()","execution_count":656,"outputs":[]},{"metadata":{"_uuid":"9f64c5b457cc5b0c59502d2b41d820876908244e"},"cell_type":"markdown","source":"#### Age"},{"metadata":{"trusted":false,"_uuid":"f7a693e5ffe639bc200c5b2b7d618bf742f9cc8e","collapsed":true},"cell_type":"code","source":"age_survived = train_dat.loc[train_dat['Survived'] == 1]['Age'].dropna()\nage_dead = train_dat.loc[train_dat['Survived'] == 0]['Age'].dropna()\n\nfig = sns.kdeplot(age_survived, label='Survived', clip=[0,80])\nfig.set(xlim=(0, 80))\nsns.kdeplot(age_dead, label='Did Not Survive')\n\nks_2samp(age_survived, age_dead)","execution_count":657,"outputs":[]},{"metadata":{"_uuid":"9b073ce3d69c999dbcf922a1318e45bca08f266b"},"cell_type":"markdown","source":"#### Sibsp"},{"metadata":{"trusted":false,"_uuid":"1e88870dc8951f91681537d796f79f7ecf9917ad","collapsed":true},"cell_type":"code","source":"sns.barplot(train_dat['SibSp'], train_dat['Survived'])\ndisplay(train_dat[['Parch', 'Survived']].groupby('Parch').mean())","execution_count":658,"outputs":[]},{"metadata":{"_uuid":"76a5d776fe8355f958ae8343af3efea71547a1e4"},"cell_type":"markdown","source":"#### Parch"},{"metadata":{"trusted":false,"_uuid":"7e1c1101e2d93b0fb5dee1d9d754f1a4e2ba26bd","collapsed":true},"cell_type":"code","source":"sns.barplot(train_dat['Parch'], train_dat['Survived'])\ndisplay(train_dat[['Parch', 'Survived']].groupby('Parch').mean())","execution_count":659,"outputs":[]},{"metadata":{"_uuid":"4c267fd89f258e21b77f94af1cb14b6fb9d0ab5b"},"cell_type":"markdown","source":"#### Fare"},{"metadata":{"trusted":false,"_uuid":"30d9596be4f682a00679bfe126447654f95a1c9b","collapsed":true},"cell_type":"code","source":"fare_survived = train_dat.loc[train_dat['Survived'] == 1]['Fare'].dropna()\nfare_dead = train_dat.loc[train_dat['Survived'] == 0]['Fare'].dropna()\n\nfig = sns.kdeplot(fare_survived, label='Survived', clip=[0,80])\nfig.set(xlim=(0, 80))\nsns.kdeplot(fare_dead, label='Did Not Survive')\nks_2samp(fare_survived, fare_dead)","execution_count":660,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3b3811f271e293f5fafdcdff7199b505662ce764","collapsed":true},"cell_type":"code","source":"fig = plt.figure(figsize=(12,8))\nsns.boxplot(train_dat['Pclass'], train_dat['Fare'])","execution_count":661,"outputs":[]},{"metadata":{"_uuid":"b5072023b0c0253c8b7e99a9a6f60e5c3eb9b186"},"cell_type":"markdown","source":"## Feature Engineering"},{"metadata":{"_uuid":"8eb7839cf14ba6fe61855490946721f5dcf39e4f"},"cell_type":"markdown","source":"#### Extract Passenger Title\nWe will attempt to extract titles from the name"},{"metadata":{"trusted":false,"_uuid":"ad296cbfdc4626a48d03af7d463ddb38da3f8be2","collapsed":true},"cell_type":"code","source":"import re as re\ndef title_extract_function(string):\n    title = re.search('([A-Za-z]+)\\.', string)[0]\n    return title\n\nfor df in all_dat:\n    df['Title'] = df['Name'].apply(title_extract_function)\n\ndisplay(pd.crosstab(all_dat[0]['Title'], all_dat[0]['Sex']))\ndisplay(pd.crosstab(all_dat[1]['Title'], all_dat[1]['Sex']))\n","execution_count":662,"outputs":[]},{"metadata":{"_uuid":"3eaef183d31811d03918b18cc0d7d93cc22b9e81"},"cell_type":"markdown","source":"Some titles are held by a select few, and will be grouped into an \"Other\" category"},{"metadata":{"trusted":false,"_uuid":"32792732bf124da3216991c1f46453db4ea1e31d","collapsed":true},"cell_type":"code","source":"for df in all_dat:\n    df['Title'] = df['Title'].replace(['Mlle.', \"Ms.\"], 'Miss.')\n    df['Title'] = df['Title'].replace(['Mme.'], 'Mrs.')\n    df['Title'] = df['Title'].replace(['Capt.','Col.','Countess.','Don.', 'Dona.','Dr.','Jonkheer.','Lady.', 'Major.', 'Rev.',\n                                       'Sir.'],'Other')\n\ntrain_dat[['Title', 'Survived']].groupby('Title').mean()","execution_count":663,"outputs":[]},{"metadata":{"_uuid":"efdb452d8ce595aad42ea39f4f9685936c26cff3"},"cell_type":"markdown","source":"#### Family Size"},{"metadata":{"trusted":false,"_uuid":"59d5ff0843a65a651e5451ebb8055e1789c2c6f5","collapsed":true},"cell_type":"code","source":"for df in all_dat:\n    df['Family Size'] = df['Parch'] + df['SibSp'] +1\n\ntrain_dat[['Family Size', 'Survived']].groupby('Family Size').mean()","execution_count":664,"outputs":[]},{"metadata":{"_uuid":"ecaac75a8c8e9fe3cbea570e58333bd65054a6bb"},"cell_type":"markdown","source":"#### Travelling Alone?"},{"metadata":{"trusted":false,"_uuid":"addf6f63c6b6da1c6cc6dc874f3e0ddeb9afacd1","collapsed":true},"cell_type":"code","source":"for df in all_dat:\n    df['Alone'] = 0\n    df.loc[df['Family Size'] == 1, 'Alone'] = 1\ntrain_dat[['Alone', 'Survived']].groupby('Alone').mean()","execution_count":665,"outputs":[]},{"metadata":{"_uuid":"769735a7ae5e0071fc5be0337e1e605b6e85df4c"},"cell_type":"markdown","source":"### Imputing Missing Data Values"},{"metadata":{"_uuid":"96e4e6b22043300be608df2c2157a31cab5d32fe"},"cell_type":"markdown","source":"#### Embarked"},{"metadata":{"trusted":false,"_uuid":"7b7b7fc8d5e5c966cdeeb956e8500dccdec07a0e","collapsed":true},"cell_type":"code","source":"random.seed(1234)\nwhile train_dat['Embarked'].isna().sum() > 0:\n    train_dat['Embarked'].fillna(random.randint(1,3), limit=1, inplace=True)\ntrain_dat['Embarked'].replace({1:\"S\", 2:\"C\", 3:\"Q\"}, inplace=True)\n\ntrain_dat[['Embarked', 'Survived']].groupby('Embarked').mean()","execution_count":666,"outputs":[]},{"metadata":{"_uuid":"fec4c2478cb0f456e4cd5dfb0df7d81147ba34a7"},"cell_type":"markdown","source":"#### Fare Class"},{"metadata":{"_uuid":"34dabece639329350ab38d22ae4ab39c1430db8e"},"cell_type":"markdown","source":"In order to fill in the missing Fare value, we use the most correlated factor(s)"},{"metadata":{"trusted":false,"_uuid":"260501fc1cd31676a61e8992191519af0eb70382","collapsed":true},"cell_type":"code","source":"Fare_corr = pd.DataFrame(train_dat.corr()['Fare'].drop(['Fare'],axis=0))\nFare_corr.reindex(Fare_corr.Fare.abs().sort_values(inplace=False, ascending=False).index)","execution_count":667,"outputs":[]},{"metadata":{"_uuid":"00ed75a9f432bfb7fb86994f665c0c0b444f414c"},"cell_type":"markdown","source":"The most significant factor that is correlated with Fare is the **Pclass**. We will impute the missing fare value based average value of the passenger's class"},{"metadata":{"trusted":false,"_uuid":"3e8d45b5e5f47f397456fde693ba6fbc8c89a189","collapsed":true},"cell_type":"code","source":"sns.barplot(x=train_dat['Pclass'], y=train_dat['Fare'])\n\naverage_fare_by_class = test_dat.groupby(by=['Pclass'], as_index=False)['Fare'].mean()\n\nif (test_dat.loc[test_dat['Fare'].isnull(),'Pclass'] == 3).values[0] == True:\n    test_dat.loc[test_dat['Fare'].isnull(),'Fare'] = average_fare_by_class[average_fare_by_class['Pclass'] == 1]['Fare'][0]\nelif (test_dat.loc[test_dat['Fare'].isnull(),'Pclass'] == 2).values[0] == True:\n    test_dat.loc[test_dat['Fare'].isnull(),'Fare'] = average_fare_by_class[average_fare_by_class['Pclass'] == 2]['Fare'][0]\nelse:\n    test_dat.loc[test_dat['Fare'].isnull(),'Fare'] = average_fare_by_class[average_fare_by_class['Pclass'] == 3]['Fare'][0]","execution_count":668,"outputs":[]},{"metadata":{"_uuid":"c4e82523c06b42922c6a14019839c6991af67f0e"},"cell_type":"markdown","source":"Now that we have filled in the missing fare value, we can cut the range of Fares into distinct categorical groups. An easy way to visualize the distribution is through a violin plot."},{"metadata":{"trusted":false,"_uuid":"2e0ee4ad01952d039d67f6c14aa70d3a3cba028d","collapsed":true},"cell_type":"code","source":"sns.violinplot(train_dat['Fare'])\nmax_fare = train_dat['Fare'].max()\nplt.plot([60, 60], [-1, 1], linewidth=2)\nplt.plot([110, 110], [-1, 1], linewidth=2)\nplt.plot([180, 180], [-1, 1], linewidth=2)\nplt.plot([max_fare, max_fare], [-1, 1], linewidth=2)","execution_count":669,"outputs":[]},{"metadata":{"_uuid":"ff81f9f2f8def1d0e67f23b9176b907e10a784fd"},"cell_type":"markdown","source":"We can use the plot to divide the Fares into four groups:\n1. Fare Type 1 = \\$0 - \\$60    \n2. Fare Type 2 = \\$61 - \\$110    \n3. Fare Type 3 = \\$111 - \\$180    \n4. Fare Type 4 = \\$181+\n\nNext, we need to test if the correlation between **Fare Type** and **Survival**"},{"metadata":{"trusted":false,"_uuid":"c412944abb4ae5fdb40cd73c01f24cac268f5a0c","collapsed":true},"cell_type":"code","source":"for df in all_dat:\n    df['Fare Type'] = pd.cut(df['Fare'], [0, 60, 110,180, 1000], labels=['1', '2', '3', \"4\"])\n\ntrain_dat[['Fare Type', 'Survived']].groupby('Fare Type').mean()","execution_count":670,"outputs":[]},{"metadata":{"_uuid":"f6cb6540aa0c69134f7d2154ffda9a88bc0249c0"},"cell_type":"markdown","source":"### Predicting the missing age values "},{"metadata":{"trusted":false,"_uuid":"684695dc6343870d2c4dd5a1b204c5e27561f4db","collapsed":true},"cell_type":"code","source":"display(all_dat[0].head(2))\ndisplay(all_dat[1].head(2))","execution_count":671,"outputs":[]},{"metadata":{"_uuid":"22335a6e85197e8ab45583175a0fa76b8106f402"},"cell_type":"markdown","source":"#### Convert Categorical Data into Dummy Variables"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"a9b5f12b088d12c0dd2b13901eeeb247d52b72c4"},"cell_type":"code","source":"categorical_df = []\nfor df in all_dat:\n    sex = pd.get_dummies(df['Sex'],drop_first=True)\n    embark = pd.get_dummies(df['Embarked'],drop_first=True)\n    title = pd.get_dummies(df['Title'], drop_first=True)\n    fare = pd.get_dummies(df['Fare Type'], drop_first=True)\n    dummies = pd.concat([sex, embark, title, fare], axis=1)\n    df2 = df.drop(['Sex','Embarked','Name','Fare','Fare Type','Title'], axis=1, inplace=True)\n    df3 = pd.concat([df, sex, embark, title, fare], axis=1)\n    categorical_df.append(df3)","execution_count":672,"outputs":[]},{"metadata":{"_uuid":"0a9c7259d65db443a56e1159ba781e7dc23e92e3"},"cell_type":"markdown","source":"#### Separate the missing data from the complete data"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"85054ae41ad2ff82771cd04244c60458b6663f2a"},"cell_type":"code","source":"missing_age = categorical_df[0][pd.isnull(categorical_df[0]).any(axis=1)].drop(['PassengerId'], axis=1)\ncomplete_age = categorical_df[0].dropna().drop(['PassengerId'], axis=1)","execution_count":695,"outputs":[]},{"metadata":{"_uuid":"231c59e56ca0ca10ddb4a331968b6b84417a9928"},"cell_type":"markdown","source":"#### Find the optimal machine learning regression algorithm\nSince we are predicting the numeric values of age, we calculated the accuracy of the model using the RMSE (root-mean-square error) and the MAE (mean absolute error).\n\nThe ML regression algorithms tested are:\n    1. Linear Regression\n    2. Bayesian Ridge\n    3. Multilayer Perceptron (MLP)\n    4. Decision Tree\n    5. Bagging Regressor\n\nWe repeated the testing of each algorithm 1,000 times with differen splits for the training and testing splits. We used the resulting averages of RMSE and MAE to determine the best algorithm to use to predict the missing age values. **The resulting algorithm with the lowest RMSE/MAE values will be used**"},{"metadata":{"trusted":false,"_uuid":"d42719db1e23d25140f953e521ff133bf4a05054","collapsed":true},"cell_type":"code","source":"total_RMSE = pd.DataFrame(columns=[\"LM RMSE\", \"BRR RMSE\", \"NNR RMSE\", \"DTR RMSE\", \"BR RMSE\"])\ntotal_MAE = pd.DataFrame(columns=[\"LM MAE\", \"BRR MAE\", \"NNR MAE\", \"DTR MAE\", \"BR MAE\"])\n\nfor i in range(0, 100):\n    x = complete_age.drop('Age',axis=1)\n    y = complete_age['Age']\n    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.30)\n    scaler = StandardScaler()  \n    scaler.fit(x_train)\n    x_train = scaler.transform(x_train)  \n    x_test = scaler.transform(x_test)\n    \n    lm = LinearRegression()\n    lm.fit(x_train, y_train)\n    lm_pred = lm.predict(x_test)\n\n    brr = BayesianRidge()\n    brr.fit(x_train, y_train)\n    brr_pred = brr.predict(x_test)\n\n    nnr = MLPRegressor(hidden_layer_sizes=(100,), activation='relu', solver='adam',    alpha=0.001,batch_size='auto',\n                   learning_rate='constant', learning_rate_init=0.01, power_t=0.5, max_iter=1000, shuffle=True,\n                   random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9,\n                   nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999,\n                   epsilon=1e-08)\n    nnr.fit(x_train, y_train)\n    nnr_pred = nnr.predict(x_test)\n\n    dtr = DecisionTreeRegressor()\n    dtr.fit(x_train,y_train)\n    dtr_pred = dtr.predict(x_test)\n\n    br = BaggingRegressor()\n    br.fit(x_train, y_train)\n    br_pred = br.predict(x_test)\n\n    RMSEs = [mean_squared_error(y_test, lm_pred), mean_squared_error(y_test, brr_pred), mean_squared_error(y_test, nnr_pred),\n            mean_squared_error(y_test, dtr_pred), mean_squared_error(y_test, br_pred)]\n    total_RMSE.loc[i] = RMSEs\n    MAEs = [mean_absolute_error(y_test, lm_pred), mean_absolute_error(y_test, brr_pred), mean_absolute_error(y_test, nnr_pred),\n            mean_absolute_error(y_test, dtr_pred), mean_absolute_error(y_test, br_pred)]\n    total_MAE.loc[i] = MAEs\n\naverage_RMSE = total_RMSE.mean(axis=0).values\naverage_MAE = total_MAE.mean(axis=0).values\n\nAge_ML_Summary = pd.DataFrame(average_RMSE, columns=[\"RMSE\"], index=['Linear Reg', \"Bayesian Ridge\", 'MLP', \n                                                                     'Decision Tree', 'Bagging Reg'])\nAge_ML_Summary['MAE'] = average_MAE\nAge_ML_Summary.transpose()","execution_count":698,"outputs":[]},{"metadata":{"_uuid":"9ae4ec166e5c876842bed4ed0a5d28c41cabbc9f"},"cell_type":"markdown","source":"The Multilayer Perceptron algorithm produced the lowest RMSE and MAE values, and will be used to predict the missing age values. But first, we need to tune the hyperparameters"},{"metadata":{"_uuid":"8a54462a388d3e874dd946d54d9a1f0e075e263a"},"cell_type":"markdown","source":"### Using Multilayer Perceptron (MLP) to predict the missing age values"},{"metadata":{"_uuid":"a2d07cc74148e79b9a5ad6e40aab863b579cc5ae"},"cell_type":"markdown","source":"#### Predict missing age values using MLP"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"8d4b542cc73b968b110bf8d54f6471d0c24bda5c"},"cell_type":"code","source":"def predict_age(df):\n    if (df['PassengerId'].max()== 891):\n        df = df.drop('Survived', axis=1)\n    missing_age = df[pd.isnull(df).any(axis=1)]\n    complete_age = df.dropna()\n    \n    x_train = complete_age.drop(['Age', 'PassengerId'],axis=1)\n    y_train = complete_age['Age']\n    x_test = missing_age.drop(['Age', 'PassengerId'], axis=1)\n    \n    scaler = StandardScaler()  \n    scaler.fit(x_train)\n    x_train = scaler.transform(x_train)  \n    x_test = scaler.transform(x_test)\n    \n    nnr = MLPRegressor(hidden_layer_sizes=(100,), activation='relu', solver='adam', alpha=0.001,batch_size='auto',\n                   learning_rate='constant', learning_rate_init=0.01, power_t=0.5, max_iter=1000, shuffle=True,\n                   random_state=1234, tol=0.0001, verbose=False, warm_start=False, momentum=0.9,\n                   nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999,\n                   epsilon=1e-08)\n    nnr.fit(x_train, y_train)\n    predicted_age = nnr.predict(x_test)\n    \n    missing_age = df[pd.isnull(df).any(axis=1)].drop('Age',axis=1)\n    age_df = missing_age[['PassengerId']]\n    age_df = age_df.assign(Age = predicted_age)\n    age_df.sort_values(by=['PassengerId'])\n    return age_df\n\ncomplete_data = []\nfor df in categorical_df:\n    pred_age = predict_age(df)\n    complete_df = df.combine_first(pred_age)\n    complete_data.append(complete_df)","execution_count":700,"outputs":[]},{"metadata":{"_uuid":"be40125d32a1362a553412b4aae5328e31a831bb"},"cell_type":"markdown","source":"### Predict Survival"},{"metadata":{"_uuid":"408a8646dc3a71055537fd0439b1826205692807"},"cell_type":"markdown","source":"#### Finding the optimal machine learning classifier algorithm"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"70b341212a70c4c73bc9135c0ebc6304ea785b75"},"cell_type":"code","source":"x = complete_age.drop('Survived',axis=1)\ny = complete_age['Survived']\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.30)\nscaler = StandardScaler()  \nscaler.fit(x_train)\nx_train = scaler.transform(x_train)  \nx_test = scaler.transform(x_test)","execution_count":704,"outputs":[]},{"metadata":{"_uuid":"4be7f9a31d7b2c4f51bcd55590ea2aa6133bcca6"},"cell_type":"markdown","source":"#### Randomized Search of Hyperparameters"},{"metadata":{"trusted":false,"_uuid":"3e6f771e993c4027cb654fc24ba17da942195220","collapsed":true},"cell_type":"code","source":"#import timeit\n#start = timeit.default_timer()\n\ntotal_accuracy = pd.DataFrame(columns=[\"Logistic Regression\", \"SVC\", \"Decision Tree\", \n                                      \"Random Forest\", \"Adaboost\", \"MLP\", \"Kneighbours\"])\n\nfor i in range(0, 5):\n    x = complete_age.drop('Survived',axis=1)\n    y = complete_age['Survived']\n    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.30)\n    scaler = StandardScaler()  \n    scaler.fit(x_train)\n    x_train = scaler.transform(x_train)  \n    x_test = scaler.transform(x_test)\n    \n    params = {'C': np.arange(0.001, 100, 0.5)}\n    logreg_grid = RandomizedSearchCV(LogisticRegression(), params)\n    logreg_grid.fit(x_train, y_train)\n    logreg_pred = logreg_grid.predict(x_test)\n    \n    params = {'C': np.arange(0.1, 5, 0.1), 'gamma': np.arange(0.00001, 1, 0.05), 'kernel': ['rbf', 'linear', 'sigmoid']}\n    svc_grid = RandomizedSearchCV(SVC(), params)\n    svc_grid.fit(x_train, y_train)\n    svc_predictions = svc_grid.predict(x_test)\n    \n    params = {'criterion': ['gini', 'entropy'], 'max_depth': range(1,50)}\n    decisiontree_grid = RandomizedSearchCV(DecisionTreeClassifier(), params)\n    decisiontree_grid.fit(x_train, y_train)\n    DT_predictions = decisiontree_grid.predict(x_test)\n    \n    params = {'criterion': ['gini', 'entropy'], 'n_estimators': range(5, 100), 'max_depth': range(1,50)}\n    rf_grid = RandomizedSearchCV(RandomForestClassifier(), params)\n    rf_grid.fit(x_train, y_train)\n    rf_predictions = rf_grid.predict(x_test)\n    \n    params = {'learning_rate': np.arange(0.01, 1.1, 0.1), 'n_estimators': np.arange(50,200, 25)}\n    adaboost_grid = RandomizedSearchCV(AdaBoostClassifier(),params)\n    adaboost_grid.fit(x_train, y_train)\n    adaboost_predictions = adaboost_grid.predict(x_test)\n    \n    params = {'activation': ['identity', 'logistic', 'tanh', 'relu'], 'solver': ['lbfgs', 'sgd', 'adam'],\n         'alpha': [0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1]}\n    mlp_grid = RandomizedSearchCV(MLPClassifier(max_iter=1000),params)\n    mlp_grid.fit(x_train, y_train)\n    mlp_predictions = mlp_grid.predict(x_test)\n\n    params = {'n_neighbors': np.arange(5,50, 2), 'weights': ['uniform', 'distance']}\n    KN_grid = RandomizedSearchCV(KNeighborsClassifier(),params)\n    KN_grid.fit(x_train, y_train)\n    KN_predictions = KN_grid.predict(x_test)\n    \n    accuracies = [accuracy_score(y_test, logreg_pred),accuracy_score(y_test, svc_predictions), \n                  accuracy_score(y_test, DT_predictions), accuracy_score(y_test, rf_predictions),\n                  accuracy_score(y_test, adaboost_predictions), accuracy_score(y_test, mlp_predictions),\n                  accuracy_score(y_test, KN_predictions)]\n    \n    total_accuracy.loc[i] = accuracies\n\naverage_accuracy = pd.DataFrame(total_accuracy.mean(axis=0), columns=['Accuracy'])\naverage_accuracy.transpose()","execution_count":711,"outputs":[]},{"metadata":{"_uuid":"e72f3123d29cb8a909c88a490c1c13949d9d9cc5"},"cell_type":"markdown","source":"Although the Logistic Regression algorithm resulted in the highest average accuracy over 100 runs, we will proceed with an MLP classifer anyways ¯\\\\\\_(ツ)_/¯"},{"metadata":{"_uuid":"4386a232187188e31d4a1f9167d6031cc94885d4"},"cell_type":"markdown","source":"#### Using an MLP Classifier for final survival predictions"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"33c92ac825529e1883a59391bb3897fa27fda5ab"},"cell_type":"code","source":"x_train = complete_data[0].drop(['Survived', 'PassengerId'], axis=1)\ny_train = complete_data[0]['Survived']\nx_test = complete_data[1].drop(['PassengerId'], axis=1)\n\nscaler = StandardScaler()  \nscaler.fit(x_train)\nx_train = scaler.transform(x_train)  \nx_test = scaler.transform(x_test)","execution_count":720,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"aaadb802123a3321d452374f4fc91c8c9c4e3601"},"cell_type":"code","source":"mlp = MLPClassifier(max_iter=2000, random_state=1234)\nmlp.fit(x_train, y_train)\nfinal_mlp = mlp.predict(x_test)","execution_count":763,"outputs":[]},{"metadata":{"_uuid":"e2760355f114bec14272a33b38ff25b8b2590b04"},"cell_type":"markdown","source":"#### Formatting output for submission"},{"metadata":{"trusted":false,"_uuid":"395a8c8373b96507960ead67e5a3ace6c3ced4e8","collapsed":true},"cell_type":"code","source":"final_submission = complete_data[1].copy()\nfinal_submission['Survived'] = final_mlp\nsubmission_df = pd.DataFrame()\nsubmission_df[['PassengerId', 'Survived']] = final_submission[['PassengerId', 'Survived']]\ndisplay(submission_df.head())\ndisplay(submission_df.groupby('Survived').count())\n\nsubmission_df.to_csv(\"Titanic Submission.csv\", index=False)","execution_count":764,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"}},"nbformat":4,"nbformat_minor":1}