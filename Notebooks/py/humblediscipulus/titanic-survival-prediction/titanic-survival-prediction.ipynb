{"cells":[{"metadata":{"trusted":true,"_uuid":"b0c798848b0d537dcdffb0f0d1aa25da3c699379"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport tensorflow as tf\nimport math\nfrom sklearn import metrics\n\nimport os\nprint(os.listdir(\"../\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"54045837923991fde18187df8c0c19751224fade"},"cell_type":"markdown","source":"## Configuring pandas"},{"metadata":{"trusted":true,"_uuid":"b2e33b82ffa34c27dfe703247591c4c05c02c419"},"cell_type":"code","source":"pd.options.display.max_rows = 25\npd.options.display.float_format = '{:.2f}'.format","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"62ceb4e22c4677c8f7dbac92fffb475ce7adce81"},"cell_type":"markdown","source":"## Importing and displaying data"},{"metadata":{"trusted":true,"_uuid":"5e02921942c2c1d88605edafd33144db698622b5"},"cell_type":"code","source":"titanic_survival_data = pd.read_csv('../input/train.csv')\ntitanic_survival_test_data = pd.read_csv('../input/test.csv')\n\n# shuffling data\ntitanic_survival_data = titanic_survival_data.reindex( np.random.permutation( titanic_survival_data.index ) )\n\ntitanic_survival_data","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0241f9447ece2052d023d6abf1747f249a2ef08d"},"cell_type":"markdown","source":"## Pre-processing data\n#### 1. Scrubbing\n   * Replacing missing data with mean, if possible\n   * Remove rows that have omitted values\n   * Removing duplicates rows\n   * Removing rows with bad labels\n   * Removing rows with bad feature values\n\n#### 2. Feature Manipulation\n    * Dropping non-relevant features\n    * One-hot encoding and binning\n    * Feature crosses"},{"metadata":{"trusted":true,"_uuid":"b4ce68152c480a2dc0652a29db334e9dfdaddd3c"},"cell_type":"code","source":"def normalize( col ):\n    return ( col - col.min() ) / col.max()\n\ndef preProcess( df, test = False ):\n    '''\n        Takes a pandas dataframe and performs scrubbing\n        \n        Args: \n            df - dataframe\n        \n        Returns:\n            a pandas dataframe\n    '''\n    # filling missing age values with mean\n    df[\"Age\"] = df[\"Age\"].fillna((df[\"Age\"].mean()))\n    \n    # removing rows with omitted values\n    df = df.drop(  df[ (df[\"Age\"].isnull() | df[\"Fare\"].isnull()  | df[\"SibSp\"].isnull() ) ].index )\n    \n    # removing duplicates\n    df = df.drop_duplicates()\n    \n    # removing rows with bad labels( No obvious way of finding out in this dataset )\n    \n    # taking only relevant features\n    if not test:\n        relevant_features = [ \"Survived\", \"Pclass\", \"Age\", \"SibSp\", \"Parch\", \"Fare\" ]\n        new_df = df[ relevant_features ].copy()\n    else:\n        relevant_features = [ \"Pclass\", \"Age\", \"SibSp\", \"Parch\", \"Fare\" ]\n        new_df = df[ relevant_features ].copy()\n    \n    # converting string column ( sex, Embarked ) to numberic datatype ( One hot encoding )\n    Sex = pd.get_dummies( df[\"Sex\"], drop_first = True )\n    Embarked = pd.get_dummies( df[\"Embarked\"], drop_first = True )\n    \n    new_df = pd.concat( [ new_df, Sex, Embarked ], axis = 1 )\n    \n    # adding new polynomial features\n    new_df[\"Age_qd\"] = df[\"Age\"]**4\n    new_df[\"Fare_qd\"] = df[\"Fare\"]**4\n    new_df[\"Pclass_qd\"] = df[\"Fare\"]**4\n    \n    # adding new feature crosses\n    new_df[\"Age_Fare\"] = df[\"Age\"] * df[\"Fare\"]\n    new_df[\"Age_Pclass\"] = df[\"Age\"] * df[\"Pclass\"]\n    \n    # feature scaling\n    new_df[\"Fare\"] = normalize( new_df[\"Fare\"] )\n    new_df[\"Age\"] = normalize( new_df[\"Age\"] )\n    new_df[\"Age_qd\"] = normalize( new_df[\"Age_qd\"] )\n    new_df[\"Age_Fare\"] = normalize( new_df[\"Age_Fare\"] )\n    new_df[\"Fare_qd\"] = normalize( new_df[\"Fare_qd\"] )\n    new_df[\"Pclass_qd\"] = normalize( new_df[\"Pclass_qd\"] )\n    new_df[\"Age_Pclass\"] = normalize( new_df[\"Age_Pclass\"] )\n    \n    return new_df","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true,"_uuid":"c24a8cadd4915f3aac6847b1a01cb8c07291f28d"},"cell_type":"code","source":"processed_data = preProcess( titanic_survival_data )\nprocessed_data","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eba58b03d1752934bfbf5e639599e44d18db7f54"},"cell_type":"markdown","source":"## Data Insight\nPlotting histograms and graphs helps us understand data relation with lables. It also gives us a hint about bad labels and features that we may miss out during preprocessing.\n\nWe can clearly see that there are some rows that have missing data for \"Age\". We need to deal with that during preprocessing."},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"e0cb02c6fa3025f4fd8bb89cf38ca027c43deae3"},"cell_type":"code","source":"processed_data.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"449ad3d8a90647a49230fd6a1b359af987e96f95"},"cell_type":"markdown","source":"## Finding correlation\nWe make use of the pearson coefficient to find out redundant coefficients and also select the most influencial coefficients.\n\nAs we can see, the \"Sex\", \"Fare\" and the \"Pclass\" feature have major effect on the label. \"Age\" comes next and finally the \"SibSp\". \"Parch\" has the least effect. Thus the features that we should be looking for are \"Age\", \"Fare\", \"Pclass\", \"SibSp\" and \"Parch\"."},{"metadata":{"trusted":true,"_uuid":"e2d067d02a497b254c8c0929f2bc9f1f823b10bf"},"cell_type":"code","source":"processed_data.corr( method=\"pearson\" )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"558ff206b80de9deafc114b8c1bb774396e12b9a"},"cell_type":"markdown","source":"## Histogram of age group distribution of passengers\nLet us divide the age groups into 20 bins.\n\n* The plot shows us that most of the people were between the age of 15 - 50.\n* A good number of infants ( below the age of 5 ) were also present on the ship."},{"metadata":{"trusted":true,"_uuid":"7f7d31341a3769834b93c5d146a5b7b42bbca57c"},"cell_type":"code","source":"plt.figure( figsize = ( 14, 8 ) )\n\nax = plt.subplot( 1, 1, 1 )\n\nax.set_title( \"Age group distribution\" )\n\nax.set_autoscaley_on( False )\nax.set_ylim( [ 0, 300 ] )\n\nax.set_autoscalex_on( False )\nax.set_xlim( [ 0, 1 ] )\n\nax.set_ylabel(\"Population\", fontsize = 12)\nax.set_xlabel(\"Age\", fontsize = 12)\n\nx = processed_data[\"Age\"]\n\nax.hist( x, bins = 20 )\n\n_ = plt.plot()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d3c14da065a1853dbcab3c4f7c245262ec307979"},"cell_type":"markdown","source":"## Scatter plot of Age vs Fare vs Survived\n**We clearly see a few out liers here**\n\n#### Looking at the plot below gives us some interesting insight. \n* Firstly, **poor** people between the age of 15 - 50 mostly ended up dead, while the **rich** people of the same age group mostly lived.\n* Secondly, **most** of the children ( below age 5 ) lived no matter their economic status.\n* Thirdly, **most** the old people ( age more than 50 ) died.\n"},{"metadata":{"trusted":true,"_uuid":"ee41c29d10a5d6ef5697613380e71ce3812044be"},"cell_type":"code","source":"plt.figure( figsize = ( 14, 14 ) )\n\nax = plt.subplot( 1, 1, 1 )\n\nax.set_title( \"Age vs Fare\" )\n\nax.set_autoscaley_on( False )\nax.set_ylim( [ 0, 1 ] )\n\nax.set_autoscalex_on( False )\nax.set_xlim( [ 0, 1 ] )\n\nax.set_ylabel(\"Age\", fontsize = 12)\nax.set_xlabel(\"Fare\", fontsize = 12)\n\nx = processed_data[\"Fare\"]\ny = processed_data[\"Age\"]\nc = [ 'red' if s == 0 else  'green' for s in processed_data[\"Survived\"] ]\n\nax.scatter( x, y, c = c, alpha = 0.5 )\n\n_ = plt.plot()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6420023f7d7cf4d4b52a66c3f1d664934387b230"},"cell_type":"markdown","source":"## Gender distribution in survival\nFrom the pie charts below, it is evident that from the surviving people, most of them were women, and that clain is backed by the other pie chart saying that the proportion of female deaths was significantly less as compared to male. Thus the data suggests that if you were a women, you would have likely to have lived."},{"metadata":{"trusted":true,"_uuid":"a0c21e3425af09cffc105a97cad7c4118501b551"},"cell_type":"code","source":"plt.figure( figsize = ( 16,12 ) )\n\nax = plt.subplot( 1, 2, 1 )\n\nlabels = 'Male', 'Female'\n\nmale_dead = len(processed_data[ (processed_data[\"male\"] == 1) & (processed_data[\"Survived\"] == 0) ])\nfemale_dead = len(processed_data[ (processed_data[\"male\"] == 0) & (processed_data[\"Survived\"] == 0) ])\n\nsizes = [male_dead, female_dead]\n\nax.pie(sizes, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=90)\nax.axis('equal')\n\nax.set_title(\"Death ratio\")\n\nax = plt.subplot( 1, 2, 2 )\n\nlabels = 'Male', 'Female'\n\nmale_dead = len(processed_data[ (processed_data[\"male\"] == 1) & (processed_data[\"Survived\"] == 1) ])\nfemale_dead = len(processed_data[ (processed_data[\"male\"] == 0) & (processed_data[\"Survived\"] == 1) ])\n\nsizes = [male_dead, female_dead]\n\nax.pie(sizes, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=90)\nax.axis('equal')\n\nax.set_title(\"Survived ratio\")\n\n_ = plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"af0fce5cbdbcc54a971367b362507b2b44a2117e"},"cell_type":"markdown","source":"## Splitting the data and prepare it for use\nNow that we have sufficient insight about our data, we will go ahead and split the data into 4 groups: Training data, Training target, Validation data and Validation Target\n\nNote: We will use the universal rule of splitting the data 70-30\n\nFor the sake of sanity checking, you can go ahead and print the final split that we get."},{"metadata":{"trusted":true,"_uuid":"334efa1fb8b4204c00f7a56a8297db58bd13417b"},"cell_type":"code","source":"def splitData( df, test = False ):\n    '''\n    Function to split data into training and validation set\n    \n    Args: \n        df - A pandas pre-processed dataset\n    Test:\n        bool - If set to true, then the data would not be split\n        \n    Return:\n        A tuple - ( training set, training target, validation set, validation target )\n    '''\n    if not test:\n        # seperating the target from the data\n        data = df[df.columns.difference(['Survived'])]\n        target = df[\"Survived\"]\n\n        training_data = data.head( 572 )\n        training_target = target.head( 572 )\n\n        validation_data = data.tail( 142 )\n        validation_target = target.tail( 142 )\n\n        return ( training_data, training_target, validation_data, validation_target )\n    else:\n        data = df[df.columns.difference(['Survived'])]\n        \n        return ( data )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"40655073b3af59faf11e9e26a62ed731ea79947c"},"cell_type":"code","source":"training_data, training_target, validation_data, validation_target = splitData( processed_data )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f57619e94c0e9327213ece7e0fa10d5f367a13c"},"cell_type":"markdown","source":"## Preparing the model\n\nNow that we have our data ready, we can now decide which model to use. We have many options here to choose from namely Neural Network and logistic classifier. I am trying to keep things simple so I will use logistic classifier. I am expecting an accuracy of 70 - 80 % from my model.\n\nWe will use the low level tf apis to implement the model. \n\n#### There are three major things to do here:\n* Define a prediction model\n* Define a cost function with respect to the model\n* Train on the data using suitable hyper-parameters\n\nNote: This is a modular approach that I think people should try to follow thus making it easy to experiment with new models without much change to the existing code"},{"metadata":{"trusted":true,"_uuid":"c0f56a945b837f259294bed9d348c05744315f42"},"cell_type":"code","source":"# define hyper params\nlearning_rate = 0.9\nepochs = 2000\nnumber_of_features = training_data.shape[1]\n\n# defining input and output placeholders ( Fare, Age, Pclass, sibSp, Parch, Sex )\nX = tf.placeholder( dtype = tf.float64 )\n\nY = tf.placeholder( dtype = tf.float64 )\n\n# list of parameters that we wish to learn (  number of features + intercept )\nW = tf.Variable( tf.random_normal( shape = [1,number_of_features], dtype=tf.float64 ) )\nb = tf.Variable( tf.random_normal( shape = [ 1, 1 ], dtype=tf.float64  ) )\n\n# define prediction model\ny_model = tf.sigmoid( tf.matmul( X, tf.transpose( W ) ) + b )\n\n# define cost function\ncost =  tf.reduce_mean( -Y * tf.log( y_model ) - ( 1 - Y ) * tf.log( 1 - y_model ) )\n\n# define training op\ntrain_op = tf.train.GradientDescentOptimizer( learning_rate =  learning_rate ).minimize( cost )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"458df79e60dbda722483ce5e5cf25c6d4f94a7f4"},"cell_type":"markdown","source":"## Defining training loop\nNow we can finally defnine the training loop and run the trian_op on the data.\nThe hyperparameters are user tunable thus may be required to change in order to get he best output.\n\nWe will also make use of our validation set to check if our model is overfitting the training data."},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"27e7166efc0f07a77968a208fd85419a1c48baad"},"cell_type":"code","source":"W_trained = []\n\nwith tf.Session() as sess:\n    \n    sess.run( tf.global_variables_initializer() )\n    \n    prev_error = 0\n    for epoch in range( epochs ):\n        error, _ = sess.run( [ cost, train_op], feed_dict = { \n            X: training_data, \n            Y: training_target\n        } )\n        \n        if epoch % 100 == 0:\n            print( epoch, error )\n        \n#         if( abs(prev_error - error) < 0.001 ):\n#             break\n            \n        prev_error = error\n    \n    W_trained, b_trained = sess.run( [ W, b] )\n    \n    print(W_trained)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f64679982d4542255bbfffb1be0e1aafcb556244"},"cell_type":"markdown","source":"## Evaluation\n\nNow that we have trained our model and have the final parameters, we can test the accuracy of the model on training and validation data"},{"metadata":{"trusted":true,"_uuid":"c3ac8ca8b1cb60c521c9b59b6b44d601ef844888"},"cell_type":"code","source":"Y_output = tf.round(y_model)\nY_orignal = tf.placeholder( dtype=tf.float64 )\n\ncorrect_prediction = tf.equal( Y_output, Y_orignal )\n\naccuracy = tf.reduce_mean( tf.cast( correct_prediction, 'float' ) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"027b23cf058aac904a55657c99bab607b10be529"},"cell_type":"code","source":"with tf.Session() as sess:\n    sess.run( tf.global_variables_initializer() )\n    \n    acc = sess.run( accuracy, feed_dict = { \n            X: training_data, \n            W: W_trained,\n            b: b_trained,\n            Y_orignal: training_target\n        } )\n    \n    print( \"Accuracy on the training set is %.2f %%\" % (acc * 100) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"25216dbbfb5d05c1b42cea6c96468a9a5dbffab9"},"cell_type":"code","source":"with tf.Session() as sess:\n    sess.run( tf.global_variables_initializer() )\n    \n    acc = sess.run( accuracy, feed_dict = { \n            X: validation_data, \n            W: W_trained,\n            b: b_trained,\n            Y_orignal: validation_target\n        } )\n    \n    print( \"Accuracy on the validation set is %.2f %%\" % (acc * 100) )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"97085609146fa4da5dd327d4bc04baff04056de2"},"cell_type":"markdown","source":"## Testing the model on test data"},{"metadata":{"trusted":true,"_uuid":"b55ba18e1eb4a2c33c1822d9f6ee411b9c243f1c"},"cell_type":"code","source":"data = splitData( preProcess(titanic_survival_test_data, test = True), test = True )\ndata","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8447475cb81b523de3253a5e4b7b7064164dfa30"},"cell_type":"code","source":"output = []\nwith tf.Session() as sess:\n    \n    output = sess.run( Y_output, feed_dict = { \n            X: data, \n            W: W_trained,\n            b: b_trained,\n        } )\n    \noutput","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c50977055756ec17183460b8547145778f6acea2"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}