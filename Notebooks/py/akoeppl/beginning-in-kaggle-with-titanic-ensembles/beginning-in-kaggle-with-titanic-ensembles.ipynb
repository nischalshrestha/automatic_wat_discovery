{"metadata": {"language_info": {"nbconvert_exporter": "python", "pygments_lexer": "ipython3", "file_extension": ".py", "name": "python", "mimetype": "text/x-python", "version": "3.6.3", "codemirror_mode": {"name": "ipython", "version": 3}}, "kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}}, "nbformat": 4, "nbformat_minor": 2, "cells": [{"cell_type": "markdown", "source": ["# Beginning in Kaggle with Titanic (Ensemble)\n", "### **Andr\u00e9 Koeppl**\n", "\n", "\n", "    "], "metadata": {"_uuid": "d078acf4cc246329c1107222b8ba73761a9b42b5", "_cell_guid": "2f790bbf-1719-4d91-977c-c0890ac78192"}}, {"cell_type": "markdown", "source": ["## 1. Introduction\n", "\n", "This is my first kernel at Kaggle. I got inspiration for this kernel from the folowing kernels:\n", "* Yassine Ghouzam, PhD: \"Titanic Top 4% with ensemble modeling\n", "*  LD Freeman, from his kernel: \"A Data Science Framework: To Achieve 99% Accuracy\". \n", "\n", "Thank you a lot guys for your willingness to share knowledge.\n", "I hope I can help some people to have more insights and receive critics ans suggestions. They will be welcome!\n", " "], "metadata": {"_uuid": "d13b45efd0801651027dec292d097dea37e8da6f", "_cell_guid": "c91b1111-1727-4fb4-97cf-e1f69d470be5"}}, {"outputs": [], "cell_type": "code", "metadata": {"_uuid": "01844bfc7df2c62c1057f31e582fc1ebba9ca8f4", "_execution_state": "idle", "collapsed": true, "_cell_guid": "67300bad-eafb-4a8e-82f6-e955a6ca070a"}, "source": ["#Importing initial libraries\n", "import pandas as pd\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "%matplotlib inline\n", "\n", "from collections import Counter\n", "\n", "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\n", "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.neighbors import KNeighborsClassifier\n", "from sklearn.tree import DecisionTreeClassifier\n", "from sklearn.neural_network import MLPClassifier\n", "from sklearn.svm import SVC\n", "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\n", "\n", "#including other classifiers\n", "from sklearn.naive_bayes import GaussianNB\n", "from mlxtend.classifier import StackingClassifier # for Stacking Ensembles\n", "\n", "sns.set(style='white', context='notebook', palette='deep')\n"], "execution_count": null}, {"cell_type": "markdown", "source": ["## 2. Load and check data\n", "### 2.1 Load data"], "metadata": {"_uuid": "8c97f422f7970e77a1aa73e3d868e59db9b0861f", "_cell_guid": "1f17e4f5-20f3-4e9c-8a56-adad76047552"}}, {"outputs": [], "cell_type": "code", "metadata": {"_uuid": "fdfaaf9242d609bb64403efe237876b435668c52", "_execution_state": "idle", "collapsed": true, "_cell_guid": "fd53a1e8-0c79-4d00-823b-0988fd16b635"}, "source": ["# Load data\n", "##### Load train and Test set\n", "df_train = pd.read_csv(\"../input/train.csv\")\n", "df_test = pd.read_csv(\"../input/test.csv\")\n", "IDtest = df_test[\"PassengerId\"]\n", "df_train.head()"], "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {"_uuid": "ead7a13b4463c852c2c15539e32c81151ca335fd", "collapsed": true, "_cell_guid": "f81b7140-87a9-4418-9f6d-7f8624c214fc"}, "source": ["print(df_train.shape)\n", "print(df_test.shape)"], "execution_count": null}, {"cell_type": "markdown", "source": ["### 2.2 Joining data frames"], "metadata": {"_uuid": "1ba7849b2184291ce0084e735e234493cd5080b8", "_cell_guid": "345b0675-6efd-4d6b-8eab-3401176dc7aa"}}, {"cell_type": "markdown", "source": ["I joint the 2 data frames ensemble to assure that the dummy variables I am going to create are the same for both data frames"], "metadata": {"_uuid": "b3b18d88ddde60eb464b0cfb5d54fcc8b48cec87", "_cell_guid": "d7a3fcce-2e70-4271-802e-f01b0596fed4"}}, {"outputs": [], "cell_type": "code", "metadata": {"_uuid": "c08679e57f6f54f26d1f69c9a16d24ea44545fcf", "collapsed": true, "_cell_guid": "3883baf5-b1dd-4397-8b58-f7a44e3e5d80"}, "source": ["## Join train and test datasets in order to obtain the same number of features during categorical conversion\n", "train_len = len(df_train)\n", "dataframe =  pd.concat(objs=[df_train, df_test], axis=0).reset_index(drop=True)\n", "dataframe.tail()"], "execution_count": null}, {"cell_type": "markdown", "source": ["## 3. Filling Missing values"], "metadata": {"_uuid": "bd05373e6eb67b7be211c60d72cac9f699b65ab6", "_cell_guid": "b9c8c51e-e77c-4aba-bb7f-ec15397cb4be"}}, {"cell_type": "markdown", "source": ["### 3.1 Simple missings substitution"], "metadata": {"_uuid": "d76e081720c04bf46cbde4aff283f15cb76c4fd1", "_cell_guid": "a5a70493-e912-494d-b26f-f8e060bb55b5"}}, {"outputs": [], "cell_type": "code", "metadata": {"_uuid": "95ee40420bb147b98b494a8ed19202782f190b9d", "collapsed": true, "_cell_guid": "9333868a-efa9-46b4-815d-5423b6d21e74"}, "source": ["#Checking missing values\n", "print(dataframe.isnull().sum())"], "execution_count": null}, {"cell_type": "markdown", "source": ["I am going to use the following strategy to treat the missing variables:\n", "\n", "* Fare missing values are little representative in the data frame so, lets fulfill them with the median of the df_train \n", "* Cabin missing values are very representative, so they will be fulfiled with a prediction from a multonomial model\n", "* Embarked missing  values are little representative in the data frame so, lets fulfill them with the most frequent value\n", "* Age missing values are very representative in this dara frame so I am creating a model to better predict values to fulfill the NaNs"], "metadata": {"_uuid": "bb3da3a6f81c7ea866256ebe0faf9ad0822c7ee6", "_cell_guid": "b3290ef3-f608-42bf-b257-29fc26d6b238"}}, {"outputs": [], "cell_type": "code", "metadata": {"_uuid": "f9e4e2f846ba4ca6919e2e8259a8c7adbfb3947b", "collapsed": true, "_cell_guid": "99831517-46e4-4be1-9c7b-7968aba24ad0"}, "source": ["Fare_median = df_train.Fare.median()\n", "dataframe.loc[dataframe.Fare.isnull(),'Fare'] = Fare_median"], "execution_count": null}, {"cell_type": "markdown", "source": ["Lets check the categorical variable Embarked."], "metadata": {"_uuid": "46b5bd580a6a01d081ab6602fe3e1daa89eee75c", "_cell_guid": "eb065294-197c-41ab-9c35-e403466cf094"}}, {"outputs": [], "cell_type": "code", "metadata": {"_uuid": "513d3c513db5a0c2dea8ff02a0b851e7e5923648", "collapsed": true, "_cell_guid": "61cff099-cfe5-4586-a576-56ecf301809b"}, "source": ["g = sns.factorplot(\"Embarked\",  data=dataframe,\n", "                   size=6, kind=\"count\", palette=\"muted\")\n", "g.despine(left=True)\n", "g = g.set_ylabels(\"Count\")"], "execution_count": null}, {"cell_type": "markdown", "source": ["Since S is the most frequent Embarkation port, let's assume the missing values to be S"], "metadata": {"_uuid": "262e0885e0abc2e84a656304c1b1b7ec2698e50d", "_cell_guid": "976fbeee-898c-4d14-a83c-0a324ba00751"}}, {"outputs": [], "cell_type": "code", "metadata": {"_uuid": "e8d1c859868a238167fd40a40f25c4ffefe72f7d", "collapsed": true, "_cell_guid": "22970259-2371-48a0-9073-724b3a466d73"}, "source": ["dataframe.loc[dataframe.Embarked.isnull(),'Embarked'] = 'S'"], "execution_count": null}, {"cell_type": "markdown", "source": ["### 3.2 Missing substitution by a predicted model"], "metadata": {"_uuid": "0af567590f85c3b5d872007557daa0f95d4f2fd2", "_cell_guid": "abbf3af2-69b4-43b7-ac35-4f4ebb8912d5"}}, {"cell_type": "markdown", "source": ["Let's take a look  at the NaNs of the categorical variable Cabin:"], "metadata": {"_uuid": "af9bb02f4811c2d89363f5419f7c2f43d8c19660", "_cell_guid": "ab835e93-b219-4d65-9495-5fd7ea278734"}}, {"outputs": [], "cell_type": "code", "metadata": {"_uuid": "56095a14f07e3445378ed00f5e5397bf72d7cc2d", "collapsed": true, "_cell_guid": "dafd660b-d201-488f-ba86-c9578fe0c3c8"}, "source": ["print(\"% of NaN values in Cabin Variable for train test:\")\n", "print(dataframe.Cabin.isnull().sum()/len(dataframe))"], "execution_count": null}, {"cell_type": "markdown", "source": ["Very representative."], "metadata": {"_uuid": "fba6ab0754b0814ca5fe7529304ffa7c243a1b51", "_cell_guid": "698a4178-677b-405e-89fd-ca6911b8658d"}}, {"outputs": [], "cell_type": "code", "metadata": {"_uuid": "83c1ae6318f7b97ce4fbcfd9193e9254b5b95109", "collapsed": true, "_cell_guid": "f1dcc7d2-4513-4e55-8073-d968ebc9c53e"}, "source": ["# Replace the Cabin categorical variable for its first letter\n", "dataframe[\"Cabin\"] = pd.Series([i[0] if not pd.isnull(i) else np.nan for i in dataframe['Cabin']])\n", "\n", "g = sns.factorplot(\"Cabin\",  data=dataframe,\n", "                   size=6, kind=\"count\", palette=\"muted\")\n", "g.despine(left=True)\n", "g = g.set_ylabels(\"Count\")"], "execution_count": null}, {"cell_type": "markdown", "source": ["Multinomial logistic regression doen't accept one of its nominal levels to have just one observation. Let's check \"T\"."], "metadata": {"_uuid": "b067ca94e5bc8c18cf0bcdaef9b8fd3898d62f32", "_cell_guid": "b8c15f22-e7aa-4c7a-b83d-73127dbe11bb"}}, {"outputs": [], "cell_type": "code", "metadata": {"_uuid": "715b92397d537a0564be21a0bbc4acf367dec5b2", "collapsed": true, "_cell_guid": "6e8f21aa-ddd3-498a-8432-4aba216a4408"}, "source": ["dataframe[dataframe.Cabin=='T']"], "execution_count": null}, {"cell_type": "markdown", "source": ["Let's check the most frequent level too"], "metadata": {"_uuid": "07b87eccb6de0a483d7a9b4c57d8250d6fc0d235", "_cell_guid": "22673b69-4c40-488c-a083-f2b0e7821e2f"}}, {"outputs": [], "cell_type": "code", "metadata": {"_uuid": "04cec9a1d7e964fcaea115b7052e9509f460492f", "collapsed": true, "_cell_guid": "bc748121-8d51-4d66-bdb5-7131b5eb77d4"}, "source": ["dataframe[dataframe.Cabin=='C'].head()"], "execution_count": null}, {"cell_type": "markdown", "source": ["Replacing \"T\" by the most frequent level \"C\""], "metadata": {"_uuid": "a299e171f15e2816fcd0325dfed87952b2692c8f", "_cell_guid": "408fe535-62e6-45d2-b2f3-ae2b22c70ca4"}}, {"outputs": [], "cell_type": "code", "metadata": {"_uuid": "df473b954cec2a4ad117821b6de31aae1a96e322", "collapsed": true, "_cell_guid": "053b2f1f-d25c-48e4-bdfc-fc060c6cba12"}, "source": ["dataframe.loc[dataframe.Cabin=='T',\"Cabin\"]='C'\n", "g = sns.factorplot(\"Cabin\",  data=dataframe,\n", "                   size=6, kind=\"count\", palette=\"muted\")\n", "g.despine(left=True)\n", "g = g.set_ylabels(\"Count\")"], "execution_count": null}, {"cell_type": "markdown", "source": ["So, let's train a model for 'Cabin'"], "metadata": {"_uuid": "73b2d9d199a2c7981568b5e7e7a5a1f176a39e87", "_cell_guid": "cc47626f-46ad-42aa-9ff2-e4c061057106"}}, {"outputs": [], "cell_type": "code", "metadata": {"_uuid": "497672ff2be4200ba259520496ec19f4f2991843", "collapsed": true, "_cell_guid": "1dec4eb9-1c55-4244-bae0-6599110dff4b"}, "source": ["#Splitting the base in two:\n", "df_Cabin_not_NaN = dataframe[dataframe.Cabin.notnull()]\n", "df_Cabin_NaN = dataframe[dataframe.Cabin.isnull()]\n", "\n", "#Splitting the dataframe in X and Y\n", "X_train_Cabin = pd.concat([df_Cabin_not_NaN[['SibSp', 'Parch', 'Pclass','Fare']]], axis=1)\n", "y_train_Cabin = df_Cabin_not_NaN.Cabin\n", "\n", "# Importing the libraries\n", "from sklearn import linear_model\n", "\n", "# Train multinomial logistic regression model\n", "mul_lr = LogisticRegression(multi_class='multinomial',\n", "                                         solver='newton-cg',max_iter = 100) .fit(X_train_Cabin, y_train_Cabin)\n", "\n", "scores = cross_val_score(mul_lr, X_train_Cabin, y_train_Cabin, cv=5, scoring='accuracy')\n", "print(\"Accuracy: %0.4f (+/- %0.4f)\" % (scores.mean(), scores.std()))\n"], "execution_count": null}, {"cell_type": "markdown", "source": ["Then let's run the prediction."], "metadata": {"_uuid": "79339803888830fcffc058f6e35dad49d6147003", "_cell_guid": "705c6ed5-ea8e-480d-892c-050a9d2f11e9"}}, {"outputs": [], "cell_type": "code", "metadata": {"_uuid": "f8789b08c85d8c26cae0627302ad7fa78bd694d7", "collapsed": true, "_cell_guid": "ea3bbb25-eb10-46f6-a0db-3d301c8cc343"}, "source": ["#Splitting the dataframe in X, using the same variables we used to train the model\n", "X_train_Cabin_Nan =  pd.concat([df_Cabin_NaN[['SibSp', 'Parch', 'Pclass','Fare']]], axis=1)\n", "\n", "# Predict y\n", "mul_lr.fit(X_train_Cabin, y_train_Cabin)\n", "y_train_Cabin_Nan = mul_lr.predict(X_train_Cabin_Nan)\n", "\n", "#Checking results\n", "auxiliar = pd.DataFrame(y_train_Cabin_Nan,columns=['Cabin'])\n", "\n", "g = sns.factorplot(\"Cabin\",  data=auxiliar,\n", "                   size=6, kind=\"count\", palette=\"muted\")\n", "g.despine(left=True)\n", "g = g.set_ylabels(\"Count\")   "], "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {"_uuid": "44f04ab86a6f954ccfc205a05288ac93c0ffcd70", "collapsed": true, "_cell_guid": "30aac559-7b32-43bf-a5b0-b484ebb5661a"}, "source": ["#Rebuilding the dataset\n", "df_Cabin_NaN.loc[:,'Cabin']= y_train_Cabin_Nan\n", "dataset = pd.concat([df_Cabin_NaN,df_Cabin_not_NaN], axis =0).reset_index(drop=True)\n", "dataset.shape\n"], "execution_count": null}, {"cell_type": "markdown", "source": ["Let's check the variable 'Age'"], "metadata": {"_uuid": "9da5520d9134a260fb38cef2d17d766647c9d2fb", "_cell_guid": "9525a4bc-8009-4dd9-aaa8-a38fe5dd403a"}}, {"outputs": [], "cell_type": "code", "metadata": {"_uuid": "54ee2c09a9344a3b2f56a2980966937c672745bd", "collapsed": true, "_cell_guid": "172a95eb-1b38-47ce-bc5e-8cffc6abc6a8"}, "source": ["print(\"% of NaN values in Age Variable for train test:\")\n", "print(dataset.Age.isnull().sum()/len(dataset))"], "execution_count": null}, {"cell_type": "markdown", "source": ["The remaing variable is Age and it is considerably representative in the dataset. Let's take a look in a correlation heat map to understant it better."], "metadata": {"_uuid": "a8156f0e8731160907ee95167a7baec386ce9e66", "_cell_guid": "b5b76f95-8109-47a7-96f5-1f44b139da42"}}, {"outputs": [], "cell_type": "code", "metadata": {"_uuid": "edd2bfad3f8da9829d5420b4dfde019239e5f91b", "collapsed": true, "_cell_guid": "0309c88a-1221-4b21-ad7d-602f8f5a7d35"}, "source": ["# Correlation matrix between numerical values (SibSp Parch Age and Fare values) and Survived \n", "g = sns.heatmap(df_train[[\"Survived\",\"SibSp\",\"Parch\",\"Age\",\"Fare\"]].corr(),annot=True, fmt = \".2f\", cmap = \"coolwarm\")"], "execution_count": null}, {"cell_type": "markdown", "source": ["So, Age is more correlated with SibSp and Parch, but one os my guess is it maybe be correlated with the categorical variables as well. Let's understand and treat the variable Name."], "metadata": {"_uuid": "b47bbd9dd9f51aef3626722fad41b4a7f8a37f55", "_cell_guid": "5ce77143-fd18-4f5c-ad9c-412a3a365d12"}}, {"outputs": [], "cell_type": "code", "metadata": {"_uuid": "9abef7610cb841f0444cbbc41eb72ffc816ea07e", "collapsed": true, "_cell_guid": "ccc9a790-c71e-4399-a894-6de237115ac1"}, "source": ["#Lets take a look into the name's variable\n", "dataset.Name.head()"], "execution_count": null}, {"cell_type": "markdown", "source": ["Let's separate the pronoum of treatment from de data"], "metadata": {"_uuid": "70e2b604b7761298fc8a61dec4b21cad5d94ffef", "_cell_guid": "7ad015f8-c7d5-4734-a692-26dd8658a202"}}, {"outputs": [], "cell_type": "code", "metadata": {"_uuid": "194aa6871cb8a2cea10e461dd14013d2c9199a93", "collapsed": true, "_cell_guid": "a211e819-3290-4835-94c3-40026c9ea450"}, "source": ["dataset_title = [i.split(\",\")[1].split(\".\")[0].strip() for i in dataset[\"Name\"]]\n", "dataset[\"Title\"] = pd.Series(dataset_title)\n", "\n", "j = sns.countplot(x=\"Title\",data=dataset)\n", "j = plt.setp(j.get_xticklabels(), rotation=45) "], "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {"_uuid": "d329803b8205196c5ef654239d79069d61567dfc", "collapsed": true, "_cell_guid": "2d926c62-28e6-43c2-a1e9-acb66e974109"}, "source": ["# Exploring Survival probability\n", "g = sns.factorplot(x=\"Title\",y=\"Survived\",data=dataset,kind=\"bar\", size = 10 , \n", "palette = \"muted\")\n", "g.despine(left=True)\n", "g = g.set_ylabels(\"survival probability\")"], "execution_count": null}, {"cell_type": "markdown", "source": ["Captain has not survived. The major and colonel are military titles and have ~50% chance of survival (with high dipsersion). It seems relevant to group possible dynamics of profession, crew membership, nobility title, and so on.."], "metadata": {"_uuid": "933cdb20f72fced571e337b21160c22acf844129", "_cell_guid": "9067ef51-e418-49e6-bb24-608ab83b935d"}}, {"outputs": [], "cell_type": "code", "metadata": {"_uuid": "5759fcb98d74913a63e39b2980d9ad1bb5746ab4", "collapsed": true, "_cell_guid": "dc978dc1-15be-49c3-82d3-1c2eb8d337ee"}, "source": ["# Let's group the military and crew titles ensemble\n", "dataset[\"Title\"] = dataset[\"Title\"].replace(['Capt', 'Col','Major'], 'Crew/military')\n", "# Let's group the profession title ensemble\n", "dataset[\"Title\"] = dataset[\"Title\"].replace(['Master', 'Dr'], 'Prof')\n", "# Let's change the type of women title\n", "dataset[\"Title\"] = dataset[\"Title\"].replace(['Miss', 'Mme','Mrs','Ms','Mlle'], 'Woman')\n", "# Let's change the type of nobility title\n", "dataset[\"Title\"] = dataset[\"Title\"].replace(['the Countess', 'Sir', 'Lady', 'Don','Jonkheer', 'Dona'], 'Noble')\n", "# Let's change the type of religious title\n", "dataset[\"Title\"] = dataset[\"Title\"].replace(['Rev'], 'Religious')\n", "\n", "j = sns.countplot(x=\"Title\",data=dataset)\n", "j = plt.setp(j.get_xticklabels(), rotation=45) "], "execution_count": null}, {"cell_type": "markdown", "source": [], "metadata": {"_uuid": "c242d381c51df1c9f64584372452fb24d504e226", "_cell_guid": "cc993e3e-e2f0-42c3-a905-ab344ac8f65f"}}, {"outputs": [], "cell_type": "code", "metadata": {"_uuid": "110a8847eaeadcc5145e7d2e6f20492ea2854126", "collapsed": true, "_cell_guid": "a903478e-63e4-4bd0-bf99-ffc48828e0de"}, "source": ["# Exploring Survival probability\n", "g = sns.factorplot(x=\"Title\",y=\"Survived\",data=dataset,kind=\"bar\", size = 6 , \n", "palette = \"muted\")\n", "g.despine(left=True)\n", "g = g.set_ylabels(\"survival probability\")"], "execution_count": null}, {"cell_type": "markdown", "source": ["Clearly we understand that they prioitize Woman, followed by the Nobles and then Professional titles. We can't make a clear conclusion regaring nobility titles because of the high sigma, but I bet it's because of the percentage of men in this group. Obviously Religious title make the moral decision in this case. Regarding officers, the dispersion indicates part of the crew decided to remain onboard and others went on the survival boats. Ordinary men had the least chance of survival."], "metadata": {"_uuid": "69bd38e5e21283b7cade3bfcfb615b70a9c2c272", "_cell_guid": "ae51911d-d253-4247-a767-a4320f3d3cd7"}}, {"cell_type": "markdown", "source": ["Age: let's create a simple model to predict values for the fullfilment of the NaNs"], "metadata": {"_uuid": "2c1655e269a5f1a79c15799cfbd224e960cd5a66", "_cell_guid": "614edc7b-469e-4b40-b027-5322ea35c47a"}}, {"outputs": [], "cell_type": "code", "metadata": {"_uuid": "52d50134c3a61826584e2a28e02cd0de0f937c01", "collapsed": true, "_cell_guid": "305d1a83-fd93-4e2d-83b6-827047e19c1f"}, "source": ["# Creating the Dummies\n", "dataset = pd.concat([dataset, pd.get_dummies(dataset[['Sex', 'Embarked','Title']])], axis=1)\n", "dataset.head()"], "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {"_uuid": "d76d55db4db7de2c711b3636ca54220fc44e2986", "collapsed": true, "_cell_guid": "602da9ed-1f39-4301-b337-a2caa43c7f59"}, "source": ["#Splitting the base in two:\n", "df_Age_not_NaN = dataset[dataset.Age.notnull()]\n", "df_Age_NaN = dataset[dataset.Age.isnull()]\n", "\n", "#Splitting the dataframe in X and Y\n", "X_train = pd.concat([df_Age_not_NaN[['SibSp', 'Parch', 'Pclass','Fare','Sex_female','Sex_male',\n", "                                     'Embarked_C', 'Embarked_Q', 'Embarked_S','Title_Crew/military',\n", "                                     'Title_Mr','Title_Noble','Title_Prof','Title_Religious',\n", "                                     'Title_Woman']]], axis=1)\n", "y_train = df_Age_not_NaN.Age\n", "\n", "# Importing the libraries\n", "from sklearn import linear_model\n", "from mlxtend.regressor import StackingRegressor\n", "from sklearn.linear_model import LinearRegression\n", "from sklearn.linear_model import Ridge\n", "from sklearn.svm import SVR\n", "from sklearn.ensemble import RandomForestRegressor\n", "from sklearn.neighbors import KNeighborsRegressor\n", "from sklearn.ensemble import GradientBoostingRegressor\n", "from sklearn.tree import DecisionTreeRegressor\n", "from sklearn.ensemble import AdaBoostRegressor\n", "\n", "# Initializing models\n", "lr = LinearRegression()\n", "svr_lin = SVR(kernel='linear')\n", "ridge = Ridge(random_state=1)\n", "svr_rbf = SVR(kernel='rbf')\n", "rf = RandomForestRegressor(random_state=1)\n", "knn = KNeighborsRegressor()\n", "gbr = GradientBoostingRegressor()\n", "dtr = DecisionTreeRegressor(max_depth=4)\n", "adr = AdaBoostRegressor(DecisionTreeRegressor(max_depth=4),\n", "                          n_estimators=300, random_state=1)\n", "                           \n", "for clf, label in zip([lr, svr_lin, ridge, rf, knn,gbr,dtr,adr, svr_rbf], ['Linear regression',\n", "                                                                           'Linear support vector machine',\n", "                                                                           'Ridge','Random Forest',\n", "                                                                           'KNeighbors','Gradient Boosting',\n", "                                                                           'Decision Tress', 'Ada Boosting',\n", "                                                                           'Rbf support vector machine']):\n", "    scores = cross_val_score(clf, X_train, y_train, cv=5, scoring='r2')\n", "    print(\"R2: %0.4f (+/- %0.4f) [%s]\" % (scores.mean(), scores.std(), label))"], "execution_count": null}, {"cell_type": "markdown", "source": ["Every model resulted in a R2 higher than zero, so they predict better if I've used an average. So let's take the best estimator, the Gradient Boosting, and refine it using the Grid Search"], "metadata": {"_uuid": "a0f732604b6716d8d1298db281280993236964f0", "_cell_guid": "01db38af-698c-4125-a507-bd62cc99145b"}}, {"outputs": [], "cell_type": "code", "metadata": {"_uuid": "b671c9ea85be13506f0c0c47043a8f0ec0cd6bd3", "collapsed": true, "_cell_guid": "4f4562ea-4d6d-492e-b8a1-2be44bd2be03"}, "source": ["#GBC\n", "params = [{ 'n_estimators' : [25, 50,100,200],\n", "              'learning_rate': [0.5, 0.1, 0.05, 0.01],\n", "              'max_depth': [2, 3, 4],\n", "              'min_samples_leaf': [1, 2, 3],\n", "              'max_features': [None, \"auto\"]}]\n", "\n", "from sklearn.model_selection import GridSearchCV\n", "\n", "grid_gbr = GridSearchCV(GradientBoostingRegressor(random_state=41), params, cv=5, scoring = 'r2')\n", "grid_gbr.fit(X_train, y_train)\n", "\n", "print (\"Gradient Boosting\")\n", "print(\"Best parameters set found on development set:\")\n", "print()\n", "print(grid_gbr.best_params_)\n", "print()\n", "print(\"Best score set found on development set:\")\n", "print()\n", "print(grid_gbr.best_score_)"], "execution_count": null}, {"cell_type": "markdown", "source": ["A little improvement in R2, which for me it's enough to stop tuning the model. Let's apply it to the NaNs in 'Age'"], "metadata": {"_uuid": "d182f63a4d797c282f7e3269076f4bedbc56b5a1", "_cell_guid": "1e8f041e-4bec-4f98-818e-84a6a21e532f"}}, {"cell_type": "markdown", "source": ["Predicting missing values..."], "metadata": {"_uuid": "34ff5f7e82d4885ae05f6c680694da705b42800d", "_cell_guid": "26075515-cb6b-42ad-b0a2-0207773a6c42"}}, {"outputs": [], "cell_type": "code", "metadata": {"_uuid": "35347a07cbb5544e6116b441febb0d36c20f431a", "collapsed": true, "_cell_guid": "d52ee74c-815d-4028-aae4-54cab0d5259c"}, "source": ["#Splitting the dataframe in X, using the same variables we used to train the model\n", "X_train_Nan = pd.concat([df_Age_NaN[['SibSp', 'Parch', 'Pclass','Fare','Sex_female','Sex_male',\n", "                                     'Embarked_C', 'Embarked_Q', 'Embarked_S','Title_Crew/military',\n", "                                     'Title_Mr','Title_Noble','Title_Prof','Title_Religious',\n", "                                     'Title_Woman']]], axis=1)\n", "#Initializing the best model\n", "from sklearn import ensemble\n", "best_gbr = GradientBoostingRegressor(learning_rate=0.1, max_depth=4, max_features= None, \n", "                                     min_samples_leaf= 1, n_estimators= 50, random_state=41)                             \n", "\n", "# Predict y\n", "best_gbr.fit(X_train, y_train)\n", "y_train_Nan = best_gbr.predict(X_train_Nan)\n", "\n", "#Checking results\n", "auxiliar = pd.DataFrame(y_train_Nan,columns=['Age'])\n", "print(\"Number of Nan: %0.2f\" %auxiliar.isnull().sum()) # no Nans\n", "print(\"-\"*10)\n", "print(\"Min value of y_train_Nan: %0.2f\" % y_train_Nan.min())   #We don't have negative 'Age'            "], "execution_count": null}, {"cell_type": "markdown", "source": ["Let's insert the replaced data from 'Age' in dataset again"], "metadata": {"_uuid": "de15b7e4a342f0ed2722681d4aae0e2b2977455b", "_cell_guid": "2331aaae-907f-4a69-b887-06bb5875f2c0"}}, {"outputs": [], "cell_type": "code", "metadata": {"_uuid": "e0a55fc68e8ee0fa80142f1bd7f88d4a8a28c7a0", "collapsed": true, "_cell_guid": "6acf1a8a-96f0-4d90-a0a9-35de71dba889"}, "source": ["#Rebuilding the dataset\n", "df_Age_NaN['Age']= y_train_Nan\n", "df = pd.concat([df_Age_NaN,df_Age_not_NaN], axis =0).reset_index(drop=True)\n", "\n"], "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {"_uuid": "79855e448c6ef2220e4a34429fb6a34ca9461fcd", "collapsed": true, "_cell_guid": "05d2e4ab-ee6d-413b-959a-3fc24c20bb10"}, "source": ["#Checking the missings\n", "print(df.isnull().sum())"], "execution_count": null}, {"cell_type": "markdown", "source": ["So we have cleaned all the missings from the  input variables"], "metadata": {"_uuid": "dca0ef06269dbd20c787046ae7446e14f8f0a503", "collapsed": true, "_cell_guid": "5e2f7323-3bd6-4131-8b42-7c6680daaa71"}}, {"cell_type": "markdown", "source": ["# 4. Feature engineering"], "metadata": {"_uuid": "9261f57398cf544e12223d1ab8d7004cf8a3eb15", "_cell_guid": "674e2766-aacf-4469-b51a-02953cc81321"}}, {"cell_type": "markdown", "source": ["I have engineered Cabin and the Name variables.\n", "Also I created dummies for  'Sex', 'Embarkation' and 'Title'.\n", "\n", "Now I am going to engineer the variable 'Fsize', which is the combination of 'Parch' and 'SibSp' and  'Ticket' as well, despite my suspition that it has few effect in the surival rate.\n", "\n", "Then I am going to create dummies for them too for 'Cabin' and 'Tiket' and discretize the numerical variables"], "metadata": {"_uuid": "16ffd929b4a59cbc04c9bcaa5e45deca046d6f26", "_cell_guid": "52835d2e-37a1-41e7-9998-0b3a077ec2c3"}}, {"cell_type": "markdown", "source": ["Working on 'Cabin'. Let's see it."], "metadata": {"_uuid": "2ff722d34539428419e04f4bef08717f39267306", "_cell_guid": "9a2fa80d-bb68-4980-a952-f22f8551772a"}}, {"outputs": [], "cell_type": "code", "metadata": {"_uuid": "30475762a1d56d2f404571c631fc18c653261d01", "collapsed": true, "_cell_guid": "414caeae-bc39-4c4c-a813-893d2d688b84"}, "source": ["df.Cabin.head(10)"], "execution_count": null}, {"cell_type": "markdown", "source": ["And create the dummies for it..."], "metadata": {"_uuid": "dac5ddac3d97cbcc5406281f78256e8c87ce0882", "_cell_guid": "4e8967a1-62bb-443c-b345-e209a1864af6"}}, {"outputs": [], "cell_type": "code", "metadata": {"_uuid": "bb8f0d3caee0cbe288fd7d190ee68afc865c4431", "collapsed": true, "_cell_guid": "f0c8f0e8-79ac-4c2a-af12-a0f165ae970e"}, "source": ["df = pd.get_dummies(df, columns = [\"Cabin\"],prefix=\"Cabin\")"], "execution_count": null}, {"cell_type": "markdown", "source": ["Let's do the same for 'Ticket'"], "metadata": {"_uuid": "c71474dc0db3d333191d9efd39e840590cf19b6c", "_cell_guid": "ca01f5bb-9966-4141-9359-12a9496f1946"}}, {"outputs": [], "cell_type": "code", "metadata": {"_uuid": "035e4d9f3df05ba728bbd005e30698ac0b843863", "collapsed": true, "_cell_guid": "1387432a-17a2-498d-85ed-f17c80287fb0"}, "source": ["df.Ticket.head(40)"], "execution_count": null}, {"cell_type": "markdown", "source": ["Let's extract the first letter of the ticket, which may be indicative of group reservations, and whenever it's not possible to find one letter, input 'X' value, as I've done with the 'Cabin' missing treatment."], "metadata": {"_uuid": "2807e7eacc018182392ec4fe087809e5a7a69719", "_cell_guid": "3fde2ae8-7f6e-45d7-a920-f7806bf41132"}}, {"outputs": [], "cell_type": "code", "metadata": {"_uuid": "f3c5411c6ec8809a6a7b000bdf8b5eb46f943f87", "collapsed": true, "_cell_guid": "2d435356-c70c-4e30-8af4-5bd27903dace"}, "source": ["## Treat Ticket by extracting the ticket prefix. When there is no prefix it returns X. \n", "\n", "Ticket = []\n", "for i in list(df.Ticket):\n", "    if not i.isdigit() :\n", "        Ticket.append(i.replace(\".\",\"\").replace(\"/\",\"\").strip().split(' ')[0]) #Take prefix\n", "    else:\n", "        Ticket.append(\"X\")\n", "        \n", "df[\"Ticket\"] = Ticket\n", "df[\"Ticket\"].head(40)"], "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {"_uuid": "68c060aa01cba441638384a89ba734c201ae2a68", "collapsed": true, "_cell_guid": "c9c75b24-ddd4-4537-86f8-f041d52206df"}, "source": ["#Generating the dummies under the prefix 'T'\n", "df = pd.get_dummies(df, columns = [\"Ticket\"], prefix=\"T\")\n", "#plotting a graph\n", "df.head()"], "execution_count": null}, {"cell_type": "markdown", "source": ["Engineering 'Family' size"], "metadata": {"_uuid": "918a69bc77f9fa32aea451097c985266e049dd82", "_cell_guid": "aafe1e94-f42c-4edb-b9e6-4b75e184267b"}}, {"cell_type": "markdown", "source": ["We can imagine that large families will have more difficulties to evacuate, looking for theirs sisters/brothers/parents during the evacuation. So, in the file I forked, the author choosed to create a \"Fize\" (family size) feature which is the sum of SibSp , Parch and 1 (including the passenger). It's a good idea, let's do it."], "metadata": {"_uuid": "aa51eac28a4545d824fa92a77676e3485b369147", "_cell_guid": "5b484538-9181-49a0-a4d8-fe66262981e4"}}, {"outputs": [], "cell_type": "code", "metadata": {"_uuid": "3caedbb01434af8c2f88bf3413cb35c6a1f8d414", "collapsed": true, "_cell_guid": "551356df-593b-44a9-8700-f92dc8c3a640"}, "source": ["# Create a family size descriptor from SibSp and Parch\n", "df[\"Fsize\"] = df[\"SibSp\"] + df[\"Parch\"] + 1"], "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {"_uuid": "0dc671708f7f83080aa0a93202b6d9f80d94cfe6", "collapsed": true, "_cell_guid": "6aa29fc4-e19d-4029-88c1-6342cc11de43"}, "source": ["g = sns.factorplot(x=\"Fsize\",y=\"Survived\",data = df)\n", "g = g.set_ylabels(\"Survival Probability\")"], "execution_count": null}, {"cell_type": "markdown", "source": ["Some models work better if we 'help' them segmenting the space of the possible levels for the variable. Let's create 4 types of Fsize:\n", "* Single individuals (1)\n", "* Couples or Parent-kid, Parent-rellative (2)\n", "* Medium families (up to 4 individuals)\n", "* Large Families (5+)\n"], "metadata": {"_uuid": "ba53a8e30105b486df9209cbe9ff09fdde50d99f", "_cell_guid": "6a3ce807-e447-430f-ab77-0a6988723394"}}, {"outputs": [], "cell_type": "code", "metadata": {"_uuid": "6ba8d70021243e42d7f60760a4edf549f3138b10", "collapsed": true, "_cell_guid": "0666a785-ea2a-4aec-a266-1ff0133f20ae"}, "source": ["# Create new feature of family size\n", "df['Single'] = df['Fsize'].map(lambda s: 1 if s == 1 else 0)\n", "df['SmallF'] = df['Fsize'].map(lambda s: 1 if  s == 2  else 0)\n", "df['MedF'] = df['Fsize'].map(lambda s: 1 if 3 <= s <= 4 else 0)\n", "df['LargeF'] = df['Fsize'].map(lambda s: 1 if s >= 5 else 0)\n", "\n", "g = sns.factorplot(x=\"Single\",y=\"Survived\",data=df,kind=\"bar\")\n", "g = g.set_ylabels(\"Survival Probability\")\n", "g = sns.factorplot(x=\"SmallF\",y=\"Survived\",data=df,kind=\"bar\")\n", "g = g.set_ylabels(\"Survival Probability\")\n", "g = sns.factorplot(x=\"MedF\",y=\"Survived\",data=df,kind=\"bar\")\n", "g = g.set_ylabels(\"Survival Probability\")\n", "g = sns.factorplot(x=\"LargeF\",y=\"Survived\",data=df,kind=\"bar\")\n", "g = g.set_ylabels(\"Survival Probability\")"], "execution_count": null}, {"cell_type": "markdown", "source": ["Now it's time for discretizing numerical variables, like 'Age'"], "metadata": {"_uuid": "069a1e14542943744e7da5f7aa70a984ab780c59", "_cell_guid": "57bcc4f6-828d-48d7-8da2-33064b6d407f"}}, {"outputs": [], "cell_type": "code", "metadata": {"_uuid": "a7c3b14d316fd9fc16e3438fbc3466bad1a87768", "collapsed": true, "_cell_guid": "f441abc0-dfd5-46b4-bf49-a3cbb72aeb63"}, "source": ["# Explore Age vs Survived\n", "g = sns.FacetGrid(df, col='Survived')\n", "g = g.map(sns.distplot, \"Age\")"], "execution_count": null}, {"cell_type": "markdown", "source": ["Visually, I am defining segments of 'Age', trying to capture possible dynamics of suvival probability"], "metadata": {"_uuid": "ca2c0113f7d72e12d6671ea0c21c71917ee8aa56", "_cell_guid": "a77d1dec-a16f-41fe-b7a5-731f97feef97"}}, {"outputs": [], "cell_type": "code", "metadata": {"_uuid": "08b29467407ec51f5b55ce6ee65cfd2444730e48", "collapsed": true, "_cell_guid": "0a401c7e-55a1-43b0-9921-29d498e3df16"}, "source": ["# Create new feature of Age\n", "df['Child'] = df['Age'].map(lambda s: 1 if s<=10 else 0)\n", "df['Young'] = df['Age'].map(lambda s: 1 if  10 < s <= 30   else 0)\n", "df['Senior'] = df['Age'].map(lambda s: 1 if 30 < s <= 40 else 0)\n", "df['Elder'] = df['Age'].map(lambda s: 1 if s > 40 else 0)\n", "\n", "g = sns.factorplot(x=\"Child\",y=\"Survived\",data=df,kind=\"bar\")\n", "g = g.set_ylabels(\"Survival Probability\")\n", "g = sns.factorplot(x=\"Young\",y=\"Survived\",data=df,kind=\"bar\")\n", "g = g.set_ylabels(\"Survival Probability\")\n", "g = sns.factorplot(x=\"Senior\",y=\"Survived\",data=df,kind=\"bar\")\n", "g = g.set_ylabels(\"Survival Probability\")\n", "g = sns.factorplot(x=\"Elder\",y=\"Survived\",data=df,kind=\"bar\")\n", "g = g.set_ylabels(\"Survival Probability\")"], "execution_count": null}, {"cell_type": "markdown", "source": ["It's time for the numerical variable 'Fare':"], "metadata": {"_uuid": "089b0fd1da22aef1315a7f6f69b881abece730ad", "_cell_guid": "e2646d88-3755-409e-b3fb-de288ef40991"}}, {"outputs": [], "cell_type": "code", "metadata": {"_uuid": "8c62ff360acaed44047a7df5a12801a93434b516", "collapsed": true, "_cell_guid": "bab4f727-d427-4745-8882-1f05d418a158"}, "source": ["# Explore Fare vs Survived\n", "g = sns.FacetGrid(df, col='Survived', size=10)\n", "g = g.map(sns.distplot, \"Fare\")"], "execution_count": null}, {"cell_type": "markdown", "source": ["Applying the same logic for 'Fare'\n"], "metadata": {"_uuid": "de9a703bedca7afd157beafc6ebf706863e8f5c7", "_cell_guid": "55047b0f-00bd-42fb-adce-0f6f992d573e"}}, {"outputs": [], "cell_type": "code", "metadata": {"_uuid": "7ff5703a33fce142b2a2ff7aef4fc429a56adbc4", "collapsed": true, "_cell_guid": "1dd4fde4-2131-4d41-857c-a6614a60a9f4"}, "source": ["# Create new feature of Fare\n", "df['F_Low'] = df['Fare'].map(lambda s: 1 if s<=50 else 0)\n", "df['F_Med'] = df['Fare'].map(lambda s: 1 if  50 < s <= 100   else 0)\n", "df['F_High'] = df['Fare'].map(lambda s: 1 if 100 < s <= 200 else 0)\n", "df['F_Ultra'] = df['Fare'].map(lambda s: 1 if s > 200 else 0)\n", "\n", "g = sns.factorplot(x=\"F_Low\",y=\"Survived\",data=df,kind=\"bar\")\n", "g = g.set_ylabels(\"Survival Probability\")\n", "g = sns.factorplot(x=\"F_Med\",y=\"Survived\",data=df,kind=\"bar\")\n", "g = g.set_ylabels(\"Survival Probability\")\n", "g = sns.factorplot(x=\"F_High\",y=\"Survived\",data=df,kind=\"bar\")\n", "g = g.set_ylabels(\"Survival Probability\")\n", "g = sns.factorplot(x=\"F_Ultra\",y=\"Survived\",data=df,kind=\"bar\")\n", "g = g.set_ylabels(\"Survival Probability\")"], "execution_count": null}, {"cell_type": "markdown", "source": ["The value paid really made difference when the subject is surviving onboard Titanic."], "metadata": {"_uuid": "4edb5cf089bc7d47adf989a6f087bd146b4dbbc3", "_cell_guid": "28d22ee1-05d2-4656-9e2c-849d55ccd381"}}, {"cell_type": "markdown", "source": ["## 5. Dataset preparation for Model"], "metadata": {"_uuid": "6109a5646307b2f34789f92728c6fce5983e5cbe", "_cell_guid": "380ea6c1-2aa7-41b2-85cc-a0dd982a3ede"}}, {"cell_type": "markdown", "source": ["I am not going to explore very well the possible exploratory analysis. Let's just ensure there's no missing values in the dataset, all dummies were created and we have sufficiently engineered all possible variables.\n", "\n", "If you want to understand better the variables, consult the following kernels, which inspired me a lot:\n", "* Yassine Ghouzam, PhD: \"Titanic Top 4% with ensemble modeling\"\n", "* LD Freeman, from his kernel: \"A Data Science Framework: To Achieve 99% Accuracy\". \n", "\n"], "metadata": {"_uuid": "affc0685e62c1aa96f81157d3cf7473e14265e38", "_cell_guid": "3f6d3125-aa2d-4b2b-916a-2a1bcdf27046"}}, {"cell_type": "markdown", "source": ["### 5.1 Macro overview"], "metadata": {"_uuid": "168c9b704e4ce08d15cd9e38b3a76e3c22974524", "_cell_guid": "f1ac32e7-5990-43aa-b6bf-987b8724ed2d"}}, {"outputs": [], "cell_type": "code", "metadata": {"_uuid": "a84ed2db989689f19f05684333375338044b367a", "collapsed": true, "_cell_guid": "5e3be1d3-25f7-4f06-905d-12981fb6c3ff"}, "source": ["df.describe()"], "execution_count": null}, {"cell_type": "markdown", "source": ["### 5.2 Outlier detection"], "metadata": {"_uuid": "f927619c2c668bb1004aa0cce909a9732b2626fd", "_cell_guid": "06c79bd7-6180-4c42-baef-8632bd1b27a1"}}, {"cell_type": "markdown", "source": ["Outliers do have an effect in training your model, let's take a look:\n"], "metadata": {"_uuid": "c4681579b062a6ff6085a53891d089cc63ea834d", "_cell_guid": "47e324b2-b9ad-442b-a181-1701c2eb5a09"}}, {"outputs": [], "cell_type": "code", "metadata": {"_uuid": "85a3c96713afbc48fbe2489d6872917743b220d8", "_execution_state": "idle", "collapsed": true, "_cell_guid": "6fde3971-ffa0-423c-991f-15f8502fe5bb"}, "source": ["# Outlier detection \n", "\n", "def detect_outliers(df,n,features):\n", "    \"\"\"\n", "    Takes a dataframe df of features and returns a list of the indices\n", "    corresponding to the observations containing more than n outliers according\n", "    to the Tukey method.\n", "    \"\"\"\n", "    outlier_indices = []\n", "    \n", "    # iterate over features(columns)\n", "    for col in features:\n", "        # 1st quartile (25%)\n", "        Q1 = np.percentile(df[col], 25)\n", "        # 3rd quartile (75%)\n", "        Q3 = np.percentile(df[col],75)\n", "        # Interquartile range (IQR)\n", "        IQR = Q3 - Q1\n", "        \n", "        # outlier step\n", "        outlier_step = 1.5* IQR\n", "        \n", "        # Determine a list of indices of outliers for feature col\n", "        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n", "        \n", "        # append the found outlier indices for col to the list of outlier indices \n", "        outlier_indices.extend(outlier_list_col)\n", "        \n", "    # select observations containing more than 2 outliers\n", "    outlier_indices = Counter(outlier_indices)        \n", "    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n", "    \n", "    return multiple_outliers   \n", "\n", "# detect outliers from Age, SibSp , Parch and Fare\n", "Outliers_to_drop = detect_outliers(df,2,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"])"], "execution_count": null}, {"cell_type": "markdown", "source": ["It's Tukey method for detecting outliers. We could use multiples of standard deviation, but this method seemed to me a good first approach."], "metadata": {"_uuid": "18484f27ce1d2c1a666d683be1c3eda8e1181b10", "_cell_guid": "095f3dd7-b424-4aa4-a59b-d3de79a59b7b"}}, {"outputs": [], "cell_type": "code", "metadata": {"_uuid": "0697589355a97c2fc8db712eaf3c35288357545d", "scrolled": false, "_execution_state": "idle", "collapsed": true, "_cell_guid": "d2eb9251-5ba8-4852-890e-a4788ac916af"}, "source": ["df.loc[Outliers_to_drop] # Show the outliers rows"], "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {"_uuid": "da54c86e727b724f10712e5ecd8b9b70702a18aa", "collapsed": true, "_cell_guid": "a6216063-f575-4167-8f0d-684129bd0f4c"}, "source": ["len(df.loc[Outliers_to_drop])/len(df)\n"], "execution_count": null}, {"cell_type": "markdown", "source": ["We detect 19 outliers and they represent less than 1.45% of the dataset. I think personally the effect can be negleted because the most impacting variables were 'Fare' with high values and 'SibSp' particulary with the value 8. It's a decision, we can always go back and refine the model aplying some type of outlier filtering\n", "\n"], "metadata": {"_uuid": "986bd26f4a01c0c6ec70a636331abb8e1cc74e24", "_cell_guid": "3b90ec87-cae4-4872-873f-49d2c96564c4"}}, {"cell_type": "markdown", "source": ["### 5.3 Train x test dataframe splitting"], "metadata": {"_uuid": "215ba3106019e77e8358d3d381c5ff4654ef58d7", "_cell_guid": "fbb72d6a-a7e0-45a6-b721-19ef44933983"}}, {"outputs": [], "cell_type": "code", "metadata": {"_uuid": "49096bcc2748d4d2a13df57264647cd804cafea5", "_execution_state": "idle", "collapsed": true, "_cell_guid": "a04185b2-d152-4888-99b8-9fcf384ce9e4"}, "source": ["## Separate train dataset and test dataset\n", "train = df[df.Survived.notnull()]\n", "test = df[df.Survived.isnull()]\n", "train.head(20)"], "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {"_uuid": "199dc9b29ddc5a4fa4e39fa3ffd8210b0ff6969f", "collapsed": true, "_cell_guid": "a25f5d5b-5184-4aea-8418-7a49fbafad78"}, "source": ["#let's check the test dataframe\n", "test.Survived.head(10)"], "execution_count": null}, {"cell_type": "markdown", "source": ["### 5.4 Dropping irrelevant variables "], "metadata": {"_uuid": "7f86e26327aca57944300c493a4e35aca79741a4", "_cell_guid": "562dfc9b-554b-4e58-b0f9-79cf023482a4"}}, {"cell_type": "markdown", "source": ["Understanding wich are the categorical variables to drop. Let's check if we have created dummies for all of them."], "metadata": {"_uuid": "d8cdd396c052f1d554ea7b89bffd7541095163dc", "_cell_guid": "eb60c55d-5134-41a7-ae9f-6cc389d675f4"}}, {"outputs": [], "cell_type": "code", "metadata": {"_uuid": "7bd729e8ae76a71422d2285235580351ec1af40f", "collapsed": true, "_cell_guid": "b1bb3093-b959-475b-b518-616cba7d7772"}, "source": ["print(train.info())\n"], "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {"_uuid": "810546330adc53b23ffcd4667bfc6cd1a0dff500", "collapsed": true, "_cell_guid": "c8e42350-4c56-4d88-bde0-cf5f62d2a603"}, "source": ["## Separate train features and label \n", "\n", "train[\"Survived\"] = train[\"Survived\"].astype(int)\n", "\n", "Y_train = train[\"Survived\"]\n", "\n", "X_train = train.drop(labels = [\"Survived\"],axis = 1)\n", "\n", "# Drop useless input variables \n", "X_train.drop(labels = [\"PassengerId\", \"Sex\",\"Name\", \"Title\", \"Embarked\"], axis = 1, inplace = True)\n", "print(X_train.info())"], "execution_count": null}, {"cell_type": "markdown", "source": ["All variables are numeric. Great!"], "metadata": {"_uuid": "eea0def47daeaf891dfb1fad97e5af177da496a7", "_cell_guid": "14dffa9e-1542-48f3-9bba-ad97f5f706eb"}}, {"cell_type": "markdown", "source": ["## 6. MODELING"], "metadata": {"_uuid": "cffc397b025d6895efc48d066b711a3983d5bf79", "_cell_guid": "72e668d1-11ff-4d9a-91c9-4f568aba9a81"}}, {"cell_type": "markdown", "source": ["### 6.1 Simple modeling\n", "#### 6.1.1 Cross validate models\n", "\n", "I compared the most  popular classifiers and evaluate the mean accuracy of each of them by a stratified kfold cross validation procedure.\n"], "metadata": {"_uuid": "c541e1052c6fb529de2fd42ae5166d735bcdecd5", "_cell_guid": "31276c4e-a992-4f42-b9d2-fe48ac3f0733"}}, {"outputs": [], "cell_type": "code", "metadata": {"_uuid": "2c660f943c3f97f08f1c6179cb4e82cab5436eb3", "_execution_state": "idle", "collapsed": true, "_cell_guid": "7e809ec9-cb78-4aad-85d8-1543515e31c7"}, "source": ["# Cross validate model with Kfold stratified cross val\n", "kfold = StratifiedKFold(n_splits=10)"], "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {"_uuid": "3b1cf6c8671e432c222d8fd8058049464d0fed86", "scrolled": true, "_execution_state": "idle", "collapsed": true, "_cell_guid": "aa6e6f87-95a0-447b-928d-b7f3508af494"}, "source": ["# Modeling step Test differents algorithms \n", "random_state = 2\n", "classifiers = []\n", "classifiers.append(SVC(random_state=random_state))\n", "classifiers.append(DecisionTreeClassifier(random_state=random_state))\n", "classifiers.append(AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state),random_state=random_state,learning_rate=0.1))\n", "classifiers.append(RandomForestClassifier(random_state=random_state))\n", "classifiers.append(ExtraTreesClassifier(random_state=random_state))\n", "classifiers.append(GradientBoostingClassifier(random_state=random_state))\n", "classifiers.append(MLPClassifier(random_state=random_state))\n", "classifiers.append(KNeighborsClassifier())\n", "classifiers.append(LogisticRegression(random_state = random_state))\n", "classifiers.append(LinearDiscriminantAnalysis())\n", "\n", "\n", "cv_results = []\n", "for classifier in classifiers :\n", "    cv_results.append(cross_val_score(classifier, X_train, y = Y_train, scoring = \"accuracy\", \n", "                                      cv = kfold, n_jobs=4))\n", "\n", "cv_means = []\n", "cv_std = []\n", "for cv_result in cv_results:\n", "    cv_means.append(cv_result.mean())\n", "    cv_std.append(cv_result.std())\n", "\n", "cv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\n", "                       \"CrossValerrors\": cv_std,\"Algorithm\":[\"SVC\",\"DecisionTree\",\n", "                                                             \"AdaBoost\",\"RandomForest\",\"ExtraTrees\",\n", "                                                             \"GradientBoosting\",\"MultipleLayerPerceptron\",\n", "                                                             \"KNeighboors\",\"LogisticRegression\",\n", "                                                             \"LinearDiscriminantAnalysis\"]})\n", "\n", "g = sns.barplot(\"CrossValMeans\",\"Algorithm\",data = cv_res, palette=\"Set3\",orient = \"h\",**{'xerr':cv_std})\n", "g.set_xlabel(\"Mean Accuracy\")\n", "g = g.set_title(\"Cross validation scores\")\n"], "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {"_uuid": "29ab679cc3635cd078df4ac4d5bc02b1677cd487", "collapsed": true, "_cell_guid": "15546427-d62e-4a9f-81ee-aa14a41ddac9"}, "source": ["cv_res"], "execution_count": null}, {"cell_type": "markdown", "source": ["I decided to choose the Linear Discriminant analysis, Random Forest, Logistic regression and Gradient Boosting for tuning by GradientSearchCV."], "metadata": {"_uuid": "a6dc3fe31873558eb587819b338499e9dcc27f2e", "_cell_guid": "830de6e5-49d1-45f4-8240-296f4f6503d4"}}, {"cell_type": "markdown", "source": ["#### 6.1.2 Hyperparameter tunning for best models\n", "\n", "I performed a grid search optimization for the tree-based algorithms."], "metadata": {"_uuid": "974fa2316b3ea11f29031561125110d0c4754b38", "_cell_guid": "0b896759-62ed-4cfd-8716-3684918e8c14"}}, {"outputs": [], "cell_type": "code", "metadata": {"_uuid": "89b4b827d0395b0621a843e2b726ecde50d4431c", "_execution_state": "idle", "collapsed": true, "_cell_guid": "ea436284-44e9-4f12-bbfa-972da4434ac9"}, "source": ["# RFC Parameters tunning \n", "RFC = RandomForestClassifier()\n", "\n", "## Search grid for optimal parameters\n", "rf_param_grid = {\"max_depth\": [None],\n", "              \"max_features\": [1, 3, 10],\n", "              \"min_samples_split\": [2, 3, 10],\n", "              \"min_samples_leaf\": [1, 3, 10],\n", "              \"bootstrap\": [False],\n", "              \"n_estimators\" :[100,300],\n", "              \"criterion\": [\"gini\"]}\n", "\n", "\n", "gsRFC = GridSearchCV(RFC,param_grid = rf_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n", "\n", "gsRFC.fit(X_train,Y_train)\n", "\n", "RFC_best = gsRFC.best_estimator_\n", "#Best set of parameters\n", "print(RFC_best)\n", "# Best score\n", "print(gsRFC.best_score_)"], "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {"_uuid": "48acde1fd7bfc7e20fb7f57903a8315cdb94ba82", "_execution_state": "idle", "collapsed": true, "_cell_guid": "4560bcb5-7d2b-4f6b-a935-59dc4a02072b"}, "source": ["# Gradient boosting tunning\n", "\n", "GBC = GradientBoostingClassifier()\n", "gb_param_grid = {'loss' : [\"deviance\"],\n", "              'n_estimators' : [100,200,300],\n", "              'learning_rate': [0.1, 0.05, 0.01],\n", "              'max_depth': [4, 8],\n", "              'min_samples_leaf': [100,150],\n", "              'max_features': [0.3, 0.1] \n", "              }\n", "\n", "gsGBC = GridSearchCV(GBC,param_grid = gb_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n", "\n", "gsGBC.fit(X_train,Y_train)\n", "\n", "GBC_best = gsGBC.best_estimator_\n", "#Best set of parameters\n", "print(GBC_best)\n", "# Best score\n", "print(gsGBC.best_score_)\n"], "execution_count": null}, {"cell_type": "markdown", "source": ["### 6.2 Ensemble modeling\n", "#### 6.2.1 Combining models\n", "\n", "I choosed a voting classifier to combine the predictions coming from the 4 classifiers.\n", "\n", "I preferred to pass the argument \"soft\" to the voting parameter to take into account the probability of each vote."], "metadata": {"_uuid": "ad5a1f0cd14b8fffa751b0494407f854f62183ae", "_cell_guid": "ecb005b7-918a-4e4a-ac6d-5f26c27d46fc"}}, {"outputs": [], "cell_type": "code", "metadata": {"_uuid": "0bce0c7d5bddcfa8b53a855576c1bc084adcfcd9", "_execution_state": "idle", "collapsed": true, "_cell_guid": "9e042586-ecc1-4aa4-96d9-050ad029fcc2"}, "source": ["#ignore warnings\n", "import warnings\n", "warnings.filterwarnings('ignore')\n", "print('-'*25)\n", "\n", "LDA =LinearDiscriminantAnalysis()\n", "logitR = LogisticRegression(random_state = random_state)\n", "votingC = VotingClassifier(estimators=[('rfc', RFC_best), ('logitR', logitR),\n", "('LDA', LDA),('gbc',GBC_best)], voting='soft', n_jobs=4,weights=[1,1,1,1])\n", "\n", "votingC = votingC.fit(X_train, Y_train)\n", "\n", "for clf, label in zip([RFC_best, logitR, LDA, GBC_best, votingC],\n", "                      ['rfc','logitR', 'lda','gbc', 'soft voting']):\n", "    scores = cross_val_score(clf, X_train, Y_train, cv=5, scoring='accuracy',pre_dispatch=4)\n", "    print(\"Accuracy: %0.4f (+/- %0.4f) [%s]\" % (scores.mean(), scores.std(), label))"], "execution_count": null}, {"cell_type": "markdown", "source": ["    You can see that in some cases voting classifier can be no better than the best model you've just tuned. Let's change the weights..."], "metadata": {"_uuid": "22760ab29521bfb5aa6fa81c7c4b65021309e92f", "_cell_guid": "aff0b11d-1e1a-4656-8bec-832e75451b86"}}, {"outputs": [], "cell_type": "code", "metadata": {"_uuid": "d050cd4eb0f7b8dd056362e675d8eb42c89e1b1c", "collapsed": true, "_cell_guid": "b30d92c0-0260-4f05-aa64-ddf6a433f6a2"}, "source": ["LDA =LinearDiscriminantAnalysis()\n", "logitR = LogisticRegression(random_state = random_state)\n", "votingC = VotingClassifier(estimators=[('rfc', RFC_best), ('logitR', logitR),\n", "('LDA', LDA),('gbc',GBC_best)], voting='soft', n_jobs=4,weights=[1,1,1,2])\n", "\n", "votingC = votingC.fit(X_train, Y_train)\n", "\n", "for clf, label in zip([RFC_best, logitR, LDA, GBC_best, votingC],\n", "                      ['rfc','logitR', 'lda','gbc', 'soft voting']):\n", "    scores = cross_val_score(clf, X_train, Y_train, cv=5, scoring='accuracy',pre_dispatch=4)\n", "    print(\"Accuracy: %0.4f (+/- %0.4f) [%s]\" % (scores.mean(), scores.std(), label))"], "execution_count": null}, {"cell_type": "markdown", "source": ["Let's try another strategy: Stacking"], "metadata": {"_uuid": "93c9eec434e8772b7af5ea23a2059890c3b9709b", "_cell_guid": "6768a410-a126-42b8-932d-9631be7908a8"}}, {"outputs": [], "cell_type": "code", "metadata": {"_uuid": "48ae87ebebf480f6e9d04d5ac6ea52422fa6ef50", "collapsed": true, "_cell_guid": "47cd90b6-6120-4b73-a5d8-7a7845618e1c"}, "source": ["lgr = LogisticRegression()\n", "sclf = StackingClassifier(classifiers=[RFC_best, logitR, LDA, GBC_best, votingC], \n", "                          meta_classifier=lgr)\n", "\n", "print('10-fold cross validation:\\n')\n", "\n", "for clf, label in zip([RFC_best, logitR, LDA, GBC_best, votingC, sclf], \n", "                      ['rfc','logitR', 'lda','gbc', 'voting', 'stacking']):\n", "\n", "    scores = cross_val_score(clf, X_train, Y_train, \n", "                                              cv=5, scoring='accuracy')\n", "    print(\"Accuracy: %0.4f (+/- %0.4f) [%s]\" \n", "          % (scores.mean(), scores.std(), label))"], "execution_count": null}, {"cell_type": "markdown", "source": ["If we run an ANOVA we couldn't say one model predicts better than other, so let's take the voting Classifier as the final model..."], "metadata": {"_uuid": "68e819858a9605ac08062986516c595803a1d670", "collapsed": true, "_cell_guid": "37ab751d-5c36-45c4-af93-ad951a5fd5a8"}}, {"cell_type": "markdown", "source": ["### 6.3 Prediction\n", "#### 6.3.1 Predict and Submit results"], "metadata": {"_uuid": "5c54037462909e4de5a6e3d2bada10054e5b1989", "_cell_guid": "5a4df7a1-00b4-4613-95e7-a99e37169c4f"}}, {"outputs": [], "cell_type": "code", "metadata": {"_uuid": "01741c8f6bfcd2a40f9a1e262bff5ab9a54355c4", "collapsed": true, "_cell_guid": "f50390eb-08f3-47e7-8dd0-c1ff3589871a"}, "source": ["test.head()"], "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {"_uuid": "6c2a0b11c445b2e9be60493c6d5b33f524057b25", "collapsed": true, "_cell_guid": "909625be-d501-452c-859b-f1a4de149524"}, "source": ["## Separate train features and label \n", "X_test = test.drop(labels = [\"Survived\"],axis = 1)\n", "\n", "# Drop useless input variables \n", "X_test.drop(labels = [\"PassengerId\", \"Sex\",\"Name\", \"Title\", \"Embarked\"], axis = 1, inplace = True)\n", "print(X_test.info())"], "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {"_uuid": "05a908322774e752b86a86275740044c45bd48b4", "_execution_state": "idle", "collapsed": true, "_cell_guid": "32abc9b7-f58d-4aa2-b002-25a84138e01c"}, "source": ["test['Survived']=votingC.predict(X_test)\n", "\n", "results =  pd.concat([test[['PassengerId', 'Survived']]], axis=1)\n", "\n", "results.to_csv(\"output_python_voting.csv\",index=False)\n"], "execution_count": null}, {"outputs": [], "cell_type": "code", "metadata": {"_uuid": "d0f07a916eda27722c3e61bec21de4247b18efe9", "collapsed": true, "_cell_guid": "13b8be0f-46c1-44da-a073-01f7c2e2e6d9"}, "source": ["results.head()"], "execution_count": null}, {"cell_type": "markdown", "source": ["Thank you very much for reading it!!"], "metadata": {"_uuid": "d2702ee58d3bd110e462b1be557ac7087197df12", "_cell_guid": "7f4d4b84-1571-42b2-88d7-82df3c376fcf"}}]}