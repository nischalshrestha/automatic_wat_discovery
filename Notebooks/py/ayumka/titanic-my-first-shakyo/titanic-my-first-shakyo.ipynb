{"cells":[{"metadata":{"_uuid":"0e08170347cf0be909ed43a8285374a4e9e7e138"},"cell_type":"markdown","source":"# Thank you ldfreeman3!\nHi, I'm Ayumu from Japan! I'm interested in python & machine learning. Thanks to [ldfreeman3's beginner-friendly kernel](https://www.kaggle.com/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy/notebook), I could walk into kaggle<3 \n\nI could reach submitting CSV, but almost all parts I still haven't understood:(  So I will learn the contents more deeply and add comment on my kernel little by little, day by day ⌒°(๑°◡°๑)°⌒ I welcome and appreciate any comments!\n\nldfreeman3さん、ありがとうございます(*≧∀≦) あなたのおかげでTitanicの生存者を予測して提出するところまで漕ぎ着けることができました! しかし写経なので内容がまだまったく頭に入っていないので、これから少しずつコードを解釈していこうと思いますＹ(๑°口°๑) どんなコメントも歓迎です^^/\n\n# import sys,  __version__, warnings...?\nIn[1] の解説をします。コードが見たい方は下へとスクロールしてください。\n`**import sys**`\n\n`sys` とは、インタプリタや実行環境に関連した変数や関数がまとめられたPython の標準ライブラリです。`sys.path` でモジュール検索パスを確認可能。以下のcodeセルでは`sys.version` を使ってPythonのバージョンを表示させています。\n\n`**print(\"pandas version: {}\".format(pd.__version__))**`\n\n__version__属性でバージョン番号が取得できます。`\"文字列\".format()` と書くと`.format()` 内の情報を`{}`があるところに表示してくれます。\n\n\n\n```python\nimport warnings\nwarnings.filterwarnings('ignore')\n```\n\nUserWarningなど、表示しなくても問題ない警告はこれを使うと表示されなくなります。スクロースが必要なほどずらずら警告が出てきたセルにこれを入れるときれいさっぱり表示されなくなって重宝します。\n\nさて、これらの前提知識を仕入れて次のcodeセルを読んでみてください。"},{"metadata":{"trusted":true,"_uuid":"6ba5558d458456bd67a4c8fa3a66b9b6e7d062ad"},"cell_type":"code","source":"import sys\nprint(\"Python version: {}\".format(sys.version))\nimport pandas as pd\nprint(\"pandas version: {}\".format(pd.__version__))\nimport matplotlib\nprint(\"matplotlib version: {}\".format(matplotlib.__version__))\n\nimport numpy as np\nprint(\"Numpy version: {}\".format(np.__version__))\nimport scipy as sp\nprint(\"SciPy version: {}\".format(sp.__version__))\nimport IPython\nfrom IPython import display\nprint(\"IPython version: {}\".format(IPython.__version__))\n\nimport sklearn\nprint(\"scikit-learn version: {}\".format(sklearn.__version__))\nimport random\nimport time\n\nimport warnings\nwarnings.filterwarnings('ignore')\nprint('-'*25)\n\nimport os\nprint(os.listdir('../input'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"79ab7e9bd369a77d46e714cc704229e18bedfcd3"},"cell_type":"markdown","source":"# sklearn、xgboost、matplotlibなどをインポート\nIn[2]の解説をしていきます。\n## sklearnからimportするもの\n```python\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n```\nここでインポートするモジュールは8個です。\nちなみにsklearnはPythonの機械学習ライブラリです。\n1.\tsklearn.svm\nSupport Vector Machine (SVM) アルゴリズムを扱うモジュール。\nSVM: 教師あり学習を用いるパターン認識モデル。\nもっと見る: [sklearn.svm](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.svm)\n2.\tsklearn.tree\n分類と回帰用の決定木ベースモデルを扱うモジュール。\nもっと見る: [sklearn.tree](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.tree)\n3.\tsklearn.linear_model\n一般化線形モデル (generalized linear model) を実行するモジュール。\nもっと見る: [sklearn.linear_model](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model)\n4.\tsklearn.neighbors\nk近傍法 (k-nearest neighbor algorithm) を実行するモジュール。k近傍法とは特徴空間における最も近い訓練例に基づいた分類の手法であり、パターン認識でよく使われる。最近傍探索問題の一つ。\nもっと見る: [sklearn.neighbors](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.neighbors)\n5.\tsklearn.naive_bayes\n単純ベイズ (Naive Bayes) アルゴリズムを実行するモジュール。\nもっと見る: [sklearn.naive_bayes](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.naive_bayes)\n6.\tsklearn.ensemble\n分類、回帰、異常検知 (anomaly detection) 用のアンサンブルベースメソッドを実行するモジュール。\nもっと見る: [sklearn.ensemble](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble)\n7.\tsklearn.discriminant_analysis\n線形判別分析 (Linear Discriminant Analysis, LDA) と2次判別 (Quadratic Discriminant Analysis）を実行するモジュール。\nもっと見る: [sklearn.discriminant_analysis](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.discriminant_analysis)\n8.\tsklearn.gaussian_process\n回帰と分類に基づいたガウス過程を実行するモジュール。\nもっと見る: [sklearn.gaussian_process](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.gaussian_process)\n\n## xgboostからXGBClassifierをインポート\nXGBoostは、高速な処理、外れ値や欠損値に強い点が持ち味らしいのですが、残念ながらよくわかりませんでしたf^^; 時間が溶けるので飛ばします。上手に説明がしてあるサイトなどあれば教えてくださいm(_)m \nXGBoostは、最も有力で・競争力のある機械学習である速さとパフォーマンスのために設計された勾配ブースト決定木の実装です。モデルの訓練に使います。\nXGBoost参考サイト: [Machine Learning Mastery](https://machinelearningmastery.com/develop-first-xgboost-model-python-scikit-learn/)\n## sklearn.preprocessingからOneHotEncoderとLabelEncoder をインポート\n\n sklearn.preprocessingパッケージは、生データを前処理するために使います。\n - OneHotEncoder: カテゴリの整数特性をOne Hot表現 (1つだけHigh(1)であり、他はLow(0)であるようなビット列) を使ってエンコードします。\n - LabelEncoder: 0 と n_classes-1 間の値でラベルをエンコードします。\n \n ## もう一度sklearnからimportするもの\n - feature_selection: 特徴選択アルゴリズムを実装するモジュール。\n - model_selection: model_selectionそのものについて説明しているものなし。\n - metrics: 作成したモデルの評価を行うモジュール\n \n # matplotlibとは\n\n"},{"metadata":{"trusted":true,"_uuid":"10c31d76f0bbcee34d51b8eac4330fecd01b4ab8"},"cell_type":"code","source":"from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\nfrom pandas.tools.plotting import scatter_matrix\n\n%matplotlib inline\nmpl.style.use('ggplot')\nsns.set_style('white')\npylab.rcParams['figure.figsize'] = 12,8","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"9ff2e99ac772eb57754132b0a4e0e87cf0569983"},"cell_type":"code","source":"# import data from file\ndata_raw = pd.read_csv('../input/train.csv')\n\ndata_val = pd.read_csv('../input/test.csv')\ndata1 = data_raw.copy(deep = True)\ndata_cleaner = [data1, data_val]\nprint(data_raw.info())\ndata_raw.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef2035c1202ae74bad9b49d8a3ce86dc71b0dcff"},"cell_type":"code","source":"print('Train columns with null values:\\n', data1.isnull().sum())\nprint(\"-\"*10)\n\nprint('Test/Validation columns with null values:\\n', data_val.isnull().sum())\nprint(\"-\"*10)\n\ndata_raw.describe(include = 'all')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5c83839c37865708f7b35618fd36546a3d93bc53"},"cell_type":"code","source":"for dataset in data_cleaner:\n    dataset['Age'].fillna(dataset['Age'].median(), inplace = True)\n    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)\n    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)\n    \ndrop_column = ['PassengerId','Cabin', 'Ticket']\ndata1.drop(drop_column, axis=1, inplace = True)\n\nprint(data1.isnull().sum())\nprint(\"-\"*10)\nprint(data_val.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"08135159249528f98556f9c5a0fcc41ba00f358f"},"cell_type":"code","source":"for dataset in data_cleaner:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n    \n    dataset['IsAlone'] = 1\n    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0\n    \n    dataset['Title'] = dataset['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n    \n    dataset['FareBin'] = pd.qcut(dataset['Fare'], 4)\n    dataset['AgeBin'] = pd.cut(dataset['Age'].astype(int), 5)\n    \nstat_min = 10\ntitle_names = (data1['Title'].value_counts() < stat_min)\n\ndata1['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)\nprint(data1['Title'].value_counts())\nprint(\"-\"*10)\n\ndata1.info()\ndata_val.info()\ndata1.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e4f89cd57d1421db8436b975fbd6d02fe095aed9"},"cell_type":"code","source":"label = LabelEncoder()\nfor dataset in data_cleaner:\n    dataset['Sex_Code'] = label.fit_transform(dataset['Sex'])\n    dataset['Embarked_Code'] = label.fit_transform(dataset['Embarked'])\n    dataset['Title_Code'] = label.fit_transform(dataset['Title'])\n    dataset['AgeBin_Code'] = label.fit_transform(dataset['AgeBin'])\n    dataset['FareBin_Code'] = label.fit_transform(dataset['FareBin'])\n    \nTarget = ['Survived']\n\ndata1_x = ['Sex', 'Pclass', 'Embarked', 'Title', 'SibSp', 'Parch', 'Age', 'Fare', 'FamilySize', 'IsAlone']\ndata1_x_calc = ['Sex_Code', 'Pclass', 'Embarked_Code', 'Title_Code', 'SibSp', 'Parch', 'Age', 'Fare']\ndata1_xy = Target + data1_x\nprint('Original X Y: ', data1_xy, '\\n')\n\ndata1_x_bin = ['Sex_Code', 'Pclass', 'Embarked_Code', 'Title_Code', 'FamilySize', 'AgeBin_Code', 'FareBin_Code']\ndata1_xy_bin = Target + data1_x_bin\nprint('Bin X Y: ', data1_xy_bin, '\\n')\n\ndata1_dummy = pd.get_dummies(data1[data1_x])\ndata1_x_dummy = data1_dummy.columns.tolist()\ndata1_xy_dummy = Target + data1_x_dummy\nprint('Dummy X Y: ', data1_xy_dummy, '\\n')\n\ndata1_dummy.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f55abf56506cf7235ca29d633578cc69e4c5d673"},"cell_type":"code","source":"# 3.24 Da-Double Check Cleaned Data\nprint('Train columns with null values: \\n', data1.isnull().sum())\nprint(\"-\"*10)\nprint(data1.info())\nprint(\"-\"*10)\n\nprint('Test/Validation columns with null values: \\n', data_val.isnull().sum())\nprint(\"-\"*10)\nprint(data_val.info())\nprint(\"-\"*10)\n\ndata_raw.describe(include = 'all')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ca1999027e95a6ce469a486e3b27b85b37da6a72"},"cell_type":"code","source":"# 3.25 Split Training and Testing Data\ntrain1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data1[data1_x_calc], data1[Target], random_state = 0)\ntrain1_x_bin, test1_x_bin, train1_y_bin, test1_y_bin = model_selection.train_test_split(data1[data1_x_bin], data1[Target] , random_state = 0)\ntrain1_x_dummy, test1_x_dummy, train1_y_dummy, test1_y_dummy = model_selection.train_test_split(data1_dummy[data1_x_dummy], data1[Target], random_state = 0)\n\n\nprint(\"Data1 Shape: {}\".format(data1.shape))\nprint(\"Train1 Shape: {}\".format(train1_x.shape))\nprint(\"Test1 Shape: {}\".format(test1_x.shape))\n\ntrain1_x_bin.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c79129441f1ba370438870a5185c8d27a181bf50"},"cell_type":"markdown","source":"# Step 4: Perform Exploratory Analysis with Statistics"},{"metadata":{"trusted":true,"_uuid":"c39a12b174a33ef489a7a64ff8da28ef1fe9752e"},"cell_type":"code","source":"for x in data1_x:\n    if data1[x].dtype != 'float64':\n        print('Survival Correction by:', x)\n        print(data1[[x, Target[0]]].groupby(x, as_index = False).mean())\n        print('-' * 10, '\\n')\n        \nprint(pd.crosstab(data1['Title'], data1[Target[0]]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"23f4cfee028be7d92c72c1a9dec53ce32b33be0b"},"cell_type":"code","source":"#graph distribution of quantitative data\nplt.figure(figsize=[16, 12])\n\nplt.subplot(231)\nplt.boxplot(x=data1['Fare'], showmeans = True, meanline = True)\nplt.title('Fare Boxplot')\nplt.ylabel('Fare ($)')\n\nplt.subplot(232)\nplt.boxplot(data1['Age'], showmeans = True, meanline = True)\nplt.title('Age Boxplot')\nplt.ylabel('Age (Years)')\n\nplt.subplot(233)\nplt.boxplot(x=data1['FamilySize'], showmeans = True, meanline = True)\nplt.title('Family Size Boxplot')\nplt.ylabel('Family Size (#)')\n\nplt.subplot(234)\nplt.hist(x = [data1[data1['Survived']==1]['Fare'], data1[data1['Survived']==0]['Fare']], stacked=True, color = ['g', 'r'], label = ['Survived', 'Dead'])\nplt.title('Fare Histgram by Survival')\nplt.xlabel('Fare ($)')\nplt.ylabel('# of Passengers')\nplt.legend()\n\nplt.subplot(235)\nplt.hist(x = [data1[data1['Survived']==1]['Age'], data1[data1['Survived']==0]['Age']], stacked=True, color = ['g', 'r'], label = ['Survived', 'Dead'])\nplt.title('Age Histgram by Survival')\nplt.xlabel('Age (Years)')\nplt.ylabel('# of Passengers')\nplt.legend()\n\nplt.subplot(236)\nplt.hist(x = [data1[data1['Survived']==1]['FamilySize'], data1[data1['Survived']==0]['FamilySize']], stacked=True, color = ['g', 'r'], label = ['Survived', 'Dead'])\nplt.title('Family Size Histgram by Survival')\nplt.xlabel('Family Size (#)')\nplt.ylabel('# of Passengers')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d669359e06950d4556ac7761dafadf9276a6f966"},"cell_type":"code","source":"#graph individual features by survival\nfig, saxis = plt.subplots(2, 3, figsize=(16, 12))\n\nsns.barplot(x = 'Embarked', y = 'Survived', data = data1, ax = saxis[0, 0])\nsns.barplot(x = 'Pclass', y = 'Survived', order = [1, 2, 3], data = data1, ax = saxis[0, 1])\nsns.barplot(x = 'IsAlone', y = 'Survived', order = [1, 0], data = data1, ax = saxis[0, 2])\n\nsns.pointplot(x = 'FareBin', y = 'Survived', data = data1, ax = saxis[1, 0])\nsns.pointplot(x = 'AgeBin', y = 'Survived', data = data1, ax = saxis[1, 1])\nsns.pointplot(x = 'FamilySize', y = 'Survived', data = data1, ax = saxis[1, 2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0a917461b8b3aa2931f29b2f6b7322cbe2987ca9"},"cell_type":"code","source":"#we know class mattered in survival, now let's compare class and a 2nd feature\nfig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(14,12))\n\nsns.boxplot(x = 'Pclass', y = 'Fare', hue = 'Survived', data = data1, ax = axis1)\naxis1.set_title('Pclass vs Fare Survival Comparison')\n\nsns.violinplot(x = 'Pclass', y = 'Age', hue = 'Survived', data = data1, split = True, ax = axis2)\naxis2.set_title('Pclass vs Age Survival Comparison')\n\nsns.boxplot(x = 'Pclass', y = 'FamilySize', hue = 'Survived', data = data1, ax = axis3)\naxis3.set_title('Pclass vs Family Size Survival Comparison')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"201c02fb95eb3b82dc6f8978eec19a776ca57c3b"},"cell_type":"code","source":"#we know sex mattered in survival, now let's compare sex and a 2nd feature\nfig, qaxis = plt.subplots(1, 3, figsize=(14, 12))\n\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'Embarked', data = data1, ax = qaxis[0])\naxis1.set_title('Sex vs Embarked Survival Comparison')\n\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'Pclass', data = data1, ax = qaxis[1])\naxis1.set_title('Sex vs Pclass Survival Comparison')\n\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'IsAlone', data = data1, ax = qaxis[2])\naxis1.set_title('Sex vs IsAlone Survival Comparison')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f40031be6a50c8bcc74c004b3b400008e281b5d4"},"cell_type":"code","source":"#more side-by-side comparisons\nfig, (maxis1, maxis2) = plt.subplots(1, 2, figsize=(14, 12))\n\nsns.pointplot(x = \"FamilySize\", y = \"Survived\", hue = \"Sex\", data = data1,\n              palette= {\"male\": \"blue\", \"female\": \"pink\"},\n              markers=[\"*\", \"o\"], linestyles=[\"-\", \"--\"], ax = maxis1)\n\nsns.pointplot(x = \"Pclass\", y = \"Survived\", hue = \"Sex\", data = data1,\n              palette= {\"male\": \"blue\", \"female\": \"pink\"},\n              markers=[\"*\", \"o\"], linestyles=[\"-\", \"--\"], ax = maxis2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5e10f21ad5f52b914ea4fbc5f8e809d7b8bf5d9f"},"cell_type":"code","source":"#how does embark port factor with class, sex, and survival compare\ne = sns.FacetGrid(data1, col = 'Embarked')\ne.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', ci=95.0, palette = 'deep',\n         order = [1,2,3], hue_order=[\"female\", \"male\"])\ne.add_legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1e173aeac173a3d7faa76452b5c586b2fc2f8b8f"},"cell_type":"code","source":"#plot distributions of age of passengers who survived or did not survive\na = sns.FacetGrid( data1, hue = 'Survived', aspect = 4 )\na.map(sns.kdeplot, 'Age', shade = True )\na.set(xlim=(0, data1['Age'].max()))\na.add_legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dd4a4ff8c356140f7abb220b6b6932b3efa413d0"},"cell_type":"code","source":"#histogram comparison of sex, class, and age by survival\nh = sns.FacetGrid(data1, row = 'Sex', col = 'Pclass', hue = 'Survived')\nh.map(plt.hist, 'Age', alpha = .75)\nh.add_legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eee15f9f4979d341f9365dbadf65a3daa6b6016b"},"cell_type":"code","source":"#pair plots of entire dataset\npp = sns.pairplot(data1, hue = 'Survived', palette = 'deep', size = 1.2, diag_kind = 'kde', diag_kws=dict(shade=True), plot_kws=dict(s=10))\npp.set(xticklabels=[])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6c4c4150c95448425375c4598187ef84b1a21552"},"cell_type":"code","source":"#correlation heatmap of dataset\ndef correlation_heatmap(df):\n    _ , ax = plt.subplots(figsize =(14, 12))\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n    \n    _ = sns.heatmap(\n        df.corr(),\n        cmap = colormap,\n        square = True,\n        cbar_kws={'shrink' :.9},\n        ax = ax,\n        annot=True,\n        linewidths= 0.1, vmax=1.0, linecolor='white',\n        annot_kws={'fontsize':12}\n        )\n    \n    plt.title('Pearson Correlation of Features', y = 1.05, size = 15)\n    \ncorrelation_heatmap(data1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2c889558cb88c23cf4cdad5a54d0cd647a09bd94"},"cell_type":"markdown","source":"# Step 5: Model Data"},{"metadata":{"trusted":true,"_uuid":"b25067137491606c672d5d7bd90a1f8e25630b2e"},"cell_type":"code","source":"#Machine Learning Algorithm (MLA) Selection and Initialization\nMLA = [\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n    \n    gaussian_process.GaussianProcessClassifier(),\n    \n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    neighbors.KNeighborsClassifier(),\n    \n    svm.SVC(probability=True),\n    svm.NuSVC(probability=True),\n    svm.LinearSVC(),\n    \n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(),\n    \n    discriminant_analysis.LinearDiscriminantAnalysis(),\n    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n    \n    XGBClassifier()\n]\n\ncv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0)\n\nMLA_columns = ['MLA Name', 'MLA Parameters', 'MLA Train Accuracy Mean', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD', 'MLA Time']\nMLA_compare = pd.DataFrame(columns = MLA_columns)\n\nMLA_predict = data1[Target]\n\nrow_index = 0\nfor alg in MLA:\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n    \n    cv_results = model_selection.cross_validate(alg, data1[data1_x_bin], data1[Target].values.ravel(), cv = cv_split)\n    \n    MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n    MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean()\n    MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()\n    \n    MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3\n    \n    alg.fit(data1[data1_x_bin], data1[Target].values.ravel())\n    MLA_predict[MLA_name] = alg.predict(data1[data1_x_bin])\n    \n    row_index += 1\n    \n    \nMLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\nMLA_compare\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"550238e52c84180a72cdad35504feaa01253101d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea4b4626c807c162e627356664b3ad455fda10cc"},"cell_type":"code","source":"sns.barplot(x='MLA Test Accuracy Mean', y = 'MLA Name', data = MLA_compare, color = 'm')\n\nplt.title('Machine Learning Algorithm Accuracy Score \\n')\nplt.xlabel('Accuracy Score (%)')\nplt.ylabel('Algorithm')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9e981a3b73b61f44adc4049b5851d995b9f64f77"},"cell_type":"markdown","source":"# 5.1 Evaluate Model Performance"},{"metadata":{"trusted":true,"_uuid":"c33bab7f5c94173c91083264ccc00e7a7817eb82"},"cell_type":"code","source":"#IMPORTANT: This is a handmade model for learning purposes only.\nfor index, row in data1.iterrows():\n    if random.random() > .5:\n        data1.set_value(index, 'Random_Predict', 1)\n    else:\n        data1.set_value(index, 'Random_Predict', 0)\n        \ndata1['Random_Score'] = 0\ndata1.loc[(data1['Survived'] == data1['Random_Predict']), 'Random_Score'] = 1\nprint('Coin Flip Model Accuracy: {:.2f}%'.format(data1['Random_Score'].mean()*100))\n\nprint('Coin Flip Model Accuracy w/SciKit: {:.2f}%'.format(metrics.accuracy_score(data1['Survived'], data1['Random_Predict'])*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e8095a5b3ef562924c3de460db48feef1978343"},"cell_type":"code","source":"#group by or pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\npivot_female = data1[data1.Sex == 'female'].groupby(['Sex', 'Pclass', 'Embarked', 'FareBin'])['Survived'].mean()\nprint('Survival Decision Tree w/Female Node: \\n', pivot_female)\n\npivot_male = data1[data1.Sex == 'male'].groupby(['Sex', 'Title'])['Survived'].mean()\nprint('\\n\\nSurvival Decision Tree w/Male Node: \\n', pivot_male)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a691250b580d0f7dc5e9f5de42c430462767975c"},"cell_type":"code","source":"#handmade data model using brain power (and Microsoft Excel Pivot Tables for quick calculations)\ndef mytree(df):\n    Model = pd.DataFrame(data = {'Predict':[]})\n    male_title = ['Master']\n    \n    for index, row in df.iterrows():\n        Model.loc[index, 'Predict'] = 0\n        \n        if (df.loc[index, 'Sex'] == 'female'):\n            Model.loc[index, 'Predict'] = 1\n            \n        if ((df.loc[index, 'Sex'] == 'female') &\n            (df.loc[index, 'Pclass'] == 3) &\n            (df.loc[index, 'Embarked'] == 'S') &\n            (df.loc[index, 'Fare'] > 8)\n           ):\n                Model.loc[index, 'Predict'] = 0\n                \n        if ((df.loc[index, 'Sex'] == 'male') &\n            (df.loc[index, 'Title'] in male_title)\n           ):\n           Model.loc[index, 'Predict'] = 1\n    return Model\n\nTree_Predict = mytree(data1)\nprint('Decision Tree Model Accuracy/Prediction Score: {:.2}%\\n'.format(metrics.accuracy_score(data1['Survived'], Tree_Predict)*100))\n\nprint(metrics.classification_report(data1['Survived'], Tree_Predict))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"129f7e1295264b6ae6b393c446fcaaad71582c17"},"cell_type":"code","source":"#Plot Accuracy Summary\nimport itertools\ndef plot_confusion_matrix(cm, classes,\n                          normalize = False,\n                          title = 'Confusion matrix',\n                          cmap = plt.cm.Blues):\n    \n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis = 1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n        \n    print(cm)\n    \n    plt.imshow(cm, interpolation = 'nearest', cmap = cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation = 45)\n    plt.yticks(tick_marks, classes)\n    \n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment = \"center\",\n                 color = \"white\" if cm[i, j] > thresh else \"black\")\n        \n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    \ncnf_matrix = metrics.confusion_matrix(data1['Survived'], Tree_Predict)\nnp.set_printoptions(precision = 2)\n\nclass_names = ['Dead', 'Survived']\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes = class_names, \n                      title = 'Confusion matrix, without normalization')\n\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes = class_names, normalize = True,\n                      title = 'Normalized confusion matrix')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"092e5ff60fd5a6b5f278fa55d01bb3222d6ffeea"},"cell_type":"code","source":"#base model\ndtree = tree.DecisionTreeClassifier(random_state = 0)\nbase_results = model_selection.cross_validate(dtree, data1[data1_x_bin], data1[Target], cv = cv_split)\ndtree.fit(data1[data1_x_bin], data1[Target])\n\nprint('BEFORE DT Parameters: ', dtree.get_params())\nprint('BEFORE DT Training w/bin score mean: {:.2f}'.format(base_results['train_score'].mean()*100))\nprint('BEFORE DT Test w/bin score mean: {:.2f}'.format(base_results['test_score'].mean()*100))\nprint('BEFORE DT Test w/bin score 3*std: +/- {:.2f}'.format(base_results['test_score'].std()*100*3))\n\nprint('-'*10)\n\nparam_grid = {'criterion': ['gini', 'entropy'],\n              'max_depth': [2, 4, 6, 8, 10, None],\n              'random_state': [0]\n             }\n\ntune_model = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), param_grid = param_grid, scoring = 'roc_auc', cv = cv_split)\ntune_model.fit(data1[data1_x_bin], data1[Target])\n\nprint('AFTER DT Parameters: ', tune_model.best_params_)\nprint('AFTER DT Training w/bin score mean: {:.2f}'.format(tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100))\nprint('AFTER DT Test w/bin score mean: {:.2f}'.format(tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))\nprint('AFTER DT Test w/bin score 3*std: +/- {:.2f}'.format(tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3))\nprint('-'*10)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5dbfd2f6bbf7e8ce4997a57c893248f32214957b"},"cell_type":"markdown","source":"# 5.13 Tune Model with Feature Selection"},{"metadata":{"trusted":true,"_uuid":"51af8099418561e8d3c4890713f54fb01d76603f"},"cell_type":"code","source":"print('BEFORE DT RFE Training Shape Old: ', data1[data1_x_bin].shape)\nprint('BEFORE DT RFE Training Columns Old: ', data1[data1_x_bin].columns.values)\n\nprint('BEFORE DT RFE Training w/bin score mean: {:.2f}'.format(base_results['train_score'].mean()*100))\nprint('BEFORE DT RFE Test w/bin score mean: {:.2f}'.format(base_results['test_score'].mean()*100))\nprint('BEFORE DT RFE Training w/bin score 3*std: +/- {:.2f}'.format(base_results['test_score'].std()*100*3))\nprint('-' * 10)\n\ndtree_rfe = feature_selection.RFECV(dtree, step = 1, scoring = 'accuracy', cv = cv_split)\ndtree_rfe.fit(data1[data1_x_bin], data1[Target].values.ravel())\n\nX_rfe = data1[data1_x_bin].columns.values[dtree_rfe.get_support()]\nrfe_results = model_selection.cross_validate(dtree, data1[X_rfe], data1[Target].values.ravel(), cv = cv_split)\n\nprint('AFTER DT RFE Training Shape New: ', data1[X_rfe].shape)\nprint('AFTER DT RFE Training Columns New: ', X_rfe)\n\nprint('AFTER DT RFE Training w/bin score mean: {:.2f}'.format(rfe_results['train_score'].mean()*100))\nprint('AFTER DT RFE Test w/bin score mean: {:.2f}'.format(rfe_results['test_score'].mean()*100))\nprint('AFTER DT RFE Training w/bin score 3*std: +/- {:.2f}'.format(rfe_results['test_score'].std()*100*3))\nprint('-'*10)\n\nrfe_tune_model = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), param_grid = param_grid, scoring = 'roc_auc', cv = cv_split)\nrfe_tune_model.fit(data1[X_rfe], data1[Target].values.ravel())\n\nprint('AFTER DT RFE Tuned Parameters: ', rfe_tune_model.best_params_)\nprint('AFTER DT RFE Tuned Training w/bin score mean: {:.2f}'.format(rfe_tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100))\nprint('AFTER DT RFE Tuned Test w/bin score mean: {:.2f}'.format(rfe_tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))\nprint('AFTER DT RFE TunedTraining w/bin score 3*std: +/- {:.2f}'.format(rfe_tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3))\nprint('-'*10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5922bc99a851c9c082b6ca270238f76ac7b3f1d5"},"cell_type":"code","source":"#Graph MLA version of Decision Tree: http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\nimport graphviz\ndot_data = tree.export_graphviz(dtree, out_file = None,\n                                feature_names = data1_x_bin, class_names = True,\n                                filled = True, rounded = True)\ngraph = graphviz.Source(dot_data)\ngraph","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e557e9b5e2467a9d927f5520cc649813516d5911"},"cell_type":"markdown","source":"# Step 6: Validate and Implement"},{"metadata":{"trusted":true,"_uuid":"2b85cc8bf9f306ae941918ac8b8db83ff4c464b5"},"cell_type":"code","source":"correlation_heatmap(MLA_predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bd3e584b0b450f97fa4bf66509e2f39f4e5e69d7"},"cell_type":"code","source":"#why choose one model, when you can pick them all with voting classifier\nvote_est = [\n    ('ada', ensemble.AdaBoostClassifier()),\n    ('bc', ensemble.BaggingClassifier()),\n    ('etc', ensemble.ExtraTreesClassifier()),\n    ('gbc', ensemble.GradientBoostingClassifier()),\n    ('rfc', ensemble.RandomForestClassifier()),\n    \n    ('gpc', gaussian_process.GaussianProcessClassifier()),\n    ('lr', linear_model.LogisticRegressionCV()),\n    \n    ('bnb', naive_bayes.BernoulliNB()),\n    ('gnb', naive_bayes.GaussianNB()),\n    \n    ('knn', neighbors.KNeighborsClassifier()),\n    ('svc', svm.SVC(probability = True)),\n    \n    ('xgb', XGBClassifier())\n]\n\nvote_hard = ensemble.VotingClassifier(estimators = vote_est, voting = 'hard')\nvote_hard_cv = model_selection.cross_validate(vote_hard, data1[data1_x_bin], data1[Target].values.ravel(), cv = cv_split)\nvote_hard.fit(data1[data1_x_bin], data1[Target].values.ravel())\n\nprint(\"Hard Voting Training w/bin score mean: {:.2f}\".format(vote_hard_cv['train_score'].mean()*100))\nprint(\"Hard Voting Test w/bin score mean: {:.2f}\".format(vote_hard_cv['test_score'].mean()*100))\nprint(\"Hard Voting Test w/bin score 3*std: +/- {:.2f}\".format(vote_hard_cv['test_score'].std()*100*3))\nprint('-'*10)\n\nvote_soft = ensemble.VotingClassifier(estimators = vote_est, voting = 'soft')\nvote_soft_cv = model_selection.cross_validate(vote_soft, data1[data1_x_bin], data1[Target].values.ravel(), cv = cv_split)\nvote_soft.fit(data1[data1_x_bin], data1[Target].values.ravel())\n\nprint(\"Soft Voting Training w/bin score mean: {:.2f}\".format(vote_soft_cv['train_score'].mean()*100))\nprint(\"Soft Voting Test w/bin score mean: {:.2f}\".format(vote_soft_cv['test_score'].mean()*100))\nprint(\"Soft Voting Test w/bin score 3*std: +/- {:.2f}\".format(vote_soft_cv['test_score'].std()*100*3))\nprint('-'*10)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"48ab7ee743ef09a86d794430d57c8e4c3a580f2d"},"cell_type":"code","source":"#WARNING: Running is very computational intensive and time expensive.\nimport warnings\nwarnings.filterwarnings('ignore')\n\ngrid_n_estimator = [10, 50, 100, 300]\ngrid_ratio = [.1, .25, .5, .75, 1.0]\ngrid_learn = [.01, .03, .05, .1, .25]\ngrid_max_depth = [2, 4, 6, 8, 10, None]\ngrid_min_samples = [5, 10, .03, .05, .10]\ngrid_criterion = ['gini', 'entropy']\ngrid_bool = [True, False]\ngrid_seed = [0]\n\ngrid_param = [\n                [{\n                    'n_estimators': grid_n_estimator,\n                    'learning_rate': grid_learn,\n                    'random_state': grid_seed\n                }],\n    \n                [{\n                    'n_estimators': grid_n_estimator,\n                    'max_samples': grid_ratio,\n                    'random_state': grid_seed\n                }],\n\n                [{\n                    'n_estimators': grid_n_estimator,\n                    'criterion': grid_criterion,\n                    'max_depth': grid_max_depth,\n                    'random_state': grid_seed\n                }],\n\n                [{\n                    'learning_rate': [.05],\n                    'n_estimators': [300],\n                    'max_depth': grid_max_depth,\n                    'random_state': grid_seed\n                }],\n\n                [{\n                    'n_estimators': grid_n_estimator,\n                    'criterion': grid_criterion,\n                    'max_depth': grid_max_depth,\n                    'oob_score': [False],\n                    'random_state': grid_seed\n                }],\n                \n                [{\n                    'max_iter_predict': grid_n_estimator,\n                    'random_state': grid_seed\n                }],\n    \n                [{\n                    'fit_intercept': grid_bool,\n                    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n                    'random_state': grid_seed\n                }],\n    \n                [{\n                    'alpha': grid_ratio,\n                }],\n    \n                [{}],\n    \n                [{\n                    'n_neighbors': [1,2,3,4,5,6,7],\n                    'weights': ['uniform', 'distance'],\n                    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n                }],\n    \n                [{\n                    'C': [1,2,3,4,5],\n                    'gamma': grid_ratio,\n                    'decision_function_shape': ['ovo', 'ovr'],\n                    'probability': [True],\n                    'random_state': grid_seed\n                }],\n    \n                [{\n                    'learning_rate': grid_learn,\n                    'max_depth': [1,2,4,6,8,10],\n                    'n_estimators': grid_n_estimator,\n                    'seed': grid_seed\n                }]\n]\n\n\nstart_total = time.perf_counter()\nfor clf, param in zip (vote_est, grid_param):\n    start = time.perf_counter()\n    best_search = model_selection.GridSearchCV(estimator = clf[1], param_grid = param, cv = cv_split, scoring = 'roc_auc')\n    best_search.fit(data1[data1_x_bin], data1[Target].values.ravel())\n    run = time.perf_counter() - start\n    \n    best_param = best_search.best_params_\n    print('The best parameter for {} is {}  with a runtime of {:.2f} seconds'.format(clf[1].__class__.__name__, best_param, run))\n    clf[1].set_params(**best_param)\n    \nrun_total = time.perf_counter() - start_total\nprint('Total optimization time was {:.2f} minutes.'.format(run_total/60))\n\nprint('-' *10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1400c7d4ab69752aeaa50b898d02338d9ab86ba5"},"cell_type":"code","source":"#Hard Vote or majority rules w/Tuned Hyperparameters\ngrid_hard = ensemble.VotingClassifier(estimators = vote_est, voting = 'hard')\ngrid_hard_cv = model_selection.cross_validate(grid_hard, data1[data1_x_bin], data1[Target].values.ravel(), cv = cv_split)\ngrid_hard.fit(data1[data1_x_bin], data1[Target].values.ravel())\n\nprint(\"Hard Voting w/Tuned Hyperparameters Training w/bin score mean: {:.2f}\".format(grid_hard_cv['train_score'].mean()*100))\nprint(\"Hard Voting w/Tuned Hyperparameters Test w/bin score mean: {:.2f}\".format(grid_hard_cv['test_score'].mean()*100))\nprint(\"Hard Voting w/Tuned Hyperparameters Test w/bin score 3*std: +/- {:.2f}\".format(grid_hard_cv['test_score'].std()*100*3))\nprint('-'*10)\n\ngrid_soft = ensemble.VotingClassifier(estimators = vote_est, voting = 'soft')\ngrid_soft_cv = model_selection.cross_validate(grid_soft, data1[data1_x_bin], data1[Target].values.ravel(), cv = cv_split)\ngrid_soft.fit(data1[data1_x_bin], data1[Target].values.ravel())\n\nprint(\"Soft Voting w/Tuned Hyperparameters Training w/bin score mean: {:.2f}\".format(grid_soft_cv['train_score'].mean()*100))\nprint(\"Soft Voting w/Tuned Hyperparameters Test w/bin score mean: {:.2f}\".format(grid_soft_cv['test_score'].mean()*100))\nprint(\"Soft Voting w/Tuned Hyperparameters Test w/bin score 3*std: +/- {:.2f}\".format(grid_soft_cv['test_score'].std()*100*3))\nprint('-'*10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9d0b6cb4880dc7782acc382b493ddffbfc81977e"},"cell_type":"code","source":"#prepare data for modeling\nprint(data_val.info())\nprint(\"-\"*10)\n\ndata_val['Survived'] = mytree(data_val).astype(int)\ndata_val['Survived'] = grid_hard.predict(data_val[data1_x_bin])\n\nsubmit = data_val[['PassengerId', 'Survived']]\nsubmit.to_csv(\"../working/submit.csv\", index=False)\n\nprint('Validation Data Distribution: \\n', data_val['Survived'].value_counts(normalize = True))\nsubmit.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a6c7e3bb9942dabf2c16452411bc87744b1ea04d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}