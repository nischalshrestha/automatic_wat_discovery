{"cells":[{"metadata":{"_uuid":"a050819492fd1e06b84b8c13111091f19eedb14c"},"cell_type":"markdown","source":"# Titanic : Machine Learning from Disaster\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\nIn this challenge, I will analyse on what sorts of people were likely to survive. In particular, I will apply machine learning to predict which passengers survived the tragedy."},{"metadata":{"trusted":true,"_uuid":"d025347e93bd8d590088a63c4c8256832ae01da7"},"cell_type":"code","source":"# Load the relevant libraries\nimport numpy as np\nimport pandas as pd\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display\nfrom sklearn.linear_model import LogisticRegression ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1d591e5ae90b52b5e5247e43d7872d32db7869b5"},"cell_type":"code","source":"# Load data\nX_data = pd.read_csv(\"../input/train.csv\")\n\n# Define the training and validation sets\nX_train = X_data.sample(frac=0.8, random_state=100)\nX_valid = X_data.drop(X_train.index)\n\n# Define the labeled data for training and validation sets\nY_data = X_data[\"Survived\"]\nY_train = X_train[\"Survived\"]\nY_valid = X_valid[\"Survived\"]\n\n# Test data\nX_test = pd.read_csv(\"../input/test.csv\")\nID_test = X_test[\"PassengerId\"]\n\n# Display sample data\ndisplay(X_train.head(5))\ndisplay(Y_train.head(5))\n\n# Errors\nerrors_list = dict()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e24f49acd264b4db94cd7d1b60a37522021fa82d"},"cell_type":"markdown","source":"## Explanation of Parameters\n\n* PassengerId: type should be integers\n* Survived: Survived or Not\n* Pclass: Class of Travel\n* Name: Name of Passenger\n* Sex: Gender\n* Age\n* SibSp: Number of Sibling/Spouse abord\n* Parch: Number of Parent/Child abord\n* Ticket\n* Fare\n* Cabin\n* Embarked: The port in which a passenger has embarked. C - Cherbourg, S - Southampton, Q = Queenstown"},{"metadata":{"trusted":false,"_uuid":"0cc4ebe1682eadcfc896db8b744b9c65f6ab823e"},"cell_type":"code","source":"# View summary of training data\ndisplay(X_train.describe())\ndisplay(Y_train.describe())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3d76acbc2a08407a88c901035519aaf083c41064"},"cell_type":"markdown","source":"### Observation 1\nYou can see that the Age count mismatching with the count of other features of data raises some questions about the data. So let us focus more deep into the data."},{"metadata":{"trusted":false,"_uuid":"9b4fd8880d892437ee0738afa3805f64a4826cf7"},"cell_type":"code","source":"# Check if any columns has NaN or empty values\ndef sanity_check_NaN(df):\n    output_dict = {}\n    for column in df:\n        if df[column].isnull().any():\n            output_dict[column] = df[column].isnull().sum()\n    \n    return output_dict\n\nsanity_check_results = sanity_check_NaN(X_data)\ndisplay(sanity_check_results)\n\n# Also check for any duplicate data\ndisplay(X_data.duplicated().sum())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a841304c00f9adbaaac47d5cc75dc20083905a82"},"cell_type":"markdown","source":"### Observation 2\nYou can see that there Age, Cabin and Embarked have missing values. Also there are no duplicate data points. A very naive way of getting over this problem is removing the rows with missing data, but since our dataset is very small we do not want to get rid of any datapoints, we will try to replace the missing data with intelligent guesses. \n\nDo note that the class of travel includes information on travel.  We will define a new binary variable called CabinAvailable to indicate whether cabin information are available and remove the Cabin column during preprocessing."},{"metadata":{"scrolled":false,"trusted":false,"_uuid":"ece19531ba4121307d94f93099b55d80ef97794d"},"cell_type":"code","source":"# Visualize the Age\n\nfig, axs = plt.subplots(1, 2)\n\n# Visualize age\nage_non_na = X_data[\"Age\"].dropna(inplace=False)\naxs[0].hist(age_non_na.tolist())\naxs[1].boxplot(age_non_na.tolist())\ndisplay(age_non_na.describe())\nage_non_na.skew()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a0dc3d8dbd5684a7ed0de0613367871e63d8a195"},"cell_type":"markdown","source":"### Observation 3\nSince Age roughly follows a normal distribution has a minor skewness, we will use a normal distribution with mean=28.00 and standard deviation = 14.526497 to randomly fill the missing age. We will also use some additional checks based on name to fill these missing ages.\n\nWe will add 2 new features hasFamily and isChild based on SibSp and Parch parameters.\n\nWe will proceed with pre-processing next."},{"metadata":{"trusted":false,"_uuid":"5731696cac0f469b58dc96df6db110c2be4fa905"},"cell_type":"code","source":"# Define a function to preprocess the data\ndef preprocess(df):\n    \n    # fill the missing ages\n    df[\"Name\"] = df[\"Name\"].str.lower()\n    #df.loc[(df[\"Age\"].isnull()) & (df[\"Name\"].str.contains(\"miss\")), \"Age\"] = random.randrange(20, 28)\n    #df.loc[(df[\"Age\"].isnull()) & (df[\"Name\"].str.contains(\"master\")), \"Age\"] = random.randrange(1, 16)\n    #df.loc[df[\"Age\"].isnull(), \"Age\"] = random.randrange(14, 43)\n\n    # fill missing Embarked values\n    df[\"Embarked\"].fillna(df[\"Embarked\"].mode()[0], inplace=True)\n    \n    # fill missing fare values\n    df[\"Fare\"].fillna(df[\"Fare\"].median(), inplace=True)\n    \n    df[\"Age\"].fillna(df[\"Age\"].mean(), inplace=True)\n    \n    # Convert categorical variables to indicator variables\n    df = df.join(pd.get_dummies(df[\"Embarked\"]))\n    df = df.join(pd.get_dummies(df[\"Sex\"]))\n    df = df.join(pd.get_dummies(df[\"Pclass\"]))\n    \n    # Add the two binary features\n    # Family feature\n    df[\"hasFamily\"] = 0\n    df.loc[(df[\"SibSp\"] != 0) | (df[\"Parch\"] != 0), \"hasFamily\"] = 1 \n    \n    # Child feature\n    df[\"isChild\"] = 0\n    df.loc[df[\"Age\"] < 16, \"isChild\"] = 1\n    \n    # hasCabin feature\n    df[\"hasCabin\"] = 1\n    df.loc[df[\"Cabin\"].isnull(), \"hasCabin\"] = 0\n    \n    # drop the columns\n    # [\"Embarked\", \"Sex\", \"Pclass\", \"Ticket\", \"Cabin\", \"Name\", \"PassengerId\", \"C\", \"Q\", \"S\", 1, 2, 3, \"SibSp\", \"Parch\", \"hasFamily\", \"isChild\", \"hasCabin\"]\n    df.drop([\"Embarked\", \"Sex\", \"Ticket\", \"Cabin\", \"Name\", \"PassengerId\"], inplace=True, axis=1)\n    \n    return df\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e0f10c0a3be975f9948f6ad7236c1fdba4dd9916"},"cell_type":"code","source":"X_data = preprocess(X_data)\nX_train = preprocess(X_train)\nX_valid = preprocess(X_valid)\n\ndisplay(X_data.head())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"28db269d803dee5d64f94e83cdf0863ba1efb56d"},"cell_type":"markdown","source":"### Note\nNow the preprocessing is complete and we will evaluate the relationships between variables."},{"metadata":{"trusted":false,"_uuid":"969818506433ae138b3dbd487ebd9cb055873879"},"cell_type":"code","source":"# Plot correlation heat map\noriginal_data = X_data.copy(deep=True)\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(original_data.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b93f60198afd4880c4d757724a30b2415afebfb5"},"cell_type":"markdown","source":"### Model 1: Logistic Regression Model\nWe will use a logistic regression classifier to predict the output"},{"metadata":{"trusted":false,"_uuid":"5cdc1e32b02bd674c367e481c27b9e8ce1d39193"},"cell_type":"code","source":"# Drop the labels in dataframes\nX_data.drop([\"Survived\"], axis = 1, inplace=True)\nX_train.drop([\"Survived\"], axis = 1, inplace=True)\nX_valid.drop([\"Survived\"], axis = 1, inplace=True)\n\n# Initialize the Classifier\nclf_lg_r = LogisticRegression(random_state=0, solver='liblinear', multi_class='ovr')\n\n# Fit the training data which is a subset of X_data\nclf_lg_r.fit(X_train, Y_train)\n\n# Evaluate the testing error on validation set\nscore_valid_lg_r = clf_lg_r.score(X_valid, Y_valid)\nprint(\"Testing Accuracy = \", score_valid_lg_r)\n\n# Fit the classifier on entire training data\nclf_lg_r.fit(X_data, Y_data)\n\n# Evaluate the training error\nscore_train_lg_r = clf_lg_r.score(X_data, Y_data)\nprint(\"Training Accuracy = \", score_train_lg_r)\n\n# Generate the output\nX_test = preprocess(X_test)\nY_test = clf_lg_r.predict(X_test)\nans = pd.DataFrame({\"PassengerId\": ID_test, \"Survived\": Y_test})\nans.to_csv(\"submit_lg_r.csv\", index = False)\n\nerrors_list[\"Logistic Classifier\"] = (score_train_lg_r, score_valid_lg_r)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dcc5541dd57409bf7c3bb7b12ca75a1032f57f75"},"cell_type":"markdown","source":"### Model 2: Random Forest Model\nWe will use a random forest classifier to predict the output\n"},{"metadata":{"trusted":false,"_uuid":"d254ab3dce5253d2a8b783395ad4c5c2c0a30754"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nclf_rf = RandomForestClassifier(n_estimators=100, random_state=0)\nclf_rf.fit(X_train, Y_train)\n\nscore_valid_rf = clf_rf.score(X_valid, Y_valid)\nprint(\"Testing accuracy = \", score_valid_rf)\n\n# fit the data\nclf_rf.fit(X_data, Y_data)\n\n# Evaluate the training error\nscore_train_rf = clf_rf.score(X_data, Y_data)\nprint(\"Training accuracy = \", score_train_rf)\n\n# Generate the output\nY_test = clf_rf.predict(X_test)\nans = pd.DataFrame({\"PassengerId\": ID_test, \"Survived\": Y_test})\nans.to_csv(\"submit_rf.csv\", index = False)\n\nerrors_list[\"Random Forest\"] = (score_train_rf, score_valid_rf)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f2a8386d0bcda00eb3a4bccd0549d80e66e59cd3"},"cell_type":"markdown","source":"### Model 3: Adaptive Boosting Model\nWe will use an adaptive boosting classifier to fit the data."},{"metadata":{"trusted":false,"_uuid":"b72b6e833b85051306c109808e1a055467e2b023"},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\n\nclf_adb = AdaBoostClassifier( n_estimators=10, learning_rate=1.1, algorithm='SAMME.R', random_state=100)\nclf_adb.fit(X_train, Y_train)\n\nscore_valid_adb = clf_adb.score(X_valid, Y_valid)\nprint(\"Testing accuracy = \", score_valid_adb)\n\n# fit the data\nclf_adb.fit(X_data, Y_data)\n\n# Evaluate the training error\nscore_train_adb = clf_adb.score(X_data, Y_data)\nprint(\"Training accuracy = \", score_train_adb)\n\n# Generate the output\nY_test = clf_adb.predict(X_test)\nans = pd.DataFrame({\"PassengerId\": ID_test, \"Survived\": Y_test})\nans.to_csv(\"submit_adb.csv\", index = False)\n\nerrors_list[\"Adaptive Boosting\"] = (score_train_adb, score_valid_adb)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"53ff47caad35cec8ae459ee0dc008122795fdb98"},"cell_type":"markdown","source":"### Model 4: Bernouli Naive Bayes Model\nWe will use a Bernouli Naive Bayes Model with binary features to train our model."},{"metadata":{"trusted":false,"_uuid":"118ea855d1311badb548fab265c4b9eef7316354"},"cell_type":"code","source":"from sklearn.naive_bayes import BernoulliNB\n\nclf_bnb = BernoulliNB(alpha=1.0, binarize=None, fit_prior=True, class_prior=None)\n\n# create binary feature matrix\nX_data_bin = X_data.copy(deep=True)\nX_train_bin = X_train.copy(deep=True)\nX_valid_bin = X_valid.copy(deep=True)\n\nX_data_bin.drop([\"Pclass\", \"SibSp\", \"Parch\", \"Age\", \"Fare\"], axis=1, inplace=True)\nX_train_bin.drop([\"Pclass\", \"SibSp\", \"Parch\", \"Age\", \"Fare\"], axis=1, inplace=True)\nX_valid_bin.drop([\"Pclass\", \"SibSp\", \"Parch\", \"Age\", \"Fare\"], axis=1, inplace=True)\n\n# fit train data\nclf_bnb.fit(X_train_bin, Y_train)\n\n# get validation error\nscore_valid_bnb = clf_bnb.score(X_valid_bin, Y_valid)\nprint(\"Testing accuracy = \", score_valid_bnb)\n\n# fit the data\nclf_bnb.fit(X_data_bin, Y_data)\n\n# Evaluate the training error\nscore_train_bnb = clf_bnb.score(X_data_bin, Y_data)\nprint(\"Training accuracy = \", score_train_bnb)\n\n# Generate the output\nX_test_bin = X_test.copy(deep=True)\nX_test_bin.drop([\"Pclass\", \"SibSp\", \"Parch\", \"Age\", \"Fare\"], axis=1, inplace=True)\nY_test = clf_bnb.predict(X_test_bin)\nans = pd.DataFrame({\"PassengerId\": ID_test, \"Survived\": Y_test})\nans.to_csv(\"submit_bnb.csv\", index = False)\n\nerrors_list[\"Bernouli Naive Bayes\"] = (score_train_bnb, score_valid_bnb)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4055f91124bc12549b25a34d76a93c5af2ce2009"},"cell_type":"markdown","source":"### Model 5: Multinomial Naive Bayes Model\nWe will use a Multinomial Naive Bayes Model to train our model."},{"metadata":{"trusted":false,"_uuid":"b9afd45ff369eb0984510280f9fdb9966bb558c6"},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\n\nclf_mnb = MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)\n\n# fit train data\nclf_mnb.fit(X_train, Y_train)\n\n# get validation error\nscore_valid_mnb = clf_mnb.score(X_valid, Y_valid)\nprint(\"Testing accuracy = \", score_valid_mnb)\n\n# fit the data\nclf_mnb.fit(X_data, Y_data)\n\n# Evaluate the training error\nscore_train_mnb = clf_mnb.score(X_data, Y_data)\nprint(\"Training accuracy = \", score_train_mnb)\n\n# Generate the output\nY_test = clf_mnb.predict(X_test)\nans = pd.DataFrame({\"PassengerId\": ID_test, \"Survived\": Y_test})\nans.to_csv(\"submit_mnb.csv\", index = False)\n\nerrors_list[\"Mulitnomial Naive Bayes\"] = (score_train_mnb, score_valid_mnb)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dbf7bed4cef1facd6bbecc22ef15b97968a19b12"},"cell_type":"markdown","source":"### Model 6: Support Vector Machine Model\nWe will use a Support Vector Machine Model."},{"metadata":{"trusted":false,"_uuid":"a545f8889af3c39d0fafc58f1b69e113a10e6485"},"cell_type":"code","source":"from sklearn.svm import SVC\n\nclf_sv = SVC(kernel ='poly', C = 1.0, degree = 2, gamma = 'auto')\n\n# fit train data\nclf_sv.fit(X_train, Y_train)\n\n# get validation error\nscore_valid_sv = clf_sv.score(X_valid, Y_valid)\nprint(\"Testing accuracy = \", score_valid_sv)\n\n# fit the data\nclf_mnb.fit(X_data, Y_data)\n\n# Evaluate the training error\nscore_train_sv = clf_sv.score(X_data, Y_data)\nprint(\"Training accuracy = \", score_train_sv)\n\n# Generate the output\nY_test = clf_sv.predict(X_test)\nans = pd.DataFrame({\"PassengerId\": ID_test, \"Survived\": Y_test})\nans.to_csv(\"submit_svc.csv\", index = False)\n\n# Best score in Kaggle: 0.78468 with C = 1.0 and linear kernel\n# Best score in Kaggle: 0.78947 with C = 1.0 and polynomial kernel with degree 2\n\nerrors_list[\"Support Vector Machine\"] = (score_train_sv, score_valid_sv)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"56c7bfc61a4390a341606b4e1671451a2b8a0f18"},"cell_type":"markdown","source":"### Model 7: Decision Tree Classifier Model\nWe will use a Decision Tree Classifier."},{"metadata":{"trusted":false,"_uuid":"8bc600ed6ed560b96a14204a9a9ff7199e0ee5ca"},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\nclf_dt = DecisionTreeClassifier(max_depth = 8)\nclf_dt.fit(X_train_bin, Y_train)\n\n# get validation error\nscore_valid_dt = clf_dt.score(X_valid_bin, Y_valid)\nprint(\"Testing accuracy = \", score_valid_dt)\n\n# fit the data\nclf_dt.fit(X_data_bin, Y_data)\n\n# Evaluate the training error\nscore_train_dt = clf_dt.score(X_data_bin, Y_data)\nprint(\"Training accuracy = \", score_train_dt)\n\n# Generate the output\nY_test = clf_dt.predict(X_test_bin)\nans = pd.DataFrame({\"PassengerId\": ID_test, \"Survived\": Y_test})\nans.to_csv(\"submit_dt.csv\", index = False)\n\nerrors_list[\"Decision Tree Classifier\"] = (score_train_dt, score_valid_dt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f9b02ffc6d185b771d5e03573d1f453f2a3fc2c8"},"cell_type":"code","source":"# Plot the errors\n\nclfs = errors_list.keys()\nerrors = list(errors_list.values())\ntrain_errors = [x[0] for x in errors]\nvalid_erros = [x[1] for x in errors]\n\n# create plot\nfig, ax = plt.subplots()\nindex = np.arange(len(clfs))\nbar_width = 0.35\nopacity = 0.8\n \nrects1 = plt.barh(index, train_errors, bar_width,\n                 alpha=opacity,\n                 color='b',\n                 label='Training Error')\n \nrects2 = plt.barh(index + bar_width, valid_erros, bar_width,\n                 alpha=opacity,\n                 color='g',\n                 label='Testing Error')\n \nplt.ylabel('Classifier')\nplt.xlabel('Error Score')\nplt.title('Model Performance')\nplt.yticks(index + bar_width, clfs)\nplt.legend(loc=9, bbox_to_anchor=(0.5, -0.2))\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b5c2057b863331fa0b5bfea762c1eda86bf518f5"},"cell_type":"markdown","source":"So we see that SVM provides better testing accuracy.\n\nNote:  Feel free to criticize any errors and also upvote if you guys found my work useful. Happy Learning!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}