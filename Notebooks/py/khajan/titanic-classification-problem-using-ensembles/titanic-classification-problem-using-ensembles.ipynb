{"cells":[{"metadata":{"_uuid":"c9b37711be6ce5074f371655529607f49544ee4b"},"cell_type":"markdown","source":"## Importing all packages and libraries required"},{"metadata":{"trusted":true,"_uuid":"388eae2af706c2e15e9145d6ab702ea9636513aa"},"cell_type":"code","source":"import os \nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5346ec682b093a53a8363c477e9c1e10d0c8f5aa"},"cell_type":"markdown","source":"# Loading all libraries "},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Print current directory files\nprint(\"Files in current directory : {}\\n\".format(os.listdir(\"../input\")))\n\n# loading train data and veiwing shape \ntrain = pd.read_csv('../input/train.csv')\nprint(\"Train shape : {}\\n\".format(train.shape))\n\n# loading test data and veiwing shape \ntest = pd.read_csv('../input/test.csv')\nprint(\"Test shape : {}\\n\".format(test.shape))\n\n# print all columns\nprint(\"Columns : {} \".format(train.columns))\n\n# View of train data \ntrain.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"01050411cb30a0eb2a4b64c6fe07b26581e1f1de"},"cell_type":"markdown","source":"# Description of titanic data"},{"metadata":{"trusted":true,"_uuid":"4ad9c4571422a0ff21fb627933c62ac46944e61b"},"cell_type":"code","source":"# Datatypes of columns of titanic data \nprint(train.dtypes)\n\n# Description of train Data\ntrain.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"06627332fe92c44f2ccf423ce65a2ec934c8b4f9"},"cell_type":"markdown","source":"# Visualizations of data"},{"metadata":{"trusted":true,"_uuid":"2613c8da98057d8804815527286ab22b6b07a09f"},"cell_type":"code","source":"# Visualizing Total count of survivors based on their sex group ( male / female)\n## Female survivors are more than male survivors \nsex_pivot = train.pivot_table(index=\"Sex\",values=\"Survived\")\nsex_pivot.plot.bar()\nplt.show()\nprint(train['Sex'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c179a002edef853547adc50b173746e79b30bb1f"},"cell_type":"code","source":"## Visualizing Total count of survivors based on their pclass ( 1st , 2nd , 3rd)\npclass_pivot = train.pivot_table(index=\"Pclass\",values=\"Survived\")\npclass_pivot.plot.bar()\nplt.show()\nprint(train['Pclass'].value_counts())\n# 1st class are have more survival rate than 2nd and 2nd pclass has more survival rate then 3rd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b8dd55a6b1b56b7d6cedc1b526af2815069fb0b5"},"cell_type":"code","source":"# Visulazing\nsns.catplot(x='Sex', y='Survived',  kind='bar', data=train, hue='Pclass');","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b3b6cd7a1df725cd5dcccab46edf503fa8c78efb"},"cell_type":"markdown","source":"# Correlation matrix for numerical variables"},{"metadata":{"trusted":true,"_uuid":"46519908462d3f8f8f09440a3f0358f083b989ca"},"cell_type":"code","source":"# Check the correlation for the current numeric feature set.\nprint(train[['Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare']].corr())\n\nsns.heatmap(train[['Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare']].corr(), annot=True, fmt = \".2f\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bcc0cef930ec1c7c98d8b45857cac385a3850602"},"cell_type":"code","source":"sns.pairplot(train,hue=\"Survived\",markers=\"+\",diag_kind=\"kde\");","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a1bf0459d1c5b20cc11c5d022eb98a0f3f4544e6"},"cell_type":"markdown","source":"# Missing data analysis"},{"metadata":{"trusted":true,"_uuid":"31623738c5b61ea85d8e68a3a4d75c849e53dc0e"},"cell_type":"code","source":"## Missing values  ( Age and Cabin has high missing values )\n\nmiss_train = pd.DataFrame(train.isnull().sum(), columns=['Miss_train_data'])\nmiss_test = pd.DataFrame(test.isnull().sum(), columns=['Miss_test_data'])\nprint(pd.concat([miss_train,miss_test], join = 'inner', axis = 1))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b658facf1516836113c1568d58b48fa9429b66a4"},"cell_type":"markdown","source":"## Fill missing value"},{"metadata":{"trusted":true,"_uuid":"761a7b188bc0eb735a1041890346b27214473225"},"cell_type":"code","source":"# Filling missing value in the Fare column which we'll fill with the mean.\nsns.catplot(x='Pclass', y='Fare', data=test, kind='point')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7267af9b24ef27cabc160366bf043f6b5c05bd4b"},"cell_type":"code","source":"# Since Fare and Pclass are related\ntest[\"Fare\"] = test[\"Fare\"].fillna(train[\"Fare\"].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"850a9c8433bfa2a104deb8acbaeb7c333dd5140a"},"cell_type":"code","source":"# Replace two missing values with most ouccuring \"S\"\ntrain[\"Embarked\"]=train['Embarked'].fillna(\"S\")\ntest[\"Embarked\"]=test[\"Embarked\"].fillna(\"S\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d2e5f420033f43939d477bb130c4717f01a8026d"},"cell_type":"code","source":"# fuction to binning of continous age into binning\ndef binning_age(df):\n    df[\"Age\"] = df[\"Age\"].fillna(-0.5)\n    cut_points = [-1,0,5,12,18,35,60,100]\n    label_names = [\"Missing\",\"Infant\",\"Child\",\"Teen\",\"Young-Adult\",\"Adult\",\"Senior\"]\n    df[\"Age_categories\"] = pd.cut(df[\"Age\"],cut_points,labels=label_names)\n    return df\n\ntrain = binning_age(train)\ntest = binning_age(test)\n\n# Plotting Age_categories\nprint(\"Missing values in Age column: {}\".format(train['Age'].isnull().sum()))\nsns.catplot(x='Age_categories', y='Survived', data=train, kind='bar')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e2e5d5adbe5ca549ae6a8f6c6ed06d26647671aa"},"cell_type":"code","source":"## Binning of Fare Cateogry in 4 bins \ndef binning_fare(df):\n    cut_points = [0,12,50,100,1000]\n    label_names = [\"0-12\",\"12-50\",\"50-100\",\"100+\"]\n    df[\"Fare_categories\"] = pd.cut(df[\"Fare\"],cut_points,labels=label_names)\n    return df\n\n# Apply Fare binning to both train and test \ntrain = binning_fare(train)\ntest = binning_fare(test)\n\nsns.catplot(x='Fare_categories', y='Survived', data=train, kind='bar')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"493a6de70c1a7456935f4bbd03dd821afb9b74f6"},"cell_type":"code","source":"train['Fare_categories'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"273432232e82154e06dac4d859fefe1a87db09d2"},"cell_type":"markdown","source":"### Mapping Name with its title"},{"metadata":{"trusted":true,"_uuid":"bf0387be2fb5216000041580b1721706e5656a44"},"cell_type":"code","source":"# Get the titles\nfor data in [train, test]:\n    # Use split to get only the titles from the name\n    data['Title'] = data['Name'].str.split(',').str[1].str.split('.').str[0].str.strip()\n    # Check the initial list of titles.\n    print(data['Title'].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"878fc3bd6f3731928e7c84ea1b3ea5bbe49398af"},"cell_type":"code","source":"## Create bins using Name \ntitles = {\n    \"Mr\" :         \"Mr\",\n    \"Mme\":         \"Mrs\",\n    \"Ms\":          \"Mrs\",\n    \"Mrs\" :        \"Mrs\",\n    \"Master\" :     \"Master\",\n    \"Mlle\":        \"Miss\",\n    \"Miss\" :       \"Miss\",\n    \"Capt\":        \"Officer\",\n    \"Col\":         \"Officer\",\n    \"Major\":       \"Officer\",\n    \"Dr\":          \"Officer\",\n    \"Rev\":         \"Officer\",\n    \"Jonkheer\":    \"Royalty\",\n    \"Don\":         \"Royalty\",\n    \"Sir\" :        \"Royalty\",\n    \"Countess\":    \"Royalty\",\n    \"Dona\":        \"Royalty\",\n    \"Lady\" :       \"Royalty\"\n}\n\nextracted_titles = train[\"Name\"].str.extract(' ([A-Za-z]+)\\.',expand=False)\ntrain[\"Title\"] = extracted_titles.map(titles)\n\nextracted_titles = test[\"Name\"].str.extract(' ([A-Za-z]+)\\.',expand=False)\ntest[\"Title\"] = extracted_titles.map(titles)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3f27e716897f0e822bf03f85edb6b847a824ddc4"},"cell_type":"markdown","source":"# Min max scaling"},{"metadata":{"trusted":true,"_uuid":"c43bca4900d718b3e989f40f4c63289bdb6426ac"},"cell_type":"code","source":"## Min maxscaling of Sibsp Parch and Fare\nfrom sklearn import preprocessing\n\nmin_max_scaler = preprocessing.MinMaxScaler()\n#columns = [\"SibSp\",\"Parch\",\"Fare\"]\ncolumns = ['Fare']\ntrain[columns] = min_max_scaler.fit_transform(train[columns])\ntest[columns] = min_max_scaler.transform(test[columns])\ntrain.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6bcb1f1ace965616409a8f51a9ed2130b2e72c05"},"cell_type":"markdown","source":"# One Hot encoding"},{"metadata":{"trusted":true,"_uuid":"113a0f10b89101a2aa445480d0fb677cbd790d34"},"cell_type":"code","source":"# Function to create one-hot key coding  using get_dummies for Categorical variable\ndef create_dummies(df,column_name):\n    dummies = pd.get_dummies(df[column_name], prefix = column_name)\n    df = pd.concat([df,dummies],axis = 1)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f1693404cc572a1988eecd3ae69224918c3a4053"},"cell_type":"code","source":"# Columns in list to create dummies variable \ncolumn = ['Age_categories','Pclass', 'Sex','Embarked','Fare_categories','Title' ]\nfor i in column:\n    train = create_dummies(train,i)\n    test = create_dummies(test,i)\n    \nprint(train.columns)\nprint(\"Train shape : {}\".format(train.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"88388e8483a8010cd89fd29411d80917b8c1d507"},"cell_type":"code","source":"# Creating a variable with Cabin or not \nfor df in [train,test]:\n    df['Has_cabin'] = df['Cabin'].notna().astype(int)  \n\n# Creating new variable using Sibsp and Parch\nfor df in [train,test]:\n    df['family_size'] = df['SibSp'] + df['Parch'] + 1\n    df['Single'] = (df['family_size'] >  1).astype(int)\n\nprint(\"New train shape : {}\\n\".format(train.shape))\nprint(train.isna().sum())\nprint(\"\\n\")\nprint(test.isna().sum())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2da15741610edf2cd1c60e29207e4765f3e19577"},"cell_type":"markdown","source":"## Dropping columns"},{"metadata":{"trusted":true,"_uuid":"f5e56924745d609c58887de1f96d905af358f966"},"cell_type":"code","source":"#[df.drop(columns=['PassengerId','Pclass','Cabin','SibSp','Parch','Age'], inplace = True) for df in [train, test]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d49097803e4ea6356fa046c60e2ad78c1bf42eda"},"cell_type":"code","source":"print(\"Train shape : {}\".format(train.shape))\ntrain.corr()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4dc4b3c4ba833193bf731a5e6a752637087de498"},"cell_type":"markdown","source":"## Selecting columns for model and spillting data ( train / test)"},{"metadata":{"trusted":true,"_uuid":"f7fc64f959bb458ecf6b0ddd277a952279f5cd6a"},"cell_type":"code","source":"# Use only feature with high coefficnets\ncolumns = ['Fare','Age_categories_Missing','Age_categories_Infant',\n           'Age_categories_Child', 'Age_categories_Teen','Age_categories_Adult',\n           'Age_categories_Senior','Pclass_1','Pclass_3','Sex_female','Embarked_C',\n           'Embarked_S','Fare_categories_0-12','Fare_categories_50-100', \n           'Fare_categories_100+', 'Title_Master','Title_Miss', 'Title_Mr',\n           'Title_Mrs', 'Title_Officer','Has_cabin','Single']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f6b01efe2a31242c9321be3861c1ee78d8ef3430"},"cell_type":"code","source":"holdout = test \n\nfrom sklearn.model_selection import train_test_split\n\nall_X = train[columns]\nall_y = train['Survived']\n\nX_train, X_test, y_train, y_test = train_test_split(all_X, all_y, test_size=0.2,random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cddb66e5b27ae8b79e4bfe718a1527adefd1176b"},"cell_type":"markdown","source":"## Classification Models report"},{"metadata":{"trusted":true,"_uuid":"6e781884f003214fa1efc1f5c58ed8bbe25546c9"},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score, cross_val_predict\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"58a81d55630539372b7cf41e66aa6b65c7d4c097"},"cell_type":"code","source":"def print_score(clf, X_train, y_train, X_test, y_test, train=True):\n    '''\n    print the accuracy score, classification report and confusion matrix of classifier\n    '''\n    if train:\n        '''\n        training performance\n        '''\n        print(\"Train Result:\\n\")\n        print(\"accuracy score: {0:.4f}\\n\".format(accuracy_score(y_train, clf.predict(X_train))))\n        print(\"Classification Report: \\n {}\\n\".format(classification_report(y_train, clf.predict(X_train))))\n        print(\"Confusion Matrix: \\n {}\\n\".format(confusion_matrix(y_train, clf.predict(X_train))))\n\n        res = cross_val_score(clf, X_train, y_train, cv=10, scoring='accuracy')\n        print(\"Average Accuracy: \\t {0:.4f}\".format(np.mean(res)))\n        print(\"Accuracy SD: \\t\\t {0:.4f}\".format(np.std(res)))\n        \n    elif train==False:\n        '''\n        test performance\n        '''\n        print(\"Test Result:\\n\")        \n        print(\"accuracy score: {0:.4f}\\n\".format(accuracy_score(y_test, clf.predict(X_test))))\n        print(\"Classification Report: \\n {}\\n\".format(classification_report(y_test, clf.predict(X_test))))\n        print(\"Confusion Matrix: \\n {}\\n\".format(confusion_matrix(y_test, clf.predict(X_test))))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"106d155d6e92dcaeb6279fc31e11a9961777cdbb"},"cell_type":"markdown","source":"# Model building \n\n## 1. Logistic rgression"},{"metadata":{"trusted":true,"_uuid":"6f8ea431ba79db0fdc671b3f89b551ffa70d7b65"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression()\nlr.fit(X_train,y_train)\nprint_score(lr, X_train, y_train, X_test, y_test, train=True)\nprint_score(lr, X_train, y_train, X_test, y_test, train=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2d8270182a66774d12e948a20e6419607eba5b7a"},"cell_type":"markdown","source":"## 2. Using support vector machine "},{"metadata":{"trusted":true,"_uuid":"f247c3d9d4246ed5222636ac38b640b6a438cdf7"},"cell_type":"code","source":"from sklearn.svm import LinearSVC\n\nsvm_clf = LinearSVC(dual=False)\nsvm_clf.fit(X_train, y_train)\nprint_score(svm_clf, X_train, y_train, X_test, y_test, train=True)\nprint_score(svm_clf, X_train, y_train, X_test, y_test, train=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"472565904efc1263b5561ff4f092dbf175925180"},"cell_type":"markdown","source":"## 3. Random Forest"},{"metadata":{"trusted":true,"_uuid":"da70eed8e371a32185d2d7380e7751fa0543e32a"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrf_clf = RandomForestClassifier()\nrf_clf.fit(X_train, y_train.ravel())\nprint_score(rf_clf, X_train, y_train, X_test, y_test, train=True)\nprint_score(rf_clf, X_train, y_train, X_test, y_test, train=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dfdf3cfaa41faa5dfcfa092fd0a85f86485fee24"},"cell_type":"markdown","source":"## 4. AdaBoost"},{"metadata":{"trusted":true,"_uuid":"cd23fb36a9171972e94fb9b8cd8208372ab51cd7"},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\n\nada_clf = AdaBoostClassifier()\nada_clf.fit(X_train, y_train)\nprint_score(ada_clf, X_train, y_train, X_test, y_test, train=True)\nprint_score(ada_clf, X_train, y_train, X_test, y_test, train=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5118672415f96e8e1744a2653a375d62f79cb651"},"cell_type":"markdown","source":"## 5. Xgboost"},{"metadata":{"trusted":true,"_uuid":"8c5d4857fd60c3f464737e272add4f2bf3d67232"},"cell_type":"code","source":"import xgboost as xgb\n\nxgb_clf = xgb.XGBClassifier()\nxgb_clf.fit(X_train, y_train)\nprint_score(xgb_clf, X_train, y_train, X_test, y_test, train=True)\nprint_score(xgb_clf, X_train, y_train, X_test, y_test, train=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d3ee982243b491324b3d17d1486394a01bbbcf96"},"cell_type":"markdown","source":"## 6. Random Forest with AdaBoost"},{"metadata":{"trusted":true,"_uuid":"6033529a76d699ae07ce6b91dc41f237764aec1b"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nada_rf_clf = AdaBoostClassifier(RandomForestClassifier())\nada_rf_clf.fit(X_train, y_train)\n\nprint_score(ada_rf_clf, X_train, y_train, X_test, y_test, train=True)\nprint_score(ada_clf, X_train, y_train, X_test, y_test, train=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"57de2ab4eb57bfc62375c9db96106094f62c7879"},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\n\ngbc = GradientBoostingClassifier()\ngbc.fit(X_train,y_train)\n\nprint_score(gbc, X_train, y_train, X_test, y_test, train=True)\nprint_score(gbc, X_train, y_train, X_test, y_test, train=False)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"737745856f22075f500a83917192922de8b5aab7"},"cell_type":"markdown","source":"# Accuracy on various models\n### Logistic regression( %82.68)  and SVM performed ( %81.56 ) on test data.\n### Adaboost quite well as SVM but hav good fit over trained data with ( %81.56) on test data\n### Xgb Boost performed well over train test and cross validation and test data ( %81.56 )\n### Random forest have high deviation during and trining and valiation causing overfitting ( %81.01)"},{"metadata":{"_uuid":"5b89054de60f9d54e0fae6eb8f7538e473900a0e"},"cell_type":"markdown","source":"## Final submission Result"},{"metadata":{"trusted":true,"_uuid":"9f95749293085cd4fd3ea3d8f7be0b85a24cabf3"},"cell_type":"markdown","source":"# Based on all the Ensemble model AdaBoost performed best among all others models.\nfrom sklearn.ensemble import AdaBoostClassifier\n\nada_clf = AdaBoostClassifier()\nada_clf.fit(all_X[columns], all_y)\nholdout_predictions = ada_clf.predict(holdout[columns])\nholdout_ids = holdout[\"PassengerId\"]\nsubmission_df = {\"PassengerId\": holdout_ids,\n                 \"Survived\": holdout_predictions}\nsubmission = pd.DataFrame(submission_df)\n\nsubmission.to_csv(\"ada_boost.csv\",index=False)"},{"metadata":{"trusted":true,"_uuid":"a8344a928b4b2efe4b5748b7a05c2937019b789a"},"cell_type":"markdown","source":"# Adaboost with random forest\nfrom sklearn.ensemble import RandomForestClassifier\n\nada_rm_clf = AdaBoostClassifier(RandomForestClassifier())\nada_rm_clf.fit(all_X[columns], all_y)\nholdout_predictions = ada_rm_clf.predict(holdout[columns])\nholdout_ids = holdout[\"PassengerId\"]\nsubmission_df = {\"PassengerId\": holdout_ids,\n                 \"Survived\": holdout_predictions}\nsubmission = pd.DataFrame(submission_df)\n\nsubmission.to_csv(\"ada_boost_rf.csv\",index=False)\n"},{"metadata":{"trusted":true,"_uuid":"19a1dc06d02a1e58981eabc45435f09c18ea8897"},"cell_type":"code","source":"import xgboost as xgb\n\nxgb_clf = xgb.XGBClassifier()\nxgb_clf.fit(all_X[columns], all_y)\nholdout_predictions = xgb_clf.predict(holdout[columns])\nholdout_ids = holdout[\"PassengerId\"]\nsubmission_df = {\"PassengerId\": holdout_ids,\n                 \"Survived\": holdout_predictions}\nsubmission = pd.DataFrame(submission_df)\n\nsubmission.to_csv(\"xg_boost.csv\",index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af45c9bab8a97e7be6d5dd4e8e809b0630f0e37a"},"cell_type":"markdown","source":"from sklearn.ensemble import RandomForestClassifier\n\nrf_clf = RandomForestClassifier()\nrf_clf.fit(all_X[columns], all_y)\nholdout_predictions = rf_clf.predict(holdout[columns])\nholdout_ids = holdout[\"PassengerId\"]\nsubmission_df = {\"PassengerId\": holdout_ids,\n                 \"Survived\": holdout_predictions}\nsubmission = pd.DataFrame(submission_df)\n\nsubmission.to_csv(\"random_forest.csv\",index=False)\n"},{"metadata":{"trusted":true,"_uuid":"2fd0b6186482c74f4f745236c7bd07837dd71e78"},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\n\ngbc = GradientBoostingClassifier()\ngbc.fit(all_X[columns], all_y)\nholdout_predictions = gbc.predict(holdout[columns])\nholdout_ids = holdout[\"PassengerId\"]\nsubmission_df = {\"PassengerId\": holdout_ids,\n                 \"Survived\": holdout_predictions}\nsubmission = pd.DataFrame(submission_df)\n\nsubmission.to_csv(\"gradient_boost.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"adb719c0159df4adf7068840cea76ef2ec4880e7"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}