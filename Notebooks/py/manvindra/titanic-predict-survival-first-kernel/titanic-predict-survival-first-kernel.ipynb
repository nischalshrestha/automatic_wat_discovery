{"nbformat_minor": 1, "metadata": {"kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}, "language_info": {"pygments_lexer": "ipython3", "version": "3.6.4", "mimetype": "text/x-python", "nbconvert_exporter": "python", "codemirror_mode": {"version": 3, "name": "ipython"}, "file_extension": ".py", "name": "python"}}, "cells": [{"execution_count": null, "metadata": {"_cell_guid": "2eaca34d-43b1-404d-9182-20026b3d791d", "_uuid": "76dafd0a1dea72ee8929d234649179406e6277a0"}, "cell_type": "code", "outputs": [], "source": ["#This cell contains basic code from Kaggle and following cells follows outlines and code from Manav Sehgal notebook(Titanic Data Science Solutions)\n", "# Also took learning with code from from https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python\n", "# Learning Python using the above notebooks.\n", "#This Python 3 environment comes with many helpful analytics libraries installed\n", "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n", "# For example, here's several helpful packages to load in \n", "\n", "import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "\n", "# Any results you write to the current directory are saved as output."]}, {"execution_count": null, "metadata": {"_cell_guid": "40f45817-2197-42fc-b19c-ff68b8cf112e", "collapsed": true, "_uuid": "7e2009ff141210a1c45eff7e7f450329ad7b9c9d"}, "cell_type": "code", "outputs": [], "source": ["# visualization libs\n", "import seaborn as sns\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline\n", "plt.rc('font', family='sans-serif') \n", "plt.rc('font', serif='Helvetica Neue') \n", "plt.rc('text', usetex='false') \n", "plt.rcParams.update({'font.size': 10})"]}, {"execution_count": null, "metadata": {"_cell_guid": "9ee598ad-61e8-46cc-a161-bbbb52d597f9", "collapsed": true, "_uuid": "e553297617ec7ec478c4eccc00992f5a64928da3"}, "cell_type": "code", "outputs": [], "source": ["#Import ML Classfication libs\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.svm import SVC, LinearSVC\n", "from sklearn.neighbors import KNeighborsClassifier\n", "from sklearn.naive_bayes import GaussianNB\n", "from sklearn.linear_model import Perceptron\n", "from sklearn.linear_model import SGDClassifier\n", "from sklearn.tree import DecisionTreeClassifier\n", "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n", "from sklearn.model_selection  import KFold;\n", "import xgboost as xgb"]}, {"execution_count": null, "metadata": {"_cell_guid": "89148bb6-90d8-4d3f-83b9-1f31e4cd6b5e", "collapsed": true, "_uuid": "22459b274d85f1db2cf058c9e7cfae67b6626f4a"}, "cell_type": "code", "outputs": [], "source": ["#Acquire data\n", "train_df = pd.read_csv('../input/train.csv')\n", "test_df = pd.read_csv('../input/test.csv')\n", "combine = [train_df, test_df] #will be helpful in finding all distinct titles."]}, {"metadata": {"_cell_guid": "f50669fc-9e92-4adf-8449-38c807592f10", "_uuid": "dcdae495018e98a7b89c8151af732a993831ea65"}, "cell_type": "markdown", "source": ["**Exploring dataset**"]}, {"execution_count": null, "metadata": {"_cell_guid": "34d8cc53-bb14-47c4-b932-a17f9ddfb2a2", "_uuid": "932b34681860e579beb443c57a1ba8006a005818"}, "cell_type": "code", "outputs": [], "source": ["print(train_df.columns.values)\n", "train_df.head()"]}, {"execution_count": null, "metadata": {"_cell_guid": "6fd40027-0ada-44bf-baf3-4453e6cc0a6e", "_uuid": "fbc43b4992f90a5137abe7749a21adbc7aa1198c"}, "cell_type": "code", "outputs": [], "source": ["#Data Type of features\n", "train_df.info()\n", "print(\"------------------\")\n", "test_df.info()"]}, {"metadata": {"_cell_guid": "42bc8f56-3eca-42dc-ac92-bacb1042fed7", "_uuid": "98196d64547d19c40c95137e4be3cd1c4bd1d979"}, "cell_type": "markdown", "source": ["From a sample of the RMS Titanic data, we can see the various features present for each passenger on the ship:\n", "- **Survived**: Outcome of survival (0 = No; 1 = Yes)\n", "- **Pclass**: Socio-economic class (1 = Upper class; 2 = Middle class; 3 = Lower class)\n", "- **Name**: Name of passenger\n", "- **Sex**: Sex of the passenger\n", "- **Age**: Age of the passenger (Some entries contain `NaN`)\n", "- **SibSp**: Number of siblings and spouses of the passenger aboard\n", "- **Parch**: Number of parents and children of the passenger aboard\n", "- **Ticket**: Ticket number of the passenger\n", "- **Fare**: Fare paid by the passenger\n", "- **Cabin** Cabin number of the passenger (Some entries contain `NaN`)\n", "- **Embarked**: Port of embarkation of the passenger (C = Cherbourg; Q = Queenstown; S = Southampton)"]}, {"metadata": {"_cell_guid": "ce3bd623-883d-4c94-9728-fd52f6f98a72", "_uuid": "e59abbf770a16d6782332856a17aadc876d23251"}, "cell_type": "markdown", "source": ["**Distribution of numberical features from training data,**\n", "* Total 891 sample, represented 40% of passengers (2,224)\n", "* 38% of passenger in train dataset survived .Actual surviving rate is 32%\n", "* More than 75% of passengers didn't travel with parent or childern. (Parch 75% =0)\n", "* Less than 25% of passengers have siblings and spouse abroad."]}, {"execution_count": null, "metadata": {"_cell_guid": "dffe9470-b298-4b11-b655-0aba8662d4e5", "_uuid": "141374f6d4238cb7be739c7f89fb3b78fd04fe85"}, "cell_type": "code", "outputs": [], "source": ["#Distribution of numerical features\n", "train_df.describe()"]}, {"metadata": {"_cell_guid": "37eec4e9-6539-45a6-9828-14a130194d5d", "_uuid": "e1bcfd8a2c32aac4c1318ff3cf2f9c6485ba8e1f"}, "cell_type": "markdown", "source": ["**Distribution of Categorical features**\n", "* Name are unique. It can be dropped but will take Title from it as new feature\n", "* Two values of sex,most are male\n", "* No seprate ticket for all Family or friends. Family can hae single tickets. Can be dropped as many duplicates and also doesn't relates to survival\n", "* Out of 3 Embarked value, S tops the list. \n", "* Cabin can be shared. "]}, {"execution_count": null, "metadata": {"_cell_guid": "5e8bf081-f23f-4a87-af63-bfb7f280a9a3", "collapsed": true, "_uuid": "c0b8b462078668ed1bf777691f11313227356832"}, "cell_type": "code", "outputs": [], "source": ["#Distribution of Categorical data\n", "train_df.describe(include=['O'])\n", "#Cabin has lot of null, drop it\n", "train_df.drop(\"Cabin\",axis=1,inplace=True)\n", "test_df.drop(\"Cabin\",axis=1,inplace=True)\n", "#Drop ticket number also\n", "train_df.drop(\"Ticket\",axis=1,inplace=True)\n", "test_df.drop(\"Ticket\",axis=1,inplace=True)"]}, {"metadata": {"_cell_guid": "8d86d441-52b4-4e4e-a66e-f0e9a7a667bc", "_uuid": "c0dc786b6a4b7f3c2ca9fe861ce25ccd3b58b22d"}, "cell_type": "markdown", "source": ["**Hypothesis we can think of**\n", "* Women and children are more likely to have survived [](http://)\n", "* Aged passengers less likely to have survived\n", "* Higher Class passengers more likely to have survived"]}, {"metadata": {"_cell_guid": "0926d17e-6a22-49ab-ad04-b1819e34125a", "_uuid": "2d65294cefe38080c084bb68caa189d3ae7453a5"}, "cell_type": "markdown", "source": ["**Visualization**"]}, {"execution_count": null, "metadata": {"_cell_guid": "c5e8a2d2-f428-48a1-ac0f-0b7a2bf4495d", "_uuid": "8c5a76454c4dfbf2810cd1f60fb8ef045bade375"}, "cell_type": "code", "outputs": [], "source": ["train_df.hist(bins=10,figsize=(10, 10),grid=True);"]}, {"execution_count": null, "metadata": {"_cell_guid": "3c62645a-e342-4a04-b11b-3b117030702a", "_uuid": "fc5f1daf867d435a7d1f2a5b962152570e05b15a"}, "cell_type": "code", "outputs": [], "source": ["# Embarked =S, Pclass=3, and No SibSp has large set of passenger who didn't survived\n", "fig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(15,5))\n", "sns.countplot(x='Survived', hue=\"Embarked\", data=train_df, order=[1,0],ax=axis1)\n", "sns.countplot(x='Survived', hue=\"Pclass\", data=train_df, order=[1,0],ax=axis2)\n", "sns.countplot(x='Survived', hue=\"SibSp\", data=train_df, order=[1,0],ax=axis3)"]}, {"execution_count": null, "metadata": {"_cell_guid": "b65a2e0d-5b19-494c-a569-dbb25099f8c0", "collapsed": true, "_uuid": "8f79cfb570ba297f29bc1efb1f347f6308cf4fa4"}, "cell_type": "code", "outputs": [], "source": ["# Remove all NULLS in the Embarked column\n", "for dataset in combine:\n", "    dataset['Embarked'] = dataset['Embarked'].fillna('S')"]}, {"execution_count": null, "metadata": {"_cell_guid": "fc11a88c-b6b7-4e41-a0fb-1b0d32204669", "_uuid": "34d24985838ec06d1220ff0c6a4e537d7bd6d838"}, "cell_type": "code", "outputs": [], "source": ["g = sns.FacetGrid(train_df, col='Survived')\n", "g.map(plt.hist, 'Age', bins=20)\n", "# Most passenger above 60 yr of age didn't survive.\n", "# Childern survival rate is higher.\n", "#Consider for Model training"]}, {"execution_count": null, "metadata": {"_cell_guid": "80c3cc0a-44d4-425c-8902-92a46166281f", "_uuid": "504dff896f33e46f9364632f80dae26b1a2f1689"}, "cell_type": "code", "outputs": [], "source": ["#Age important variable, Fill Null value with random assigment between +-1SD from mean\n", "average_age_train   = train_df[\"Age\"].mean()\n", "std_age_train       = train_df[\"Age\"].std()\n", "count_nan_age_train = train_df[\"Age\"].isnull().sum()\n", "average_age_test   = test_df[\"Age\"].mean()\n", "std_age_test       = test_df[\"Age\"].std()\n", "count_nan_age_test = test_df[\"Age\"].isnull().sum()\n", "\n", "\n", "random_age1=np.random.randint(average_age_train - std_age_train, average_age_train + std_age_train, size = count_nan_age_train)\n", "random_age2=np.random.randint(average_age_test - std_age_test, average_age_test + std_age_test, size = count_nan_age_test)\n", "train_df[\"Age\"][np.isnan(train_df[\"Age\"])] = random_age1\n", "test_df[\"Age\"][np.isnan(test_df[\"Age\"])] = random_age2\n", "train_df['Age'] = train_df['Age'].astype(int)\n", "test_df['Age'] = test_df['Age'].astype(int)\n", "train_df['CategoricalAge'] = pd.cut(train_df['Age'], 5)"]}, {"execution_count": null, "metadata": {"_cell_guid": "5e003d05-94d9-435e-9107-53ce1e1c8059", "_uuid": "59af31e6132902308385e03dce66e849eb14010d"}, "cell_type": "code", "outputs": [], "source": ["g = sns.FacetGrid(train_df, col=\"Sex\", row=\"Survived\", margin_titles=True)\n", "g.map(plt.hist, \"Age\");\n", "#Female has higher survival rate"]}, {"execution_count": null, "metadata": {"_cell_guid": "0ce349e8-0bf1-40e7-b634-9b044ad43fa4", "_uuid": "2fed52b763fdfc96e4287e2c96654639f877f8e4"}, "cell_type": "code", "outputs": [], "source": ["g = sns.FacetGrid(train_df, hue=\"Survived\", col=\"Pclass\", margin_titles=True)\n", "g=g.map(plt.scatter, \"Fare\", \"Age\",edgecolor=\"w\").add_legend();\n", "#High Class and Fare have better survival rate. Create band of fare (Think Decision Tree split)\n", "#Fill Null\n", "test_df[\"Fare\"].fillna(test_df[\"Fare\"].median(), inplace=True)"]}, {"execution_count": null, "metadata": {"_cell_guid": "1a175df6-24c8-4915-986d-45e127e7d72f", "collapsed": true, "_uuid": "4c76595ee67e81a87f4b9d079cda1759e073a1a6"}, "cell_type": "code", "outputs": [], "source": ["#Fare null values imputation\n", "for dataset in combine:\n", "    dataset['Fare'] = dataset['Fare'].fillna(train_df['Fare'].median())"]}, {"execution_count": null, "metadata": {"_cell_guid": "8318e061-fe26-4c53-9877-3fb990a66133", "collapsed": true, "_uuid": "7813fb198758086ed28dafb2d2f6b969bbdd076c"}, "cell_type": "code", "outputs": [], "source": ["#Fare categories \n", "train_df['CategoricalFare'] = pd.qcut(train_df['Fare'], 4)"]}, {"execution_count": null, "metadata": {"_cell_guid": "5eceb435-a64d-42e6-9a39-edac159d14ad", "collapsed": true, "_uuid": "751bb6f95c41566520f7f3ee314554fa01214561"}, "cell_type": "code", "outputs": [], "source": ["# from https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python\n", "#Title from names\n", "# Define function to extract titles from passenger names\n", "import re\n", "def get_title(name):\n", "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n", "    # If the title exists, extract and return it.\n", "    if title_search:\n", "        return title_search.group(1)\n", "    return \"\"\n", "# Create a new feature Title, containing the titles of passenger names\n", "for dataset in combine:\n", "    dataset['Title'] = dataset['Name'].apply(get_title)\n", "# Group all non-common titles into one single grouping \"Rare\"\n", "for dataset in combine:\n", "    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n", "\n", "    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n", "    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n", "    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')  \n", "for dataset in combine:\n", "    # Mapping Sex\n", "    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n", "    \n", "    # Mapping titles\n", "    title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n", "    dataset['Title'] = dataset['Title'].map(title_mapping)\n", "    dataset['Title'] = dataset['Title'].fillna(0)\n", "    \n", "    # Mapping Embarked\n", "    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n", "    \n", "    # Mapping Fare\n", "    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] \t\t\t\t\t\t        = 0\n", "    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n", "    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n", "    dataset.loc[ dataset['Fare'] > 31, 'Fare'] \t\t\t\t\t\t\t        = 3\n", "    dataset['Fare'] = dataset['Fare'].astype(int)\n", "    \n", "    # Mapping Age\n", "    dataset.loc[ dataset['Age'] <= 16, 'Age'] \t\t\t\t\t       = 0\n", "    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n", "    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n", "    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n", "    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4 ;\n", "    "]}, {"execution_count": null, "metadata": {"_cell_guid": "23e29ad9-8461-4399-b544-32cf9f3d7892", "_uuid": "d24b42ae594af03346623967c4aad7678c5406b0"}, "cell_type": "code", "outputs": [], "source": ["train_df.head()"]}, {"execution_count": null, "metadata": {"_cell_guid": "c2be5a48-83e7-43a9-b7cb-0fb64b8e0389", "collapsed": true, "_uuid": "25bd1631d2af54f6dcc76b647e48443741b9dbbb"}, "cell_type": "code", "outputs": [], "source": ["# Feature selection\n", "drop_elements = ['PassengerId', 'Name']\n", "train_set = train_df.drop(train_df.columns[3], axis = 1)\n", "train_set = train_set.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\n", "test_set  = test_df.drop(drop_elements, axis = 1)"]}, {"execution_count": null, "metadata": {"_cell_guid": "35f6881b-6aab-4c9d-9ef4-1752dbbc5543", "_uuid": "894c1a7d88bf7e22721ff8ec69e2addf7a775e75"}, "cell_type": "code", "outputs": [], "source": ["test_set.head()"]}, {"execution_count": null, "metadata": {"_cell_guid": "8994cab3-4134-4c48-9b78-08ca4e268205", "_uuid": "457e0c324e6803b9a1668bef83c438b4c5a43a28"}, "cell_type": "code", "outputs": [], "source": ["colormap = plt.cm.viridis\n", "plt.figure(figsize=(12,12))\n", "plt.title('Pearson Correlation of Features', y=1.05, size=15)\n", "sns.heatmap(train_set.astype(float).corr(),linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True)"]}, {"metadata": {"_cell_guid": "57c190a9-1ece-404a-b205-77b43578ca23", "_uuid": "38ba16f5681078aee130d79604bfd5f6abaff1c8"}, "cell_type": "markdown", "source": ["**Ensemble and stacking models**\n", "[https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python/notebook](http://)"]}, {"execution_count": null, "metadata": {"_cell_guid": "57458fcf-cabc-4a7d-8bd2-f24802697afe", "collapsed": true, "_uuid": "f7c765ccd9d8d4250dfeb11404e0618391faf7a5"}, "cell_type": "code", "outputs": [], "source": ["#training ,test data\n", "train = train_set.drop([\"Survived\",\"PassengerId\"] , axis=1)\n", "x_train = train.values\n", "y_train = train_set[\"Survived\"].ravel()\n", "x_test = test_set.values\n", "#x_train.shape, y_train.shape, x_test.shape"]}, {"execution_count": null, "metadata": {"_cell_guid": "490a1973-8e2b-4ebf-b963-641ba8a6efe8", "_uuid": "2d08aa5654028ef849174c20bdc94036f1a7b108"}, "cell_type": "code", "outputs": [], "source": ["# Some useful parameters which will come in handy later on\n", "ntrain = train_set.shape[0]\n", "ntest = test_set.shape[0]\n", "print(ntrain,ntest)\n", "SEED = 0 # for reproducibility\n", "NFOLDS = 5 # set folds for out-of-fold prediction\n", "kf = KFold(n_splits= NFOLDS, random_state=SEED)\n", "\n", "# Class to extend the Sklearn classifier\n", "class SklearnHelper(object):\n", "    def __init__(self, clf, seed=0, params=None):\n", "        params['random_state'] = seed\n", "        self.clf = clf(**params)\n", "\n", "    def train(self, x_train, y_train):\n", "        self.clf.fit(x_train, y_train)\n", "\n", "    def predict(self, x):\n", "        return self.clf.predict(x)\n", "    \n", "    def fit(self,x,y):\n", "        return self.clf.fit(x,y)\n", "    \n", "    def feature_importances(self,x,y):\n", "        return(self.clf.fit(x,y).feature_importances_)"]}, {"execution_count": null, "metadata": {"_cell_guid": "aaff3b87-a2dd-4e7e-b831-eb754c0607fd", "collapsed": true, "_uuid": "9beed9c216cf4d4e5dea5349eb953b59cbba6638"}, "cell_type": "code", "outputs": [], "source": ["#Out of Fold Prediction\n", "def get_oof(clf, x_train, y_train, x_test):\n", "    oof_train = np.zeros((ntrain,))\n", "    oof_test = np.zeros((ntest,))\n", "    oof_test_skf = np.empty((NFOLDS, ntest))\n", "    i=0;\n", "    for train_index, test_index in kf.split(x_train):\n", "        x_tr, x_te = x_train[train_index], x_train[test_index]\n", "        y_tr = y_train[train_index]\n", "\n", "        clf.train(x_tr, y_tr)\n", "\n", "        oof_train[test_index] = clf.predict(x_te)\n", "        oof_test_skf[i, :] = clf.predict(x_test)\n", "        i=i+1\n", "    oof_test[:] = oof_test_skf.mean(axis=0)\n", "    \n", "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)"]}, {"execution_count": null, "metadata": {"_cell_guid": "ba9065a7-e577-4fd7-8cbc-6e559fce29aa", "collapsed": true, "_uuid": "235a60b3bc5eb1e0d18cb38e423da6afc2df2c47"}, "cell_type": "code", "outputs": [], "source": ["#Setting params for classifiers\n", "# Random Forest params\n", "rf_params = {\n", "    'n_jobs' : -1,\n", "    'n_estimators': 500,\n", "     'warm_start': True, \n", "     #'max_features': 0.2,\n", "    'max_depth': 6,\n", "    'min_samples_leaf': 2,\n", "    'max_features' : 'sqrt'\n", "}\n", "# Extra Trees Parameters\n", "et_params = {\n", "    'n_jobs': -1,\n", "    'n_estimators':500,\n", "    #'max_features': 0.5,\n", "    'max_depth': 86,\n", "    'min_samples_leaf': 2\n", "}\n", "# AdaBoost parameters\n", "ada_params = {\n", "    'n_estimators': 500,\n", "    'learning_rate' : 0.75\n", "}\n", "# Gradient Boosting parameters\n", "gb_params = {\n", "    'n_estimators': 500,\n", "     #'max_features': 0.2,\n", "    'max_depth': 5,\n", "    'min_samples_leaf': 2\n", "}\n", "\n", "# Support Vector Classifier parameters \n", "svc_params = {\n", "    'kernel' : 'linear',\n", "    'C' : 0.025\n", "    }"]}, {"execution_count": null, "metadata": {"_cell_guid": "dc82537c-9a4e-4697-ba17-9be29385d90c", "collapsed": true, "_uuid": "772bb79fa95ae9887923fcc89cd2794fc01b44f5"}, "cell_type": "code", "outputs": [], "source": ["#Create object of each classifier\n", "rf = SklearnHelper(clf=RandomForestClassifier, seed=SEED, params=rf_params)\n", "et = SklearnHelper(clf=ExtraTreesClassifier, seed=SEED, params=et_params)\n", "ada = SklearnHelper(clf=AdaBoostClassifier, seed=SEED, params=ada_params)\n", "gb = SklearnHelper(clf=GradientBoostingClassifier, seed=SEED, params=gb_params)\n", "svc = SklearnHelper(clf=SVC, seed=SEED, params=svc_params)"]}, {"execution_count": null, "metadata": {"_cell_guid": "87aa8ec3-22a2-43b2-ae43-bbc123c1c90c", "_uuid": "88ac52e4af8272caa3df245fb352fb5d670f1d01"}, "cell_type": "code", "outputs": [], "source": ["#fit \n", "et_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test) # Extra Trees\n", "rf_oof_train, rf_oof_test = get_oof(rf,x_train, y_train, x_test) # Random Forest\n", "ada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) # AdaBoost \n", "gb_oof_train, gb_oof_test = get_oof(gb,x_train, y_train, x_test) # Gradient Boost\n", "svc_oof_train, svc_oof_test = get_oof(svc,x_train, y_train, x_test) # Support Vector Classifier\n", "\n", "print(\"_____ Complete\")"]}, {"execution_count": null, "metadata": {"_cell_guid": "689fc0bf-bc80-43a3-bc18-d2f8e6209bb9", "_uuid": "e36f074cf63425e261c313174aa54a0a2aa5de66"}, "cell_type": "code", "outputs": [], "source": ["#Feature Importance\n", "rf_feature=rf.feature_importances(x_train,y_train)\n", "et_feature = et.feature_importances(x_train, y_train)\n", "ada_feature = ada.feature_importances(x_train, y_train)\n", "gb_feature = gb.feature_importances(x_train,y_train)"]}, {"metadata": {"_cell_guid": "e2932881-93e2-4702-9262-b57a7be536de", "_uuid": "162634d4836b049dd0e80272c25db7f58df22b33"}, "cell_type": "markdown", "source": [" Not able to store by using \n", ">  rf_features=list(rf_feature)\n", "et_features=list(et_feature)\n", "ada_features=list(ada_feature)\n", "gb_features=list(gb_feature)"]}, {"execution_count": null, "metadata": {"_cell_guid": "4d0f13e8-8549-4f03-b144-deb84287bb8f", "collapsed": true, "_uuid": "be01298a3230f00ff9f9c8cfd2acf2eba3736894"}, "cell_type": "code", "outputs": [], "source": ["cols = train.columns.values\n", "# Create a dataframe with features\n", "feature_dataframe = pd.DataFrame( {'features': cols,\n", "     'Random Forest Feat': rf_feature,\n", "     'Extra Trees Feat': et_feature,\n", "      'AdaBoost Feat': ada_feature,\n", "    'GB Feat': gb_feature\n", "    })"]}, {"execution_count": null, "metadata": {"_cell_guid": "4a581118-a2fc-4ec3-a3fa-471b773d41d0", "_uuid": "d5eede5fd2fc8d9d81d646d2179b28dedbeaa489"}, "cell_type": "code", "outputs": [], "source": ["feature_dataframe.head()"]}, {"execution_count": null, "metadata": {"_cell_guid": "ca6619bf-e3d3-4a88-b2b3-fe79cf731c14", "_uuid": "727e96ca101b065eb401c9992c13e9e7ef348ce3"}, "cell_type": "code", "outputs": [], "source": ["fig, axs = plt.subplots(figsize=(20,10), ncols=2, nrows=2)\n", "\n", "g=sns.stripplot(y=feature_dataframe['Random Forest Feat'].values,\n", "              x=feature_dataframe['features'].values, data=feature_dataframe\n", "               ,size=20,ax=axs[0][0]);\n", "g.axes.set_title('Randrom Forest feature importance', fontsize=20,color=\"r\")\n", "\n", "g=sns.stripplot(y=feature_dataframe['Extra Trees Feat'].values,\n", "              x=feature_dataframe['features'].values, data=feature_dataframe\n", "               ,size=20,ax=axs[0][1]);\n", "g.axes.set_title('Extra Trees feature importance', fontsize=20,color=\"r\")\n", "\n", "g=sns.stripplot(y=feature_dataframe['AdaBoost Feat'].values,\n", "              x=feature_dataframe['features'].values, data=feature_dataframe\n", "               ,size=20,ax=axs[1][0]);\n", "g.axes.set_title('Adaboost feature importance', fontsize=20,color=\"r\")\n", "\n", "g=sns.stripplot(y=feature_dataframe['GB Feat'].values,\n", "              x=feature_dataframe['features'].values, data=feature_dataframe\n", "               ,size=20,ax=axs[1][1]);\n", "g.axes.set_title('GB feature importance', fontsize=20,color=\"r\")\n"]}, {"execution_count": null, "metadata": {"_cell_guid": "6befc740-6d1b-47d5-a1e8-1eb9468990f9", "_uuid": "4509fc5872c39bdf122113014a5a2d736cb34b6f"}, "cell_type": "code", "outputs": [], "source": ["# Create the new column containing the average of values\n", "\n", "feature_dataframe['mean'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise\n", "feature_dataframe.head(3)"]}, {"execution_count": null, "metadata": {"_cell_guid": "8aac4d01-61e7-4d80-acbe-53648197d8d8", "_uuid": "246c7407072f1b45cf2303ae2e7a4c8c4125ad5d"}, "cell_type": "code", "outputs": [], "source": ["base_predictions_train = pd.DataFrame( {'RandomForest': rf_oof_train.ravel(),\n", "     'ExtraTrees': et_oof_train.ravel(),\n", "     'AdaBoost': ada_oof_train.ravel(),\n", "      'GradientBoost': gb_oof_train.ravel()\n", "    })\n", "base_predictions_train.head()"]}, {"execution_count": null, "metadata": {"_cell_guid": "b8e2d06f-cb90-48b9-92e1-42b209037529", "_uuid": "8e739e739b0f1c69f1b1e01085031a4edebab54e"}, "cell_type": "code", "outputs": [], "source": ["sns.heatmap(base_predictions_train.astype(float).corr().values, \n", "        xticklabels=base_predictions_train.columns.values,\n", "        yticklabels=base_predictions_train.columns.values)"]}, {"execution_count": null, "metadata": {"_cell_guid": "54005ffb-c148-4df4-99af-029d446c7b10", "collapsed": true, "_uuid": "6c06d9f0dbc6406de614278676cc0200918b73ff"}, "cell_type": "code", "outputs": [], "source": ["x_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\n", "x_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)"]}, {"execution_count": null, "metadata": {"_cell_guid": "767bb966-d482-49e7-9818-ccb10dd553a2", "_uuid": "f6bbcb640368f3e618135209fe721650c4ba7906"}, "cell_type": "code", "outputs": [], "source": ["x_train.shape"]}, {"execution_count": null, "metadata": {"_cell_guid": "bb22ceb0-df33-4cbf-8746-3fb7660f327c", "collapsed": true, "_uuid": "12bcf4d70f55849ea9da64f12a1fc23269229302"}, "cell_type": "code", "outputs": [], "source": ["gbm = xgb.XGBClassifier(\n", "    #learning_rate = 0.02,\n", " n_estimators= 2000,\n", " max_depth= 4,\n", " min_child_weight= 2,\n", " #gamma=1,\n", " gamma=0.9,                        \n", " subsample=0.8,\n", " colsample_bytree=0.8,\n", " objective= 'binary:logistic',\n", " n_jobs= -1,\n", " scale_pos_weight=1).fit(x_train, y_train)\n", "predictions = gbm.predict(x_test)"]}, {"execution_count": null, "metadata": {"_cell_guid": "735011a7-a6a9-4111-8175-8022481868a2", "collapsed": true, "_uuid": "26eed3c13b8b7bb22007b2da694ce2f3c7b0d80c"}, "cell_type": "code", "outputs": [], "source": ["# Generate Submission File \n", "Submission = pd.DataFrame({ 'PassengerId': test_df['PassengerId'],\n", "                            'Survived': predictions })\n", "Submission.to_csv(\"Submission.csv\", index=False)"]}], "nbformat": 4}