{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n\n# for handling data\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# for visualisation\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n# for machine learning\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier,GradientBoostingClassifier, ExtraTreesClassifier)\nfrom sklearn.cross_validation import KFold\nfrom sklearn import preprocessing\nfrom sklearn.metrics import accuracy_score\n\n# importing data\ndf_train=pd.read_csv('../input/train.csv',sep=',')\ndf_test=pd.read_csv('../input/test.csv',sep=',')\ndf_data = df_train.append(df_test) # The entire data: train + test.","execution_count":89,"outputs":[]},{"metadata":{"_uuid":"fd73d71c88fd02a1632788ae37ad59716638aea2"},"cell_type":"markdown","source":"**Missing Data**"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"print(pd.isnull(df_data).sum())\nprint(pd.isnull(df_train).sum())\nprint(pd.isnull(df_test).sum())","execution_count":90,"outputs":[]},{"metadata":{"_uuid":"48afccfe21aea53f7993904f37ce6d35a2515810"},"cell_type":"markdown","source":"**Description of Training and Testing Data**"},{"metadata":{"trusted":true,"_uuid":"d6c5b381b595fec4737bd95275eb44e17970b320"},"cell_type":"code","source":"df_train.describe()","execution_count":91,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d79e1d2b43172f1a6e0c357a9381bbedd1475a19"},"cell_type":"code","source":"df_test.describe()","execution_count":92,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3c6da0bed1aec5188a9a0954343b4d776ee6f7b5"},"cell_type":"code","source":"df_data.columns","execution_count":93,"outputs":[]},{"metadata":{"_uuid":"3cd25c46f1e68414a719ac2ea84ae63d0e5429cd"},"cell_type":"markdown","source":"**Extracting Name Titles.**\n        We can use the help of name titles to get to know more about passenger categories like Gender, Class, etc."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0a0216c1a6762e73f2bfcee838b35b35c0154689"},"cell_type":"code","source":"df_data[\"Title\"] = df_data.Name.str.extract(' ([A-Za-z]+)\\.', expand=False) #Creating new column name Title","execution_count":94,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"541d189b6e5fbd73af4076e927c83a01981654ed"},"cell_type":"code","source":"df_data.Title.head() #Lets see the result.","execution_count":95,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9e38640ccdb4a2e6119d6ce4313c3857373dd69c"},"cell_type":"code","source":"df_data.Title.tail()","execution_count":96,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be760f92e8f2291b4e51946cb2177f6e7d58ac03"},"cell_type":"code","source":"df_data.Title","execution_count":97,"outputs":[]},{"metadata":{"_uuid":"4bc3b9de3b6bec619787ee7f9c402f5b72353af2"},"cell_type":"markdown","source":"> **Titles like Dona, Mrs, Miss can be classified as Single Title for eg. Miss  \nTitles like Lady Countess Sir can be classified as Single Title for eg. Honor**"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1e39134d5339f0cf2d188e4d70e5af2dce761f79"},"cell_type":"code","source":"#classify common titles. \ndf_data[\"Title\"] = df_data[\"Title\"].replace('Master', 'Master')\ndf_data[\"Title\"] = df_data[\"Title\"].replace('Mlle', 'Miss')\ndf_data[\"Title\"] = df_data[\"Title\"].replace(['Mme', 'Dona', 'Ms'], 'Mrs')\ndf_data[\"Title\"] = df_data[\"Title\"].replace(['Don','Jonkheer'],'Mr')\ndf_data[\"Title\"] = df_data[\"Title\"].replace(['Capt','Rev','Major', 'Col','Dr'], 'Millitary')\ndf_data[\"Title\"] = df_data[\"Title\"].replace(['Lady', 'Countess','Sir'], 'Honor')","execution_count":98,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"094de03eb145e138f4328b17d736c71a11da1be5"},"cell_type":"code","source":"# Assign in df_train and df_test:\ndf_train[\"Title\"] = df_data['Title'][:891]\ndf_test[\"Title\"] = df_data['Title'][891:]\n\n# convert Title categories to Columns\ntitledummies=pd.get_dummies(df_train[['Title']], prefix_sep='_') #Title\ndf_train = pd.concat([df_train, titledummies], axis=1) \nttitledummies=pd.get_dummies(df_test[['Title']], prefix_sep='_') #Title\ndf_test = pd.concat([df_test, ttitledummies], axis=1) ","execution_count":99,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"41febc027e1a0c7dcacab706061b43190db468ec"},"cell_type":"code","source":"#Fill the na values in Fare\ndf_data[\"Embarked\"]=df_data[\"Embarked\"].fillna('S') #NAN Values set to S class\ndf_train[\"Embarked\"] = df_data['Embarked'][:891] # Assign Columns to Train Data\ndf_test[\"Embarked\"] = df_data['Embarked'][891:] #Assign Columns to Test Data\nprint('Missing Data Fixed') # Print confirmation (looks good xD)","execution_count":100,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d152f52e73da8ffcd4a743702117bac364cb78d6"},"cell_type":"code","source":"# convert Embarked categories to Columns\ndummies=pd.get_dummies(df_train[[\"Embarked\"]], prefix_sep='_') #Embarked\ndf_train = pd.concat([df_train, dummies], axis=1) \ndummies=pd.get_dummies(df_test[[\"Embarked\"]], prefix_sep='_') #Embarked\ndf_test = pd.concat([df_test, dummies], axis=1)\nprint(\"Embarked created\")","execution_count":101,"outputs":[]},{"metadata":{"_uuid":"6a107a725aa24a57fa66b8a7ed611decbe351b87"},"cell_type":"markdown","source":"**Fixing Missing Fare Value **"},{"metadata":{"trusted":true,"_uuid":"d800d76692fb4a5dacd015c446a0071b81a84d8c","collapsed":true},"cell_type":"code","source":"# Fill the na values in Fare based on average fare\nimport warnings\nwarnings.filterwarnings('ignore')\ndf_data[\"Fare\"]=df_data[\"Fare\"].fillna(np.median(df_data[\"Fare\"]))\ndf_train[\"Fare\"] = df_data[\"Fare\"][:891]\ndf_test[\"Fare\"] = df_data[\"Fare\"][891:]","execution_count":102,"outputs":[]},{"metadata":{"_uuid":"118a45e92138a100e9e56c1ce22f8e10172061b1"},"cell_type":"markdown","source":"**Fixing missing Age Data.**"},{"metadata":{"trusted":true,"_uuid":"7ab6e190b5c546ace860599a027f771a07b07e3c"},"cell_type":"code","source":"titles = ['Master', 'Miss', 'Mr', 'Mrs', 'Millitary','Honor']\nfor title in titles:\n    age_to_impute = df_data.groupby('Title')['Age'].median()[title]\n    df_data.loc[(df_data['Age'].isnull()) & (df_data['Title'] == title), 'Age'] = age_to_impute\n# Age in df_train and df_test:\ndf_train[\"Age\"] = df_data['Age'][:891]\ndf_test[\"Age\"] = df_data['Age'][891:]\nprint('Missing Ages Estimated')","execution_count":103,"outputs":[]},{"metadata":{"_uuid":"f8a800b66e11b7b21802388d18601a6589dfb2a9"},"cell_type":"markdown","source":"**Name Feature**\n\nWe can drop the name feature now that we've extracted the titles."},{"metadata":{"trusted":true,"_uuid":"63c96eff06717f57ad5606eaadba2b69a5384f97"},"cell_type":"code","source":"#map each Sex value to a numerical value\nsex_mapping = {\"male\": 0, \"female\": 1}\ndf_train['Sex'] = df_train['Sex'].map(sex_mapping)\ndf_test['Sex'] = df_test['Sex'].map(sex_mapping)\n\ndf_train.head()","execution_count":104,"outputs":[]},{"metadata":{"_uuid":"5e58d1e7439ca05d05b6388a1b5c5c91848e8f20"},"cell_type":"markdown","source":"**Non numeric features**\n\nWe are going to use a decision tree model. The model requires only numeric values, but one of our features is categorical: \"female\" or \"male\". this can easily be fixed by encoding this feature: \"male\" = 1, \"female\" = 0. \n\nAlso we will drop **Cabin** because lots of data is missing and it can create trouble.\n\n"},{"metadata":{"trusted":true,"_uuid":"7b7169cb473f529869ad1929d0a693de88be1965","collapsed":true},"cell_type":"code","source":"df_train = df_train.drop(['Cabin'], axis = 1)\ndf_test = df_test.drop(['Cabin'], axis = 1)\ndf_train = df_train.drop(['Name'], axis = 1)\ndf_test = df_test.drop(['Name'], axis = 1)","execution_count":105,"outputs":[]},{"metadata":{"_uuid":"e52daf92dbec55767d6acf89562c8c78d64cf77e"},"cell_type":"markdown","source":"## Some Final Encoding\n\nThe last part of the preprocessing phase is to normalize labels. The LabelEncoder in Scikit-learn will convert each unique string value into a number, making out data more flexible for various algorithms. \n\nThe result is a table of numbers that looks scary to humans, but beautiful to machines. "},{"metadata":{"trusted":true,"_uuid":"df13793dcd226c671779a4311fee39dae62f043f"},"cell_type":"code","source":"from sklearn import preprocessing\ndef encode_features(df_train, df_test):\n    features = ['Title','Embarked','Ticket']\n    \n    df_combined = pd.concat([df_train[features], df_test[features]])\n    \n    for feature in features:\n        le = preprocessing.LabelEncoder()\n        le = le.fit(df_combined[feature])\n        df_train[feature] = le.transform(df_train[feature])\n        df_test[feature] = le.transform(df_test[feature])\n    return df_train, df_test\n    \ndf_train, df_test = encode_features(df_train, df_test)\ndf_train.head()","execution_count":106,"outputs":[]},{"metadata":{"_uuid":"6c67c9466087ff198c5cb19728eb5d7f5192fe08"},"cell_type":"markdown","source":"**Working on Prediction now**"},{"metadata":{"_uuid":"d277d38f728341b5d7c1e191b19375e5ae027de9"},"cell_type":"markdown","source":"**Predict**\n\nWe have our training data, and we have our test data. but in order to evaluate our model we need to split the training dataset into a train dataset and an evaluation dataset (validation). The validation data would be used to evaluate the model, while the training data would be used to train the data.\n\nTo do that, we can use the function \"train_test_split\" from the sklearn module. the sklean module is probably the most commonly used library in most simple machine learning tasks (this does not include deep learning where other libraries can be more popular)\n"},{"metadata":{"trusted":true,"_uuid":"e6717f84a70318d472327bdc41dbc6f64cb7a636","collapsed":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\npredictors = df_train.drop(['Survived', 'PassengerId'], axis=1)\ntarget = df_train[\"Survived\"]\nX_train, X_val, y_train, y_val = train_test_split(predictors, target, test_size = 0.22, random_state = 0)","execution_count":107,"outputs":[]},{"metadata":{"_uuid":"52f7afbd5dbc001d061cb556dec1493f78d9fcf5"},"cell_type":"markdown","source":"\n**Training the model**\n\nNow we are finally ready, and we can train the model.\n\nFirst, we need to import our model - A decision tree classifier (again, using the sklearn library).\n\nThen we would feed the model both with the data (X_train) and the answers for that data (y_train)\n"},{"metadata":{"trusted":true,"_uuid":"07b191febc23b9f869aa30863c34b66278bb2acd"},"cell_type":"code","source":"#importing Logistic Regression classifier\n\nfrom sklearn.linear_model import LogisticRegression\nmodel1 = LogisticRegression()\nmodel1.fit(X_train,y_train)","execution_count":108,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a72b34c00ab296aa160d64e01f18cd24b94d234b"},"cell_type":"code","source":"#printing the training score\nprint('The training score for logistic regression is:',(model1.score(X_train,y_train)*100),'%')\nprint('Validation accuracy', accuracy_score(y_val, model1.predict(X_val)))","execution_count":109,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c953370d2c63a018a28b611cd08d9b4c8003490f"},"cell_type":"code","source":"#importing random forest classifier\n\nfrom sklearn.ensemble import RandomForestClassifier\nmodel2 = RandomForestClassifier(n_estimators=6)\nmodel2.fit(X_train,y_train)","execution_count":110,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4d313a8931714467d82e894769bd81d60d9a3341"},"cell_type":"code","source":"#printing the training score\nprint('The training score for logistic regression is:',(model2.score(X_train,y_train)*100),'%')\nprint('Validation accuracy', accuracy_score(y_val, model2.predict(X_val)))","execution_count":111,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"410426f71958f9c7691107df74897a5ce72197b3"},"cell_type":"code","source":"#importing Gradient boosting classifier\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nmodel3 = GradientBoostingClassifier(n_estimators=7,learning_rate=1.1)\nmodel3.fit(X_train,y_train)","execution_count":112,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"54e316414227adf272d5c3495a2b1175b8858adf"},"cell_type":"code","source":"#printing the training score\nprint('The training score for logistic regression is:',(model3.score(X_train,y_train)*100),'%')\nprint('Validation accuracy', accuracy_score(y_val, model3.predict(X_val)))","execution_count":113,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3466737a2e996a0fc2391acd61dd83572a86bc2a"},"cell_type":"code","source":"from sklearn.model_selection import KFold #for K-fold cross validation\nfrom sklearn.model_selection import cross_val_score #score evaluation\nfrom sklearn.model_selection import cross_val_predict #prediction\nfrom sklearn import svm #support vector Machine\nkfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\nxyz=[]\naccuracy=[]\nstd=[]\ndf_test = df_test.dropna()\nclassifiers=['Logistic Regression','Random Forest','GradientBoosting']\nmodels=[LogisticRegression(),RandomForestClassifier(n_estimators=100),GradientBoostingClassifier(n_estimators=7,learning_rate=1.1)]\nfor i in models:\n    model = i\n    cv_result = cross_val_score(model,predictors,target, cv = kfold,scoring = \"accuracy\")\n    cv_result=cv_result\n    xyz.append(cv_result.mean())\n    std.append(cv_result.std())\n    accuracy.append(cv_result)\nnew_models_dataframe2=pd.DataFrame({'CV Mean':xyz,'Std':std},index=classifiers)       \nnew_models_dataframe2","execution_count":114,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4d8406479d1359debe20c9b471b9ff76cf785361"},"cell_type":"code","source":"plt.subplots(figsize=(12,6))\nbox=pd.DataFrame(accuracy,index=[classifiers])\nbox.T.boxplot()","execution_count":115,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"db36c5b0145d86079fbd1a0234ea18f6e345ae0f"},"cell_type":"code","source":"new_models_dataframe2['CV Mean'].plot.barh(width=0.8)\nplt.title('Average CV Mean Accuracy')\nfig=plt.gcf()\nfig.set_size_inches(8,5)\nplt.show()","execution_count":116,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"89c9737a52a0f7ce133d8ddf00ff404e19725820"},"cell_type":"code","source":"prediction = model2.predict(df_test)\npassenger_id = df_data[892:].PassengerId\ntest = pd.DataFrame( { 'PassengerId': passenger_id , 'Survived': prediction } )\ntest.shape\ntest.head()\ntest.to_csv( 'titanic_pred.csv' , index = False )","execution_count":123,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b00da2615de164573f340f1a5d86ceec9f817424"},"cell_type":"code","source":"\n\nsubmission = pd.read_csv('titanic_pred.csv')\nprint(submission.head())\nprint(submission.tail())\n\n","execution_count":124,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e928c4356930e76587fedec4180a284f86fbd941"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"cfd9b27c9f2290f92a6a31fdd1aace16749736b8"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}