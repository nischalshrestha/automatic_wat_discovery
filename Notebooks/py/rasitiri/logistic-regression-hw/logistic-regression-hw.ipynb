{"cells":[{"metadata":{"_uuid":"0949c170c1644b6c37a295c89c92d463734f38e5"},"cell_type":"markdown","source":"Hello everyone. I am doing Logistic Regression hw with Python.\nFirst of all, I am importing some libraries.\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ebac1de9467aa102e7f44d6ebe5294e9827e3ef1"},"cell_type":"markdown","source":"I am including Titanic Data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/train.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"13869900e428d4407152df1388d914cac91775c4"},"cell_type":"markdown","source":"Top 5 row"},{"metadata":{"trusted":true,"_uuid":"e99a91bf2f5d79088c2b6c95adf764cc1aa8e7fe"},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9a3a9d8f1b01b82bfc520f1ce5ee95bd1166906a"},"cell_type":"markdown","source":"**General overview**"},{"metadata":{"trusted":true,"_uuid":"4774d5570dee7af6adcbe72b53f8fdd544feedc0"},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"838b85ed96fcb469c17e7f08402411339eed4d9b"},"cell_type":"markdown","source":"I'm throwing out unnecessary information from dataset."},{"metadata":{"trusted":true,"_uuid":"511cbc6ec8c24669ca7fbece78ea2ee4a3d3d92b"},"cell_type":"code","source":"data.drop(['PassengerId','Cabin','Name','Sex','Ticket','Embarked','Age'],axis=1,inplace=True)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4db0650746343f4592045ae43238de124a2e7d50"},"cell_type":"markdown","source":"I am defining X and Y values"},{"metadata":{"trusted":true,"_uuid":"a1a917124fd0b19e8b6bf291a55ed75153361392"},"cell_type":"code","source":"y=data.Survived.values\nx_data = data.drop(['Survived'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"31c1aaea2f2302560a33eec2c8e01a52ae94d57d"},"cell_type":"markdown","source":"Normalization process for  X values"},{"metadata":{"trusted":true,"_uuid":"0dd2af8c67ac3fea7a23a9d7a84d5e72592fc76d"},"cell_type":"code","source":"x = (x_data - np.min(x_data)) / (np.max(x_data) - np.min(x_data)).values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"00a5981f965bac055069bc4d3355441c6d6f75c8"},"cell_type":"markdown","source":"Train - Test split process for Data \n\n%80 Train\n\n%20 Test"},{"metadata":{"trusted":true,"_uuid":"fb8fcb36dc1e7be6a446a2b8b425955a6c4354d7"},"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2,random_state=42)\n\nx_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T\ny_test = y_test.T\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"20934848b3fbeb613ed4f8718641178cc6cbb883"},"cell_type":"markdown","source":"I am creating initialize_weights_and_bias(acronym  => iwab ) and sigmoid functions for Logistic Regression Model\n\n\n"},{"metadata":{"trusted":true,"_uuid":"691f2bd71b479c87d7fd9e4cf4cf94b23418f1c4"},"cell_type":"code","source":"def iwab(dimension):\n    \n    w = np.full((dimension,1),0.01)\n    b = 0.0\n    return w,b\n\ndef sigmoid(z):\n    \n    y_head = 1/(1+ np.exp(-z))\n    return y_head","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"27cff7ed23d0b6149bfc174319eb248341dd3d6c"},"cell_type":"markdown","source":"Than I will create forward_backward_propagation function(acronym  = > fbp )"},{"metadata":{"trusted":true,"_uuid":"81ddfbcf86897c11e59dcdd97359e93fce158c95"},"cell_type":"code","source":"def fbp(w,b,x_train,y_train):\n    #Forward\n    z = np.dot(w.T,x_train) + b\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))/x_train.shape[1]\n    \n    #Backward\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))/x_train.shape[1]\n    derivative_bias = np.sum(y_head-y_train)/x_train.shape[1]                \n    gradients = {\"Derivative Weight\": derivative_weight, \"Derivative Bias\": derivative_bias}\n    \n    return cost,gradients","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e4bf558f2acbb442d9ce16ce7eef1045f296801a"},"cell_type":"markdown","source":"I will create a function for updating parameter"},{"metadata":{"trusted":true,"_uuid":"de62c97275c97e9c3053ebe53984387b81cc5020"},"cell_type":"code","source":"def update(w, b, x_train, y_train, learning_rate,number_of_iterarion):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    \n    for i in range(number_of_iterarion):\n        cost,gradients = fbp(w,b,x_train,y_train)\n        cost_list.append(cost)\n        w = w - learning_rate * gradients[\"Derivative Weight\"]\n        b = b - learning_rate * gradients[\"Derivative Bias\"]\n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n            \n    parameters = {\"weight\": w,\"bias\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a3e295070dc1dca83587e253f9085f4ddf53d670"},"cell_type":"markdown","source":"Now, I am creating prediction function  "},{"metadata":{"trusted":true,"_uuid":"fd4d8b030155ff4fe8de746283bed5cbc99648ef"},"cell_type":"code","source":"def predict(w,b,x_test):\n    \n    z = sigmoid(np.dot(w.T,x_test)+b)\n    y_pre = np.zeros((1,x_test.shape[1]))\n    \n    #   if z value is bigger than 0.5, our prediction is sign one (y_head=1),\n    #   if z value is smaller than 0.5, our prediction is sign zero (y_head=0),\n    \n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            y_pre[0,i] = 0\n        else:\n            y_pre[0,i] = 1\n\n    return y_pre","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e437073e48dbe9243c110c3e64e4f50df6401380"},"cell_type":"markdown","source":"Finally,  I will create Logistic Regression function   ( almost  I come to the end of the road  :)  )\n"},{"metadata":{"trusted":true,"_uuid":"196c7d96ad2fa788afcca32ea5589fe69588a30a"},"cell_type":"code","source":"def logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n  \n    dimension =  x_train.shape[0] \n    w,b = iwab(dimension)\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    print(\"Test Accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a1b1c994b61ced3fba1997629303ee1820c2ed4c"},"cell_type":"code","source":"logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 1, num_iterations = 50)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f04ee047b503f3fcad2faa5e02f330d66109e6e9"},"cell_type":"code","source":"logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 2, num_iterations = 200)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cd4cc6a03cc7a4b9c0d227d9b68f96aa5f4596cf"},"cell_type":"code","source":"logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 3, num_iterations = 500)  ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5e6abb2c1eea90b7485aa7884e0537bbe0497355"},"cell_type":"markdown","source":"I tried different learning rate  and num iterations values for finding best test accuracy score. The best one I found test accuracy score: %71.5 for learning_rate = 3, num_iterations = 500.(\tmustn't grumble )\n\nIt's could be better.\n"},{"metadata":{"_uuid":"31e6d146af25256e5e8f742862950f44cc8ca791"},"cell_type":"markdown","source":"Terminally,  I will create Logistic Regression Model with Sklearn Library"},{"metadata":{"trusted":true,"_uuid":"fd7d5feaf082f42ee40b6197886521dd5fa06a67"},"cell_type":"code","source":"lr = LogisticRegression()\nlr.fit(x_train.T,y_train.T)\nprint('Test Accuracy:',lr.score(x_test.T,y_test.T))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"56d0f3c7b61e9161a3d8b8bb7d62dfebb95bb5aa"},"cell_type":"markdown","source":"As you can see Test accuracy is %70.94\n\nIt's could be better in the same way\n\nI will be waiting for your comment "},{"metadata":{"trusted":true,"_uuid":"1f9824cf0a78cee0323d51e585ecd5b7020cb9e8"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}