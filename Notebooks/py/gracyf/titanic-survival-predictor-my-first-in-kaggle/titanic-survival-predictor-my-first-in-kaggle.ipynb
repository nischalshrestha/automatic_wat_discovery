{"cells":[{"metadata":{"_cell_guid":"df4f51fd-074e-4e7c-ae63-463385c3f53e","_uuid":"ef23a14f9a29708c3dca23ebbb5ccdefa4391e3e","trusted":true},"cell_type":"code","source":"%matplotlib notebook\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"aa7dfeff29078eaccbe1787b32f7a6ba2a91f7ac"},"cell_type":"code","source":"train = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4fb71e4a7cce9ea363a66683baaab7100086c875"},"cell_type":"code","source":"train.shape\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"174e822d9e9e9c8023c35f40d636cda652de6701"},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b900259d9e5a7a8ddeca557f71ccd3d8f456dc3f"},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c7ad325c3675cc7dc14fdf0ba94be4cf0956310c"},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8fb927033c8e722a96f509112ada666a2bd14957"},"cell_type":"markdown","source":"**Exploratory Data Analysis**","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"7190f640450d5ea3d680fe1fccc36a1db37e23f6"},"cell_type":"code","source":"#Combine test and train data sets\ntitanic = pd.concat([train,test],axis=0)\n#Fill the test set values of Survived with 0 \ntitanic['Survived'].fillna(0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"15c22e221e2c412530a5c4912acefd8478517e53"},"cell_type":"code","source":"titanic.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d12b32e7b618e6eb595aee189aaae35c72074616"},"cell_type":"code","source":"titanic.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"56f09ae875e8e9bb4b510ad320a7df969b572ca6"},"cell_type":"markdown","source":"There are totally 1309 rows and Age, Embarked, Cabin had lots of missing values. Survived is the target variable and is empty for all test dataset.","outputs":[],"execution_count":null},{"metadata":{"trusted":true,"_uuid":"f5e10255c231ad5c528a46ebf02c17e9aa9db3fd"},"cell_type":"code","source":"titanic.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bb610a1b56af999244e5ec854156d31cb4e0b3c9"},"cell_type":"code","source":"titanic.dtypes","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c5abf1a124731844d223afe7c0d09610ef1500ad"},"cell_type":"markdown","source":"Numeric types : Age ( continuous ), SibSp( discrete ), Parch ( discrete ), Fare ( continuous ) Identifiers : PassendgerId To predict : Survived ( category binomial ) Category : Sex, Embarked Ordinal : Pclass\n\n(A categorical variable (sometimes called a nominal variable) is one that has two or more categories, but there is no intrinsic ordering to the categories. )","outputs":[],"execution_count":null},{"metadata":{"trusted":true,"_uuid":"67a2030c29597f302bb9ab7a4b4a004cbe25bfdc"},"cell_type":"code","source":"x = titanic['PassengerId'].unique()\nx.shape ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"27ee13c2f6519c866920edd06ef0a3089ea226d8"},"cell_type":"markdown","source":"All PassengerIds are unique in both train and test sets. So there are no duplicate rows.","outputs":[],"execution_count":null},{"metadata":{"trusted":true,"_uuid":"34b69249c08776b9ea0471b8dd3de74a4338d1b3"},"cell_type":"code","source":"train.describe(include=['O'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e87d7afe6a87d72f91eb6bbce35e31d753f0ad7c"},"cell_type":"markdown","source":"We have lots of duplicates in Ticket column and cabin column. Lots of people stayed in same cabin or cabin information unknown.","outputs":[],"execution_count":null},{"metadata":{"trusted":true,"_uuid":"56b3d355225385c99a39e8855955c47eb6779401"},"cell_type":"code","source":"titanic.corr()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"034b27b1aaefbe2bbcddf8bb95b2b299c1600efd"},"cell_type":"markdown","source":"None of the columns are highly correlated.","outputs":[],"execution_count":null},{"metadata":{"trusted":true,"_uuid":"4c3cb173b482801c3990f67af559807c72a42fa7"},"cell_type":"code","source":"#To find if any row is a duplicate\ntitanic[titanic.nunique(axis=1) == 1] == True","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"45d05aede960944550a120a0939dc037f7ce8b53"},"cell_type":"markdown","source":"There are no duplicate rows in the dataset.","outputs":[],"execution_count":null},{"metadata":{"trusted":true,"_uuid":"b8eac6acca77d5836609236aa5e49fce416bdbf0"},"cell_type":"code","source":"#Duplicate rows are verified again.\ntitanic[titanic.duplicated() == True]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6661f07415dacfc7ad46af476d12ed2d7f21fbff"},"cell_type":"code","source":"#Plot the scatter matrix from train data set.\nfrom matplotlib import cm\ncmap = cm.get_cmap('gnuplot')\nscatter = pd.plotting.scatter_matrix(train, marker = 'o', s=40, hist_kwds={'bins':15}, figsize=(9,9), cmap=cmap)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4e6c2de3e5c6ae1a7dea7643f51b47bab8ff484a"},"cell_type":"code","source":"#Density plots of the all columns in \nfig = plt.figure(figsize = (10,15))\nax = fig.gca()\ntrain.plot(kind='density', subplots=True, layout=(3,3), sharex=False,ax = ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4e73f9b563f9baa7f52d489cf60f2ae7255d4c50"},"cell_type":"code","source":"fig = plt.figure(figsize = (8,12))\nax = fig.gca()\ntrain.plot(kind='box', subplots=True, layout=(3,3), sharex=False, sharey=False,ax=ax)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3163cada02c67b99a049557578c785d35fdd6b6d"},"cell_type":"code","source":"train.groupby('Survived').hist();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4acd124a1c85c577117dbdc63f2723936d391f0a"},"cell_type":"code","source":"import seaborn as sns\nsns.pairplot(train, hue='Survived', diag_kind='kde', size=2);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f759fd1e0102202bddca2f9b8954bc61e534f320"},"cell_type":"markdown","source":"Not much of interesting clusters and outliers are found.","outputs":[],"execution_count":null},{"metadata":{"trusted":true,"_uuid":"999eff85f84256a9186f73c9cebbfd766db2377d"},"cell_type":"code","source":"# absolute numbers\ntrain[\"Survived\"].value_counts()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"17c2f88716e4b7caedc871b3877dbb51a8ac2052"},"cell_type":"code","source":"# percentages\ntrain[\"Survived\"].value_counts(normalize = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d6813510dc136be678bff6405d1acc82577b5651"},"cell_type":"markdown","source":"Only 38% of the people survived in this Titanic crash.","outputs":[],"execution_count":null},{"metadata":{"trusted":true,"_uuid":"4bd8d705f78d43a421c97a95389e677850be631c"},"cell_type":"code","source":"#Percentage of people survived w.r.t age\nplt.figure()\ntrain['Age'].groupby(train['Survived']).hist(alpha=0.6,bins=50)\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"94ba4c96daccda253442eb52f8ec7dcaceedd21f"},"cell_type":"markdown","source":"Many of the mid aged people did not survive as compared to those who died. Many infants and people nearing 80 years of age has survived.","outputs":[],"execution_count":null},{"metadata":{"trusted":true,"_uuid":"3ec90ddd993036a08bf47d9557e3dee366cba138"},"cell_type":"code","source":"train.groupby('Survived').Age.value_counts().unstack()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9b7eaf29ee507181b9b3cd8ce3f66483a692f2a2"},"cell_type":"code","source":"plt.figure()\nsns.barplot(x=\"Sex\", y=\"Survived\", hue=\"Pclass\", data=train);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9bf575c45fe28dbb60f9ddf8fcbf6aa03c2b9353"},"cell_type":"code","source":"plt.figure()\nsns.barplot(x=\"Sex\", y=\"Survived\", hue=\"Embarked\", data=train);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4fff540d47674110ff43c6d18c16e04d110405cb"},"cell_type":"markdown","source":"The Percentage of Female survivors is greater than male survivors as we can see from the histogram above and the percentage below.","outputs":[],"execution_count":null},{"metadata":{"trusted":true,"_uuid":"142e166de2e1de1707559fb97e619ac3ca41e018"},"cell_type":"code","source":"train.groupby('Survived').Sex.value_counts(normalize = True).unstack()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"267369ad0f76774667156df661f20c6de311aaf9"},"cell_type":"code","source":"import seaborn as sns\ng = sns.FacetGrid(train, col='Survived')\ng.map(plt.hist, 'Age', bins=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5a187cc8cada5952f7f21e461ff6c6d690d80ecd"},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nplt.figure()\nsns.distplot(train['Age'].dropna())\nplt.show()\n#Age distribution follows a right skewed distribution. This has to be converted to a normal distribution.\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"31dff493bf0c64baee6b45f41992a6b5c1bcdf1f"},"cell_type":"code","source":"#Compare Age and Pclass\ng = sns.FacetGrid(train, row='Survived', col='Pclass')\ng.map(sns.distplot, \"Age\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a9799628981b9d6bed90719717fe9acda75d93d0"},"cell_type":"code","source":"category_group=train.groupby(['Survived','Pclass']).count()['PassengerId']\ncategory_group.unstack().head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a210ac9b36a1198148fa5540aae8266ea7815d26"},"cell_type":"markdown","source":"Pclass 1 has highest survival rate.","outputs":[],"execution_count":null},{"metadata":{"trusted":true,"_uuid":"6a345b81219ba1e6dd2dc3fd01e0ea335817b7d8"},"cell_type":"code","source":"g = sns.FacetGrid(train, row='Survived', col='Sex')\ng.map(sns.distplot, \"Age\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"98dd33885f483f8d88c6c0cfa97d5c3826f60f94"},"cell_type":"markdown","source":"Larger the family death rate is high, but with one sibling survival rate is high.With no sibling or spouse nothing significant.","outputs":[],"execution_count":null},{"metadata":{"trusted":true,"_uuid":"a03385a1cea02bb6b92915f4e0f99eafbaa25c23"},"cell_type":"code","source":"train[\"Survived\"][train[\"Sex\"] == 'male'].value_counts(normalize = True)\ntrain[\"Survived\"][train[\"Sex\"] == 'female'].value_counts(normalize = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"79452b7c20d9eab581f18e40390bbd8a97730092"},"cell_type":"code","source":"category_group=train.groupby(['Survived','SibSp']).count()['PassengerId']\ncategory_group.unstack().head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"51edc4c9850dd6c5c8b271e2f735ed6d54ef0cf5"},"cell_type":"code","source":"category_group=train.groupby(['Survived','Parch']).count()['PassengerId']\ncategory_group.unstack().head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"daa26e3a7b51acc1a2a78dd624384bf5ee7b746b"},"cell_type":"markdown","source":"People who travelled alone has expired than people who travelled with their sibiling or parent and children.","outputs":[],"execution_count":null},{"metadata":{"trusted":true,"_uuid":"3904407ff98189405ae4ce355d16fc9d7909b03d"},"cell_type":"code","source":"plt.figure()\nsns.heatmap(train.corr(), annot=True, fmt=\".2f\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"497ada1472cdf98b4a61fb6ddbde17b647eae362"},"cell_type":"markdown","source":"Correlation Matrix Plot Correlation gives an indication of how related the changes are between two variables. If two variables change in the same direction they are positively correlated. If the change in opposite directions together (one goes up, one goes down), then they are negatively correlated.\n\nYou can calculate the correlation between each pair of attributes. This is called a correlation matrix. You can then plot the correlation matrix and get an idea of which variables have a high correlation with each other.\n\nThis is useful to know, because some machine learning algorithms like linear and logistic regression can have poor performance if there are highly correlated input variables in your data.","outputs":[],"execution_count":null},{"metadata":{"trusted":true,"_uuid":"87b6d1f76dc7b886979b4f7f14a37774d710ef9e"},"cell_type":"code","source":"category_group=train.groupby(['Survived','Embarked']).count()['PassengerId']\ncategory_group.unstack().head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"209ab9f47e5faa3fcdcb21b1891b08a4e31d1eda"},"cell_type":"code","source":"category_group.unstack().plot(kind='bar',stacked=True,title=\"Survival As per Embarked Region\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"30bf3130bd4c5dc0fb86d0d7a95da1df2f7c7f8b"},"cell_type":"markdown","source":"**Work with Ouliers and Missing Values**","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"5fa251b7a88c2b1b1fda498dca8756df90e6feaf"},"cell_type":"code","source":"# Find potential outliers in values array\n# and visualize them on a plot\n\ndef is_outlier(value, p25, p75):\n    \"\"\"Check if value is an outlier\n    \"\"\"\n    lower = p25 - 1.5 * (p75 - p25)\n    upper = p75 + 1.5 * (p75 - p25)\n    return value <= lower or value >= upper\n \n \ndef get_indices_of_outliers(values):\n    \"\"\"Get outlier indices (if any)\n    \"\"\"\n    p25 = np.percentile(values, 25)\n    p75 = np.percentile(values, 75)\n     \n    indices_of_outliers = []\n    for ind, value in enumerate(values):\n        if is_outlier(value, p25, p75):\n            indices_of_outliers.append(ind)\n    return indices_of_outliers\n \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e789c3facc35d5948921599f0eeec3b016014f68"},"cell_type":"code","source":"indices_of_outliers = get_indices_of_outliers(train['Age'])\n \nfig = plt.figure()\nax = fig.add_subplot(111)\nax.plot(train['Age'], 'b-', label='distances')\nax.plot(\n    indices_of_outliers,\n    train['Age'][indices_of_outliers],\n    'ro',\n    markersize = 7,\n    label='outliers')\nax.legend(loc='best')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b800579a838087acff2db314e053e9152b751ece"},"cell_type":"markdown","source":"No potential outliers in Age column.\n\nFrom df.info() method we can find that Age has only 1046 non-null values remaning are null values. Cabin has loads of missing data. (only 295 is non-null out of 1309 ) Embarked has two missing data.","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"625064cbd641431124ac1da75dda239cacea2c9e"},"cell_type":"code","source":"#Use Label encoding for categorical variables\nfrom sklearn import preprocessing\nle = preprocessing.LabelEncoder()\nle.fit(titanic['Sex'])\ntitanic['Sex'] = le.transform(titanic['Sex'])\n\n#To include the Embarked feature in knn fit to predict age..there are very few say 2 missing values in Embarked.. using the most\n#occuring value of embarked column for this missing values\n\ntitanic['Embarked'] = titanic['Embarked'].fillna(\"S\")\nle.fit(titanic['Embarked'])\ntitanic['Embarked'] = le.transform(titanic['Embarked'])\n#Fare contains one null value..replacec it with mean of Fare value\ntitanic[titanic['Fare'].isnull() == True]\ntitanic['Fare'] = titanic['Fare'].fillna(titanic['Fare'].mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"329480f021c3807a61d6b40432627d7859d77fd2"},"cell_type":"code","source":"#Impute Age\ndf = titanic[['PassengerId','Pclass', 'Sex', 'SibSp','Parch','Fare','Embarked','Age']]\ntrain_df = df[df['Age'].isnull() == False]\ntest_df = df[df['Age'].isnull() == True]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b0a429ef9399621a395723605094206240759000"},"cell_type":"code","source":" \nX_train_df = train_df[['PassengerId','Pclass', 'Sex', 'SibSp','Parch','Fare','Embarked']]\ny_train_df = train_df['Age']\nX_test_df = test_df[['PassengerId','Pclass', 'Sex', 'SibSp','Parch','Fare','Embarked']]\ny_test_df = test_df['Age']\n\nfrom sklearn.neighbors import KNeighborsRegressor\n\nknn = KNeighborsRegressor(n_neighbors = 5)\n\nknn.fit(X_train_df , y_train_df)\n\npred_values = knn.predict(X_test_df)\n\ntest_df['Age'] = pred_values","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"75da03eab8f147f05a89c2ab4c8d8069716645b6"},"cell_type":"code","source":"df_complete = pd.concat([train_df, test_df])\ndf_complete.sort_values(['PassengerId'],inplace=True)\ntitanic['Age'] = df_complete['Age']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ebd5d0d0370dec87293b2c47f54c8fbbfe6a6506"},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nplt.figure()\nsns.distplot(df_complete['Age'].dropna())\nplt.show()\n#After filling up the missing values still Age distribution follows a right skewed distribution. \n# We will have to scale it","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"515199fedcfdb65daa2148afcca565b29b78ca8d"},"cell_type":"markdown","source":"**Feature Generation**","outputs":[],"execution_count":null},{"metadata":{"trusted":true,"_uuid":"018dc222b4f9da76c1fac3cd733736cde5f60eef"},"cell_type":"code","source":"#MinMax Scalar for numeric columns...\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ndf_scaled = scaler.fit_transform(titanic['Fare'].reshape(-1, 1))\ntitanic['Fare_Scaled'] = df_scaled\ntitanic['AgeCategory'] = titanic['Age'].apply(lambda x: 'Infant' if x < 1.0 else('Child' if x < 10.0 else('Adult' if x < 75 else 'OldAged')))\nle.fit(titanic['AgeCategory'])\ntitanic['AgeCategory'] = le.transform(titanic['AgeCategory'])\ntitanic['FamilyCount'] = titanic['SibSp'] + titanic['Parch'] + 1\n##So categories familycount into singles midsizedfamily largefamily >  & labelencode 0,1,2\ntitanic['FamilyCategory'] = titanic['FamilyCount'].apply(lambda x: 0 if x == 1 else( 2 if x > 4  else 1))\n\n#work on cabin level ...if Nan put as unknown 'U' else the first letter of the cabin...all cabins in same level has starts with same letter\ntitanic['Cabin'] = titanic['Cabin'].fillna(value=\"U\")\n\ntitanic['CabinLevel'] =   titanic['Cabin'].apply( lambda x : x[:1])\n\ntitanic['CabinLevel'] =   titanic['CabinLevel'].apply( lambda x : 1 if x == 'A' else( 3 if (x == \"F\" or x == \"G\")  else (0 if x == \"U\" else 2) ))\n#Title Can be separated from Name\ntitanic['Title'] = titanic['Name'].apply(lambda x :(x.split(\", \")[1]).split(\".\")[0] )\n#100 % survival rate for Lady, Mlle, Mme, Sir and The Countess...so might be high ranked people \n# Can replace Lady, Mlle , Mme , the Countess - Lady\n# Capt, Col, Jonkheer, Rev  - Ranked Personal\n# Miss and Ms to - Miss\ntitanic['Title'] = titanic['Title'].apply(lambda x: 'Lady' if ( x == \"Mlle\" or x == \"Mme\" or x == \"the Countess\" ) else ('Ranked' if (x == \"Capt\" or x == \"Col\" or x == \"Rev\") else ( \"Miss\" if (x == \"Ms\") else x)) )\nle.fit(titanic['Title'])\ntitanic['Title'] = le.transform(titanic['Title'])\n\n\n#Pclass and Sex can be combined as depending on pclass and sex the survival rate looks dependent\ntitanic['Pclass_Sex'] = titanic['Pclass'] * titanic['Sex']\n\ntitanic['Embarked_Sex'] = titanic['Embarked'] * titanic['Sex']\n","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"14ef6cfa14c8f19f8c0e9e82ad77b5f95e587509"},"cell_type":"code","source":"#Using target to generate features..how many people in each embarked survived..mean of that value \ntrain.sort_values(['Embarked'])\n#total survived for each embarked , total passengers in each embarked\ntotal_passengers_Emb_C = train[train['Embarked'] == 'C'].count()['PassengerId']\ntotal_passengers_survived_Emb_C = train[ (train['Embarked'] == 'C') & ( train['Survived'] == 1)].count()['PassengerId']\n#df[df['Survived'] == 1 ].count()['PassengerId']\nEmb_C_mean = (total_passengers_survived_Emb_C * 1.0) / total_passengers_Emb_C\nEmb_C_mean\n\ntotal_passengers_Emb_S = train[train['Embarked'] == 'S'].count()['PassengerId']\ntotal_passengers_survived_Emb_S = train[ (train['Embarked'] == 'S') & ( train['Survived'] == 1)].count()['PassengerId']\n#df[df['Survived'] == 1 ].count()['PassengerId']\nEmb_S_mean = (total_passengers_survived_Emb_S * 1.0) / total_passengers_Emb_S\nEmb_S_mean\n\ntotal_passengers_Emb_Q = train[train['Embarked'] == 'Q'].count()['PassengerId']\ntotal_passengers_survived_Emb_Q = train[ (train['Embarked'] == 'Q') & ( train['Survived'] == 1)].count()['PassengerId']\n#df[df['Survived'] == 1 ].count()['PassengerId']\nEmb_Q_mean = (total_passengers_survived_Emb_Q * 1.0) / total_passengers_Emb_Q\n\ntitanic['Emb_Surivived_mean'] = titanic['Embarked'].apply(lambda x: 0.55 if x == \"C\" else( 0.39 if x == \"Q\" else 0.34))\ntitanic['Emb_label_encoding'] = titanic['Embarked'].apply(lambda x: 1 if x == \"C\" else( 3 if x == \"Q\" else 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"37cee5b5c572e85b07fa5a844e2f0b5ea9180669"},"cell_type":"code","source":"g = sns.PairGrid(titanic,\n                 x_vars=[\"Embarked\", \"AgeCategory\", \"CabinLevel\"],\n                 y_vars=[\"Pclass\", \"Sex\"],\n                 aspect=.75, size=3.5)\ng.map(sns.violinplot, palette=\"pastel\");","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2de1306519235e491bd52423ae12458a1950779c"},"cell_type":"markdown","source":"**Modelling**","outputs":[],"execution_count":null},{"metadata":{"_uuid":"2bb316080b50307292695de950c15a1979fa9102"},"cell_type":"markdown","source":"As a learning process, I will try all possible models.","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"506555bb15b02137093611684bb62fea49a0d1ac"},"cell_type":"code","source":"titanic.sort_values('PassengerId',inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"b3329748b31b972c30f188eb2d7c50be50e03ac9"},"cell_type":"code","source":"#Rearrange columns and drop the unnecessary ones\ntitanic = titanic[['PassengerId','Embarked','Pclass','Sex','Fare_Scaled','AgeCategory','FamilyCount','FamilyCategory','CabinLevel','Title','Pclass_Sex','Embarked_Sex','Emb_label_encoding','Survived']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"db4eba336ede5e06334f71f790ef6ea96c8fcf53"},"cell_type":"code","source":"X_train = titanic.iloc[:,0:13]\ny_train = titanic.iloc[:,13:14]\nX_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"be6c25204a53c7ad45cc0e2b3f6693a6baf274bf"},"cell_type":"markdown","source":"First the very basic and famous Random Forest Classifier and find the important features.","outputs":[],"execution_count":null},{"metadata":{"trusted":true,"_uuid":"cc658afeec5095376fc2973f7a8e6e2a06c1ba48"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier().fit(X_train, np.ravel(y_train))\nfeature_df = (X_train.columns)\nfeature_df = feature_df.tolist()\nfeature_df\n\nfeature_imp = clf.feature_importances_.tolist()\nfeature_imp\n\n\nfeat = pd.DataFrame({'feat':feature_df})\nfeat['ImpVal'] = feature_imp\nfeat.sort_values('ImpVal',inplace=True,ascending=False)\nfeat","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"82bebdf919d4b708db6e94fca23d716bede5f6f0"},"cell_type":"code","source":"#Drop the column with least value for importance and proceed iwth rest as there are very few columns in this data set.\ntitanic = titanic[['PassengerId','Embarked','Pclass','Sex','Fare_Scaled','AgeCategory','FamilyCount','FamilyCategory','CabinLevel','Title','Pclass_Sex','Survived']]\n#Now we can split train and test and apply other ML algo\ntitanic.Survived = titanic.Survived.astype(int)\ntrainSet = titanic[titanic['PassengerId'].isin(range(1,892))]\ntestSet =  titanic[titanic['PassengerId'].isin(range(892,1310))]\n#titanic.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"587d16965e409be92b8c60d1b6b557293b85ab4a"},"cell_type":"code","source":"#Random Forest Predictor\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(trainSet.iloc[:,1:11], trainSet.iloc[:,11:12], random_state = 3)\nclf = RandomForestClassifier().fit(X_train, np.ravel(y_train))\n\nprint('Accuracy of RandomForest classifier on training set: {:.2f}'\n     .format(clf.score(X_train, np.ravel(y_train))))\nprint('Accuracy of RandomForest classifier on validation set: {:.2f}'\n     .format(clf.score(X_val, np.ravel(y_val))))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9b7d76d6a57bec837cf40ac93284a910e2358085"},"cell_type":"markdown","source":"This gave the result of 0.741 on the public leaderboard in kaggle.","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"d4f516bd37e8fa39220c48a6952cb2418df7780a"},"cell_type":"code","source":"#To get the submission file for kaggle \npred_values = clf.predict(testSet.iloc[:,1:11])\nresults = test['PassengerId']\nresults = results.to_frame()\nresults['Survived'] = pred_values\nresults.to_csv(\"results.csv\", sep=',',index=False) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eff474151f0938bea69190bd9150c4a5578ce4a7"},"cell_type":"markdown","source":"**Naive Bayes Predictor**","outputs":[],"execution_count":null},{"metadata":{"trusted":true,"_uuid":"f41b14265a87b01b988a7416a656c0038be2b87f"},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\n\nmodel = GaussianNB()\nmodel.fit(X_train,np.ravel(y_train))\nprint(model.score(X_train,np.ravel(y_train)))\nprint(model.score(X_val,np.ravel(y_val)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bd76b99e3d15bcce7f1e915c27021e185473f0fa"},"cell_type":"markdown","source":"**MLP Classifier**","outputs":[],"execution_count":null},{"metadata":{"trusted":true,"_uuid":"a16e86212c1822ab3c9fdc7b4df4ff799c30b4e4"},"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\n\nmodel = MLPClassifier()\nmodel.fit(X_train,np.ravel(y_train))\nprint(model.score(X_train,np.ravel(y_train)))\nprint(model.score(X_val,np.ravel(y_val)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"404f4fc1546f42a92cfdc6a2f338129940a10af1"},"cell_type":"markdown","source":"**Gradient boosting Ensemble**","outputs":[],"execution_count":null},{"metadata":{"trusted":true,"_uuid":"076b89d87fd88b1e7ea74053c918f038e9555b10"},"cell_type":"code","source":"#Gradient Boost \nfrom sklearn.ensemble import GradientBoostingClassifier\nclf_gb = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n     max_depth=1, random_state=0).fit(X_train, np.ravel(y_train))\nprint('Accuracy of Gradient Boost Decision Tree classifier on training set: {:.2f}'\n     .format(clf_gb.score(X_train, np.ravel(y_train))))\nprint('Accuracy of Gradient Boost Decision Tree classifier on validation set: {:.2f}'\n     .format(clf_gb.score(X_val, np.ravel(y_val))))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7ba5c8d9062fa009d21b52215139c62902f1b0b7"},"cell_type":"markdown","source":"**Gradient Boost with GridSearchCV to tune params**","outputs":[],"execution_count":null},{"metadata":{"trusted":true,"_uuid":"5fcf4eff62175a36f2556f9bc0ab0230b9c64868"},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\ngrid_values = {'n_estimators': [50, 100, 200, 300,500] , 'learning_rate' : [0.1,0.5,1.0] , 'max_depth' : [1,2,3,4,5] }\nclf_gb_grid = GradientBoostingClassifier()\n\ngrid_clf_gb_acc = GridSearchCV(clf_gb_grid, param_grid = grid_values)\ngrid_clf_gb_acc.fit(X_train,np.ravel( y_train))\ny_decision_fn_scores_acc = grid_clf_gb_acc.decision_function(X_val) \n\nprint('Grid best parameter (max. accuracy): ', grid_clf_gb_acc.best_params_)\nprint('Grid best score (accuracy): ', grid_clf_gb_acc.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6e2be334f2eeb4963affac120cf362641ee1f00e"},"cell_type":"markdown","source":"**The most famous XGBOOST Classifier**","outputs":[],"execution_count":null},{"metadata":{"trusted":true,"_uuid":"3033643daab9a81d047e564a9e7cd9b21e81dd58"},"cell_type":"code","source":"import xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier\n\nxgb1 = XGBClassifier(\n learning_rate =0.1,\n n_estimators=1000,\n max_depth=5,\n min_child_weight=1,\n gamma=0,\n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread=4,\n scale_pos_weight=1,\n seed=27).fit(X_train, np.ravel(y_train))\n\nprint('Accuracy of XGBOOST classifier on training set: {:.2f}'\n     .format(xgb1.score(X_train, np.ravel(y_train))))\nprint('Accuracy of XGBOOST classifier on validation set: {:.2f}'\n     .format(xgb1.score(X_val, np.ravel(y_val))))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f17c7d2ce3e77ae3963f8fa36096d8f2520d6555"},"cell_type":"markdown","source":"**GridSearchCV with XGBoost**","outputs":[],"execution_count":null},{"metadata":{"trusted":true,"_uuid":"08d71a007061583e2e5d979bcac82958fde6d0f2"},"cell_type":"code","source":"grid_values = {'n_estimators': [300] , 'learning_rate' : [0.05] , 'max_depth' : [5] , 'min_child_weight' : [1],\n              'colsample_bytree': [0.8] , 'subsample' : [0.6], 'gamma': [0]}\nclf_xgb_grid = XGBClassifier(seed=2,objective= 'binary:logistic',nthread=-1,scale_pos_weight=1)\n\nclf_xgb_grid_acc = GridSearchCV(clf_xgb_grid, param_grid = grid_values)\nclf_xgb_grid_acc.fit(X_train, np.ravel(y_train))\n#y_decision_fn_scores_acc = clf_xgb_grid_acc.decision_function(X_val) \n\nprint('Grid best parameter (max. accuracy): ', clf_xgb_grid_acc.best_params_)\nprint('Grid best score (accuracy): ', clf_xgb_grid_acc.best_score_)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df7b21abcb7b851c5ea5a032158a1f636cb77c6f"},"cell_type":"markdown","source":"**Using VecStack Package**","outputs":[],"execution_count":null},{"metadata":{"trusted":true,"_uuid":"7512336896d22a0a6d8e5d317885ed9968cd0086"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom vecstack import stacking\n\n# Make train/test split\n# As usual in machine learning task we have X_train, y_train, and X_test\n#X_train, X_test, y_train, y_test = train_test_split(trainSet.iloc[:,1:11], trainSet.iloc[:,11:12], \n#    test_size = 0.2, random_state = 0)\n\nX_train =  trainSet.iloc[:,1:11]\ny_train =  trainSet.iloc[:,11:12]\nX_test =   testSet.iloc[:,1:11]\ny_test =   testSet.iloc[:,11:12]\n\n# Caution! All models and parameter values are just \n# demonstrational and shouldn't be considered as recommended.\n# Initialize 1st level models.\nmodels = [\n    ExtraTreesClassifier(random_state = 0, n_jobs = -1, \n        n_estimators = 100, max_depth = 3),\n        \n    RandomForestClassifier(random_state = 0, n_jobs = -1, \n        n_estimators = 100, max_depth = 3),\n        \n    XGBClassifier(seed = 0,  learning_rate = 0.1, \n        n_estimators = 100, max_depth = 3)]\n    \n# Compute stacking features\nS_train, S_test = stacking(models, X_train, y_train, X_test, \n    regression = False, metric = accuracy_score, n_folds = 4, \n    stratified = True, shuffle = True, random_state = 0, verbose = 2)\n\n# Initialize 2nd level model\nmodel = XGBClassifier(seed = 0,  learning_rate = 0.1, \n    n_estimators = 100, max_depth = 3)\n    \n# Fit 2nd level model\nmodel = model.fit(S_train, np.ravel(y_train))\n\n# Predict\ny_pred = model.predict(S_test)\n\n# Final prediction score\nprint('Final prediction score: [%.8f]' % accuracy_score(np.ravel(y_test), np.ravel(y_pred)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ddd7979927b17b23d1824f46ca970ecdd778962c"},"cell_type":"markdown","source":"**Ensemble ( Stacking)**","outputs":[],"execution_count":null},{"metadata":{"_uuid":"8db1025a2dabe8213ff64bf226db90d30a2b6c60"},"cell_type":"markdown","source":"Generating our Base First-Level Models So now let us prepare five learning models as our first level classification. These models can all be conveniently invoked via the Sklearn library and are listed as follows:\n\nRandom Forest classifier Extra Trees classifier AdaBoost classifer Gradient Boosting classifer Support Vector Machine Parameters\n\nJust a quick summary of the parameters that we will be listing here for completeness,\n\nn_jobs : Number of cores used for the training process. If set to -1, all cores are used.\n\nn_estimators : Number of classification trees in your learning model ( set to 10 per default)\n\nmax_depth : Maximum depth of tree, or how much a node should be expanded. Beware if set to too high a number would run the risk of overfitting as one would be growing the tree too deep\n\nverbose : Controls whether you want to output any text during the learning process. A value of 0 suppresses all text while a value of 3 outputs the tree learning process at every iteration.","outputs":[],"execution_count":null},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"8f427395635f3d2957ccfcf6da51f218d343eae5"},"cell_type":"code","source":"from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n                              GradientBoostingClassifier, ExtraTreesClassifier)\n\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier\n\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import KFold\n\nfrom sklearn.naive_bayes import BernoulliNB, MultinomialNB\n\nSEED = 0 # for reproducibility\nNFOLDS = 5 # set folds for out-of-fold prediction\nkf = KFold(n_splits = NFOLDS, random_state=SEED,shuffle=False)\nclass SklearnHelper(object):\n    def __init__(self, clf, seed=0, params=None):\n        #params['random_state'] = seed\n        self.clf = clf(**params)\n\n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        return self.clf.predict(x)\n    \n    def fit(self,x,y):\n        return self.clf.fit(x,y)\n    \n    def score(self,x,y):\n        return self.clf.score(x,y)\n    \n    def feature_importances(self,x,y):\n        print(self.clf.fit(x,y).feature_importances_)\n\n        \n# Put in our parameters for said classifiers\n# Random Forest parameters\nrf_params = {\n    'n_jobs': -1,\n    'random_state' : 0,\n    'n_estimators': 500,\n     'warm_start': True, \n     #'max_features': 0.2,\n    'max_depth': 6,\n    'min_samples_leaf': 2,\n    'max_features' : 'sqrt',\n    'verbose': 0\n}\n\n# Extra Trees Parameters\net_params = {\n    'n_jobs': -1,\n    'random_state' : 0,\n    'n_estimators':500,\n    #'max_features': 0.5,\n    'max_depth': 8,\n    'min_samples_leaf': 2,\n    'verbose': 0\n}\n\n# AdaBoost parameters\nada_params = {\n    'n_estimators': 500,\n    'random_state' : 0,\n    'learning_rate' : 0.75\n}\n\n# Gradient Boosting parameters\ngb_params = {\n    'n_estimators': 500,\n    'random_state' : 0,\n     #'max_features': 0.2,\n    'max_depth': 5,\n    'min_samples_leaf': 2,\n    'verbose': 0\n}\n\n# Support Vector Classifier parameters \nsvc_params = {\n    'kernel' : 'linear',\n    'random_state' : 0,\n    'C' : 0.025\n    }\n\nxgboost_params = {\n    'learning_rate'  : 0.1,\n    'seed' : 0,\n    'n_estimators' : 1000,\n    'max_depth' : 5,\n    'min_child_weight' : 1,\n    'gamma' : 0,\n    'subsample' : 0.8,\n    'colsample_bytree' : 0.8,\n    'objective' : 'binary:logistic',\n}\n\nbinomialnb_params = {\n    'alpha' : .01\n}\n\nmultinomialnb_params = {\n    'alpha' : .01\n}\n\n# Create 5 objects that represent our 4 models\nrf = SklearnHelper(clf=RandomForestClassifier, seed=SEED, params=rf_params)\net = SklearnHelper(clf=ExtraTreesClassifier, seed=SEED, params=et_params)\nada = SklearnHelper(clf=AdaBoostClassifier, seed=SEED, params=ada_params)\ngb = SklearnHelper(clf=GradientBoostingClassifier, seed=SEED, params=gb_params)\nsvc = SklearnHelper(clf=SVC, seed=SEED, params=svc_params)\nxgb = SklearnHelper(clf=XGBClassifier, seed=SEED, params=xgboost_params)\nbernb = SklearnHelper(clf=BernoulliNB, seed=SEED, params=binomialnb_params)\nmultinb = SklearnHelper(clf=MultinomialNB, seed=SEED, params=multinomialnb_params)\n\ndef get_oof(clf, X_train, y_train, X_val,y_val,trainSet,testSet):\n    clf.train(X_train,y_train)\n    print('Accuracy of The classifier on training set: {:.2f}'\n     .format(clf.score(X_train, y_train)))\n    print('Accuracy of The classifier on validation set: {:.2f}'\n     .format(clf.score(X_val, y_val)))\n    return clf.predict(trainSet),clf.predict(testSet)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bcd2b4bbf611f3e8713a01649d3c0133e0ae42c5"},"cell_type":"code","source":"train_pred_val_rf, test_pred_val_rf = get_oof(rf,X_train, np.ravel(y_train), X_val,y_val,trainSet.iloc[:,1:11],testSet.iloc[:,1:11]) # Random Forest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cfdd2526b107491b1077c862fbbd39de839f1f9e"},"cell_type":"code","source":"train_pred_val_et,test_pred_val_et =  get_oof(et, X_train, np.ravel(y_train), X_val,y_val,trainSet.iloc[:,1:11],testSet.iloc[:,1:11]) #Extra Trees","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5188711004adb76c1f90fd6aad0938da426bd4a7"},"cell_type":"code","source":"train_pred_val_ada,test_pred_val_ada = get_oof(ada, X_train, np.ravel(y_train), X_val,y_val,trainSet.iloc[:,1:11],testSet.iloc[:,1:11]) # AdaBoost ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1cd13463f41ceffa7b2ed4262b8fca68e4d9e458"},"cell_type":"code","source":"train_pred_val_gb, test_pred_val_gb = get_oof(gb,X_train, np.ravel(y_train), X_val,y_val,trainSet.iloc[:,1:11],testSet.iloc[:,1:11]) # Gradient Boost","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5c1914e4c049c46a7ccee87c521d0e10db3dac3b"},"cell_type":"code","source":"train_pred_val_svc, test_pred_val_svc = get_oof(svc,X_train, np.ravel(y_train), X_val,y_val,trainSet.iloc[:,1:11],testSet.iloc[:,1:11]) # Support Vector Classifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dab115d09ff9dd619082912139e3d79e637e055a"},"cell_type":"code","source":"train_pred_val_xgb,test_pred_val_xgb = get_oof(xgb,X_train, np.ravel(y_train), X_val,y_val,trainSet.iloc[:,1:11],testSet.iloc[:,1:11]) #XGBoost","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"319f94d65e9fd962817769d59981b6c867f9ffa4"},"cell_type":"code","source":"train_pred_val_bernb,test_pred_val_bernb = get_oof(bernb,X_train, np.ravel(y_train), X_val,y_val,trainSet.iloc[:,1:11],testSet.iloc[:,1:11]) #Bernouli Naive Bayes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c1c62570cb94c8768fe13bebbe8a8a995a146731"},"cell_type":"code","source":"train_pred_val_multinb,test_pred_val_multinb = get_oof(multinb,X_train, np.ravel(y_train), X_val,y_val,trainSet.iloc[:,1:11],testSet.iloc[:,1:11]) #Multinomial Naive bayes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ff2f913afa8c4ea7892a8ff53b393d03caf3a7a2"},"cell_type":"code","source":"#Removed highly correlated models and selecting only three for futhre processing\nbase_predictions_train = pd.DataFrame( {\n     'RandomForest': train_pred_val_rf.ravel(),\n    # 'ExtraTrees' : train_pred_val_et.ravel(),\n    # 'AdaBoost': train_pred_val_ada.ravel(),\n     'GradientBoost': train_pred_val_gb.ravel(),\n     # 'SupportVector' : train_pred_val_svc.ravel(),\n     # 'XGBoost' :   train_pred_val_xgb.ravel() , \n     # 'BernouliNB' : train_pred_val_bernb.ravel(),\n      'MultinomialNB' : train_pred_val_multinb.ravel()\n    })\nbase_predictions_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9c7fc0012696a22bc55e8433ffbcf1dd1e043b57"},"cell_type":"code","source":"base_predictions_test = pd.DataFrame( {\n     'RandomForest': test_pred_val_rf.ravel(),\n     #'ExtraTrees' : test_pred_val_et.ravel(),\n     #'AdaBoost': test_pred_val_ada.ravel(),\n      'GradientBoost': test_pred_val_gb.ravel(),\n     # 'SupportVector' : test_pred_val_svc.ravel(),\n     # 'XGBoost' :   test_pred_val_xgb.ravel(),\n     # 'BernouliNB' : test_pred_val_bernb.ravel(),\n      'MultinomialNB' : test_pred_val_multinb.ravel()\n    })\nbase_predictions_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fd22b72b38cc9bd5786c6d5d4ab25ef05de84d61"},"cell_type":"code","source":"base_predictions_test.corr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8ec9bf4d5d7bae97058c22bda6f5448fcb9333f0"},"cell_type":"code","source":"import seaborn as sns\nplt.figure(figsize = (8,8))\nsns.heatmap(base_predictions_test.corr(), annot=True, fmt=\".2f\")","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"25f8848f916d43ae54f05235837f0c4ac58b79e3"},"cell_type":"code","source":"y_train = np.ravel(trainSet['Survived'])","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"17f26d2157cd034b574f852771c55c960207c937"},"cell_type":"code","source":"#Second level model XGBoost\nxgb1 = XGBClassifier(\n learning_rate =0.1,\n n_estimators=1000,\n max_depth=5,\n min_child_weight=1,\n gamma=0,\n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread=4,\n scale_pos_weight=1,\n seed=27).fit(base_predictions_train,y_train)\npredictions = xgb1.predict(base_predictions_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af5bef6930e6140f06e855faf020b44fc63d9fab"},"cell_type":"code","source":"xgb1.score(base_predictions_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"0968f7014691c2be379d347dc7c298de56acb5ad"},"cell_type":"code","source":"results['Survived'] = predictions\nresults.to_csv(\"results_ensemble.csv\", sep=',',index=False) #).77 kaggle only, might be tuning params is required.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d23cfa2156a8f552535cb10060175c5d1d6c94a7"},"cell_type":"markdown","source":"**Voting Classifier**\nVoting is one of the simplest ways of combining the predictions from multiple machine learning algorithms.\n\nIt works by first creating two or more standalone models from your training dataset. A Voting Classifier can then be used to wrap your models and average the predictions of the sub-models when asked to make predictions for new data.\n\nThe predictions of the sub-models can be weighted, but specifying the weights for classifiers manually or even heuristically is difficult. More advanced methods can learn how to best weight the predictions from submodels, but this is called stacking (stacked aggregation) and is currently not provided in scikit-learn.\n\nYou can create a voting ensemble model for classification using the VotingClassifier class.","outputs":[],"execution_count":null},{"metadata":{"trusted":true,"_uuid":"1256eba30a1b92346bb215b0c8ab01c5a5c64b0d"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(trainSet.iloc[:,1:11], trainSet.iloc[:,11:12], random_state = 3)\n\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import VotingClassifier\n\nseed = 7\nkfold = model_selection.KFold(n_splits=10, random_state=seed)\n# create the sub models\nestimators = []\nmodel1 = LogisticRegression()\nestimators.append(('logistic', model1))\nmodel2 = DecisionTreeClassifier()\nestimators.append(('cart', model2))\nmodel3 = SVC()\nestimators.append(('svm', model3))\n# create the ensemble model\nensemble = VotingClassifier(estimators)\nresults = model_selection.cross_val_score(ensemble, X_train, np.ravel(y_train), cv=kfold)\nprint(results.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5baa9e0ac44968e9696306a966354a9da18a9cb5"},"cell_type":"code","source":"ensemble.fit(X_train, y_train)\npred_values = ensemble.predict(testSet.iloc[:,1:11])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a94eea0dc0dd535e1409428bc27a85559a218825"},"cell_type":"markdown","source":"This is my first kernel for kaggle. Kindly leave your valuable feedbacks. Thanks","outputs":[],"execution_count":null}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}