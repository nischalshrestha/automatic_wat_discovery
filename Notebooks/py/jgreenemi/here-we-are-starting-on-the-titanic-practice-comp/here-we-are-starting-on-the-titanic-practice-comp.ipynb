{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","collapsed":true,"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nfrom collections import Set","execution_count":158,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true},"cell_type":"markdown","source":"## Here We Are: Starting on the Titanic Practice Comp\n\nThis will serve as a sandbox notebook for my getting familiar with Kaggle, competitions, and working with a wide variety of datasets.\n\nPlease ignore the following - it is, indeed, scratchspace.\n\n$\\sum^{k/t}$ and $\\underset{k\\times l}{A}$"},{"metadata":{"_cell_guid":"ccafbd88-874b-400e-a94f-93a99103a610","_uuid":"e7ca1fbb1698d93f9ee4dbc7f0f8752035c1d9e4","trusted":true},"cell_type":"code","source":"# This is scratchspace where Python code will go.\n\npd.DataFrame(columns=['test', 'test2']).describe()\n\n# Scratchspace ends here.\n# -----------------------------------------------","execution_count":159,"outputs":[]},{"metadata":{"_cell_guid":"9ff58d32-f231-4a81-8218-08abc09af110","_uuid":"6d26ea11a97d585d8d4979f9cb21dce7bb0924cb","trusted":true},"cell_type":"code","source":"# Input data is in \"../input\". Here we have the Titanic data files.\nprint(os.listdir(\"../input\"))\nPATH = '../input'\nfile_train = f'{PATH}/train.csv'\nfile_test = f'{PATH}/test.csv'\nfile_gender_submission = f'{PATH}/gender_submission.csv'","execution_count":160,"outputs":[]},{"metadata":{"_cell_guid":"18024d9c-689f-42bc-82e3-123c30ed8552","_uuid":"c44d43eb2af70bb166988e9e723c0a835ea5602c","trusted":true},"cell_type":"code","source":"data_train_all = pd.read_csv(file_train, sep=',')\ndata_test = pd.read_csv(file_test, sep=',')\ndata_test.head()","execution_count":161,"outputs":[]},{"metadata":{"_cell_guid":"25860897-41fc-4a9c-834e-8b6a3c8aab3f","_uuid":"ba4466be27908ae9ad4f657c071c483bcf241a22","trusted":true},"cell_type":"code","source":"# Verify the contents of the files are what we expect.\nprint(f'{data_train_all.shape} for training\\n{data_test.shape} for testing')","execution_count":162,"outputs":[]},{"metadata":{"_cell_guid":"a4e60981-a9ce-4d12-9cee-60fe90a2e75a","_uuid":"4e4c778ae57f42ba6127e21038cb5d8ac49b8977","collapsed":true},"cell_type":"markdown","source":"Now that we have the data loaded in, let's...\n\n## Process The Data\n\nWe'll start by separating out features as categorical or continuous, omitting those that are not likely to be useful right off the bat."},{"metadata":{"_cell_guid":"f257b3b7-f7ba-4ff3-87ad-227aeee65bd3","_uuid":"483ca4aac63a24dd62646b880a4616195ceedb67","collapsed":true,"trusted":true},"cell_type":"code","source":"features_categorical = [\n    'Survived',\n    'PassengerId',\n    'Pclass',  # Socioeconomic status, generally.\n    'Sex'\n]\n\nfeatures_continuous = [\n    'Age',\n    'SibSp', # Sibling or Spouse\n    'Parch' # Parents or Children\n]","execution_count":163,"outputs":[]},{"metadata":{"_cell_guid":"4be9691a-c1d3-4641-a26b-3a9a22ef6b05","_uuid":"b1564e1921b3583d9b16eca9733be22ada02f4d5","trusted":true},"cell_type":"code","source":"# When we go to create the data_test object below we want to get all features except 'Survived' since that won't be present outside the training data.\nprint(features_categorical[1:])\n","execution_count":164,"outputs":[]},{"metadata":{"_cell_guid":"2da96f34-3765-4db8-8e6e-342568d8c212","_uuid":"a7dec4da9bdcb2150b2ab0a64e9ca419c24312fe","trusted":true},"cell_type":"code","source":"# Drop the columns we don't plan to use.\ndata_train_all = data_train_all[features_continuous + features_categorical]\ndata_test = data_test[features_continuous + features_categorical[1:]]\ndata_test.head()","execution_count":165,"outputs":[]},{"metadata":{"_cell_guid":"13ed07e7-90cf-4230-9a72-2d4e9ab0a044","_uuid":"897fe198394b39d9348f09974efc7754c02a0cd5","trusted":true},"cell_type":"code","source":"# Let's have a look through the describe() results for the numerical columns.\ndata_test.describe()","execution_count":166,"outputs":[]},{"metadata":{"_cell_guid":"eaaf66ec-97f6-43fc-945b-aa54fda6bed7","_uuid":"398cdbe3698942e60c84318a2611c2e58de17c5c","collapsed":true,"trusted":true},"cell_type":"code","source":"#data_train_all.get(features_categorical).head()\n#data_train_all.get(features_continuous).head()","execution_count":167,"outputs":[]},{"metadata":{"_cell_guid":"15690a0a-540e-4648-a8f6-453094b5e883","_uuid":"0299a5d28b67d415cfb51217b206c3deab93ba90"},"cell_type":"markdown","source":"We have columns with missing data. Let's find where they are, and determine how best to handle them. For numerical values, we may wish to replace them with an average over the rest of the column. For categorical values, we will likely create a separate class for them, unless we want to try to guess (based on similar passengers' data) what would make most sense for them to have in that particular field."},{"metadata":{"_cell_guid":"d8acb3e5-b753-4009-ad87-8463e83144aa","_uuid":"84d5c48d73da8fd85c9c0bde10d5fe9442638df5","trusted":true},"cell_type":"code","source":"for featurename in (features_categorical + features_continuous):\n    if featurename not in ['PassengerId']:\n        print(f'{featurename} Unique Values:')\n        print(data_train_all[featurename].unique())\n    #if featurename in ['Age']:\n    #    for age in data_train[featurename]:\n    #        if 'nan' in str(age).lower():\n    #            print(age)\n","execution_count":168,"outputs":[]},{"metadata":{"_cell_guid":"48d35a95-1507-40d7-8b51-be73e76421ff","_uuid":"2f10a52b7ec75133518a039666f4e74406b145a8"},"cell_type":"markdown","source":"From this output we can tell that Age contains `nan` values. All other columns (except for `PassengerId` which we intentionally skipped since it's huge to read through) appear to have all sane values.\n\nAlthough my gut tells me this is probably not a solid idea, I'll replace all `nan` values in Age with the Age column's average, just to have a complete column."},{"metadata":{"_cell_guid":"204856ab-eb10-4ab9-b960-e36d225958b6","_uuid":"bca83cf3e6caf558c17c22bf633a31629c8839e5","trusted":true},"cell_type":"code","source":"mean_age = data_train_all['Age'].mean()  # 29.69911764705882\ndata_train_all['Age'] = data_train_all['Age'].fillna(mean_age)\n\nif 'nan' in str(data_train_all['Age']).lower():\n    print('Missed at least one!')    \n    \nmean_age_test = data_test['Age'].mean()  # 29.69911764705882.  Wasn't expecting that.\ndata_test['Age'] = data_test['Age'].fillna(mean_age_test)\n\nif 'nan' in str(data_test['Age']).lower():\n    print('Missed at least one!')    \nprint(f'{mean_age} vs {mean_age_test}')","execution_count":169,"outputs":[]},{"metadata":{"_cell_guid":"431791ae-eacc-4e74-8e4a-c4dc2b731d9e","_uuid":"26284b8c1b88d934810fb167e5f8c41641ef4012"},"cell_type":"markdown","source":"Rerunning the check for unique values, we can see that there are no NaN values in Age anymore."},{"metadata":{"_cell_guid":"5f0d121c-cb74-4f0f-bd1e-0eef6624f06f","_uuid":"84c1e603875fc7dbbbf2ce0a5cb344b10f7bd4a4","trusted":true},"cell_type":"code","source":"for featurename in (features_categorical + features_continuous):\n    if featurename not in ['PassengerId']:\n        print(f'{featurename} Unique Values:')\n        print(data_train_all[featurename].unique())","execution_count":170,"outputs":[]},{"metadata":{"_cell_guid":"0422686b-db96-4fa2-8ea5-498adca1d75e","_uuid":"bc49321e1fc17778df325c09fe3bcfa923b9a771","trusted":true},"cell_type":"code","source":"for featurename in (features_categorical[1:] + features_continuous):\n    if featurename not in ['PassengerId']:\n        print(f'{featurename} Unique Values:')\n        print(data_test[featurename].unique())","execution_count":171,"outputs":[]},{"metadata":{"_cell_guid":"1a6e3497-d676-4f2b-aee4-5ab1d9bd7b0d","_uuid":"a5b32a5d74f69bcef1733c038f714c330cf39806"},"cell_type":"markdown","source":"## Feature Generation\n\nIn the lightest sense, that is. We can encode our categorical features as numerical values where they are not already. Luckily, the only one where this is the case is the Sex column."},{"metadata":{"_cell_guid":"70392f7e-5d63-42ae-962e-b3336e0cb281","_uuid":"ce386523e0fd137c4d179df6d88febdae27b096e","collapsed":true,"trusted":true},"cell_type":"code","source":"data_train_all['Sex'] = data_train_all['Sex'].replace(to_replace='male', value=0)\ndata_train_all['Sex'] = data_train_all['Sex'].replace(to_replace='female', value=1)\ndata_test['Sex'] = data_test['Sex'].replace(to_replace='male', value=0)\ndata_test['Sex'] = data_test['Sex'].replace(to_replace='female', value=1)","execution_count":172,"outputs":[]},{"metadata":{"_cell_guid":"cdb62754-5c2e-4d63-b28c-51b474c6d13d","_uuid":"8b9a4666e8a4b0f8fcb27c1bd8068d460a2cf033","trusted":true},"cell_type":"code","source":"data_test['Sex'].unique()","execution_count":173,"outputs":[]},{"metadata":{"_cell_guid":"a3117565-4687-443f-8034-896aaa35cc36","_uuid":"737a009bc3885d30de05eb5c9eb40ac411510539"},"cell_type":"markdown","source":"Nice! That makes things easier for us."},{"metadata":{"_cell_guid":"fccac303-2da1-45ab-8245-b48a6d31dc76","_uuid":"4695c8ef98145021c721ea180887779496b2cf25"},"cell_type":"markdown","source":"## Visualizing the Data\n\nI'm going to skip over this for now. In a real competition or production system, I'd be diving more into the data, but for now I'll forego that in the interest of time."},{"metadata":{"_cell_guid":"f174d070-e56a-4ada-8c1f-b09eb114a6eb","_uuid":"7d4520027d48d4a41d5b1b68599f1ea6864c1b5f"},"cell_type":"markdown","source":"## Modeling!\n\nJumping into the fun stuff. Depending on your definition of fun.\n\nWe'll hit the easy button and work this as a logistic regression problem, since that's what we're familiar with."},{"metadata":{"_cell_guid":"4404d7a6-5ffc-4517-8a95-f55e7cc911e2","_uuid":"28ff65d14f4f8fc0be113df9b15391d7166f9c09","collapsed":true,"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler","execution_count":174,"outputs":[]},{"metadata":{"_cell_guid":"a4766d61-9134-4f7a-a56a-95bc4e403707","_uuid":"89dbe7eb4690856b23abd26e28abec5f14972b3b","trusted":true},"cell_type":"code","source":"# Split the data into training and evaluation sets. This must be done after the preprocessing to avoid duplicating work across two DataFrames.\ndata_train, data_eval = np.split(\n    data_train_all, \n    [\n        int(0.8*len(data_train_all))\n    ]\n)\n\n# Could have done this with sklearn with the shufflesplit. Can try on later notebooks.\n\nprint(f'{len(data_train)} / {len(data_eval)}\\n{len(data_train)/len(data_train_all)} / {len(data_eval)/len(data_train_all)}')","execution_count":175,"outputs":[]},{"metadata":{"_cell_guid":"96b53e1a-f2d2-47cf-a47d-b4f5b5f3926c","_uuid":"6fa0873fb8f7d1c6c4fdbd68d6b34aa31985dc07","trusted":true},"cell_type":"code","source":"X = data_train.drop(columns=['Survived'])\ny = data_train['Survived']\nX_eval = data_eval.drop(columns=['Survived'])\ny_eval = data_eval['Survived']\n\ny_eval.head()","execution_count":176,"outputs":[]},{"metadata":{"_cell_guid":"8bed6150-7215-4fde-bf81-70c8a22699b6","_uuid":"66a963fe6b9d35cee393e90dc20621cc9786e310","collapsed":true,"trusted":true},"cell_type":"code","source":"# Standardize our features.\nscaler = StandardScaler()\nX_std = scaler.fit_transform(X)\nX_eval_std = scaler.fit_transform(X_eval)","execution_count":177,"outputs":[]},{"metadata":{"_cell_guid":"c107a0b5-ba6f-4a76-9b61-53d9e01f1267","_uuid":"4e8b958c4bf423258e3578bedb29df1d06889639","collapsed":true,"trusted":true},"cell_type":"code","source":"# Create the Logistic Regression.\nclf = LogisticRegression(random_state=0)","execution_count":178,"outputs":[]},{"metadata":{"_cell_guid":"6f23d33d-a74d-46a1-8476-e9ffd5191355","_uuid":"0ddd476705ba181177e65f924f8d57480cf1cc5d","collapsed":true,"trusted":true},"cell_type":"code","source":"# And now we train!\n#model = clf.fit(X_std, y)\n\n# Gonna try feeding in unstandardized features.\nmodel = clf.fit(X, y)","execution_count":179,"outputs":[]},{"metadata":{"_cell_guid":"76d76cda-8562-4ae8-8f83-59e69411dbc6","_uuid":"7406a319a1afacdae7c17a06c3dca46a711714fe","trusted":true},"cell_type":"code","source":"# Once the model has been trained, we want to pass it a prediction to test its output. Let's build one.\nprediction = pd.DataFrame({\n    'Age': [12, 30],\n    'SibSp': [2, 1],\n    'Parch': [0, 1],\n    'Pclass': [1, 3],\n    'Sex': [0, 1],\n    'Survived': [0, 1]\n})\n\nmodel.predict(prediction)\n#model.predict_proba(prediction)","execution_count":180,"outputs":[]},{"metadata":{"_cell_guid":"ebe952f2-1eab-4cd0-83f5-20ac0d45b7d4","_uuid":"9f5cfea795ad2088abfbc753e7fa70a477c1361b"},"cell_type":"markdown","source":"Now that we've got a model and a toy prediction entry to give it, let's process the evaluation set the same way we did the training set, and see how it does."},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"826fe79855f51826f5c38c911df540152ad5944b"},"cell_type":"code","source":"evaluation_predictions = model.predict(X_eval)\ntotal_eval_predictions = len(evaluation_predictions)\nincorrect_counter = 0","execution_count":181,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"90d2eed13df8b2955b277e2687748132ccefc0c9"},"cell_type":"code","source":"for i in range(0, total_eval_predictions):\n    if evaluation_predictions[i] != y_eval.iloc[i]:\n        incorrect_counter += 1\nprint(f'The resulting accuracy: {(total_eval_predictions - incorrect_counter) / total_eval_predictions * 100}% correct!')","execution_count":182,"outputs":[]},{"metadata":{"_cell_guid":"b48508d3-2833-4b1a-85ed-b41d360c59cf","_uuid":"5b317dfaa8084f78735a897fd94a6f8952c95614","collapsed":true},"cell_type":"markdown","source":"## Submission\n\nNow we craft the CSV to submit.\n\nRules:\n\n\n\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.\n\nThe file should have exactly 2 columns:\n\n    PassengerId (sorted in any order)\n    Survived (contains your binary predictions: 1 for survived, 0 for deceased)\n```\nPassengerId,Survived\n 892,0\n 893,1\n 894,0\n Etc.\n```"},{"metadata":{"_cell_guid":"6519beed-0154-4d72-b567-5c1a82d7db02","_uuid":"731bc3f61618d50a7315adc4e404ed40e3579ce4","collapsed":true,"trusted":true},"cell_type":"code","source":"X_test = data_test\nX_test_std = scaler.fit_transform(X_test)\n\ntest_prediction_results = model.predict(X_test)","execution_count":183,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"92d2515682435ae00c15bb5257d7788815e60f23"},"cell_type":"code","source":"submission = pd.DataFrame({\n    'PassengerId': X_test['PassengerId'],\n    'Survived': test_prediction_results\n})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a6e06ba6d60d9ba239c9115614167b5c91b40ce8"},"cell_type":"code","source":"submission.to_csv('titanic-test-results.csv', sep=',', index=False)\npd.read_csv('titanic-test-results.csv')","execution_count":184,"outputs":[]},{"metadata":{"_cell_guid":"7fada9f0-f91e-4bff-a170-ea46372c13e2","_uuid":"0c107bde5453db5d214fcd02d8ef8294f850580c","collapsed":true},"cell_type":"markdown","source":"## And we're done!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}