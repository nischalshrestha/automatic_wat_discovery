{"cells":[{"metadata":{"_uuid":"696ea18698a853caf1ec2c11f9cc51844b64197d"},"cell_type":"markdown","source":"**Downloading the necessary libraries**\n\nFor the Titanic Classification Problem we will use:\n* Pandas\n* Numpy\n* Scikit-Learn\n* Maplotlib\n* Seaborn\n* OS\n\n\n\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Import visualization library, as well as Machine Learning classes\nimport seaborn as sns\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.metrics import roc_auc_score, classification_report, confusion_matrix, roc_curve\nimport matplotlib.pyplot as plt\nimport warnings\n\n# Any results you write to the current directory are saved as output.\n\n# Import the train data\n# Display and review summary statistics \ntrain_data = pd.read_csv('../input/train.csv', index_col=0)\n\n# Handle missing data by assigning the mean value of the dataset\nmean_value_age = round(train_data['Age'].mean())\ntrain_data['Age'].fillna(mean_value_age, inplace=True)","execution_count":23,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4d90cc7ff1f9a57f9c33ba1af63f1c25df441d43","collapsed":true},"cell_type":"code","source":"train_data.head(10)","execution_count":2,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d736c251281434f07b299c9b9c60b41d19553424","scrolled":true,"collapsed":true},"cell_type":"code","source":"# Compare the surviving and non-surviving passengers by the ticket fare and gender\nplt.subplots(figsize=(20, 10))\n\nplt.subplot(2,2,1)\nplt.title('Surviving passengers \\n by gender and ticket price')\nsns.swarmplot(x='Sex', y='Fare', hue='Pclass', data=train_data[train_data['Survived']==1])\nplt.legend(loc='upper left', title='Passenger Class')\n\nplt.subplot(2,2,2)\nplt.title('Non-surviving passengers\\n by gender and ticket price')\nsns.swarmplot(x='Sex', y='Fare', hue='Pclass', data=train_data[train_data['Survived']==0])\nplt.legend(loc='upper left', title='Passenger Class')\n\nplt.subplot(2,2,3)\nplt.title('Surviving passengers \\n by gender and ticket price')\nsns.swarmplot(x='Sex', y='Fare', hue='Embarked', data=train_data[train_data['Survived']==1])\nplt.legend(loc='upper left', title='Location Embarked')\n\nplt.subplot(2,2,4)\nplt.title('Non-surviving passengers\\n by gender and ticket price')\nsns.swarmplot(x='Sex', y='Fare', hue='Embarked', data=train_data[train_data['Survived']==0])\nplt.legend(loc='upper left', title='Location Embarked')\n\nplt.tight_layout()\nplt.show()","execution_count":2,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1821686d03194627348a0f6aedc8c89f94d8fb66","collapsed":true},"cell_type":"code","source":"# Continue to use the plots to explore the relationship between features\nplt.figure(figsize=(6,4))\nplt.title('Breakdown of survival \\n by age and embarkation location')\nsns.swarmplot(x='Embarked', y='Age', hue='Survived', data=train_data)\nplt.legend(loc='upper left', title='Survived vs Fatalities')\n\nplt.figure(figsize=(6,4))\nplt.title('Breakdown of survival \\n by fare and embarkation location')\nsns.swarmplot(x='Embarked', y='Fare', hue='Survived', data=train_data)\nplt.legend(loc='upper left', title='Survived vs Fatalities')\n\nplt.figure(figsize=(6,4))\nplt.title('Survived vs Fatalities')\nsns.countplot(train_data['Survived'])\n\nsns.pairplot(train_data)\n","execution_count":34,"outputs":[]},{"metadata":{"_uuid":"b67f6fd156d2087c12da177ba1fa8d58dc03d95f"},"cell_type":"markdown","source":"** Preprocessing the Data by selecting the features that we will use to predict the survival encoding of the passengers in the test set**\n\n*In the chunk of code below we select continuous variables such as age and ticket price paid as well as discrete variables such as passenger class, number of siblings/spouses on board, number of parents/children on board, gender and embarkation location. *"},{"metadata":{"trusted":true,"_uuid":"95e0bb5d2b269d668e2d6445e20eb11bdd058c40","collapsed":true},"cell_type":"code","source":"# Get dummy variables for sex, pclass, embarked\ntrain_data['FamilySize'] = train_data['SibSp'] + train_data['Parch']\n\nprocessed_data = pd.get_dummies(data=train_data, columns=['Sex', 'Pclass', 'Embarked'])\n\n# Get the feature columns of interest and assign them to X\nX = processed_data[['Age', 'Fare', 'FamilySize', 'SibSp', 'Sex_female', 'Sex_male', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'Embarked_C', 'Embarked_Q', 'Embarked_S']]\ny = processed_data[['Survived']]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","execution_count":24,"outputs":[]},{"metadata":{"_uuid":"23b282a03a8b096b1556b7c38452a170253cab1c"},"cell_type":"markdown","source":"** Implementing a Basic KNN model**\n\n*This will allow to get a feel for the model accuracy and will set the stage for further hyper parameter tuning.*\n*We picked 22 n neighbors and the distance method for weights parameter.*\n*The model performed at almost 75.98% accuracy.*"},{"metadata":{"trusted":true,"_uuid":"e967001895f1c38ca521b1b3dab69747d4d621d8","scrolled":true,"collapsed":true},"cell_type":"code","source":"# Perform a train test split on the preliminary train data\nN_NEIGHBORS = 22\nknn = KNeighborsClassifier(n_neighbors=N_NEIGHBORS, weights='distance')\nknn.fit(X_train, y_train.values.ravel())\nprint('Accuracy of the basic model with', N_NEIGHBORS, 'neighbors is', knn.score(X_test, y_test))\n","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"c3f98d8c3ef5549ce78bd4ecd57066cbb63222d5"},"cell_type":"markdown","source":"*The code below plots the accuracy vs. number of n_neighbors, using the uniform method for the weight parameter.*\n*The maximum accuracy is achieved at 6 and 26 neighbors, when the accuracy is 74.3%*"},{"metadata":{"trusted":true,"_uuid":"3c61128caf488cddbe24eba3885db63708353e6a","collapsed":true},"cell_type":"code","source":"# Determine the best value for n_neighbors that increases accuracy\nneighbors = np.arange(1,40)\ntrain_accuracy = np.empty(len(neighbors))\ntest_accuracy = np.empty(len(neighbors))\nfor i, k in enumerate(neighbors):\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train, y_train.values.ravel())\n    \n    train_accuracy[i] = knn.score(X_train, y_train)\n    test_accuracy[i] = knn.score(X_test, y_test)\n    \n# Plot the relationship between n_neighbors and accuracy\nplt.figure(figsize=(7,5))\nplt.title('Training vs Testing Accuracy')\nplt.plot(neighbors, train_accuracy, label = 'Training Accuracy')\nplt.plot(neighbors, test_accuracy, label = 'Testing Accuracy')\nplt.xlabel('K Neighbors')\nplt.ylabel('Accuracy')\nplt.legend()\n\nmax_val = test_accuracy.max()\nindex_number_of_max_val = list(test_accuracy).index(max_val)\nprint('Highest accuracy is', max_val, ', achieved with ', index_number_of_max_val+1, ' n_neighbors')","execution_count":129,"outputs":[]},{"metadata":{"_uuid":"16a037a742b14a471a24ad14300606e5538abcea"},"cell_type":"markdown","source":"*This code modifies the one above slightly by changing the weights parameter to distance method. We can see that this change paid off, with the new accuracy at 75.98% with 22 neighbors.*"},{"metadata":{"trusted":true,"_uuid":"6a33244725b7accad78e21d1598a659ebed50575","collapsed":true},"cell_type":"code","source":"# Determine the best value for n_neighbors that increases accuracy\nneighbors = np.arange(1,40)\ntrain_accuracy = np.empty(len(neighbors))\ntest_accuracy = np.empty(len(neighbors))\nfor i, k in enumerate(neighbors):\n    knn = KNeighborsClassifier(n_neighbors=k, weights='distance')\n    knn.fit(X_train, y_train.values.ravel())\n    \n    train_accuracy[i] = knn.score(X_train, y_train)\n    test_accuracy[i] = knn.score(X_test, y_test)\n    \n# Plot the relationship between n_neighbors and accuracy\nplt.figure(figsize=(7,5))\nplt.title('Training vs Testing Accuracy')\nplt.plot(neighbors, train_accuracy, label = 'Training Accuracy')\nplt.plot(neighbors, test_accuracy, label = 'Testing Accuracy')\nplt.xlabel('K Neighbors')\nplt.ylabel('Accuracy')\nplt.legend()\n\nmax_val = test_accuracy.max()\nindex_number_of_max_val = list(test_accuracy).index(max_val)\nprint('Highest accuracy is', max_val, ', achieved with ', index_number_of_max_val+1, ' n_neighbors')","execution_count":130,"outputs":[]},{"metadata":{"_uuid":"100be2f497a28e5374e7e8121880c9be0971d539"},"cell_type":"markdown","source":"*We then perform a CV on the Classifier, using 22 n_neighbors. The mean accuracy of 5 fold CV is 71.62%*"},{"metadata":{"trusted":true,"_uuid":"490ca685180b042a02fef1154fbaac169a8002ed","scrolled":false,"collapsed":true},"cell_type":"code","source":"# Perform Cross Validation on the train data\nN_NEIGHBORS_CV = 22\ncv = cross_val_score(KNeighborsClassifier(n_neighbors=N_NEIGHBORS_CV), X, y.values.ravel(), cv=5)\narray = [1,2,3,4,5]\ncv_accuracy_scores = pd.DataFrame({'CV Holdout Set': array, 'Accuracy': cv})\ncv_accuracy_scores\nprint('Mean accuracy of Cross Validation with', N_NEIGHBORS_CV, 'neighbors is', cv.mean())","execution_count":133,"outputs":[]},{"metadata":{"_uuid":"02bccd2cf8a4edef16d126a51d488175b1c6bf24"},"cell_type":"markdown","source":"*Then, we implement a GridSearchCV  that searches for the optimal n_neighbors parameter only. It reveals that 7 is the optimal value for the parameter, with the average accuracy across 5 fold CV at 71.83%*"},{"metadata":{"trusted":true,"_uuid":"4d62d9b848a7490537d18763b1ea46d61279ce24","collapsed":true},"cell_type":"code","source":"param_grid = {'n_neighbors': np.arange(1,50)}\nknn_pre = KNeighborsClassifier()\nknn_cv = GridSearchCV(knn_pre, param_grid, cv=5)\nknn_cv.fit(X,y.values.ravel())\nprint(knn_cv.best_params_)\nprint(knn_cv.best_score_)\nprint(knn_cv.best_estimator_)\nwarnings.filterwarnings(action='ignore')","execution_count":134,"outputs":[]},{"metadata":{"_uuid":"52a6138285a281273e31bf64992c2f8916003894"},"cell_type":"markdown","source":"*Finally, we implement a GridSearchCV with both weights and n_neighbors parameters. The search determines that the best parameter for n_neighbors is 8 and the best parameter for weight is distance, with the average accuracy across 5 fold CV at 74.07%*"},{"metadata":{"trusted":true,"_uuid":"a64f699b25126fab2bd009d5d07ec1f6f011b149","collapsed":true},"cell_type":"code","source":"weight_list = ['uniform', 'distance']\nn_neighbors = list(range(1, 40))\nparam_grid = {'n_neighbors': n_neighbors, 'weights': weight_list}\nknn_e = KNeighborsClassifier()\ngrid_knn = GridSearchCV(knn_e, param_grid, cv=10, scoring='accuracy')\ngrid_knn.fit(X,y.values.ravel())\nprint(grid_knn.best_score_)\nprint(grid_knn.best_params_)\nprint(grid_knn.best_estimator_)\n","execution_count":135,"outputs":[]},{"metadata":{"_uuid":"20d4f936666051fef4bbd91205f698eeb0314782"},"cell_type":"markdown","source":"**Basic Logistic Regression Model**\n\n*This model is a basic model with default hyperparameters, in order for us to see what accuracy to expect.\n*We obtain *"},{"metadata":{"trusted":true,"_uuid":"63cbe6faa842630f4978d953fb40ece61e50ce87","collapsed":true},"cell_type":"code","source":"logreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\nprint('Score of default model is', logreg.score(X_test, y_test))\ny_pred = logreg.predict_proba(X_test)[:,1]\nprint('AUC score of default model is', roc_auc_score(y_test, y_pred))\ny_pred = logreg.predict(X_test)","execution_count":136,"outputs":[]},{"metadata":{"_uuid":"22e82959c6792ac36263f62692d460cc189300f2"},"cell_type":"markdown","source":"*Below is the confusion matrix that shows the True Positive, True Negative, False Positive and False Negative values, as well as the classification report that computes Precision, Recall and F1 score based on the confusion matrix*"},{"metadata":{"trusted":true,"_uuid":"4b79d773f748808fa144cb08c6d1e3c43be7e99e","collapsed":true},"cell_type":"code","source":"print(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","execution_count":137,"outputs":[]},{"metadata":{"_uuid":"d5683c4700a1554eede5b0cbca5d6b8e51c646c6"},"cell_type":"markdown","source":"*Here is the ROC Curve of the above basic Logistic Regression Model*"},{"metadata":{"trusted":true,"_uuid":"c5222a2dfcadb8fc5a985814d46e315676af36b0","collapsed":true},"cell_type":"code","source":"y_pred_prob = logreg.predict_proba(X_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n\nplt.figure()\nplt.plot([0,1], [0,1], 'k--')\nplt.plot(fpr, tpr)\nplt.title('ROC Curve')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\n\nprint('The AUC score of the baseline logit model is', roc_auc_score(y_test, y_pred_prob))\n","execution_count":138,"outputs":[]},{"metadata":{"_uuid":"4d627ac2b1a098efaa3ac9a1a5cf01f1d92fe9c5"},"cell_type":"markdown","source":"**Let's tune the hyperparameters of the baseline model.**\n\n*We need to tune the C and the penalty parameters of the baseline model and see if we can improve on the accuracy.*"},{"metadata":{"trusted":true,"_uuid":"b7c78b88f9a2e5de8321b168acf4a8bab24f0938","collapsed":true},"cell_type":"code","source":"params = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000], 'penalty': ['l1', 'l2'] }\nlogit_grid = GridSearchCV(logreg, params, cv=5)\nlogit_grid.fit(X_train, y_train.values.ravel())\n","execution_count":139,"outputs":[]},{"metadata":{"_uuid":"0c982f8c058d75c9291d895ba5350cb906561faf"},"cell_type":"markdown","source":"*Once we have tuned the hyperparameters, let's see how the accuracy and AUC score change as a result. It improved by relatively little, 0.00128, to 87.77%. However, since the accuracy of the base model with C (regularization parameter) equal to 1 was higher, I will simply proceed with that result.*"},{"metadata":{"trusted":true,"_uuid":"b700cb4ed2dfcafc3b350e57321197b9fb34ffe8","collapsed":true},"cell_type":"code","source":"print('Score of tuned model is', logit_grid.score(X_test, y_test))\ny_pred_grid = logit_grid.predict_proba(X_test)[:,1]\nprint('AUC score of tuned model is', roc_auc_score(y_test, y_pred_grid))\n\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_grid)\n\nplt.figure()\nplt.plot([0,1], [0,1], 'k--')\nplt.plot(fpr, tpr)\nplt.title('ROC Curve')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\n\nprint('The model has improved by', roc_auc_score(y_test, y_pred_grid) - roc_auc_score(y_test, y_pred_prob))\n\nprint(logit_grid.best_params_)\nprint(logit_grid.best_score_)\nprint(logit_grid.best_estimator_)\n\ny_pred_grid_binary = logreg.predict(X_test)\nprint(confusion_matrix(y_test, y_pred_grid_binary))\nprint(classification_report(y_test, y_pred_grid_binary))","execution_count":140,"outputs":[]},{"metadata":{"_uuid":"dd94278f89ff583b6ef6b608b30d7359a5e4358f"},"cell_type":"markdown","source":"**Basic DecisionTreeClassifier**"},{"metadata":{"trusted":true,"_uuid":"d0ed4c04f956806bfe4401d4bb1b891442433de8"},"cell_type":"code","source":"tree = DecisionTreeClassifier()\ntree.fit(X_train, y_train.values.ravel())\nprint('The baseline Decision Tree model accuracy is', tree.score(X_test, y_test))\nprint(confusion_matrix(y_test, tree.predict(X_test)))\nprint(classification_report(y_test, tree.predict(X_test)))\n","execution_count":25,"outputs":[]},{"metadata":{"_uuid":"80ed9c69e558716f86e17a17dcd1a53f44c7fdb7"},"cell_type":"markdown","source":"*Based on the graph, the best maximum depths is 9*"},{"metadata":{"trusted":true,"_uuid":"9e185b486f43e0a6fc306706cb23c984a4a82cad"},"cell_type":"code","source":"max_depths = np.linspace(1,32,32,endpoint=True)\ntrain_results_tree = []\ntest_results_tree = []\n\nfor max_depth in max_depths:\n    tree = DecisionTreeClassifier(max_depth = max_depth)\n    tree.fit(X_train, y_train.values.ravel())\n    train_pred = tree.predict(X_train)\n    train_results_tree.append(roc_auc_score(y_train, train_pred))\n\n    tree = DecisionTreeClassifier(max_depth = max_depth)\n    tree.fit(X_train, y_train.values.ravel())\n    test_pred = tree.predict(X_test)\n    test_results_tree.append(roc_auc_score(y_test, test_pred))\n\nplt.title('Train vs Test Accuracy')    \nplt.plot(max_depths, train_results_tree, label='Train')\nplt.plot(max_depths, test_results_tree, label='Test')\nplt.xlabel('Depth')\nplt.ylabel('Accuracy')\nplt.legend()","execution_count":26,"outputs":[]},{"metadata":{"_uuid":"aecd0cb856748f962ebdd821c5db14b50122defd"},"cell_type":"markdown","source":"*The maximum depth parameter that maximized AUC score is 3*"},{"metadata":{"trusted":true,"_uuid":"dda69c34d2a631c77e844b5c5388ec0fbe860a51"},"cell_type":"code","source":"tree = DecisionTreeClassifier()\nmax_depths = {'max_depth':np.linspace(1,40,40, endpoint=True)}\ntree_grid = GridSearchCV(tree, max_depths, scoring = 'roc_auc', cv=5)\ntree_grid.fit(X_train, y_train.values.ravel())\n\nprint(tree_grid.best_estimator_)\nprint(tree_grid.best_params_)\nprint(tree_grid.best_score_)\n\n","execution_count":27,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9fe8950a733dcde74a69eeec2bd6121d43218788","collapsed":true},"cell_type":"code","source":"test_data = pd.read_csv('../input/test.csv')\n","execution_count":28,"outputs":[]},{"metadata":{"_uuid":"67425f64bb45b68683ccdcf6447147f6e24846a7"},"cell_type":"markdown","source":"**RandomForestClassifier, baseline model with default parameters**\n\n*The model shows the AUC score to be 87.44%*"},{"metadata":{"trusted":true,"_uuid":"af0f9ebe1649fbd29923a63efdabc8226eefab94","collapsed":true},"cell_type":"code","source":"forest = RandomForestClassifier()\nforest.fit(X_train, y_train.values.ravel())\ny_pred_forest = forest.predict_proba(X_test)[:,1]\n\nprint('The baseline accuracy is', forest.score(X_test, y_test))\nprint('The baseline model AUC score is', roc_auc_score(y_test, y_pred_forest))\n","execution_count":147,"outputs":[]},{"metadata":{"_uuid":"60c36ecaf04aa1b2ef83c9f86799f95d7b4c9b58"},"cell_type":"markdown","source":"*Tuning the n_estimators to 500 improves the model AUC score to over 88.63%*"},{"metadata":{"trusted":true,"_uuid":"70faa787e7b0bf4c794e55bd3814972a0e3803b5","collapsed":true},"cell_type":"code","source":"forest_tuned = RandomForestClassifier(n_estimators = 500)\nforest_tuned.fit(X_train, y_train.values.ravel())\ny_pred_forest_tuned = forest_tuned.predict(X_test)\ny_pred_forest_tuned_proba = forest_tuned.predict_proba(X_test)[:,1]\n\nprint('The tuned model Accuracy score is', forest_tuned.score(X_test, y_test))\nprint('The tuned model AUC score is', roc_auc_score(y_test, y_pred_forest_tuned_proba))\nprint(confusion_matrix(y_test, y_pred_forest_tuned))\nprint(classification_report(y_test, y_pred_forest_tuned))","execution_count":150,"outputs":[]},{"metadata":{"_uuid":"1a84cb3850bdd90344871156b4da6c0e4cb5775c"},"cell_type":"markdown","source":"**Now that we have trained 4 classification models, we can apply them to predict the Survival Rate on the test.csv file**\n\n*From the models that we trained, namely KNN, Logistic Regression, Decision Tree Classifier and Random Forest Classifier, we can see that Random Forest method with tuned n_estimators parameter is the one that consistently produces highest AUC score. Therefore we will use RF on the test data and write the results to a submission file.*"},{"metadata":{"trusted":true,"_uuid":"65ae064a6ff4b63c16c46ecf321118f1878ec27d"},"cell_type":"code","source":"test_data = pd.read_csv('../input/test.csv')\ntest_data.describe()","execution_count":29,"outputs":[]},{"metadata":{"_uuid":"5e8579bfc91bc117152119e0608337f848f2000c"},"cell_type":"markdown","source":"**Preprocessing the data**"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d59b2cf95b8498f1debe551cf4e46b1efba3388e"},"cell_type":"code","source":"def numeric_missing_value(data, column):\n    # Function takes in data in the form of a Pandas DataFrame and column names\n    # in list form, iterates through the columns and replaces the NaN values by\n    # the mean of the column.\n    for i in column:\n        try:\n            mean_value = round(data[i].mean())\n            data[i].fillna(mean_value, inplace = True)\n        except:\n            raise('Columns need to be numeric type')\n            \nnumeric_missing_value(test_data, ['Age', 'Fare'])","execution_count":30,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9aaf69931da76dbb27222ef452b94b4e409ddee1"},"cell_type":"code","source":"test_data.head()","execution_count":31,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2b492b9b751dec43149b3d9e6d13f81245b4b705","collapsed":true},"cell_type":"code","source":"test_data['FamilySize'] = test_data['Parch'] + test_data['SibSp']\n# Get dummy variables for sex, pclass, embarked\ntest_data = pd.get_dummies(data=test_data, columns=['Sex', 'Pclass', 'Embarked'])\n\n\n# Get the feature columns of interest and assign them to X\nX_for_prediction = test_data[['Age', 'Fare', 'FamilySize', 'SibSp', 'Sex_female', 'Sex_male', 'Pclass_1', 'Pclass_2', 'Pclass_3', 'Embarked_C', 'Embarked_Q', 'Embarked_S']]\n\n\n# Get dummy variables for sex, pclass, embarked\n\n# Get the feature columns of interest and assign them to X\n","execution_count":32,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5f9b7edbf7767a42f690e55439452bc6a11db34f","scrolled":true},"cell_type":"code","source":"test_data.head()\n#X_for_prediction","execution_count":33,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4d9a46022be69958993a83b7d9b4c5c5c15e63ec","collapsed":true},"cell_type":"code","source":"y_pred = tree_grid.predict(X_for_prediction)\nresults = {'PassengerID': test_data['PassengerId'], 'Survived': y_pred}\nsubmission_df = pd.DataFrame(results)\n\nsubmission_df.to_csv('submission_2.csv')\n","execution_count":34,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"adafd935c1aee2618c547ac35d4bf4c31e8ff4f0","collapsed":true},"cell_type":"code","source":"# IMPROVEMENTS TO THE MODELS ABOVE\n\n# Focus on the Accuracy score, not AUC score\n# Feature Engineering: FamilySize, int_term(PclassFemale), title\n# XGBoosting with GridSearchCV\n# Get the title of passengers, group them into classes\n# For KNN perform scaling for the feature variables that have the biggest range, standartize them\n# Produce a holdout data set, perform the classification methods above and test on the holdout set. This will be an \n# indicator on performance on the test.csv data","execution_count":107,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7efaac108e200e2a3619f6bdcab72204cbff1413"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}