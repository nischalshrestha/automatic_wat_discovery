{"nbformat_minor": 1, "nbformat": 4, "cells": [{"source": "All of my effort in Titanic are contained in this Notebook. You can upvote it for encouragement if you can find some help from this Notebook.\nAs my purpose is to study the basic data process techniques by Titanic data set, I believe there still have many details can be dredged to improve my model, but I think it's better enough for me.<br/>\nI am a student from China, and Waiting to go to NEU for my postgraduate study. I know my great distance from good English expression. So, please forgive my poor English if it has brought you extra difficulty in reading.<br/>\nIt's so cool that I can find so many people from all over the world. I hope my English doesn't matter you.<br/>\n**Please contact me without hesitation If you have some suggestions on my Notebook or my English : ).**<br/>\nThanks for watching!", "outputs": [], "cell_type": "markdown", "metadata": {"_execution_state": "idle", "_uuid": "49aa494b0d2ef1b510ddeb7a973ba6856db16013", "_cell_guid": "6208096b-db97-4bf2-ba23-dbc09cad451e"}, "execution_count": null}, {"source": "import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.cross_validation import KFold\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\ntrain_data = pd.read_csv('../input/train.csv')\ntest_data = pd.read_csv('../input/test.csv')\n\nid_list = test_data[\"PassengerId\"]\ntrain_data = train_data.drop(['PassengerId'], axis=1)\ntest_data = test_data.drop(['PassengerId'], axis=1)", "outputs": [], "cell_type": "code", "metadata": {"_execution_state": "idle", "trusted": true, "_uuid": "113ccd16253bc599b0238841be42380caec58f17", "_cell_guid": "dec56701-3397-44f5-a00a-8dbdb63928d9"}, "execution_count": 1}, {"source": "train_data.head()", "outputs": [], "cell_type": "code", "metadata": {"_execution_state": "idle", "trusted": true, "_uuid": "3cc6b2b0ae47ed985784f3cb37e5aaae4f0cb67d", "_cell_guid": "ca372f83-b5af-40dc-bb58-f45c3a4ab3e7"}, "execution_count": 2}, {"source": "train_data.info()", "outputs": [], "cell_type": "code", "metadata": {"_execution_state": "idle", "trusted": true, "_uuid": "418f416d1dc49f5b67c0ff6341819b92389a147a", "_cell_guid": "209ffc42-fb0b-4987-9193-03c0f14acacc"}, "execution_count": 3}, {"source": "There are some missing values exist in columns *Age*, *Embarked* and *Cabin*.\nIn general, we using mode to fill *Embarked* because it has just 2 missing value, and using linear regression to predict *Age* because the age maybe a very important feature in this question.\nIn fact, there are some missing value in column *Fare* at test set. with respect to *Fare*, I believe use mean value is better.", "outputs": [], "cell_type": "markdown", "metadata": {"_execution_state": "idle", "_uuid": "4745117d010ae351f9326dd8a0e3974cfcc03a83", "_cell_guid": "0bb04860-e70a-4f39-913f-f182b8b2aafc"}, "execution_count": null}, {"source": "def feature_normalization(feat_vector):\n    # scala value to [-1, +1]\n    max_value = max(feat_vector)\n    min_value = min(feat_vector)\n    mean_value = feat_vector.mean()\n    return (feat_vector - mean_value) / (max_value - min_value)\n\n\n# It's not reasonable to transfer *Embarked* or *Pclass* into continuous value\ndef dummy(data, columns):\n    for column in columns:\n        if column not in data.columns:\n            continue\n        dummy_data = pd.get_dummies(data[column], drop_first=True)\n        # rename columns: column name + 1,2,3\n        num = len(dummy_data.loc[1, :])\n        dummy_data.columns = [column+str(x+1) for x in range(num)]\n        data = pd.concat([data, dummy_data], axis=1)\n\n        data = data.drop(column, axis=1)\n    return data", "outputs": [], "cell_type": "code", "metadata": {"_execution_state": "idle", "collapsed": true, "trusted": true, "_uuid": "c5e6928818be1ae62b9d9a266d40fe88af2883f2", "_cell_guid": "1f94d5c8-5a6b-4336-b182-4c0f1ce9bef6"}, "execution_count": 4}, {"source": "def cabin_extract(data):\n    # classify Cabin by fare\n    data['Cabin'] = data['Cabin'].fillna('X')\n    data['Cabin'] = data['Cabin'].apply(lambda x: str(x)[0])\n    data['Cabin'] = data['Cabin'].replace(['A', 'D', 'E', 'T'], 'M')\n    data['Cabin'] = data['Cabin'].replace(['B', 'C'], 'H')\n    data['Cabin'] = data['Cabin'].replace(['F', 'G'], 'L')\n    data['Cabin'] = data['Cabin'].map({'X': 0, 'L': 1, 'M': 2, 'H': 3}).astype(int)\n    return data\n\ntrain_data = cabin_extract(train_data)\ntest_data = cabin_extract(test_data)\n# show the connection between Cabin and Survive rate\nax = plt.axes()\ndata = train_data.groupby(['Cabin'])[['Survived']].count()\nsns.barplot(x=data.index, y=data['Survived'], alpha=0.8, color='violet', ax=ax)\ndata = train_data.groupby(['Cabin'])[['Survived']].sum()\nsns.barplot(x=data.index, y=data['Survived'], alpha=0.8, color='cornflowerblue', ax=ax)\nax.set_title('Survived or Not')\nsns.plt.show()\ntrain_data.groupby(['Cabin'])[['Fare']].mean()  # mean fare of each cabin class", "outputs": [], "cell_type": "code", "metadata": {"_execution_state": "idle", "trusted": true, "_uuid": "e8e1107a9436217fe0d2edf65ac37f53b3e5d89e", "_cell_guid": "02ab4b76-3d8a-4a39-9f33-e3087d096863"}, "execution_count": 5}, {"source": "def SexByPclass(data):\n    data['Sex'] = data['Sex'].map({'female': 1, 'male':0})\n    data['Sex'] = data['Sex'].astype(int)\n    data['Pclass'] = data['Pclass'].map({1: 3, 2: 2, 3:1}).astype(int)\n    # data['SexByPclass'] = feature_normalization((data['SexTemp'] * data['Pclass']).astype(int))\n    data.loc[data['Sex']==0, 'SexByPclass'] = data.loc[data['Sex']==0, 'Pclass']\n    data.loc[data['Sex']==1, 'SexByPclass'] = data.loc[data['Sex']==1, 'Pclass'] + 3\n    data['SexByPclass'] = data['SexByPclass'].astype(int)\n    return data\n\ntrain_data = SexByPclass(train_data)\ntest_data = SexByPclass(test_data)\ntemp = pd.crosstab([train_data.SexByPclass,], train_data.Survived.astype(bool))\ntemp.plot(kind='bar', stacked=True, color=['violet','cornflowerblue'], alpha=0.8, grid=False)", "outputs": [], "cell_type": "code", "metadata": {"_execution_state": "idle", "trusted": true, "_uuid": "47edacd941407cf76f5aa050c83965f4dee9826b", "_cell_guid": "95a1d1ad-7f77-44b5-ac92-78747afd3010"}, "execution_count": 6}, {"source": "def fill_missing_embarked(data):\n    freq_port = data['Embarked'].mode()[0]\n    data['Embarked'] = data['Embarked'].fillna(freq_port)\n    data['Embarked'] = data['Embarked'].map({'S': 0, 'Q': 1, 'C': 2}).astype(int)\n    return data\n\ntrain_data = fill_missing_embarked(train_data)\ntest_data = fill_missing_embarked(test_data)\ntemp = pd.crosstab(train_data.Embarked, train_data.Survived)\ntemp.plot(kind='barh', stacked=True, color=['darksalmon','tomato'], grid=False)", "outputs": [], "cell_type": "code", "metadata": {"_execution_state": "idle", "trusted": true, "_uuid": "b0667eda7f3c4690cc5ca7c9a092db88b929d718", "_cell_guid": "4c5d9f28-0033-4f04-b79b-7c45f2465df3"}, "execution_count": 7}, {"source": "We can find that 'S' is the most frequent one, and the passenger from port  'C' are more likely to survive.", "outputs": [], "cell_type": "markdown", "metadata": {"_execution_state": "idle", "_uuid": "b6f43cf0db90708c1b54dd16f738c77ccdd46b56", "_cell_guid": "a4782286-feab-4b1f-ad6c-d8cb866889d4"}, "execution_count": null}, {"source": "def fare_info(data):\n    data.loc[data.Fare.isnull(), 'Fare'] = data['Fare'].mean()\n    data['Fare'] = data['Fare'].astype(int)\n    return data\n\ntrain_data = fare_info(train_data)\ntest_data = fare_info(test_data)\n\nfrom matplotlib.ticker import MultipleLocator, FormatStrFormatter\nxmajorLocator   = MultipleLocator(20)  # set the major locator to multiples of 5\nxmajorFormatter = FormatStrFormatter('%1.0f')\n\nsns.plt.figure(figsize=(13, 5))\nax = sns.plt.axes()\nsns.distplot(train_data['Fare'], kde = True, rug = True)\nsns.distplot(train_data.loc[train_data['Survived']==1, 'Fare'], kde = True, rug = True)\nax.xaxis.set_major_locator(xmajorLocator)  \nax.xaxis.set_major_formatter(xmajorFormatter) \nsns.plt.show()\n\nax = sns.boxplot(x=\"Pclass\", y=\"Fare\", hue=\"Survived\", data=train_data);\nax.set_yscale('log')\n\nsns.plt.show()", "outputs": [], "cell_type": "code", "metadata": {"_execution_state": "idle", "trusted": true, "_uuid": "3bd37c155ad0bcc75806d89aa847cd12844ab68f", "_cell_guid": "5a6f5972-23e0-408d-be9e-b0989f52a542"}, "execution_count": 8}, {"source": "From the distribution, we can get, the people whose ticket *Fare* range from 0 to 20 are in low survive rate.", "outputs": [], "cell_type": "markdown", "metadata": {"_execution_state": "idle", "_uuid": "12be042ff4f41493733ea08871ad2dec963b5d02", "_cell_guid": "5123d1ab-85de-4ccb-9a10-ad51b1784436"}, "execution_count": null}, {"source": "def fare_stage(data, mean_fare):\n    data.loc[data['Fare'] < mean_fare[1], 'FareStage'] = 1\n    data.loc[(data['Fare'] > mean_fare[1]) & (data['Fare'] < mean_fare[2]), 'FareStage'] = 2\n    data.loc[(data['Fare'] > mean_fare[2]) & (data['Fare'] < mean_fare[3]), 'FareStage'] = 3\n    data.loc[data['Fare'] > mean_fare[3], 'FareStage'] = 4\n    return data\n\ncombine = pd.concat([train_data.drop('Survived', 1), test_data])\nmean_fare = combine.groupby('Pclass')['Fare'].mean()\ntrain_data = fare_stage(train_data, mean_fare)\ntest_data = fare_stage(test_data, mean_fare)", "outputs": [], "cell_type": "code", "metadata": {"_execution_state": "idle", "collapsed": true, "trusted": true, "_uuid": "9fc4003959fde3fec220511f9a3073c2171e0030", "_cell_guid": "83f0d177-7d1c-407b-a4aa-edadb75511d1"}, "execution_count": 9}, {"source": "def name_extract(data):\n    # extract Title from name\n    data['Title'] = data.Name.str.extract('([A-Za-z]+)\\.', expand=False)\n    # delete rare title\n    data['Title'] = data['Title'].replace(['Lady', 'Countess', 'Capt', 'Col',\n        'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    data['Title'] = data['Title'].replace(['Mlle', 'Ms'], 'Miss')\n    data['Title'] = data['Title'].replace('Mme', 'Mrs')\n    data['Surname'] = data['Name'].apply(lambda x: str(x).split('.')[1].split(' ')[1])\n    data['Surname'] = data.Surname.str.replace('(', '')\n    title_map = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n    data['Title'] = data['Title'].map(title_map).astype(int)\n    data['SurnameLen'] = data['Name'].apply(lambda x: len(str(x).split('.')[1])).astype(int)\n                                            \n    return data.drop('Name', axis=1) \n\n\n\ntrain_data = name_extract(train_data)\ntest_data = name_extract(test_data)\n\ntemp = pd.crosstab(train_data.SurnameLen, train_data.Survived)\ntemp.plot(kind='bar', stacked=True, color=['orchid','cornflowerblue'], grid=False, figsize=(18, 5))", "outputs": [], "cell_type": "code", "metadata": {"_execution_state": "idle", "trusted": true, "_uuid": "c82431827eb646c33f7727302003a246275fdf4a", "_cell_guid": "539c5aea-1402-4b64-8e88-047fc6bd7113"}, "execution_count": 10}, {"source": "#sns.distplot(train_data['Title'], kde=True, rug=False)\n#plt.show()\nax = plt.axes()\ndata = train_data.groupby(['Title'])[['Survived']].count()\nsns.barplot(x=data.index, y=data['Survived'], alpha=0.8, color='orchid', ax=ax)\ndata = train_data.groupby(['Title'])[['Survived']].sum()\nsns.barplot(x=data.index, y=data['Survived'], alpha=0.8, color='cornflowerblue', ax=ax)\nax.set_title('Survived or Not')\nsns.plt.show()\nax = plt.axes()\ndata = train_data.groupby(['Title'])[['Survived']].mean()\nsns.barplot(x=data.index, y=data['Survived'], alpha=0.8, ax=ax)\nax.set_title('Survived Rate of Each Title')\nsns.plt.show()", "outputs": [], "cell_type": "code", "metadata": {"_execution_state": "idle", "trusted": true, "_uuid": "84ee0ac21c4ce445113a7d37a0a253c22f5456f1", "_cell_guid": "7abb552f-2614-482d-a6f9-b9c086c9ca7e"}, "execution_count": 11}, {"source": "def surname_extract(train, test):\n    Survived = train['Survived']\n    combine = pd.concat([train_data.drop('Survived',1),test_data])\n    combine['CommonSurname'] = np.where(combine.groupby(['Surname'])['Pclass'].transform('count') > 1, 1, 0)\n    combine.loc[combine['CommonSurname']==0, 'Surname'] = 'Rare'\n    train = combine.iloc[:len(train)]\n    train['Survived'] = Survived\n    test = combine.iloc[len(train):]\n    return  train, test\n\ntrain_data, test_data = surname_extract(train_data, test_data)\ntemp = pd.crosstab(train_data.Surname, train_data.Survived)\ntemp.plot(kind='bar', stacked=True, color=['darksalmon','tomato'], grid=False, figsize=(28, 5))\n#I can't find the way to use Surname as its completely sporadic disturbution\ntrain_data = train_data.drop('Surname', axis=1)\ntest_data = test_data.drop('Surname', axis=1)", "outputs": [], "cell_type": "code", "metadata": {"_execution_state": "idle", "trusted": true, "_uuid": "9729a6935ebc8bb15274cfde9ff1f7ea82b3083f", "_cell_guid": "36be1835-cbb4-4257-ad5e-05b1e24524b8"}, "execution_count": 12}, {"source": "def family_info(data):\n    data['FamliySize'] = data['SibSp'] + data['Parch'] + 1\n    data['Alone'] = data['Alone'] = (data['SibSp'] == 0) & (data['Parch'] == 0)\n    data['Alone'] = data['Alone'].astype(int)\n    # you should delete linear relation between family-size and sibsp and parch\n    return data\n    \ntrain_data = family_info(train_data)\ntest_data = family_info(test_data)", "outputs": [], "cell_type": "code", "metadata": {"_execution_state": "idle", "collapsed": true, "trusted": true, "_uuid": "f711059f138d4a9e70e0731d2c94737d397cfa44", "_cell_guid": "e9611a00-713d-467c-a0f5-2e3dfaa84c31"}, "execution_count": 13}, {"source": "def fill_missing_age(train_set, test_set):\n    '''\n    combina train and test data set,\n    using linear regression to fill missing value of age in two data sets\n    return: tuple constructed by train and test data set\n    '''\n    train_dropped = train_set[['Pclass', 'SibSp', 'Fare', 'FamliySize', 'Alone', 'Age']]\n    test_dropped = test_set[['Pclass', 'SibSp', 'Fare', 'FamliySize', 'Alone', 'Age']]\n    combine = pd.concat([train_dropped,test_dropped])\n    # training regression model\n    train = combine[combine.Age.notnull()]\n    model = LinearRegression()\n    model.fit(train.drop(['Age'], axis=1), train['Age'])\n    \n    data = combine.loc[combine.Age.isnull()]\n    predict_ages = model.predict(data.drop('Age', axis=1))\n    combine.loc[combine.Age.isnull(), 'Age'] = predict_ages\n    combine['Age'] = combine['Age'].astype(int)\n    train_set['Age'] = combine.iloc[:len(train_set)]['Age']\n    test_set['Age'] = combine.iloc[len(train_set):]['Age']\n\n    return train_set, test_set\n\ntrain_data, test_data = fill_missing_age(train_data, test_data)\n", "outputs": [], "cell_type": "code", "metadata": {"_execution_state": "idle", "collapsed": true, "trusted": true, "_uuid": "47c49e32452263c69aca54da73017397e38c175a", "_cell_guid": "9c3029c8-e9d2-498e-bd8b-5eaf8540a9e5"}, "execution_count": 14}, {"source": "def age_distribution(data):\n    xmajorLocator   = MultipleLocator(5)  # set the major locator to multiples of 5\n    xmajorFormatter = FormatStrFormatter('%1.0f')\n    sns.plt.figure(figsize=(12, 5))\n    ax = sns.plt.axes()\n    sns.kdeplot(data['Age'], shade=False, label='Train age distribution')\n    sns.kdeplot(data.loc[data['Survived']==1, 'Age'], shade=False, label='Survived distribution')   \n    ax.xaxis.set_major_locator(xmajorLocator)  \n    ax.xaxis.set_major_formatter(xmajorFormatter)  \n    sns.plt.show()\n\nage_distribution(train_data.loc[train_data[\"Sex\"]==1, :])\nage_distribution(train_data.loc[train_data[\"Sex\"]==0, :])", "outputs": [], "cell_type": "code", "metadata": {"_execution_state": "idle", "trusted": true, "_uuid": "c194aa6785eba226e6b1c9476ca11261eea1f596", "_cell_guid": "1704b29c-60c4-4dc2-b2f0-5337e94a3ce3"}, "execution_count": 15}, {"source": "draw_data = pd.crosstab(train_data.Age, train_data.Survived)\ndraw_data.div(draw_data.sum(1).astype(float), axis=0).plot(\n    kind='bar', stacked=True, color=['orchid','cornflowerblue'],grid=False, figsize=(15, 5))", "outputs": [], "cell_type": "code", "metadata": {"trusted": true, "_uuid": "b570e42f6ff0ef4c6f99f453e7ed7449b9f4ae01"}, "execution_count": 16}, {"source": "def age_extract(data):\n    # From the upper kde plot, these two kde convex have some junctions.\n    # When the green line is higher than the blue one, \n    # it indicates that the peopel whose age are in this stage have more opportunity to survive.\n    data.loc[data['Age'] < 15, 'Age'] = 1\n    data.loc[(data['Age'] < 32) & (data['Age'] > 14.9), 'Age'] = 3\n    data.loc[(data['Age'] < 60) & (data['Age'] > 31.9), 'Age'] = 2\n    data.loc[data['Age'] > 59.9, 'Age'] = 4\n    data['Age'] = data['Age'].astype(int)\n    return data\n\ntrain_data = age_extract(train_data)\ntest_data = age_extract(test_data)\ntemp = pd.crosstab([train_data.Age, train_data.Sex], train_data.Survived)\ntemp.plot(kind='bar', stacked=True, color=['orchid','cornflowerblue'], grid=False, figsize=(8, 5))", "outputs": [], "cell_type": "code", "metadata": {"_execution_state": "idle", "trusted": true, "_uuid": "6c3d82efd4223876721197cb11a53954df4998cf", "_cell_guid": "27b0d077-83f8-4700-9efe-a6b7e99a74b6"}, "execution_count": 17}, {"source": "def SexByAge(data):\n    data.loc[data['Sex']==0, 'SexByAge'] = data.loc[data['Sex']==0, 'Age']\n    data.loc[data['Sex']==1, 'SexByAge'] = data.loc[data['Sex']==1, 'Age'] + 5\n    data['SexByAge'] = data['SexByAge'].astype(int)\n    return data\n\n\ntrain_data = SexByAge(train_data)\ntest_data = SexByAge(test_data)\ndraw_data = pd.crosstab(train_data.SexByAge, train_data.Survived)\ndraw_data.div(draw_data.sum(1).astype(float), axis=0).plot(\n    kind='bar', stacked=True, color=['orchid','cornflowerblue'],grid=False, figsize=(8, 5))", "outputs": [], "cell_type": "code", "metadata": {"_execution_state": "idle", "trusted": true, "_uuid": "5dbf9a7b4ad1f804b01bb92cb41c7b01a1d5aa66", "_cell_guid": "9d382524-843c-4a89-9e10-58c9c151c0d4"}, "execution_count": 18}, {"source": "**Children and women go first**", "outputs": [], "cell_type": "markdown", "metadata": {"_execution_state": "idle", "_uuid": "40dd8f7fbd814e8ed68dc331b2678af931578601", "_cell_guid": "519d6029-477e-416c-bfa1-da22af09939e"}, "execution_count": null}, {"source": "def ticket_extract(train, test):\n    data = pd.concat([train.drop('Survived',1),test])\n    Survived = train['Survived']\n    data['Ticket'] = data.Ticket.str.replace('.', '')\n    data['Ticket'] = data.Ticket.str.replace('/', '')\n    data['SharedTicket'] = np.where(data.groupby('Ticket')['Fare'].transform('count') > 1, 1, 0)\n\n    data['TicketNumLen'] = data['Ticket'].apply(lambda x: len(str(x).split(' ')[-1])).astype(int)\n    data['Ticket'] = data.Ticket.str.replace(' ', '')\n    data['TicketHead'] = data.Ticket.str.extract('(\\D*)', expand=False)\n    data['TicketHead'] = data['TicketHead'].replace('', 'NULL')\n    data = data.drop('Ticket', axis=1)\n    train = data.iloc[:len(train)]\n    train.loc[:, 'Survived'] = Survived\n    test = data.iloc[len(train):]\n    return  train, test\n\n\n\ntrain_data, test_data = ticket_extract(train_data, test_data)\n\ndata_set = pd.concat([train_data.drop('Survived',1),test_data])\ntemp = data_set.groupby(['TicketHead'])[['Fare']].mean()\ntemp['Survived'] = train_data.groupby(['TicketHead'])[['Survived']].sum()\ntemp['Count'] = data_set.groupby(['TicketHead'])['Fare'].count()\ntemp['SurviveRatio'] = temp['Survived'] / temp['Count']\n\ntemp['Count'].plot(kind='bar', title='Ticket Head:Count')\nplt.show()\ntemp['Fare'].plot(kind='bar', title='Ticket Head:Mean Fare')\nplt.show()\nprint(temp)", "outputs": [], "cell_type": "code", "metadata": {"_execution_state": "idle", "trusted": true, "_uuid": "f2ec5ce8aaec5da24b5fe206088f93dc52fdb946", "_cell_guid": "48de7676-7601-417a-a5ec-992efdd0b46b"}, "execution_count": 19}, {"source": "def transfer_ticket_head(data):\n    data['TicketHead'] = data['TicketHead'].replace('PC', '4')\n    data['TicketHead'] = data['TicketHead'].replace(['FC', 'WEP', 'SOC'], '2')\n    data['TicketHead'] = data['TicketHead'].replace(['CA', 'FCC', 'PPP', 'SC', 'SCAH', 'SCPARIS', 'SCParis', 'WC', 'NULL'], '3')\n    classes = ['A', 'AS', 'AQ','C', 'CASOTON', 'Fa', 'LINE', 'LP','PP', 'SCA', 'SCAHBasle', 'SCOW' ,'SOP', 'SOPP', 'SOTONO', 'SOTONOQ', \n               'SP', 'STONO', 'STONOQ', 'SWPP']\n    data['TicketHead'] = data['TicketHead'].replace(classes, '1')\n    data['TicketHead'] = data['TicketHead'].astype(int)\n    return data\n\ntrain_data = transfer_ticket_head(train_data)\ntest_data = transfer_ticket_head(test_data)\ntemp = pd.crosstab(train_data.TicketHead, train_data.Survived)\ntemp.div(temp.sum(1).astype(float), axis=0).plot(kind='bar', stacked=True, color=['orchid','cornflowerblue'], grid=False, figsize=(8, 5))", "outputs": [], "cell_type": "code", "metadata": {"_execution_state": "idle", "trusted": true, "_uuid": "2d617342cca99ddb71ad9bfd4e9cc47649e7b734", "_cell_guid": "21465a5a-513a-4839-be93-d4045a53409e"}, "execution_count": 20}, {"source": "temp = pd.crosstab(train_data.TicketNumLen, train_data.Survived)\ntemp.plot(kind='bar', color=['orchid','cornflowerblue'], grid=False, figsize=(8, 5))\nplt.show()", "outputs": [], "cell_type": "code", "metadata": {"_execution_state": "idle", "trusted": true, "_uuid": "3226097da9d9bf16d6b808b97cb81adf074f5134", "_cell_guid": "359e4d37-61d5-4e46-8e64-9a12ffb13b53"}, "execution_count": 21}, {"source": "corr = train_data.corr()\nf, ax = plt.subplots(figsize=(25,16))\nsns.plt.yticks(fontsize=18)\nsns.plt.xticks(fontsize=18)\n\nsns.heatmap(corr, cmap='inferno', linewidths=0.1,vmax=1.0, square=True, annot=True)", "outputs": [], "cell_type": "code", "metadata": {"_execution_state": "idle", "trusted": true, "_uuid": "0acc5ff4dffc2de9d43191e5b544d83d26794e13", "_cell_guid": "f0159441-c3b5-4ea2-aa51-704713810b11"}, "execution_count": 22}, {"source": "The good correlation between *Embarked* and *Survived* makes me wonder.  I also find out some better score in column *Embarked* of heap map upper. So, may we can discover the mysterious of *Embarked* in the follow joint graph.", "outputs": [], "cell_type": "markdown", "metadata": {"_execution_state": "idle", "_uuid": "6c211b36e90c4ea1844de1dc6d81c1cd07e57689", "_cell_guid": "0498e062-6988-42fb-b1b3-2fa77d765114"}, "execution_count": null}, {"source": "combine = pd.concat([train_data.drop('Survived', axis=1), test_data])\ncombine['EmbarkedTemp'] = combine['Embarked'].map({0: 'S', 1: 'Q', 2: 'C'})\nplotVars = ['Fare', 'Cabin', 'Pclass']\nsns.set()\nsns.pairplot(combine, vars=plotVars, hue='EmbarkedTemp', kind='reg')\nsns.plt.show()\ndel combine", "outputs": [], "cell_type": "code", "metadata": {"_execution_state": "idle", "trusted": true, "_uuid": "3ad62ca1fba58ca08a6108fcd22e7c922c34aa5e", "_cell_guid": "7a91386b-0634-4362-94e5-cbf1b166029b"}, "execution_count": 23}, {"source": "We can find, the people from 'C' embarked port are in higher fare and better ticket class and better cabin than 'S' and 'Q'.", "outputs": [], "cell_type": "markdown", "metadata": {"_execution_state": "idle", "_uuid": "a1dd71f63614ce487495b01bcac322f15066eb80", "_cell_guid": "2d13c433-9c22-46b7-b8bf-1129aef65239"}, "execution_count": null}, {"source": "plotVars = ['Fare', 'Sex','Age', 'FamliySize', 'Cabin', 'SurnameLen', 'Pclass', 'Title']\nsns.set()\nsns.pairplot(train_data, vars=plotVars, hue='Survived')\nsns.plt.show()", "outputs": [], "cell_type": "code", "metadata": {"_execution_state": "idle", "trusted": true, "_uuid": "ea8e5ed7a00589ae5be31184dc005ea27676090c", "_cell_guid": "2b5f4fa8-9d26-4aab-84a0-4f7c775a622b"}, "execution_count": 24}, {"source": "# Embarked should be transformed to one-hot variable\ntrain_data = dummy(train_data, ['Embarked'])\ntest_data = dummy(test_data, ['Embarked'])", "outputs": [], "cell_type": "code", "metadata": {"_execution_state": "idle", "collapsed": true, "trusted": true, "_uuid": "7149f61ab28633bc015540b156d86e243e9a933f", "_cell_guid": "ca8c498f-f73e-44e3-8c04-cdfda05283f9"}, "execution_count": 25}, {"source": "train_data.info()", "outputs": [], "cell_type": "code", "metadata": {"_execution_state": "idle", "trusted": true, "_uuid": "34cbf5d8046808c127d4f17b742a530a10bf4d81", "_cell_guid": "54e76b3f-90d3-444f-98e4-f9086ff15b9a"}, "execution_count": 26}, {"source": "##Random Forest<br/>\nIn my codes, the RF will be the most important classifier in voting", "outputs": [], "cell_type": "markdown", "metadata": {"_execution_state": "idle", "_uuid": "516b886328f641d604b52af492a0f0ce82d55fa5", "_cell_guid": "6245d256-aac6-42fc-b64d-7748ad074a52"}, "execution_count": null}, {"source": "# search for the best parameters of random forest\ndef parameter_evaluate(data):\n    clf_ev = RandomForestClassifier()\n    x, y = data.drop(['Survived'], axis=1), data['Survived']\n    parameters = {'n_estimators': [100, 300], 'max_features': [3, 4, 5, 'auto'],\n                  'min_samples_leaf': [9, 10, 12], 'random_state': [7]}\n    grid_search = GridSearchCV(estimator=clf_ev, param_grid=parameters, cv=10, scoring='accuracy')\n    print(\"parameters:\")\n    # train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.2, random_state=42)\n    grid_search.fit(x, y)\n    print(\"Best score: %0.3f\" % grid_search.best_score_)\n    print(\"Best parameters set:\")\n    bsp = grid_search.best_estimator_.get_params()  # the dict of parameters with best score\n    for param_name in sorted(bsp.keys()):\n        print(\"\\t%s: %r\" % (param_name, bsp[param_name]))\n    return bsp\n\nparameters = parameter_evaluate(train_data)  \n# we don't need to search everytime after getting best parameters\n# if we haven't change anything, we can use the following parameters to accelebrate our code\n# parameters = {'n_estimators': 100, 'max_features': 5, 'min_samples_leaf': 10, 'random_state': 7}\nrf = RandomForestClassifier(**parameters)\nrf.fit(train_data.drop(['Survived'], axis=1), train_data['Survived'])", "outputs": [], "cell_type": "code", "metadata": {"_execution_state": "idle", "trusted": true, "_uuid": "a178d17956b6132f18df6783e016b4f46a84b27e", "_cell_guid": "b83eb1d8-ee21-4567-a51f-b671e888b2a8"}, "execution_count": 27}, {"source": "names = train_data.drop(['Survived'], axis=1).columns\nratios = rf.feature_importances_\n\nfeature_important = pd.DataFrame(index=names, data=ratios, columns=['importance'])\nfeature_important = feature_important.sort_values(by=['importance'], ascending=True)\nfeature_important.plot(kind='barh', stacked=True, color=['cornflowerblue'], grid=False, figsize=(8, 5))", "outputs": [], "cell_type": "code", "metadata": {"_execution_state": "idle", "trusted": true, "_uuid": "b30c9b7649ba5e31f7e4f655cd2f400d44f0b9be", "_cell_guid": "022f4e3a-69a0-430f-922d-b0f279aa1086"}, "execution_count": 28}, {"source": "Then, I would like to delete some features which will make a bad influence on model. The Basis that I delete these features are **the feature importance** and **the heat map between features** Above. <br/>\nMoreover, if there is a linear relation between feature A and B, remove A or B even it's much important. Just as *SexByAge* and *SexByAgeByPclass*, I got a higher score after removing *SexByAge*. ", "outputs": [], "cell_type": "markdown", "metadata": {"_execution_state": "idle", "_uuid": "99a8ad7f88422f4aae1708ba99c74f20b9d4ebb0", "_cell_guid": "59db0521-54d0-43b1-ab06-b442fde73f27"}, "execution_count": null}, {"source": "##Logistic Regression\nBecause I need to keep the accuracy of RF, so I decide to bypass the process feature scaling.", "outputs": [], "cell_type": "markdown", "metadata": {"_execution_state": "idle", "_uuid": "0beca500deb91d9741509e9f2cde864ea5fe9ef3", "_cell_guid": "37ef4686-cc90-48ca-b252-9c7fa1a95aa6"}, "execution_count": null}, {"source": "lr = LogisticRegression(random_state=7)\nlr.fit(train_data.drop(['Survived'], axis=1), train_data['Survived'])", "outputs": [], "cell_type": "code", "metadata": {"_execution_state": "idle", "trusted": true, "_uuid": "efab8e3dbd29b0077354f9b9939cac032f6c978d", "_cell_guid": "909be1a6-b6d2-4bee-b8e2-135e52be6f36"}, "execution_count": 29}, {"source": "##Feature Selection  \n###Ablative analysis<br/>\nAfter the above feature extractions and analysis, we should select the useful features to complete our work. \nWhile the feature number have influence on Random Forest's predict, we use Decision Tree to do this feature selection.", "outputs": [], "cell_type": "markdown", "metadata": {"_execution_state": "idle", "_uuid": "86bc96c95417c7c000e4c555883bc7e6b5afa0a0", "_cell_guid": "2523fef4-53a8-4f62-bf66-adbc66ca0335"}, "execution_count": null}, {"source": "def k_fold_to_get_acc(clf, x_split, y_split):\n    kf = KFold(len(y_split), n_folds=10)\n    acc_t = 0\n    acc_v = 0\n    for train, valid in kf:\n        x_train, y_train = x_split.iloc[train], y_split.iloc[train]\n        x_valid, y_valid = x_split.iloc[valid], y_split.iloc[valid]\n        \n        clf = clf.fit(x_train, y_train)\n        pred = clf.predict(x_train)\n        acc_t += np.mean(pred==y_train)  # accuracy on train\n        pred = clf.predict(x_valid)\n        acc_v += np.mean(pred==y_valid)  # accuracy on validation\n    return acc_t * 10, acc_v * 10\n\n\ndef ablative_analysis(clf, train, feats):\n    x, y = train.drop('Survived', 1), train[\"Survived\"]\n    acc = {}\n    dropped = []\n    print('all reserved - %%%.2f  %%%.2f' % k_fold_to_get_acc(clf, x, y))\n    while(len(feats) > 0):\n        item = feats[0]\n        feats = feats[1:]\n        dropped.append(item)\n        acc[item] = k_fold_to_get_acc(clf, x.drop(dropped, 1), y)\n    for key in acc:\n        acc_t, acc_v = acc[key]\n        print('del %s -> %%%.2f  %%%.2f' % (key, acc_t, acc_v))\n    print('Remains:')\n    print(list(x.drop(dropped, 1).columns))\n    \nfeatures = list(feature_important.index)[:-6]\ndtc = DecisionTreeClassifier(criterion='gini', random_state=7)\nablative_analysis(dtc, train_data, features)", "outputs": [], "cell_type": "code", "metadata": {"_execution_state": "idle", "trusted": true, "_uuid": "9f2eea6ce42472cd9c14426471eeb68b5cf65ca6", "_cell_guid": "98be62dc-5f9a-4874-a4fb-535976acbde4"}, "execution_count": 30}, {"source": "We can find, keep all features we extract will lead to overfitting.<br/>\nThere were some superfluous feature in our model. <br/>\nWe should delete the features *Parch* and *SibSp*because they have linear relation with other features.<br/>\nAnd delete *TicketHead* and *SurnameLen* because of its lower importance.\nBesides, the features I used finally, were tested by submiting. As a freshman to Machine Learning, I haven't formed my own way to select features effectively.<br/>\nI will keep on studying to refresh this notebook! If you have better method on feature selection, please let me know,I'd appreciate it very much!", "outputs": [], "cell_type": "markdown", "metadata": {"_execution_state": "idle", "_uuid": "b78678c3c4f89e4599207e7e570c6e4cf6ce6857", "_cell_guid": "50ccfe1a-7c5e-490b-a3de-7863af8da0ad"}, "execution_count": null}, {"source": "##K Neighbors", "outputs": [], "cell_type": "markdown", "metadata": {"_execution_state": "idle", "_uuid": "4427dcd05332ad050cd5e03bbcf046c4a7473d89", "_cell_guid": "baeef155-34e0-409c-b206-572285e541af"}, "execution_count": null}, {"source": "knn = KNeighborsClassifier()\nknn.fit(train_data.drop(['Survived'], axis=1), train_data['Survived'])", "outputs": [], "cell_type": "code", "metadata": {"_execution_state": "idle", "trusted": true, "_uuid": "aee7309c1255ebda4a34f397993aa94c9be5ba91", "_cell_guid": "0af948a6-0ad2-42fc-b1d0-d3b404e9d94b"}, "execution_count": 31}, {"source": "##Ensemble RF, LR, KNN by voting", "outputs": [], "cell_type": "markdown", "metadata": {"_execution_state": "idle", "_uuid": "fd77afac581db626b64351151a68fe5dccacd942", "_cell_guid": "562bcb36-8026-46ad-8a5b-6bb4499ea7b0"}, "execution_count": null}, {"source": "eclf1 = VotingClassifier(estimators=[\n        ('lr', lr), ('rf', rf), ('knn', knn)], voting='soft', weights=[1, 2, 1])\neclf1 = eclf1.fit(train_data.drop(['Survived'], axis=1), train_data['Survived'])\nresults = eclf1.predict(test_data)\noutput = pd.DataFrame({'PassengerId': id_list, \"Survived\": results})\noutput.to_csv('prediction.csv', index=False)", "outputs": [], "cell_type": "code", "metadata": {"_execution_state": "idle", "collapsed": true, "trusted": true, "_uuid": "8d0cf0af5116fb98eade55d1711ee086c79f78db", "_cell_guid": "6e54ae43-de73-4825-bd45-c547bc2cb6a1"}, "execution_count": 32}, {"source": "", "outputs": [], "cell_type": "markdown", "metadata": {"_execution_state": "idle", "_uuid": "a57f1cd28c258c79a87edca987e4f670d0232614", "_cell_guid": "29db08ee-fc2e-4d10-b798-4cb2b3a4cd4f"}, "execution_count": null}, {"source": "", "outputs": [], "cell_type": "code", "metadata": {"_execution_state": "idle", "collapsed": true, "trusted": true, "_uuid": "137ebcad29cd1490adc4168afdc264222208d301", "_cell_guid": "1f47154d-948d-4d14-9a61-39422e30e3e5"}, "execution_count": null}], "metadata": {"language_info": {"pygments_lexer": "ipython3", "nbconvert_exporter": "python", "version": "3.6.1", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}, "name": "python", "mimetype": "text/x-python"}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}}