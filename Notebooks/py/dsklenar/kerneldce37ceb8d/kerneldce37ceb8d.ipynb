{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\ntrain_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')\n\n# First, let's look at the first few rows. There are 12 columns.\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Let's look at missing data. Cabin (687), Age (177) are the most missing values. Embarked has 2 missing values.\ntrain_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"020caa55137d1b24f5dec5e0b20b5202004b4b7b"},"cell_type":"code","source":"# Next, let's print some basic statistics about each available feature.\ntrain_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d6bb42ca15efaac31d7255d6cb8ad47e7e04d1c6"},"cell_type":"code","source":"# PassengerId is sequential and has no correlation with survival. Let's prove that here.\nimport matplotlib.pyplot as plt \n\nimport seaborn as sns\nsns.boxplot(x=\"Survived\", y=\"PassengerId\", data=train_df)\n\n# The distribution looks the same, so we can drop PassengerId\n\ntrain_df = train_df.drop('PassengerId', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d0f480c616bef1e602ef40eaea24215dab9f148a"},"cell_type":"code","source":"# Continue with the univariate analysis.\n\n# 1. Age\n\nage_plot = sns.FacetGrid(train_df, col='Survived')\nage_plot.map(plt.hist, 'Age', bins=25)\n\n# Visually, it looks like there are distinct groups of ages, this might be a good candidate for categorization\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc5a5646fca880716a0f9dc0548d34a97a234eed"},"cell_type":"code","source":"# Embarked -- not sure what to think of this, does it have importance? Let's visualize it first\nsns.barplot(x=\"Embarked\", y=\"Survived\", data=train_df)\n\n# It looks like there's a significant difference.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4c4f43263d2938117398f219eaec460c31066c3b"},"cell_type":"code","source":"# Let's look at the fare.\n# sns.boxplot(x=\"Survived\", y=\"Fare\", data=train_df)\n\n# There are significant outliers that skew that graph, but it appears that increased fare increased chances of survival.\n# Lets visualize the data with outliers removed\n\nsns.boxplot(x=\"Survived\", y=\"Fare\", data=train_df[train_df.Fare < 150])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d59e1c7896c5df39880c7e41c55f16a1ce04e69e"},"cell_type":"code","source":"# Next, visualize the Pclass.\nsns.barplot(x=\"Pclass\", y=\"Survived\", data=train_df)\n\n# First class users are clearly more likely to survive.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b804c3feb69b05a872a44f3f45d9921b0d1e0702"},"cell_type":"code","source":"# Look at sibling and spouse data.\nsns.barplot(x=\"SibSp\", y=\"Survived\", data=train_df)\n\n# Wow! those with 5 or 8 siblings have no chance of survival. Those with 1 or 2 have a higher mean probability","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"685f4f57b2fa9ed261641cec0c6567287bd96f11"},"cell_type":"code","source":"# Look at parent or child data\nsns.barplot(x=\"Parch\", y=\"Survived\", data=train_df)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc19f79ee7a40439ab7768046865c59772ee7d6b"},"cell_type":"code","source":"# That completes the univariate analysis. Now lets look at bivariate to see which variables look correlated.\n\n# Naively drop rows with missing data so we can visualize. Also drop Cabin, since it has the most missing data. We will revisit that.\ndropped_train_df = train_df.drop('Cabin', axis=1).dropna()\ng = sns.pairplot(dropped_train_df, hue='Survived')\n\n# Whoa, that's a lot of information. We can refer to this ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e12a561b3b9bc7c9d384e4d4528bd7ce2f755852"},"cell_type":"code","source":"# We want to imput the missing age values intelligently. Let's look at the correlation matrix and see which parameters will be good at estimating\n# the missing age values.\n\n# Drop survived because it shouldn't be used to estimate the training data.\ncorr = dropped_train_df.drop('Survived', axis=1).corr()\n\nprint(corr)\n\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# The following was taken from the Seaborn docs:\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n\n\n# Age has the highest correlation with Pclass, Sibsp and Parch, so we'll build a model to fill that way.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d13187fba62ad493703d65cb99fd8fe7695b4101"},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nlinear_regression = LinearRegression()\nlinear_reg_df = dropped_train_df.drop('Survived', axis=1)\n\nage = linear_reg_df.Age.values.reshape(-1,1)\nage_input_df = linear_reg_df[['Pclass', 'SibSp', 'Parch']].values.reshape(-1, 3)                  \n\nlinear_regression.fit(age_input_df, age)\n\n# Show summary statistics for the model:\nprint('Coefficients: \\n', linear_regression.coef_)\nprint('Intercept: \\n', linear_regression.intercept_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a095272ad69dd428961974a1f558dafe0046fac9"},"cell_type":"code","source":"# We have our age, let's fill missing data.\ntrain_df['Age'] = train_df.apply(\n    lambda row: \n            -6.01730031 * row.Pclass + -3.93983573 * row.SibSp + 1.40296411 * row.Parch + 45.75401239\n            if np.isnan(row.Age) else row.Age, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3dc34c7be565582433091cf6ce0285370bae53cb"},"cell_type":"code","source":"train_df.isnull().sum() \n\n# We have age data now for all passengers!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"94e7b01500a3250df47d07004ef0ebe608ddc599"},"cell_type":"code","source":"# Lets look at the missing embarked data:\n\ntrain_df[pd.isna(train_df['Embarked'])]\n\nembarked_dropped_train_df = train_df.drop(['Cabin', 'SibSp', 'Parch', 'Ticket', 'Survived'], axis=1).dropna()\ng = sns.pairplot(embarked_dropped_train_df, hue='Embarked')\n\n# Hard to see a clear trend, so we'll drop Embarked.\n\n# Also, I'll drop the Cabin data for now, since it's hard to make anything out of the different values.\ntrain_df = train_df.drop('Cabin', axis=1).dropna()\ntrain_df.isnull().sum()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"25ea733b0083f3fcc16ae22d520ea9cef31e530e"},"cell_type":"code","source":"# We have our training data set! Let's build a logistic model.\n\n# Still need to convert the class data (Pclass and Embarked) into one-hot encoded data.\nfinal_training_df = pd.get_dummies(train_df, columns=[\"Sex\", \"Embarked\", \"Pclass\"]).drop(['Ticket', 'Name'], axis=1)\n\nfinal_training_df.head()\n# Time to build the Logistic Regression\n\ny_train = final_training_df.Survived.values.reshape(-1,1)\nX_train = final_training_df.drop('Survived', axis=1).values.reshape(-1, 12)   \n\nfrom sklearn.linear_model import LogisticRegression\n\nlogistic_regression = LogisticRegression()\nlogistic_regression.fit(X_train, y_train)\n\n# Show summary statistics for the model:\nprint('Coefficients: \\n', logistic_regression.coef_)\nprint('Intercept: \\n', logistic_regression.intercept_)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"347caf6721b35c03783508c9e54075d8ff5d0e97"},"cell_type":"code","source":"# Let's evaluate the accuracy. Since we don't have the test data labels, look at the training data.\n\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\ny_pred = logistic_regression.predict(X_train)\n\nprint(confusion_matrix(y_train, y_pred))  \nprint(classification_report(y_train, y_pred))\nprint(accuracy_score(y_train, y_pred))\n\n# well, we defintely didn't overfit the data :-)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aba5cf7f2b51eca03875686c9f5a8b8d90551bf0"},"cell_type":"code","source":"# Let's perform the same transform on the test data.\n\ntest_df.describe()\ntest_df.isnull().sum()\n\ndropped_test_df = test_df.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1) # Drop unused features\ndropped_test_df['Age'] = dropped_test_df.apply(\n    lambda row: \n            -6.01730031 * row.Pclass + -3.93983573 * row.SibSp + 1.40296411 * row.Parch + 45.75401239\n            if np.isnan(row.Age) else row.Age, axis=1)\nfinal_training_df = pd.get_dummies(dropped_test_df, columns=[\"Sex\", \"Embarked\", \"Pclass\"])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"06a9eb593d2f023eb81f9d8c0c70d55b423e4fe0"},"cell_type":"code","source":"# One Fare value is still missing. Impute with the average.\n\nfinal_training_df = final_training_df.fillna(final_training_df.mean())\nfinal_training_df.isnull().sum()\n\nsubmission = logistic_regression.predict(final_training_df)\nsubmission_df = pd.DataFrame({'PassengerId': test_df['PassengerId'],'Survived': submission})\nsubmission_df.to_csv('Titanic Predictions 1.csv',index=False)\n\n# This scores a 77% on the test data -- let's see if we can do better. We'll still use the same training data, but add\n# the name title trick that was shown in class.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6c94a23e609dcaf68ba0e11490dbb0d4f91e10c3"},"cell_type":"code","source":"# Output the unique title names.\ntrain_df['Title'] = train_df['Name']\ntrain_df['Title'] = train_df['Title'].str.extract('([A-Za-z]+)\\.', expand=True)\ntrain_df.Title.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"baa507c0602feb93bbfcb3d790de49237bca850d"},"cell_type":"code","source":"# Since gender is captured in a separate feature, we can combine these.\n# replace rare titles with more common ones\nmapping = {\n    'Mlle': 'Miss', \n    'Major': 'Military', \n    'Col': 'Military', \n    'Sir': 'Honorific',\n    'Don': 'Honorific', \n    'Mme': 'Miss', \n    'Jonkheer': 'Honorific', \n    'Lady': 'Honorific',\n    'Capt': 'Military', \n    'Countess': 'Honorific', \n    'Ms': 'Miss', \n    'Dona': 'Mrs',\n    'Master': 'Professional',\n    'Dr': 'Professional',\n    'Rev': 'Professional'\n}\n\ntrain_df.replace({'Title': mapping}, inplace=True)\ntrain_df.Title.value_counts()\n\n# Great! we reduced it down to six classes: Mr, Miss, Mrs, Professional, Military, Honorific","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"92dbe03c53b1fecb164bee1b491f0e8f294c9ce0"},"cell_type":"code","source":"# Let's see how much that improved our model.\nfinal_training_df = pd.get_dummies(train_df, columns=[\"Sex\", \"Embarked\", \"Pclass\", \"Title\"]).drop(['Ticket', 'Name'], axis=1)\n\nfinal_training_df.head()\nfinal_training_df.columns\n\ny_train = final_training_df.Survived.values.reshape(-1,1)\nX_train = final_training_df.drop('Survived', axis=1).values.reshape(-1, 18)   \n\nfrom sklearn.linear_model import LogisticRegression\n\nlogistic_regression = LogisticRegression()\nlogistic_regression.fit(X_train, y_train)\n\n# Show summary statistics for the model:\nprint('Coefficients: \\n', logistic_regression.coef_)\nprint('Intercept: \\n', logistic_regression.intercept_)\n\ny_pred = logistic_regression.predict(X_train)\n\nprint(confusion_matrix(y_train, y_pred))  \nprint(classification_report(y_train, y_pred))\nprint(accuracy_score(y_train, y_pred)) \n\n# 2% increase on our training set up to 82%!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4c8f4bbb14391dea0ae0e1a12001ef91296ee5ab"},"cell_type":"code","source":"# Submit this to see if it improves our test set.\nupdated_test_data = test_df\nupdated_test_data['Title'] = test_df['Name']\nupdated_test_data['Title'] = updated_test_data['Title'].str.extract('([A-Za-z]+)\\.', expand=True)\nupdated_test_data.replace({'Title': mapping}, inplace=True)\n\nupdated_test_data.head()\n\npassenger_ids = updated_test_data['PassengerId']\ndropped_test_df = updated_test_data.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1) # Drop unused features\ndropped_test_df['Age'] = dropped_test_df.apply(\n    lambda row: \n            -6.01730031 * row.Pclass + -3.93983573 * row.SibSp + 1.40296411 * row.Parch + 45.75401239\n            if np.isnan(row.Age) else row.Age, axis=1)\nfinal_test_data = pd.get_dummies(dropped_test_df, columns=[\"Sex\", \"Embarked\", \"Pclass\", \"Title\"])\nfinal_test_data = final_test_data.fillna(final_test_data.mean())\nfinal_test_data['Title_Honorific'] = 0\n\nsubmission_classes = logistic_regression.predict(final_test_data)\nsubmission_df = pd.DataFrame({'PassengerId': passenger_ids,'Survived': submission_classes})\nsubmission_df.to_csv('Titanic Predictions 2.csv',index=False)\n\n# That made our submission worse (down to 70.7%)! We'll drop that feature on the next iteration","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fc8e48cb63a38d79ab82ef20280b024067f875c5"},"cell_type":"code","source":"# Let's try one more thing, using categorical age data, rather than the the age alone.\n\n# Since age is a continuous variable, there won't be any age clustering, so let's start by\n# arbitrarily creating age categories: \n\n# - Child (0-12)\n# - Teen (13-18)\n# - Young Adult (19-30)\n# - Adult (31-65)\n# - Senior (> 65)\n\ndef age_category(age):\n    if age < 12:\n        cat='child'\n    elif age < 19:\n        cat='teen'\n    elif age < 31:\n        cat='young adult'\n    elif age < 66:\n        cat='adult'\n    else:\n        cat='senior'\n    return cat\n\nage_categories = train_df.Age.apply(age_category)\nsns.countplot(age_categories) # The age category histogram looks correct.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ddedd5d149e5f9ae106cfdb6656ec2162046ca36"},"cell_type":"code","source":"# Training the new model.\n\nage_categories_train_df = train_df\nage_categories_train_df['Age Category'] = age_categories\n\n# # Still need to convert the categorical data (Sex, Pclass and Embarked and Age Category) into one-hot encoded data.\nfinal_training_df = pd.get_dummies(age_categories_train_df, columns=[\"Sex\", \"Embarked\", \"Pclass\", \"Age Category\"]).drop(['Ticket', 'Title', 'Name', 'Age'], axis=1)\nfinal_training_df.head()\n\n# Time to build the Logistic Regression\n\ny_train = final_training_df.Survived.values.reshape(-1,1)\nX_train = final_training_df.drop('Survived', axis=1).values.reshape(-1, 16)   \n\nlogistic_regression = LogisticRegression()\nlogistic_regression.fit(X_train, y_train)\n\n# Show summary statistics for the model:\nprint('Coefficients: \\n', logistic_regression.coef_)\nprint('Intercept: \\n', logistic_regression.intercept_)\n\ny_pred = logistic_regression.predict(X_train)\n\nprint(confusion_matrix(y_train, y_pred))  \nprint(classification_report(y_train, y_pred))\nprint(accuracy_score(y_train, y_pred)) # 81.2% accuracy on the training set, that seems to be where we plateau.\n\n# Predict the survival, and save the output.\n\nupdated_test_data = test_df\npassenger_ids = updated_test_data['PassengerId']\n\ndropped_test_df = updated_test_data.drop(['PassengerId', 'Name', 'Ticket', 'Cabin', 'Title'], axis=1) # Drop unused features\ndropped_test_df['Age'] = dropped_test_df.apply(\n    lambda row: \n            -6.01730031 * row.Pclass + -3.93983573 * row.SibSp + 1.40296411 * row.Parch + 45.75401239\n            if np.isnan(row.Age) else row.Age, axis=1)\n\ntest_age_categories = train_df.Age.apply(age_category)\n\ndropped_test_df['Age Category'] = test_age_categories\n\nfinal_test_data = pd.get_dummies(dropped_test_df, columns=[\"Sex\", \"Embarked\", \"Pclass\", \"Age Category\"])\n\nfinal_test_data = final_test_data.drop('Age', axis=1)\nfinal_test_data = final_test_data.fillna(final_test_data.mean()) # Fill Missing Fare data\n\nsubmission_classes = logistic_regression.predict(final_test_data)\nsubmission_df = pd.DataFrame({'PassengerId': passenger_ids,'Survived': submission_classes})\nsubmission_df.to_csv('Titanic Predictions 3.csv',index=False)\n\n# Sweet, that got us to 78.95% -- in the top 32%!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b68e419d2fd7dd6e217fa92d6bf37fdf862efadb"},"cell_type":"code","source":"# Let's see what other sklearn ML models we can use. Hany keeps mention SVCs, so let's give those a try.\n\nfrom sklearn.svm import SVC\n\nsvc = SVC(gamma='auto') # Use the default values, since I'm not sure how this works :-)\nsvc.fit(X_train, y_train) \n\nsvc_predictions = svc.predict(final_test_data)\nsvc_submission_df = pd.DataFrame({'PassengerId': passenger_ids,'Survived': svc_predictions})\nsvc_submission_df.to_csv('Titanic Predictions 4.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d853cf3d39b127dd902f442f9a97973ad481be21"},"cell_type":"code","source":"# Final notes:\n\n# - Now that I'm at the end, I realize that I should have used k-fold cross validation in order to prevent overfitting of the data\n#   and returning an accurate assessment of the accuracy of my various logistic models.\n# - The Pipeline functionality in sklearn seems really nice for not repeating a lot of code\n# - SVC has a lot of hyperparamets that could be tuned.\n\n# I am pretty happy with top 32% for my first attempt!","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"06fca76c26781a89812890dda79c196c39329c25"},"cell_type":"code","source":"# Bonus round: let's try deep learning.\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.utils import plot_model\nfrom keras import regularizers\nfrom keras.wrappers.scikit_learn import KerasClassifier\n\ndef create_model(dropout_rate = 0.2, number_of_units = 16, regularization_rate = 0.001):\n    classifier = Sequential()\n\n    classifier.add(Dense(units = number_of_units, kernel_initializer = 'uniform', activation = 'relu', input_dim = 16, kernel_regularizer=regularizers.l2(regularization_rate)))\n    classifier.add(Dropout(rate = dropout_rate))\n    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n\n    classifier.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    classifier.summary()\n    return classifier\n\nclassifier = create_model()    \nhistory = classifier.fit(X_train, y_train, batch_size = 300, epochs = 1000)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a2743a96097b4f27063d8b5c9487e5ac99c70a5e"},"cell_type":"code","source":"sns.tsplot(history.history['acc']) # Seems like we can keep training....","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b01ec218174598d38c85e92a7800f97b2c16b09b"},"cell_type":"code","source":"# Let's try a grid search for hyperparameters.\n\nfrom sklearn.model_selection import GridSearchCV\n\ndef run_grid_search():\n    model = KerasClassifier(build_fn=create_model)\n\n    number_of_units = [14, 16]\n    batch_size = [10, 100]\n    epochs = [300, 400]\n    dropout_rate = [0.15]\n    regularization_rate = [0.001]\n\n    param_grid = dict(batch_size=batch_size, epochs=epochs, dropout_rate=dropout_rate, number_of_units=number_of_units, regularization_rate=regularization_rate)\n    grid = GridSearchCV(estimator=model, param_grid=param_grid)\n    grid_result = grid.fit(X_train, y_train)\n\n    # Summarize results\n    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n\n    means = grid_result.cv_results_['mean_test_score']\n    stds = grid_result.cv_results_['std_test_score']\n    params = grid_result.cv_results_['params']\n\n    for mean, stdev, param in zip(means, stds, params):\n        print(\"%f (%f) with: %r\" % (mean, stdev, param))\n\n# run_grid_search()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e475399c91f832feb2a8e2f8ce2f662a672f1e0a"},"cell_type":"code","source":"nn_predictions = classifier.predict(final_test_data)\nnn_predictions = nn_predictions.round().astype(int)\n\nnn_predictions = nn_predictions.reshape(-1)\n\n\nprint(nn_predictions)\n\nnn_submission_df = pd.DataFrame({'PassengerId': passenger_ids,'Survived': nn_predictions})\nnn_submission_df.to_csv('Titanic Predictions 5.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"972d22c4274c377fda19315f894b5e51da790ff6"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}