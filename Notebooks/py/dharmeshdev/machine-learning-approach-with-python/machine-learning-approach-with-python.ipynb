{"cells":[{"metadata":{"_uuid":"4d320788a7e585182f327097b15624da411f277e"},"cell_type":"markdown","source":"# Machine Learning Approach with Python\n\nThis notebook covers the basic Machine Learning process in Python step-by-step.\n\n## **Table of Contents:**\n* Introduction\n* Breif History of RMS Titanic\n* Import Libraries\n* Loading the Data\n* Data Exploration / Analysis\n* Visualizing Data\n* Data Preprocessing\n    - Missing Data\n    - Feature Engineering\n    - Converting Features\n* Building Machine Learning Models\n    - Splitting up the training & test data.\n    - Training 10 different models\n    - Which is the best model ?\n    - K-Fold Cross Validation \n    - Hyperparameter Tuning    \n    - Confusion Matrix\n* Submission\n* Summary"},{"metadata":{"_uuid":"e110169461a32e95ea57ca7b3f35a787f1d84cf1"},"cell_type":"markdown","source":"## **Introduction**\n\nIn this kernel, I will go through the whole process of creating several machine learning models on the famous Titanic dataset, which is used by many people as beginner guide for getting started with Data Science / Machine Learning. It provides information on the fate of passengers on the Titanic, summarized according to economic status (class), sex, age, and survival. In this challenge, we are asked to predict whether a passenger on the Titanic would have been survived or not. Let us go through step by step process to match our prediction with the actual result."},{"metadata":{"_uuid":"c6471864743e537c6343ccdeaeceff82004f6ec3"},"cell_type":"markdown","source":"## **Breif History of RMS Titanic**\n\nRMS Titanic was a British passenger liner that sank in the North Atlantic Ocean in the early morning hours of 15 April 1912, after it collided with an iceberg during its maiden voyage from Southampton to New York City. There were an estimated 2,224 passengers and crew aboard the ship, and more than 1,500 died, making it one of the deadliest commercial peacetime maritime disasters in modern history. The RMS Titanic was the largest ship afloat at the time it entered service and was the second of three Olympic-class ocean liners operated by the White Star Line. The Titanic was built by the Harland and Wolff shipyard in Belfast. Thomas Andrews, her architect, died in the disaster."},{"metadata":{"_uuid":"b71d0cda58f07553b765077ff8e18a9f35c32249"},"cell_type":"markdown","source":"## **Import Libraries**\n*Note: As this is a step by step tutorial we will import the model based libraries later in the tutorial*"},{"metadata":{"trusted":true,"_uuid":"46296c3718e5ac5123737dcce18c28ec73282793"},"cell_type":"code","source":"# linear algebra\nimport numpy as np \n\n# data processing\nimport pandas as pd \n\n# data visualization\nimport seaborn as sns\nsns.set(font_scale=1) # default settings for cleaner graphs\n\n# another data visualization\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a3d2280822e7c3e2084f9c0913a7e181c9620f4e"},"cell_type":"markdown","source":"## **Loading the Data**"},{"metadata":{"_uuid":"27dd6ee702dd59339a6aea801e1a3f37462a6f93"},"cell_type":"markdown","source":"#### Load the train & test data"},{"metadata":{"trusted":true,"_uuid":"351c8e7343f9d75a79b54e58c8fd6e260b51ed98"},"cell_type":"code","source":"# This creates a pandas dataframe and assigns it to the train variable.\ntrain_df = pd.read_csv(\"../input/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"734f468296b31b2696eb6d5f4daf67a35e2a8d1a"},"cell_type":"code","source":"# This creates a pandas dataframe and assigns it to the test variable.\ntest_df = pd.read_csv(\"../input/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aa2e7f9433d085dd5ed8336c69f1ecaf2a80b5a1"},"cell_type":"markdown","source":"## **Data Exploration/Analysis**"},{"metadata":{"trusted":true,"_uuid":"10a09851cafc852c68d88c39f7397c933874d6a0"},"cell_type":"code","source":"# Print the first 5 rows of the train dataframe.\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aadc791f1c92da92c30b73931a7b780fbd2785fc"},"cell_type":"code","source":"# Print the first 5 rows of the test dataframe.\n# note their is no Survived column here which is our target varible we are trying to predict\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aa2b13e60eecf1e4e087a50a6ba8add724880007"},"cell_type":"code","source":"# lets print data info\ntrain_df.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8a1238ddf8ad10ae713fedc8dba9683cb41c7dbb"},"cell_type":"markdown","source":"_**From the above information we can say that the training-set has a total of 891 examples and 11 features + the target variable (survived). 2 of the features are floats (Age, Fare), 5 are integers(PassengerId, Survived, Pclass, SibSp, Parch) and 5 are objects(Name, Sex, Ticket, Cabin, Embarked). Below I have listed the features with a short description:**_\n\n\n    survival:\tSurvival status (0 - Not Survived, 1 - Survived)\n    PassengerId: Unique Id of a passenger.\n    pclass:\tTicket class\t\n    sex:\tSex\t\n    Age:\tAge in years\t\n    sibsp:\tNumber of siblings / spouses aboard the Titanic\t\n    parch:\tNumber of parents / children aboard the Titanic\t\n    ticket:\tTicket number\t\n    fare:\tPassenger fare\t\n    cabin:\tCabin number\t\n    embarked:\tPort of Embarkation\n"},{"metadata":{"trusted":true,"_uuid":"06dc6dfc8f06415114da4312463ae7a6658b1ef8"},"cell_type":"code","source":"train_df.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ae8135259065577a8497191e6138f6bc432bf6ae"},"cell_type":"markdown","source":"Above we can see that **38% of the training-set survived the Titanic**. Age and Fare are measured on very different scaling, So we need to do feature scaling before predictions.  On top of that, we can already detect some features, that contain missing values, like the **'Age'** feature."},{"metadata":{"_uuid":"f8ef02523ee553c59202063578bae0ab374b5fd3"},"cell_type":"markdown","source":"## **Visualizing Data**\n\n*Let's compare certain features with survival to see which one **correlates** better. Also Visualizing data is crucial for recognizing underlying patterns to exploit in the model.*"},{"metadata":{"trusted":true,"_uuid":"665483e3757c31a6ca969f7bc4ae91dfb86421a5"},"cell_type":"code","source":"sns.barplot(x=\"Embarked\", y=\"Survived\", hue=\"Sex\", data=train_df);\n# From below we can see that Male/Female that Emarked from C have higher chances of survival \n# compared to other Embarked points.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"251912237a03c22c287b8ed5256c5e339ebaffaa"},"cell_type":"code","source":"sns.pointplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=train_df,\n              palette={\"male\": \"blue\", \"female\": \"pink\"},\n              markers=[\"*\", \"o\"], linestyles=[\"-\", \"--\"]);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"476be7529a18c216cf68855ec1a5092f2f5ad653"},"cell_type":"code","source":"g = sns.FacetGrid(train_df, col=\"Sex\", row=\"Survived\", margin_titles=True)\ng.map(plt.hist, \"Age\",color=\"purple\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6834a69544a48cb0978ea23b883678857012fc8d"},"cell_type":"code","source":"corr=train_df.corr()#[\"Survived\"]\nplt.figure(figsize=(10, 10))\n\nsns.heatmap(corr, vmax=.8, linewidths=0.01,\n            square=True,annot=True,cmap='YlGnBu',linecolor=\"white\")\nplt.title('Correlation between features');","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b4d78c4b391f1064bb8ed9728914e57742e87812"},"cell_type":"markdown","source":"## **Data Preprocessing**\n\n### **Dealing with missing values**\n\n**It's important to fill missing values because some machine learning algorithms can't accept them eg SVM.**\n\n*On the contrary, filling missing values with the mean/median/mode is also a prediction which may not be 100% accurate, instead, you can use models like Decision Trees and Random Forest which handles missing values very well.*"},{"metadata":{"trusted":true,"_uuid":"a87d2f6d16eb10aa51fdc2cec234a22ccf98cc51"},"cell_type":"code","source":"#lets see which are the columns with missing values in train dataset\ntrain_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f038eaee8d6f5630666e8e6be784310701d61590"},"cell_type":"code","source":"labels = []\nvalues = []\nnull_columns = train_df.columns[train_df.isnull().any()]\nfor col in null_columns:\n    labels.append(col)\n    values.append(train_df[col].isnull().sum())\n\nind = np.arange(len(labels))\nwidth=0.6\nfig, ax = plt.subplots(figsize=(6,5))\nrects = ax.barh(ind, np.array(values), color='purple')\nax.set_yticks(ind+((width)/2.))\nax.set_yticklabels(labels, rotation='horizontal')\nax.set_xlabel(\"Count of missing values\")\nax.set_ylabel(\"Column Names\")\nax.set_title(\"Columns with missing values in train dataset\");","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cc2366a8235f4677afe21cd2ace5d2fd67c10fd3"},"cell_type":"code","source":"#lets see which are the columns with missing values in test dataset\ntest_df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1f341fa4f3e986c7fdea8cb8564c3baef7f2a2aa"},"cell_type":"code","source":"labels = []\nvalues = []\nnull_columns = test_df.columns[test_df.isnull().any()]\nfor col in null_columns:\n    labels.append(col)\n    values.append(test_df[col].isnull().sum())\n\nind = np.arange(len(labels))\nwidth=0.6\nfig, ax = plt.subplots(figsize=(6,5))\nrects = ax.barh(ind, np.array(values), color='purple')\nax.set_yticks(ind+((width)/2.))\nax.set_yticklabels(labels, rotation='horizontal')\nax.set_xlabel(\"Count of missing values\")\nax.set_ylabel(\"Column Names\")\nax.set_title(\"Columns with missing values in test dataset\");","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"da816028cfc3a87c7c5ffea4124eb1c2a7057c05"},"cell_type":"markdown","source":"### Fill Missing Values in Embarked Column (train dataset)"},{"metadata":{"trusted":true,"_uuid":"a98f90e451dbd263dfcfa540689d9c31b425ed7e"},"cell_type":"code","source":"#Lets check which rows have null Embarked column\ntrain_df[train_df['Embarked'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3da454c43fb80fd0e8748c402b74fe233fbf3ba3"},"cell_type":"markdown","source":"**PassengerId 62 and 830** have missing embarked values\n\nBoth have ***Passenger class 1*** , ***fare $80.*** , ***Ticket - 113572*** and ***Cabin - B28***\n\nLets plot a graph to visualize and try to guess from where they embarked"},{"metadata":{"trusted":true,"_uuid":"687b97fd28476af316ed7ba35259ed173544ba5b"},"cell_type":"code","source":"from numpy import median\nsns.barplot(x=\"Embarked\", y=\"Fare\", hue=\"Pclass\", data=train_df, estimator=median)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9ffe69d8a1fe5a4ba1662d16a4d8877ffc75c523"},"cell_type":"markdown","source":"We can see that for ***1st class*** median line is coming around ***fare $80*** for ***embarked*** value ***'C'***.\nSo we can replace NA values in Embarked column with 'C'"},{"metadata":{"trusted":true,"_uuid":"c8cbadb72a5572b67567fb3a26f5944a1e57fd64"},"cell_type":"code","source":"train_df[\"Embarked\"] = train_df[\"Embarked\"].fillna('C')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7ae0d23a572fa4c1a2563f52124af2723446675a"},"cell_type":"markdown","source":"### Fill Missing Values in Fare Column (test dataset)"},{"metadata":{"trusted":true,"_uuid":"5b0cb060f3f9c5c0fa989a1de3061c8af6ad1fdd"},"cell_type":"code","source":"#Lets check which rows have null Fare column in test dataset\ntest_df[test_df['Fare'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eb5e98b26bef7744ed023bbc0dbaf4a7af6b86f8"},"cell_type":"code","source":"# we can replace missing value in fare by taking median of all fares of those passengers\n# who share 3rd Passenger class and Embarked from 'S' , so lets find out those rows\ntest_df[(test_df['Pclass'] == 3) & (test_df['Embarked'] == 'S')].head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6aa63c90f383481f7d656c0f99f2e44623de3533"},"cell_type":"code","source":"# now lets find the median of fare for those passengers.\n\ndef fill_missing_fare(df):\n    median_fare = df[(df['Pclass'] == 3) & (df['Embarked'] == 'S')]['Fare'].median()\n    df[\"Fare\"] = df[\"Fare\"].fillna(median_fare)\n    return df\n\ntest_df = fill_missing_fare(test_df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bfe8317fc2ba3554a35b1d4fa73ebb5bd179d454"},"cell_type":"markdown","source":"### Fill Missing Values in Age Column (train & test dataset)\n\n**When dealing with large missing values in a particular column, simply removing the feature does not make sense when the feature is relevant in prediction.**\n\nAge seems to be a promising feature.\nSo it doesn't make sense to simply fill null values out with a median/mean/mode.\n\nWe will use ***Random Forest*** algorithm to predict ages. "},{"metadata":{"trusted":true,"_uuid":"444f221b0a77e0dd6d6321f545527e0066df862f"},"cell_type":"code","source":"# age distribution in train dataset\nwith sns.plotting_context(\"notebook\",font_scale=1.5):\n    sns.set_style(\"whitegrid\")\n    sns.distplot(train_df[\"Age\"].dropna(),\n                 bins=80,\n                 kde=False,\n                 color=\"red\")\n    plt.title(\"Age Distribution\")\n    plt.ylabel(\"Count\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"30b0ceee809e29309e9b35167f94a6e0887524b1"},"cell_type":"code","source":"# age distribution in test dataset\nwith sns.plotting_context(\"notebook\",font_scale=1.5):\n    sns.set_style(\"whitegrid\")\n    sns.distplot(test_df[\"Age\"].dropna(),\n                 bins=80,\n                 kde=False,\n                 color=\"red\")\n    plt.title(\"Age Distribution\")\n    plt.ylabel(\"Count\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"58c8eedc636bda23ef08734fb28c6481508d3a28"},"cell_type":"code","source":"# predicting missing values in age using Random Forest\n# import the RandomForestRegressor Object\nfrom sklearn.ensemble import RandomForestRegressor\n\ndef fill_missing_age(df):\n    \n    #Feature set\n    age_df = df[['Age','Pclass','SibSp','Parch','Fare']]\n    \n    # Split sets into train and test\n    train  = age_df.loc[ (df.Age.notnull()) ]# known Age values\n    test = age_df.loc[ (df.Age.isnull()) ]# null Ages\n    \n    # All age values are stored in a target array\n    y = train.values[:, 0]\n    \n    # All the other values are stored in the feature array\n    X = train.values[:, 1::]\n    \n    # Create and fit a model\n    rtr = RandomForestRegressor(n_estimators=2000, n_jobs=-1)\n    rtr.fit(X, y)\n    \n    # Use the fitted model to predict the missing values\n    predictedAges = rtr.predict(test.values[:, 1::])\n    \n    # Assign those predictions to the full data set\n    df.loc[ (df.Age.isnull()), 'Age' ] = predictedAges \n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9bfdbd8ab9e50163b43adaf0307644614efac10b"},"cell_type":"code","source":"train_df = fill_missing_age(train_df)\ntest_df = fill_missing_age(test_df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df9187f8a132e3f228971edd3e95817e8a2c476d"},"cell_type":"markdown","source":"_**Note: We will deal some of the features with missing values in Feature Engineering Section**_"},{"metadata":{"_uuid":"64e60158b8cc6a060fbc06a4cf9bb0dab5d6deba"},"cell_type":"markdown","source":"## **Feature Engineering**"},{"metadata":{"_uuid":"52426f03cc754c9ecefe7534c7a7563190e1d3c1"},"cell_type":"markdown","source":"### Cabin\n**We will build a new feature called 'Deck' from Cabin, and remove the Cabin feature from the final dataset**"},{"metadata":{"trusted":true,"_uuid":"323a9533644529600e90b16d8b21f10b4ee3c63d"},"cell_type":"code","source":"train_df[\"Deck\"] = train_df.Cabin.str[0] # the first character denotes which deck the passenger is allocated\ntest_df[\"Deck\"] = test_df.Cabin.str[0]\ntrain_df[\"Deck\"].unique() # 0 is for null values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dee969fe6712f6d6468db137cfd9c52294b6e5cb"},"cell_type":"code","source":"sns.set(font_scale=1)\ng = sns.factorplot(\"Survived\", col=\"Deck\", col_wrap=4,\n                    data=train_df[train_df.Deck.notnull()],\n                    kind=\"count\", size=2.5, aspect=.8);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1e2580bbe99e56371d1378828a14ab2f1d163b99"},"cell_type":"markdown","source":"_From the above figure we can see that Deck C & B has the higher number of survival, contrary they have the higher number of non-survival too._"},{"metadata":{"trusted":true,"_uuid":"916f1970eaf3c14db72e48ca61402bbf08908e64"},"cell_type":"code","source":"train_df.Deck.fillna('Z', inplace=True)\ntest_df.Deck.fillna('Z', inplace=True)\n\nsorted_deck_values = train_df[\"Deck\"].unique() # Z is for null values\nsorted_deck_values.sort()\nsorted_deck_values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"685264fc9d00c04b4eeb284b78c77e9f6666d08b"},"cell_type":"code","source":"# Drop Cabin Feature from final dataset for test and train\ntrain_df = train_df.drop(['Cabin'], axis=1)\ntest_df = test_df.drop(['Cabin'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7cf468c7fd8cc687f8baac9f3484f333028e24f5"},"cell_type":"markdown","source":"### **SibSp and Parch**:\nSibSp and Parch would make more sense as a combined feature, that shows the total number of relatives, a person has on the Titanic. I will create it below and also a feature that shows if someone is not alone."},{"metadata":{"trusted":true,"_uuid":"714fc0099c5d08a0e63825c6d0ae1779cf2bc47d"},"cell_type":"code","source":"data = [train_df, test_df]\n\nfor dataset in data:\n    dataset['relatives'] = dataset['SibSp'] + dataset['Parch']\n    dataset.loc[dataset['relatives'] > 0, 'not_alone'] = 0\n    dataset.loc[dataset['relatives'] == 0, 'not_alone'] = 1\n    dataset['not_alone'] = dataset['not_alone'].astype(int)\n\ntrain_df['not_alone'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2071cf88bf61b8d14c399ae89525c3bc943c951b"},"cell_type":"code","source":"sns.set(font_scale=1)\nsns.factorplot(\"Survived\", col=\"relatives\", col_wrap=4,\n                    data=train_df,\n                    kind=\"count\", size=2.5, aspect=.8);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8dff9a4cf252c476c39f90f47d9e60b1f0b015ee"},"cell_type":"code","source":"sns.catplot('relatives','Survived',kind='point', \n                      data=train_df, aspect = 2.5, )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5b716fe1472e224efa64f0956c4023cde78f493f"},"cell_type":"markdown","source":"Here we can see that you had a high probability of survival with 1 to 3 relatives, but a lower one if you had less than 1 or more than 3 (except for some cases with 6 relatives)."},{"metadata":{"_uuid":"a043a1af8db87cd1889e86a4ab957ae148017aa2"},"cell_type":"markdown","source":"### Name\n**We will build a new feature called 'Title' from Name, and remove the Name feature from the final dataset**"},{"metadata":{"trusted":true,"_uuid":"4efffa48981a57b2e5d25e4b5b0682bf14cc778f"},"cell_type":"code","source":"data = [train_df, test_df]\n\nfor dataset in data:\n    # extract titles\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n    # replace titles with a more common title or as Rare\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr',\\\n                                            'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"66545a8f3a35fc2f11218d9c7d8d0474bec9792d"},"cell_type":"code","source":"# Drop Name Feature from final dataset for test and train\ntrain_df = train_df.drop(['Name'], axis=1)\ntest_df = test_df.drop(['Name'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5ff1c1887edbbd12aa3c72a684ffecb04837c66c"},"cell_type":"markdown","source":"## **Converting Features, Transforming Data and Cleanup**\nIn below section, we will\n- convert categorical features(Title, Sex, Deck, Embarked) into numerical type.\n- Feature Scaling 'Age' & 'Fare' \n- drop ticket feature"},{"metadata":{"trusted":true,"_uuid":"ff6077bed27bfe61d63175bbf4b1baef41c30d07"},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"82774b300e1cc9dcb5861d5c79d299e5570052ea"},"cell_type":"markdown","source":"### **Converting Title Feature**"},{"metadata":{"trusted":true,"_uuid":"04681ca6d235b2798006eb842119f49b35b67564"},"cell_type":"code","source":"data = [train_df, test_df]\ntitles = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n\nfor dataset in data:\n    # convert titles into numbers\n    dataset['Title'] = dataset['Title'].map(titles)\n    \n    # filling NaN with 0, to be safe\n    dataset['Title'] = dataset['Title'].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"02d0fd9c290507f54964e14c2715598e117557a2"},"cell_type":"markdown","source":"### **Converting 'Sex' feature intor numeric**"},{"metadata":{"trusted":true,"_uuid":"105f183f6f7d8e3484297c4b2d6016c6fb127c24"},"cell_type":"code","source":"genders = {\"male\": 0, \"female\": 1}\ndata = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Sex'] = dataset['Sex'].map(genders)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c5a33769dc45e93d0e7e1075c68671ec8f81eeba"},"cell_type":"markdown","source":"### **Converting 'Deck' feature intor numeric**"},{"metadata":{"trusted":true,"_uuid":"61d8c0304519afdc660aa948cf47d112d22ab00a"},"cell_type":"code","source":"deck = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7, \"T\": 8, \"Z\": 9}\ndata = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Deck'] = dataset['Deck'].map(deck)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"717fb3ff58c8332e08897ee13cc4a96f73b0d5eb"},"cell_type":"markdown","source":"### **Converting 'Embarked' feature intor numeric**"},{"metadata":{"trusted":true,"_uuid":"8e36ace73c40cbfd74080d486b1453306220ee28"},"cell_type":"code","source":"ports = {\"S\": 0, \"C\": 1, \"Q\": 2}\ndata = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Embarked'] = dataset['Embarked'].map(ports)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b087093e335574ce65e43d7545d88114ea23dd33"},"cell_type":"markdown","source":"### **Drop Ticket Feature**"},{"metadata":{"trusted":true,"_uuid":"e4a3f917e6bbb9b69c57ce81db7c5d8d30d5f7dd"},"cell_type":"code","source":"train_df['Ticket'].describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"91646b40372e3861e93f019c7d4cc88ef1836c26"},"cell_type":"markdown","source":"Since the Ticket attribute has 681 unique values, it will be a bit tricky to convert them into useful categories. So we will drop it from the dataset."},{"metadata":{"trusted":true,"_uuid":"a9965b9e87e7c01286ec3e912195039bdfccbb3a"},"cell_type":"code","source":"train_df = train_df.drop(['Ticket'], axis=1)\ntest_df = test_df.drop(['Ticket'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"09b7ad212cc19b6e6973112068e0b8e642773c6c"},"cell_type":"markdown","source":"## **Feature Scaling**\nWe can see that Age, Fare are measured on different scales, so we need to do Feature Scaling first before we proceed with predictions."},{"metadata":{"trusted":true,"_uuid":"78954ed3a3cd2811d2377c6cdbbf991dbd648d1f"},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\ndata = [train_df, test_df]\n\nfor dataset in data:\n    std_scale = StandardScaler().fit(dataset[['Age', 'Fare']])\n    dataset[['Age', 'Fare']] = std_scale.transform(dataset[['Age', 'Fare']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3a40858ae7b33e7e4a89d6a5d7dff7f0ecb327c1"},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b8597f240facfb3ec0506238440d58d46ef2adc2"},"cell_type":"code","source":"test_df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d7ff7cc4c42a5059c52e9bb63790c31b00c9fac3"},"cell_type":"markdown","source":"## **Building Machine Learning Models**\n\nIn this section, we will be dealing with the following parts.\n\nFirst, separate the features(X) from the labels(y). \n\n**X_train:** All features minus the value we want to predict (Survived) from the training set.\n\n**y_train:** Only the value we want to predict(Survived) from the training set. \n\n**X_test:** All features from the test set.\n\nSecond, we will use multiple classifiers from Scikit-learn library to train on the dataset, to find out which classifier works the best.\n\nThird, we will run the best classifier through the K-Fold pattern to identify its mean accuracy and std deviation\n\nAnd finally, evaluate the effectiveness of the best trained classifier on the final test data set."},{"metadata":{"_uuid":"8668002fd83ebaf7b4616047edbf695e50d30547"},"cell_type":"markdown","source":"### **Splitting up the training & test data.**"},{"metadata":{"trusted":true,"_uuid":"279a6de673f4d24ed029015bb56037f88d8661c6"},"cell_type":"code","source":"X_train = train_df.drop(['Survived', 'PassengerId'], axis=1)\ny_train = train_df[\"Survived\"]\nX_test  = test_df.drop(\"PassengerId\", axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"54baad84f65e3ed65f61a23da62d1deff1b3f742"},"cell_type":"markdown","source":"### Training the Classification Algorithms\n\nNow that we have preprocessed the data and built our training and testing datasets, we can start to deploy different classification algorithms. It's relatively easy to test multiple models; as a result, we will compare and contrast the performance of 10 different algorithms."},{"metadata":{"trusted":true,"_uuid":"e45a992b6acc3aa5573ee8ccaaacebba468d7606"},"cell_type":"code","source":"# Now that we have our dataset, we can start building algorithms! \n# We'll need to import each algorithm we plan on using\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB # (Naive Bayes)\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\n\n# define scoring method\nscoring = 'accuracy'\n\n# Define models to train\n\nnames = [\"Random Forest\", \"AdaBoost\",\"Nearest Neighbors\", \n         \"Naive Bayes\",\"Decision Tree\",\"Logistic Regression\",\"Gaussian Process\",\n         \"SVM RBF\", \"SVM Linear\", \"SVM Sigmoid\"]\n\nclassifiers = [\n    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n    AdaBoostClassifier(),\n    KNeighborsClassifier(n_neighbors = 3),\n    GaussianNB(),\n    DecisionTreeClassifier(max_depth=5),\n    LogisticRegression(random_state = 0),\n    GaussianProcessClassifier(1.0 * RBF(1.0)),\n    SVC(kernel = 'rbf', random_state=0),\n    SVC(kernel = 'linear', random_state=0),\n    SVC(kernel = 'sigmoid', random_state=0)\n]\n\nmodels = dict(zip(names, classifiers))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e4422909e0d42771d63f045784db2db223e71811"},"cell_type":"markdown","source":"### **Which is the best Model ?**"},{"metadata":{"trusted":true,"_uuid":"57ea149de317c763099255d19fac1b780ba247bc"},"cell_type":"code","source":"# Lets determine the accuracy score of each classifier\n\nmodels_accuracy_score = []\n\nfor name in models:\n    model = models[name]\n    model.fit(X_train, y_train)\n    predictions = model.predict(X_train)\n    models_accuracy_score.append((name, round(accuracy_score(y_train, predictions)* 100, 2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5c7c49bd96f4372d6b6c3f6351a1c6cd96d6ab80"},"cell_type":"code","source":"results = pd.DataFrame({\n    'Model': names,\n    'Score': [curr_model_score[1] for curr_model_score in models_accuracy_score]})\nresult_df = results.sort_values(by='Score', ascending=False)\nresult_df = result_df.set_index('Score')\nresult_df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f2a374dfa996ebf090107ed50403bd9602ac70c6"},"cell_type":"markdown","source":"As we can see that K-Nearest Neighbors classifier goes on the first place. We will perform GridSearch to find optimal parameters for our best classifier\n\nBut first, let us check, how K-Nearest Neighbors classifier performs when we use k-fold cross validation."},{"metadata":{"_uuid":"b3b112a5d88d2fc195993e08385dea39e8c9776b"},"cell_type":"markdown","source":"### **K-Fold Cross Validation**"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"65ec94a41d4ef8225dbf60a3d54e773c1529253c"},"cell_type":"code","source":"# import K-fold class\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n# define seed for reproducibility\nseed = 1\n\nkfold = KFold(n_splits=10, random_state = seed)\nmodel = KNeighborsClassifier(n_neighbors = 3)\ncv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n\nprint(\"Model Name: KNN\")\nprint(\"Scores:\", cv_results)\nprint(\"Mean:\", cv_results.mean())\nprint(\"Standard Deviation:\", cv_results.std())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"24d4d33971ccfefd5245a635cb3b56b9a89388ba"},"cell_type":"markdown","source":"## **Hyperparameter Tuning**\n\nLet us find out what are the optimal parameters for K-Nearest Neighbors classifier to get better accuracy\n\n_**Note: the following section might take some few minutes to process**_"},{"metadata":{"trusted":true,"_uuid":"6a40f266f71af9e6ddb4413543a65745c7d9b542"},"cell_type":"code","source":"# Applying Grid Search to find the best parameters\nfrom sklearn.model_selection import GridSearchCV\n\nclassifier = KNeighborsClassifier()\n\nparameters = [{'n_neighbors': np.arange(1,30), 'algorithm': ['ball_tree', 'kd_tree', 'brute'], \n               'leaf_size': np.arange(1,30), 'p': [1,2]}\n             ]\n\ngrid_search = GridSearchCV(estimator = classifier,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\n\ngrid_search = grid_search.fit(X_train, y_train)\nbest_accuracy = grid_search.best_score_\nbest_parameters = grid_search.best_params_\n\n\nprint(\"Accuracy: \", round(best_accuracy * 100, 2))\nprint(\"Best Params: \", best_parameters)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d38066997fe4f427ff3513e5dc43c1355f8fb46b"},"cell_type":"markdown","source":"**Let us test the K-Nearest Neighbors classifier again with the new optimal parameters**"},{"metadata":{"trusted":true,"_uuid":"68beb946f111eb28111e08f6d142f4de3c4257b0"},"cell_type":"code","source":"# perform classification again using optimal parameter from grid search cv\nclassifier = grid_search.best_estimator_\nclassifier.fit(X_train, y_train)\ny_train_predictions = classifier.predict(X_train)\ny_test_predictions = classifier.predict(X_test)\n\nprint(\"Model accuracy score: \", round(accuracy_score(y_train, y_train_predictions) * 100, 2))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b006894d1e70f08427947cce91954dbca7aafff4"},"cell_type":"markdown","source":"### **Confusion Matrix**\nNow that we have a proper model, let's evaluate it's performance in a more accurate way using confusion matrix."},{"metadata":{"trusted":true,"_uuid":"35882273a905b225a8460e6cdea3aba9bae932b8"},"cell_type":"code","source":"# confusion matrix\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\n\nconfusion_matrix(y_train, y_train_predictions)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e0fadb24a0654402bded585ad2ce9769568e6382"},"cell_type":"markdown","source":"## **Submission**"},{"metadata":{"trusted":true,"_uuid":"706c1b05ef21e26e1e27a34b646384fdbf182742"},"cell_type":"code","source":"submission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": y_test_predictions\n    })\nsubmission.to_csv('submission.csv', index=False)\noutput = pd.DataFrame({ 'PassengerId' : test_df[\"PassengerId\"], 'Survived': y_test_predictions })\noutput.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a599b0290dcc564cfa4709538393d6ab2eca512c"},"cell_type":"markdown","source":"## **Summary**\n\nThis project helped me in understanding ML/Data Science fundamentals and concepts that I learned from various books and online tutorials to apply to a problem. This project had a heavy focus on the data preparation part since this is what data scientists work on most of their time.\n\nI started with understanding the problem and what output is expected. During data exploration section I got to understand more about the data and what features are associated. Visualising Data helped me in understanding what features can be relevant for better prediction and also understand the correlation between features. The most part that I enjoyed was data preprocessing because the cleaner and relevant the data I provide the better the algorithms work. Feature Engineering and Feature Scaling section helped me improve features quality for better prediction. Then I trained the input data on 10 different machine learning models, picked one of them with better accuracy score to predict the output. Overall I enjoyed learning and building a machine learning program and improving my ML/Data Science knowledge."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.2"}},"nbformat":4,"nbformat_minor":1}