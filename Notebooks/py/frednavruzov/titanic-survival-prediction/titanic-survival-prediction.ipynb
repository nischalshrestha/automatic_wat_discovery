{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b6fcfa4b-d0df-3881-e034-aef974ebc0d8"
      },
      "source": [
        "## Titanic Oracle (decide others' fate simulation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6a861558-9946-6fd7-a935-a6b4ee5864ea"
      },
      "outputs": [],
      "source": [
        "# disable IPython warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# import required libraries/packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# models\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
        "\n",
        "# utility / measurements\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, KFold, train_test_split\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.feature_extraction import FeatureHasher\n",
        "from sklearn.preprocessing import Binarizer, OneHotEncoder, StandardScaler, FunctionTransformer\n",
        "from sklearn.grid_search import RandomizedSearchCV, GridSearchCV\n",
        "from sklearn.base import TransformerMixin\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# allow inline plotting\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "0d8e3554-6141-8a83-2a53-3328288a5f04"
      },
      "source": [
        "### Initial information about features\n",
        "\n",
        "<pre>\n",
        "VARIABLE DESCRIPTIONS:\n",
        "survival        Survival\n",
        "                (0 = No; 1 = Yes)\n",
        "pclass          Passenger Class\n",
        "                (1 = 1st; 2 = 2nd; 3 = 3rd)\n",
        "name            Name\n",
        "sex             Sex\n",
        "age             Age\n",
        "sibsp           Number of Siblings/Spouses Aboard\n",
        "parch           Number of Parents/Children Aboard\n",
        "ticket          Ticket Number\n",
        "fare            Passenger Fare\n",
        "cabin           Cabin\n",
        "embarked        Port of Embarkation\n",
        "                (C = Cherbourg; Q = Queenstown; S = Southampton)\n",
        "\n",
        "SPECIAL NOTES:\n",
        "Pclass is a proxy for socio-economic status (SES)\n",
        " 1st ~ Upper; 2nd ~ Middle; 3rd ~ Lower\n",
        "\n",
        "Age is in Years; Fractional if Age less than One (1)\n",
        " If the Age is Estimated, it is in the form xx.5\n",
        "\n",
        "With respect to the family relation variables (i.e. sibsp and parch)\n",
        "some relations were ignored.  The following are the definitions used\n",
        "for sibsp and parch.\n",
        "\n",
        "Sibling:  Brother, Sister, Stepbrother, or Stepsister of Passenger Aboard Titanic\n",
        "Spouse:   Husband or Wife of Passenger Aboard Titanic (Mistresses and Fiances Ignored)\n",
        "Parent:   Mother or Father of Passenger Aboard Titanic\n",
        "Child:    Son, Daughter, Stepson, or Stepdaughter of Passenger Aboard Titanic\n",
        "\n",
        "Other family relatives excluded from this study include cousins,\n",
        "nephews/nieces, aunts/uncles, and in-laws.  Some children travelled\n",
        "only with a nanny, therefore parch=0 for them.  As well, some\n",
        "travelled with very close friends or neighbors in a village, however,\n",
        "the definitions do not support such relations.\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "19cca0a0-fb6a-31ae-d354-d36f9c3b6f2f"
      },
      "outputs": [],
      "source": [
        "# import data and take a first look on it\n",
        "df_train = pd.read_csv('../input/train.csv', sep=',', header=0)\n",
        "df_test = pd.read_csv('../input/test.csv', sep=',', header=0)\n",
        "print('train DF shape: {}'.format(df_train.shape))\n",
        "df_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "6b32b2d7-2182-b139-b7fa-2363a8726078"
      },
      "source": [
        "#### So, we see 1 identifier column (PassengerId), 10 features(X) and 1 label \"Survived\" (y) <br> Let's extract exploratory statistics to get info about class balance/ missed NA values/ outliers etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a71081b4-98c8-f2d3-28cc-c51862bc1e23"
      },
      "outputs": [],
      "source": [
        "print(df_train.count())\n",
        "print('\\nTarget class balance:\\ndied: {}\\nsurvived: {}'.format(*df_train['Survived'].value_counts().values))\n",
        "df_train.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "bd7a70b5-eb8b-dac1-b4f0-fab07947508d"
      },
      "source": [
        "#### We see, that some columns have missing data (\"Age\", \"Cabin\", \"Embarked\"), \"Age\" has NaN values, and \"Fare\" *probably* has big outliers - <br> Let's fix that and write function for preprocessing stage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5229791c-c16b-b8ce-2c69-b9f792d580f2"
      },
      "outputs": [],
      "source": [
        "def clean_data(df, outlier_columns=None):\n",
        "    \"\"\"\n",
        "    Function that cleans input data and transforms some existing features into new ones\n",
        "    \n",
        "    - **parameters**, **types**, **return** and **return types**::\n",
        "        :param df: dataframe that holds initial unprocessed data\n",
        " \n",
        "        :return: return transformed and augmented copy of initial DataFrame\n",
        "        :rtype: pd.DataFrame object\n",
        "    \"\"\"\n",
        "    \n",
        "    if type(df) != type(pd.DataFrame()):\n",
        "        raise TypeError('please use a pandas.DataFrame object as input')\n",
        "    \n",
        "    pdf = df.copy() # make a copy to make all changes do not touch initial DataFrame\n",
        "    \n",
        "    # 1.fillna and missing data\n",
        "    \n",
        "    # numerical features\n",
        "    nulls = pdf.select_dtypes(include=[np.number]).isnull().astype(int).sum()\n",
        "    for i in nulls.index:\n",
        "        if nulls[i] > 0:\n",
        "            # group data by gender and pclass\n",
        "            ms = df.dropna().groupby(['Sex', 'Pclass']).median()[i] # group medians\n",
        "\n",
        "            d = {(i1, i2): ms.loc[(ms.index.get_level_values('Sex') == i1) &\n",
        "                                  (ms.index.get_level_values('Pclass') == i2)].values[0]\n",
        "                 for i1 in ms.index.levels[0] for i2 in ms.index.levels[1]}\n",
        "\n",
        "            pdf['median'] = pdf.apply(lambda row: d[(row['Sex'], row['Pclass'])], axis=1)\n",
        "            pdf[i].fillna(pdf['median'], inplace=True)\n",
        "\n",
        "    # categorical features\n",
        "    nulls = df.select_dtypes(exclude=[np.number]).isnull().astype(int).sum()\n",
        "    for i in nulls.index:\n",
        "        if nulls[i] > 0 and i == 'Cabin':\n",
        "            pdf[i].fillna('1', inplace=True)\n",
        "        elif nulls[i] > 0:\n",
        "            # group data by gender and pclass\n",
        "            ms = pdf.dropna().groupby(['Sex', 'Pclass'])[i].agg(lambda x:x.value_counts().index[0]) # group modes\n",
        "\n",
        "            d = {(i1, i2): ms.loc[(ms.index.get_level_values('Sex') == i1) & \n",
        "                                  (ms.index.get_level_values('Pclass') == i2)].values[0]\n",
        "                 for i1 in ms.index.levels[0] for i2 in ms.index.levels[1]}\n",
        "\n",
        "            pdf['mode'] = pdf.apply(lambda row: d[(row['Sex'], row['Pclass'])], axis=1)\n",
        "            pdf[i].fillna(pdf['mode'], inplace=True)\n",
        "    \n",
        "    # 3. extract additional features\n",
        "    # DECK ----------------------------------------\n",
        "    pdf['Deck'] = pdf['Cabin'].str.lower().str[0]\n",
        "    # Title ---------------------------------------\n",
        "    pdf['Title'] = pdf['Name'].str.replace('(.*, )|(\\\\..*)', '').str.lower()\n",
        "    rare_titles = ['dona', 'lady', 'the countess','capt', 'col', 'don', 'dr', 'major', 'rev', 'sir', 'jonkheer']\n",
        "    ud = dict.fromkeys(rare_titles, 'rare title'); ud.update({'mlle':'miss', 'ms':'miss', 'mme':'mrs'})# merge titles\n",
        "    pdf['Title'] = pdf['Title'].replace(ud)\n",
        "    # IsChild -------------------------------------\n",
        "    pdf['IsChild'] = ((pdf['Age'] < 18) & (pdf['Title'].isin(['master', 'miss']))).astype(int)\n",
        "    # IsMother -------------------------------------\n",
        "    pdf['IsMother'] = ((pdf['Age'] > 18) & (pdf['Title'] == 'mrs') & (pdf['Parch'] > 0)).astype(int)\n",
        "    \n",
        "    # 3. transform old features\n",
        "    pdf['IsMale'] = pdf['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n",
        "    pdf['Embarked'] = pdf['Embarked'].map( {'S': 1, 'C': 2, 'Q': 3} ).astype(int)\n",
        "    \n",
        "    \n",
        "    pdf['Title'] = pdf['Title'].map({'miss':1,\n",
        "                                     'mrs':2,\n",
        "                                     'master':3,\n",
        "                                     'mr':4,\n",
        "                                     'rare title':5\n",
        "                                    }).astype(int)# to ints\n",
        "    \n",
        "    pdf['Deck'] = pdf['Deck'].map(dict(zip('1abcdefgt', range(0,9)))).astype(int) # map to ints\n",
        "    \n",
        "    \n",
        "    # 4. substitute outliers (for numerical columns)\n",
        "    if outlier_columns:\n",
        "        for c in outlier_columns:\n",
        "            q = pdf[c].quantile([0.2, 0.8]).values\n",
        "            pdf[c] = pdf[c].apply(lambda x: q[0] if x < q[0] else min(q[1], x))\n",
        "    \n",
        "    # 5. drop-redundant: drop useless features\n",
        "    pdf.drop([\n",
        "            'Cabin', \n",
        "            'Name', \n",
        "            'Sex', \n",
        "            'Ticket', \n",
        "            'median', \n",
        "            'mode',\n",
        "            ], axis=1, inplace=True, errors='ignore')\n",
        "     \n",
        "    return pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d46e6e2a-a917-a076-bcc8-26090b394d70"
      },
      "source": [
        "### Now apply this function to train/test to get slightly better statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "48c992e2-2e9d-e05d-3f58-3cae1cf321f1"
      },
      "outputs": [],
      "source": [
        "train = clean_data(df_train, outlier_columns=['Fare'])\n",
        "test = clean_data(df_test, outlier_columns=['Fare'])\n",
        "print(train.count())\n",
        "train.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "5c90f8ac-b884-7d3f-7dc8-16e9245850a2"
      },
      "source": [
        "#### Check for possible multicollinearity (will affect linear models or make us use regularization)\n",
        "\n",
        "We see that \"Fare\" highly correlates with \"PClass\", however, such linear dependency is expected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d24ad20f-aac8-f85b-2edf-089b5b5a6433"
      },
      "outputs": [],
      "source": [
        "train.select_dtypes(include=[np.number]).drop(['PassengerId', 'IsMale', 'IsChild', 'IsMother'], errors='ignore', \n",
        "                                              axis=1).corr(method='pearson')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "047cb3d1-3755-9ff3-082a-a6ce4d1bb0a4"
      },
      "source": [
        "#### Let's also make classes balanced (add samples of class \"survived\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "33a95d53-7395-3713-ae08-ff5bc18c33ad"
      },
      "outputs": [],
      "source": [
        "samples_to_add = train['Survived'].value_counts().values[0] - train['Survived'].value_counts().values[1]\n",
        "add_survived = train[train['Survived'] == 1].sample(n=samples_to_add, replace=True)\n",
        "\n",
        "train = pd.concat([train, add_survived], axis=0)\n",
        "train['Survived'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "f53bdf78-ab65-617b-3dd4-f5e3839f3f16"
      },
      "source": [
        "### Now we are ready to apply One-Hot-Encoding to categorical features, normalize numerical features to get the same scale and start classifier model selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "be079e93-3644-52e2-d463-cf7e8679c01c"
      },
      "outputs": [],
      "source": [
        "# split train on train/holdout parts\n",
        "print(\"initial train shape: {}\".format(train.shape))\n",
        "\n",
        "train_tr, train_ho, y_train, y_ho = train_test_split(train.drop(['Survived', 'PassengerId'], axis=1), \n",
        "                                                     train['Survived'].values,\n",
        "                                                     test_size=0.3, \n",
        "                                                     stratify=train['Survived'].values, \n",
        "                                                     random_state=42)\n",
        "print (\"X train shape: {}, X holdout shape: {}\".format(train_tr.shape, train_ho.shape))\n",
        "print (\"y train shape: {}, y holdout shape: {}\".format(y_train.shape, y_ho.shape))\n",
        "\n",
        "# define column types for proper transformation/encoding\n",
        "binary_cols = ['IsMale', 'IsChild', 'IsMother']\n",
        "categorical_cols = ['Pclass', 'Embarked', 'Deck', 'Title']\n",
        "numeric_cols = set(train.columns) - set(['Survived', 'PassengerId'] + binary_cols + categorical_cols)\n",
        "\n",
        "# making correspondent boolean column indices\n",
        "bdata_indices = np.array([(col in binary_cols) for col in train_tr.columns], dtype=bool)\n",
        "cdata_indices = np.array([(col in categorical_cols) for col in train_tr.columns], dtype=bool)\n",
        "ndata_indices = np.array([(col in numeric_cols) for col in train_tr.columns], dtype=bool)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e2e7eff5-2360-6ff4-ef22-857366776a3f"
      },
      "outputs": [],
      "source": [
        "# simple class to get rid of sparse format, incompatible with some classifiers\n",
        "class DenseTransformer(TransformerMixin):\n",
        "\n",
        "    def transform(self, X, y=None, **fit_params):\n",
        "        return X.todense()\n",
        "\n",
        "    def fit_transform(self, X, y=None, **fit_params):\n",
        "        self.fit(X, y, **fit_params)\n",
        "        return self.transform(X)\n",
        "\n",
        "    def fit(self, X, y=None, **fit_params):\n",
        "        return self\n",
        "    \n",
        "    def get_params(self, deep=True):\n",
        "        return dict()\n",
        "\n",
        "# create pipeline of transformation/extraction steps + classification step\n",
        "def make_pipe(classifier):\n",
        "    pipe = Pipeline(\n",
        "        steps = [\n",
        "            ('feature_processing', FeatureUnion(\n",
        "                    transformer_list = [\n",
        "                        # binary data\n",
        "                        ('binary_processing', FunctionTransformer(lambda x: x[:, bdata_indices])),\n",
        "\n",
        "                        # categorical data\n",
        "                        ('categorical_processing', Pipeline(steps = [\n",
        "                                    ('selecting', FunctionTransformer(lambda x: x[:, cdata_indices])),\n",
        "                                    #('label_encoding', LabelEncoder()),\n",
        "                                    #('hot_encoding', FeatureHasher())\n",
        "                                    ('hot_encoding', OneHotEncoder(handle_unknown='ignore'))\n",
        "                                ]\n",
        "                             )\n",
        "                        ),\n",
        "\n",
        "                        # numeric data\n",
        "                        ('numeric_processing', Pipeline(steps = [\n",
        "                                    ('selecting', FunctionTransformer(lambda x: x[:, ndata_indices])),\n",
        "                                    ('scaling', StandardScaler(with_mean=0.))\n",
        "                                ]\n",
        "                             )\n",
        "                        ),\n",
        "                    ]\n",
        "                )\n",
        "            ),\n",
        "            ('dense', DenseTransformer()),\n",
        "            ('clf', classifier)\n",
        "        ]\n",
        "    )\n",
        "    return pipe\n",
        "\n",
        "# base classificators\n",
        "clfs = [\n",
        "    ('SGDClassifier', SGDClassifier(random_state=42)),\n",
        "    ('LogisticRegression', LogisticRegression(random_state=42)),\n",
        "    ('LinearSVC', LinearSVC(random_state=42)),\n",
        "    ('KNN', KNeighborsClassifier(n_neighbors=10)),\n",
        "    ('RandomForestClassifier', RandomForestClassifier(random_state=42)),\n",
        "    ('GradientBoostingClassifier', GradientBoostingClassifier(random_state=42)),\n",
        "    ('GaussianNB', GaussianNB()),\n",
        "    ('MultinomialNB', MultinomialNB()),\n",
        "]\n",
        "\n",
        "# cross-validation\n",
        "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42) # use it to preserve equal class balance in folds\n",
        "scores = [] # to hold cross-validation scores for base estimators\n",
        "\n",
        "for c in clfs:\n",
        "    pipe = make_pipe(c[1])\n",
        "    score  = cross_val_score(pipe, X=train_tr.values, y=y_train, cv=cv).mean()\n",
        "    scores.append([c[0], score])\n",
        "\n",
        "for s in sorted(scores, key=lambda x: x[1], reverse=True):\n",
        "    pass\n",
        "    print(\"model: {}, accuracy={}\".format(*s))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "304d47f4-515c-53dd-7976-29019ca4fb3c"
      },
      "source": [
        "#### It seems that Gradient Boosting, Random Forest  and LinearSVC have the best baseline accuracy - let's use them further as parts of VotingClassifier and search for best estimator params via GridSearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "fda389dd-1b36-74d0-f0ca-8822ee441b31"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# 1. Random Forest --------------------------------------------------------------------------------------------\n",
        "estimator_rf = make_pipe(RandomForestClassifier(random_state=7, n_jobs=-1, oob_score=True))\n",
        "\n",
        "# set params grid for GridSearch to search through\n",
        "params_grid_rf = {\n",
        "    # 'clf__bootstrap': [True, False],\n",
        "    #'clf__criterion': [\"gini\", \"entropy\"],\n",
        "    'clf__max_depth': [None, 2, 3, 4, 5],\n",
        "    #'clf__max_features': [\"sqrt\", \"log2\", None],\n",
        "    'clf__min_samples_leaf': [1, 3],\n",
        "    'clf__n_estimators': [10, 20, 30, 50, 100],\n",
        "    #'clf__warm_start': [True, False]\n",
        "}\n",
        "\n",
        "# perform randomized search (100 iterations) because no of combinations is rather huge (2*2*5*3*2*5*2 = 1200)\n",
        "grid_cv_rf = GridSearchCV(estimator_rf, param_grid=params_grid_rf, scoring='accuracy', cv=5)\n",
        "grid_cv_rf.fit(train_ho, y_ho) # fit it on hold-out sample\n",
        "\n",
        "# 2. Gradient Boosting -----------------------------------------------------------------------------------------\n",
        "estimator_gb = make_pipe(GradientBoostingClassifier(random_state=42))\n",
        "\n",
        "# set params grid for GridSearch to search through\n",
        "params_grid_gb = {\n",
        "    #'clf__loss': ['exponential', 'deviance'],\n",
        "    'clf__learning_rate': [0.01, 0.1, 0.5, 1, 10],\n",
        "    #'clf__n_estimators': [50, 100, 200],\n",
        "    'clf__max_depth': [2, 3],\n",
        "    'clf__subsample': [0.25, 0.5, 1.0]\n",
        "}\n",
        "\n",
        "# perform usual GridSearch\n",
        "grid_cv_gb = GridSearchCV(estimator_gb, param_grid=params_grid_gb, scoring='accuracy', cv=5)\n",
        "grid_cv_gb.fit(train_ho, y_ho) # fit it on hold-out sample\n",
        "\n",
        "# 3. SVC -------------------------------------------------------------------------------------------------------\n",
        "estimator_svc = make_pipe(LinearSVC(random_state=42))\n",
        "\n",
        "# set params grid for GridSearch to search through\n",
        "params_grid_svc = {\n",
        "    'clf__fit_intercept': [True, False],\n",
        "    'clf__dual': [True, False],\n",
        "    #'clf__loss': [\"hinge\", \"squared_hinge\"],\n",
        "    #'clf__penalty': ['l1', 'l2'], # L-1, L-2 euclidean,\n",
        "    'clf__C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
        "}\n",
        "\n",
        "# perform usual GridSearch\n",
        "grid_cv_svc = GridSearchCV(estimator_svc, param_grid=params_grid_svc, scoring='accuracy', cv=5)\n",
        "grid_cv_svc.fit(train_ho, y_ho) # fit it on hold-out sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "277218ca-49c2-504a-3d05-3a013e32aafd"
      },
      "outputs": [],
      "source": [
        "# best params obtained\n",
        "print(grid_cv_svc.best_params_)\n",
        "print(grid_cv_gb.best_params_)\n",
        "print(grid_cv_rf.best_params_)\n",
        "\n",
        "# best score obtained\n",
        "print(grid_cv_svc.best_score_)\n",
        "print(grid_cv_gb.best_score_)\n",
        "print(grid_cv_rf.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "6a8807b2-d2a6-a889-0cea-0d26e0916923"
      },
      "source": [
        "#### Appropriate models are found, let's fit it on whole train dataset, construct VotingClassifier and then make predictions on test (to get submission .csv file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4dd3d4cf-cf5d-f5b6-90bc-f20b639f0d64"
      },
      "outputs": [],
      "source": [
        "# construct ensemble\n",
        "ensemble = VotingClassifier(estimators=[\n",
        "                                        ('rf', grid_cv_rf.best_estimator_), \n",
        "                                        ('gb', grid_cv_gb.best_estimator_), \n",
        "                                        ('svc', grid_cv_svc.best_estimator_),\n",
        "                                       ], voting='hard')\n",
        "# fit it to the whole train dataset\n",
        "ensemble.fit(train.drop(['Survived', 'PassengerId'], axis=1), train['Survived'].values)\n",
        "\n",
        "# make prediction on test\n",
        "prediction = ensemble.predict(test.drop(['PassengerId'], axis=1)) # predict labels based on X_test\n",
        "\n",
        "answers = pd.DataFrame({'Survived': prediction}, test['PassengerId']) # predict labels\n",
        "answers.to_csv('titanic-submission-Navruzov.csv') # save to submission file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d012a64a-78c8-8edf-f867-b2991b9ec23b"
      },
      "source": [
        "<img src=\"https://i.ytimg.com/vi/0FHEeG_uq5Y/maxresdefault.jpg\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7525a0ca-730b-1cbd-6f27-9698a5b890a2"
      },
      "outputs": [],
      "source": ""
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "525cf456-e18a-395a-9571-32d99506e97a"
      },
      "outputs": [],
      "source": ""
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "15e134ee-2013-92ab-7b64-4bc8137361c6"
      },
      "outputs": [],
      "source": ""
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d59068d0-3aaa-a56b-3e3e-6389f8a0a8e7",
        "collapsed": true
      },
      "outputs": [],
      "source": ""
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}