{"cells":[{"metadata":{"_uuid":"9205ce46f051287e68e86266a504581d8383364d"},"cell_type":"markdown","source":"# Titanic Classification"},{"metadata":{"_uuid":"d200085760e9af845d0387fe9b91878115f626e1"},"cell_type":"markdown","source":"### The following notebook explains in brief how to train a simple machine learning model to tackle the Titanic classification challenge on Kaggle. \n\n#### This is my first Kaggle challenge attempt and the work here is much inspired from already existing kernels on Kaggle."},{"metadata":{"_uuid":"0d17c1d5a94e522677b9d29a4f0d01426d83e4c7"},"cell_type":"markdown","source":"### Importing the necessary libraries:"},{"metadata":{"trusted":true,"_uuid":"8ab608707b813f7dba936deffa3d440b45f73dea"},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pandas import Series,DataFrame\nimport seaborn as sns\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"45cac432a8ed7f458de51dda0ef9c02148c1d37d"},"cell_type":"markdown","source":"### Importing the dataset:"},{"metadata":{"trusted":true,"_uuid":"ef41512e993cfc40e9d893a81b5721c0fcda855e"},"cell_type":"code","source":"train = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\ndata = pd.concat([train,test],axis=0,sort=False)\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cd287d8e75d8e8ff85fa700037024072f709fdc6"},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bdfb83d6f9c174a28dabaf2cb98d00945cf38635"},"cell_type":"markdown","source":" Let us know the likelihood of survival of passengers based on some of their data. The float values represent the likelihood of their survival."},{"metadata":{"trusted":true,"_uuid":"ed79bb8870d7a9d9cb1d693f1b1a77eac6fd2d05"},"cell_type":"code","source":"train[['Pclass','Survived']].groupby(train['Pclass']).mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"178087995dee93cd57b2b3e5a329713dd4e8d0dc"},"cell_type":"markdown","source":"Above, we get Class wise likelihood of Survival. As we can see, the passengers with class 1 tickets are more likely to survive than the remaining two."},{"metadata":{"trusted":true,"_uuid":"ff57517d86b67f60eb0eca191c62cc9ec6085a38"},"cell_type":"code","source":"train[['Sex','Survived']].groupby(train['Sex']).mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fdd94b267cdb418dc6527f8f75729d8710b438d0"},"cell_type":"markdown","source":"Similarly, we see gender wise likelihood of Survival."},{"metadata":{"_uuid":"e11a345b6dc8972eac385bbb9e00b8a655de0223"},"cell_type":"markdown","source":"There are two columns named number of siblings/spouses and parents/children. We can use that data and create a new column named Family size and then group them to check the likelihood of survival based on their family sizes:"},{"metadata":{"trusted":true,"_uuid":"7149fb3c20d652de97249fa3761954f3f6af4162"},"cell_type":"code","source":"for row in train:\n    train['Family size'] = train['SibSp'] + train['Parch'] + 1\n\nfor row in test:\n    test['Family size'] = test['SibSp'] + test['Parch'] + 1\ntrain[['Family size','Survived']].groupby(train['Family size']).mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3109843cc72a44c20ea131bd632bff2d7a99b276"},"cell_type":"markdown","source":"Let us check how being a lone passenger contributes to survival. Below we can see that lone passengers are less likely to survive."},{"metadata":{"trusted":true,"_uuid":"a32a88fc9b79c54670aca153696d4d374259168b"},"cell_type":"code","source":"for row in train:\n        train['isAlone'] = 0\n        train.loc[train['Family size']==1, 'isAlone'] = 1\n\nfor row in test:\n        test['isAlone'] = 0\n        test.loc[test['Family size']==1, 'isAlone'] = 1\n        \ntrain[['isAlone','Survived']].groupby(train['isAlone']).mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aa94acc42618f00a4ddc73455ffd787e950efe18"},"cell_type":"markdown","source":"The Embarked column records data of port of Embarkation of the passengers. Since there are some missing values in the Embarked column, we fill those missing values by mode ie. most occuring value ie. 'S'."},{"metadata":{"trusted":true,"_uuid":"b78a56d84d365d653a5899abcc6652be86dda7b6"},"cell_type":"code","source":"data['Embarked'].groupby(data['Embarked']).count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"148058641aa6e4d3cbb6da6cc1ed0d9b67c57126"},"cell_type":"code","source":"train['Embarked'] = train['Embarked'].fillna('S')\ntrain[['Embarked','Survived']].groupby(train['Embarked']).mean()\n\ntest['Embarked'] = test['Embarked'].fillna('S')\ntrain[['Embarked','Survived']].groupby(train['Embarked']).mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0154e49be1eef70bc9ec28792b61df8c4be1ad18"},"cell_type":"markdown","source":"Let us impute the missing values of Fare details by the median of the records and then categorize the fare details into 10 ranges. "},{"metadata":{"trusted":true,"_uuid":"2118a97a0b4414892230b6e1a6f37b70e605ebbe"},"cell_type":"code","source":"train['Fare'] = train['Fare'].fillna(train['Fare'].median())\ntrain['CategoricalFare'] = pd.qcut(train['Fare'],4)\n\ntest['Fare'] = test['Fare'].fillna(test['Fare'].median())\ntest['CategoricalFare'] = pd.qcut(test['Fare'],4)\ntrain[['CategoricalFare','Survived']].groupby(train['CategoricalFare']).mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"712dcbd4f773e84bbbbd2d82a88fb53da4806542"},"cell_type":"markdown","source":"Now let us deal with the Age values:"},{"metadata":{"trusted":true,"_uuid":"8f1e882e18c192694b1a5944983705335579092a"},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d3edcf6a7e4defead60356caccd7faefdf811197"},"cell_type":"markdown","source":"As one can notice, out of total 891, only 714 values of age are recorded while the remaining data is missing. Since the 177 records whose ages are missing still might represent crucial data about other attributes, we simply cannot discard those records. A better method to deal with them is by replacing them by random numbers among the upper and lower range of standard deviation from the mean values of age. Let us do that:"},{"metadata":{"trusted":true,"_uuid":"090de29411159dc6aaa807815a17ad2b48b1fb9d"},"cell_type":"code","source":"for row in train:\n    avg = train['Age'].mean()\n    std = train['Age'].std()\n    null_count = train['Age'].isnull().sum()\n    age_null_random_list = np.random.randint(avg - std, avg + std, size=null_count)\n    train['Age'][np.isnan(train['Age'])] = age_null_random_list\n    train['Age'] = train['Age'].astype(int)\n\nfor row in test:\n    avg = test['Age'].mean()\n    std = test['Age'].std()\n    null_count = test['Age'].isnull().sum()\n    age_null_random_list = np.random.randint(avg - std, avg + std, size=null_count)\n    test['Age'][np.isnan(test['Age'])] = age_null_random_list\n    test['Age'] = test['Age'].astype(int)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0cc27c0b422ed3410c0d795ca1bbf6d584fe2a42"},"cell_type":"code","source":"train['CategoricalAge'] = pd.cut(train['Age'],5)\n\ntest['CategoricalAge'] = pd.cut(test['Age'],5)\ntrain[['CategoricalAge','Survived']].groupby(train['CategoricalAge']).mean()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"43766885971a23049684cdd337ed3aa1861ffc20"},"cell_type":"markdown","source":"Now that we have imputed some missing values, let us handle the categorical values and handle the ranged values"},{"metadata":{"trusted":true,"_uuid":"e8fb3bacad0fea19069563131cbb81d3a81484f3"},"cell_type":"code","source":" # Mapping Fare\ntrain.loc[ train['Fare'] <= 7.91, 'Fare'] = 0\ntrain.loc[(train['Fare'] > 7.91) & (train['Fare'] <= 14.454), 'Fare'] = 1\ntrain.loc[(train['Fare'] > 14.454) & (train['Fare'] <= 31), 'Fare']   = 2\ntrain.loc[ train['Fare'] > 31, 'Fare'] = 3\n\ntrain.loc[ train['Age'] <= 16, 'Age'] = 0\ntrain.loc[(train['Age'] > 16) & (train['Age'] <= 32), 'Age'] = 1\ntrain.loc[(train['Age'] > 32) & (train['Age'] <= 48), 'Age'] = 2\ntrain.loc[(train['Age'] > 48) & (train['Age'] <= 64), 'Age'] = 3\ntrain.loc[ train['Age'] > 64, 'Age'] = 4\n\n\ntest.loc[ test['Fare'] <= 7.91, 'Fare'] = 0\ntest.loc[(test['Fare'] > 7.91) & (test['Fare'] <= 14.454), 'Fare'] = 1\ntest.loc[(test['Fare'] > 14.454) & (test['Fare'] <= 31), 'Fare']   = 2\ntest.loc[ test['Fare'] > 31, 'Fare'] = 3\n\ntest.loc[ test['Age'] <= 16, 'Age'] = 0\ntest.loc[(test['Age'] > 16) & (test['Age'] <= 32), 'Age'] = 1\ntest.loc[(test['Age'] > 32) & (test['Age'] <= 48), 'Age'] = 2\ntest.loc[(test['Age'] > 48) & (test['Age'] <= 64), 'Age'] = 3\ntest.loc[ test['Age'] > 64, 'Age'] = 4\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"543ef972a149e78e2a8cf2043e592282a8d68493"},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6aa23231fed55eb2f60455ca22e597e959a3f01f"},"cell_type":"code","source":"test.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d362ee0c2dd1a89542222f09c87bdd0761a7e71c"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5c642e2bf92b9a5b3ac61eee88c350fd55e97b1e"},"cell_type":"code","source":"train['Sex'] = train['Sex'].map({'female':0,'male':1})\ntest['Sex'] = test['Sex'].map({'female':0,'male':1})\ntrain['Embarked'] = train['Embarked'].map({'S':0,'C':1,'Q':2})\ntest['Embarked'] = test['Embarked'].map({'S':0,'C':1,'Q':2})","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1938fc1a313b139db620e34718b9b7d37327d6c6"},"cell_type":"markdown","source":"Now that all our feature engineering and data cleaning is done, let us select the attributes from the dataset and perform visualization.\n\nBut before that, let us get rid of unnecessary columns. We can get rid of columns such as Passenger Names, the Ticket nos., Cabin records, SibSp & Parch(Since we already used them in Family size), CategoricalFare and CategoricalAge."},{"metadata":{"trusted":true,"_uuid":"18e6383b52f812ed067fe3b499943fa311933bb7"},"cell_type":"code","source":"train = train.drop(['Name','SibSp','Parch','Ticket','Cabin','CategoricalFare','CategoricalAge'],axis = 1)\ntest = test.drop(['Name','SibSp','Parch','Ticket','Cabin','CategoricalFare','CategoricalAge'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9d1f9e07c369c67ee27e5ea3f51af281830b63d3"},"cell_type":"code","source":"sns.pairplot(train,hue='Survived')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fb989c9807569887c832b5b350848c51d53749ca"},"cell_type":"markdown","source":"Sex wise distribution of survival.\n\nFemale --> 0\nMale --> 1"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"a33f42ab1021aff5b9306099233e4fb67aee6f58"},"cell_type":"code","source":"sns.countplot(x=train['Sex'],data=train,hue=\"Survived\",orient='v')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d87d226e7a296568e81ffc610ea898d3028aa039"},"cell_type":"markdown","source":"Sex wise distribution of Age.\n\nFemale --> 0\nMale --> 1"},{"metadata":{"trusted":true,"_uuid":"26e276f4f013327089f258baaa9e6a45b16a3b27"},"cell_type":"code","source":"plt.figure(figsize=(12,10))\nsns.countplot(y=train['Age'],data=train,hue='Sex')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7f3c2c4f93d9a8ad7723bb38ecad97f1671de0f3"},"cell_type":"markdown","source":"The records representing Embarkation and the Class of the passengers"},{"metadata":{"trusted":true,"_uuid":"61466b092f18d1ef6be23877008c6d9b78fe40d7"},"cell_type":"code","source":"plt.figure(figsize=(15,15))\nsns.jointplot(train['Embarked'],train['Pclass'],kind=\"kde\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b548e7ef036127d8661888febcf28d4e099e58e8"},"cell_type":"markdown","source":"Class wise Fare distribution which tells the relativity among the two attributes:"},{"metadata":{"trusted":true,"_uuid":"26c76e132385fe2b02c6298a7f694c5d6c86daec"},"cell_type":"code","source":"plt.figure(figsize=(12,10))\nsns.countplot(x=train['Pclass'],data=train,hue=\"Fare\") ","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"df78a9f3f14587a74167415cbcfcc4a22472668c"},"cell_type":"code","source":"explode =(0,0.05,0.05,0.05)\nplt.figure(figsize=(10,10))\nplt.pie(train['Fare'].groupby(train['Fare']).sum(),labels=['Category 0','Category 1','Category 2','Category 3'],\n        colors=['gold','#e33d3d','#33d9ed','#7ae10c'],\n        explode=explode,shadow=True,autopct='%1.1f%%')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"493a3262ea21bf697dd2d19a702d1967777b12e5"},"cell_type":"markdown","source":"### Classification Modelling"},{"metadata":{"_uuid":"8ba82d6246d29c56e139d3e4be4f02b9e726bfa5"},"cell_type":"markdown","source":"We will be creating our classification model on several different classification algorithms including some Ensembling models such as Random Forests, Adaboost, Gradient boosting etc.\n\nRefer to the links below to know more about the different classification models:\n<pre>\n<a href=\"https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm\">K nearest Neighbors</a>\n<a href=\"https://en.wikipedia.org/wiki/Support_vector_machine\">Support Vector Machines</a>\n<a href=\"https://en.wikipedia.org/wiki/Decision_tree\">Decision Trees</a>\n<a href=\"https://en.wikipedia.org/wiki/Random_forest\">Random Forests</a>\n<a href=\"https://en.wikipedia.org/wiki/AdaBoost\">Adaboost</a>\n<a href=\"https://en.wikipedia.org/wiki/Gradient_boosting\">Gradient boosting</a>\n<a href=\"https://en.wikipedia.org/wiki/Naive_Bayes_classifier\">Naive Bayes Classification</a>\n<a href=\"https://en.wikipedia.org/wiki/Logistic_regression\">Logistic Regression</a>\n</pre>"},{"metadata":{"trusted":true,"_uuid":"239e9772350c060762796da0d6ef0ee6cc591949"},"cell_type":"code","source":"from sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\n\nx = train.iloc[:,[0,2,3,4,5,6,7,8]].values\ny = train.iloc[:,1].values\n\nclassifiers = [\n    KNeighborsClassifier(4),\n    SVC(probability=True),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    AdaBoostClassifier(),\n    GradientBoostingClassifier(),\n    GaussianNB(),\n    LogisticRegression()]\n\nsss = StratifiedShuffleSplit(n_splits=10, test_size=0.1, random_state=0)\n\nchart = pd.DataFrame(columns=[\"Classifier\", \"Accuracy\"])\n\nacc_dict = {}\n\nfor train_index, test_index in sss.split(x, y):\n    x_train, x_test = x[train_index], x[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\nfor clf in classifiers:\n    name = clf.__class__.__name__\n    clf.fit(x_train, y_train)\n    train_predictions = clf.predict(x_test)\n    acc = accuracy_score(y_test, train_predictions)\n    if name in acc_dict:\n        acc_dict[name] += acc\n    else:\n        acc_dict[name] = acc\n\nfor clf in acc_dict:\n    acc_dict[clf] = acc_dict[clf] / 10.0\n    log_entry = pd.DataFrame([[clf, acc_dict[clf]]], columns=[\"Classifier\", \"Accuracy\"])\n    chart = chart.append(log_entry)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0110b34c41bf2226de35ea16e3138afcab732799"},"cell_type":"markdown","source":"What we have done here is fit the training data to all the models and mapped their accuracies into a bar chart. The plot quite delineates which model is able to classify the Survival of the passengers better."},{"metadata":{"trusted":true,"_uuid":"a073cdd25e36bd9810cf95603ee1e5b1ac241d9f"},"cell_type":"code","source":"plt.xlabel('Accuracy')\nplt.title('Classifier Accuracy')\nsns.set_color_codes(\"muted\")\nsns.barplot(x='Accuracy', y='Classifier', data=chart, palette=\"Blues\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"540b6ad3158680cb2e55d3ca99265b5c291040dd"},"cell_type":"code","source":"chart['Accuracy']= chart['Accuracy']*1000\nchart","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e130991dccf09633eb551a3296c10367faba1817"},"cell_type":"markdown","source":"* As one can notice, the Logistic Regression classifier shows the maximum accuracy in classifying the survival of passengers, hence we will use it to predict our values."},{"metadata":{"trusted":true,"_uuid":"8eb488e7835af8ffd83d7fffdc8a06bc86d63a5d"},"cell_type":"code","source":"final_classifier = LogisticRegression()\nfinal_classifier.fit(train.iloc[:,[0,2,3,4,5,6,7,8]].values,train.iloc[:,1].values)\nresult = final_classifier.predict(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"410af4bc0a3cc9cdfe4dce6ef45f0b6193d1feac"},"cell_type":"code","source":"result = DataFrame(result)\nresult","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"07d5e4213214d73d1f019ae9f465dd8d0006199d"},"cell_type":"markdown","source":"Therefore, we conclude the titanic classification modelling using Python. The better classifier was Logistic Regression, whereas the algorithm to perform poorly on the data was Support Vector Classifier.  "},{"metadata":{"_uuid":"7e250a158b911adbcd15024209856290ac061d7c"},"cell_type":"markdown","source":"  "},{"metadata":{"_uuid":"c328b68c593d4aff520704f24d7086a6fdd1b55b"},"cell_type":"markdown","source":"## Thanks for Reading."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}