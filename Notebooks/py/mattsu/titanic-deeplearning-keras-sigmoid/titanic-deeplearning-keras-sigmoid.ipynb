{"nbformat": 4, "nbformat_minor": 1, "metadata": {"kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}, "language_info": {"pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py", "version": "3.6.4", "codemirror_mode": {"version": 3, "name": "ipython"}, "mimetype": "text/x-python", "name": "python"}}, "cells": [{"cell_type": "markdown", "source": ["## Introduction: \n", "This kernel use keras framework.\n", "From the result, we can see tensorflow neural networks can't achieve higher accuracy than 80%.\n", "Why? My conclusion is the data size is too small that DNN can't work well at all.\n", "\n", "However, here I represent a typical flow for tensorflow.\n", "\n", "As the traning dataset is **too small**, there're obvious **overfitting**\n", "have to try to avoid overfitting:\n", "- decrease features\n", "- add more data\n", "- regularization\n", "\n", "**My found is that, feature engineering is still the most important if dataset is so small.**"], "metadata": {"_uuid": "f9b2d0c99c1179880bcc451d598885e270cfcc37", "_cell_guid": "eece8554-fe86-4d2b-ad51-d2587af09500"}}, {"cell_type": "markdown", "source": ["## Here's the main structure\n", "1. feature engineering: throw away useless feature, and transforming usable feature\n", "2. data cleaning and normalization (fill N/A, and separate categories)\n", "3. train/test separation\n", "4. neural network design\n", "5. train and predict"], "metadata": {"_uuid": "2b12150fa23a0ef10a0ef6c91a4a7f9b8b482791", "_cell_guid": "790cdcb9-4c7c-46b4-a7dd-d21fa1941584"}}, {"outputs": [], "cell_type": "code", "execution_count": null, "source": ["import numpy as np \n", "import pandas as pd \n", "%matplotlib inline\n", "import matplotlib.pyplot as plt\n", "import tensorflow as tf\n", "from tensorflow.python.framework import ops"], "metadata": {"_uuid": "797f943ab7df6ac0a6ad4db1c74554a699f5f79a", "collapsed": true, "_cell_guid": "6a678bbb-246e-4847-abf4-74f47eaa15b7"}}, {"outputs": [], "cell_type": "code", "execution_count": null, "source": ["train = pd.read_csv(\"../input/train.csv\")\n", "test = pd.read_csv(\"../input/test.csv\")\n", "# start with some EDA\n", "train.head()\n", "print(train['Embarked'].unique(), train['Pclass'].unique())\n", "train[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n"], "metadata": {"_uuid": "868fd262679aabeaa2ed66a5458575db031c4ef4", "_cell_guid": "32e2edd2-0b6d-4fd4-88a9-1c9dad1f4793"}}, {"outputs": [], "cell_type": "code", "execution_count": null, "source": ["# remove useless feature\n", "for df in [train, test]:\n", "    df.drop(labels=[\"PassengerId\", \"Cabin\", \"Name\", \"Ticket\"], axis=1, inplace=True)"], "metadata": {"_uuid": "e41acba602271f7dd46c7aa5b426a35a8a476a46", "collapsed": true, "_cell_guid": "667b674a-6b85-4005-af36-bda16419353f"}}, {"outputs": [], "cell_type": "code", "execution_count": null, "source": ["# fill missing value\n", "# another way is to use sklearn imputation to fill na\n", "for df in [train, test]:\n", "    for col in [\"Age\", \"Fare\"]:\n", "        df[col] = df[col].fillna(np.mean(df[col]))"], "metadata": {"_uuid": "3971e27842b9fa69a372139de97a94900f8523d2", "collapsed": true, "_cell_guid": "b19528a5-4779-408a-8c19-08a2b260fb45"}}, {"outputs": [], "cell_type": "code", "execution_count": null, "source": ["# data normalization\n", "from sklearn import preprocessing\n", "min_max_scaler = preprocessing.MinMaxScaler()\n", "for df in [train, test]:\n", "    for col in [\"Age\", \"Fare\"]:\n", "        x = df[[col]].values.astype(float)\n", "        df[col] = min_max_scaler.fit_transform(x)"], "metadata": {"_uuid": "90c82fc7b9198482a33962c4d19de80c3ca35ba2", "collapsed": true, "_cell_guid": "7a0a7efa-7be1-47f6-a83b-904aa344f35d"}}, {"outputs": [], "cell_type": "code", "execution_count": null, "source": ["# transform string\n", "for df in [train, test]:\n", "    # remove duplicate features to avoid overfitting\n", "    df['is_male'] = np.where(df['Sex']==\"male\", 1, 0)\n", "    df['is_female'] = np.where(df['Sex']==\"female\", 1, 0)\n", "    df['EmbarkedS'] = np.where(df['Embarked']==\"S\", 1, 0)\n", "    df['EmbarkedC'] = np.where(df['Embarked']==\"C\", 1, 0)\n", "    df['EmbarkedQ'] = np.where(df['Embarked']==\"Q\", 1, 0)\n", "    df['Pclass1'] = np.where(df['Pclass']==1, 1, 0)\n", "    df['Pclass2'] = np.where(df['Pclass']==2, 1, 0)\n", "    df['Pclass3'] = np.where(df['Pclass']==3, 1, 0)\n", "    df['is_single'] = np.where(np.logical_and(df['SibSp']==0, df['Parch']==0), 1, 0)\n", "\n", "# then remove transformed columns\n", "for df in [train, test]:\n", "    df.drop(labels=[\"Sex\", \"Embarked\", 'Pclass'], axis=1, inplace=True)"], "metadata": {"_uuid": "b56c6c541966746a8e227a6318706577571b74cc", "collapsed": true, "_cell_guid": "ca6c62b4-99de-4523-9c6c-0e052486f151"}}, {"outputs": [], "cell_type": "code", "execution_count": null, "source": ["# load up train/validation set! \n", "train_size = int(train.shape[0] * 0.85)\n", "\n", "train_dataset = train[:train_size]\n", "val_dataset = train[train_size:]\n", "\n", "X_train = train_dataset.drop(labels=[\"Survived\"], axis=1).values\n", "Y_train = train_dataset[\"Survived\"].values\n", "\n", "X_val = val_dataset.drop(labels=[\"Survived\"], axis=1).values\n", "Y_val = val_dataset[\"Survived\"].values\n", "\n", "input_size = len(train_dataset.columns) - 1  # number of final features "], "metadata": {"_uuid": "c70a7a39485eb7592c3bed9eaad59922640154d8", "collapsed": true, "_cell_guid": "dc07b57f-b4b1-4d65-9f96-2aaa85db0b2d"}}, {"outputs": [], "cell_type": "code", "execution_count": null, "source": ["X_train.shape\n", "train.head()"], "metadata": {"_uuid": "b8fd21f05842c2d4621ed46000e738a7f5a46826", "_cell_guid": "486f9e6e-fd4a-4d11-8e8c-af6cad88f15f"}}, {"outputs": [], "cell_type": "code", "execution_count": null, "source": ["from keras.models import Sequential\n", "from keras.layers import Dense,Activation,Dropout\n", "from keras.utils import np_utils\n", "from keras import optimizers, losses, metrics\n", "\n", "model = Sequential()\n", "k_init = 'glorot_uniform'\n", "optimizer = optimizers.Adam() #'rmsprop' #optimizers.Adam()\n", "\n", "model.add(Dense(64,input_dim=input_size, kernel_initializer=k_init))\n", "model.add(Activation(\"relu\"))\n", "model.add(Dropout(0.3))\n", "\n", "model.add(Dense(64, kernel_initializer=k_init))\n", "model.add(Activation(\"relu\"))\n", "model.add(Dropout(0.3))\n", "\n", "model.add(Dense(1, kernel_initializer=k_init))\n", "model.add(Activation(\"sigmoid\"))  # or softmax with category_crossentropy\n", "\n", "model.compile(loss=losses.binary_crossentropy, optimizer=optimizer, \n", "              metrics=[metrics.binary_accuracy]) \n", "\n", "history = model.fit(X_train, Y_train, validation_data=(X_val, Y_val),\n", "                    epochs=200, batch_size=100, \n", "                    verbose=0)\n", "# loss_and_metrics = model.evaluate(X_val, Y_val_cat)\n", "# print(loss_and_metrics)\n", "\n", "# pre-selected paramters\n", "# best_epochs = 200\n", "# best_batch_size = 5\n", "# best_init = 'glorot_uniform'\n", "# best_optimizer = 'rmsprop'"], "metadata": {"_uuid": "702e95c7f99f2d6dcc6e9fcf37ef72b164213a1c", "scrolled": false, "_cell_guid": "e0453809-34ca-4694-9128-28e0a3875ded"}}, {"outputs": [], "cell_type": "code", "execution_count": null, "source": ["acc = history.history['binary_accuracy']\n", "val_acc = history.history['val_binary_accuracy']\n", "loss = history.history['loss']\n", "val_loss = history.history['val_loss']\n", "\n", "epochs = range(1, len(acc) + 1)\n", "plt.plot(epochs, loss, 'bo', label='Training loss')\n", "plt.plot(epochs, val_loss,'b', label='Validation loss')\n", "\n", "plt.legend()\n", "plt.xlabel('Epochs')\n", "plt.ylabel('Loss')\n", "plt.show()\n", "\n", "plt.clf()\n", "\n", "epochs = range(1, len(acc) + 1)\n", "plt.plot(epochs, acc, 'bo', label='Training Acc')\n", "plt.plot(epochs, val_acc,'b', label='Validation Acc')\n", "plt.legend()\n", "plt.xlabel('Epochs')\n", "plt.ylabel('Acc')\n", "plt.show()"], "metadata": {"_uuid": "09e7a19be571a2fef51fc99cfcd3fd4508ea0cc4", "_cell_guid": "68a564f9-0918-45df-8308-2362bfd9d6e3"}}, {"outputs": [], "cell_type": "code", "execution_count": null, "source": ["y_final = model.predict_classes(test.values).reshape(-1) # ravel()\n", "df_test = pd.read_csv(\"../input/test.csv\")\n", "output = pd.DataFrame({'PassengerId': df_test['PassengerId'], 'Survived': y_final})\n", "surv_num = sum(output[\"Survived\"] != 0) / len(output)\n", "print(f\"Survive ratio: {surv_num}\")"], "metadata": {"_uuid": "484bce759e2f82a770cfc284b243e2629d4a4454", "_cell_guid": "1cb0856a-417c-447b-94ba-197606256ce5"}}, {"outputs": [], "cell_type": "code", "execution_count": null, "source": ["output.to_csv('prediction-ann.csv', index=False)\n", "output"], "metadata": {"_uuid": "451b62a454d70b45fefccc5e46f0681a62dc6c77", "_kg_hide-output": false, "_cell_guid": "6c6a6ccd-5735-4034-aa61-e732c318bdf4"}}]}