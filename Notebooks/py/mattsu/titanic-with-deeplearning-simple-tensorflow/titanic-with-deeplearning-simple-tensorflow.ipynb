{"nbformat_minor": 1, "cells": [{"cell_type": "markdown", "metadata": {"_uuid": "5dfbb511bef41aff1a9c1c5e983b2e39d9f829fc", "_cell_guid": "2fcf84fe-b8ad-454b-9041-db050a5e51f7"}, "source": ["## Introduction: \n", "This kernel use the raw tensorflow code, to learn the basics building blocks.\n", "From the result, we can see tensorflow neural networks can't achieve higher accuracy than 80%.\n", "Why? My conclusion is the data size is too small that DNN can't work well at all.\n", "\n", "However, here I represent a typical flow for tensorflow.\n", "\n", "As the traning dataset is **too small**, there're obvious **overfitting**\n", "have to try to avoid overfitting:\n", "- decrease features\n", "- add more data\n", "- regularization\n", "\n", "**My found is that, feature engineering is still the most important if dataset is so small.**"]}, {"cell_type": "markdown", "metadata": {"_uuid": "454b9eadb97258d1879bc072fa02553520f8acaa", "_cell_guid": "ee171d1b-5ae0-4a60-aac3-a33612aee67f"}, "source": ["## Here's the main structure\n", "1. feature engineering: throw away useless feature, and transforming usable feature\n", "2. data cleaning and normalization (fill N/A, and separate categories)\n", "3. train/test separation\n", "4. neural network design\n", "5. train and predict"]}, {"source": ["import numpy as np \n", "import pandas as pd \n", "%matplotlib inline\n", "import matplotlib.pyplot as plt\n", "import tensorflow as tf\n", "from tensorflow.python.framework import ops"], "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "797f943ab7df6ac0a6ad4db1c74554a699f5f79a", "_cell_guid": "6a678bbb-246e-4847-abf4-74f47eaa15b7"}, "outputs": [], "execution_count": 51}, {"source": ["train = pd.read_csv(\"../input/train.csv\")\n", "test = pd.read_csv(\"../input/test.csv\")\n", "# start with some EDA\n", "train.head()\n", "print(train['Embarked'].unique(), train['Pclass'].unique())\n", "train[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n"], "cell_type": "code", "metadata": {"_uuid": "868fd262679aabeaa2ed66a5458575db031c4ef4", "_cell_guid": "32e2edd2-0b6d-4fd4-88a9-1c9dad1f4793"}, "outputs": [], "execution_count": 52}, {"source": ["# remove useless feature\n", "for df in [train, test]:\n", "    df.drop(labels=[\"PassengerId\", \"Cabin\", \"Name\", \"Ticket\"], axis=1, inplace=True)"], "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "e41acba602271f7dd46c7aa5b426a35a8a476a46", "_cell_guid": "667b674a-6b85-4005-af36-bda16419353f"}, "outputs": [], "execution_count": 53}, {"source": ["# fill missing value\n", "# another way is to use sklearn imputation to fill na\n", "for df in [train, test]:\n", "    for col in [\"Age\", \"Fare\"]:\n", "        df[col] = df[col].fillna(np.mean(df[col]))"], "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "3971e27842b9fa69a372139de97a94900f8523d2", "_cell_guid": "b19528a5-4779-408a-8c19-08a2b260fb45"}, "outputs": [], "execution_count": 54}, {"source": ["# data normalization\n", "from sklearn import preprocessing\n", "min_max_scaler = preprocessing.MinMaxScaler()\n", "for df in [train, test]:\n", "    for col in [\"Age\", \"Fare\"]:\n", "        x = df[[col]].values.astype(float)\n", "        df[col] = min_max_scaler.fit_transform(x)"], "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "90c82fc7b9198482a33962c4d19de80c3ca35ba2", "_cell_guid": "7a0a7efa-7be1-47f6-a83b-904aa344f35d"}, "outputs": [], "execution_count": 55}, {"source": ["# transform string\n", "for df in [train, test]:\n", "    # remove duplicate features to avoid overfitting\n", "    #df['is_male'] = np.where(df['Sex']==\"male\", 1, 0)\n", "    df['is_female'] = np.where(df['Sex']==\"female\", 1, 0)\n", "    df['EmbarkedS'] = np.where(df['Embarked']==\"S\", 1, 0)\n", "    df['EmbarkedC'] = np.where(df['Embarked']==\"C\", 1, 0)\n", "    df['EmbarkedQ'] = np.where(df['Embarked']==\"Q\", 1, 0)\n", "    df['Pclass1'] = np.where(df['Pclass']==1, 1, 0)\n", "    df['Pclass2'] = np.where(df['Pclass']==2, 1, 0)\n", "    df['Pclass3'] = np.where(df['Pclass']==3, 1, 0)\n", "    df['is_single'] = np.where(np.logical_and(df['SibSp']==0, df['Parch']==0), 1, 0)\n", "\n", "# then remove transformed columns\n", "for df in [train, test]:\n", "    df.drop(labels=[\"Sex\", \"Embarked\", 'Pclass'], axis=1, inplace=True)"], "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "b56c6c541966746a8e227a6318706577571b74cc", "_cell_guid": "ca6c62b4-99de-4523-9c6c-0e052486f151"}, "outputs": [], "execution_count": 56}, {"source": ["# load up train/validation set! \n", "train_size = int(train.shape[0] * 0.85)\n", "\n", "train_dataset = train[:train_size]\n", "val_dataset = train[train_size:]\n", "\n", "X_train = train_dataset.drop(labels=[\"Survived\"], axis=1).values\n", "Y_train = train_dataset[\"Survived\"].values\n", "\n", "X_val = val_dataset.drop(labels=[\"Survived\"], axis=1).values\n", "Y_val = val_dataset[\"Survived\"].values\n", "\n", "input_size = len(train_dataset.columns) - 1  # number of final features "], "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "c70a7a39485eb7592c3bed9eaad59922640154d8", "_cell_guid": "dc07b57f-b4b1-4d65-9f96-2aaa85db0b2d"}, "outputs": [], "execution_count": 57}, {"source": ["X_train = X_train.reshape((X_train.shape[1], X_train.shape[0]))\n", "X_val = X_val.reshape((X_val.shape[1], X_val.shape[0]))\n", "\n", "Y_train = Y_train.reshape((1, Y_train.shape[0]))\n", "Y_val = Y_val.reshape((1, Y_val.shape[0]))"], "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "b5147b56e4211a19ea3812b12142284aca25d754", "_cell_guid": "2f86f027-adb3-4157-bb0c-98e7cafe2d97"}, "outputs": [], "execution_count": 58}, {"source": ["def create_placeholders(n_x, n_y):\n", "    X = tf.placeholder(tf.float32, shape=[n_x, None])\n", "    Y = tf.placeholder(tf.float32, shape=[n_y, None])\n", "    return X, Y\n", "\n", "def initialize_parameters():\n", "    # when I increase this to 2 times of input, the cost decrease rapidly\n", "    # when I increase this to 1000, boom! train acc get to 97%, but test acc is below 60%\n", "    output_size = 1\n", "    l1_size = 64 #int(input_size * 2) \n", "    l2_size = 64 #int(input_size * 1)\n", "    l3_size = output_size#int(input_size * 1)\n", "    l4_size = output_size\n", "    \n", "    W1 = tf.get_variable(\"W1\", [l1_size, input_size],\n", "                         initializer=tf.contrib.layers.xavier_initializer()) # seed=1\n", "    b1 = tf.get_variable(\"b1\", [l1_size, 1], initializer=tf.zeros_initializer())\n", "    W2 = tf.get_variable(\"W2\", [l2_size, l1_size], \n", "                         initializer=tf.contrib.layers.xavier_initializer())\n", "    b2 = tf.get_variable(\"b2\", [l2_size, 1], initializer=tf.zeros_initializer())\n", "    W3 = tf.get_variable(\"W3\", [l3_size, l2_size], \n", "                         initializer=tf.contrib.layers.xavier_initializer())\n", "    b3 = tf.get_variable(\"b3\", [l3_size, 1], initializer=tf.zeros_initializer())\n", "    W4 = tf.get_variable(\"W4\", [l4_size, l3_size], \n", "                         initializer=tf.contrib.layers.xavier_initializer())\n", "    b4 = tf.get_variable(\"b4\", [l4_size, 1], initializer=tf.zeros_initializer())\n", "\n", "    parameters = {\"W1\": W1,\n", "                  \"b1\": b1,\n", "                  \"W2\": W2,\n", "                  \"b2\": b2,\n", "                  \"W3\": W3,\n", "                  \"b3\": b3,\n", "                  \"W4\": W4,\n", "                  \"b4\": b4\n", "                 }\n", "\n", "    return parameters\n"], "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "9a3ce80562beb901607034d876f638d499013d50", "_cell_guid": "5e7fb842-b502-4262-b6c5-48f0f2e8cb17"}, "outputs": [], "execution_count": 59}, {"source": ["def forward_propagation(X, parameters):\n", "    \"\"\"\n", "    LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n", "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n", "                  the shapes are given in initialize_parameters\n", "    Z3 -- the output of the last LINEAR unit\n", "    \"\"\"\n", "    # Retrieve the parameters from the dictionary \"parameters\"\n", "    W1 = parameters['W1']\n", "    b1 = parameters['b1']\n", "    W2 = parameters['W2']\n", "    b2 = parameters['b2']\n", "    W3 = parameters['W3']\n", "    b3 = parameters['b3']\n", "    W4 = parameters['W4']\n", "    b4 = parameters['b4']\n", "\n", "    keep_prob = 0.7 #0.3  # if dropout too many, the predict result will be always 0\n", "    Z1 = tf.add(tf.matmul(W1, X), b1)  # Z1 = np.dot(W1, X) + b1\n", "    A1 = tf.nn.dropout(tf.nn.relu(Z1), keep_prob=keep_prob)  # A1 = relu(Z1)\n", "    Z2 = tf.add(tf.matmul(W2, A1), b2)  # Z2 = np.dot(W2, a1) + b2\n", "    A2 = tf.nn.dropout(tf.nn.relu(Z2), keep_prob=keep_prob)  # A2 = relu(Z2)\n", "    Z3 = tf.add(tf.matmul(W3, A2), b3)  # Z3 = np.dot(W3,Z2) + b3\n", "    A3 = tf.nn.dropout(tf.nn.relu(Z3), keep_prob=keep_prob)  \n", "    Z4 = tf.add(tf.matmul(W4, A3), b4)  \n", "\n", "    return Z3\n", "\n", "def forward_propagation_for_predict(X, parameters):\n", "    \"\"\"\n", "    Returns: Z3 -- the output of the last LINEAR unit\n", "    \"\"\"\n", "    W1 = parameters['W1']\n", "    b1 = parameters['b1']\n", "    W2 = parameters['W2']\n", "    b2 = parameters['b2']\n", "    W3 = parameters['W3']\n", "    b3 = parameters['b3']\n", "    W4 = parameters['W4']\n", "    b4 = parameters['b4']\n", "    # Numpy Equivalents:\n", "    Z1 = tf.add(tf.matmul(W1, X), b1)  # Z1 = np.dot(W1, X) + b1\n", "    A1 = tf.nn.relu(Z1)  # A1 = relu(Z1)\n", "    Z2 = tf.add(tf.matmul(W2, A1), b2)  # Z2 = np.dot(W2, a1) + b2\n", "    A2 = tf.nn.relu(Z2)  # A2 = relu(Z2)\n", "    Z3 = tf.add(tf.matmul(W3, A2), b3)  # Z3 = np.dot(W3,Z2) + b3\n", "    A3 = tf.nn.relu(Z3)  # A2 = relu(Z2)\n", "    Z4 = tf.add(tf.matmul(W4, A3), b4)  # Z3 = np.dot(W3,Z2) + b3\n", "\n", "    return Z3\n"], "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "c9b8999a3ac37a33d3f3aca6babfc975d8c4e3c7", "_cell_guid": "3a694d5b-2f28-4b1f-858d-ef2362935f02"}, "outputs": [], "execution_count": 60}, {"source": ["def compute_cost(Z3, Y):\n", "    \"\"\"\n", "    Z3 -- output of forward propagation (output of the last LINEAR unit)\n", "    Y -- \"true\" labels vector placeholder, same shape as Z3\n", "    Return: cost - Tensor of the cost function\n", "    \"\"\"\n", "    # to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n", "    logits = tf.transpose(Z3)\n", "    labels = tf.transpose(Y)\n", "    cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels))\n", "    return cost\n"], "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "dbda0697203096cbd88fbda066babbb442058fb0", "_cell_guid": "79f57c28-ff61-4b89-8081-e2b6f631e327"}, "outputs": [], "execution_count": 61}, {"source": ["def model(X_train, Y_train, X_val, Y_val, learning_rate=0.0001,\n", "          num_epochs=1500, print_cost=True):\n", "    \"\"\"\n", "    Returns:\n", "    parameters -- parameters learnt by the model. They can then be used to predict.\n", "    \"\"\"\n", "    ops.reset_default_graph()  # to be able to rerun the model without overwriting tf variables\n", "    (n_x, m) = X_train.shape  # (n_x: input size, m : number of examples in the train set)\n", "    n_y = Y_train.shape[0]  # n_y : output size\n", "    costs, val_losses = [], []  # To keep track of the cost\n", "\n", "    X, Y = create_placeholders(n_x, n_y)\n", "    parameters = initialize_parameters()\n", "\n", "    # Forward propagation: Build the forward propagation in the tensorflow graph\n", "    Z3 = forward_propagation(X, parameters)\n", "\n", "    # Cost function: Add cost function to tensorflow graph\n", "    cost = compute_cost(Z3, Y)\n", "    val_cost = compute_cost(forward_propagation_for_predict(X, parameters), Y)\n", "\n", "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n", "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n", "\n", "    # Initialize all the variables\n", "    init = tf.global_variables_initializer()\n", "    # Start the session to compute the tensorflow graph\n", "    with tf.Session() as sess:\n", "        sess.run(init)\n", "\n", "        # Do the training loop\n", "        for epoch in range(num_epochs):\n", "            _, epoch_cost = sess.run([optimizer, cost],\n", "                                     feed_dict={X: X_train, Y: Y_train})\n", "\n", "            # Print the cost every epoch\n", "            if print_cost == True and epoch % 5 == 0:\n", "                epoch_val_loss = sess.run(val_cost, feed_dict={X: X_val, Y: Y_val})\n", "                if epoch % 100 == 0:\n", "                    print (\"Loss after epoch %i: training[%f] | dev[%f]\" % (epoch, epoch_cost, epoch_val_loss))\n", "\n", "                val_losses.append(epoch_val_loss)\n", "                costs.append(epoch_cost)\n", "\n", "        # plot the cost\n", "        plt.plot(np.squeeze(costs))\n", "        plt.plot(np.squeeze(val_losses))\n", "        plt.legend([\"training\", \"test\"])\n", "        plt.xlabel('iterations (per tens)')\n", "        plt.title(\"Learning rate =\" + str(learning_rate))\n", "        plt.show()\n", "\n", "        # lets save the parameters in a variable\n", "        parameters = sess.run(parameters)\n", "        print (\"Parameters have been trained!\")\n", "\n", "        # Calculate the correct predictions\n", "        y_hat = tf.cast(tf.greater(tf.sigmoid(Z3),0.5), tf.float32)\n", "        correct_prediction = tf.equal(y_hat, Y)\n", "\n", "        # Calculate accuracy on the test set\n", "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n", "\n", "        print (\"Train Accuracy: \", \"{:3.2f}%\".format(100*accuracy.eval({X: X_train, Y: Y_train})))\n", "        print (\"Test Accuracy:\", \"{:3.2f}%\".format(100*accuracy.eval({X: X_val, Y: Y_val})))\n", "\n", "        return parameters\n"], "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "784dba0fc11a95dd0543581c2d1db99e05c3ca92", "_cell_guid": "06934c86-7949-412c-9c0a-98303c3f4101"}, "outputs": [], "execution_count": 62}, {"source": ["parameters = model(X_train, Y_train, X_val, Y_val,learning_rate=0.001, num_epochs=2000)"], "cell_type": "code", "metadata": {"scrolled": false, "_uuid": "30c3ae49963ab71344651ca7b6ac88ed95d8e49c", "_cell_guid": "3d502fb3-d5ef-411c-bd2e-748308abd7a2"}, "outputs": [], "execution_count": 63}, {"source": ["\n", "def predict(X, parameters):\n", "    W1 = tf.convert_to_tensor(parameters[\"W1\"])\n", "    b1 = tf.convert_to_tensor(parameters[\"b1\"])\n", "    W2 = tf.convert_to_tensor(parameters[\"W2\"])\n", "    b2 = tf.convert_to_tensor(parameters[\"b2\"])\n", "    W3 = tf.convert_to_tensor(parameters[\"W3\"])\n", "    b3 = tf.convert_to_tensor(parameters[\"b3\"])\n", "    W4 = tf.convert_to_tensor(parameters[\"W4\"])\n", "    b4 = tf.convert_to_tensor(parameters[\"b4\"])\n", "\n", "    params = {\"W1\": W1,\n", "              \"b1\": b1,\n", "              \"W2\": W2,\n", "              \"b2\": b2,\n", "              \"W3\": W3,\n", "              \"b3\": b3,\n", "             \"W4\": W4,\n", "              \"b4\": b4}\n", "\n", "    x = tf.placeholder(\"float\", [input_size, None])\n", "\n", "    z3 = forward_propagation_for_predict(x, params)\n", "    p = tf.sigmoid(z3) \n", "\n", "    sess = tf.Session()\n", "    prediction = sess.run(p, feed_dict={x: X})\n", "\n", "    return prediction"], "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "f899f039ac84863c6ea0d4b9a7e8ef225d30f204", "_cell_guid": "a99f7353-0dc3-46a6-b2de-4589cef12427"}, "outputs": [], "execution_count": 64}, {"source": ["# load up test set\n", "final_test = test.values.T\n", "y_pred = predict(final_test, parameters)\n", "\n", "y_final = (y_pred > 0.5).astype(int).ravel()  #.reshape(X_val.shape[0])"], "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "01619f3eabfdce1b4d8576be9a020367f3b7d34f", "_cell_guid": "406f442e-3782-4602-9809-806943d0d766"}, "outputs": [], "execution_count": 65}, {"source": ["df_test = pd.read_csv(\"../input/test.csv\")\n", "output = pd.DataFrame({'PassengerId': df_test['PassengerId'], 'Survived': y_final})\n", "# output[\"Survived\"].unique()\n", "surv_num = sum(output[\"Survived\"] != 0) / len(output)\n", "print(f\"Survive ratio: {surv_num}\")"], "cell_type": "code", "metadata": {"_uuid": "484bce759e2f82a770cfc284b243e2629d4a4454", "_cell_guid": "1cb0856a-417c-447b-94ba-197606256ce5"}, "outputs": [], "execution_count": 66}, {"source": ["output.to_csv('prediction-ann.csv', index=False)\n", "output"], "cell_type": "code", "metadata": {"_uuid": "451b62a454d70b45fefccc5e46f0681a62dc6c77", "_cell_guid": "6c6a6ccd-5735-4034-aa61-e732c318bdf4", "_kg_hide-output": false}, "outputs": [], "execution_count": 67}, {"source": [], "cell_type": "code", "metadata": {"collapsed": true, "_uuid": "835b4b60ea0bf9e3ec5cc64db67f1eb7b2dc2b09", "_cell_guid": "06db8995-fade-4a44-a0c3-19caf39892d4"}, "outputs": [], "execution_count": null}], "metadata": {"language_info": {"file_extension": ".py", "mimetype": "text/x-python", "version": "3.6.4", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "codemirror_mode": {"version": 3, "name": "ipython"}, "name": "python"}, "kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}}, "nbformat": 4}