{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf #build mode\nimport matplotlib.pyplot as plt\nfrom tensorflow.python.data import Dataset #organize input data\nimport functools\nimport datetime\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"#define data file path and columns\nTRAIN_PATH = '../input/train.csv'\nTEST_PATH = '../input/test.csv'\n\nCSV_COLUMNS_TRAIN = ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age',\n                'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\nCSV_COLUMNS_TEST = ['PassengerId', 'Pclass', 'Name', 'Sex', 'Age',\n                'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\nDROP_COLUMNS = ['PassengerId', 'Cabin', 'Ticket']\nTARGET_COLUMN = ['Survived']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"35feb52edc9b31c92dec8a1fb57fa9aed5ca2dbc"},"cell_type":"code","source":"def load_data():\n    train_data = pd.read_csv(TRAIN_PATH, header=0,\n                             names=CSV_COLUMNS_TRAIN)\n\n    #shuffule the training data before split it to train and validation dataset\n    train_data.reindex(np.random.permutation(train_data.index))\n\n    test_data = pd.read_csv(TEST_PATH, header=0, names=CSV_COLUMNS_TEST)\n\n    raw_data = [train_data, test_data]\n    for data in raw_data:\n        data.drop(columns=DROP_COLUMNS, inplace=True)\n        data['Age'].fillna(data['Age'].median(), inplace=True)\n        data['Fare'].fillna(data['Fare'].median(), inplace=True)\n        data['Embarked'].fillna(data['Embarked'].mode()[0], inplace=True)\n        data['Title'] = data['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n        data.drop(columns=['Name'], inplace=True)\n        if 'Survived' in data:\n            data.dropna(subset=['Survived'], inplace=True)\n\n    train_features, train_label = train_data, train_data.pop('Survived')\n    return train_features, train_label, test_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c91521e8e405ff0fce7a17e324147c25bf340e78"},"cell_type":"code","source":"    train_features, train_label,  test_features = load_data()\n    all_features = train_features.append(test_features)\n    print(all_features.info())\n    print(train_label.describe(include = 'all'))\n    print(all_features.describe())\n    print(all_features.head(10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1be73ab384c44d499b94f490e83fc2feb73ca21f"},"cell_type":"code","source":"def input_fn(features, label, batch_size=10, epochs=None, shuffle=True):\n    if label is None:\n        inputs = dict(features)\n    else:\n        inputs = (dict(features), label)\n    dataset = Dataset.from_tensor_slices(inputs)\n    if shuffle:\n        dataset = dataset.shuffle(1000)\n    dataset = dataset.repeat(epochs).batch(batch_size)\n    return dataset.make_one_shot_iterator().get_next()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2c759b3e5f083493446c4c31988801d6009d0fa0"},"cell_type":"code","source":"def construct_feature_columns(features):\n    '''\n    assemble feature columns\n    '''\n    return [\n        tf.feature_column.indicator_column(\n            tf.feature_column.categorical_column_with_vocabulary_list('Pclass', [1,2,3])),\n        tf.feature_column.indicator_column(\n            tf.feature_column.categorical_column_with_vocabulary_list('Sex', ['male', 'female'])\n        ),\n        tf.feature_column.bucketized_column(\n            tf.feature_column.numeric_column('Age'),\n            boundaries= list(np.percentile(features['Age'], np.arange(10,100,20)))\n        ),\n        tf.feature_column.indicator_column(\n            tf.feature_column.categorical_column_with_identity('SibSp', num_buckets=8, default_value=0)\n        ),\n        tf.feature_column.indicator_column(\n            tf.feature_column.categorical_column_with_identity('Parch', num_buckets=5, default_value=0)\n        ),\n        tf.feature_column.bucketized_column(\n            tf.feature_column.numeric_column('Fare'), boundaries=list(np.percentile(features['Fare'], np.arange(10,100,20)))\n        ),\n        tf.feature_column.indicator_column(\n            tf.feature_column.categorical_column_with_vocabulary_list('Embarked', ['S', 'C', 'Q'])\n        ),\n        tf.feature_column.indicator_column(\n            tf.feature_column.categorical_column_with_vocabulary_list('Title', ['Mr', 'Mrs','Miss', 'Master'])\n        )\n    ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1f74178f03a13a6db62821ada5b46a44bee6a471"},"cell_type":"code","source":"def write_predict_csv(predicts):\n    '''\n    write predicts to gender_submission.csv\n    '''\n    test_data = pd.read_csv(TEST_PATH, header=0, names=CSV_COLUMNS_TEST)\n    predict_df = pd.DataFrame()\n    predict_df['PassengerId'] = test_data['PassengerId']\n    predict_df['Survived'] = predicts\n    predict_df.to_csv('gender_submission.csv', index=False)\n    pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0a6779c4f01fe41abac8ee1e64ca4bff3403fd71"},"cell_type":"code","source":"    train_features, train_label, test_features = load_data()\n\n    startIdx = int(len(train_features) * 0.75)\n\n    train_input_fn = functools.partial(input_fn, train_features[:startIdx], train_label[:startIdx],\n                                              batch_size=50)\n    train_eval_input_fn = functools.partial(input_fn, train_features[:startIdx], train_label[:startIdx],\n                                             epochs=1, shuffle=False)\n    validation_input_fn = functools.partial(input_fn, train_features[startIdx:], train_label[startIdx:],\n                                                            epochs=1, shuffle=False)\n    test_input_fn = functools.partial(input_fn, test_features, None, epochs=1, shuffle=False)\n\n    my_run_config = tf.estimator.RunConfig(save_checkpoints_secs=5)\n\n    my_estimator = tf.estimator.DNNClassifier(\n        hidden_units=[16, 16],\n        feature_columns=construct_feature_columns(train_features),\n        model_dir='modedir',\n        config=my_run_config\n    )\n\n    steps_per_period = 5\n    periods = 20\n    train_losses = []\n    validation_losses = []\n    train_accuracy = []\n    validation_accuracy = []\n\n    for period in range(periods):\n\n        my_estimator.train(train_input_fn, steps=steps_per_period)\n        train_result = my_estimator.evaluate(train_eval_input_fn)\n        validation_result = my_estimator.evaluate(validation_input_fn)\n        train_losses.append(train_result['average_loss'])\n        validation_losses.append(validation_result['average_loss'])\n        train_accuracy.append(train_result['accuracy'])\n        validation_accuracy.append(validation_result['accuracy'])\n        print('train loss is {}, and val loss is {}'.format(train_result['average_loss'], validation_result['average_loss']))\n        print('train acc is {}, and val acc is {}'.format(train_result['accuracy'], validation_result['accuracy']))\n\n    predict_result = my_estimator.predict(test_input_fn)\n    predicts = []\n    for predict in predict_result:\n        predicts.append(predict['probabilities'])\n    predicts = np.argmax(predicts, axis=1)\n    write_predict_csv(predicts)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}