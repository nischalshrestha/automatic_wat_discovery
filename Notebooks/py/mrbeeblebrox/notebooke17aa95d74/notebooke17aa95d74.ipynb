{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "66905fa2-c8cb-e86e-4459-27ac850b9001"
      },
      "source": [
        "1.1 Setup and Load Dataset\n",
        "-------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ba713a4a-3710-92a3-0e51-5b3824bb3f7d"
      },
      "outputs": [],
      "source": [
        "# Load in our libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import sklearn\n",
        "import xgboost as xgb\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import plotly.offline as py\n",
        "py.init_notebook_mode(connected=True)\n",
        "import plotly.graph_objs as go\n",
        "import plotly.tools as tls\n",
        "\n",
        "# Going to use these 5 base models for the stacking\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.cross_validation import KFold;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f50e4fe0-48ab-e96e-e94c-089f380d3c2d"
      },
      "outputs": [],
      "source": [
        "# Load in the train and test datasets\n",
        "train = pd.read_csv('../input/train.csv')\n",
        "test = pd.read_csv('../input/test.csv')\n",
        "\n",
        "\n",
        "\n",
        "# Store our passenger ID for easy access\n",
        "PassengerId = test['PassengerId']\n",
        "\n",
        "#Check to see what we have\n",
        "train.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "cb7232d5-388f-f21b-306f-042af92bba6f"
      },
      "source": [
        "2.1 Preprocess Data into One Normalized Set\n",
        "----------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "fb8c10f0-7f84-4da1-28dc-94d64fe0ae4d"
      },
      "outputs": [],
      "source": [
        "trainONS = train.copy()\n",
        "testONS = test.copy()\n",
        "\n",
        "full_data = [trainONS, testONS]\n",
        "drop_elementsONS = list()\n",
        "#preprocessing will normalize and digitize all possible information into a number between 0 and 1\n",
        "#this should allow use to use the data in any type of model (specifically neural nets)\n",
        "\n",
        "\n",
        "#Change Pclass to numbers between 0 and 1 \n",
        "for dataset in full_data:\n",
        "    dataset['Class1'] = dataset[\"Pclass\"].apply(lambda x: 0 if x != 1 else 1)\n",
        "    dataset['Class2'] = dataset[\"Pclass\"].apply(lambda x: 0 if x != 2 else 1)\n",
        "    dataset['Class3'] = dataset[\"Pclass\"].apply(lambda x: 0 if x != 3 else 1)\n",
        "\n",
        "drop_elementsONS.append('Pclass')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define function to extract titles from passenger names\n",
        "def get_title(name):\n",
        "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
        "    # If the title exists, extract and return it.\n",
        "    if title_search:\n",
        "        return title_search.group(1)\n",
        "    return \"\"\n",
        "\n",
        "#Must get the title and parse\n",
        "for dataset in full_data:\n",
        "    dataset['Title'] = dataset['Name'].apply(get_title)\n",
        "    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona', ''], 'Rare')\n",
        "    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n",
        "    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n",
        "    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n",
        "    \n",
        "    dataset['isMrTitle'] = dataset[\"Title\"].apply(lambda x: 0 if x != 'Mr' else 1)\n",
        "    dataset['isMissTitle'] = dataset[\"Title\"].apply(lambda x: 0 if x != 'Miss' else 1)\n",
        "    dataset['isMrsTitle'] = dataset[\"Title\"].apply(lambda x: 0 if x != 'Mrs' else 1)\n",
        "    dataset['isRareTitle'] = dataset[\"Title\"].apply(lambda x: 0 if x != 'Rare' else 1)\n",
        "\n",
        "drop_elementsONS.append('Title')\n",
        "drop_elementsONS.append('Name')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Make gender into numbers 0 == female, 1 == male (not sexist)\n",
        "for dataset in full_data:\n",
        "    dataset['Gender'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n",
        "    \n",
        "    \n",
        "drop_elementsONS.append('Sex')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "#Find family information\n",
        "for dataset in full_data:\n",
        "    #todo Normalize\n",
        "    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n",
        "    dataset['IsAlone'] = 0\n",
        "    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n",
        "    #Maybe try getting if parents or children\n",
        "\n",
        "    \n",
        "#Embarked parsing\n",
        "for dataset in full_data:\n",
        "    dataset['EmbarkedS'] = dataset[\"Embarked\"].apply(lambda x: 0 if x != 'S' else 1)\n",
        "    dataset['EmbarkedC'] = dataset[\"Embarked\"].apply(lambda x: 0 if x != 'C' else 1)\n",
        "    dataset['EmbarkedQ'] = dataset[\"Embarked\"].apply(lambda x: 0 if x != 'Q' else 1)\n",
        "    \n",
        "drop_elementsONS.append('Embarked')  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Find Per Person Fare using the tickets\n",
        "combinedData = trainONS.copy().append(testONS.copy(), ignore_index=True)\n",
        "for dataset in full_data:\n",
        "    dataset['PerPersonFare'] = dataset['Fare']\n",
        "    for rowIndex in range(0, dataset['PassengerId'].size):\n",
        "        selectedTicket = dataset['Ticket'][rowIndex]\n",
        "        numOfTickets = combinedData['Ticket'].value_counts()[selectedTicket]\n",
        "        dataset.set_value(rowIndex, 'PerPersonFare', dataset['Fare'][rowIndex]/numOfTickets)\n",
        "        #people per ticket should pretty much be family size\n",
        "        dataset.set_value(rowIndex, 'PeoplePerTicket', numOfTickets)\n",
        "        \n",
        "        if dataset['Fare'][rowIndex] != dataset['Fare'][rowIndex]:\n",
        "            if dataset['Class1'][rowIndex] == 1:\n",
        "                dataset.set_value(rowIndex, 'PerPersonFare', 26)\n",
        "            elif dataset['Class2'][rowIndex] == 1:\n",
        "                dataset.set_value(rowIndex, 'PerPersonFare', 13)\n",
        "            elif dataset['Class3'][rowIndex] == 1:\n",
        "                dataset.set_value(rowIndex, 'PerPersonFare', 7)\n",
        "            print(dataset['PerPersonFare'][rowIndex])\n",
        "        #todo Normalize\n",
        "        \n",
        "        \n",
        "drop_elementsONS.append('Fare')        \n",
        "drop_elementsONS.append('Ticket')  \n",
        "#drop_elements.append('NumOfTickets')  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Age editing\n",
        "for dataset in full_data:\n",
        "    dataset['EditedAge'] = dataset[\"Age\"].apply(lambda x: 0 if x != x else x)\n",
        "    dataset['hasAge'] = dataset[\"Age\"].apply(lambda x: 0 if x != x else 1)\n",
        "    #todo Normalize\n",
        "    #can also check to see if parch == 2 and People per ticket != 4 if that is case than probably a child\n",
        "\n",
        "drop_elementsONS.append('Age')  \n",
        "\n",
        "\n",
        "# Define function to extract cabin number\n",
        "def get_cabin_num(name):\n",
        "    if type(name) != float:\n",
        "        cabin_search = re.search('([0-9]+)', name)\n",
        "        \n",
        "        if cabin_search:\n",
        "            \n",
        "            return cabin_search.group(0)\n",
        "    return \"0\"\n",
        "\n",
        "def get_cabin_letter(name):\n",
        "    if type(name) != float:\n",
        "        cabin_search = re.search('[A-G][0-9]{1,4}', name)\n",
        "        if cabin_search:\n",
        "            cabin_search = re.search('[A-G]', cabin_search.group(0))\n",
        "            if cabin_search:\n",
        "                return cabin_search.group(0)\n",
        "    return \"\"\n",
        "\n",
        "for dataset in full_data:\n",
        "    dataset['CabinNum'] = dataset['Cabin'].apply(get_cabin_num)\n",
        "    dataset['isCabinLetters'] = dataset['Cabin'].apply(get_cabin_letter)\n",
        "    dataset['isCabinA'] = dataset['isCabinLetters'].apply(lambda x: 0 if x != 'A' else 1)\n",
        "    dataset['isCabinB'] = dataset['isCabinLetters'].apply(lambda x: 0 if x != 'B' else 1)\n",
        "    dataset['isCabinC'] = dataset['isCabinLetters'].apply(lambda x: 0 if x != 'C' else 1)\n",
        "    dataset['isCabinD'] = dataset['isCabinLetters'].apply(lambda x: 0 if x != 'D' else 1)\n",
        "    dataset['isCabinE'] = dataset['isCabinLetters'].apply(lambda x: 0 if x != 'E' else 1)\n",
        "    dataset['isCabinF'] = dataset['isCabinLetters'].apply(lambda x: 0 if x != 'F' else 1)\n",
        "    dataset['isCabinG'] = dataset['isCabinLetters'].apply(lambda x: 0 if x != 'G' else 1)\n",
        "\n",
        "    \n",
        "drop_elementsONS.append('Cabin')     \n",
        "drop_elementsONS.append('isCabinLetters')  \n",
        "\n",
        "trainONS = trainONS.drop(drop_elementsONS, axis = 1)\n",
        "testONS = testONS.drop(drop_elementsONS, axis = 1)\n",
        "\n",
        "testONS.head(500)\n",
        "#train['perPersonFare'] = train['Fare']/train['Ticket'].value_counts();\n",
        "#train[['editedFare', 'Fare']].head(500)\n",
        "#train = train.drop(drop_elements, axis = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "dbd42673-f749-7588-b1a8-3da6c0dabbd4"
      },
      "source": [
        "## 2.2 Get Age Ranges ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4eb7de7a-8fa8-1fa8-6040-b451cbbe6a6f"
      },
      "outputs": [],
      "source": [
        "\n",
        "trainONSWAR = trainONS.copy()\n",
        "testONSWAR = trainONS.copy()\n",
        "\n",
        "full_data = [trainONSWAR, testONSWAR]\n",
        "drop_elementsONSWAR = list()\n",
        "\n",
        "for dataset in full_data:\n",
        "    for rowIndex in range(0, dataset['PassengerId'].size):\n",
        "        dataset.set_value(rowIndex, 'Age0to4', 0)\n",
        "        dataset.set_value(rowIndex, 'Age5to10', 0)\n",
        "        dataset.set_value(rowIndex, 'Age11to13', 0)\n",
        "        dataset.set_value(rowIndex, 'Age14to18', 0)\n",
        "        dataset.set_value(rowIndex, 'Age19to22', 0)\n",
        "        dataset.set_value(rowIndex, 'Age23to29', 0)\n",
        "        dataset.set_value(rowIndex, 'Age30to42', 0)\n",
        "        dataset.set_value(rowIndex, 'Age43to55', 0)\n",
        "        dataset.set_value(rowIndex, 'Age56to65', 0)\n",
        "        dataset.set_value(rowIndex, 'Age66up', 0)\n",
        "        if dataset['hasAge'][rowIndex] == 1:\n",
        "            if dataset['EditedAge'][rowIndex] <= 4:\n",
        "                dataset.set_value(rowIndex, 'Age0to4', 1)\n",
        "            elif dataset['EditedAge'][rowIndex] <= 10:\n",
        "                dataset.set_value(rowIndex, 'Age5to10', 1)\n",
        "            elif dataset['EditedAge'][rowIndex] <= 13:\n",
        "                dataset.set_value(rowIndex, 'Age11to13', 1)\n",
        "            elif dataset['EditedAge'][rowIndex] <= 18:\n",
        "                dataset.set_value(rowIndex, 'Age14to18', 1)\n",
        "            elif dataset['EditedAge'][rowIndex] <= 22:\n",
        "                dataset.set_value(rowIndex, 'Age19to22', 1)\n",
        "            elif dataset['EditedAge'][rowIndex] <= 29:\n",
        "                dataset.set_value(rowIndex, 'Age23to29', 1)\n",
        "            elif dataset['EditedAge'][rowIndex] <= 42:\n",
        "                dataset.set_value(rowIndex, 'Age30to42', 1)\n",
        "            elif dataset['EditedAge'][rowIndex] <= 55:\n",
        "                dataset.set_value(rowIndex, 'Age43to55', 1)\n",
        "            elif dataset['EditedAge'][rowIndex] <= 65:\n",
        "                dataset.set_value(rowIndex, 'Age56to65', 1)\n",
        "            elif dataset['EditedAge'][rowIndex] <= 120:\n",
        "                dataset.set_value(rowIndex, 'Age66up', 1)\n",
        "                \n",
        "drop_elementsONSWAR.append('hasAge')\n",
        "drop_elementsONSWAR.append('EditedAge')\n",
        "\n",
        "trainONSWAR = trainONSWAR.drop(drop_elementsONSWAR, axis = 1)\n",
        "testONSWAR = testONSWAR.drop(drop_elementsONSWAR, axis = 1)\n",
        "\n",
        "trainONSWAR.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "17b688c6-dac3-d9d9-fa95-ab468bdbb513"
      },
      "source": [
        "## 2.3 Get No Missing Data Set ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d26546b1-5649-aa47-46be-52a41e94480a"
      },
      "outputs": [],
      "source": [
        "trainNMD = trainONS.copy()\n",
        "testNMD = trainONS.copy()\n",
        "\n",
        "drop_elementsNMD = list()\n",
        "\n",
        "\n",
        "drop_elementsNMD.append('hasAge')\n",
        "drop_elementsNMD.append('EditedAge')\n",
        "drop_elementsNMD.append('CabinNum')\n",
        "drop_elementsNMD.append('isCabinA')\n",
        "drop_elementsNMD.append('isCabinB')\n",
        "drop_elementsNMD.append('isCabinC')\n",
        "drop_elementsNMD.append('isCabinD')\n",
        "drop_elementsNMD.append('isCabinE')\n",
        "drop_elementsNMD.append('isCabinF')\n",
        "drop_elementsNMD.append('isCabinG')\n",
        "drop_elementsNMD.append('EmbarkedC')\n",
        "drop_elementsNMD.append('EmbarkedS')\n",
        "drop_elementsNMD.append('EmbarkedQ')\n",
        "\n",
        "\n",
        "trainNMD = trainNMD.drop(drop_elementsNMD, axis = 1)\n",
        "testNMD = testNMD.drop(drop_elementsNMD, axis = 1)\n",
        "\n",
        "trainNMD.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "51e3eca3-07b9-9eb7-31fd-6aecdf0cab0a"
      },
      "source": [
        "## 2.4 Get Missing Data Set ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a285e49f-47ab-69b2-e437-7ca32127494c"
      },
      "outputs": [],
      "source": ""
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "6a72a688-372b-84ae-45e8-8b8e592dca1e"
      },
      "source": [
        "## 3.1 Build Models ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "092162f5-48db-b1db-5d99-9b709ec68d96"
      },
      "outputs": [],
      "source": [
        "# Some useful parameters which will come in handy later on\n",
        "ntrainONS = trainONS.shape[0]\n",
        "ntestONS = testONS.shape[0]\n",
        "\n",
        "ntrainONSWAR = trainONSWAR.shape[0]\n",
        "ntestONSWAR = testONSWAR.shape[0]\n",
        "\n",
        "ntrainNMD = trainNMD.shape[0]\n",
        "ntestNMD = testNMD.shape[0]\n",
        "\n",
        "\n",
        "SEED = 0 # for reproducibility\n",
        "NFOLDS = 5 # set folds for out-of-fold prediction\n",
        "kfONS = KFold(ntrainONS, n_folds= NFOLDS, random_state=SEED)\n",
        "kfONSWAR = KFold(ntrainONSWAR, n_folds= NFOLDS, random_state=SEED)\n",
        "kfNMD = KFold(ntrainNMD, n_folds= NFOLDS, random_state=SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0b20b3f8-02c4-0f7b-c65b-ef27b2e318fb"
      },
      "outputs": [],
      "source": [
        "# Class to extend the Sklearn classifier\n",
        "class SklearnHelper(object):\n",
        "    def __init__(self, clf, seed=0, params=None):\n",
        "        params['random_state'] = seed\n",
        "        self.clf = clf(**params)\n",
        "\n",
        "    def train(self, x_train, y_train):\n",
        "        self.clf.fit(x_train, y_train)\n",
        "\n",
        "    def predict(self, x):\n",
        "        return self.clf.predict(x)\n",
        "    \n",
        "    def fit(self,x,y):\n",
        "        return self.clf.fit(x,y)\n",
        "    \n",
        "    def feature_importances(self,x,y):\n",
        "        print(self.clf.fit(x,y).feature_importances_)\n",
        "    \n",
        "# Class to extend XGboost classifer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "eac71e26-167d-f236-312f-14964d4ef0c5"
      },
      "outputs": [],
      "source": [
        "def get_oofONS(clf, x_train, y_train, x_test):\n",
        "    oof_train = np.zeros((ntrainONS,))\n",
        "    oof_test = np.zeros((ntestONS,))\n",
        "    oof_test_skf = np.empty((NFOLDS, ntestONS))\n",
        "\n",
        "    for i, (train_index, test_index) in enumerate(kfONS):\n",
        "        x_tr = x_train[train_index]\n",
        "        y_tr = y_train[train_index]\n",
        "        x_te = x_train[test_index]\n",
        "\n",
        "        clf.train(x_tr, y_tr)\n",
        "\n",
        "        oof_train[test_index] = clf.predict(x_te)\n",
        "        oof_test_skf[i, :] = clf.predict(x_test)\n",
        "\n",
        "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
        "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)\n",
        "\n",
        "def get_oofONSWAR(clf, x_train, y_train, x_test):\n",
        "    oof_train2 = np.zeros((ntrainONSWAR,))\n",
        "    oof_test2 = np.zeros((ntestONSWAR,))\n",
        "    oof_test_skf2 = np.empty((NFOLDS, ntestONSWAR))\n",
        "\n",
        "    for i, (train_index, test_index) in enumerate(kfONSWAR):\n",
        "        x_tr = x_train[train_index]\n",
        "        y_tr = y_train[train_index]\n",
        "        x_te = x_train[test_index]\n",
        "\n",
        "        clf.train(x_tr, y_tr)\n",
        "\n",
        "        oof_train2[test_index] = clf.predict(x_te)\n",
        "        oof_test_skf2[i, :] = clf.predict(x_test)\n",
        "\n",
        "    oof_test2[:] = oof_test_skf2.mean(axis=0)\n",
        "    return oof_train2.reshape(-1, 1), oof_test2.reshape(-1, 1)\n",
        "\n",
        "def get_oofNMD(clf, x_train, y_train, x_test):\n",
        "    oof_train = np.zeros((ntrainNMD,))\n",
        "    oof_test = np.zeros((ntestNMD,))\n",
        "    oof_test_skf = np.empty((NFOLDS, ntestNMD))\n",
        "\n",
        "    for i, (train_index, test_index) in enumerate(kfNMD):\n",
        "        x_tr = x_train[train_index]\n",
        "        y_tr = y_train[train_index]\n",
        "        x_te = x_train[test_index]\n",
        "\n",
        "        clf.train(x_tr, y_tr)\n",
        "\n",
        "        oof_train[test_index] = clf.predict(x_te)\n",
        "        oof_test_skf[i, :] = clf.predict(x_test)\n",
        "\n",
        "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
        "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f182e621-6bcb-40f8-f49e-85cea91866d7"
      },
      "outputs": [],
      "source": [
        "# Put in our parameters for said classifiers\n",
        "# Random Forest parameters\n",
        "rf_params = {\n",
        "    'n_jobs': -1,\n",
        "    'n_estimators': 500,\n",
        "     'warm_start': True, \n",
        "     #'max_features': 0.2,\n",
        "    'max_depth': 6,\n",
        "    'min_samples_leaf': 2,\n",
        "    'max_features' : 'sqrt',\n",
        "    'verbose': 0\n",
        "}\n",
        "\n",
        "# Extra Trees Parameters\n",
        "et_params = {\n",
        "    'n_jobs': -1,\n",
        "    'n_estimators':500,\n",
        "    #'max_features': 0.5,\n",
        "    'max_depth': 8,\n",
        "    'min_samples_leaf': 2,\n",
        "    'verbose': 0\n",
        "}\n",
        "\n",
        "# AdaBoost parameters\n",
        "ada_params = {\n",
        "    'n_estimators': 500,\n",
        "    'learning_rate' : 0.75\n",
        "}\n",
        "\n",
        "# Gradient Boosting parameters\n",
        "gb_params = {\n",
        "    'n_estimators': 500,\n",
        "     #'max_features': 0.2,\n",
        "    'max_depth': 5,\n",
        "    'min_samples_leaf': 2,\n",
        "    'verbose': 0\n",
        "}\n",
        "\n",
        "# Support Vector Classifier parameters \n",
        "svc_params = {\n",
        "    'kernel' : 'linear',\n",
        "    'C' : 0.025\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5dc6dd59-f03a-ba22-4b55-0b9971985fc0"
      },
      "outputs": [],
      "source": [
        "# Create 5 objects that represent our 4 models\n",
        "rf = SklearnHelper(clf=RandomForestClassifier, seed=SEED, params=rf_params)\n",
        "et = SklearnHelper(clf=ExtraTreesClassifier, seed=SEED, params=et_params)\n",
        "ada = SklearnHelper(clf=AdaBoostClassifier, seed=SEED, params=ada_params)\n",
        "gb = SklearnHelper(clf=GradientBoostingClassifier, seed=SEED, params=gb_params)\n",
        "svc = SklearnHelper(clf=SVC, seed=SEED, params=svc_params)\n",
        "\n",
        "rfwar = SklearnHelper(clf=RandomForestClassifier, seed=SEED, params=rf_params)\n",
        "etwar = SklearnHelper(clf=ExtraTreesClassifier, seed=SEED, params=et_params)\n",
        "adawar = SklearnHelper(clf=AdaBoostClassifier, seed=SEED, params=ada_params)\n",
        "gbwar = SklearnHelper(clf=GradientBoostingClassifier, seed=SEED, params=gb_params)\n",
        "svcwar = SklearnHelper(clf=SVC, seed=SEED, params=svc_params)\n",
        "\n",
        "rfnmd = SklearnHelper(clf=RandomForestClassifier, seed=SEED, params=rf_params)\n",
        "etnmd = SklearnHelper(clf=ExtraTreesClassifier, seed=SEED, params=et_params)\n",
        "adanmd = SklearnHelper(clf=AdaBoostClassifier, seed=SEED, params=ada_params)\n",
        "gbnmd = SklearnHelper(clf=GradientBoostingClassifier, seed=SEED, params=gb_params)\n",
        "svcnmd = SklearnHelper(clf=SVC, seed=SEED, params=svc_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "477c75b7-b7de-0136-2334-c459c0057a26"
      },
      "outputs": [],
      "source": [
        "# Create Numpy arrays of train, test and target ( Survived) dataframes to feed into our models\n",
        "y_trainONS = trainONS['Survived'].ravel()\n",
        "trainONS = trainONS.drop(['Survived'], axis=1)\n",
        "x_trainONS = trainONS.values # Creates an array of the train data\n",
        "x_testONS = testONS.values # Creats an array of the test data\n",
        "\n",
        "y_trainONSWAR = trainONSWAR['Survived'].ravel()\n",
        "yytrainONSWAR = trainONSWAR.drop(['Survived'], axis=1)\n",
        "x_trainONSWAR = trainONSWAR.values # Creates an array of the train data\n",
        "x_testONSWAR = testONSWAR.values # Creats an array of the test data\n",
        "\n",
        "y_trainNMD = trainNMD['Survived'].ravel()\n",
        "yytrainNMD = trainNMD.drop(['Survived'], axis=1)\n",
        "x_trainNMD = trainNMD.values # Creates an array of the train data\n",
        "x_testNMD = testNMD.values # Creats an array of the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2765c78a-69ae-f9b9-09a6-986b58016924"
      },
      "outputs": [],
      "source": [
        "# Create our OOF train and test predictions. These base results will be used as new features\n",
        "et_oof_trainONS, et_oof_testONS = get_oofONS(et, x_trainONS, y_trainONS, x_testONS) # Extra Trees\n",
        "rf_oof_trainONS, rf_oof_testONS = get_oofONS(rf,x_trainONS, y_trainONS, x_testONS) # Random Forest\n",
        "ada_oof_trainONS, ada_oof_testONS = get_oofONS(ada, x_trainONS, y_trainONS, x_testONS) # AdaBoost \n",
        "gb_oof_trainONS, gb_oof_testONS = get_oofONS(gb,x_trainONS, y_trainONS, x_testONS) # Gradient Boost\n",
        "svc_oof_trainONS, svc_oof_testONS = get_oofONS(svc,x_trainONS, y_trainONS, x_testONS) # Support Vector Classifier\n",
        "\n",
        "\n",
        "et_oof_trainONSWAR, et_oof_testONSWAR = get_oofONSWAR(etwar, x_trainONSWAR,\n",
        "                                                      y_trainONSWAR, x_testONSWAR) # Extra Trees\n",
        "rf_oof_trainONSWAR, rf_oof_testONSWAR = get_oofONSWAR(rfwar,x_trainONSWAR,\n",
        "                                                      y_trainONSWAR, x_testONSWAR) # Random Forest\n",
        "ada_oof_trainONSWAR, ada_oof_testONSWAR = get_oofONSWAR(adawar, x_trainONSWAR,\n",
        "                                                        y_trainONSWAR, x_testONSWAR) # AdaBoost \n",
        "gb_oof_trainONSWAR, gb_oof_testONSWAR = get_oofONSWAR(gbwar,x_trainONSWAR,\n",
        "                                                      y_trainONSWAR, x_testONSWAR) # Gradient Boost\n",
        "svc_oof_trainONSWAR, svc_oof_testONSWAR = get_oofONSWAR(svcwar,x_trainONSWAR,\n",
        "                                                        y_trainONSWAR, x_testONSWAR) # Support Vector Classifier\n",
        "\n",
        "\n",
        "et_oof_trainNMD, et_oof_testNMD = get_oofNMD(etnmd, x_trainNMD, y_trainNMD, x_testNMD) # Extra Trees\n",
        "rf_oof_trainNMD, rf_oof_testNMD = get_oofNMD(rfnmd,x_trainNMD, y_trainNMD, x_testNMD) # Random Forest\n",
        "ada_oof_trainNMD, ada_oof_testNMD = get_oofNMD(adanmd, x_trainNMD, y_trainNMD, x_testNMD) # AdaBoost \n",
        "gb_oof_trainNMD, gb_oof_testNMD = get_oofNMD(gbnmd,x_trainNMD, y_trainNMD, x_testNMD) # Gradient Boost\n",
        "svc_oof_trainNMD, svc_oof_testNMD = get_oofNMD(svcnmd,x_trainNMD, y_trainNMD, x_testNMD) # Support Vector Classifier\n",
        "\n",
        "\n",
        "print(\"Training is complete\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6e0f469a-78f7-2b00-8396-d44e9d7326bf"
      },
      "outputs": [],
      "source": [
        "rf_featureONS = rf.feature_importances(x_trainONS,y_trainONS)\n",
        "et_featureONS = et.feature_importances(x_trainONS, y_trainONS)\n",
        "ada_featureONS = ada.feature_importances(x_trainONS, y_trainONS)\n",
        "gb_featureONS = gb.feature_importances(x_trainONS, y_trainONS)\n",
        "\n",
        "rf_featureONSWAR = rf.feature_importances(x_trainONSWAR, y_trainONSWAR)\n",
        "et_featureONSWAR = et.feature_importances(x_trainONSWAR, y_trainONSWAR)\n",
        "ada_featureONSWAR = ada.feature_importances(x_trainONSWAR, y_trainONSWAR)\n",
        "gb_featureONSWAR = gb.feature_importances(x_trainONSWAR, y_trainONSWAR)\n",
        "\n",
        "rf_featureNMD = rf.feature_importances(x_trainNMD, y_trainNMD)\n",
        "et_featureNMD = et.feature_importances(x_trainNMD, y_trainNMD)\n",
        "ada_featureNMD = ada.feature_importances(x_trainNMD, y_trainNMD)\n",
        "gb_featureNMD = gb.feature_importances(x_trainNMD, y_trainNMD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "141d3f02-0e52-a997-ae3f-e6e73e558e54"
      },
      "outputs": [],
      "source": [
        "base_predictions_trainONS = pd.DataFrame( {'RandomForest': rf_oof_trainONS.ravel(),\n",
        "     'ExtraTrees': et_oof_trainONS.ravel(),\n",
        "     'AdaBoost': ada_oof_trainONS.ravel(),\n",
        "      'GradientBoost': gb_oof_trainONS.ravel()\n",
        "    })\n",
        "base_predictions_trainONS.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9a8e5cc9-9f87-51cb-84fe-39c171548933"
      },
      "outputs": [],
      "source": [
        "x_trainONS = np.concatenate(( et_oof_trainONS, rf_oof_trainONS, ada_oof_trainONS,\n",
        "                          gb_oof_trainONS, svc_oof_trainONS), axis=1)\n",
        "x_testONS = np.concatenate(( et_oof_testONS, rf_oof_testONS, ada_oof_testONS,\n",
        "                         gb_oof_testONS, svc_oof_testONS), axis=1)\n",
        "\n",
        "x_trainONSWAR = np.concatenate(( et_oof_trainONSWAR, rf_oof_trainONSWAR, ada_oof_trainONSWAR,\n",
        "                          gb_oof_trainONSWAR, svc_oof_trainONSWAR), axis=1)\n",
        "x_testONSWAR = np.concatenate(( et_oof_testONSWAR, rf_oof_testWAR, ada_oof_testWAR,\n",
        "                         gb_oof_testWAR, svc_oof_testWAR), axis=1)\n",
        "\n",
        "x_trainNMD = np.concatenate(( et_oof_trainNMD, rf_oof_trainNMD, ada_oof_trainNMD,\n",
        "                          gb_oof_trainNMD, svc_oof_trainNMD), axis=1)\n",
        "x_testNMD = np.concatenate(( et_oof_testNMD, rf_oof_testNMD, ada_oof_testNMD,\n",
        "                         gb_oof_testNMD, svc_oof_testNMD), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3965976c-ec1a-a035-6c32-0c1b1cf8c789"
      },
      "outputs": [],
      "source": [
        "gbmONS = xgb.XGBClassifier(\n",
        "    #learning_rate = 0.02,\n",
        " n_estimators= 2000,\n",
        " max_depth= 4,\n",
        " min_child_weight= 2,\n",
        " #gamma=1,\n",
        " gamma=0.9,                        \n",
        " subsample=0.8,\n",
        " colsample_bytree=0.8,\n",
        " objective= 'binary:logistic',\n",
        " nthread= -1,\n",
        " scale_pos_weight=1).fit(x_trainONS, y_trainONS)\n",
        "predictionsONS = gbmONS.predict(x_testONS)\n",
        "\n",
        "gbmONSWAR = xgb.XGBClassifier(\n",
        "    #learning_rate = 0.02,\n",
        " n_estimators= 2000,\n",
        " max_depth= 4,\n",
        " min_child_weight= 2,\n",
        " #gamma=1,\n",
        " gamma=0.9,                        \n",
        " subsample=0.8,\n",
        " colsample_bytree=0.8,\n",
        " objective= 'binary:logistic',\n",
        " nthread= -1,\n",
        " scale_pos_weight=1).fit(x_trainONSWAR, y_trainONSWAR)\n",
        "predictionsONSWAR = gbmONSWAR.predict(x_testONSWAR)\n",
        "\n",
        "gbmNMD = xgb.XGBClassifier(\n",
        "    #learning_rate = 0.02,\n",
        " n_estimators= 2000,\n",
        " max_depth= 4,\n",
        " min_child_weight= 2,\n",
        " #gamma=1,\n",
        " gamma=0.9,                        \n",
        " subsample=0.8,\n",
        " colsample_bytree=0.8,\n",
        " objective= 'binary:logistic',\n",
        " nthread= -1,\n",
        " scale_pos_weight=1).fit(x_trainNMD, y_trainNMD)\n",
        "predictionsNMD = gbmNMD.predict(x_testNMD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "dcdbe938-9324-7f83-05eb-9953c23b7c01"
      },
      "outputs": [],
      "source": [
        "# Generate Submission File \n",
        "StackingSubmissionONS = pd.DataFrame({ 'PassengerId': PassengerId,\n",
        "                            'Survived': predictionsONS })\n",
        "StackingSubmissionONS.to_csv(\"StackingSubmissionONS.csv\", index=False)\n",
        "\n",
        "# Generate Submission File \n",
        "StackingSubmissionONSWAR = pd.DataFrame({ 'PassengerId': PassengerId,\n",
        "                            'Survived': predictionsONSWAR })\n",
        "StackingSubmissionONSWAR.to_csv(\"StackingSubmissionONSWAR.csv\", index=False)\n",
        "\n",
        "# Generate Submission File \n",
        "StackingSubmissionNMD = pd.DataFrame({ 'PassengerId': PassengerId,\n",
        "                            'Survived': predictionsNMD })\n",
        "StackingSubmissionNMD.to_csv(\"StackingSubmissionNMD.csv\", index=False)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}