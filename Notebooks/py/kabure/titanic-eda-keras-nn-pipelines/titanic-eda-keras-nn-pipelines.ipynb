{"cells":[{"metadata":{"_cell_guid":"adfd17d9-7925-49bd-94bf-854780a835c0","_uuid":"cdbc543b754b4cb269704504cca86466954846f2"},"cell_type":"markdown","source":"<h1> Welcome to my Titanic Kernel! </h1>\n<h2>This kernel will provide a analysis through the Titanic Disaster to understand the Survivors patterns</h2><br>\n\nI will handle with data (<i>transform, missings, manipulation</i>), explore the data (<i>descritive and visual</i>) and also create a Deep Learning model"},{"metadata":{"_cell_guid":"f47e97ab-eb3e-49c7-8ea6-25a6d9f8312d","_uuid":"10bb3506c6e83b4a8938f81620474ae1e214a544"},"cell_type":"markdown","source":"Are you looking for another interesting Kernels? <a href=\"https://www.kaggle.com/kabure/kernels\">CLICK HERE</a> <br>\nGive me your feedback and if yo like this kernel, votes up"},{"metadata":{"_cell_guid":"4247239a-11d0-4a90-8431-c410409d7a1c","_uuid":"6008e6d18320ba0ff790618af888d3a4c25839bf"},"cell_type":"markdown","source":"<i>*I'm from Brazil, so english is not my first language, sorry about some mistakes</i>"},{"metadata":{"_cell_guid":"45132ff6-d170-4229-a9f9-62cfc3d6f33b","_uuid":"fa61bfbef5735a7c6a83747d15724da0fc73e7ad"},"cell_type":"markdown","source":"# Table of Contents:\n\n**1. [Introduction](#Introduction)** <br>\n**2. [Librarys](#Librarys)** <br>\n**3. [Knowning the data](#Known)** <br>\n**4. [Exploring some Variables](#Explorations)** <br>\n**5. [Preprocessing](#Prepocess)** <br>\n**6. [Modelling](#Model)** <br>\n**7. [Validation](#Validation)** <br>\n"},{"metadata":{"_cell_guid":"61a2256a-c100-405b-a6a7-7382ec84ef90","_uuid":"23440568688f65c1549aaa44c364e2a8aebeeb86"},"cell_type":"markdown","source":"<a id=\"Introduction\"></a> <br> \n# **1. Introduction:** \n<h3> The data have 891 entries on train dataset and 418 on test dataset</h3>\n- 10 columns in train_csv and 9 columns in train_test\n"},{"metadata":{"_cell_guid":"543344f8-6da7-4119-a60d-e4059d4aaf60","_uuid":"25b0f965e63fe79715087729784ece8d424820ad"},"cell_type":"markdown","source":"<h2>Competition Description: </h2>\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\nIn this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy."},{"metadata":{"_cell_guid":"9b61b570-3c40-4fe9-a6dd-cd010c2d55c4","_uuid":"73cc777de74395eb8d0e6ced15f8783c085bcc21"},"cell_type":"markdown","source":"<h3>Data Dictionary</h3><br>\nVariable\tDefinition\tKey<br>\n<b>survival</b>\tSurvival\t0 = No, 1 = Yes<br>\n<b>pclass</b>\tTicket class\t1 = 1st, 2 = 2nd, 3 = 3rd<br>\n<b>sex</b>\tSex\t<br>\n<b>Age</b>\tAge in years\t<br>\n<b>sibsp</b>\t# of siblings / spouses aboard the Titanic\t<br>\n<b>parch</b>\t# of parents / children aboard the Titanic\t<br>\n<b>ticket</b>\tTicket number\t<br>\n<b>fare</b>\tPassenger fare\t<br>\n<b>cabin</b>\tCabin number\t<br>\n<b>embarked\t</b>Port of Embarkation\tC = Cherbourg, Q = Queenstown, S = Southampton<br>\n<h3>Variable Notes</h3><br>\n<b>pclass: </b>A proxy for socio-economic status (SES)<br>\n1st = Upper<br>\n2nd = Middle<br>\n3rd = Lower<br>\n<b>age: </b>Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br>\n<b>sibsp:</b> The dataset defines family relations in this way...<br>\n- <b>Sibling </b>= brother, sister, stepbrother, stepsister<br>\n- <b>Spouse </b>= husband, wife (mistresses and fianc√©s were ignored)<br>\n\n<b>parch: </b>The dataset defines family relations in this way...<br>\n- <b>Parent</b> = mother, father<br>\n- <b>Child </b>= daughter, son, stepdaughter, stepson<br>\n\nSome children travelled only with a nanny, therefore parch=0 for them.<br>"},{"metadata":{"_cell_guid":"e5b2eb99-c1fb-4a8a-bb2b-fbb159358ffd","_uuid":"2d1b974a41c9b8ae24da08ba4793afec3d516ecd"},"cell_type":"markdown","source":"I am using the beapproachs as possible but if you think I can do anything another best way, please, let me know."},{"metadata":{"_cell_guid":"ab631e54-b2cc-45f1-bbcd-295e26f97a32","_uuid":"cd0487ef702d4ac4e7154af34346ddf9b856ab28"},"cell_type":"markdown","source":"<a id=\"Librarys\"></a> <br> \n# **2. Librarys:** "},{"metadata":{"_cell_guid":"d018a79d-a8da-4dab-b289-afd411c89231","_uuid":"eb9b1cb179aae5859c84388844468c5225567241","trusted":true},"cell_type":"code","source":"#This librarys is to work with matrices\nimport pandas as pd \n# This librarys is to work with vectors\nimport numpy as np\n# This library is to create some graphics algorithmn\nimport seaborn as sns\n# to render the graphs\nimport matplotlib.pyplot as plt\n# import module to set some ploting parameters\nfrom matplotlib import rcParams\n# Library to work with Regular Expressions\nimport re\n\n# This function makes the plot directly on browser\n%matplotlib inline\n\n# Seting a universal figure size \nrcParams['figure.figsize'] = 10,8","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"61a77a63-b9ed-455b-b3d2-fde823676ed8","_uuid":"688fd0574f7f65b0fdaa5774cf6fc7626bd24d01","trusted":true},"cell_type":"code","source":"# Importing train dataset\ndf_train = pd.read_csv(\"../input/train.csv\")\n\n# Importing test dataset\ndf_test = pd.read_csv(\"../input/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6149e4cd-1340-4db1-9d3f-04c67e780aa3","_uuid":"d87340ef76b49cd3a6d346390c38cb4d140220ae"},"cell_type":"markdown","source":"<a id=\"Known\"></a> <br> \n# **3. First look at the data:** "},{"metadata":{"_cell_guid":"1ed66b34-9219-49a2-a1f4-04fd9d00777e","_uuid":"069f3ccadab3fcc3af53c594e1b14c91bd801b85"},"cell_type":"markdown","source":"I will start looking the type and informations of the datasets"},{"metadata":{"_cell_guid":"869cbf05-810c-44ff-b7ef-456d375dfecc","_uuid":"bae6dabd016b97a9d9dc888514da0a7e96ac64a5","trusted":true},"cell_type":"code","source":"#Looking data format and types\nprint(df_train.info())\n\n# printing test info()\nprint(df_test.info())","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"fea21fe3-c16c-42c2-bd1f-6cc635388698","_uuid":"5dee2b72c3008163bc058add517bbdb060ff5eef","trusted":true},"cell_type":"code","source":"#Some Statistics\ndf_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f66ac581-59c6-4c57-be3f-e9ceea2babe2","_uuid":"ce135284f5dfa4f04c60a25a74680fac870bdf58","trusted":true},"cell_type":"code","source":"#Take a look at the data\nprint(df_train.head())","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"cc879cc5-0cc7-4fc0-8e0f-826b299270a9","_uuid":"612fec9e24c56a509b472266db3122560b90aa07"},"cell_type":"markdown","source":"<a id=\"Known\"></a> <br> \n# **4. Exploring the data:** "},{"metadata":{"_cell_guid":"f1220151-2300-4875-a74a-586a55c53c1b","_uuid":"0de2d141e1d7adc2b8807123b2b60cefc343acba"},"cell_type":"markdown","source":"<h2>To try a new approach in the data, I will start the data analysis by the Name column"},{"metadata":{"_cell_guid":"27f2ad66-2927-41f2-a737-7eed308f20ca","_uuid":"25a943a87526ed842c24848fa61f4390b3cb43b4","trusted":true},"cell_type":"code","source":"#Looking how the data is and searching for a re patterns\ndf_train[\"Name\"].head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9252092d-97c6-4d06-a607-da8e728d5a09","_uuid":"457f672a2823b533dd98e04f764e1fe4d3017372","trusted":true},"cell_type":"code","source":"#GettingLooking the prefix of all Passengers\ndf_train['Title'] = df_train.Name.apply(lambda x: re.search(' ([A-Z][a-z]+)\\.', x).group(1))\n\n#defining the figure size of our graphic\nplt.figure(figsize=(12,5))\n\n#Plotting the result\nsns.countplot(x='Title', data=df_train, palette=\"hls\")\nplt.xlabel(\"Title\", fontsize=16) #seting the xtitle and size\nplt.ylabel(\"Count\", fontsize=16) # Seting the ytitle and size\nplt.title(\"Title Name Count\", fontsize=20) \nplt.xticks(rotation=45)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"dae2f74f-ae05-4ef2-9a1b-27eeea339db1","_uuid":"da70c5433f031f1437ea4fdb9b612945d02f0e7e","trusted":true},"cell_type":"code","source":"#Doing the same on df_test with regular expressions\ndf_test['Title'] = df_test.Name.apply(lambda x: re.search(' ([A-Z][a-z]+)\\.', x).group(1))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ca04f234acf5cbd7dc8e9b06f79a4deb91703aec"},"cell_type":"markdown","source":"## Grouping some titles and ploting the results"},{"metadata":{"_cell_guid":"29c83a0e-0c30-41c1-87fe-9a417603eed0","_uuid":"72e1ca619c6aba4c333faa33471b6e1bd2ed8ed0","trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"#Now, I will identify the social status of each title\n\nTitle_Dictionary = {\n        \"Capt\":       \"Officer\",\n        \"Col\":        \"Officer\",\n        \"Major\":      \"Officer\",\n        \"Dr\":         \"Officer\",\n        \"Rev\":        \"Officer\",\n        \"Jonkheer\":   \"Royalty\",\n        \"Don\":        \"Royalty\",\n        \"Sir\" :       \"Royalty\",\n        \"the Countess\":\"Royalty\",\n        \"Dona\":       \"Royalty\",\n        \"Lady\" :      \"Royalty\",\n        \"Mme\":        \"Mrs\",\n        \"Ms\":         \"Mrs\",\n        \"Mrs\" :       \"Mrs\",\n        \"Mlle\":       \"Miss\",\n        \"Miss\" :      \"Miss\",\n        \"Mr\" :        \"Mr\",\n        \"Master\" :    \"Master\"\n                   }\n    \n# we map each title to correct category\ndf_train['Title'] = df_train.Title.map(Title_Dictionary)\ndf_test['Title'] = df_test.Title.map(Title_Dictionary)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"09ba74cffc1100812d8b64e8120070818efce009"},"cell_type":"markdown","source":"## Title grouped"},{"metadata":{"_cell_guid":"dbb86411-81f9-4230-aa48-08c50150bb7e","_uuid":"10b11ef5b4e3581e66762649880049637824fa88","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#printing the chance to survive by each title\nprint(\"Chances to survive based on titles: \") \nprint(df_train.groupby(\"Title\")[\"Survived\"].mean())\n\n# figure size\nplt.figure(figsize=(12,5))\n\n#Plotting the count of title by Survived or not category\nsns.countplot(x='Title', data=df_train, palette=\"hls\",\n              hue=\"Survived\")\nplt.xlabel(\"Titles\", fontsize=16)\nplt.ylabel(\"Count\", fontsize=16)\nplt.title(\"Title Grouped Count\", fontsize=20)\nplt.xticks(rotation=45)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b8977d6e-0a6a-471d-8a18-8930f1e8c3bf","_uuid":"bf3ae181bb1a31e50802e25af3f0e47833421489"},"cell_type":"markdown","source":"It's interesting... Children's and ladys first, huh?"},{"metadata":{"_cell_guid":"63ec428b-865c-4eea-8332-fe0f4b0d8d22","_uuid":"c889e2c9390a745fa64cf1019320128310233dfc"},"cell_type":"markdown","source":"<h1> Now I will handle the Age variable that has a high number of NaN's, using some columns to correctly input he missing Age's"},{"metadata":{"_cell_guid":"2df4f1ce-2767-4629-b023-0ec998fe91fd","scrolled":false,"_uuid":"4d0dd241125f42e477f9d9a9212e9bd2baf23411","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#First I will look my distribuition without NaN's\n#I will create a df to look distribuition \nage_high_zero_died = df_train[(df_train[\"Age\"] > 0) & \n                              (df_train[\"Survived\"] == 0)]\nage_high_zero_surv = df_train[(df_train[\"Age\"] > 0) & \n                              (df_train[\"Survived\"] == 1)]\n\n#figure size\nplt.figure(figsize=(10,5))\n\n# Ploting the 2 variables that we create and compare the two\nsns.distplot(age_high_zero_surv[\"Age\"], bins=24, color='g')\nsns.distplot(age_high_zero_died[\"Age\"], bins=24, color='r')\nplt.title(\"Distribuition and density by Age\",fontsize=20)\nplt.xlabel(\"Age\",fontsize=15)\nplt.ylabel(\"Distribuition Died and Survived\",fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"81eb2ace-d258-4310-9867-9a8fb23f1070","_uuid":"94f5cabb66d7825afa9ddfaa84de11ac5f72b009","trusted":true},"cell_type":"code","source":"#Let's group the median age by sex, pclass and title, to have any idea and maybe input in Age NAN's\nage_group = df_train.groupby([\"Sex\",\"Pclass\",\"Title\"])[\"Age\"]\n\n#printing the variabe that we created by median\nprint(age_group.median())","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b267c345-0425-45b9-8073-851554382efe","_uuid":"3687716569e4415764193cf3efdbb759d5d622ef"},"cell_type":"markdown","source":"This might show us a better way to input the NAN's \n\n<b>For example: </b> an male in 2 class that is a Officer the median Age is 42. <br>\nAnd we will use that to complete the missing data\n"},{"metadata":{"_cell_guid":"87ba3b6f-9a4c-4d0d-b1f6-2cf0ef45311d","_uuid":"3af4bfdefd77e3099be92daa789455fc0a34e345","trusted":true},"cell_type":"code","source":"#inputing the values on Age Na's \n# using the groupby to transform this variables\ndf_train.loc[df_train.Age.isnull(), 'Age'] = df_train.groupby(['Sex','Pclass','Title']).Age.transform('median')\n\n# printing the total of nulls in Age Feature\nprint(df_train[\"Age\"].isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f8adc6e3-3103-4dcf-b861-69c636d1ac9a","_uuid":"0ae11c621c4b36d52f599390e68f7836d3374d03","trusted":true},"cell_type":"code","source":"#Let's see the result of the inputation\n\n#seting the figure size\nplt.figure(figsize=(12,5))\n\n#ploting again the Age Distribuition after the transformation in our dataset\nsns.distplot(df_train[\"Age\"], bins=24)\nplt.title(\"Distribuition and density by Age\")\nplt.xlabel(\"Age\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a04c3054-0ee4-47cc-a7a7-5599d8f28669","_uuid":"b70b8c6d33c0b266004929debe8d5993e7114c83","trusted":true},"cell_type":"code","source":"#separate by survivors or not\n\n# figure size\nplt.figure(figsize=(12,5))\n\n# using facetgrid that is a great way to get information of our dataset\ng = sns.FacetGrid(df_train, col='Survived',size=5)\ng = g.map(sns.distplot, \"Age\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b0dddd05-63c3-4bba-92e8-4671c24ddd2f","_uuid":"cbb38e630257944309fc80414b397472281882c5"},"cell_type":"markdown","source":"Now let's categorize them "},{"metadata":{"_cell_guid":"09c0ec2f-4271-4f72-a2ef-4ea407752751","_uuid":"525d53d8b8adc49d04c6db8a9ac778427f28a086","trusted":true},"cell_type":"code","source":"#df_train.Age = df_train.Age.fillna(-0.5)\n\n#creating the intervals that we need to cut each range of ages\ninterval = (0, 5, 12, 18, 25, 35, 60, 120) \n\n#Seting the names that we want use to the categorys\ncats = ['babies', 'Children', 'Teen', 'Student', 'Young', 'Adult', 'Senior']\n\n# Applying the pd.cut and using the parameters that we created \ndf_train[\"Age_cat\"] = pd.cut(df_train.Age, interval, labels=cats)\n\n# Printing the new Category\ndf_train[\"Age_cat\"].head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"52abbfa0-8ba6-4b4e-998e-7d7f34e7491f","scrolled":false,"_uuid":"928b946783769488edba7961a728a4d265fe2721","trusted":true},"cell_type":"code","source":"#Do the same to test dataset \ninterval = (0, 5, 12, 18, 25, 35, 60, 120)\n\n#same as the other df train\ncats = ['babies', 'Children', 'Teen', 'Student', 'Young', 'Adult', 'Senior']\n\n# same that we used above in df train\ndf_test[\"Age_cat\"] = pd.cut(df_test.Age, interval, labels=cats)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a65eb046-e49d-4be6-bda7-c69364f69eb9","_uuid":"3e7f681f7f7fb285da11801cd8a7896b7e8d7a5b","trusted":true},"cell_type":"code","source":"#Describe of categorical Age\n\n# Using pd.crosstab to understand the Survived rate by Age Category's\nprint(pd.crosstab(df_train.Age_cat, df_train.Survived))\n\n#Seting the figure size\nplt.figure(figsize=(12,5))\n\n#Plotting the result\nsns.countplot(\"Age_cat\",data=df_train,hue=\"Survived\", palette=\"hls\")\nplt.xlabel(\"Categories names\", fontsize=18)\nplt.xlabel(\"Count\", fontsize=18)\nplt.title(\"Age Distribution \", fontsize=20)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5af6dec2-b158-4464-a575-0bbb0e34597c","_uuid":"97b914a88fe66c7ff5655cf9c70f3eef5c57a1c5"},"cell_type":"markdown","source":"Now it look's better"},{"metadata":{"_uuid":"011e87cf65d9075e6f1bc9fce3a566af347d2308"},"cell_type":"markdown","source":"### Looking the Fare distribuition to survivors and not survivors\n"},{"metadata":{"_cell_guid":"adb09853-725a-4c34-99e4-581f5fee8635","scrolled":false,"_uuid":"5b598cf71c186dab9547fa21bd31184e3f64ed90","trusted":true},"cell_type":"code","source":"\n# Seting the figure size\nplt.figure(figsize=(12,5))\n\n# Understanding the Fare Distribuition \nsns.distplot(df_train[df_train.Survived == 0][\"Fare\"], \n             bins=50, color='r')\nsns.distplot(df_train[df_train.Survived == 1][\"Fare\"], \n             bins=50, color='g')\nplt.title(\"Fare Distribuition by Survived\", fontsize=20)\nplt.xlabel(\"Fare\", fontsize=15)\nplt.ylabel(\"Density\",fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b8df1880-dd5f-4c98-a4a3-a3fee8950eaa","_uuid":"410262c53a3e6b2754834ced213cf5b342734ab6"},"cell_type":"markdown","source":"<br>\nDescription of Fare variable<br>\n- Min: 0<br>\n- Median: 14.45<br>\n- Mean: 32.20<br>\n- Max: 512.32<br> \n- Std: 49.69<br>\n\n<h3>I will create a categorical variable to treat the Fare expend</h3><br>\nI will use the same technique used in Age but now I will use the quantiles to binning\n\n"},{"metadata":{"_cell_guid":"0f80bee7-c523-4eae-bad1-3267413b8f71","_uuid":"9f997138277822312c4ba718313d16ede922787e","trusted":true},"cell_type":"code","source":"#Filling the NA's with -0.5\ndf_train.Fare = df_train.Fare.fillna(-0.5)\n\n#intervals to categorize\nquant = (-1, 0, 8, 15, 31, 600)\n\n#Labels without input values\nlabel_quants = ['NoInf', 'quart_1', 'quart_2', 'quart_3', 'quart_4']\n\n#doing the cut in fare and puting in a new column\ndf_train[\"Fare_cat\"] = pd.cut(df_train.Fare, quant, labels=label_quants)\n\n#Description of transformation\nprint(pd.crosstab(df_train.Fare_cat, df_train.Survived))\n\nplt.figure(figsize=(12,5))\n\n#Plotting the new feature\nsns.countplot(x=\"Fare_cat\", hue=\"Survived\", data=df_train, palette=\"hls\")\nplt.title(\"Count of survived x Fare expending\",fontsize=20)\nplt.xlabel(\"Fare Cat\",fontsize=15)\nplt.ylabel(\"Count\",fontsize=15)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a5792df7-20c7-4338-b953-1a2ff0c1994a","_uuid":"ac340253f27b7aa74834d0b9973aa5ddc0410750","trusted":true},"cell_type":"code","source":"# Replicate the same to df_test\ndf_test.Fare = df_test.Fare.fillna(-0.5)\n\nquant = (-1, 0, 8, 15, 31, 1000)\nlabel_quants = ['NoInf', 'quart_1', 'quart_2', 'quart_3', 'quart_4']\n\ndf_test[\"Fare_cat\"] = pd.cut(df_test.Fare, quant, labels=label_quants)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c939b96c-c89c-46db-b25c-86d14be29a68","_uuid":"fce46c428e704651712f042f67ff6e697d6c86dc"},"cell_type":"markdown","source":"<h2>To complete this part, I will now work on \"Names\""},{"metadata":{"_kg_hide-output":false,"_cell_guid":"5fc23992-efec-4b03-9b76-e7cb271866a0","_kg_hide-input":false,"_uuid":"3b0759682656ce4b4ca00919385353090e9ef207","trusted":true},"cell_type":"code","source":"#Now lets drop the variable Fare, Age and ticket that is irrelevant now\ndel df_train[\"Fare\"]\ndel df_train[\"Ticket\"]\ndel df_train[\"Age\"]\ndel df_train[\"Cabin\"]\ndel df_train[\"Name\"]\n\n#same in df_test\ndel df_test[\"Fare\"]\ndel df_test[\"Ticket\"]\ndel df_test[\"Age\"]\ndel df_test[\"Cabin\"]\ndel df_test[\"Name\"]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"090c0116-c368-45bd-8853-549d732bf10d","_uuid":"f4d484d256fdeadd6efcf6808efa5b67a1f4c318","trusted":true},"cell_type":"code","source":"#Looking the result of transformations\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"523e7063-a749-4701-9e34-5c32d973b02b","_uuid":"7b9f10c15d9a0a91fbba4ff86af9c3ae5d9b17ce"},"cell_type":"markdown","source":"<h1>It's looking ok"},{"metadata":{"_cell_guid":"7231a339-a6ac-4b67-bab4-76b44d561cbb","_uuid":"6e15db3f01cb5866f42a7ca01132408380f1ac57"},"cell_type":"markdown","source":"Now, lets start explore the data"},{"metadata":{"_cell_guid":"9b349042-d45c-4546-8ebb-1e1ff7ea32a1","_uuid":"62c29ef25da65e21e10bc3aece1b89210cc93725","trusted":true},"cell_type":"code","source":"# Let see how many people die or survived\nprint(\"Total of Survived or not: \")\nprint(df_train.groupby(\"Survived\")[\"PassengerId\"].count())\n\nplt.figure(figsize=(12,5))\n\nsns.countplot(x=\"Survived\", data=df_train,palette=\"hls\")\nplt.title('Total Distribuition by survived or not')\nplt.xlabel('Target Distribuition')\nplt.ylabel('Count')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7cb09fcc-c9da-4d1c-b194-777f2668af3a","_uuid":"19ffc35bad061c08909622eaf75edff7f0555935","trusted":true},"cell_type":"code","source":"print(pd.crosstab(df_train.Survived, df_train.Sex))\n\nplt.figure(figsize=(12,5))\nsns.countplot(x=\"Sex\", data=df_train, hue=\"Survived\",palette=\"hls\")\nplt.title('Sex Distribuition by survived or not', fontsize=20)\nplt.xlabel('Sex Distribuition',fontsize=15)\nplt.ylabel('Count', fontsize=15)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"161d8e96-6ce1-434e-aa89-522a7c032b08","_uuid":"cb8b4949900ea4485b4163f43adbb58ab819c59d"},"cell_type":"markdown","source":"<h2>We can look that % dies to mens are much higher than female"},{"metadata":{"_cell_guid":"336eef09-ba35-4556-915d-40a253f9d4a6","_uuid":"24198d30b6383cee9316e15d91ef14546d7ed170"},"cell_type":"markdown","source":"<h1>Now, lets do some exploration in Pclass and Embarked to see if might have some information to build the model"},{"metadata":{"_cell_guid":"384311d4-98ef-4094-984a-cc938337eea4","_uuid":"df6f8b4204617c225e27a6ea3a86a37ff3999124","trusted":true},"cell_type":"code","source":"# Distribuition by class\nprint(pd.crosstab(df_train.Pclass, df_train.Embarked))\n\nplt.figure(figsize=(12,5))\n\nsns.countplot(x=\"Embarked\", data=df_train, hue=\"Pclass\",palette=\"hls\")\nplt.title('Embarked x Pclass Count', fontsize=20)\nplt.xlabel('Embarked with PClass',fontsize=15)\nplt.ylabel('Count', fontsize=15)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"040cc5d0-428d-48d6-abcb-ca39833bd79a","_uuid":"33cef2140fc4a7d6f9578f1899b4b115ee99c3d9","trusted":true},"cell_type":"code","source":"#lets input the NA's with the highest frequency\ndf_train[\"Embarked\"] = df_train[\"Embarked\"].fillna('S')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f43b043c-29ba-46d5-b1aa-7f621cb4fc5a","_uuid":"6805122a7e726de8a172a76a5ea7f62b44629adf","trusted":true},"cell_type":"code","source":"# Exploring Survivors vs Embarked\nprint(pd.crosstab(df_train.Survived, df_train.Embarked))\n\nplt.figure(figsize=(12,5))\n\nsns.countplot(x=\"Embarked\", data=df_train, hue=\"Survived\",palette=\"hls\")\nplt.title('Class Distribuition by survived or not',fontsize=20)\nplt.xlabel('Embarked',fontsize=15)\nplt.ylabel('Count', fontsize=15)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"74624db8-52ee-406f-8c83-c66d04a5f72b","_uuid":"12a92fdc61b8eab85e116005a212dc2923176149","trusted":true},"cell_type":"code","source":"# Exploring Survivors vs Pclass\nprint(pd.crosstab(df_train.Survived, df_train.Pclass))\n\nplt.figure(figsize=(12,5))\n\nsns.countplot(x=\"Pclass\", data=df_train, hue=\"Survived\",palette=\"hls\")\nplt.xlabel('PClass',fontsize=15)\nplt.ylabel('Count', fontsize=15)\nplt.title('Class Distribuition by Survived or not', fontsize=20)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6cabb14b-3ac5-475f-8f0f-8429d37d7430","_uuid":"2a934265a75eb91f93463542913568093da91e8b"},"cell_type":"markdown","source":"<b>Looking the graphs, is clear that 3st class and Embarked at Southampton have a high probabilities to not survive</b>"},{"metadata":{"_cell_guid":"866f1844-9d35-4c5f-8e03-0800e2b249fa","_uuid":"d2de53d7016f6ec0375605051b9461f5478415b4"},"cell_type":"markdown","source":"To finish the analysis I let's look the Sibsp and Parch variables"},{"metadata":{"_cell_guid":"a2a62a1c-ff6c-4d94-a4f2-9c558e4b7ea7","_uuid":"4fd1de75be65d657b8ebc82850660ced882f595c","trusted":true},"cell_type":"code","source":"g = sns.factorplot(x=\"SibSp\",y=\"Survived\",data=df_train,kind=\"bar\", size = 5, aspect= 1.6, palette = \"hls\")\ng.set_ylabels(\"Probability(Survive)\", fontsize=15)\ng.set_xlabels(\"SibSp Number\", fontsize=15)\n\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b6ffe75f-2b81-426e-8b8a-cefb33c6eb43","_uuid":"b2c65fecab3b51000ddb2936e147909d980ba98d"},"cell_type":"markdown","source":"Interesting. With 1 or 2 siblings/spouses have more chance to survived the disaster"},{"metadata":{"_cell_guid":"f1188532-8814-412f-b50a-566d06e68f12","_uuid":"c254a31e5223d5ff370ba74b3bd182aef993e8b5","trusted":true},"cell_type":"code","source":"# Explore Parch feature vs Survived\ng  = sns.factorplot(x=\"Parch\",y=\"Survived\",data=df_train, kind=\"bar\", size = 6,palette = \"hls\")\ng = g.set_ylabels(\"survival probability\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"12aff201-0df6-46ff-a5bd-e9f9332595da","_uuid":"7e771dea89a3d8d364aee95167131ba453956a7d"},"cell_type":"markdown","source":"We can see a high standard deviation in the survival with 3 parents/children person's <br>\nAlso that small families (1~2) have more chance to survival than single or big families"},{"metadata":{"_cell_guid":"73dea415-a5ba-48d5-b4b3-d63662830baa","_uuid":"24ef1e013773a21896a59b3a583899bec419aa8a"},"cell_type":"markdown","source":"So to Finish our exploration I will create a new column to with familiees size"},{"metadata":{"_cell_guid":"88aa5f83-d960-40a9-a833-03c07a32a1af","_uuid":"92831e8bd12e5f3d134e23aebe2d40de578bb396","trusted":true},"cell_type":"code","source":"#Create a new column and sum the Parch + SibSp + 1 that refers the people self\ndf_train[\"FSize\"] = df_train[\"Parch\"] + df_train[\"SibSp\"] + 1\n\ndf_test[\"FSize\"] = df_test[\"Parch\"] + df_test[\"SibSp\"] + 1","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2cf6d69d-cfad-4a04-9e51-a3a19a89e667","_uuid":"dda6e7d8d1baa0ccb438b7bd47d883c12d0931c6","trusted":true},"cell_type":"code","source":"print(pd.crosstab(df_train.FSize, df_train.Survived))\nsns.factorplot(x=\"FSize\",y=\"Survived\", data=df_train, kind=\"bar\",size=6, aspect=1.6)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e8675cf1-c998-4c27-8b41-492fedc80928","_uuid":"be2d10ea57b777eda5a5a25fe318c559db3872ce","trusted":true},"cell_type":"code","source":"del df_train[\"SibSp\"]\ndel df_train[\"Parch\"]\n\ndel df_test[\"SibSp\"]\ndel df_test[\"Parch\"]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"49b1a53d-2ed1-4eb4-81d1-aba80f31a6a7","_uuid":"d21770ff6dc461cb29879d16e37eb4b182b59789"},"cell_type":"markdown","source":"OK, its might be enough to start with the preprocess and builting the model\n"},{"metadata":{"_cell_guid":"df28b5cb-30bd-4201-954d-ab4695e1f75d","_uuid":"5c9f022143411b7c59c7f49205e65ff2b44226dd"},"cell_type":"markdown","source":"<a id=\"Preprocess\"></a> <br> \n# **5. Preprocessing :** "},{"metadata":{"_cell_guid":"a528235e-1ce4-4ec7-bb2d-18f8e1a897c7","_uuid":"b74cefa05a294976b2cc005f613cfb9427190f9f","trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f810cb12-1ac6-467f-9ed8-f96ba7af5c94","_uuid":"840ce4a7f3e53e5a2939625273f14e81b879f3d3"},"cell_type":"markdown","source":"Now we might have information enough to think about the model structure"},{"metadata":{"_cell_guid":"93854fb1-9a60-412d-8eca-4c3e8631a148","_uuid":"9a83734eca0f33ff1a356d2d3e54ef8acc0f1992","trusted":true},"cell_type":"code","source":"df_train = pd.get_dummies(df_train, columns=[\"Sex\",\"Embarked\",\"Age_cat\",\"Fare_cat\",\"Title\"],\\\n                          prefix=[\"Sex\",\"Emb\",\"Age\",\"Fare\",\"Prefix\"], drop_first=True)\n\ndf_test = pd.get_dummies(df_test, columns=[\"Sex\",\"Embarked\",\"Age_cat\",\"Fare_cat\",\"Title\"],\\\n                         prefix=[\"Sex\",\"Emb\",\"Age\",\"Fare\",\"Prefix\"], drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":false,"_cell_guid":"1e77fefc-c14a-4421-a1de-3b7e7c02548b","_kg_hide-input":false,"_uuid":"86e6246b1bf1aa8a8368df43924ec34abedf3e69","trusted":true},"cell_type":"code","source":"#Finallt, lets look the correlation of df_train\nplt.figure(figsize=(15,12))\nplt.title('Correlation of Features for Train Set')\nsns.heatmap(df_train.astype(float).corr(),vmax=1.0,  annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"33676b33-5811-4b0a-aa63-eeb79182c4f6","_uuid":"2a89b78f76ab49ed08dc18783c859f8ad9d027e1","trusted":true},"cell_type":"code","source":"df_train.shape","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":false,"_cell_guid":"68bef82a-971a-4cee-8351-b91f5d5d7689","_kg_hide-input":false,"_uuid":"baef942a3e7342838698c591fa0e082a5e059596","trusted":true},"cell_type":"code","source":"train = df_train.drop([\"Survived\",\"PassengerId\"],axis=1)\ntrain_ = df_train[\"Survived\"]\n\ntest_ = df_test.drop([\"PassengerId\"],axis=1)\n\nX_train = train.values\ny_train = train_.values\n\nX_test = test_.values\nX_test = X_test.astype(np.float64, copy=False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5eb9a2b2-96b6-4da9-9f5c-c76e661daf78","_uuid":"065a617bcfd50e11f9f4eb5832163def96111ca2","trusted":true},"cell_type":"code","source":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5963f209-f2d8-407a-90f0-41f8cf51cae0","_uuid":"649f305a1329ee5fd008781d05037063c8f38ef5"},"cell_type":"markdown","source":"<a id=\"Model\"></a> <br> \n# **6. Modelling : ** "},{"metadata":{"_cell_guid":"3b7d2991-185f-414c-bfd2-9784c24e6849","_uuid":"be32404370cc653e8836da43b1ff318439aa930a"},"cell_type":"markdown","source":"<h3>Titanic survivors prediction: <br>\na binary classification example</h3>\nTwo-class classification, or binary classification, may be the most widely applied kind of machine-learning problem."},{"metadata":{"_cell_guid":"2f44ebf8-0b96-453e-8697-cad046e48ba1","_uuid":"71df1ec0631d10ed2e4f02dd3a0a99bc349c4d97","trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout\nimport keras\nfrom keras.optimizers import SGD\nimport graphviz","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d2b852ad-3b25-4287-87e5-b0931da78395","_uuid":"1946910459dce1528b9bdccca57ab50fadbe64d7"},"cell_type":"markdown","source":"<h1>Anatomy of a neural network: </h1>\n\nAs you saw in the previous chapters, training a neural network revolves around the following\nobjects:\n- Layers, which are combined into a network (or model)\n- The input data and corresponding targets\n- The loss function, which defines the feedback signal used for learning\n- The optimizer, which determines how learning proceeds\n\n\n\n\n<h2> Layers: the building blocks of deep learning</h2>\nfrom keras import layers<br>\nlayer = layers.Dense(32, input_dim=data_dimension)) \n\n- We can think of layers as the LEGO bricks of deep learning, a metaphor that is\nmade explicit by frameworks like Keras. Building deep-learning models in Keras is\ndone by clipping together compatible layers to form useful data-transformation pipelines.\n\n\n<h2>What are activation functions, and why are they necessary?</h2>\nWithout an activation function like relu (also called a non-linearity), the Dense layer would consist of two linear operations‚Äîa dot product and an addition: <br><br>\n<i>output = dot(W, input) + b</i><br><br>\n\nSo the layer could only learn linear transformations (affine transformations) of the\ninput data: the hypothesis space of the layer would be the set of all possible linear\ntransformations of the input data into a 16-dimensional space. \n\n\n<h2>Loss functions and optimizers:<br>\nkeys to configuring the learning process</h2>\nOnce the network architecture is defined, you still have to choose two more things:\n- <b>Loss function (objective function) </b>- The quantity that will be minimized during\ntraining. It represents a measure of success for the task at hand.\n- <b>Optimizer</b> - Determines how the network will be updated based on the loss function.\nIt implements a specific variant of stochastic gradient descent (SGD)."},{"metadata":{"_kg_hide-input":false,"_uuid":"fdf8b2d922fc42a36edf3d89c4005e4fad12d195","_cell_guid":"d130235e-9fbb-43c2-b8eb-bcd4c92253b3","scrolled":false,"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"# Creating the model\nmodel = Sequential()\n\n# Inputing the first layer with input dimensions\nmodel.add(Dense(18, \n                activation='relu',  \n                input_dim=20,\n                kernel_initializer='uniform'))\n#The argument being passed to each Dense layer (18) is the number of hidden units of the layer. \n# A hidden unit is a dimension in the representation space of the layer.\n\n#Stacks of Dense layers with relu activations can solve a wide range of problems\n#(including sentiment classification), and you‚Äôll likely use them frequently.\n\n# Adding an Dropout layer to previne from overfitting\nmodel.add(Dropout(0.50))\n\n#adding second hidden layer \nmodel.add(Dense(12,\n                kernel_initializer='uniform',\n                activation='relu'))\n\n# Adding another Dropout layer\nmodel.add(Dropout(0.50))\n\n# adding the output layer that is binary [0,1]\nmodel.add(Dense(1,\n                kernel_initializer='uniform',\n                activation='sigmoid'))\n#With such a scalar sigmoid output on a binary classification problem, the loss\n#function you should use is binary_crossentropy\n\n#Visualizing the model\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bdc58455-2654-4e94-b85b-1e3454b9216a","_uuid":"8ef0e256989724dd31b6bae532d3a2af02ba542e"},"cell_type":"markdown","source":"Stacks of Dense layers with relu activations can solve a wide range of problems (including sentiment classification), and you‚Äôll likely use them frequently."},{"metadata":{"_cell_guid":"961e2f6b-361e-4d02-9842-a508ee757796","_uuid":"1569f597285e0bd77aca9219241c0296acd43276"},"cell_type":"markdown","source":"Finally, we need to choose a loss function and an optimizer. "},{"metadata":{"_kg_hide-input":false,"_uuid":"6df58ad87298e7900e7781ae67f2bd6a3b781678","_cell_guid":"62502eed-b769-485c-ab1b-81de2c497cb7","scrolled":false,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"#Creating an Stochastic Gradient Descent\nsgd = SGD(lr = 0.01, momentum = 0.9)\n\n# Compiling our model\nmodel.compile(optimizer = sgd, \n                   loss = 'binary_crossentropy', \n                   metrics = ['accuracy'])\n#optimizers list\n#optimizers['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n\n# Fitting the ANN to the Training set\nmodel.fit(X_train, y_train, \n               batch_size = 60, \n               epochs = 30, verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"975927a6-dfd2-442f-a5a8-1fb40a328214","_uuid":"9369c7449f6c21db79d09d9b52e5a7fe0d5ee8af"},"cell_type":"markdown","source":"Because you‚Äôre facing a binary classification problem and the output of your network is a probability (you end your network with a single-unit layer with a sigmoid activation), it‚Äôs best to use the <i>binary_crossentropy</i> loss."},{"metadata":{"_cell_guid":"45db3684-2673-463b-8c6e-d69f4db342c6","_uuid":"a0a0368d971a4dac38fbeefd99dbf74b3df41721"},"cell_type":"markdown","source":"<h1>Evaluating the model</h1>"},{"metadata":{"_cell_guid":"f6cf3ac1-c5aa-4912-a780-ba678c7e2c94","_uuid":"7aebe64fd7a8cf9d733897d69bba928b9c9503d1","trusted":true},"cell_type":"code","source":"scores = model.evaluate(X_train, y_train, batch_size=30)\nprint(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"816fe767-0da2-4b10-9b6c-d1263c0e5b6a","_uuid":"d69013e65e6fbe0e563540bab5d0cf3059b584de"},"cell_type":"markdown","source":"Not bad result to a simple model! Let's now verify the validation of our model, to see and understand the learning curve"},{"metadata":{"_cell_guid":"da9cd0f5-6220-4d0c-993b-a76fa1b317d9","_uuid":"e9de5ca3d82964f5da91eeeb36f1466aadb9054e"},"cell_type":"markdown","source":"<a id=\"Validation\"></a> <br> \n# **7. Validation: ** "},{"metadata":{"_cell_guid":"af1a5f3a-aaa1-471c-b230-b7697862f4ef","_uuid":"89c5fdce121c30cc80105b4ad152940ed850534a","trusted":true},"cell_type":"code","source":"# Fit the model\nhistory = model.fit(X_train, y_train, validation_split=0.20, \n                    epochs=180, batch_size=10, verbose=0)\n\n# list all data in history\nprint(history.history.keys())","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"15c6728d-e12f-4fc6-bd09-af4f4d06bf66","_uuid":"62472cde48707cee16733fd71c1dd1327631b0e4"},"cell_type":"markdown","source":"Let's look this keys values further"},{"metadata":{"_cell_guid":"4f21c6de-73fd-4cd9-8bed-296fcfeaa449","_uuid":"1d16d3b6dd4c15b2263c01a1f148e14610ec3a65","trusted":true},"cell_type":"code","source":"# summarizing historical accuracy\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"2988b955-4297-4f2d-b0d7-f6a79154ab86","_uuid":"91acde1d44a3e11b1ca45f1b6119411451589681"},"cell_type":"markdown","source":"Why this occurs and how to solve this problem in graph? it's a overffiting? "},{"metadata":{"_cell_guid":"1ca2b7d5-6eb7-4eee-8a42-e11db6866a69","_uuid":"ee846c160eb2f86a86903f9e2fe167aa5c2ba12b","trusted":true},"cell_type":"code","source":"\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"72fe4aff-59cf-4280-868f-71df47fb0ac9","_uuid":"95837b222c4bf3f4ee2358b49f51f0a276565c23","trusted":true},"cell_type":"code","source":"y_pred = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"85e133ea-91e6-4726-b8ef-faaee0679919","_uuid":"1c63aed38c73eed2204ae2220eb69f854edc1901","collapsed":true},"cell_type":"markdown","source":"<h1>It's my first Deep Learning implementation... I am studying about this and I will continue editing this Kernel to improve the results</h1>"},{"metadata":{"_cell_guid":"d30ca8b1-5f30-4d1e-9d4e-d5456e2002c3","_uuid":"6088e685d44c0ef56e0f6452427d46ead92715e6"},"cell_type":"markdown","source":"Give me your feedback how can I increase this model =) "},{"metadata":{"_cell_guid":"4ada7405-512f-4bd5-9ad9-782d673efe9b","_uuid":"9d119e200a82d5e1850bba3835b5c74a2d75df6d","trusted":true},"cell_type":"code","source":"# Trying to implementing the TensorBoard to evaluate the model\n\ncallbacks = [\n    keras.callbacks.TensorBoard(log_dir='my_log_dir',\n                                histogram_freq=1,\n                                embeddings_freq=1,\n                               )\n]\n\n#history = classifier.fit(X_train, y_train,\n#                         epochs=80,\n#                         batch_size=10,\n#                         validation_split=0.2,\n#                         callbacks=callbacks)\n\n#Its backing an error \n#ValueError: No variables to save\n\n#How to solve this ?","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6c3069a85900822d42baebb3d7bb9d10311049b9"},"cell_type":"code","source":"#Importing the auxiliar and preprocessing librarys \nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.cross_validation import cross_val_score\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.model_selection import train_test_split, KFold, cross_validate\nfrom sklearn.metrics import accuracy_score\n\n#Models\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import RidgeClassifier, SGDClassifier, LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, BaggingClassifier, VotingClassifier, RandomTreesEmbedding","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e2a6cd84-ef92-4f11-ae13-5e5e7e984d4e","_uuid":"d6ae353cac40213ee9b1c129550832de4e679d12","trusted":true},"cell_type":"code","source":"clfs = []\nseed = 3\n\nclfs.append((\"LogReg\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"LogReg\", LogisticRegression())])))\n\nclfs.append((\"XGBClassifier\",\n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"XGB\", XGBClassifier())]))) \nclfs.append((\"KNN\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"KNN\", KNeighborsClassifier())]))) \n\nclfs.append((\"DecisionTreeClassifier\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"DecisionTrees\", DecisionTreeClassifier())]))) \n\nclfs.append((\"RandomForestClassifier\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"RandomForest\", RandomForestClassifier())]))) \n\nclfs.append((\"GradientBoostingClassifier\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"GradientBoosting\", GradientBoostingClassifier(max_features=15, n_estimators=150))]))) \n\nclfs.append((\"RidgeClassifier\", \n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"RidgeClassifier\", RidgeClassifier())])))\n\nclfs.append((\"BaggingRidgeClassifier\",\n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"BaggingClassifier\", BaggingClassifier())])))\n\nclfs.append((\"ExtraTreesClassifier\",\n             Pipeline([(\"Scaler\", StandardScaler()),\n                       (\"ExtraTrees\", ExtraTreesClassifier())])))\n\n#'neg_mean_absolute_error', 'neg_mean_squared_error','r2'\nscoring = 'accuracy'\nn_folds = 7\n\nresults, names  = [], [] \n\nfor name, model  in clfs:\n    kfold = KFold(n_splits=n_folds, random_state=seed)\n    cv_results = cross_val_score(model, X_train, y_train, cv= 5, scoring=scoring, n_jobs=-1)    \n    names.append(name)\n    results.append(cv_results)    \n    msg = \"%s: %f (+/- %f)\" % (name, cv_results.mean(),  cv_results.std())\n    print(msg)\n    \n# boxplot algorithm comparison\nfig = plt.figure(figsize=(15,6))\nfig.suptitle('Classifier Algorithm Comparison', fontsize=22)\nax = fig.add_subplot(111)\nsns.boxplot(x=names, y=results)\nax.set_xticklabels(names)\nax.set_xlabel(\"Algorithmn\", fontsize=20)\nax.set_ylabel(\"Accuracy of Models\", fontsize=18)\nax.set_xticklabels(ax.get_xticklabels(),rotation=45)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e299f733-14be-4e8e-9f1a-cc50064acfff","_uuid":"adda421a3dfde3ba2343a679eed28bf96a37e1e2","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}