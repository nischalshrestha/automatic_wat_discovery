{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom sklearn.preprocessing import Imputer\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\n#print(os.listdir(\"../input\"))\ntrain_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')\n\nprint(train_df.columns.values)\n\ntrain_df.head()\n\ntrain_df.tail()\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_df.info()","execution_count":64,"outputs":[]},{"metadata":{"_cell_guid":"4003d4a8-13f5-44f4-9808-b17d8b519454","_uuid":"37a230c9de604f3f4dbca7aeb2d81f31abdfa6c0","trusted":true},"cell_type":"code","source":"test_df.info()","execution_count":65,"outputs":[]},{"metadata":{"_cell_guid":"13c48fbf-0834-4103-8dbb-ab5f36b3a59a","_uuid":"482ce88567f896f2e28ecd64ad80c1613438ff5e","trusted":true},"cell_type":"code","source":"train_df.describe()","execution_count":66,"outputs":[]},{"metadata":{"_cell_guid":"5e5a5471-9e88-4363-aea9-c64ed2fb0965","_uuid":"e74ce5a15555e9d1cacce7bbe45cf7cb04f01d94","trusted":true},"cell_type":"code","source":"train_df.describe(include=['O'])","execution_count":67,"outputs":[]},{"metadata":{"_cell_guid":"b1607fea-930a-473a-b1ed-d0a4524d0fc3","_uuid":"fad0227bcd70ebd6c37953d978aa5115e1bc35dd","trusted":true},"cell_type":"code","source":"def drop(data, columns):\n    return data.drop(columns, axis=1)\n\ncoltodrop = [\"PassengerId\",\"Name\", \"Cabin\", \"Ticket\"]\ntest_p_id= test_df[\"PassengerId\"]\n\ntrain_data = drop(train_df, coltodrop)\ntest_data = drop(test_df, coltodrop)\n\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8c8ea091-ac1e-4ec8-bfa2-6fded70acb75","_uuid":"b6b3ddfe3c5e3c1906dfef9f53348ef1ff717a8a","trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\ndef categ_to_int(data, lista, column):\n    le = LabelEncoder()\n    le.fit(lista)\n    data[column]=le.transform(data[column]) \n    return data\n\ntrain_data = categ_to_int(train_data,[\"male\",\"female\"], \"Sex\")\ntest_data = categ_to_int(test_data,[\"male\",\"female\"], \"Sex\")\n\ntrain_data.head()","execution_count":69,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e065175c97c8c9284d4f0b126f433569df5f93d3"},"cell_type":"code","source":"train_data[\"Embarked\"]=train_data[\"Embarked\"].fillna('Z')\ntrain_data = categ_to_int(train_data,[\"Q\", \"C\", \"S\", \"Z\"], \"Embarked\")\ntest_data = categ_to_int(test_data,[\"Q\", \"C\", \"S\", \"Z\"], \"Embarked\")\nprint(train_data[\"Embarked\"].describe())\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6e390cc573a3a80f8015ae1515406babaf72a1ec"},"cell_type":"code","source":"#centralize and normalize data\ndef nan_rid(data, columns):\n    for column in columns:\n        imputer=Imputer()\n        data[column]=imputer.fit_transform(data[column].values.reshape(-1,1))\n    return data\n\nnan_columns = [\"Age\", \"Fare\"]\n\n\ntrain_data = nan_rid(train_data, nan_columns)\ntest_data = nan_rid(test_data, nan_columns)\ntrain_data.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9daecf07-c2ad-420b-b55e-f9d09aab9ee5","_uuid":"1ac975311341662f7b908cb341a30ee202d8945d","trusted":true},"cell_type":"code","source":"def dummy_data(data, columns):\n    for column in columns:\n        data = pd.concat([data, pd.get_dummies(data[column], prefix=column)], axis=1)\n        data = data.drop(column, axis=1)\n    return data\n\n\ndummy_columns = [\"Pclass\", \"Sex\", \"Embarked\"]\ntrain_data=dummy_data(train_data, dummy_columns)\ntest_data=dummy_data(test_data, dummy_columns)\ntrain_data.head()\n\ntest_data[\"Embarked_3\"]= 0\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"56a99aa9-605d-4adb-bd2b-b6e01c573503","_uuid":"f4ff0d90ee4c6e84fb518bb6813f16d63a207c5a","trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\ndef normalize_data(data, column):\n    scaler = MinMaxScaler()\n    data[column] = scaler.fit_transform(data[column].values.reshape(-1,1))\n    return data\n\ntrain_data = normalize_data(train_data, \"Age\")\ntest_data = normalize_data(test_data, \"Age\")\n\ntrain_data = normalize_data(train_data, \"Fare\")\ntest_data = normalize_data(test_data, \"Fare\")\n\ntest_data[\"Embarked_3\"]= 0\n\ntest_data.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a8a29e71-0d77-46f4-b1ba-c3214be5a8d9","_uuid":"354c84864dabfce9227b63b1d25e062f3ded3fa7","collapsed":true,"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelBinarizer\nfrom sklearn.model_selection import train_test_split\n\ndef getDevset(data, fraction=(1 - 0.8)):\n    data_y = data[\"Survived\"]\n    lb = LabelBinarizer()\n    data_y = lb.fit_transform(data_y)\n\n    data_x = data.drop([\"Survived\"], axis=1)\n\n    train_x, dev_x, train_y, dev_y = train_test_split(data_x, data_y, test_size=fraction)\n\n    return train_x.values, train_y, dev_x, dev_y\n\ntrain_x, train_y, dev_x, dev_y = getDevset(train_data)\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"edeb8b86-377c-4d67-9e50-9e29b0efca6f","_uuid":"64ffdb1aa58357e107828f817c413a41d4c7be3c","collapsed":true,"trusted":true},"cell_type":"code","source":"from collections import namedtuple\n\ndef build_neural_network(hidden_unit1=10,hidden_unit2=10, lmbda=0.1):\n    tf.reset_default_graph()\n    inputs = tf.placeholder(tf.float32, shape=[None, train_x.shape[1]])\n    labels = tf.placeholder(tf.float32, shape=[None, 1])\n    learning_rate = tf.placeholder(tf.float32)\n    is_training=tf.Variable(True,dtype=tf.bool)\n    \n    initializer = tf.contrib.layers.xavier_initializer()\n    regularizer = tf.contrib.layers.l2_regularizer(scale = lmbda)\n    fc = tf.layers.dense(inputs, hidden_unit1, activation=None,kernel_initializer=initializer, kernel_regularizer = regularizer)\n    fc=  tf.layers.batch_normalization(fc, training=is_training)\n    fc= tf.nn.relu(fc)\n    fc2 = tf.layers.dense(fc, hidden_unit2, activation=None,kernel_initializer=initializer, kernel_regularizer = regularizer)\n    fc2= tf.layers.batch_normalization(fc2, training=is_training)\n    fc2=tf.nn.relu(fc)\n    \n    \n    logits = tf.layers.dense(fc2, 1, activation=None)\n    cross_entropy =  tf.nn.sigmoid_cross_entropy_with_logits(labels=labels, logits=logits)\n    \n    cost = tf.reduce_mean(cross_entropy)\n    \n    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n\n    predicted = tf.nn.sigmoid(logits)\n    correct_pred = tf.equal(tf.round(predicted), labels)\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n\n    # Export the nodes \n    export_nodes = ['inputs', 'labels', 'learning_rate','is_training', 'logits',\n                    'cost', 'optimizer', 'predicted', 'accuracy']\n    Graph = namedtuple('Graph', export_nodes)\n    local_dict = locals()\n    graph = Graph(*[local_dict[each] for each in export_nodes])\n\n    return graph\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"29ab4b88-92d0-4551-b231-ff59a1a5123f","_uuid":"632e4cfcd0f87eb3b7ac40cac364da67b63b9048","collapsed":true,"trusted":true},"cell_type":"code","source":"def get_batch(data_x,data_y,batch_size=32):\n    batch_n=len(data_x)//batch_size\n    for i in range(batch_n):\n        batch_x=data_x[i*batch_size:(i+1)*batch_size]\n        batch_y=data_y[i*batch_size:(i+1)*batch_size]\n        \n        yield batch_x,batch_y","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"61fa7637-19ae-4213-9db7-7f0debe4bcba","_uuid":"8b8b395a488af4f71b15af8d9e1ddc88b3ba1609","collapsed":true,"trusted":true},"cell_type":"code","source":"def TrainingParameters(hidden_unit1,hidden_unit2, lmbda, epochs, learning_rate,batch_size):\n\n    model = build_neural_network(hidden_unit1,hidden_unit2, lmbda)\n    saver = tf.train.Saver()\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        for e in range(epochs):\n            for batch_x,batch_y in get_batch(train_x,train_y,batch_size):\n                feed = {model.inputs: train_x,\n                        model.labels: train_y,\n                        model.learning_rate: learning_rate,\n                        model.is_training:True\n                       }\n\n                train_loss, _, train_acc = sess.run([model.cost, model.optimizer, model.accuracy], feed_dict=feed)\n\n        feed = {model.inputs: dev_x,\n                model.labels: dev_y,\n                model.is_training:False\n                }\n        val_loss, val_acc = sess.run([model.cost, model.accuracy], feed_dict=feed)\n        \n        saver.save(sess, \"./titanic.ckpt\")\n        \n        values = [hidden_unit1,hidden_unit2, lmbda, epochs, learning_rate,batch_size, train_loss,train_acc, val_loss, val_acc]\n        \n        return values","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a001421f-a45d-41fc-a7f3-0d02334ad74d","_uuid":"b85498d3cb2f7cd0ece84f31d7492aa605f6c8d8","collapsed":true,"trusted":true},"cell_type":"code","source":"# test different hyper parameters to find the best model\n\ndef test_hyperparameters():\n    import random as r\n\n    learning_rate = 0.001\n    hidden_unit1 = np.arange(10, 20, 5)\n    hidden_unit2 = np.arange(3, 10, 2)\n    epochs = np.arange(100, 200, 50)\n    batch_size = np.array([16,32,64])\n    colectValues = []\n\n\n    batch = [16,32,64]\n\n    for x in range(0, 100):\n        h1 = r.randint(10, 20)\n        h2 = r.randint(5, 15)\n        ep = r.randint(1,5) * 100\n        bs = r.choice(batch)\n        ex = -4 *np.random.rand()\n        lmbda = 10**ex\n        Results  =  TrainingParameters(int(h1),int(h2),lmbda,int(ep),learning_rate,int(bs))\n        print (Results)\n        colectValues.append(Results)\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"75e56c9f-ed3a-4388-8f75-f9affff9a678","_uuid":"9baeb07358d85f272d6c4fbcd1cc8fa0685ba686","trusted":true},"cell_type":"code","source":"#test_hyperparameters()\nfinalresult = TrainingParameters(18, 8, 0.0004046098616621213, 300, 0.001, 64)\n\nprint(finalresult)","execution_count":102,"outputs":[]},{"metadata":{"_cell_guid":"cc3d528a-723d-4106-986e-000ea391fc7f","_uuid":"cf1d8c365928c963de74f2d6ea0b260b3e3bf24c","trusted":true},"cell_type":"code","source":"\nmodel = build_neural_network(18, 8, 0.0004046098616621213)\n\nrestorer=tf.train.Saver()\nwith tf.Session() as sess:\n    restorer.restore(sess,\"./titanic.ckpt\")\n    feed={\n        model.inputs:test_data,\n        model.is_training:False\n    }\n    test_predict=sess.run(model.predicted,feed_dict=feed)\n    \n\n\nfrom sklearn.preprocessing import Binarizer\nbinarizer=Binarizer(0.5)\ntest_predict_result=binarizer.fit_transform(test_predict)\ntest_predict_result=test_predict_result.astype(np.int32)\ntest_predict_result[:10]","execution_count":104,"outputs":[]},{"metadata":{"_cell_guid":"71582c47-f19c-49ce-82bb-a797ec201edd","_uuid":"e621f02d3a9e124e6013d33427936b4c823cb142","trusted":true},"cell_type":"code","source":"passenger_id=test_p_id.copy()\nevaluation=passenger_id.to_frame()\nevaluation[\"Survived\"]=test_predict_result\nevaluation[:10]","execution_count":105,"outputs":[]},{"metadata":{"_cell_guid":"7fdcef57-7baa-4e49-8dc5-06a739178abc","_uuid":"7f9440bdf852a2f13b129f6289cc00f29bf1d923","trusted":true},"cell_type":"code","source":"print(evaluation)\nevaluation.to_csv(\"evaluation_submission.csv\",index=False)","execution_count":106,"outputs":[]},{"metadata":{"_cell_guid":"98e4ce80-a823-4ad4-b5d4-cb2d0d245cad","_uuid":"5d31aff15b1706e23acfd96b95c4c19df143523d","collapsed":true,"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}