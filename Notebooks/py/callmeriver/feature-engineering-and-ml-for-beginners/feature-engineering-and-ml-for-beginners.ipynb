{"cells":[{"metadata":{"_uuid":"b7630f19e9b358d8968e3ced61c206c92619556b"},"cell_type":"markdown","source":"**Titanic Survival Predictions**\n \n \n**I hope you'll find value in this project of mine. Any upvotes or comments of support are highly appreciated :)**\n\nI tried to make this kernal a fun kernal as well as an educational one. you'll see later on a funny incedent when trying to deal with missing age data, however what was important about displaying it in that sense was not just for entertainment. **it's also about how to deal with such incidents since they are bound to happen.**\n\nWell let's get right to it shall we :)\n\nWe are here to predict survival rates of those unfortune lads on the RMS Titanic (Jack.. don't go jack..).\n(btw he was a fictional character... just saying...)\n\nThis project is divided into 4 parts:\n> 1. Importing Data\n> 2. Exploring the Data\n> 3. Feature Engineering and Dealing with Missing Data\n> 4. Fitting the data to several ML models.\n\nWho will survive? if you watched the movie you'd probably guess.. women and children only! \n\nWell, let's see if that true. I'm sure there's more to this tragic event.\n\n**First things First! let's import some libraries:**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n#data analysis libraries \n\n#visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport os\nprint(os.listdir(\"../input\"))\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"Okay, after importing important libraries and inputting our data. let's take a peak."},{"metadata":{"trusted":true,"_uuid":"4b1244844161fce00cc34d7cb3849459a1ed0692"},"cell_type":"code","source":"#import train and test CSV files\ntrain = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\n\n#take a look at the training data and it's shape\nprint(train.shape, test.shape)\ntrain.describe(include=\"all\")\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6c81425d0111ef2fbf56ed5ed2d84487581fb042"},"cell_type":"markdown","source":"After import libraries, naturally the first step after that would be to check the nature of our data. it's always important to see the shape of the data before doing anything else. \n\nWe have more rows for the training data than the test data, and it seems that we have an extra column for the training data (can you guess what it is?) \n\nIt seems that we have a lot of NaNs to deal with as well. that's to be expected though.\n\nlet's see our columns:"},{"metadata":{"trusted":true,"_uuid":"bb6c4524d1c70d681993c66e5efd396e0d6724a3"},"cell_type":"code","source":"print (train.columns)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c081b10873d88aaf3845230fe4c06aa405bfd052"},"cell_type":"markdown","source":"if you haven't figured out what's the extra column, it's actually 'Survived'. that's what we are trying to do here after all. to predict who survived and who didn't.\n\nIt doesn't seem that we have too many features to deal with. let's see the first few rows of the training data. maybe we can see something there:"},{"metadata":{"trusted":true,"_uuid":"a7e394b0c067a60f3c7d91bc6a2619f73b2f69cc"},"cell_type":"code","source":"train.head()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"47df3afe7f7ad620e7c8323d8469765ca5361e0c"},"cell_type":"markdown","source":"it doesn't seem like we don't need passengerId since it won't help us with our predictions (no correlation between it and 'Survived')\n\nlet's go ahead and drop it (but will save it for later submisson of the results file):"},{"metadata":{"trusted":true,"_uuid":"1dee0da8038c854cbd548a6bf4d9db341f34a4be"},"cell_type":"code","source":"#Save the 'Id' column\ntrain_ID = train['PassengerId']\ntest_ID = test['PassengerId']\n\ntrain = train.drop('PassengerId',axis=1)\ntest = test.drop('PassengerId',axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"47649645ca71ce23c9350c8b8c8264f744c9d566"},"cell_type":"markdown","source":"Now let's see some correlations :)"},{"metadata":{"trusted":true,"_uuid":"de1673020377532bee7c9a214110a97dcb234273"},"cell_type":"code","source":"corrmat = train.corr()\nplt.subplots(figsize=(20, 9))\nsns.heatmap(corrmat, vmax=.8, annot=True);\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c0d3d5a88621d593adad1ff8452c1d7a62850230"},"cell_type":"markdown","source":"It seems that amongst the correlations in our heatmap (the numerical features), 'Pclass' seems to have the highest absolute value (0.34).\n\nBut wait, the categorical features (sex, cabin, ticket, embarked)?\n\n**What about the 'Sex' feature ?**"},{"metadata":{"trusted":true,"_uuid":"1e4c360d22ec46adede7b096900a51e7f1208064"},"cell_type":"code","source":"sns.countplot(x='Sex', hue='Survived', data=train, palette='RdBu')\ntrain[['Sex', 'Survived']].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b4afb897056acf1220ae0a3fe77b7f1d3daec972"},"cell_type":"markdown","source":"Aha.. we can see that from all survivors, about 3/4 of them were women.\n\n\n**What about the 'Embarked' feature ?**"},{"metadata":{"trusted":true,"_uuid":"819c9fdfb95b5990a82f1fe8228d588363556586"},"cell_type":"code","source":"# What about the 'Embarked' feature ?\nsns.countplot(x='Embarked', hue='Survived', data=train, palette='RdBu')\nplt.xticks([0,1,2],['Southampton','Cherbourg ','Queenstown '])\n# train[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ff0388e0160ec2754ce366f29672b8e4a6e3454e"},"cell_type":"markdown","source":"Aha.. another important insight, from the count plot we can  understand that the majority of the passengers were embarking from Southampton.\n\nNow, let's check how many survived from each class (since it had the highest correlation with survived as we saw from the heatmap)."},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"8abcf699494a37b81b943043b9cfab5636d9b724"},"cell_type":"code","source":"sns.barplot(train.Pclass ,train.Survived)\n# train[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"99681b46def63ee7680ab1df1e9d66156ee27d30"},"cell_type":"markdown","source":"As expected, those of the first class had the greatest survival rate while those of the third where the most unfortunate :(\n\nRemember that we said that it was mostly women and children?\n\nwell then, let's check the age feature and how to correlates with Survival."},{"metadata":{"trusted":true,"_uuid":"d652f2c3a3dbc9f1fba8e9374f45328cdbc94be8"},"cell_type":"code","source":"g = sns.FacetGrid(train, col='Survived')\ng.map(plt.hist, 'Age', bins=20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"50654637faae86f6b52b7522e70fe5421725b22d"},"cell_type":"markdown","source":"**Some conclusions from the graph:**\n\n1. if it looks strange to you that there are people with ages 0 who survived, that's because their age was missing.\n\n2. most people that didn't survive were between the ages of 15-25\n\n3. most people who survived we between the ages of 20-35\n\n4. babies of ages <5 had high survival rate.\n\n(These insights are based on the graph. To get more accurate results, we could simply filter the data by age groups and plot a bar plot, or even add age groups as features to the dataframe. **we will do that later when we will try to fill missing data**)\n\nokay..\n\nBefore we dive into feature engineering, let's join our training data and test data so that we won't get lost later and stay consistent with changes across the data"},{"metadata":{"trusted":true,"_uuid":"f19f7c083dbd63761ec32aece2e2f976745d973f"},"cell_type":"code","source":"ntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train.Survived.values\nall_data = pd.concat((train, test)).reset_index(drop=True)\n# all_data.drop(['Survived'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fbc97bada26a4c27a94163a937495bc4f1f49d4d"},"cell_type":"markdown","source":"Now let's see how much of our data is missing:\n"},{"metadata":{"trusted":true,"_uuid":"945b1e6fa1e761b8b74091935d4c5e78473297b1"},"cell_type":"code","source":"missingData = all_data.isnull().sum().sort_values(ascending=False)\npercentageMissing = ((all_data.isnull().sum()/all_data.isnull().count())*100).sort_values(ascending=False)\ntotalMissing = pd.concat([missingData, percentageMissing], axis=1, keys=['Total','Percentage'])\ntotalMissing","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ae8b8532a6b508151c71d245079d0502b0cb58df"},"cell_type":"markdown","source":"**Feature engineering & Dealing with missing data**\n\nMore than 3/4 of Cabin data is missing. hmmm.. \nI want to understand why. could that be indicative of survival/non-survival?\n\nwell, let's see.."},{"metadata":{"trusted":true,"_uuid":"bd090ddcda93c47522dd0948ad4231a89567d25a"},"cell_type":"code","source":"all_data[\"hasCabin\"] = (all_data[\"Cabin\"].notnull().astype('int'))\nsns.barplot(x=\"hasCabin\", y=\"Survived\", data=all_data)\nplt.show()\nall_data[['hasCabin', 'Survived']].groupby(['hasCabin'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n\n# all_data = all_data.dropna('Cabin',axis=1)\n# all_data = all_data.dropna('Embarked', axis=0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b51eb912ef39ab7268ab1715d8d47dd1d1096b46"},"cell_type":"markdown","source":"As suspected, Having a Cabin, as a good predictor of survival.\nThat's great. this could be an important feature in the endouver of predicting survival. \n\nlet's drop the 'Cabin' feature, and stay with 'hasCabin'.\n\nwe can also drop the Ticket column since it doesn't have any special pattern that could aid us with predictions."},{"metadata":{"trusted":true,"_uuid":"1f6681f5d530a13b9adcd74acb55d3c3ef7d686d"},"cell_type":"code","source":"all_data = all_data.drop('Cabin',axis=1)\nall_data = all_data.drop('Ticket',axis=1)\n\n\n# replacing the 2 missing values in the Embarked feature with S\n# since majority of people embarked in Southampton (S)\nall_data = all_data.fillna({\"Embarked\": \"S\"})\n\n\nall_data.head()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"02c9333fc5db29073b4255750790bd984d0678e6"},"cell_type":"markdown","source":"As we saw from the missing data table, the second most missing data we are dealing with is the 'Age' feature. \n\nA creative way to deal with this would be to look at people's names and try to classify them to age groups based on their titles. \n\nlet's try to extract all the titles (notice that each title ends with a '.') "},{"metadata":{"trusted":true,"_uuid":"77d636b57414910bcac9979efa9341c22632c01c"},"cell_type":"code","source":"all_data['Title'] = all_data['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(all_data['Title'], all_data['Sex'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b4dceccc7eac93ac977b06c186e4b6bc6976256"},"cell_type":"markdown","source":"We can replace many titles with a more common name or classify them as 'other'.\n\n"},{"metadata":{"trusted":true,"_uuid":"c8f70dbdf94efd400e1ea80c16b773d38c546590"},"cell_type":"code","source":"#get cell value: all_data.loc[0].at['Title']\n#set cell value: all_data.at[0,'Title'] = 'Mr'\n\nfor i,row in all_data.iterrows():\n    x = all_data.loc[i].at['Title']\n    if x in ['Capt','Col','Don' ,'Dr' ,'Major','Rev' ,'Sir']:\n        all_data.at[i,'Title']= 'Mr'\n    if x in ['Mlle','Ms' ,'Dona' ,'Lady']:\n        all_data.at[i,'Title']= 'Miss'\n    if x in ['Countess','Jonkheer','Mme']:\n        all_data.at[i,'Title'] = 'other'\n        \npd.crosstab(all_data['Title'], all_data['Sex'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6339b7b19f3e7a062b51d94623a1dec4c85d559b"},"cell_type":"markdown","source":"LOL!!!!\n\nit seems like we have a female Mr amongst our ship :P\n\nlet's try to figure out why...\n\nif you look at the Dr title, you'll realize that we have 7 male doctors and 1 female doctor.\n\nIt seems that i assumed all doctors were female (sorry for the sexism, wasn't intentional i swear :) )\n\nlet's fix that shall we ;)"},{"metadata":{"trusted":true,"_uuid":"38ae62050d26d234b5889e6b15dc6069a38d9d0d"},"cell_type":"code","source":"allFemales = all_data[all_data['Sex']=='female'] # select all females\nThatOneFemale = allFemales[all_data['Title']=='Mr'] # select all females with title Mr\nThatOneFemale","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"19ab92d17d2b062fda4396091b6c1bac6d0474ea"},"cell_type":"code","source":"# extracted the index of ThatOneFemale to be 796\nall_data.at[796,'Title']='Mrs'\n\npd.crosstab(all_data['Title'], all_data['Sex'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"18ba82a08d70bc1b2ead301e5992cd8b9391b793"},"cell_type":"markdown","source":"Alrighty then, moving on!\n\nNow, let's convert the categorical titles to numeric.\n"},{"metadata":{"trusted":true,"_uuid":"4adf5e9bcca51261ff68430371862ca8e614e43c"},"cell_type":"code","source":"title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"other\": 5}\nfor i,row in all_data.iterrows():\n    if all_data.loc[i].at['Title'] in title_mapping:\n        all_data.at[i,'Title']= title_mapping[all_data.loc[i].at['Title']]\n# all_data['Title']\nall_data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d24d98f962462c8fbc19ffa5514e56dc10ad3d4"},"cell_type":"markdown","source":"Now let's fill in missing Age data based on the mean for each Title:"},{"metadata":{"trusted":true,"_uuid":"7aa4ea88f14165af57e259be15c0018111877fd4"},"cell_type":"code","source":"Mr_age = all_data[all_data['Title']==1].Age.mean()\nMiss_age = all_data[all_data['Title']==2].Age.mean()\nMrs_age = all_data[all_data['Title']==3].Age.mean()\nMaster_age = all_data[all_data['Title']==4].Age.mean()\nOther_age = all_data[all_data['Title']==5].Age.mean()\nprint(Mr_age, Miss_age, Mrs_age , Master_age, Other_age)\n\ngroup_age_mapping = {1:Mr_age, 2: Miss_age, 3:Mrs_age, 4:Master_age, 5:Other_age}\n\nfor index,row in all_data.iterrows():\n    if np.isnan(all_data.loc[index].at['Age']):\n        all_data.at[index,'Age'] = group_age_mapping[all_data.loc[index].at['Title']]\n        ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8666cc763849e649abf4f9b02814915ee175824e"},"cell_type":"markdown","source":"Now drop the Name column since we don't need it anymore."},{"metadata":{"trusted":true,"_uuid":"18f597e48272af6f20864996cd686e41592b2637"},"cell_type":"code","source":"all_data.drop('Name',axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c123020d2bdf0c48020f067a4ed2204df7c8a8d7"},"cell_type":"markdown","source":"coverting the 'Sex' and 'Embarked' features to numeric:"},{"metadata":{"trusted":true,"_uuid":"0e13ac29d28582007430e9f03ce48e346e9b15a9"},"cell_type":"code","source":"sex_mapping = {\"male\": 0, \"female\": 1}\nembarked_mapping = {\"S\": 1, \"C\": 2, \"Q\": 3}\n\nfor i,row in all_data.iterrows():\n    if all_data.loc[i].at['Sex'] in sex_mapping:\n        all_data.at[i,'Sex']= sex_mapping[all_data.loc[i].at['Sex']]\n    if all_data.loc[i].at['Embarked'] in embarked_mapping:\n        all_data.at[i,'Embarked']= embarked_mapping[all_data.loc[i].at['Embarked']]\nall_data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f3c21163bdd605b1cc99df967c8848203ce9cc5d"},"cell_type":"markdown","source":"Now for the Fare, we can complete the missing value, with the most frequent value. \n\nand dropping th survived column since we won't be needing that anymore (but will be used for modeling)."},{"metadata":{"trusted":true,"_uuid":"d6ed08be474399ecaa381cc6b90b2344f8e4bd40"},"cell_type":"code","source":"mode = all_data['Fare'].mode() # extract the mode\nall_data['Fare'].fillna(mode[0], inplace=True) # fill NaNs with the mode\n\nall_data.drop('Survived',axis=1, inplace=True) # drop survived column\n\nmissingData = all_data.isnull().sum().sort_values(ascending=False)\npercentageMissing = ((all_data.isnull().sum()/all_data.isnull().count())*100).sort_values(ascending=False)\ntotalMissing = pd.concat([missingData, percentageMissing], axis=1, keys=['Total','Percentage'])\ntotalMissing\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"332d7f9816679aceb2c70c392847ef803048ca23"},"cell_type":"markdown","source":"Cool! no more missing values :)\n\nlet's take a look at our data and how it's coming along.\n"},{"metadata":{"trusted":true,"_uuid":"411912a34068abe9cb18fd1624e7bf9e79034165"},"cell_type":"code","source":"all_data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f04bc05350d596606a88466a18bf2c8ea578b39"},"cell_type":"markdown","source":"Hmmm, everything seems in order. \n\nWe just have this 'Fare' feature that looks like it has potential to guide us with our predictions. \n\nLet catogrize the 'Fare' feature as well. we'll do that by dividing it to 4 caregories."},{"metadata":{"trusted":true,"_uuid":"a7b4e44c9ae0d0909a1adaf4b627fa056f3bc11d"},"cell_type":"code","source":"FareBand = pd.qcut(all_data['Fare'], 4)\nFareBand.unique()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d6e22c5ddf2b4ed081e3106da25778fdfaad91ec"},"cell_type":"markdown","source":"okay, let's adjust the 'Fare' feature:"},{"metadata":{"trusted":true,"_uuid":"dd79c0e99b507ef8cdd652b0e1fc109bf1054bcc"},"cell_type":"code","source":"# get cell value: all_data.loc[0].at['Title']\n# set cell value: all_data.at[0,'Title'] = 'Mr'\n\nfor i,row in all_data.iterrows():\n    currFare=all_data.loc[i].at['Fare']\n    if (currFare > -0.001 and currFare <=7.896):\n        all_data.at[i,'Fare'] = 1\n    if (currFare > 7.896 and currFare <=14.454):\n        all_data.at[i,'Fare'] = 2\n    if (currFare > 140454 and currFare <=31.275):\n        all_data.at[i,'Fare'] = 3\n    if (currFare > 31.275 and currFare <=512.329):\n        all_data.at[i,'Fare'] = 4\n        \nall_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3557aa73cad98751f8f8e2a8e66d88ac00773d6f"},"cell_type":"markdown","source":"Awesome! now that our data is ready. we can go ahead to the fun part of this project :)\n\n***Modeling and Machine Learn***\nFirst things first, let's split our data to training data (to fit the model) and test data (to evaluate the model later on)."},{"metadata":{"trusted":true,"_uuid":"d1e7e293a81a56cf8358f38b85612147c1c2b046"},"cell_type":"code","source":"target = train['Survived']\ntrainData = all_data[0:ntrain]\ntestData = all_data[ntrain:]\ntarget.shape, trainData.shape, testData.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5cf606b43980cdf2659abf864733131d21b2f37d"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test,y_train, y_test = train_test_split(trainData, target, test_size=0.2, random_state=0)\n\nX_train.shape, y_train.shape, X_test.shape, y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ffc59cdc7eba3f59243458ad79bd0f24e6096379"},"cell_type":"markdown","source":"**Selecting the Best Model**\n\nAn accepted approch for selecting the best model is to try many differnt model and choose the model with the best results!"},{"metadata":{"trusted":true,"_uuid":"ec8bf3969d739974ad6aa1f169919750abf378a6"},"cell_type":"code","source":"# Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\ngaussian = GaussianNB()\ngaussian.fit(X_train, y_train)\ny_pred = gaussian.predict(X_test)\nacc_gaussian = round(accuracy_score(y_pred, y_test), 2)\nprint(acc_gaussian)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"db182d02e46197e10188d3e18d83a34d88a2595b"},"cell_type":"code","source":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test)\nacc_logreg = round(accuracy_score(y_pred, y_test), 2)\nprint(acc_logreg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f53fa572dcb2d321f6d4fdaea2827b210f66eb5f"},"cell_type":"code","source":"# Support Vector Machines\nfrom sklearn.svm import SVC\n\nsvc = SVC()\nsvc.fit(X_train, y_train)\ny_pred = svc.predict(X_test)\nacc_svc = round(accuracy_score(y_pred, y_test), 2)\nprint(acc_svc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d874ca97e0fee9c24d9b8801015feaa9fba91242"},"cell_type":"code","source":"#Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\n\ndecisiontree = DecisionTreeClassifier()\ndecisiontree.fit(X_train, y_train)\ny_pred = decisiontree.predict(X_test)\nacc_decisiontree = round(accuracy_score(y_pred, y_test), 2)\nprint(acc_decisiontree)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7977663f86fa157ba11f137fe06f652b59eb8d55"},"cell_type":"code","source":"# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n\nrandomforest = RandomForestClassifier()\nrandomforest.fit(X_train, y_train)\ny_pred = randomforest.predict(X_test)\nacc_randomforest = round(accuracy_score(y_pred, y_test) , 2)\nprint(acc_randomforest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d7af94d8896c3b389c8598948827be2aa0845329"},"cell_type":"code","source":"# Gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngbk = GradientBoostingClassifier()\ngbk.fit(X_train, y_train)\ny_pred = gbk.predict(X_test)\nacc_gbk = round(accuracy_score(y_pred, y_test) , 2)\nprint(acc_gbk)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1701bf296fe3b4baee29f4bed0976e015280c84c"},"cell_type":"code","source":"#predictions for submission\npredictions = gbk.predict(testData)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"83a4d8cd0eca7fc1088a701929d9e67a0130e335"},"cell_type":"markdown","source":"There are more classificationn algorithms that we can try. \n\n* like KNN or k-Nearest Neighbors\n* Perceptron\n* Stochastic Gradient Descent\n\nand more...\n\nhowever, it seems that Gradient Boosting Classifier gives us the best model. let's go ahead and submit the results ;)"},{"metadata":{"trusted":true,"_uuid":"3dac1784bf6f183e33abca49aceb31bc394b5e4d"},"cell_type":"code","source":"output = pd.DataFrame({ 'PassengerId' : test_ID, 'Survived': predictions })\noutput.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"949ed5933831bb4528dda5ad6a681ae6eec21639"},"cell_type":"markdown","source":"**Thank you for joining me on this wonderful journey to Data Science :)**\n\n**I hope you found value in this project of mine. Any upvotes or comments of support are highly appreciated :)**"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}