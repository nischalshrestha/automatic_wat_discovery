{"cells":[{"metadata":{"_uuid":"347aa77215e2e4f028a5f8cd04a15034b34edda2"},"cell_type":"markdown","source":"My Introduction: I am a newbie in Machine Learning and Python. I started taking classroom course for Machine Learning recently, so, quickly went through Python concepts and syntaxes to start with classroom course. Now, trying this competition to improve my skills and to see where I stand. This is just my modified model over my baseline model (https://www.kaggle.com/abhinav9384/titanic-solution-baseline-model/). which gave me some rank around 9k  (almost in the end).\n\nThis model gave me jump in 5646 places jump in leaderboard, giving me rank 3317. Pls do visit both and provide your suggestions. This is still work in progress as you can see from my TO DO list.\n\nSince I am newbie in both ML and Python, did lot of googling and kernel browsing to see how to write logics in python, in addition to what I learned in class. You may see some of your code here, Thanks all for help by posting your kernels as Public. This helps newbies like to me to learn a lot from you guys!!"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\ntrain_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')\ncombine_df = [train_df, test_df]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b22fed05a9b26d597b358dea2227fd722038fd89"},"cell_type":"code","source":"# TO DO activities\n# 1. check usefulness of ticket column\n# 2. do missing value imputation in embarked and check if it is useful\n# Till then , drop both of them to get model going\n# 3. instead of pd.get dummies, do 1 hot encoding on title and gender\n# 4. data visualization\n# 5. do age imputation through code, not manually\n# 6. Do Grid Search / Paramter Tuning\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fa599581c36ad1c6d37499c9944a9d27ee6cd3b8"},"cell_type":"code","source":"\n# count the number of NaN values in each column\nprint(train_df.isnull().sum())\nprint(test_df.isnull().sum())\n\n#Imputation for Fare since its just 1 record missing in test.csv/test.df\n#Lets see what data this contains\ntest_df[test_df.Fare.isnull()]\n'''\ntest_df[(test_df[\"Pclass\"] == 3) & (test_df[\"Sex\"] == \"male\") & (test_df[\"Embarked\"] == \"S\") \n& (test_df[\"Age\"] > 49.9) & (test_df[\"Age\"] < 75)]\n#since test df has only 1 row which satisfies above conditions, checking in train_df\ntrain_df[(train_df[\"Pclass\"] == 3) & (train_df[\"Sex\"] == \"male\") & (train_df[\"Embarked\"] == \"S\") \n& (train_df[\"Age\"] > 49.9) & (train_df[\"Age\"] < 75)]\n'''\n#By seeing above data, filling only NaN fare in test_df, with passenger id of 327 who has similar \n#characteristics : male, pclass=3, age=61\ntest_df[\"Fare\"] = test_df[\"Fare\"].fillna(6.2375)\n#confirm imputation happened\ntest_df[test_df.Fare.isnull()]  #gives no results after imputation\ntest_df[test_df[\"PassengerId\"] == 1044]\n\n#Before doing age imputation, create new features: familySize, isAlone, Title\nfor df in combine_df:\n    df[\"FamilySize\"] = df[\"SibSp\"] + df[\"Parch\"] + 1\ntrain_df[\"IsAlone\"] = [1 if i<2 else 0 for i in train_df.FamilySize]\ntest_df[\"IsAlone\"] = [1 if i<2 else 0 for i in test_df.FamilySize]\n\nfor df in combine_df:\n    df['Title'] = df.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\n#Check if any relation between Age and Title\n'''\ntrain_df[['Title', 'Sex', 'Age']].groupby(['Title', 'Sex'], as_index=False).mean()\ntrain_df[['Title', 'Sex', 'Age']].groupby(['Title', 'Sex'], as_index=False).max()\ntrain_df[['Title', 'Sex', 'Age']].groupby(['Title', 'Sex'], as_index=False).min()\n\ntest_df[['Title', 'Sex', 'Age']].groupby(['Title', 'Sex'], as_index=False).mean()\ntest_df[['Title', 'Sex', 'Age']].groupby(['Title', 'Sex'], as_index=False).max()\ntest_df[['Title', 'Sex', 'Age']].groupby(['Title', 'Sex'], as_index=False).min()\n'''\n#Surprise in data\n#Some girls with age less than 18 are also marked as Mrs in data\n\n\n# reduce titles so that it is easy to categorize people in groups\n#Not changing Title Capt as its 60+ yrs, just want to keep it separate\nfor df in combine_df:\n       df['Title'] = np.where((df['Sex'] == 'male') & \n            (df['Title'].isin (['Col', 'Don', 'Dr', 'Rev', 'Sir', 'Jonkheer', 'Major'])), \n                              'Mr', df['Title'])\n# marking all ladies in just master and Ms\n#This is because some girls with age less than 18 are also marked as Mrs in data\nfor df in combine_df:        \n       df['Title'] = np.where((df['Sex'] == 'female') & \n            (df['Title'].isin (['Countess', 'Lady', 'Dr', 'Dona', 'Mrs', 'Miss', 'Mme', 'Mlle'])), \n                              'Ms', df['Title'])\n\n\n#Do age imputation based on Title & Sex\n# We have 2 values in Sex and 4 values in Title (Capt, Master, Mr, Ms)\n# create temporary df and store all results of sex, title, age\n# then take their mean and impute\ntmp_df1 = train_df.iloc[:, [4, 14, 5]]\ntmp_df2 = test_df.iloc[:, [3, 13, 4]]\ntmp_df = pd.concat([tmp_df1, tmp_df2], axis = 0)\ntmp_df.dropna(inplace=True)\n\n'''\ntmp_df[['Title', 'Sex', 'Age']].groupby(['Title', 'Sex'], as_index=False).mean()\nabove o/p looks is. Using it manually to impute values as not able to write python code\n\n    Title     Sex        Age\n0    Capt    male  70.000000\n1  Master    male   5.482642\n2      Mr    male  32.722682\n3      Ms  female  28.687088\n'''\n#if sex= female & title is Ms, then impute age as 29.68\ntrain_df['Age'] = np.where(((train_df.Age.isnull()) & \n        (train_df['Sex'] == 'female') & (train_df['Title'] == 'Ms')), 29.68, train_df['Age'])\ntest_df['Age'] = np.where(((test_df.Age.isnull()) & \n        (test_df['Sex'] == 'female') & (test_df['Title'] == 'Ms')), 29.68, test_df['Age'])\n\n#if sex= male & title is Capt, then impute age as 70\ntrain_df['Age'] = np.where(((train_df.Age.isnull()) & \n        (train_df['Sex'] == 'male') & (train_df['Title'] == 'Capt')), 70, train_df['Age'])\ntest_df['Age'] = np.where(((test_df.Age.isnull()) & \n        (test_df['Sex'] == 'male') & (test_df['Title'] == 'Capt')), 70, test_df['Age'])\n\n\n#if sex= male & title is Mr, then impute age as 32.72\ntrain_df['Age'] = np.where(((train_df.Age.isnull()) & \n        (train_df['Sex'] == 'male') & (train_df['Title'] == 'Mr')), 32.72, train_df['Age'])\ntest_df['Age'] = np.where(((test_df.Age.isnull()) & \n        (test_df['Sex'] == 'male') & (test_df['Title'] == 'Mr')), 32.72, test_df['Age'])\n\n\n#if sex= male & title is Master, then impute age as 5.48\ntrain_df['Age'] = np.where(((train_df.Age.isnull()) & \n        (train_df['Sex'] == 'male') & (train_df['Title'] == 'Master')), 5.48, train_df['Age'])\ntest_df['Age'] = np.where(((test_df.Age.isnull()) & \n        (test_df['Sex'] == 'male') & (test_df['Title'] == 'Master')), 5.48, test_df['Age'])\n\ntrain_df = pd.get_dummies(train_df, prefix=\"G\", columns=[\"Sex\"])\ntest_df = pd.get_dummies(test_df, prefix=\"G\", columns=[\"Sex\"])\n\ntrain_df = pd.get_dummies(train_df, columns=[\"Title\"])\ntest_df = pd.get_dummies(test_df, columns=[\"Title\"])\n\nX = train_df.loc[:, ['Pclass', 'Age', 'FamilySize', 'IsAlone', 'G_female', 'G_male', 'Title_Master', 'Title_Mr', 'Title_Ms']].values\ny = train_df.loc[:,['Survived']].values\n\n#to be used in submission\nX_test_df = test_df.loc[:, ['Pclass', 'Age', 'FamilySize', 'IsAlone', 'G_female', 'G_male', 'Title_Master', 'Title_Mr', 'Title_Ms']].values\n\n# Splitting the dataset into the Training set and Test set from train_df itself\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n\n#Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n#test.csv data\nX_test_df = sc.transform(X_test_df)\n\n#-------------------- Logistic Regression ------------------------------\n# Fitting Logistic Regression to the Training set\nfrom sklearn.linear_model import LogisticRegression\nclassifier_lr = LogisticRegression()\nclassifier_lr.fit(X_train, y_train)\n\n\n# Predicting the Test set results\ny_pred_lr = classifier_lr.predict(X_test)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix,accuracy_score,classification_report\ncm_lr = confusion_matrix(y_test, y_pred_lr)\nac_lr = accuracy_score(y_test, y_pred_lr)\nprint(classification_report(y_test, y_pred_lr))\n\n\n#-------------------- KNN ------------------------------\n# Fitting KNN to the Training set\nfrom sklearn.neighbors import KNeighborsClassifier\nclassifier_knn = KNeighborsClassifier(n_neighbors = 5, p = 2)\nclassifier_knn.fit(X_train, y_train)\n\n# Predicting the Test set results\ny_pred_knn = classifier_knn.predict(X_test)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix,accuracy_score,classification_report\ncm_knn = confusion_matrix(y_test, y_pred_knn)\nac_knn = accuracy_score(y_test, y_pred_knn)\nprint(classification_report(y_test, y_pred_knn))\n\n#-------------------- Decision Tree ------------------------------\nfrom sklearn.tree import DecisionTreeClassifier\nclassifier_dt = DecisionTreeClassifier()\nclassifier_dt.fit(X_train, y_train)\n\n\n# Predicting the Test set results\ny_pred_dt = classifier_dt.predict(X_test)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix,accuracy_score,classification_report\ncm_dt = confusion_matrix(y_test, y_pred_dt)\nac_dt = accuracy_score(y_test, y_pred_dt)\nprint(classification_report(y_test, y_pred_dt))\n\n#-------------------- Random Forest ------------------------------\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier_rf = RandomForestClassifier(n_estimators=100)\nclassifier_rf.fit(X_train, y_train)\n\n\n# Predicting the Test set results\ny_pred_rf = classifier_rf.predict(X_test)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix,accuracy_score,classification_report\ncm_rf = confusion_matrix(y_test, y_pred_rf)\nac_rf = accuracy_score(y_test, y_pred_rf)\nprint(classification_report(y_test, y_pred_rf))\n\n#-------------------- SVM ------------------------------\n# Fitting SVM to the Training set\nfrom sklearn.svm import SVC, LinearSVC\nclassifier_svc = SVC()\nclassifier_svc.fit(X_train, y_train)\n\n# Predicting the Test set results\ny_pred_svc = classifier_knn.predict(X_test)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix,accuracy_score,classification_report\ncm_svc = confusion_matrix(y_test, y_pred_svc)\nac_svc = accuracy_score(y_test, y_pred_svc)\nprint(classification_report(y_test, y_pred_svc))\n\n\nscore_dict = {\"Random Forest Score\": round(ac_rf*100, 2),\n              \"Decision Tree Score\": round(ac_dt*100, 2),\n              \"KNN Score\": round(ac_knn*100, 2),\n              \"Logistic Regression Score\": round(ac_lr*100, 2),\n              \"SVC\": round(ac_svc*100, 2)\n        }\n\n#Improving model performance\n#-------------------- Applying k-fold cross validation ------------------------\nfrom sklearn.model_selection import cross_val_score\n# -- for logistic regression\naccuracies_lr = cross_val_score(classifier_lr, X = X_train, y = y_train, cv=10)\naccuracies_lr.mean()\naccuracies_lr.std()\n# -- for SVC\naccuracies_svc = cross_val_score(classifier_svc, X = X_train, y = y_train, cv=10)\naccuracies_svc.mean()\naccuracies_svc.std()\n# -- for random forest regression\naccuracies_rf = cross_val_score(classifier_rf, X = X_train, y = y_train, cv=10)\naccuracies_rf.mean()\naccuracies_rf.std()\n\n#Applying grid search to find best model and best parameters\n#SVC\n'''\nfrom sklearn.model_selection import GridSearchCV\nparameters_svc = [{'C':[1, 10, 100, 1000], 'kernel': ['linear']},\n                  {'C':[1, 10, 100, 1000], 'kernel': ['rbf'], 'gamma':[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]}\n                 ]\n\ngrid_search_svc = GridSearchCV(estimator = classifier_svc,\n                                param_grid = parameters_svc,\n                                scoring = 'accuracy',\n                                cv = 10,\n                                n_jobs = -1\n                                )\ngrid_search_svc = grid_search_svc.fit(X_train, y_train) \nbest_accuracy_svc = grid_search_svc.best_score_\nbest_params_svc = grid_search_svc.best_params_\n\n# Fitting Final Model on training set\nfrom sklearn.svm import SVC, LinearSVC\nclassifier_svc = SVC(C = 1, kernel = 'rbf', gamma = 0.4, random_state = 0)\nclassifier_svc.fit(X_train, y_train)\ncm_svc_tuned = confusion_matrix(y_test, y_pred_svc)\nac_svc_tuned = accuracy_score(y_test, y_pred_svc)\nprint(classification_report(y_test, y_pred_svc))'''\n#this gives no improvement, so not using it\n\n#Random Forest\nparameters_rf = {\"n_estimators\": [10, 20, 30, 50, 100, 300, 500, 1000],\n              \"max_depth\": [1, 3, 5],\n              \"min_samples_split\": [5, 10, 15],\n              \"min_samples_leaf\": [5, 10, 15],\n              \"min_weight_fraction_leaf\": [0.1, 0.05, 0.005]}\n\ngrid_search_rf = GridSearchCV(estimator = classifier_rf,\n                                param_grid = parameters_rf,\n                                scoring = 'accuracy',\n                                cv = 10,\n                                n_jobs = -1\n                                )\ngrid_search_rf = grid_search_rf.fit(X_train, y_train) \nbest_accuracy_rf = grid_search_rf.best_score_\nbest_params_rf = grid_search_rf.best_params_\n\n# Fitting Final Model on training set\nclassifier_rf_tuned = RandomForestClassifier(random_state = 0)\nclassifier_rf_tuned.fit(X_train, y_train)\ncm_rf_tuned = confusion_matrix(y_test, y_pred_rf)\nac_rf_tuned = accuracy_score(y_test, y_pred_rf)\nprint(classification_report(y_test, y_pred_rf))\n\n\n\n#Since lr score is best, using it to derive y_pred for test_df\ny_pred_final = classifier_lr.predict(X_test_df)\n\nsubmission = pd.DataFrame({\"PassengerId\": test_df[\"PassengerId\"], \"Survived\": y_pred_final})\nsubmission.to_csv(\"Titanic_Baseline_Model_Submission.csv\", index=False)\n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}