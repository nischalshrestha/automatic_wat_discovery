{"cells":[{"metadata":{"_uuid":"599be56df6392bbd8ee761dc8696439430981aa1"},"cell_type":"markdown","source":"My Introduction: I am a newbie i Machine Learning and Python. I started taking classroom course for Machine Learning, quickly went through Python concepts and syntaxes to start with classroom course. Now, trying this competition to improve my skills and to see where I stand. This is just my baseline model. No fancy things done (although I dont know anyways :) ). Missing data imputation done in simplest way. This gave some rank around 9k  (almost in the end).\n\nMy modified model (https://www.kaggle.com/abhinav9384/titanic-survival-modified) gave me jump in 5646 places jump in leaderboard, giving me rank 3317. Pls do visit both and provide your suggestions.\n\nSince I am newbie in both ML and Python, did lot of googling and kernel browsing to see how to write logics in python, in addition to what I learned in class. You may see some of your code here, Thanks all for help by posting your kernels as Public. This helps newbies like to me to learn a lot from you guys!!"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\n#import dataset\ntrain_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')\n# count the number of NaN values in each column\n#import dataset\n# count the number of NaN values in each column\nprint(train_df.isnull().sum())\nprint(test_df.isnull().sum())\n# above tells imputation required for three columns: age. cabin, embarked\n\ntrain_df[\"Survived\"].value_counts()\ntrain_df[\"Pclass\"].value_counts()\ntrain_df[\"Sex\"].value_counts()\ntrain_df[\"Embarked\"].value_counts()\n\n# Taking care of missing data\ntrain_df[\"Age\"] = train_df[\"Age\"].fillna(np.mean(train_df[\"Age\"]))\ntest_df[\"Age\"] = test_df[\"Age\"].fillna(np.mean(test_df[\"Age\"]))\n\n#covert Sex column to integer\ntrain_df = pd.get_dummies(train_df, prefix=\"G\", columns=[\"Sex\"])\ntest_df = pd.get_dummies(test_df, prefix=\"G\", columns=[\"Sex\"])\n\n#Deine X & y\nX = train_df.iloc[:, [2, 4, 5, 6, 11, 12]].values\ny = train_df.iloc[:,1:2].values\n\n#to be used in submission\nX_test_df = test_df.iloc[:, [1, 3, 4, 5, 10, 11]].values\nprint(test_df.isnull().sum())\n\n\n# Splitting the dataset into the Training set and Test set from train_df itself\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n\n#Feature Scaling\n#TO DO try diff scaling and normalization\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\nX_test_df = sc.transform(X_test_df)\n\n#-------------------- Logistic Regression ------------------------------\n# Fitting Logistic Regression to the Training set\nfrom sklearn.linear_model import LogisticRegression\nclassifier_lr = LogisticRegression()\nclassifier_lr.fit(X_train, y_train)\n\n\n# Predicting the Test set results\ny_pred_lr = classifier_lr.predict(X_test)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix,accuracy_score,classification_report\ncm_lr = confusion_matrix(y_test, y_pred_lr)\nac_lr = accuracy_score(y_test, y_pred_lr)\nprint(classification_report(y_test, y_pred_lr))\n\n#-------------------- KNN ------------------------------\n# Fitting KNN to the Training set\nfrom sklearn.neighbors import KNeighborsClassifier\nclassifier_knn = KNeighborsClassifier(n_neighbors = 5, p = 2)\nclassifier_knn.fit(X_train, y_train)\n\n# Predicting the Test set results\ny_pred_knn = classifier_knn.predict(X_test)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix,accuracy_score,classification_report\ncm_knn = confusion_matrix(y_test, y_pred_knn)\nac_knn = accuracy_score(y_test, y_pred_knn)\nprint(classification_report(y_test, y_pred_knn))\n\n#-------------------- Decision Tree ------------------------------\nfrom sklearn.tree import DecisionTreeClassifier\nclassifier_dt = DecisionTreeClassifier()\nclassifier_dt.fit(X_train, y_train)\n\n\n# Predicting the Test set results\ny_pred_dt = classifier_dt.predict(X_test)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix,accuracy_score,classification_report\ncm_dt = confusion_matrix(y_test, y_pred_dt)\nac_dt = accuracy_score(y_test, y_pred_dt)\nprint(classification_report(y_test, y_pred_dt))\n\n#-------------------- Random Forest ------------------------------\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier_rf = RandomForestClassifier(n_estimators=100)\nclassifier_rf.fit(X_train, y_train)\n\n\n# Predicting the Test set results\ny_pred_rf = classifier_rf.predict(X_test)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix,accuracy_score,classification_report\ncm_rf = confusion_matrix(y_test, y_pred_rf)\nac_rf = accuracy_score(y_test, y_pred_rf)\nprint(classification_report(y_test, y_pred_rf))\n\n#-------------------- SVM ------------------------------\n# Fitting SVM to the Training set\nfrom sklearn.svm import SVC, LinearSVC\nclassifier_svc = SVC()\nclassifier_svc.fit(X_train, y_train)\n\n# Predicting the Test set results\ny_pred_svc = classifier_knn.predict(X_test)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix,accuracy_score,classification_report\ncm_svc = confusion_matrix(y_test, y_pred_svc)\nac_svc = accuracy_score(y_test, y_pred_svc)\nprint(classification_report(y_test, y_pred_svc))\n\nscore_dict = {\"Random Forest Score\": round(ac_rf*100, 2),\n              \"Decision Tree Score\": round(ac_dt*100, 2),\n              \"KNN Score\": round(ac_knn*100, 2),\n              \"Logistic Regression Score\": round(ac_lr*100, 2),\n              \"SVC\": round(ac_svc*100, 2)\n        }\n\n#Since rf score is best, using it to derive y_pred for test_df\ny_pred_final = classifier_rf.predict(X_test_df)\n\nsubmission = pd.DataFrame({\"PassengerId\": test_df[\"PassengerId\"], \"Survived\": y_pred_final})\nsubmission.to_csv(\"Titanic_Baseline_Model_Submission.csv\", index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"submission.to_csv(\"Titanic_Baseline_Model_Submission.csv\")\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}