{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# I have followed this great kernel has a guide: https://www.kaggle.com/carlbeckerling/kaggle-titanic-tutorial/notebook\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport tensorflow as tf\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"14f7dbe444002f6413bc729f9c80ec3b7a3bf0c7"},"cell_type":"code","source":"# let's load the data into some variables,for that we use the pandas library\ntrain = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\n\nprint(train.keys())\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c08639db5a4de7f07edd534dc6865bb2d7f0092a"},"cell_type":"code","source":"# train_x will have train except PassengerId column\n# train_y will have the survived \ntrain_x, train_y = train, train.pop(\"Survived\")\n\n# same as above\n# test is used to evaluate the model, for the time being I am not doing that because I just want to do a submission file to see the whole process\n# TODO: do the evaluation of the model to check the accuracy\n# test_x, test_y = test, test.pop(\"Survived\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"39a3ba5c11d77e49bad6bb8f0657e6af6f8e6953"},"cell_type":"code","source":"print(train_x.keys())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4918d08f5196d87098672bf61bd1079f760f95ae"},"cell_type":"code","source":"# let's define the feature columns, for sure there is a best practice here, but this is my first Notebook so I will add a TODO here.\n# TODO: Look for a good strategy to determine which columns are good candidates to be Feature Columns, for the time being I am using common sense :)\n\n# I will set as feature column the Pclass, Sex and Age columns as I think those features where important in the time of the sinking.\n# Let's explore the data of these columns to look for nulls.\n\ntrain_x['Pclass'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be74318c570de268dc3883c45fcbf3343238b1bf"},"cell_type":"code","source":"train_x['Sex'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"93911fe4a66a6d52aa0e1bfa3f2fc190f1efe505"},"cell_type":"code","source":"train_x['Age'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7d24d3adc56cb72a1a2501436ab79afcce408736"},"cell_type":"code","source":"# We see there are 714 over 891 rows with that field available so we need to fill up those. \n# I am gonna use the method of replacing with Mean but I set another TODO here\n# TODO: Use an alternate method and check which one gives better predictions: https://www.analyticsindiamag.com/5-ways-handle-missing-values-machine-learning-datasets/\n\nage_mean = train_x['Age'].mean()\ntrain_x['Age'].fillna(age_mean, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"78c6d6f5f9ced517a4a857b615a60bfa9dc744be"},"cell_type":"code","source":"# Pclass is an ordinal?? column, so we need to do some work here.\n# Let's create three boolean columns\n\ndef create_dummies(df,column_name):\n    dummies = pd.get_dummies(df[column_name],prefix=column_name)\n    df = pd.concat([df,dummies],axis=1)\n    return df\n\ntrain_x = create_dummies(train_x,\"Pclass\")\ntrain_x.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2d5c23c5771eb37473fd2e1cd789ec2d515c760e"},"cell_type":"code","source":"# Same for Sex column\ntrain_x = create_dummies(train_x,\"Sex\")\ntrain_x.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c0e9ec1618b1ec9586c00598d960ce381a830da4"},"cell_type":"code","source":"# We need to do the same for the test data\ntest = create_dummies(test,\"Pclass\")\ntest = create_dummies(test,\"Sex\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"20018f73d167894137b230460c638e3184c88dbb"},"cell_type":"code","source":"# Now is time to remove all the columns we don't need, so we are keeping only the feature branches\ntrain_final = train_x[['Age','Pclass_1','Pclass_2','Pclass_3', 'Sex_female', 'Sex_male']]\ntrain_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"33dfa4dd908952f7024075e37244f7e70be9693e"},"cell_type":"code","source":"my_feature_columns = []\nfor key in train_final.keys():\n    my_feature_columns.append(tf.feature_column.numeric_column(key=key))\n\nprint(my_feature_columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b81db1ff002883c10b4ea747e48a4ddef193660d"},"cell_type":"code","source":"# Build 2 hidden layer DNN with 10, 10 units respectively.\nclassifier = tf.estimator.DNNClassifier(\n    feature_columns=my_feature_columns,\n    # Two hidden layers of 10 nodes each.\n    hidden_units=[10, 10],\n    # The model must choose between 3 classes.\n    n_classes=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6a55537d764a50918cecd3d34d78f7fd11d750cf"},"cell_type":"code","source":"def train_input_fn(features, labels, batch_size):\n    \"\"\"An input function for training\"\"\"\n    # Convert the inputs to a Dataset.\n    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n\n    # Shuffle, repeat, and batch the examples.\n    # shuffle the dataset is very important to avoid overfitting.\n    # one epoch = one forward pass and one backward pass of all the training examples\n    # batch size = the number of training examples in one forward/backward pass. The higher the batch size, the more memory space you'll need.\n    # number of iterations = number of passes, each pass using [batch size] number of examples. To be clear, one pass = one forward pass + one backward pass (we do not count the forward pass and backward pass as two different passes).\n    # Example: if you have 1000 training examples, and your batch size is 500, then it will take 2 iterations to complete 1 epoch.\n    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n\n    return dataset\n\n\n# Train the Model.\nbatch_size = 100\ntrain_steps = 1000\nclassifier.train(\n    input_fn=lambda:train_input_fn(train_final, train_y, batch_size),\n    steps=train_steps)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4bf1e3f7b71dbccff8f31c3ce1251c45b3eec174"},"cell_type":"code","source":"train_final.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0c59308e92b7b54311845fc38ef59da044260eb1"},"cell_type":"code","source":"predictions = test[['Age','Pclass_1','Pclass_2','Pclass_3','Sex_female','Sex_male']]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0c12eab25afe5530b38d3578f9bd8f459296031a"},"cell_type":"code","source":"def eval_input_fn(features, labels, batch_size):\n    \"\"\"An input function for evaluation or prediction\"\"\"\n    features=dict(features)\n    if labels is None:\n        # No labels, use only features.\n        inputs = features\n    else:\n        inputs = (features, labels)\n\n    # Convert the inputs to a Dataset.\n    dataset = tf.data.Dataset.from_tensor_slices(inputs)\n\n    # Batch the examples\n    assert batch_size is not None, \"batch_size must not be None\"\n    dataset = dataset.batch(batch_size)\n\n    # Return the dataset.\n    return dataset\n\n\nprediction_final = classifier.predict(\n        input_fn=lambda:eval_input_fn(predictions,\n                                      labels=None,\n                                      batch_size=batch_size))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1a1580579730342e7e120b2002941e7c1b60901b"},"cell_type":"code","source":"result = []\nfor pred_dict in prediction_final:\n    template = ('\\nPrediction is \"{}\" ({:.1f}%)')\n\n    class_id = pred_dict['class_ids'][0]\n    probability = pred_dict['probabilities'][class_id]\n\n    result.append(probability)\n    print(probability)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"08564f72c2f283d7d6f7278076fe3d1d77d5c22f"},"cell_type":"code","source":"holdout_ids = test[\"PassengerId\"]\nsubmission_df = {\"PassengerId\": holdout_ids,\n                 \"Survived\": result}\nsubmission = pd.DataFrame(submission_df)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e23d036c1e52e4d319bc270d890b15b89c7cadfa"},"cell_type":"code","source":"submission.to_csv('titanic_submission.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}