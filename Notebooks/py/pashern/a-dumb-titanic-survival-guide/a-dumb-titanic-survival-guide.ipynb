{"metadata": {"language_info": {"codemirror_mode": {"version": 3, "name": "ipython"}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "mimetype": "text/x-python", "version": "3.6.3", "file_extension": ".py", "name": "python"}, "kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}}, "nbformat_minor": 1, "nbformat": 4, "cells": [{"metadata": {"_uuid": "f34957005f8e952f29120fa382a6ccf411c6e892", "_cell_guid": "9e92e7fb-89ea-4e39-af84-f90e426e1f49"}, "cell_type": "markdown", "source": ["*Titanic Disaster: Train, Trick & Rewrite History*"]}, {"metadata": {"_uuid": "09144bcdb0e541dc39055380f30990170b07dc38", "_cell_guid": "cae93f7d-5913-4bca-801b-9308b593c4c3"}, "cell_type": "markdown", "source": ["# **Introduction:**\n", "\n", "This is my first kernel submission. This kernel is trained on the given dataset and uses Decision Trees to predict the Survival of any passenger.  And as the title suggests, if the classifier predicts a passenger did not survive, then I have used a plain brute force method to look through the combination of the features that the passenger would have had a possiblity of changing (Fare, Ticket Class & Port of Embarkation)  to find the first best chance of Survival. In short, I call it '*A Dumb Titanic Survival Guide*'. Please note that the data clean up and  correlation of features have been used from this [tutorial](http://https://www.kaggle.com/startupsci/titanic-data-science-solutions).\n", "\n", "To begin with I have split the code into two parts:\n", "\n", "**Part 1 **- This part contains three steps to the code to load the data, perform workflow goals as mentioned in the tutorial above and train a decision tree classifier and calculate it's accuracy.\n", "\n", "**Part 2** - In this part I pick a random test data and predict the Survival using the classifier trained in step 3.  If the passenger did not survive, then I try to find the first best combination of features (Fare, Ticket Class & Port of Embarkation) for that passenger that increases his/her chance of survivial. \n", "\n", "Please run this kernel and leave your comments/suggestions to improve. \n", "\n", "Thanks.\n", "\n", "Pranesh"]}, {"metadata": {"_uuid": "77fc048982a3ab8c1421a7d4a9cc718909d591d9", "_cell_guid": "3a9febf0-b9fd-474f-93c8-13e7e4f7b5c4"}, "cell_type": "markdown", "source": ["###  **Part 1: ** \n", "**Step 1: ** Load the training and test data for the given datasets."]}, {"metadata": {"_uuid": "b86f1ffd2223ca2a6919c1e0440ff0a0cbb4d798", "_cell_guid": "de4292d9-22e9-4ef4-9ec0-58ef8474e2fd", "scrolled": false}, "execution_count": null, "cell_type": "code", "outputs": [], "source": ["# data analysis and wrangling\n", "import pandas as pd\n", "import numpy as np\n", "import random as rnd\n", "\n", "# visualization\n", "import seaborn as sns\n", "import matplotlib.pyplot as plt\n", "import time as tm\n", "%matplotlib inline\n", "\n", "# machine learning\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.svm import SVC, LinearSVC\n", "from sklearn.ensemble import RandomForestClassifier\n", "from sklearn.neighbors import KNeighborsClassifier\n", "from sklearn.naive_bayes import GaussianNB\n", "from sklearn.linear_model import Perceptron\n", "from sklearn.linear_model import SGDClassifier\n", "from sklearn.tree import DecisionTreeClassifier\n", "\n", "train_df = pd.read_csv('../input/train.csv')\n", "test_df = pd.read_csv('../input/test.csv')\n", "combine = [train_df, test_df]\n", "\n", "#print(train_df.columns.values)\n", "print('Training & test data loaded.')"]}, {"metadata": {"_uuid": "b5eed5c5a0d5b09876f5395f4bddc8c236e99f4e", "_cell_guid": "6946f915-3df1-4c0b-b765-0c09084a9935"}, "cell_type": "markdown", "source": ["**Step 2:**  Perform workflow goals as described in this [tutorial](https://www.kaggle.com/pashern/titanic-data-science-solutions)."]}, {"metadata": {"_uuid": "568b60c3e70294bf5ef1022e666ba9c7c34300fa", "_cell_guid": "1de220c7-217c-451a-a6e3-ee4846dbec5c"}, "execution_count": null, "cell_type": "code", "outputs": [], "source": ["train_df[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n", "\n", "train_df[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n", "\n", "train_df[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n", "\n", "\n", "train_df = train_df.drop(['Ticket', 'Cabin'], axis=1)\n", "test_df = test_df.drop(['Ticket', 'Cabin'], axis=1)\n", "combine = [train_df, test_df]\n", "\n", "\"After\", train_df.shape, test_df.shape, combine[0].shape, combine[1].shape\n", "\n", "for dataset in combine:\n", "    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n", "\n", "pd.crosstab(train_df['Title'], train_df['Sex'])\n", "\n", "for dataset in combine:\n", "    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n", " \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n", "\n", "    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n", "    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n", "    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n", "    \n", "train_df[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()\n", "\n", "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n", "for dataset in combine:\n", "    dataset['Title'] = dataset['Title'].map(title_mapping)\n", "    dataset['Title'] = dataset['Title'].fillna(0)\n", "\n", "\n", "train_df = train_df.drop(['Name', 'PassengerId'], axis=1)\n", "test_df = test_df.drop(['Name'], axis=1)\n", "combine = [train_df, test_df]\n", "\n", "for dataset in combine:\n", "    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n", "\n", "guess_ages = np.zeros((2,3))\n", "\n", "for dataset in combine:\n", "    for i in range(0, 2):\n", "        for j in range(0, 3):\n", "            guess_df = dataset[(dataset['Sex'] == i) & \\\n", "                                  (dataset['Pclass'] == j+1)]['Age'].dropna()\n", "\n", "            # age_mean = guess_df.mean()\n", "            # age_std = guess_df.std()\n", "            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)\n", "\n", "            age_guess = guess_df.median()\n", "\n", "            # Convert random age float to nearest .5 age\n", "            guess_ages[i,j] = int( age_guess/0.5 + 0.5 ) * 0.5\n", "            \n", "    for i in range(0, 2):\n", "        for j in range(0, 3):\n", "            dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == i) & (dataset.Pclass == j+1),\\\n", "                    'Age'] = guess_ages[i,j]\n", "\n", "    dataset['Age'] = dataset['Age'].astype(int)\n", "\n", "train_df['AgeBand'] = pd.cut(train_df['Age'], 5)\n", "train_df[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)\n", "\n", "for dataset in combine:    \n", "    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n", "    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n", "    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n", "    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n", "    dataset.loc[ dataset['Age'] > 64, 'Age']\n", "\n", "    \n", "train_df = train_df.drop(['AgeBand'], axis=1)\n", "combine = [train_df, test_df]\n", "\n", "for dataset in combine:\n", "    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n", "\n", "train_df[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n", "\n", "for dataset in combine:\n", "    dataset['IsAlone'] = 0\n", "    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n", "\n", "train_df[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()\n", "\n", "train_df = train_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\n", "test_df = test_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\n", "combine = [train_df, test_df]\n", "\n", "for dataset in combine:\n", "    dataset['Age*Class'] = dataset.Age * dataset.Pclass\n", "\n", "train_df.loc[:, ['Age*Class', 'Age', 'Pclass']].head(10)\n", "\n", "freq_port = train_df.Embarked.dropna().mode()[0]\n", "\n", "for dataset in combine:\n", "    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)\n", "    \n", "train_df[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n", "\n", "\n", "for dataset in combine:\n", "    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n", "\n", "test_df['Fare'].fillna(test_df['Fare'].dropna().median(), inplace=True)\n", "\n", "train_df['FareBand'] = pd.qcut(train_df['Fare'], 4)\n", "train_df[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)\n", "\n", "for dataset in combine:\n", "    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n", "    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n", "    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n", "    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n", "    dataset['Fare'] = dataset['Fare'].astype(int)\n", "\n", "train_df = train_df.drop(['FareBand'], axis=1)\n", "combine = [train_df, test_df]\n", "\n", "X_train = train_df.drop(\"Survived\", axis=1)\n", "Y_train = train_df[\"Survived\"]\n", "X_test  = test_df.drop(\"PassengerId\", axis=1).copy()\n", "\n", "print('Workflow goals performed.')\n"]}, {"metadata": {"_uuid": "fcc0ba50bf1ce30d4f44686654983c3aa3caece8", "_cell_guid": "e3284b3a-7016-485c-8ec7-d6c701139e02"}, "cell_type": "markdown", "source": ["**Step 3:**  Use Decision Tree classifer  to train the data and test it on the test set.  The accuracy is about 86.76%"]}, {"metadata": {"_uuid": "280c8f79284d3e14e29b5405bce8cfbb738c63ee", "_cell_guid": "265ab73a-72ee-46d6-9412-92d8af04eb46"}, "execution_count": null, "cell_type": "code", "outputs": [], "source": ["#create a decision tree classifer\n", "decision_tree = DecisionTreeClassifier()\n", "\n", "#train the classifier using the training set\n", "decision_tree.fit(X_train, Y_train)\n", "\n", "#predict the test set using the classifier\n", "Y_pred = decision_tree.predict(X_test)\n", "\n", "#calclualte the accuracy on the classifier\n", "acc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\n", "print('The accuracy for the given training data using Decision Tree classifier is: {0:.2f} %' \n", "      .format(acc_decision_tree))"]}, {"metadata": {"collapsed": true}, "execution_count": null, "cell_type": "code", "outputs": [], "source": ["#submission steps\n", "submission = pd.DataFrame({\n", "        \"PassengerId\": test_df[\"PassengerId\"],\n", "        \"Survived\": Y_pred\n", "    })\n", "\n", "submission.to_csv('mysubmission.csv', index=False)"]}, {"metadata": {"_uuid": "b98a6089924b59b07185cee48a0b51aea988c34c", "_cell_guid": "486fbdc9-c09a-4bf0-b362-e5358c9ad1e1"}, "cell_type": "markdown", "source": ["###  **Part 2: ** \n", "\n", "Pick a random test data  and try to preict the survival for that passenger.  If the passenger did not survive, then loop through the three freatures Fare, Ticket Class & Port of Embarkation to find the first best combination for the classifier that would predict that the passenger survived.  The feature values (Pclass_list, Fare_list, Embarked_list) in the below code are ordered in the decreasing order of correlation with survival.   This was learnt from step 2 in Part 1.  I have used dictionaries to reverse the converting effect on the data performed again in step 2 in Part 1.  "]}, {"metadata": {"_uuid": "832ccd908dda648a9f4123b4c07d40f87a29a799", "_cell_guid": "09f58e7a-83b6-425f-ba98-a4978738c346"}, "execution_count": null, "cell_type": "code", "outputs": [], "source": ["#Feature reversal dictionaries\n", "Fare_dict = {0:\"around \u00a37.5\", 1:\"between \u00a37.5 & \u00a314.5\", 2:\"between \u00a314.5 & \u00a331\", 3:\"more than \u00a331\"}\n", "Embarked_dict = {0:\"Southampton\", 1:\"Cherbourg\", 2:\"Queenstown\"}\n", "Pclass_dict = {1:\"upper\", 2:\"middle\", 3:\"lower\"}\n", "Sex_dict = {1:'female', 0:'male'}\n", "Pronoun_dict = {1:\"She\", 0:\"He\"}\n", "Age_dict = {0:\"below 16\", 1:\"between 16 & 32\", 2:\"between 32 & 48\", 3:\"between 48 & 64\", 4:\"older than 64\"}\n", "Survive_dict = {0:\"unfortunately did not make it alive :(\",\n", "                1:\"miraculously survived and lived happily there after :)\"}\n", "\n", "#Function that tries to predict tbe best combination to help the passenger survive. \n", "def possibs(curr_row_in):\n", "    curr_row = curr_row_in\n", "    #print((curr_row))\n", "    pclass = curr_row[0][0]\n", "    sex = curr_row[0][1]\n", "    age = curr_row[0][2]\n", "    fare = curr_row[0][3]\n", "    embarked = curr_row[0][4]\n", "    title = curr_row[0][5]\n", "    isalone = curr_row[0][6]\n", "    ageclass = curr_row[0][7]\n", "    Pclass_list = [1,2,3]\n", "    Fare_list = [2,3,1,0]\n", "    Embarked_list = [1,2,0]\n", "    #count = 0\n", "    \n", "    for pc in Pclass_list:\n", "        for f in Fare_list:\n", "            for em in Embarked_list: \n", "                #count = count + 1\n", "                possibs_row = [pc, sex, age, f, em, title, isalone, ageclass]\n", "                df = pd.DataFrame([possibs_row])\n", "                #print('Count:', count)\n", "                survival = decision_tree.predict(df)\n", "                #print(pc,f,em,survival)\n", "                if survival == 1:\n", "                    return ((pc,f,em))\n", "\n", "\n", "# Pick a random row from the test set.\n", "row, col = X_test.shape\n", "dom =  np.random.randint(0, high=row, size=1)\n", "Y_pred = decision_tree.predict(X_test.loc[dom])\n", "\n", "#convert the random row to a list and assign \n", "curr_row = X_test.loc[dom].values.flatten().tolist()\n", "\n", "#unpack the list to variables\n", "pclass = curr_row[0]\n", "sex = curr_row[1]\n", "age = curr_row[2]\n", "fare = curr_row[3]\n", "embarked = curr_row[4]\n", "title = curr_row[5]\n", "isalone = curr_row[6]\n", "ageclass = curr_row[7]\n", "\n", "\n", "#Predict the survival of the random passenger\n", "print('\\nWe are going to predict the survival of a {0:s} passenger, who was aged {1:s}. {2:s} embarked at {3:s} and paid a fare {4:s} to get a {5:s} class ticket. The passenger {6:s}'\n", "      .format(Sex_dict[sex], Age_dict[age], Pronoun_dict[sex], Embarked_dict[embarked], Fare_dict[fare], Pclass_dict[pclass], Survive_dict[Y_pred[0]]))\n", "\n", "#If the passenger did not survive, then call the function to get the best feautre combinations for survvial.\n", "if(Y_pred[0]!=1):\n", "    #function call\n", "    (pc,f,em) = possibs([curr_row])\n", "    \n", "    #outputs\n", "    print('\\n\\nSince the passenger did not survive the tragedy, now let\\'s try to use \\'The Dumb Titanic Survival Guide\\' to predict the best options that would have helped the passenger survive.' )\n", "    print('\\n\\npredicting...\\n\\n')\n", "    tm.sleep(2)\n", "    print('The Dumb Titanic Survival Guide says...\\n')\n", "    tm.sleep(1)\n", "    print('The passenger would have had a better chance of survival if {0:s} had purchased {1:s} class ticket for a price {2:s} and embarked at {3:s}'\n", "         .format(Pronoun_dict[sex], Pclass_dict[pc], Fare_dict[f], Embarked_dict[em]))\n", "\n", "###THE END###"]}, {"metadata": {"_uuid": "2884abe90f3cfd4c7fec03c568fac65426b6dc0d", "_cell_guid": "7f0a585e-2cbf-4860-b19e-5fc216411390"}, "cell_type": "markdown", "source": ["*Thank you for trying this kernel.  Please leave your valuable feedback in the comments and upvote if you liked the appach. *"]}]}