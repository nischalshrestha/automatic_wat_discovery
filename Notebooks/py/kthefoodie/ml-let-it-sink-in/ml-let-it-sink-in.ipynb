{"cells": [{"metadata": {"_uuid": "d7b79ca07e489d4fc21d69f128b35e799a46423c", "_cell_guid": "4dbc5bf3-8f5e-4039-97a6-a967a39dbe19", "nbpresent": {"id": "4abc419e-7cb3-414e-9b13-af0f746b32e0"}}, "source": ["\n", "# Introduction\n", "\n", "Objective of this notebook is to implement many algorithms for the purpose of demonstration. Intention is to write instructive, clear code, rather than efficient one. Focus will be on having clear reasoning for all the steps, which will highlight a general startegy to tackle any problem. There will also be links directing to the relevnt material, if you don't know (or need to revisit) a particular topic.\n", "\n", "I hope this will be useful to you. If you have any doubts or suggestion, feel free to comment.\n", "\n", "Link to notebooks referred to:\n", "1. [Introduction to Ensembling/Stacking in Python](https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python/notebook)\n", "2. [Deep Visualisations - Simple Methods](https://www.kaggle.com/jkokatjuhha/deep-visualisations-simple-methods)\n", "3. [A Journey through Titanic](https://www.kaggle.com/omarelgabry/a-journey-through-titanic)\n", "4. [Titanic best working Classifier](https://www.kaggle.com/sinakhorami/titanic-best-working-classifier)\n", "----\n", "# Index\n", "\n", "1. [Getting our tools ready](#1)\n", "2. [Understanding data](#2)<br>\n", "&nbsp;&nbsp;2.1 [Filling up missing values](#2.1)<br>\n", "&nbsp;&nbsp;&nbsp;&nbsp;2.1.1 ['Fare'](#2.1.1)<br>\n", "&nbsp;&nbsp;&nbsp;&nbsp;2.1.2 ['Age'](#2.1.2)<br>\n", "&nbsp;&nbsp;&nbsp;&nbsp;2.1.3 ['Fare'](#2.1.3)<br>\n", "&nbsp;&nbsp;2.2 [Feature engineering and Data visualisation](#2.2)<br>\n", "&nbsp;&nbsp;&nbsp;&nbsp;2.2.1 ['Cabin'](#2.2.1)<br>\n", "&nbsp;&nbsp;&nbsp;&nbsp;2.2.2 ['family size](#2.2.2)<br>\n", "&nbsp;&nbsp;&nbsp;&nbsp;2.2.3 ['title'](#2.2.3)<br>\n", "&nbsp;&nbsp;&nbsp;&nbsp;2.2.4 ['Converting data to suitable numeric values'](#2.2.4)<br>\n", "&nbsp;&nbsp;&nbsp;&nbsp;2.2.5 ['Correlations'](#2.2.5)<br>\n", "3. [Models and predictions](#3)<br>\n", "&nbsp;&nbsp;3.1[First level model](#3.1)<br>\n", "&nbsp;&nbsp;3.2[Second level model](#3.2)<br>"], "cell_type": "markdown"}, {"metadata": {"_uuid": "8d813f7eac95b9b401cb6d0f74f9072c4a67b86a", "_cell_guid": "c1df3ffe-407f-4ef9-ba05-5f0ec5931774", "nbpresent": {"id": "3f989972-7b8b-4501-a399-2f4ce3168f60"}}, "source": ["<a id='1'></a>\n", "## 1. Getting our tools ready\n", "If anyone is wondering why python is one of the most preferred language by Data Scientists, simple answer is 'great libraries'. Python has fantastic tools to handle data systematically ([pandas](http://pandas.pydata.org/)), tools to do computations efficiently and easily ([numpy](http://www.numpy.org/)), tools to visualise data ([matplotlib](https://matplotlib.org/), [seaborn](https://seaborn.pydata.org/), [plotly](https://plot.ly/python/)) and last but not the least tools for machine learning ([scikit-learn](http://scikit-learn.org/)).\n", "\n", "Our first step is to import required libraries and set few options."], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "e752b740a9ea4a280a162da6c54ffebec2f410ea", "_cell_guid": "5b31002d-a237-4e5b-bbd4-7d6057fc0ac2", "nbpresent": {"id": "e581a703-d0f7-4363-9094-09308f365fae"}}, "outputs": [], "source": ["# Importing required libraries\n", "\n", "import pandas as pd\n", "import numpy as np\n", "import re\n", "pd.set_option('display.max_columns', 50)\n", "pd.set_option('display.max_rows', 20)\n", "\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "sns.set_style('whitegrid')\n", "%matplotlib inline\n", "\n", "import warnings\n", "warnings.filterwarnings('ignore')\n", "\n", "import sklearn\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.svm import SVC\n", "from sklearn.tree import DecisionTreeClassifier\n", "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n", "from sklearn.neighbors import KNeighborsClassifier\n", "from sklearn.model_selection import KFold\n", "from sklearn.metrics import roc_auc_score\n", "from sklearn.metrics import roc_curve"], "cell_type": "code"}, {"metadata": {"_uuid": "325f64480452efd2516a8a7cfb63a2720276ed44", "_cell_guid": "1ffec2ec-26c2-4c5d-9d82-c5fa26146575"}, "source": ["<a id='2'></a>\n", "## 2. Understanding data\n", "Before we start doing any kind of analysis, we need to know what kind of data is available to us. Here, we have data in form of 2 files, train.csv and test.csv. Let us first load the data and store it in a variable for easy access. We will use pandas.read_csv for this.\n", "Reason we are given train and test data seperately is that everyone makes the prediction on same test data. So it is easy to compare results amongest submissions."], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "c82488fd9ec10767265ac606caca781abef6e897", "_cell_guid": "58644d4e-a53b-4deb-ae2e-0fde8072d47d", "nbpresent": {"id": "f35771a8-51c5-4e1c-8a47-274e2a3fc6e9"}}, "outputs": [], "source": ["# Reading data into a dataframe\n", "train = pd.read_csv(\"../input/train.csv\")\n", "test = pd.read_csv(\"../input/test.csv\")"], "cell_type": "code"}, {"metadata": {"_uuid": "c6d3f95d6538595028d204a08641a31ee9485c2c", "_cell_guid": "f93e195a-f57b-4701-95cc-32a69177f59d"}, "source": ["Now let us look at first few rows of the data."], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "f1892e99fb331864ec087892c5c519678686b70f", "_cell_guid": "9a1f0074-04b0-43ff-967c-9cd432fb6b94", "nbpresent": {"id": "c40871bd-6b4f-4c37-8f14-4b80c17fb911"}}, "outputs": [], "source": ["train.head()"], "cell_type": "code"}, {"metadata": {"_uuid": "78b11839903096291d29dd1bb174b2b1848401a8", "_cell_guid": "854b1fc9-63e2-4f8a-81f1-8a99e9fafaf7"}, "source": ["Next step is to understand columns and datatypes in the dataframe. Typically, such description is available along with data. ([Titanic Data description](https://www.kaggle.com/c/titanic/data)). (If it isn't, we can simply use pandas methods, dataframe.dtypes and dataframe[column_name].value_counts().)\n", "\n"], "cell_type": "markdown"}, {"metadata": {"_uuid": "857fcb1036f76423447b7067d5afdeb04887e872", "_cell_guid": "1b085721-3026-4232-ab5e-f1adde4f4aa0"}, "source": ["### Titanic Data Dictionary\n", "\n", "Variable\tDefinition\tKey\n", "survival\tSurvival\t0 = No, 1 = Yes<br>\n", "pclass\tTicket class\t1 = 1st, 2 = 2nd, 3 = 3rd<br>\n", "sex\tSex\t<br>\n", "Age\tAge in years\t<br>\n", "sibsp\t# of siblings / spouses aboard the Titanic\t<br>\n", "parch\t# of parents / children aboard the Titanic\t<br>\n", "ticket\tTicket number\t<br>\n", "fare\tPassenger fare\t<br>\n", "cabin\tCabin number\t<br>\n", "embarked\tPort of Embarkation\tC = Cherbourg, Q = Queenstown, S = Southampton <br>\n", "<br>\n", "<br>\n", "Variable Notes<br>\n", "<br>\n", "pclass: A proxy for socio-economic status (SES)<br>\n", "1st = Upper<br>\n", "2nd = Middle<br>\n", "3rd = Lower<br>\n", "<br>\n", "age: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br>\n", "<br>\n", "sibsp: The dataset defines family relations in this way...<br>\n", "Sibling = brother, sister, stepbrother, stepsister<br>\n", "Spouse = husband, wife (mistresses and fianc\u00e9s were ignored)<br>\n", "<br>\n", "parch: The dataset defines family relations in this way...<br>\n", "Parent = mother, father<br>\n", "Child = daughter, son, stepdaughter, stepson<br>\n", "Some children travelled only with a nanny, therefore parch=0 for them."], "cell_type": "markdown"}, {"metadata": {"_uuid": "e4640d83f0f1f433911493866ad42899ca5cebd2", "_cell_guid": "8ef77574-61f6-410e-87a0-8f3fb38c94a4"}, "source": ["----\n", "Let's check for missing values in each column."], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "a0a952259d51aec9b8ef07491f1cd7ab0b9ee397", "_cell_guid": "f142506d-d7ed-4a0d-a972-0d729027038e"}, "outputs": [], "source": ["train.info()\n", "# can also use train.isnull().sum()"], "cell_type": "code"}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "86af8f8fa075b1ea8749dac1b957e3a723c0de98", "_cell_guid": "ba2395c0-72e7-49a5-8fdc-4127b0c4e47a"}, "outputs": [], "source": ["test.info()"], "cell_type": "code"}, {"metadata": {"_uuid": "a4c476071ba86c5757d8c168a939606acd3180e5", "_cell_guid": "2da264eb-fa65-4962-b2c7-80d18babe0b7"}, "source": ["Both datasets have a lot of missing values for 'Cabin' column. Few walues for 'Age' column are also missing. In train dataset, 'Embarked' column has 2 missing values. In test dataset, one value for 'fare' is missing.\n", "\n", "Missing values in 'Cabin' can be interpreted as not having a cabin.\n", "For remaining columns, we will use imputation techniques to fill up the values."], "cell_type": "markdown"}, {"metadata": {"_uuid": "9ef932974c1a5e8e994e41e489f135a9f6f210a3", "_cell_guid": "12e3a201-f73f-4c2f-bdba-296f2b9d93c5"}, "source": ["<a id='2.1'></a>\n", "## 2.1 Filling up missing values\n", "There are 2 ways main approaches to deal with missing data. One is to ignore rows with missing data. Other is to try to fill up these values. This is called 'Data Imputation' ([Imputation](https://en.wikipedia.org/wiki/Imputation_(statistics)). There are several data imputation techniques."], "cell_type": "markdown"}, {"metadata": {"_uuid": "382a209603f2370171a2786cdfbe493d49c3c3ed", "_cell_guid": "dafb41f4-5e6e-4194-a06f-6cd750a81816"}, "source": ["<a id='2.1.1'></a>\n", "### 2.1.1 'Embarked'\n", "This column is categorical type. As data description suggests, there are only 3 values viz., 'S', 'C' and 'Q'. We will take the most common value and use that to fill up missing values."], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "6a8a3714a9777d63f8269df11a72c3172708cbf5", "_cell_guid": "e5d888d4-709a-4869-957f-e3a614bbfc31"}, "outputs": [], "source": ["train['Embarked'] = train['Embarked'].fillna(np.argmax(train['Embarked'].value_counts()))"], "cell_type": "code"}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "bd127017551ef38816aaab009d2adaf51f522903", "_cell_guid": "3906b183-9355-49ad-b837-2ec771641f57"}, "outputs": [], "source": ["train['Embarked'].unique()"], "cell_type": "code"}, {"metadata": {"_uuid": "5e322c6ecd29e3da687ea740b42fcd4d8bddeec4", "_cell_guid": "6cbd1882-b5a3-493b-8416-12343850d894"}, "source": ["<a id='2.1.2'></a>\n", "### 2.1.2 'Age'\n", "\n", "Using same value for every missing point will introduce bias. We'll use random integers, but in a such a way that it doesn't affect the distribution very much. One way to achieve this is to draw raandom integers from the range mean +- standard deviation."], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "fe03b361d6082330c231dac4bc790263a62d96c7", "_cell_guid": "5fd1bc13-5ff3-4da3-ae8a-4474b59e6edb"}, "outputs": [], "source": ["age_combined = np.concatenate((test['Age'].dropna(), train['Age'].dropna()), axis=0)\n", "mean = age_combined.mean()\n", "std_dev = age_combined.std()\n", "train_na = np.isnan(train['Age'])\n", "test_na = np.isnan(test['Age'])\n", "impute_age_train = np.random.randint(mean - std_dev, mean + std_dev, size = train_na.sum())\n", "impute_age_test = np.random.randint(mean - std_dev, mean + std_dev, size = test_na.sum())\n", "train[\"Age\"][train_na] = impute_age_train\n", "test[\"Age\"][test_na] = impute_age_test\n", "new_age_combined = np.concatenate((test[\"Age\"],train[\"Age\"]), axis = 0)"], "cell_type": "code"}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "3395c5fe5b6613768508758a61dd61ed1d667fa0", "_cell_guid": "713bc611-d459-4e8c-972f-0406e14d252d"}, "outputs": [], "source": ["# Check the effect of imputation on the distribution\n", "_ = sns.kdeplot(age_combined)\n", "_ = sns.kdeplot(new_age_combined)"], "cell_type": "code"}, {"metadata": {"_uuid": "e878e19fd49bd5fc7761d7cf80cd121d62bb6374", "_cell_guid": "63e3d095-3631-4ee8-8983-8735896e596b"}, "source": ["<a id='2.1.3'></a>\n", "### 2.1.3 'Fare' \n", "\n", "There is only one missing value for 'Fare' column in test dataset. So we will use median value."], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "5c0793c198e70a2ef71bb0a6237c5d39e0a00778", "_cell_guid": "88a7af1e-a1fa-48df-a71e-84abd8bcb463"}, "outputs": [], "source": ["print(test['Fare'].isnull().sum())\n", "test['Fare'] = test['Fare'].fillna(np.median(train['Fare']))\n", "print(test['Fare'].isnull().sum())"], "cell_type": "code"}, {"metadata": {"_uuid": "bf8ab55eb3cc7534e019cf731315f1769266d017", "_cell_guid": "90744a4f-f9fb-407b-a48d-2a760d72beb6"}, "source": ["<a id='2.2'></a>\n", "## 2.2 Feature engineering and Data visualisation\n", "\n", "#### Feature engineering\n", "\n", "The features in your data will directly influence the predictive models you use and the results you can achieve. Feature engineering is the process of transforming raw data into features that better represent the underlying problem to the predictive models. ([Feature engineering](https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/))\n", "\n", "#### Data visualisation\n", "\n", "Plots and information graphics are powerful way to understand data. They make complex data more accessible, understandable and usable. Rather than a list of numbers, a simple distribution plot conveys a lot more information lot more quickly. That is why data visualisations are extremely important in any ML project."], "cell_type": "markdown"}, {"metadata": {"_uuid": "18f8d01770b9e69463ba717bbf31502d796fa1da", "_cell_guid": "4a796999-a883-4249-9e49-8c3fbbdf0d32"}, "source": ["<a id='2.2.1'></a>\n", "### 2.2.1 'Cabin'\n", "\n", "As mentioned earlier, missing cabin values will be interpreted as not having a cabin.\n", "1 will represent having a cabin, 0 wil represent otherwise."], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "344852d32edd4064d1b03d07c0ab4756dc539b60", "_cell_guid": "abcb4835-4828-4fda-8332-291f669fb8c6"}, "outputs": [], "source": ["train[\"Cabin\"] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n", "test[\"Cabin\"] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)"], "cell_type": "code"}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "33ab9f05bdd493881d75c1cc8acad1bd42aa3538", "_cell_guid": "0aef7f4f-239e-4586-b838-c6a029e8c480"}, "outputs": [], "source": ["mean_survival_cabin = train[[\"Cabin\", \"Survived\",'Sex']].groupby(['Cabin','Sex'],as_index=False).mean()\n", "sns.set(font_scale=1.7)\n", "ax = sns.barplot(x='Cabin', y='Survived', hue = 'Sex', data=mean_survival_cabin)\n", "ax.legend(loc = 'upper left')"], "cell_type": "code"}, {"metadata": {"_uuid": "cc0918c3aa34ed232dc589f718972c1d151b62ba", "_cell_guid": "d0f4154e-bc83-4fc6-b260-7c1595e37839"}, "source": ["Women have much higher rate of survival. Also, those who had cabin, have much higher rate of survival."], "cell_type": "markdown"}, {"metadata": {"_uuid": "26914615bbe26fae6b6726fd2aec2f956be878ae", "_cell_guid": "9e213b26-688a-4ec3-832a-f93cac5b7ba6"}, "source": ["<a id='2.2.2'></a>\n", "### 2.2.2 'family size'\n", "\n", "From the description,<br>\n", "sibsp - # of siblings / spouses aboard the Titanic<br>\n", "parch - # of parents / children aboard the Titanic\n", "\n", "This can be combined into one variable, viz., 'family size'."], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "59884f23a73411a4177f7e2d0734c48c1bf6f673", "_cell_guid": "bc3c65a3-bce3-4776-b5db-9eff0207d74b"}, "outputs": [], "source": ["train['family_size'] = train['SibSp'] + train['Parch'] + 1\n", "test['family_size'] = test['SibSp'] + test['Parch'] + 1\n", "cols_to_drop = ['SibSp','Parch']\n", "train.drop(cols_to_drop, inplace = True, axis = 1)\n", "test.drop(cols_to_drop, inplace = True, axis = 1)"], "cell_type": "code"}, {"execution_count": null, "metadata": {"collapsed": true, "scrolled": true, "_uuid": "2c774a1fc877862ddd7cb4334eec3cf8d55409d9", "_cell_guid": "ff44fb7d-243c-4ad1-a308-8515a923c09b"}, "outputs": [], "source": ["sns.factorplot('family_size','Survived', hue = 'Sex', data=train, size = 5, aspect = 3)"], "cell_type": "code"}, {"metadata": {"_uuid": "de8c0c70d7e2dcea9de965ed97d8577551188c22", "_cell_guid": "d5222a7e-d94a-469a-a7ed-8d4ea4baef87"}, "source": ["The females traveling alone or with up to 3 more family members had a higher chance to survive. The chances for survival decrease once family size exceeds 4.<br>\n", "For men, survival rate is low but increases as family size increases till upto 3 other members. After that it drops again.<br>\n", "Overall, one can say that, after family size exceeds 4, survival rate is pretty low for both men and women, presumably because one would spend time searching for family members."], "cell_type": "markdown"}, {"metadata": {"_uuid": "db3315418744c340eec4900801ef5ecf58262d62", "_cell_guid": "4d9e74fd-aad2-434c-bbca-bb1a4f752027"}, "source": ["<a id='2.2.3'></a>\n", "### 2.2.3 'title'\n", "\n", "Title of a person contains a lot of useful information. It is a combination of gender, martial status (for women) and age."], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "d1109b21f6b5e502a46fae2d62a461022e7a737d", "_cell_guid": "1211c128-84d7-4fd6-a807-87036f8d1540"}, "outputs": [], "source": ["# Lets try to find a pattern in 'Name' column\n", "train[['Name']].head(20)"], "cell_type": "code"}, {"metadata": {"_uuid": "8416d510e12b078f132555358094660b1d1afa25", "_cell_guid": "7fe31330-7cfd-41e9-9945-7728f26ba8ef"}, "source": ["Name seems to have format - last name, title. name<br>\n", "So our pattern is ', (title). '. Note that spaces are important."], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "ec592829deeccd0a77a98bf9bdd69e442b053548", "_cell_guid": "76b4fbe9-2f19-4855-8c50-139234011064"}, "outputs": [], "source": ["titles_train = train['Name'].str.extract(' ([A-Za-z]+)\\.')"], "cell_type": "code"}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "6465923d6f2500d0823495e35fcec7413be0e4fe", "_cell_guid": "d7b9cf00-fb7c-4aea-9a98-6c53cf0b5b1b"}, "outputs": [], "source": ["print(titles_train.value_counts(),'\\n')\n", "print('Null value count is ', titles_train.isnull().sum())"], "cell_type": "code"}, {"metadata": {"_uuid": "075a9b06d1ec307f77abc3b10b72053054a2c5c5", "_cell_guid": "4b8f5a7a-7ce3-494f-956e-27c5ab9b6c67"}, "source": ["Luckily, there are no missing values. Otherwise, we might have had to predict  it using gender and age.<br>\n", "Note that 'Countess','Dona','Lady' and 'Mme' all are used for married women, so those will be replaced with 'Mrs'.<br>\n", "Similarly, 'Mlle' and 'Ms' will both be mapped to 'Miss'.<br>\n", "All titles with lesss than 10 entries will be mapped to 'rare'. Reason is to avoid creating too many categories."], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "aa57e08dfccd843b0ee45c5e6623507a91b3e22b", "_cell_guid": "e9ac7846-91a8-4fa1-9bf8-260bf6d2ffdb"}, "outputs": [], "source": ["titles_train.replace(['Countess', 'Dona', 'Lady', 'Mme'], 'Mrs', inplace = True)\n", "titles_train.replace(['Mlle', 'Ms'], 'Miss', inplace = True)\n", "rare_titles = []\n", "temp = titles_train.value_counts()\n", "for title in temp.index:\n", "    if temp[title] < 10:\n", "        rare_titles.append(title)\n", "        \n", "titles_train.replace(rare_titles, 'rare', inplace = True)"], "cell_type": "code"}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "50d480436c49402a896b91d87322711417bef449", "_cell_guid": "8ab08172-3abf-4b2f-9361-f46bb0186158"}, "outputs": [], "source": ["train['title'] = titles_train\n", "del titles_train"], "cell_type": "code"}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "631979c224f0e616b1a29b7033ad9f141f20b3ea", "_cell_guid": "7885c5d7-68de-46ae-b096-a75d96a4c856"}, "outputs": [], "source": ["# Repeat the procedure with test dataset.\n", "titles_test = test['Name'].str.extract(' ([A-Za-z]+)\\.')\n", "print(titles_test.value_counts(),'\\n')\n", "print('Null value count is ', titles_test.isnull().sum())"], "cell_type": "code"}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "885783102200d4f98ca48ac02be2e6a6aa6adc7b", "_cell_guid": "f9bd9567-b9fa-42c7-910a-3d3a59bfacab"}, "outputs": [], "source": ["titles_test.replace(['Countess', 'Dona', 'Lady', 'Mme'], 'Mrs', inplace = True)\n", "titles_test.replace(['Mlle', 'Ms'], 'Miss', inplace = True)\n", "rare_titles = []\n", "temp = titles_test.value_counts()\n", "for title in temp.index:\n", "    if temp[title] < 10:\n", "        rare_titles.append(title)\n", "        \n", "titles_test.replace(rare_titles, 'rare', inplace = True)\n", "test['title'] = titles_test\n", "del titles_test"], "cell_type": "code"}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "b4fa5116e7ac4d2c4c22b656a8a7db6e4972dc6a", "_cell_guid": "d84a5f13-b631-40d9-bb4b-06aa270dd419"}, "outputs": [], "source": ["test['title'].value_counts()"], "cell_type": "code"}, {"metadata": {"_uuid": "d35db22b8c6ce32dfd6e9c206a5ef2e6f83d71c0", "_cell_guid": "9b81d207-6257-4c0b-9030-dbb185b328b9"}, "source": ["There are some more possible features that we can create. But these three should be good enough. You can refer to noebooks linked at the top, if you are curious about other possible feature."], "cell_type": "markdown"}, {"metadata": {"_uuid": "61fb1421e7301a0d773912c4f99b42a81f66a345", "_cell_guid": "bfd14764-f183-44ba-8ec5-a7bfa2743ae4"}, "source": ["<a id='2.2.4'></a>\n", "### 2.2.4 Converting data to suitable numerical values\n", "\n", "We need to convert all the data to numerical values, so that we will be able to use ML algorithms. Also, some continuous data will be converted to discrete data. Reason for doing so is that, exact value of the variable isn't relevant, but in what range or group that value belongs to is relevant.<br>\n", "We will also [dummy code](https://en.wikiversity.org/wiki/Dummy_variable_(statistics) categorical variables. This will be done for variables which doesn't have numeric relationship and/or meaning amongest it's possible values. ([why do we need to dummy code categorical variables](https://stats.stackexchange.com/questions/115049/why-do-we-need-to-dummy-code-categorical-variables))"], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "c6841cc5c741362ae5a80d516547c0ca6cd969b0", "_cell_guid": "a8a4afd1-d635-4732-bccf-ebf2f72c9e22"}, "outputs": [], "source": ["# put train and test datasets in one list for the ease of doing operations.\n", "data = [train, test]\n", "\n", "# delete 'Ticket' and 'Name' columns\n", "for df in data:\n", "    df.drop(['Ticket','Name'], inplace = True, axis = 1)"], "cell_type": "code"}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "4329f6d61ffca3dace3e120147feb304a4883e4c", "_cell_guid": "00361afd-544f-40c3-8683-cca6681b0cf6"}, "outputs": [], "source": ["# 'Sex' column - straightforward 0 and 1 mapping\n", "train['Sex'] = train['Sex'].apply(lambda x:1 if x == 'female' else 0)\n", "test['Sex'] = test['Sex'].apply(lambda x:1 if x == 'female' else 0)"], "cell_type": "code"}, {"metadata": {"_uuid": "9d973467134abccc741c66fdd0c42db28a989ede", "_cell_guid": "2395cb97-5d8c-4198-9c81-7b70514afcb5"}, "source": ["#### 'Age' column\n", "\n", "Let us first see the distribution."], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "94fdaebe51d022c6450ce9a3cb70d3b87d368e78", "_cell_guid": "f9493622-8a72-4a5a-881c-91e5223243a4"}, "outputs": [], "source": ["f, [ax1,ax2] = plt.subplots(1,2, figsize = (20,5))\n", "sns.distplot(train['Age'][train['Survived'] == 1][train['Sex'] == 0], hist = False, ax = ax1, norm_hist = True, \n", "             label = 'Survived')\n", "sns.distplot(train['Age'][train['Sex'] == 0], hist = False, ax = ax1, norm_hist = True, label = 'Male age distribution')\n", "sns.distplot(train['Age'][train['Survived'] == 0][train['Sex'] == 0], hist = False, ax = ax2, norm_hist = True, \n", "             label = 'Didn\\'t Survive')\n", "sns.distplot(train['Age'][train['Sex'] == 0], hist = False, ax = ax2, norm_hist = True, label = 'Male age distribution')"], "cell_type": "code"}, {"metadata": {"_uuid": "5d9693e1ec80107f478dd319111c8b9f904611ef", "_cell_guid": "9d98138b-6c48-4941-bf13-8c3d44078cf9"}, "source": ["In a given plot, if both distributions are identical, it means that survival isn't dependant on variable under consideration. Here, both distributions differ considerably only for age < 15 and some difference for age group 15 to 30."], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "4f78c02b5e0542bbe116a39605c202481bac2325", "_cell_guid": "740bca5a-c411-401b-9626-a5f22fa012a1"}, "outputs": [], "source": ["f, [ax1,ax2] = plt.subplots(1,2, figsize = (20,5))\n", "sns.distplot(train['Age'][train['Survived'] == 1][train['Sex'] == 1], hist = False, ax = ax1, norm_hist = True, \n", "             label = 'Survived')\n", "sns.distplot(train['Age'][train['Sex'] == 1], hist = False, ax = ax1, norm_hist = True, label = 'Female age distribution')\n", "sns.distplot(train['Age'][train['Survived'] == 0][train['Sex'] == 1], hist = False, ax = ax2, norm_hist = True, \n", "             label = 'Didn\\'t Survive')\n", "sns.distplot(train['Age'][train['Sex'] == 1], hist = False, ax = ax2, norm_hist = True, label = 'Female age distribution')\n", "ax1.legend(loc = 'upper right')\n", "ax2.legend(loc = 'upper right')"], "cell_type": "code"}, {"metadata": {"_uuid": "74be72a4269bbbbb32ac074fd0e6df116d7442d0", "_cell_guid": "2d128531-1da7-49af-8fd0-990037ea7582"}, "source": ["There is very little diffrence between actual age distribution and distribution of survived females. Again, we see slightly higher survival rate for small values of age. This is suggests that children were given priority on the life boats."], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "abe8399c28daabb5b5e98558741ab309df119947", "_cell_guid": "f0549c2f-9ad3-4224-ab15-dd90e275e205"}, "outputs": [], "source": ["# We will now create age groups and check survival rate.\n", "cut_offs = [0,15,30,80]\n", "temp = pd.DataFrame(columns = ['Sex','Survived','age_group'])\n", "for i in range(1,len(cut_offs)):\n", "    df = train[[\"Survived\",'Sex']][train['Age']>cut_offs[i-1]][train['Age']<=cut_offs[i]].groupby(['Sex'],as_index=False).mean()\n", "    df['age_group'] = 'less than ' + str(cut_offs[i])\n", "    temp = temp.append(df, ignore_index = True)"], "cell_type": "code"}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "20f29324bd3a86f088ae5b4eb910a20cbbda8baf", "_cell_guid": "560fe6de-8b68-4b0c-8c48-e4fa8e8d9076"}, "outputs": [], "source": ["ax = sns.barplot(x = 'age_group', y = 'Survived', hue = 'Sex', data = temp)\n", "ax.legend(bbox_to_anchor=(1.25, 1))"], "cell_type": "code"}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "d6e563c9a039aa171b3966e90e82920c21afac7e", "_cell_guid": "906668e1-f5c7-46f5-b254-5534c188e402"}, "outputs": [], "source": ["# Let us map values in age column to appropriate age groups.\n", "train['Age'] = train['Age'].apply(lambda x: 1 if x <= 15 else 2 if x <= 30 else 3)\n", "test['Age'] = test['Age'].apply(lambda x: 1 if x <= 15 else 2 if x <= 30 else 3)\n", "\n", "train = pd.get_dummies(data = train, columns = ['Age'])\n", "test = pd.get_dummies(data = test, columns = ['Age'])\n", "\n", "# 2nd age group has lowest survival rate overall, so we will treat that as a base case and delete that column.\n", "train.drop(['Age_2'], axis = 1, inplace = True)\n", "test.drop(['Age_2'], axis = 1, inplace = True)"], "cell_type": "code"}, {"metadata": {"_uuid": "1e8c466e3300e56e19bb313c5e0f2490c4ea9ac5", "_cell_guid": "d0aaaa2d-aff7-4d34-94c8-c25754154afa"}, "source": ["#### 'Fare' column\n", "\n", "Fare is a combination of Passenger class, where the passenger embarked and also, whether or not they had a cabin. Let us try to see "], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "52f864c30ccf8f1a16c635d51e0f20dae857453f", "_cell_guid": "2f9fe1c5-021c-4b50-bd3f-61bc03bbe13b"}, "outputs": [], "source": ["# 'Fare' is expected to correlate with 'Pclass'\n", "f, [ax1, ax2] = plt.subplots(1,2, figsize = (20,5))\n", "sns.barplot(hue = 'Embarked', y = 'Fare', x = 'Pclass', data = train[train['Cabin'] == 0], ax = ax1, hue_order = ['S','C','Q'])\n", "sns.barplot(hue = 'Embarked', y = 'Fare', x = 'Pclass', data = train[train['Cabin'] == 1], ax = ax2, hue_order = ['S','C','Q'])\n", "ax1.set_title('Doesn\\'t have Cabin')\n", "ax2.set_title('Has Cabin')\n", "ax2.set_ylim([0,180])\n", "plt.show()"], "cell_type": "code"}, {"metadata": {"_uuid": "a7a40e344f02c654af4f511696ba9884c195db41", "_cell_guid": "9fe4fca6-6f95-41f3-96bc-0c74673af850"}, "source": ["As expected, there is correlation between Pclass and Fare. Additionally, 'Embarked' and 'Cabin' seems to have some impact as well, but there is no obvious trend."], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "01cb2140281c030a53c0fed054b26f2c139f8139", "_cell_guid": "454839cb-45be-4a4e-b372-09bd3c3688d6"}, "outputs": [], "source": ["def map_fare(x, cut_offs = None):\n", "    if cut_offs == None:\n", "        cut_offs = train['Fare'].describe()[['min','25%','50%','75%','max']]\n", "    cut_offs = np.sort(cut_offs)\n", "    for i in range(1,len(cut_offs)):\n", "        if x <= cut_offs[i]:\n", "            return i"], "cell_type": "code"}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "cb3a55c86b2d910fcbabef0b3f8ed8a8fcf1142c", "_cell_guid": "71c9b265-be27-41b3-a2c9-848f59a9d032"}, "outputs": [], "source": ["# Let us find Pearson correlation coefficient between Pclass and mapped values of 'Fare'\n", "mapped_fares = train['Fare'].apply(map_fare)\n", "mapped_fares.corr(train['Pclass'])"], "cell_type": "code"}, {"metadata": {"_uuid": "4bc5b246873363b0f537de8bce2984a02030829c", "_cell_guid": "0a22fff9-c7c9-4c12-8fb5-ff8691a5ad88"}, "source": ["Correlation coefficient value is negative because numerically lower value of class means higher fare.<br>\n", "Note that having correlated variables doesn't affect the model, rather it affects the interpretation of the results."], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "3fdb88d3df19f003ac7ca1e08ce606a15bb7d687", "_cell_guid": "4e33be53-85f7-41a4-bced-b2f141916689"}, "outputs": [], "source": ["test['Fare'] = test['Fare'].apply(map_fare)\n", "train['Fare'] = mapped_fares"], "cell_type": "code"}, {"metadata": {"_uuid": "607fb6f991495554e31761aeb51fca8c44b999a5", "_cell_guid": "037345dd-31dc-4320-8efd-b3ac10b64336"}, "source": ["#### 'Embarked' and 'title' columns\n", "These are still categorical type. We will use dummy variables to encode them."], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "6a70689535affcdac2448092789531c5789a19fb", "_cell_guid": "a5757054-af78-4941-a596-6f41d2852e29"}, "outputs": [], "source": ["train = pd.get_dummies(train, columns = ['Embarked', 'title'])\n", "test = pd.get_dummies(test, columns = ['Embarked', 'title'])\n", "\n", "train.drop(['Embarked_S','title_rare'], inplace = True, axis = 1)\n", "test.drop(['Embarked_S','title_rare'], inplace = True, axis = 1)"], "cell_type": "code"}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "b8e2590b2ea7316ff40dd0911804e5d2942817a2", "_cell_guid": "e726ffcb-6907-437b-868d-1b1e0dce25ef"}, "outputs": [], "source": ["train.drop('PassengerId', axis = 1, inplace = True)\n", "test_passenger_id = test['PassengerId']\n", "test.drop('PassengerId', axis = 1, inplace = True)"], "cell_type": "code"}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "17d6240476c72aa913fcdc0a049dd7366a8b3283", "_cell_guid": "e1bc2ef0-0342-4b21-a98d-4792f6a5f19b"}, "outputs": [], "source": ["# Let us check dtypes to ensure every variable is numeric.\n", "train.dtypes"], "cell_type": "code"}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "340793c5b81d1a8400f10f527be992351ae348b7", "_cell_guid": "29baf976-9f2c-4788-81ca-4ed4db0cdc1c"}, "outputs": [], "source": ["test.dtypes"], "cell_type": "code"}, {"metadata": {"_uuid": "de172579834693ae43bf46ee3a2cec9c71da2cc6", "_cell_guid": "8af1212a-5f1b-46dc-a53c-a90ee7c54c4d"}, "source": ["#### 2.2.5 Correlations\n", "Let us explore through correlation values.\n", "<a id='2.2.5'></a>"], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "c45a6e8899529cb4c0c80373881d7346a6c6ab19", "_cell_guid": "675cc46b-aebb-47c3-92b4-de5e255fc4f1"}, "outputs": [], "source": ["correlations = train.corr()"], "cell_type": "code"}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "6974b9e7a15624d0b36efd213d14941cfb5825c9", "_cell_guid": "7c2c2b50-2f7c-4b54-8cf3-46d925719074"}, "outputs": [], "source": ["d = {}\n", "for col in correlations:\n", "    temp = correlations[col].drop(col)\n", "    for row in temp.index:\n", "        if abs(temp[row]) > 0.5:\n", "            if row + ' - ' + col not in d:\n", "                d[col + ' - ' + row] = float('{:.4f}'.format(temp[row]))\n", "d"], "cell_type": "code"}, {"metadata": {"_uuid": "a907574966845335987b81e8e796815c0ed53915", "_cell_guid": "5397e939-069a-4154-8334-9aa2188ed388"}, "source": ["Possible explanations for these observations :\n", "\n", "1. Title 'Master' is used for young age boys.\n", "2. 'Fare', 'Cabin', 'Pclass' are expected to be correlated to each other.\n", "3. Title does depend on Gender of the person.\n", "4. Correlation between Survival and gender indicates that females had higher chances of survival."], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "4f5608d8eb3301b4e3060172f09402aa1fbc0c8a", "_cell_guid": "09cbe26b-60e4-4e48-9c0a-c2b4e0cc1fbe"}, "outputs": [], "source": ["# If you wish to see diagramatic representation of correlations, you can 'uncomment' code in this cell.\n", "\n", "# f, ax = plt.subplots(1,1,figsize = (12,12))\n", "# sns.set(font_scale = 1)\n", "# sns.heatmap(correlations,square=True, annot=True, ax = ax, cmap = 'PuBu', cbar=True,\n", "#             cbar_kws={\"shrink\": 0.75}, fmt = '.2f')\n", "# plt.setp(ax.get_xticklabels(), fontsize=14)\n", "# plt.setp(ax.get_yticklabels(), fontsize=14)\n", "# plt.show()"], "cell_type": "code"}, {"metadata": {"_uuid": "8e26cf6db339d923129ecfefbf2309e8010f5702", "_cell_guid": "0a97184c-624c-4f29-93c5-9b43cc481781"}, "source": ["<a id='3'></a>\n", "## 3. Models and predictions\n", "\n", "Now that we have created and modified our features, it is time to train various models and obtain predictions on test dataset. We will first train few models and then use model ensembling ([kaggle-ensembling-guide](https://mlwave.com/kaggle-ensembling-guide/)). Ensembled models typically have a lower generalisation error. It is a strategy used by many Kaggle compitition winners. There is a simple reasoning why ensembling reduces error. Say you have 2 fairly uncorrelated models. Then probability of same example being misclassified is low.<br>\n", "[This notebook](https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python/notebook) is excellent example of ensembling."], "cell_type": "markdown"}, {"metadata": {"collapsed": true, "_uuid": "c89c478eed83bc043729de890efe89c2032b67f9", "_cell_guid": "c7bdd01f-df89-439e-b9ad-a421438de40c"}, "source": ["<a id='3.1'></a>\n", "### 3.1 First level models\n", "There are many classifier algorithms available in sklearn library. We will train 8 first level models. Note that, typically you will not train so many models. It is done here for the purpose of demonstartion. You are advised to go through all of them and play around with various parameters to see what effect they have on the output, accuracy, time taken from training etc.<br>"], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "a28b6c38975bf5a1e3abd9270ba2aa59752d68b4", "_cell_guid": "71e1d54e-bc90-4cd9-b8c4-f11f1289ab80"}, "outputs": [], "source": ["seed = 0  # Seed to use when calling functions involving random selection. Important for reproducibility\n", "kf = KFold(n_splits = 4, random_state = seed)\n", "survived = train['Survived']\n", "train.drop('Survived', axis = 1, inplace = True)"], "cell_type": "code"}, {"metadata": {"_uuid": "a89f016079a6619add0518a1eb456741a66f74a3", "_cell_guid": "476ac732-d0e6-4778-ac8a-8a2b5c698f7f"}, "source": ["KFold ([sklearn.model_selection.KFold](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html)):<br>\n", "Provides train/test indices to split data in train/test sets. Split dataset into k consecutive folds (without shuffling by default). <br>\n", "Each fold is then used once as a validation while the k - 1 remaining folds form the training set.<br>"], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "6b79a05df3cd5fb7cb4c6ffb8dc20959c2694294", "_cell_guid": "a99e6e38-dde3-4d0f-a70a-9334b66cc63d"}, "outputs": [], "source": ["list_of_indices = []\n", "for (_, temp) in kf.split(train.index):\n", "    for index in temp:\n", "        list_of_indices.append(index)\n", "train_predictions = pd.DataFrame(index = list_of_indices)\n", "test_predictions = pd.DataFrame()"], "cell_type": "code"}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "6900099698c88ada273b47ee604938c34529acbf", "_cell_guid": "91bc750d-e153-44e4-a774-0009101d98f5"}, "outputs": [], "source": ["def train_model(clf_name, clf, prediction_df, train_df, test_df):\n", "    prediction_df[clf_name] = [-1]*prediction_df.shape[0]\n", "    temp = pd.DataFrame()\n", "    \n", "    for i, (train_index, test_index) in enumerate(kf.split(train_df.index)):\n", "        x = train_df.loc[train_index]\n", "        y = survived.loc[train_index]\n", "        test_values = train_df.loc[test_index]\n", "        \n", "        clf.fit(x,y)\n", "        \n", "        prediction_df[clf_name].loc[test_index] = list(clf.predict(test_values))\n", "        temp[i] = list(clf.predict(test_df))\n", "        \n", "    test_predictions[clf_name] = temp.apply(lambda x: x.value_counts().index[0], axis = 1)"], "cell_type": "code"}, {"metadata": {"_uuid": "7653cd0ffa8b767b146848a988e22e773d83a093", "_cell_guid": "32309248-f0b6-460a-b83f-2aecd9b39cab"}, "source": ["### Logistic regression"], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "a84a0cadb24adf35970c0bbac4607188cfdc7289", "_cell_guid": "12bc2fd2-87fa-4a5b-b58b-1bce81255bda"}, "outputs": [], "source": ["# Initialize the model with desired parameters.\n", "lr = LogisticRegression(random_state = seed)\n", "train_model(clf_name = 'logistic_regression', clf = lr, prediction_df = train_predictions, train_df = train, test_df = test)"], "cell_type": "code"}, {"metadata": {"_uuid": "4b079bf3ba24bb7f25daf9a4db516642e5839330", "_cell_guid": "25afe5a0-8138-4eb6-9e76-82c306c30c0e"}, "source": ["### SVC"], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "ddda303cb00640c3a406e3317c85419001c8c939", "_cell_guid": "110f6209-7c40-4148-84f6-8b6ea0b18632"}, "outputs": [], "source": ["# Initialize the model with desired parameters.\n", "svc = SVC(random_state = seed, kernel = 'linear', C = 0.025)\n", "train_model(clf_name = 'SVC', clf = svc, prediction_df = train_predictions, train_df = train, test_df = test)"], "cell_type": "code"}, {"metadata": {"_uuid": "cb6dc3506befbac76df793c5478af4f69b0cd268", "_cell_guid": "6838a8ed-75fc-4f16-802b-85912adb943d"}, "source": ["### Decision Tree Classifier"], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "2b913daba124f0629cf54a1feff1051d05ae3cda", "_cell_guid": "b7b04c0e-7890-41bc-a37a-475f3a9c8bf4"}, "outputs": [], "source": ["# Initialize the model with desired parameters.\n", "dtc = DecisionTreeClassifier(random_state = seed, max_depth = 10, min_samples_split = 30)\n", "train_model(clf_name = 'decision_tree_classifier', clf = dtc, prediction_df = train_predictions, train_df = train, test_df = test)"], "cell_type": "code"}, {"metadata": {"_uuid": "ae2f09af175f9921715a28facef4991231cb86bc", "_cell_guid": "cf66ce47-c547-4642-af11-c40f8f222e7a"}, "source": ["### Random Forest Classifier"], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "fa4b8a9bbc3004dcfecdd622c8f1331f213c735d", "_cell_guid": "e8f32cea-76c4-4871-8654-46ed473d867b"}, "outputs": [], "source": ["# Initialize the model with desired parameters.\n", "rfc = RandomForestClassifier(random_state = seed, n_estimators = 500, warm_start = True,\n", "                             max_depth = 5, min_samples_leaf = 5)\n", "train_model(clf_name = 'random_forest_classifier', clf = rfc, prediction_df = train_predictions, train_df = train, test_df = test)"], "cell_type": "code"}, {"metadata": {"_uuid": "9e896fea00a54887fa51e6fe6584e52448913aad", "_cell_guid": "db456412-6b39-4381-a01a-09ceddfb87ae"}, "source": ["### Extra Trees Classifier"], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "1055066ac5c515c7a8f7858906ad58e847360d01", "_cell_guid": "e749e0ac-ea23-48aa-b437-3b286558633b"}, "outputs": [], "source": ["# Initialize the model with desired parameters.\n", "etc = ExtraTreesClassifier(random_state = seed, n_estimators = 500, warm_start = True,\n", "                             max_depth = 8, min_samples_leaf = 5)\n", "train_model(clf_name = 'extra_trees_classifier', clf = etc, prediction_df = train_predictions, train_df = train, test_df = test)"], "cell_type": "code"}, {"metadata": {"_uuid": "6324bd1864747858f296b2b8c87aeace6eec9c8d", "_cell_guid": "9ab8a9b6-51c9-42f2-9e5e-557610262064"}, "source": ["### Gradient Boosting Classifier"], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "42f4dfa92a0247e9c6a11b62385a19ab187fe7c8", "_cell_guid": "fd1ea854-f756-45d3-97a5-fa48445da34a"}, "outputs": [], "source": ["# Initialize the model with desired parameters.\n", "gbc = GradientBoostingClassifier(random_state = seed, n_estimators = 50, warm_start = True, learning_rate = 0.1,\n", "                                 max_depth = 5, min_samples_leaf = 25)\n", "train_model(clf_name = 'gradient_boosting_classifier', clf = gbc, prediction_df = train_predictions, train_df = train, test_df = test)"], "cell_type": "code"}, {"metadata": {"_uuid": "dd5e265818c766f4cac66a01f135bceb515af399", "_cell_guid": "5aeae757-1b1f-4847-9a00-34beb669251a"}, "source": ["### Ada Boost Classifier"], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "9eafed9b6336f00f31350ee03447260f31a4ce1e", "_cell_guid": "41678dc1-c74d-4cac-8fbb-8d30c82de168"}, "outputs": [], "source": ["# Initialize the model with desired parameters.\n", "abc = AdaBoostClassifier(random_state = seed, n_estimators = 500)\n", "train_model(clf_name = 'ada_boost_classifier', clf = abc, prediction_df = train_predictions, train_df = train, test_df = test)"], "cell_type": "code"}, {"metadata": {"_uuid": "79f20d095f1c290c3d454595c23555a83708cea8", "_cell_guid": "1d91d226-9aa3-462f-8c77-9f79b0847099"}, "source": ["### K-neighbours Classifier"], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "ce218bb225082c41222db7a2b677b7300f1250fc", "_cell_guid": "f833ca7e-ed70-45f3-885c-b85d0a9458cb"}, "outputs": [], "source": ["# Initialize the model with desired parameters.\n", "knc = KNeighborsClassifier(p = 2, n_neighbors = 3)\n", "train_model(clf_name = 'k_neighbors_classifier', clf = knc, prediction_df = train_predictions, train_df = train, test_df = test)"], "cell_type": "code"}, {"metadata": {"_uuid": "2038d892ea2b8ee646bd7f99d42db1ccd6380820", "_cell_guid": "9006cae9-0040-40e0-9991-ef1cd5817a6c"}, "source": ["### Accuracy and AUC scores\n", "\n", "There are various objective criterias to judge how good the model is. Confusion matrix ([insert link here](wiki confusion matrix)) provides list of many such indicators, which can be calculated easily. Depending upon the problem at hand, appropriate indicators must be selected to decide usefullness of the model.<br>\n", "\n", "#### Accuracy\n", "\n", "Accuracy of the model is simply number of correct predictions divided by number of total predictions.\n", "\n", "#### AUC score\n", "\n", "AUC is area under ROC curve. Higher AUC score usually means better model. Although it is not always true."], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "e19f5f71e0551807fa992693c82c058617a3d08e", "_cell_guid": "dc92ebd3-dfd4-4e73-b61f-1c6c65db37df"}, "outputs": [], "source": ["accuracy = {}\n", "for col in train_predictions.columns:\n", "    accuracy[col] = sum([1 if train_predictions[col].loc[i] == survived.loc[i] else 0 for i in survived.index])/791"], "cell_type": "code"}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "fbb75cd5b0c18ce396443a259c34070ceb4cdd73", "_cell_guid": "4b4e8f33-4444-4540-a2f6-705eacda64ce"}, "outputs": [], "source": ["fig, ax = plt.subplots(1,1, figsize = (10,5))\n", "sns.barplot(x = sorted(accuracy, key = accuracy.get, reverse = True), y = np.sort(list(accuracy.values()))[::-1],\n", "            ax = ax, color = 'c')\n", "for label in ax.get_xticklabels():\n", "    label.set_rotation(90)\n", "    label.set_fontsize(15)"], "cell_type": "code"}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "f585f6f4e864d638da91dc588a1416ba9c99c9e6", "_cell_guid": "0bc13ade-85fd-44f3-a1d9-1793841fa81e"}, "outputs": [], "source": ["auc_score = {}\n", "for col in train_predictions.columns:\n", "    auc_score[col] = roc_auc_score(survived, train_predictions[col])"], "cell_type": "code"}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "4843c33300330d82003b01cffd83b200842327dc", "_cell_guid": "7272b494-2cf0-4ab4-9eac-6842ecbd4959"}, "outputs": [], "source": ["fig, ax = plt.subplots(1,1, figsize = (10,5))\n", "sns.barplot(x = sorted(auc_score, key = auc_score.get, reverse = True), y = np.sort(list(auc_score.values()))[::-1],\n", "            ax = ax, color = 'c')\n", "for label in ax.get_xticklabels():\n", "    label.set_rotation(90)\n", "    label.set_fontsize(15)"], "cell_type": "code"}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "45cb5a39f4e329e31432e90aec8e095a9f45f76e", "_cell_guid": "f2d53e08-c6d1-453f-ac44-04579cd3025d"}, "outputs": [], "source": ["corr = train_predictions.corr()\n", "f, ax = plt.subplots(1,1,figsize = (12,12))\n", "sns.set(font_scale = 1)\n", "sns.heatmap(corr,square=True, annot=True, ax = ax, cmap = 'PuBu', cbar=True,\n", "            cbar_kws={\"shrink\": 0.75}, fmt = '.2f')\n", "plt.setp(ax.get_xticklabels(), fontsize=14)\n", "plt.setp(ax.get_yticklabels(), fontsize=14)\n", "plt.show()"], "cell_type": "code"}, {"metadata": {"_uuid": "fd11fb7845d666eb7951d8da418a41fad6471d08", "_cell_guid": "ae634d67-007a-42dd-8d42-101be3546e4c"}, "source": ["<a id = '3.2'></a>\n", "### 3.2 Second level model\n", "\n", "Second level models train on predictions of first level model. Such models usually have lower generalization error. This is explained in more detail with a simple model below."], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "2aedc149adf024bc1add0d998ed4d52e98509940", "_cell_guid": "53655844-d1b4-4ad5-8377-eb991390d86a"}, "outputs": [], "source": ["first_level_models = train_predictions.columns"], "cell_type": "code"}, {"metadata": {"_uuid": "8f69fb6f629d694dff58275cd65eee1918c7f452", "_cell_guid": "915f6051-df5e-4ea6-9d72-a8a2be586e58"}, "source": ["#### Majority voting\n", "\n", "Now we have 7 first level models and their predictions. Chances that a particular example will be classified into wrong category by majority of the models are low. So, our simple second level model will be to look at the predictions from all 7 models and take majority vote."], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "cf35f5be9128845440141bf2810aa3a8e151c09f", "_cell_guid": "c8891be3-c5da-4f81-a1d0-5d68ce8024ca"}, "outputs": [], "source": ["train_predictions['majority_voting_all_models'] = train_predictions[first_level_models].apply(lambda x: x.value_counts().index[0], axis = 1)\n", "test_predictions['majority_voting_all_models'] = test_predictions[first_level_models].apply(lambda x: x.value_counts().index[0], axis = 1)\n", "accuracy['majority_voting_all_models'] = sum([1 if train_predictions['majority_voting_all_models'].loc[i] == survived.loc[i] else 0 for i in survived.index])/791\n", "accuracy['majority_voting_all_models']"], "cell_type": "code"}, {"metadata": {"_uuid": "81582476a1e1ffc4c146465d5676d5f02e0cba85", "_cell_guid": "e67c801b-4526-41b7-b5e4-c12f435067fe"}, "source": ["Note that there is strong correlation between many of the selected models. This means they will dominate the voting and will not allow us to benefit from the differences between models. So, let us select 3 models with highest accuracy but low correlation."], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "f178bfa41273755a48d0206fe4021040020e7f86", "_cell_guid": "6d4c14c8-9212-43a9-979e-dd4c0a65db29"}, "outputs": [], "source": ["# 'logistic_regression','extra_trees_classifier','gradient_boosting_classifier' and 'random_forest_classifier' are top 4 models.\n", "# But 'extra_trees_classifier' and 'random_forest_classifier' have high correlation, so we'' choose only 1 out of these 2.\n", "selected_cols = ['logistic_regression','extra_trees_classifier','gradient_boosting_classifier']\n", "train_predictions['majority_voting_selected_cols'] = train_predictions[selected_cols].apply(lambda x: x.value_counts().index[0],\n", "                                                                                            axis = 1)\n", "test_predictions['majority_voting_selected_cols'] = test_predictions[selected_cols].apply(lambda x: x.value_counts().index[0],\n", "                                                                                            axis = 1)\n", "accuracy['majority_voting_selected_cols'] = sum([1 if train_predictions['majority_voting_selected_cols'].loc[i] == survived.loc[i] else 0 for i in survived.index])/791\n", "accuracy['majority_voting_selected_cols']"], "cell_type": "code"}, {"metadata": {"_uuid": "e9ef2a1f6cd1c92a712989f3f628afc5cf8cbd27", "_cell_guid": "178d96a2-d4ef-4ab3-9f7f-9bbbeac101a9"}, "source": ["#### Logistic regression\n", "Let us try to train Logistic regression model as a second level model. See if it performs better than majority voting."], "cell_type": "markdown"}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "a4c232e8b8bcc2e57f564ccc7246cae4e5316550", "_cell_guid": "f3332d4e-de90-4512-86ab-2f769dc742c0"}, "outputs": [], "source": ["lr_second_level = LogisticRegression(random_state = seed)\n", "train_model(clf_name = 'logistic_regression_second_level', clf = lr_second_level, prediction_df = train_predictions,\n", "            train_df = train_predictions[first_level_models], test_df = test_predictions[first_level_models])\n", "accuracy['logistic_regression_second_level'] = sum([1 if train_predictions['logistic_regression_second_level'].loc[i] == survived.loc[i] else 0 for i in survived.index])/791\n", "accuracy['logistic_regression_second_level']"], "cell_type": "code"}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "fb6fb0b124a95f96660c6ff1ab429231e2785f2f", "_cell_guid": "70589308-9044-43b3-b423-df314994c528"}, "outputs": [], "source": ["lr_second_level_selected_cols = LogisticRegression(random_state = seed)\n", "train_model(clf_name = 'logistic_regression_second_level_selected_cols', clf = lr_second_level, prediction_df = train_predictions,\n", "            train_df = train_predictions[selected_cols], test_df = test_predictions[selected_cols])\n", "accuracy['logistic_regression_second_level_selected_cols'] = sum([1 if train_predictions['logistic_regression_second_level_selected_cols'].loc[i] == survived.loc[i] else 0 for i in survived.index])/791\n", "accuracy['logistic_regression_second_level_selected_cols']"], "cell_type": "code"}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "a2f0e6e5c329b2c88e008d0596a801398293968a", "_cell_guid": "6d3a0a28-181b-4627-b650-0609caead20c"}, "outputs": [], "source": ["most_accurate_clfs = sorted(accuracy, key = accuracy.get, reverse = True)\n", "most_accurate_clfs"], "cell_type": "code"}, {"execution_count": null, "metadata": {"collapsed": true, "_uuid": "cc9066ef3bc25082cff75ef42e085966d3ad91d1", "_cell_guid": "c2e40670-b7c7-479a-901a-77408ca7a834"}, "outputs": [], "source": ["submission = pd.DataFrame({'PassengerId' : test_passenger_id,\n", "                          'survived' : test_predictions['logistic_regression_second_level_selected_cols']})\n", "submission.to_csv('titanic.csv', index = False)"], "cell_type": "code"}], "metadata": {"anaconda-cloud": {}, "kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}, "language_info": {"name": "python", "mimetype": "text/x-python", "file_extension": ".py", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "version": "3.6.3", "nbconvert_exporter": "python"}}, "nbformat": 4, "nbformat_minor": 1}