{"nbformat": 4, "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"pygments_lexer": "ipython3", "nbconvert_exporter": "python", "codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "version": "3.6.3", "name": "python"}}, "nbformat_minor": 1, "cells": [{"source": ["## Introduction"], "metadata": {"_uuid": "c1114ce636175580d91cf6729d367678514d1bbd", "_cell_guid": "4fa2d088-1d5e-4fee-886f-01326574b28c"}, "cell_type": "markdown"}, {"source": ["This kernel is an attempt to perform supervised learning using Python on the Titanic dataset and is largely based on the following Kernels:\n", "* https://www.kaggle.com/sinakhorami/titanic-best-working-classifier\n", "* https://www.kaggle.com/omarelgabry/a-journey-through-titanic\n", "* https://www.kaggle.com/wei10117/titanic-machine-learning-from-disaster"], "metadata": {"_uuid": "1226ca39d84d8cee669908972fd18c8a5282f994", "_cell_guid": "99824de0-4cc9-4409-933f-cd85e8623e7d"}, "cell_type": "markdown"}, {"source": ["import matplotlib.mlab as mlab\n", "import matplotlib.pyplot as plt\n", "import numpy as np\n", "import pandas as pd\n", "import re as re\n", "\n", "train = pd.read_csv('../input/train.csv', header = 0, dtype={'Age': np.float64})\n", "test  = pd.read_csv('../input/test.csv' , header = 0, dtype={'Age': np.float64})\n", "full_data = [train, test]\n", "\n", "print (train.info())\n", "print (test.info())"], "outputs": [], "metadata": {"_uuid": "bcb08fb7c612bfd2ce016eaba746179f6f43b75e", "_cell_guid": "902607d4-23b8-45ad-b1a1-961561eb7360"}, "execution_count": null, "cell_type": "code"}, {"source": ["# preview the train data\n", "train.head()"], "outputs": [], "metadata": {"_uuid": "dfc4a214f96d86cc438253ac3f9d007233e42b2f", "_cell_guid": "0e0480a0-6336-48cb-b203-cb9b4c391768"}, "execution_count": null, "cell_type": "code"}, {"source": ["# preview the test data\n", "test.head()"], "outputs": [], "metadata": {"_uuid": "c5ec25dcd2606a3ba85a171d17deb30345c5e4f7", "_cell_guid": "68361d22-a9fc-4327-923d-679a3c2f1358"}, "execution_count": null, "cell_type": "code"}, {"source": ["## Features Engineering"], "metadata": {"_uuid": "f892ac3a4e3e1d8641c32c295ddcd5318732dc76", "_cell_guid": "31bd8786-c154-4169-9241-0b58b1be35b5"}, "cell_type": "markdown"}, {"source": ["### Pclass  \n", "Pclass         891 non-null int64 \n", "Pclass         418 non-null int64  \n", "This is a numerical value already and there are no missing values.  If we explore the probability of survival for the various Pclass options we can note that there is a distinct difference in the probability of survival.  Intuitively this makes sense as we know passengers belonging to higher classes were more likely to have access to a lifeboat."], "metadata": {"_uuid": "59a1354932d6d929b2f9917b4e2424a23245d6c3", "_cell_guid": "e6cb77b7-9909-4e4d-98a1-8437b560885c"}, "cell_type": "markdown"}, {"source": ["pclass = train[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean()\n", "samples = train[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).count()\n", "pclass['Samples'] = samples['Survived']\n", "print (pclass)"], "outputs": [], "metadata": {"_uuid": "5c1e2e07ba0a00199679224646e69e22727d14ca", "_cell_guid": "2aedca4c-7571-47d5-ab26-4f3a44b60c26"}, "execution_count": null, "cell_type": "code"}, {"source": ["### Sex  \n", "Sex            891 non-null object  \n", "Sex            418 non-null object  \n", "Sex also has no missing values and is a categorical variable.  If we explore the probability of survival we find that females were far more likely to survive than men, another result we would intuitively expect as women had preference for life boats."], "metadata": {"_uuid": "6939d3be8fc6a767d74af740bc55e746374fa8bf", "_cell_guid": "8f1f5cc4-cb6c-4eb9-a3b1-77d2e92479f5"}, "cell_type": "markdown"}, {"source": ["sex = train[['Sex', 'Survived']].groupby(['Sex'], as_index=False).mean()\n", "samples = train[['Sex', 'Survived']].groupby(['Sex'], as_index=False).count()\n", "sex['Samples'] = samples['Survived']\n", "print (sex)"], "outputs": [], "metadata": {"_uuid": "ca9ff6f9211cbd7fdcee7c83cbae1d84863c4c5b", "_cell_guid": "bd0c1b40-2b70-44fb-b6aa-5dce4bccf5c9"}, "execution_count": null, "cell_type": "code"}, {"source": ["### SibSp and Parch  \n", "SibSp          891 non-null int64  \n", "Parch          891 non-null int64  \n", "SibSp          418 non-null int64  \n", "Parch          418 non-null int64  \n", "We can create a feature called \"FamilySize\" by using the number of parents and number siblings features.  This shows an affect on the survival of passengers."], "metadata": {"_uuid": "17b8de41013f97a698d2324e74dcd996a44854c5", "_cell_guid": "a4ff32aa-21e2-48b6-9a5f-7cb6c954be09"}, "cell_type": "markdown"}, {"source": ["for dataset in full_data:\n", "    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n", "family_size = train[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean()\n", "samples = train[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).count()\n", "family_size['Samples'] = samples['Survived']\n", "print (family_size)"], "outputs": [], "metadata": {"_uuid": "caf89e9acafd374e0a838a87b6da8a088faf5ba2", "_cell_guid": "26514e26-573a-41e0-93e3-96d67f4cbdc9"}, "execution_count": null, "cell_type": "code"}, {"source": ["Create another feature to determine whether a passenger \"IsAlone\" or not.  This clearly has an impact on the survival of passengers showing that passengers who are alone are more likely to survive.  This can explained by the fact that they do not need to find their family members before boarding a lifeboat and they can also jump onto a lifeboat when there is only 1 spot left... something someone with family would be unlikely to do."], "metadata": {"_uuid": "0505ce361e85600d73be3fc57680e94f22fda073", "_cell_guid": "3fc0341e-beca-403a-8c5e-4d3c5e1295e8"}, "cell_type": "markdown"}, {"source": ["for dataset in full_data:\n", "    dataset['IsAlone'] = 0\n", "    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n", "is_alone = train[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()\n", "samples = train[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).count()\n", "is_alone['Samples'] = samples['Survived']\n", "print (is_alone)"], "outputs": [], "metadata": {"_uuid": "3b907d90033d20e69954c78570723b36adcde191", "_cell_guid": "761ad26d-a5ad-47b3-8838-5996693d5e4b"}, "execution_count": null, "cell_type": "code"}, {"source": ["### Fare  \n", "Fare           891 non-null float64  \n", "Fare           417 non-null float64  \n", "It seems as though there are no missing values in the training set but 1 missing value in the test set.  A look at the histogram shows that the majority of fees are between 1 and 100. The qcut function splits the data into quantiles - so these are bins with evenly distributed samples  \n", "http://pandas.pydata.org/pandas-docs/version/0.15.0/generated/pandas.qcut.html"], "metadata": {"_uuid": "57de63287cc30bb80198ac88c4b26750aa85ae9b", "_cell_guid": "3d467d6c-8746-477f-bb46-65dd6d3e08b0"}, "cell_type": "markdown"}, {"source": ["# the histogram of the data\n", "n, bins, patches = plt.hist(train['Fare'], 10, facecolor='blue', alpha=0.75)\n", "plt.xlabel('Fare')\n", "plt.ylabel('Number of Samples')\n", "plt.title(r'Histogram of Fare for passengers in training data')\n", "plt.show()\n", "\n", "for dataset in full_data:\n", "    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\n", "\n", "# Create categorical fare values by splitting data in quantiles\n", "train['CategoricalFare'] = pd.qcut(train['Fare'], 10)\n", "categorical_fare = train[['CategoricalFare', 'Survived']].groupby(['CategoricalFare'], as_index=False).mean()\n", "samples = train[['CategoricalFare', 'Survived']].groupby(['CategoricalFare'], as_index=False).count()\n", "categorical_fare['Samples'] = samples['Survived']\n", "print (categorical_fare)"], "outputs": [], "metadata": {"_uuid": "0d62bf24ebe1b0f29eb657b54cd9cceb0007c827", "_cell_guid": "9985abf2-4064-4998-af72-5261ade285c2"}, "execution_count": null, "cell_type": "code"}, {"source": ["### Age\n", "Age            714 non-null float64  \n", "Age            332 non-null float64  \n", "There are alot of missing values in the training and in the test set.  We will replace the missing values with the mean of the test set.  \n", "We can see that the first two age categories (which are below 16) have a higher probability of survival and the last bin (ages > 72) also have a higher probability fo survival.  This again makes intuitive sense as children and elderly were more likely to get preference for a lfe boat.\n"], "metadata": {"_uuid": "0c5f894e558c9dbfeddaee29aa858559cdf383f0", "_cell_guid": "253b29e6-4516-4dc9-8d0a-266388ec2fd5"}, "cell_type": "markdown"}, {"source": ["for dataset in full_data:\n", "    dataset['Age'] = dataset['Age'].fillna(train['Age'].median())\n", "    train['Age'] = train['Age'].fillna(train['Age'].median())\n", "    test['Age'] = test['Age'].fillna(train['Age'].median())\n", "    \n", "# the histogram of the data\n", "n, bins, patches = plt.hist(train['Age'], 10, facecolor='blue', alpha=0.75)\n", "plt.xlabel('Age')\n", "plt.ylabel('Number of Samples')\n", "plt.title(r'Histogram of Age for passengers in training data')\n", "plt.show()\n", "\n", "# Create categorical age values by splitting data in quantiles\n", "#train['CategoricalAge'] = pd.qcut(train['Age'], 10, duplicates='drop')\n", "train['CategoricalAge'] = pd.cut(train['Age'], 10)\n", "categorical_age = train[['CategoricalAge', 'Survived']].groupby(['CategoricalAge'], as_index=False).mean()\n", "samples = train[['CategoricalAge', 'Survived']].groupby(['CategoricalAge'], as_index=False).count()\n", "categorical_age['Samples'] = samples['Survived']\n", "print (categorical_age)"], "outputs": [], "metadata": {"_uuid": "2008b2bfcdb8c8da773c0840714b55c4f872c4ef", "_cell_guid": "b151dc94-9ce4-421a-bf68-9ce19a2f887d"}, "execution_count": null, "cell_type": "code"}, {"source": ["### Title  \n", "Considering the title of people.  There are 4 main titles  \n", "* Mr\n", "* Mrs\n", "* Miss\n", "* Master\n", "Then there are a number of other titles that occur very infrequently"], "metadata": {"_uuid": "727c5d5f3ce0e57cb9f9ed84e8bdf1bb14dfa734", "_cell_guid": "5c3280d7-cdfc-4328-8187-b88136686ea6"}, "cell_type": "markdown"}, {"source": ["def get_title(name):\n", "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n", "    # If the title exists, extract and return it.\n", "    if title_search:\n", "        return title_search.group(1)\n", "    return \"\"\n", "\n", "for dataset in full_data:\n", "    dataset['Title'] = dataset['Name'].apply(get_title)\n", "\n", "print(pd.crosstab(train['Title'], train['Sex']))"], "outputs": [], "metadata": {"_uuid": "f2461e06c147c66804b3278b070839dba471c001", "_cell_guid": "80fd8c2b-7042-482f-92f7-f75e98d5f71c"}, "execution_count": null, "cell_type": "code"}, {"source": ["The remaining titles are collected up and classed as \"Rare\".  Looking at the probability of survival across titles we see that there is a difference.  This makes intuitive senses as we already know gender and wealth play a role in the probability of survival."], "metadata": {"_uuid": "91aa79ba20636743842c71485367659e80c0f37e", "_cell_guid": "baa54e0c-46c9-46f8-a9ba-bfeeb5825966"}, "cell_type": "markdown"}, {"source": ["for dataset in full_data:\n", "    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n", " \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n", "\n", "    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n", "    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n", "    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n", "\n", "print (train[['Title', 'Survived']].groupby(['Title'], as_index=False).mean())"], "outputs": [], "metadata": {"_uuid": "7023edb6519b2028b6f09df6b3c4ce280404fc83", "_cell_guid": "b45cf4a8-3ea4-4533-8ba5-0ec934069f2b"}, "execution_count": null, "cell_type": "code"}, {"source": ["## Data Cleaning\n", "In this section we will perform the following actions on the data:\n", "* Clean data by imputing missing values\n", "* Map features into numerical values"], "metadata": {"_uuid": "c1dfe23af783e67779f12e644848f2e36f518872", "_cell_guid": "5d9b0c26-82cc-4eed-b970-7f09496fdc70"}, "cell_type": "markdown"}, {"source": ["for dataset in full_data:\n", "    # Mapping Sex\n", "    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n", "\n", "    # Mapping titles\n", "    title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n", "    dataset['Title'] = dataset['Title'].map(title_mapping)\n", "    dataset['Title'] = dataset['Title'].fillna(0)\n", "    \n", "    # Mapping Fare\n", "    dataset.loc[ dataset['Fare'] <= 7.55, 'Fare'] = 0\n", "    dataset.loc[(dataset['Fare'] > 7.55) & (dataset['Fare'] <= 7.854), 'Fare'] = 1\n", "    dataset.loc[(dataset['Fare'] > 7.854) & (dataset['Fare'] <= 8.05), 'Fare'] = 2\n", "    dataset.loc[(dataset['Fare'] > 8.05) & (dataset['Fare'] <= 10.5), 'Fare'] = 3\n", "    dataset.loc[(dataset['Fare'] > 10.5) & (dataset['Fare'] <= 14.454), 'Fare'] = 4\n", "    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 21.679), 'Fare'] = 5\n", "    dataset.loc[(dataset['Fare'] > 21.679) & (dataset['Fare'] <= 27.0), 'Fare'] = 6\n", "    dataset.loc[(dataset['Fare'] > 27.0) & (dataset['Fare'] <= 39.688), 'Fare'] = 7\n", "    dataset.loc[(dataset['Fare'] > 39.688) & (dataset['Fare'] <= 77.958), 'Fare'] = 8\n", "    dataset.loc[ dataset['Fare'] > 77.958, 'Fare'] = 9\n", "    dataset['Fare'] = dataset['Fare'].astype(int)\n", "    \n", "    # Mapping Age\n", "    dataset.loc[ dataset['Age'] <= 8.378, 'Age'] = 0\n", "    dataset.loc[(dataset['Age'] > 8.378) & (dataset['Age'] <= 16.336), 'Age'] = 1\n", "    dataset.loc[(dataset['Age'] > 16.336) & (dataset['Age'] <= 24.294), 'Age'] = 2\n", "    dataset.loc[(dataset['Age'] > 24.294) & (dataset['Age'] <= 32.252), 'Age'] = 3\n", "    dataset.loc[(dataset['Age'] > 32.252) & (dataset['Age'] <= 40.21), 'Age'] = 4\n", "    dataset.loc[(dataset['Age'] > 40.21) & (dataset['Age'] <= 48.168), 'Age'] = 5\n", "    dataset.loc[(dataset['Age'] > 48.168) & (dataset['Age'] <= 56.126), 'Age'] = 6\n", "    dataset.loc[(dataset['Age'] > 56.126) & (dataset['Age'] <= 64.084), 'Age'] = 7\n", "    dataset.loc[(dataset['Age'] > 64.084) & (dataset['Age'] <= 72.042), 'Age'] = 8\n", "    dataset.loc[ dataset['Age'] > 72.042, 'Age'] = 9\n", "    dataset['Age'] = dataset['Age'].astype(int)\n", "\n", "# Feature Selection\n", "drop_elements = ['Name', 'Ticket', 'Cabin', 'SibSp','Parch', 'FamilySize', 'Embarked']\n", "train = train.drop(drop_elements, axis = 1)\n", "train = train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\n", "\n", "test  = test.drop(drop_elements, axis = 1)\n", "\n", "print (train.head(10))\n", "print (test.head(10))\n", "\n", "#train = train.values\n", "#test  = test.values"], "outputs": [], "metadata": {"_uuid": "471749de49d89b5d49f9bdfdbd3c20326569f8d9", "scrolled": true, "_cell_guid": "c91147e9-ea9a-4976-b6d2-09b82cdc5abc"}, "execution_count": null, "cell_type": "code"}, {"source": ["## Modelling  \n", "Now that we have performed the features engineering and have a clean training and test data set, we can implement various types of machine learning models and test which ones perform the best.\n"], "metadata": {"_uuid": "99c9c07e63692475a0f9efb85b36244e8426b9f1", "_cell_guid": "35670061-b391-4668-bf3e-74ceb5545705"}, "cell_type": "markdown"}, {"source": ["# import machine learning libraries\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.svm import SVC, LinearSVC\n", "from sklearn.ensemble import RandomForestClassifier\n", "from sklearn.neighbors import KNeighborsClassifier\n", "from sklearn.naive_bayes import GaussianNB\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.model_selection import cross_val_score\n", "\n", "# define training (X_train) and testing (y_train) sets\n", "X_train = train.drop(['PassengerId','Survived'],axis=1)\n", "y_train = train[\"Survived\"]\n", "X_test = test.drop('PassengerId',axis=1)\n", "\n", "print (X_train.head(10))\n", "print (y_train.head(10))"], "outputs": [], "metadata": {"_uuid": "9b00acded751a0cce691fec186c875d66f79ecdc", "_cell_guid": "5d4968c9-b08c-40b1-915d-fbe09398de8e"}, "execution_count": null, "cell_type": "code"}, {"source": ["# Logistic Regression\n", "# Submission Results: 0.76555\n", "    \n", "logreg = LogisticRegression()\n", "\n", "logreg.fit(X_train, y_train)\n", "\n", "#Use the model to make prediction on test data\n", "y_pred = logreg.predict(X_test)\n", "logregsubmission = pd.DataFrame({\n", "        \"PassengerId\": test[\"PassengerId\"],\n", "        \"Survived\": y_pred\n", "    })\n", "logregsubmission.to_csv('logreg.titanic.csv', index=False)\n", "\n", "logreg.score(X_train, y_train)"], "outputs": [], "metadata": {"_uuid": "9f579ad606d2c265fc6671535f6ac8ae532ec254", "_cell_guid": "e5092cd1-470e-4708-9423-d53f5746ad88"}, "execution_count": null, "cell_type": "code"}, {"source": ["# SVM\n", "# Submission results: 0.76555\n", "\n", "svc = SVC()\n", "\n", "svc.fit(X_train, y_train)\n", "\n", "#Use the model to make prediction on test data\n", "y_pred = svc.predict(X_test)\n", "svcsubmission = pd.DataFrame({\n", "        \"PassengerId\": test[\"PassengerId\"],\n", "        \"Survived\": y_pred\n", "    })\n", "svcsubmission.to_csv('svc.titanic.csv', index=False)\n", "\n", "svc.score(X_train, y_train)"], "outputs": [], "metadata": {"_uuid": "648f54ab2e948699e6a3c58324fba253562d5ccb", "_cell_guid": "3702636a-c2b0-48e8-b41d-8546ba214402"}, "execution_count": null, "cell_type": "code"}, {"source": ["# Random Forests\n", "# Submission Results: 0.74162\n", "\n", "random_forest = RandomForestClassifier(n_estimators=100)\n", "\n", "random_forest.fit(X_train, y_train)\n", "\n", "#Use the model to make prediction on test data\n", "y_pred = random_forest.predict(X_test)\n", "random_forestsubmission = pd.DataFrame({\n", "        \"PassengerId\": test[\"PassengerId\"],\n", "        \"Survived\": y_pred\n", "    })\n", "random_forestsubmission.to_csv('random_forest.titanic.csv', index=False)\n", "\n", "random_forest.score(X_train, y_train)"], "outputs": [], "metadata": {"_uuid": "470c4ece773ab8869235e14108b86137087cce1e", "_cell_guid": "501385af-49ba-458e-ac79-2ab20c88dd56"}, "execution_count": null, "cell_type": "code"}, {"source": ["# K Nearest Neighbors\n", "# Submission results: 0.76076\n", "\n", "knn = KNeighborsClassifier(n_neighbors = 3)\n", "\n", "knn.fit(X_train, y_train)\n", "\n", "#Use the model to make prediction on test data\n", "y_pred = knn.predict(X_test)\n", "knnsubmission = pd.DataFrame({\n", "        \"PassengerId\": test[\"PassengerId\"],\n", "        \"Survived\": y_pred\n", "    })\n", "knnsubmission.to_csv('knn.titanic.csv', index=False)\n", "\n", "knn.score(X_train, y_train)"], "outputs": [], "metadata": {"_uuid": "3eb5e7b6868b7b3b84a9a2272925b1c6658fa492", "_cell_guid": "31322a98-bec7-4671-ad13-f18d39769f69"}, "execution_count": null, "cell_type": "code"}, {"source": ["# Gaussian Naive Bayes\n", "# Submission results: 0.72727\n", "gaussian = GaussianNB()\n", "\n", "gaussian.fit(X_train, y_train)\n", "\n", "#Use the model to make prediction on test data\n", "y_pred = gaussian.predict(X_test)\n", "gaussiansubmission = pd.DataFrame({\n", "        \"PassengerId\": test[\"PassengerId\"],\n", "        \"Survived\": y_pred\n", "    })\n", "gaussiansubmission.to_csv('gaussian.titanic.csv', index=False)\n", "\n", "gaussian.score(X_train, y_train)"], "outputs": [], "metadata": {"_uuid": "3c8e28d20cd9f60943875a1b90306812b7a785db", "_cell_guid": "28b47b56-0c29-49e3-934c-586623eafb93"}, "execution_count": null, "cell_type": "code"}]}