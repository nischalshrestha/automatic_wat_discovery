{"cells":[{"metadata":{"_uuid":"7d9809c6440cd4343524e559e20e0e5736889ad0"},"cell_type":"markdown","source":" author: André Daniël VOLSCHENK  \nKaggle project {Titanic: Machine Learning from Disaster}  \nkaggle.com/andredanielvolschenk  \n\nWelcome to my first Data Science exploration project!  \nThe purpose of this kernel is to provide a complete example of a Data Science process, as applied to the simple \"Titanic\" dataset.\n\nThat being said, I hope you enjoy this journey of data-driven discovery!   \n\nNOTE: I have decided to hide most of my code, in order to make this notebook easily readable.  \nTo view the code for any step, simply click the \"Code\" button found on the right hand margin of this notebook."},{"metadata":{"_uuid":"d2678983b2d3f5125e6b1b35bfb01ec5af5ef7fb"},"cell_type":"markdown","source":"# Problem statement\n\nBinary classification:  \nIt is your job to predict if a passenger survived the sinking of the Titanic or not.  \nMetric: \"accuracy”\n\nData Dictionary:  \n\n|Variable\t     | Definition\t\t    | Key\n|-------------------------------------------------------------------------------------------------\n|survival \t                | Survival \t\t                   |  0 = No, 1 = Yes\n|pclass \t\t            | Ticket class \t\t             |  1 = 1st, 2 = 2nd, 3 = 3rd\n|sex \t\t                  | Sex \t\n|Age \t\t                 | Age in years \t\n|sibsp \t\t                 | # of siblings / spouses aboard the Titanic \t\n|parch \t\t                | # of parents / children aboard the Titanic \t\n|ticket \t\t             | Ticket number \t\n|fare \t\t                  | Passenger fare \t\n|cabin \t\t                | Cabin number \t\n|embarked \t           | Port of Embarkation      |  C = Cherbourg, Q = Queenstown, S = Southampton\n\nVariable Notes:\n\npclass: A proxy for socio-economic status (SES)  \n1st = Upper  \n2nd = Middle  \n3rd = Lower  \n\nage: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\nsibsp: The dataset defines family relations in this way...  \nSibling = brother, sister, stepbrother, stepsister  \nSpouse = husband, wife (mistresses and fiancés were ignored)  \n\nparch: The dataset defines family relations in this way...  \nParent = mother, father  \nChild = daughter, son, stepdaughter, stepson  \nSome children travelled only with a nanny, therefore parch=0 for them.  "},{"metadata":{"_uuid":"f31aa1d92e4b73e0d9f946eaf4770b67e83d46ed"},"cell_type":"markdown","source":"# Import libraries"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_kg_hide-output":false,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nmpl.style.use('ggplot') # if error, use plt.style.use('ggplot') instead\n\nimport seaborn as sns\nsns.set_style(\"whitegrid\")     # need grid to plot seaborn plot\n\nimport scipy.stats as ss\nimport math\n\nimport sklearn as skl\nimport sklearn.metrics as sklm\nimport sklearn.feature_selection as fs\nimport sklearn.model_selection as ms\n\nimport sklearn.preprocessing as prep\nimport sklearn.decomposition as decomp\n\nimport sklearn.pipeline as pipe\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true},"cell_type":"markdown","source":"# Global variables"},{"metadata":{"_kg_hide-input":true,"_uuid":"f8331da464dfebe0b0cc9dc0474373b592f82c05","trusted":true},"cell_type":"code","source":"glbl = {}   # define dict\nglbl['show_figs'] = 1      # flag to enable printing figures\nglbl['n_jobs'] = 1        # -1 to use all available CPUs\nglbl['random_state'] = 5   # = None if we dont need demo mode\nglbl['n_iter'] = 100        # how many search iterations\nglbl['n_splits']=10            # cross validation splits","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e79c54caf1e0e07f4b61a1aaf187fc84f2f8d592"},"cell_type":"markdown","source":"# Load\nLets load the training and testing data."},{"metadata":{"_kg_hide-input":true,"_uuid":"21c66c22629ad7b2046c438da7ee26a962ce78a1","trusted":true},"cell_type":"code","source":"path = '../input/train.csv'\ndata1 = pd.read_csv(path, sep=',', index_col = 'PassengerId')\ndel(path)\n\n# load the competition test data\npath = '../input/test.csv'\ndata2 = pd.read_csv(path, sep=',', index_col = 'PassengerId')\ndel(path)\n\nprint('data1 shape:', data1.shape)\nprint('data2 shape:', data2.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"160c8054b39c94d9f4df4a5d1ffbfb63d7383cc3"},"cell_type":"markdown","source":"`data1` is from 1 through 891  \n`data2` is from 892 through 1309"},{"metadata":{"_uuid":"fc5b0d70d9a2e8fe547d64c6f62ad70cef23ee62"},"cell_type":"markdown","source":"# Merge\nWe need to merge these so that we can clean both simultaneously. This is important to ensure that `data1` and `data2` have even number of features, and that their features are represented in the same way."},{"metadata":{"_kg_hide-input":true,"_uuid":"bb80fca917fe55fea8e85f459f7d37f3fa68246d","trusted":true},"cell_type":"code","source":"data = data1.append(data2, sort=False)  # Append rows of data2 to data1\n\n# clean workspace\ndel(data1, data2)\n\nprint('data shape:', data.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f92ea53dd43d95b1a87c993a3a9435dd0611923f"},"cell_type":"markdown","source":"# View/summarize\nView Column DataTypes"},{"metadata":{"_kg_hide-input":true,"_uuid":"89aea029b6a2b0ea2b7ddc4ac20c7835e090b5aa","scrolled":true,"trusted":true},"cell_type":"code","source":"data.dtypes","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"664222c11951034e18c2e1f77544e18fd783f4da"},"cell_type":"markdown","source":"We clearly need to convert some datatypes. For example, `Pclass` is ordinal, `Sex` is nominal, etc.  \nLets now also look at a sample of the data"},{"metadata":{"_kg_hide-input":true,"_uuid":"dd603dd9fcd5dc6821ac0f61b2162cfb5fdaa21c","scrolled":true,"trusted":true},"cell_type":"code","source":"data.sample(10)     # take a random sample of 10 observations","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2cc5c04f22fabf9537f21d96e7c04ffbec1fb0a4"},"cell_type":"markdown","source":"Some features look useless, for example: `Ticket`, `Cabin`, `Embarked`. These will not contribute to predictive power.  \n`Name` would be useless, but we can actually extract some info from it, namely, Title. After that, we can delete it.  \n\n# Cleansing\nLets drop useless features...\nNow see if we have ?s"},{"metadata":{"_kg_hide-input":true,"_uuid":"300f995898e71f670263636b8ceb03fd40b72bfd","trusted":true},"cell_type":"code","source":"# drop useless features\ndata.drop(labels=['Ticket', 'Cabin', 'Embarked'], axis=1, inplace=True)\n\n# see if we have ?s\nprint ('number of nans in data:', (data.astype(np.object) == '?').any().sum() ) # we have no ?s","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2d06b4f9d6b4c17e779cd46aacff25a2d1995b09"},"cell_type":"markdown","source":"We have 0 ?s in our dataset.  \nLets see if we have any nans"},{"metadata":{"_kg_hide-input":true,"_uuid":"7db92e80fc803691a44d429eb1ed1c77e5e77cad","trusted":true},"cell_type":"code","source":"data.isnull().sum() # check each column.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6e605f0298ed9d6046a3198c0b36f4c495480ceb"},"cell_type":"markdown","source":"418 nans in `Survived`. We can ignore this, since these are the Kaggle testing data, for which we are not given labels.  \n263 nans in `Age`.  \n1 nan in `Fare`  \nFor the `Age` feature, that is 263/1309 ~ 20% of the data !!!  \n\nFirst we consider the missing `Fare`. we will use the median `Fare` for the `Pclass`...  \nLets see if we still have nans in `Fare`:"},{"metadata":{"_kg_hide-input":true,"_uuid":"36471682006a092c5006e2ed57e6d58fa704494b","trusted":true},"cell_type":"code","source":"for Pcl in data.Pclass.unique():   # 1, 2, 3\n    med = data[['Fare']].where(data.Pclass==Pcl).median()    # get median over all data for that class\n    data.loc[ ((data.Fare.isnull() == True) & (data.Pclass==Pcl)) , 'Fare'] = med[0] # med is series\n# clean workspace:\ndel(Pcl, med)\n\n# see if we have nans:\ndata.isnull().sum() # check each column.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e8b02981f2396381f8c082188f14a3b0ae5aea68"},"cell_type":"markdown","source":"Only the `Age` nans remain  \n\nI hypothesize that none of the current variables strongly correlate with `Age`. I further speculate that a `Title` feature would correlate with `Age`. We first have to create `Title`.  \n\n# Feature Engineering\nFirst, we noticed easlier that a dot (period) is placed after titles in the `Name` column.  \nThe title is early in the `Name` column, so we use the FIRST occurence of the period.  \n\nLets write a function to extract `Title` from `Name`...  \nLets now see the unique titles in our dataset"},{"metadata":{"_kg_hide-input":true,"_uuid":"839c2569cddef0a4a28285bcfb58b034ac7cfa5b","trusted":true},"cell_type":"code","source":"def extract_title(x):   # x is entire row\n    string=x['Title']\n    ix=string.find(\".\")    # use .find to find the first dot\n    for i in range(0,ix):\n        if (string[ix-i] == ' '):  # if we find space, then stop iterating\n            break                   # break out of for-loop\n    return string[(ix-i+1):ix]  # return everything after space up till before the dot\n\ndata['Title'] = data.Name  # for now copy name directly\ndata['Title']=data.apply(extract_title, axis=1)     # axis = 1 : apply function to each row\ndata.drop(labels=['Name'], axis=1, inplace=True)  # we can even drop the 'Name' column now\n\ndata.Title.unique()   # lets see the unique titles in our dataset","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f2814266447c063eb5f60d53e5532ebc4b362398"},"cell_type":"markdown","source":"Lets keep titles: 'Mr', 'Mrs', 'Miss', 'Master'. All others will be converted to one of these.  \nNotes: Mme is Mrs, and Mlle is Miss, in french. ‘Jonkheer’ is a male honorific for Dutch nobility  \n\n# Feature engineering\nWe declare the function to standardize the `Title` feature...  \nLets see the unique titles in our dataset"},{"metadata":{"_kg_hide-input":true,"_uuid":"971d59913c1b5232162a2919b84704bcf881d4d6","trusted":true},"cell_type":"code","source":"# standardize 'Title'\ndef standardize_title(x):   # x is an entire row\n    Title=x['Title']\n    \n    if x.Sex == 'male':\n        if Title != 'Master':   # we can keep 'Master' title, but we want to change all others to Mr\n            return 'Mr'\n        else:\n            return Title\n    if x.Sex == 'female':\n        if Title in ['Miss', 'Mlle', 'Ms']:\n            return 'Miss'\n        else:\n            return 'Mrs'\n\ndata['Title']=data.apply(standardize_title, axis=1)     # axis = 1 : apply function to each row\n\ndata.Title.unique()   # lets see the unique titles in our dataset","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7e775592a4ba4925854f2396df94f9af397f8e5b"},"cell_type":"markdown","source":"Perfect. All titles have been standardized!\n\n# Visualization\n\nWe have seen earlier taht the NaNs in `Age` corrupt 20% of our data.   \nWe dont want to lose 20% of our data; more data is often better than a good classifier.  \nLets look at how `Title` correlates with `Age`"},{"metadata":{"_kg_hide-input":true,"_uuid":"18ad8e7d183c5bf50a720f95986afac6c5ca0b0d","trusted":true},"cell_type":"code","source":"if glbl['show_figs']:\n    fig = plt.figure(figsize=(8,6))   # define (new) plot area\n    ax = fig.gca()   # define axis\n    data[['Age', 'Title']].boxplot(by = 'Title', ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"19213fbbd7df7e0b63c660c20c916a9843631942"},"cell_type":"markdown","source":"Indeed titles seem to have significantly different age dispersions.  \n Are there any other features that correlate with `Age`?"},{"metadata":{"_kg_hide-input":true,"_uuid":"c1e3f9a8b5611d196fa5f19aeb78b731fae4c8dc","trusted":true},"cell_type":"code","source":"if glbl['show_figs']:\n    fig = plt.figure(figsize=(8,6))   # define (new) plot area\n    ax = fig.gca()   # define axis\n    plt.imshow(data.corr(), cmap=plt.cm.Blues, interpolation='nearest') \t# plots the correlation matrix of data\n    plt.colorbar()\n    tick_marks = [i for i in range(len(data.columns))]\n    plt.xticks(tick_marks, data.columns, rotation='vertical')\n    plt.yticks(tick_marks, data.columns)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c4c698dc943d6d0b5231f5d75ccf1350489e36ee"},"cell_type":"markdown","source":"`SibSp` correlates somewhat with `Age`. Lets look at the distribution too:"},{"metadata":{"_kg_hide-input":true,"_uuid":"cb563ac2600f0516ad02c6e6caf656aceebfee5f","trusted":true},"cell_type":"code","source":"if glbl['show_figs']:\n    fig = plt.figure(figsize=(8,6))   # define (new) plot area\n    ax = fig.gca()   # define axis\n    data[['Age', 'SibSp']].boxplot(by = 'SibSp', ax=ax)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9adcf52086caebc8ad1d8a35ec1885f391b6ba32"},"cell_type":"markdown","source":"The `Age` varies greatly within the lower `SibSp` feature classes. It would not be wise to estimate `Age` from `SibSp`.  \n\n# Clean\n\nWe will replace nans in `Age` with the median of the given `Title` class.  \nLets declare a function to do that...   \nLets see if the nans in `Age` have eliminated:"},{"metadata":{"_kg_hide-input":true,"_uuid":"4054812384d59c41f68134f6a3be03ea418e1a0e","trusted":true},"cell_type":"code","source":"for titl in data.Title.unique():\n    med = data[['Age']].where(data.Title==titl).median()\n    data.loc[ (data.Age.isnull() == True) & (data.Title==titl) , 'Age'] = med[0] # med is a series. must be a scalar\ndel(titl, med)\n\n# let us see if we have nans:\ndata.isnull().sum() # check each column.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7edd64d59017fb6913a163be48a42f83ee4ea95b"},"cell_type":"markdown","source":"The nans in `Age` have been eliminated!\n\n# Feature engineering\n\nWe have successfully cleaned our features, and created a new one: `Title`.  \nPerhaps we can think of more features to create from existing ones, that may provide predictive power?\n\nWe can make a new feature called `FamSize` to indicate the family size of any passenger. Perhaps this can replace `Parch` and `SibSp`, or add to predictive power?"},{"metadata":{"_kg_hide-input":true,"_uuid":"7f98b40aa8bbc837d7dcc11d0c13a067c426a2cf","trusted":true},"cell_type":"code","source":"data['FamSize'] = data.SibSp + data.Parch + 1\nprint(data.columns)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9d85acf62c0ce93ce5e73c8f5ba4c25e39192fcb"},"cell_type":"markdown","source":"# View / summarize\n\nwe have noted earlier that the data-types of our features is NOT correct. Lets take a look again:\n\nOur features are ..."},{"metadata":{"_kg_hide-input":true,"_uuid":"f6157a52b377b37f27a10b533c701119cccec9b3","trusted":true},"cell_type":"code","source":"data.dtypes","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bde737ed0205e1896d3d711f27ce4a77a462d8ab"},"cell_type":"markdown","source":"...clearly not the right data types  \n\nLets list the data types we have and how they should be represented:\n* Nominal (as bool) : `Sex` , `Title` [also `Survived`, but this we cant do now because it has nans]\n* Ordinal (as ordered category) : `Pclass`\n* Count (as int) : `SibSp`, `Parch`, `FamSize`\n* Real-value (as float) : `Age`, `Fare`  \n\n# Feature representation\nLets fix our data types and see if they are correct now:"},{"metadata":{"_kg_hide-input":true,"_uuid":"d7208054cafeed97d7f78803fc4dafa82a45a980","trusted":true},"cell_type":"code","source":"# nominal\ndata.Sex = data.Sex.map({'female':0, 'male':1})\n\ndata = pd.get_dummies(data,columns=['Title'])       # turn nominal to bool\n\n# ordinal\nordered_categs = [1, 2, 3]  # categories for Pclass\ncategs = pd.api.types.CategoricalDtype(categories = ordered_categs, ordered=True)\ndata.Pclass = data.Pclass.astype(categs) # ordinal\n#\ndel(ordered_categs) # clean workspace\n\ndata.dtypes","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a7022a94d318050ce6a6c637b3753914c0817f02"},"cell_type":"markdown","source":"`Survived` will remain a float cause it has nans in it.  \nWe still need to turn uint8 into bool and store features in their smallest representation.\n\n# Feature representation\n\nLet us write a function that converts datatypes to it's lowest numeric representation...\nLets see if our dtypes are correct now"},{"metadata":{"_kg_hide-input":true,"_uuid":"2b540ab078be41e74c29e133870c37786f944b4f","trusted":true},"cell_type":"code","source":"def to_lowest_numeric(x):\n    # x is a column\n    if (x.apply(np.isreal).all(axis=0)) & ((str(x.dtypes) != 'category')): # if this column is numeric, but NOT categorical\n        x = pd.to_numeric(x, errors='coerce', downcast='float') # first downcast floats\n        x = pd.to_numeric(x, errors='coerce', downcast='unsigned') # now downcast ints\n    \n    # now to handle booleans:\n    # if x has only the ints 0 and 1  OR  x has only 'True' and 'False' strings\n    if set(x.unique()) == set(np.array([0, 1])) :\n        x2=x.astype('bool')\n        return x2\n    elif set(x.unique()) == set(np.array(['True', 'False'])):\n        x2 = x=='True'\n        return x2\n    else:\n        return x\n#\n\ndata = data.apply(to_lowest_numeric, axis=0)\n\n# View Column DataTypes\ndata.dtypes","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"869270b918254023c3dd7196279f02a29ca93adb"},"cell_type":"markdown","source":"Compare the dtypes to how they were before the function was applied.  \nThe data types now look correct, and their encoding is in its lowest numeric form.  \nNote: `Survived` will remain a float due to nans from `data2`.\n\nNext we will transform our features to attempt to Normalize and Standardize them. we will be using methods, like PCA, which require this. Furthermore, PCA does not handle outliers. There are classifiers which we shall use, that require this type of data.\n\n# Split\n\nBefore we can transform our features, we have to do data splitting.  \nWe should not use tranforms fitted on all our data, to prevent data leakage.\n\nFirst we seperate `data1` and `data2` from `data`.  \nrecall:\n* `data` is from 1 through 891\n* `data2` is from 892 through 1309  \nNext, we split `data1` into `X` and `y`"},{"metadata":{"_kg_hide-input":true,"_uuid":"9139669982ed7c5ac21230b71443f46cddf99633","trusted":true},"cell_type":"code","source":"data1 = data.iloc[0:891, :]     # iloc is incl:excl\ndata2 = data.iloc[891:1309, :]\n\n# we will split data1 into out train and test sets.\n# we will use data2 for the Kaggle submission at the end\n\nX = data1.drop(labels=['Survived'], axis=1)\ny=data1['Survived']    # return a series\ndata2\n\n# clean up Workspace\ndel(data, data1)\n\nprint('X shape:', X.shape)\nprint('y shape:', y.shape)\nprint('data2 shape:', data2.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3cf2f8600457f3d78da7d58782e08be9b82c5199"},"cell_type":"markdown","source":"Lets see if the dtype of `y` is boolean as it should be"},{"metadata":{"_kg_hide-input":true,"_uuid":"ce153c1c472285b01b0684a4c9456f41fb1d6e75","trusted":true},"cell_type":"code","source":"print ('y dtype:', y.dtypes ) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"75036ba1865398ecfd68481811365ff6aa8564fd"},"cell_type":"markdown","source":"No it is not boolean. Lets downcast it to boolean."},{"metadata":{"_kg_hide-input":true,"_uuid":"5cec5b2d5f23d221f49de66238c242e5f27f930c","trusted":true},"cell_type":"code","source":"y = to_lowest_numeric(y)\nprint ('y dtype:', y.dtypes ) # = 'bool'     this is correct.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e83f904aaca74d3c7a14360812ab8b943bb538c7"},"cell_type":"markdown","source":"Yes it is boolean. Good.  \nNext we will split `X` and `y` into new sets. The train:test ratio shall be 80:20."},{"metadata":{"_kg_hide-input":true,"_uuid":"0a85027affc803284a5013275beb8d93a00f7376","trusted":true},"cell_type":"code","source":"# let us split the train:test ratio at 80:20\nX_train, X_test, y_train, y_test = ms.train_test_split(X, y, test_size=0.2, \n                                                       random_state = glbl['random_state'])\n\ndel(X,y) # clean up Workspace\n\nprint('X_train shape:', X_train.shape)\nprint('X_test shape:', X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"064e6b6a8bedf1942f0a6c4ccbdabe88341f6589"},"cell_type":"markdown","source":"We will be looking at transforms, dimensionality reduction, and learning next. In these steps it is important to prevent data leakage, so we will be fitting each step with the X_train, and applying it blindly to X_test.  \n\nFor now, lets just make a copy of X_train to investigate the effects of our transforms. We can alter this copy without actually altering X_train."},{"metadata":{"_kg_hide-input":true,"_uuid":"be09c36a0618969ddec727b8de769722a9ff401e","trusted":true},"cell_type":"code","source":"X_train2 = X_train.copy()\nprint('X_train2 shape:', X_train2.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6610aaedcaca71cfe93497a1559ed72e0cc0e504"},"cell_type":"markdown","source":"# View / summarize\nLets look at the mean and standard deviation of `X_train2`"},{"metadata":{"_kg_hide-input":true,"_uuid":"42295efaa70ccf15317d1587062c8308416583da","trusted":true},"cell_type":"code","source":"X_train2.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"988108daf18d90c31cc30e15fe8d8eb74211a42f"},"cell_type":"markdown","source":"These features have very different means and standard deviations - this clearly needs standardization.   \nIdeally our data should be normally distributed. Lets take a look at that next.\n\n# Visualize\n\nLet us look at the distributions of Quantitative data. We will use Violin plots to show distributions of all Quantitative variables."},{"metadata":{"_kg_hide-input":true,"_uuid":"02c92d384b00840355869073fd7fc1c3c7f6c9dd","trusted":true},"cell_type":"code","source":"def all_violin(X):  # X is a dataframe\n    fig = plt.figure(figsize=(8,6))   # define (new) plot area\n    ax = fig.gca()   # define axis\n    ax.set_title('Violin plots of quantitative variables')\n    \n    # lets store up all the columns that are from Quantitative variables\n    # aka we will find columns that are not Nominal (bool dtype) or Ordinal (category dtype)\n    dtypes = pd.DataFrame( X.dtypes )\n    dtypes = dtypes.astype('str').values.reshape(-1,)\n    valid = ( (dtypes != 'bool') & (dtypes != 'category') )\n    \n    X = X.iloc[:,valid]     # contains only columns that are not bool and not category\n    \n    ax = sns.violinplot(data=X)     # plot on single axis\n\nif glbl['show_figs']:\n    all_violin(X_train2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fc21232167882482b643b7e16f1a441326cacf69"},"cell_type":"markdown","source":"Clearly our ranges are completely different across quantitative variables. Furthermore, the features (except maybe `Age`) do not resemble a normal distribution, so we have established the need to transforms.  \n\nLets also look at the boxplots:"},{"metadata":{"_kg_hide-input":true,"_uuid":"954e61e4a6366c6d2c37cd83dd30e8346b4b7a0d","trusted":true},"cell_type":"code","source":"def all_box(X):  # X is a dataframe\n    fig = plt.figure(figsize=(8,6))   # define (new) plot area\n    ax = fig.gca()   # define axis\n    ax.set_title('Boxplots of quantitative variables')\n    \n    # lets store up all the columns that are from Quantitative variables\n    # aka we will find columns that are not Nominal (bool dtype) or Ordinal (category dtype)\n    dtypes = pd.DataFrame( X.dtypes )\n    dtypes = dtypes.astype('str').values.reshape(-1,)\n    valid = ( (dtypes != 'bool') & (dtypes != 'category') )\n    \n    X = X.iloc[:,valid]     # contains only columns that are not bool and not category\n    \n    ax = sns.boxplot(data=X)     # plot on single axis\n\nif glbl['show_figs']:\n    all_box(X_train2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e2bddde80ca0b881e26a0ec1823f404cc9a8c281"},"cell_type":"markdown","source":"From these boxplots, we can see that some of our quantitative variables suffer greatly from outliers.  \n\n# Transforms: Normalization\n\nLets Normalize !\nDone to in hopes of making data:\n* Normally distributed\n* Homogeneity of variance  \nThe latter is more important than the former\n\nLet us apply the Quantile normalizer to the relevent data.  \nThe Quantile transformer is robust to outliers, which is perfect, since we have shown that we have serious outliers.  \nLets look at what those features look like now."},{"metadata":{"_kg_hide-input":true,"_uuid":"082dc03dec7fa520b1a638805a146d1fb58846cb","trusted":true},"cell_type":"code","source":"norm = prep.QuantileTransformer()\nf=X_train2.loc[:,['Age', 'SibSp', 'Parch', 'Fare', 'FamSize']]\nnorm.fit( np.array(f) )\nf = norm.transform(np.array(f))\nX_train2.loc[:,['Age', 'SibSp', 'Parch', 'Fare', 'FamSize']]=f\n\ndel(f)\n\nif glbl['show_figs']:\n    all_violin(X_train2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"abe8ae67602be0c257ccfe73432342516e8126da"},"cell_type":"markdown","source":"`Age` and `Fare` look more normally distributed than they did before.\nNormalization is beneficial for the standardization transformation coming next, but it is not vital.\n\n# Transforms: Scaling\nLets remind ourselves again about the mean and standard deviation..."},{"metadata":{"_kg_hide-input":true,"_uuid":"dd621225517aa778200e588acf8944ff8ac5a12c","trusted":true},"cell_type":"code","source":"X_train2.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9ed64db3c37e6838f3f65a34720e1565e1bce8fe"},"cell_type":"markdown","source":"Standard deviation is still totally different, but the data types are still fine.  \nWe will use the Standard Scaler to standardize the relevent features.  \nThis should make the means=0 and the std=1."},{"metadata":{"_kg_hide-input":true,"_uuid":"a32999a7389d91b0c2d390284e866e1441147cad","trusted":true},"cell_type":"code","source":"scaler = prep.StandardScaler()\n\nf = X_train2.loc[:,['Age','SibSp','Parch','Fare','FamSize']]\n\nscaler.fit( f )\nf = scaler.transform( f )\nf = pd.DataFrame (f)\n\nX_train2.loc[:,['Age','SibSp','Parch','Fare','FamSize']] = f.values\n\n# clean up workspace\ndel(f)\n\nX_train2.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"43e0f4a0f71d44553daa778c426096164a0f42e2"},"cell_type":"markdown","source":"It looks like our quantitative variables have (approximately) zero mean and unit variance. Therefore standardized.  \nData types are still correct.\n\n# Visualize\n\n Lets see how this affected the relevent features."},{"metadata":{"_kg_hide-input":true,"_uuid":"bfa9e66264475797504b578f4caf74c997099656","trusted":true},"cell_type":"code","source":"if glbl['show_figs']:\n    all_violin(X_train2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7c8c769bc9a5e478ded7c5e9eef824dbafa56c98"},"cell_type":"markdown","source":"Lets also look their boxplots:"},{"metadata":{"_kg_hide-input":true,"_uuid":"e5398beafbd92676d8daf7ad2399b34b9f26d485","trusted":true},"cell_type":"code","source":"if glbl['show_figs']:\n    all_box(X_train2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dd78f524ddeb117e5dc561e635dadba9ac0aab46"},"cell_type":"markdown","source":"Looks like we have removed the outliers.  \nInteresting that the `Parch` boxplot shows outliers, however we can rest assured that the data is standardized, as we confirmed in the table of means and stds earlier.\n\n# Dimensionality reduction\n\nWe want to eliminate redundant or harmful features in the model selection stage. This can be done using some feature selection method or feature projection method.  \nLet us demonstrate each of these approaches.\n* We can select a Feature selection method : RFE (Recursive feature elimination)\n* We select a feature projection method : PCA (Principle Component Analysis)  \nTo use PCA, our data must be standardized, and should not have outliers. Our data satisfies these requirements.\n\nLet's investigate PCA briefly. We will compute the principle components for the training feature subset...  \nLets look at the shape of `X_train2`, and lets also look at the explained variance per PC graphically:"},{"metadata":{"_kg_hide-input":true,"_uuid":"e11db1d8df79f575e7cbaecc55da3b2d5ce950ce","trusted":true},"cell_type":"code","source":"pca = decomp.PCA()\npca = pca.fit(X_train2)\n\ndef plotPCA_explained(mod):\n    fig = plt.figure(figsize=(8,6))   # define (new) plot area\n    ax = fig.gca()   # define axis\n    plt.suptitle('Scree plot of explained variance per principle component')\n    ax.set_xlabel('number of components')   # Set text for the x axis\n    ax.set_ylabel('explained variance')   # Set text for y axis\n    comps = mod.explained_variance_ratio_\n    x = range(len(comps))\n    x = [y + 1 for y in x]          \n    plt.plot(x,comps)\n#\nprint ('X_train2 shape:', X_train2.shape)\nif glbl['show_figs']:\n    plotPCA_explained(pca)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aa225b8dea8af2b395fcddadf45f59d01a6ff5f0"},"cell_type":"markdown","source":"This curve is often referred to as a scree plot. Notice that the explained variance decreases rapidly until the 6th component and then slowly, thereafter. The first few components explain a large fraction of the variance and therefore contain much of the explanatory information in the data. The components with small explained variance are unlikely to contain much explanatory information. Often the inflection point or 'knee' in the scree curve is used to choose the number of components selected.   \nNow it is time to create a PCA model with a reduced number of components. The code in the cell below trains and fits a PCA model with 6 components, and then transforms the features using that model."},{"metadata":{"_kg_hide-input":true,"_uuid":"13d1023bc83d7eec35425c80f6893ec2f05e004a","trusted":true},"cell_type":"code","source":"pca_6 = decomp.PCA(n_components = 6)\npca_6.fit(X_train2)\nX_trainPCA = pca_6.transform(X_train2)\nprint ('X_train2 shape:' ,X_trainPCA.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"61029d934350ce54683aba0a4e3085bbba4df923"},"cell_type":"markdown","source":"The data shape has been reduced from (712, 11) to (712,6).  \n\nNow lets rank features by importance in PCA space, and visualize the 1st and 2nd principle components."},{"metadata":{"_kg_hide-input":true,"_uuid":"0baf8867261883b9d80f0232cf7af343e7cb9bf4","trusted":true},"cell_type":"code","source":"def drawPCAVectors(transformed_features, components_, columns):\n    fig = plt.figure(figsize=(8,6))   # define (new) plot area\n    ax = fig.gca()   # define axis\n    plt.suptitle('Features in principle component space')\n    ax.set_xlabel('PC1')   # Set text for the x axis\n    ax.set_ylabel('PC2')   # Set text for y axis\n    num_columns = len(columns)\n    # This funtion will project your *original* feature (columns) onto your principal component feature-space, so that you can visualize how \"important\" each one was in the multi-dimensional scaling\n    # Scale the principal components by the max value in the transformed set belonging to that component\n    xvector = components_[0] * max(transformed_features[:,0])\n    yvector = components_[1] * max(transformed_features[:,1])\n    ## visualize projections\n    # Sort each column by it's length. These are your *original* columns, not the principal components.\n    important_features = { columns[i] : math.sqrt(xvector[i]**2 + yvector[i]**2) for i in range(num_columns) }\n    important_features = sorted(zip(important_features.values(), important_features.keys()), reverse=True)\n    print(\"Features by importance:\\n\", important_features)\n    ax = plt.axes()\n    for i in range(num_columns):\n        # Use an arrow to project each original feature as a labeled vector on your principal component axes\n        plt.arrow(0, 0, xvector[i], yvector[i], color='b', width=0.0005, head_width=0.02, alpha=0.75)\n        plt.text(xvector[i]*1.2, yvector[i]*1.2, list(columns)[i], color='b', alpha=0.75)\n    return ax\n\nif glbl['show_figs']: \n    drawPCAVectors(X_trainPCA, pca_6.components_, X_train2.columns)\n\n# clean workspace\ndel(X_trainPCA)\n\n# until now we used X_train2, which was a copy of X_train. X_train2 was used to illustrate thet transforms that we will be using in the pipeline. We did not want to alter X_train in any way, but now we can actually implement our transforms as part of our pipeline for real. We can therefore delete X_train2\ndel(X_train2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aa4f02d6747d4f4ccb0673ea5fabb051f6884d8c"},"cell_type":"markdown","source":"# Model selection: Pipeline\nIn our pipeline we want to include:\n* Feature Normalizer transform : QuantileTransformer\n* Feature Scaler transform  : standardScaler\n* Dimensionality reduction  : RFE or PCA\n* Estimator  : test multiple\n\nFirst, we will define a function that returns an estimator and its parameter distributions.  \nFor example, the following function requires only an input like `logit` and the y_train data to return a Logistic Regression classfier and parameter distributions that we can search over when we want to optimize it.  \n\nNext, we declare a function where we can specify which estimator we want to use. The function creates 2 full pipelines for this estimator, one using RFE, and one using PCA.  \n\nLets try this out!  \nWe will make a pipeline for logit model (using`idx = 0`):"},{"metadata":{"_kg_hide-input":true,"_uuid":"fc9d955ca1a4ab55f7b8c88d91a03dd8b074b230","trusted":true},"cell_type":"code","source":"def get_estimator(est, y_train):\n    #\n    ratio_classes =  pd.Series(y_train).value_counts(normalize=True)\n    #\n    if (est == 'LogisticRegression') | (est == 'logit'):\n        from sklearn.linear_model import LogisticRegression as estimator\n        parameter_dist = {\n                'penalty' : ['l2'], # 'penalty' : ['l1', 'l2'],     # default l2\n                'class_weight' : [{0:ratio_classes[0], 1:ratio_classes[1]}], #class_weight : dict or ‘balanced’, default: None\n                'C': ss.expon(scale=100), # must be a positive float\n        }\n    elif (est == 'KNeighborsClassifier') | (est == 'knc'):\n        from sklearn.neighbors import KNeighborsClassifier as estimator\n        parameter_dist = {\n                'n_neighbors' : ss.randint(1, 11),\n                # 'weights' : ['uniform', 'distance'],\n                # 'algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute'],\n        }\n    elif (est == 'LinearSVC') | (est == 'lsvc'):\n        from sklearn.svm import LinearSVC as estimator\n        parameter_dist = {\n                'C': ss.expon(scale=10),\n                # 'penalty' : ['l1', 'l2'],\n                # 'multi_class' : ['ovr', 'crammer_singer'],\n                'class_weight' : [{0:ratio_classes[0], 1:ratio_classes[1]}], #class_weight : dict or ‘balanced’\n        }\n    elif (est == 'SVC') | (est == 'svc'):\n        from sklearn.svm import SVC as estimator\n        parameter_dist = {\n                'C': ss.expon(scale=10),\n                'gamma' : ss.expon(scale=0.1), # float, optional (default=’auto’). If gamma is ‘auto’ then 1/n_features will be used instead.\n                # 'kernel' : ['rbf', 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'],\n                'class_weight' : [{0:ratio_classes[0], 1:ratio_classes[1]}], #class_weight : dict or ‘balanced’\n        }\n    elif (est == 'DecisionTreeClassifier') | (est == 'dtc'):\n        from sklearn.tree import DecisionTreeClassifier as estimator\n        parameter_dist = {\n                'criterion' : ['entropy'], # 'criterion': ['gini', 'entropy'],\n                # 'splitter' : ['best', 'random'],\n                # 'max_depth': [None, 3],'min_samples_split': ss.randint(2, 11),\n                'min_samples_leaf': ss.randint(1, 11),\n                'max_features': ss.uniform(0.0, 1.0), # we have to make this a float\n                'class_weight' : [{0:ratio_classes[0], 1:ratio_classes[1]}], #class_weight : dict or ‘balanced’\n        }\n    elif (est == 'RandomForestClassifier') | (est == 'rfc'):\n        from sklearn.ensemble import RandomForestClassifier as estimator\n        parameter_dist = {\n                # 'max_depth': [None, 3],\n                'n_estimators' : ss.randint(8, 20), # 'n_estimators' : integer, optional (default=10)\n                'criterion': ['gini', 'entropy'],\n                'max_features': ss.uniform(0.0, 1.0), # we have to make this a float\n                'min_samples_split': ss.randint(2, 11),\n                'min_samples_leaf': ss.randint(1, 11),\n                \"bootstrap\": [True, False],\n                'class_weight' : [{0:ratio_classes[0], 1:ratio_classes[1]}], #class_weight : dict or ‘balanced’\n        }\n    #\n    estimator=estimator()\n    \n    if 'random_state' in estimator.get_params():\n        estimator.set_params(random_state=glbl['random_state'])\n    #\n    return estimator, parameter_dist\n\n\n\ndef createPipes(y_train, idx=0):\n    \n    '''\n    idx specifies which estimator we want to use.\n    In this project we chose to demonstrate 6 classifiers:\n        Logistic Regression     idx=0\n        k-nearest neighbours    idx=1\n        linear SVM              idx=2\n        SVM                     idx=3\n        Decision Tree           idx=4\n        Random Forest           idx=5\n    \n    Outputs: \n        pipe1 { a pipeline using RFE for dimensionality reduction }\n        param_dist1 { parameter distributions of the components in pipe1 }\n        pipe2 { a pipeline using PCA for dimensionality reduction }\n        param_dist2 { parameter distributions of the components in pipe2 }\n    '''\n    \n    # the normalizer\n    norm = prep.QuantileTransformer(random_state=glbl['random_state'])\n    # the scaler\n    scaler=prep.StandardScaler()\n    # the classfier estimator as a function of input 'idx'\n    classifiers = ['logit', 'knc', 'lsvc', 'svc', 'dtc', 'rfc'] # let us test these classfiers\n    estimator, est_param_dist = get_estimator(classifiers[idx], y_train)\n    # dimensionality reduction methods\n    rfe = fs.RFE(estimator = estimator)\n    rfe_param_dist = {\n        'n_features_to_select': ss.randint(1,11),   # since we have 11 features\n    }\n    pca = decomp.PCA()\n    pca_param_dist = {\n        'n_components': ss.randint(1,10),   # since we have 11 features\n        'random_state' : [glbl['random_state']]\n    }\n    \n    #pipe1 uses RFE with estimator, pipe2 uses PCA with estimator\n    \n    pipe1 = pipe.Pipeline([\n            ('norm', norm),\n            ('scaler', scaler),\n            ('rfe', rfe),\n            ('est', estimator)\n    ])\n    \n    pipe2 = pipe.Pipeline([\n            ('norm', norm),\n            ('scaler', scaler),\n            ('pca', pca),\n            ('est', estimator)\n    ])\n    \n    pca_param_dist = {f'pca__{k}': v for k, v in pca_param_dist.items()}    # add 'pca__' string to all keys\n    rfe_param_dist = {f'rfe__{k}': v for k, v in rfe_param_dist.items()}    # add 'rfe__' string to all keys\n    est_param_dist = {f'est__{k}': v for k, v in est_param_dist.items()}    # add 'est__' string to all keys\n    # adding the transformer name in the parameter name is required for pipeline\n    \n    # merge dictionaries\n    param_dist1 = {**rfe_param_dist,  **est_param_dist}\n    param_dist2 = {**pca_param_dist,  **est_param_dist}\n    \n    return pipe1, param_dist1, pipe2, param_dist2\n\n\npipe1, param_dist1, pipe2, param_dist2 = createPipes(y_train, idx=0)  # create pipelines for logit model\n\nprint('pipe1\\n',pipe1)\nprint('estimator of pipe1:', pipe1.named_steps['est'])\nprint('------------------------------------------------')\nprint('pipe2\\n',pipe2)\nprint('estimator of pipe2:', pipe2.named_steps['est'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4be8eabdbf3c1e431e2f61da2e226a24b6066ef6"},"cell_type":"markdown","source":"We can see that our 2 pipelines were successfully created.\n\n# Model Selection\n\nEach pipeline has hyper-parameters that can be optimized. We need to decide how we will decide which hyper-parameter combination sets (aka models) to try. We also have to decide how we will obtain an accurate estimate of each model's performance (aka how will we validate performance). Lastly we need to specify what a good model performance means - this is the objective function\n\nWe need:\n* an objective function\n* a validation method, and\n* a hyper-parameter tuner\n\nOur objective function is simply the performance metric: 'Accuracy'  \nOur validation method shall be 'Nested CV'  \nOur hyper-parameter tuner shall be 'Random Search'\n\nNow let us declare a function that does all the above...  \n\nRecall that we created pipelines for the logit model. We called them `pipe1` and `pipe2`. Their parameter distributions were saved as `param_dist1` and `param_dist2`, respectively.  \nNow lets see how well `pipe1` does and do model selection:"},{"metadata":{"_kg_hide-input":true,"_uuid":"648d0ca44cd53357cc41ee2a20410192b39e0d26","trusted":true},"cell_type":"code","source":"def modelSel(pipeline, param_dist, X_train, y_train):\n    \n    '''\n    This function performs model selection given a pipeline and it's distribution\n    inputs:\n        pipeline { the pipeline to undergo model selection }\n        param_dist { the parameter distributions of the pipeline }\n    outputs:\n        inner { the model selection object. Contains parameters like best_estimator, best_params, best_index }\n        outer { the outer CV results }\n        this function also prints out the training score, as well as the testing score for the best estimator\n    '''\n    \n    # define the randomly sampled folds for the inner and outer Cross Validation loops:\n    insideCV = ms.KFold(n_splits=glbl['n_splits'], shuffle = True, random_state=glbl['random_state'])\n    outsideCV = ms.KFold(n_splits=glbl['n_splits'], shuffle = True, random_state=glbl['random_state'])\n    \n    ## Perform the Random search over the parameters\n    inner = ms.RandomizedSearchCV(estimator = pipeline,\n                                param_distributions = param_dist,\n                                n_iter=glbl['n_iter'], # Number of models that are tried\n                                cv = insideCV, # Use the inside folds\n                                scoring = 'accuracy',\n                                n_jobs=glbl['n_jobs'],\n                                return_train_score = True,\n                                random_state=glbl['random_state'])\n    # The cross validated random search object, 'inner', has been created.\n    \n    # Fit the cross validated grid search over the data \n    inner.fit(X_train, y_train)\n    # we have now scored each of the n_iter models (hyper-param combo) and we have an average score for each. we can use these scores as a model selection step, or we can feed these scores into an optimization algorithm. we wont use optim algo in this project, so we use best_estimator as our selected estimator.\n    \n    print('best accuracy on inner (train) set', inner.best_score_)\n    \n    # -------------------------------------------------\n    \n    # the inner loop evaluates model performance. we decided to let it do our model selection too.\n    # the estimate of the classifier is not reliable though. So we need to have an\n    # outer CV where we evaluate the 'best_estimator'\n    \n    outer = ms.cross_val_score(inner.best_estimator_, X_train, y_train, cv = outsideCV,\n                               n_jobs=glbl['n_jobs'])\n    \n    print('For outer (testing) set:')\n    #print('Outcomes by cv fold')\n    #for i, x in enumerate(outer):\n    #    print('Fold %2d    %4.3f' % (i+1, x))\n    print('Mean outer performance metric = %4.7f' % np.mean(outer))\n    print('stddev of the outer metric       = %4.7f' % np.std(outer))\n    #\n        \n    return inner, outer\n#\n\ninner1, outer1 = modelSel(pipe1, param_dist1, X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b0d091218e2702fa03f2a5940a1b16cba401bb36"},"cell_type":"markdown","source":"Lets see how well `pipe2` does and do model selection:"},{"metadata":{"_kg_hide-input":true,"_uuid":"712e9746c46525601f3d0b941f3a461248692f63","scrolled":false,"trusted":true},"cell_type":"code","source":"inner2, outer2 = modelSel(pipe2, param_dist2, X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"929356d41a1e184bec136c4cf9c3a159d77b6259"},"cell_type":"markdown","source":"`pipe2` achieved a better accuracy in the outer CV loop.  \nIt achived 82.4%, whereas `pipe1` achieved 82.0%.  \nWe select `inner2` and `outer2` as our best model.  \nRecall that `pipe1`and `pipe2`use RFE and PCA, respectively. So for Logistic Regression, PCA was the better step.  \n\nLets explore this winning pipeline and its best model a little ...  \n* We will look at 3 random models in the pipeline"},{"metadata":{"_kg_hide-input":true,"_uuid":"d691496d2961f940ae936233ea36be4c2b81416b","trusted":true},"cell_type":"code","source":"inner=inner2\nouter=outer2\n\n# clean worspace\ndel(pipe1, param_dist1, pipe2, param_dist2)\ndel(inner1, outer1, inner2, outer2)\n\n\n# the results of each model in the pipeline\ninner_results = pd.DataFrame( inner.cv_results_ )# the score for each model. there are n_iter models\n# look at 3 random models in the pipeline\ninner_results.sample(3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4091c6844f317889dcccb5e78161bd9ecb872d31"},"cell_type":"markdown","source":"* Print the parameter values of the best model"},{"metadata":{"_kg_hide-input":true,"_uuid":"ee22c4f620483781edc353fea54504f83dfe3d9e","trusted":true},"cell_type":"code","source":"# print the parameter values of the best model\nprint( inner.best_estimator_ )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6677d3a2f898b8eeef6b043521e2343d1d524c4d"},"cell_type":"markdown","source":"* Print parameters of the best model"},{"metadata":{"_kg_hide-input":true,"_uuid":"871f8e70373fa80463d80e60f3d3bc3a9c72ff99","scrolled":true,"trusted":true},"cell_type":"code","source":"# print parameters of the best model\nprint(inner.best_params_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"687453727fb39549349b48256bebf35607311e5a"},"cell_type":"markdown","source":"* 'Inner' contains many models. Lets print the index of the best model"},{"metadata":{"_kg_hide-input":true,"_uuid":"2364ad3f7be80f9e4649ee6bbc2d1029dac57ab6","trusted":true},"cell_type":"code","source":"# 'inner' contains many models. What is the index of the best model?\nprint( inner.best_index_ ) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dc8336dd20d9b640d5f53ed9fc04bca0e898c76f"},"cell_type":"markdown","source":"Great!  \nWe have a lot of data available to us all wrapped in 'inner' and 'outer'\n\n\n# Visualize\nNow we may ask ourselves:  \nWhat types of analysis /checks can I do with the scores that I get from the outer K folds?  \n3 checks that we can do that will provide us with insight:  \n* 1  LEARNING CURVE  \ncheck for stability of the predictions (use iterated/repeated cross-validation)\n{if we use a learning curve to do this, we can also see the effect of number of\ntraining samples on performance.}\n* 2 VALIDATION CURVE  \ncheck for the stability/variation of the optimized hyper-parameters.\nFor one thing, wildly scattering hyper-parameters may indicate that the inner optimization\ndidn't work. For another thing, this may allow you to decide on the hyperparameters without\nthe costly optimization step in similar situations in the future. With costly I do not refer\nto computational resources but to the fact that this \"costs\" information that may better be\nused for estimating the \"normal\" model parameters.\n* 3 INNER VS OUTER LOOP  \ncheck for the difference between the inner and outer estimate of the chosen model.\nIf there is a large difference (the inner being very overoptimistic), there is a risk that\nthe inner optimization didn't work well because of overfitting.  \n\nLets look at the simplest check first: check 3"},{"metadata":{"_kg_hide-input":true,"_uuid":"4237681968196b8a7a070c7a1e86213cf62e6c38","trusted":true},"cell_type":"code","source":"if glbl['show_figs']:\n    \n    # lets look at the simplest check first: check 3\n    innerTest_mean, innerTest_std = inner_results.loc[\n        inner.best_index_,['mean_test_score','std_test_score']]\n    outerTest_mean = np.mean(outer)\n    outerTest_std = np.std(outer)\n    \n    print('The inner CV mean and standard deviation are, respectively:')\n    print( innerTest_mean , innerTest_std)\n    print('The outer CV mean and standard deviation are, respectively:')\n    print( outerTest_mean , outerTest_std)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a8b9ecf00c63392bdaf3e80ae53e613a2f545403"},"cell_type":"markdown","source":"What can we summize?  \nThe model doesnt seem to be overoptimistic at all! The inner mean is not optimistic relative to the outer mean.  \nThe standard deviation of the outer is slightly larger inner CV, though. But this is not high enough to indicate some instability.  \nBased on these results we are not overfitting, and our estimate of instability is likely stable with a stddev of about 5%.  \n\n\nLets now look at Check 1: Plot learning curves"},{"metadata":{"_kg_hide-input":true,"_uuid":"6344090962d8cafdc806d1ddc357f5da1434d9d5","trusted":true},"cell_type":"code","source":"def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"\n    Taken from http://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html#sphx-glr-auto-examples-model-selection-plot-learning-curve-py\n    Generate a simple plot of the test and training learning curve.\n\n    Parameters\n    ----------\n    estimator : object type that implements the \"fit\" and \"predict\" methods\n        An object of that type which is cloned for each validation.\n\n    title : string\n        Title for the chart.\n\n    X : array-like, shape (n_samples, n_features)\n        Training vector, where n_samples is the number of samples and\n        n_features is the number of features.\n\n    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n        Target relative to X for classification or regression;\n        None for unsupervised learning.\n\n    ylim : tuple, shape (ymin, ymax), optional\n        Defines minimum and maximum yvalues plotted.\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n          - None, to use the default 3-fold cross-validation,\n          - integer, to specify the number of folds.\n          - An object to be used as a cross-validation generator.\n          - An iterable yielding train/test splits.\n\n        For integer/None inputs, if ``y`` is binary or multiclass,\n        :class:`StratifiedKFold` used. If the estimator is not a classifier\n        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validators that can be used here.\n\n    n_jobs : int or None, optional (default=None)\n        Number of jobs to run in parallel.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    train_sizes : array-like, shape (n_ticks,), dtype float or int\n        Relative or absolute numbers of training examples that will be used to\n        generate the learning curve. If the dtype is float, it is regarded as a\n        fraction of the maximum size of the training set (that is determined\n        by the selected validation method), i.e. it has to be within (0, 1].\n        Otherwise it is interpreted as absolute sizes of the training sets.\n        Note that for classification the number of samples usually have to\n        be big enough to contain at least one sample from each class.\n        (default: np.linspace(0.1, 1.0, 5))\n    \"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = ms.learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, random_state=glbl['random_state'],\n        train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n    \n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Inner CV (Training) score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Outer CV (Testing) score\")\n    \n    plt.legend(loc=\"best\")\n    return plt\n#\nCV = ms.KFold(n_splits=glbl['n_splits'], shuffle = True, random_state=glbl['random_state'])\nif glbl['show_figs']:\n    # check 1\n    # Plot learning curves\n    # Learning curves are a good way to see the overfitting effect on the training set and the effect of the training size on the accuracy.\n    \n    plot_learning_curve(inner.best_estimator_, \"logit learning curves\", X_train, \n                            y_train, cv=CV, n_jobs=glbl['n_jobs'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cbb0562015209519fa09e8635e87ad930bbdc274"},"cell_type":"markdown","source":"From this plot, we see that the red training curve is approaching the green testing curve with more training samples, and intersects at about 650 samples. This suggests that we had enough training samples to bring the inner training fold to be a good estimation of the testing score.  \nUp till 650 samples, the red (training score) line is far above the green (testing) line, so with lower samples we have overoptimistic overfitting training loop.  \nThe deviation fill around the red line is not very wide... this indicates that the training scores were quite stable.\nThe green line has a wide deviation. The standard deviations in the outer CV are large, so the cross validated scores are not stable.  \nThis check contradicts the previous check in some ways. This check suggests the stability estimate of the model is inaccurate. The model is unstable with it's prediction score.  \n\nNow let's look at check 2: Validation curve  \n\nWe will look at the effect of a hyper-parameter on the score.\n\nRecall that earlier we printed the parameters of the model using `print(inner.best_params_)`  \nThe output was  \n`{'est__C': 21.75031832298129, 'est__class_weight': {0: 0.6151685393258427, 1: 0.3848314606741573}, 'est__penalty': 'l2', 'pca__n_components': 9, 'pca__random_state': 5}`  \n    \nLets look at the effect of varying logit parameter `C`, and the effect of varying PCA parameter `n_components`  \n\nFirst we declare a function to plot the validation curve:\n\nLets first see the effect of 'C' on score"},{"metadata":{"_kg_hide-input":true,"_uuid":"d1d5982a06087a40464485bcab4545a528d2db27","trusted":true},"cell_type":"code","source":"def plot_validation_curve(estimator, X, y, param_name, param_range, cv=10, scoring=\"accuracy\",n_jobs=1):\n    plt.figure()\n    train_scores, test_scores = ms.validation_curve(\n        estimator, X, y, param_name=param_name, param_range=param_range,\n        cv=cv, scoring=scoring, n_jobs=n_jobs)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    \n    # the following is used to generate the title (objNmFull) of the figure\n    if (  str (type(estimator)) == \"<class 'sklearn.pipeline.Pipeline'>\"  ):\n        objNm = param_name.split('__')   # what is before the '__' ?\n        # which object in the pipeline are we considering?\n        objStr=str( estimator.named_steps[objNm[0]] )\n        # split at '(' and keep what is before\n        objNmFull = str( \"Validation Curve for Pipeline for \" + objStr.split('(')[0] )\n        objNm = objNm[1]\n    else:\n        objNm = param_name\n        objStr=str( estimator )\n        objNmFull = str( \"Validation Curve for \" + objStr.split('(')[0] )\n    objNmFull = objNmFull + ' for parameter ' + objNm\n    \n    plt.title(objNmFull)        # make title\n    plt.xlabel(param_name)\n    plt.ylabel(\"Score\")\n    plt.ylim(0.0, 1.1)\n    lw = 2\n    plt.semilogx(param_range, train_scores_mean, label=\"Inner CV (Training) score\",\n                 color=\"darkorange\", lw=lw)\n    plt.fill_between(param_range, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.2,\n                     color=\"darkorange\", lw=lw)\n    plt.semilogx(param_range, test_scores_mean, label=\"Outer CV (Testing) score\",\n                 color=\"navy\", lw=lw)\n    plt.fill_between(param_range, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.2,\n                     color=\"navy\", lw=lw)\n    plt.legend(loc=\"best\")\n    plt.show()\n#\n\n# lets first see the effect of 'C' on score\nCV = ms.KFold(n_splits=glbl['n_splits'], shuffle = True, random_state=glbl['random_state'])\nplot_validation_curve(inner.best_estimator_, X_train, y_train,\n                      'est__C', np.logspace(-1, 5, 20), cv=CV,\n                      scoring=\"accuracy\", n_jobs=glbl['n_jobs'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c6497356986973ab9489c08e6c82e6597a38e728"},"cell_type":"markdown","source":"Parameter `C` has virtually no effect at all on score.  \nNow lets see the effect of n_components on score"},{"metadata":{"_kg_hide-input":true,"_uuid":"f88ba2384161b278372410d959d5213584662f14","trusted":true},"cell_type":"code","source":"# now lets see the effect of n_components on score\nplot_validation_curve(inner.best_estimator_, X_train, y_train,\n                      'pca__n_components',\n                      np.linspace(1, 10, 10, dtype = int),\n                      cv=CV, scoring=\"accuracy\",n_jobs=glbl['n_jobs'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f337fbc17d4faa873aa1493983cc7f7630637f60"},"cell_type":"markdown","source":"We see significant variation in performance for `n_components`.  \n\nWhat value does this add to our investigation?  \nFrom the validation curves, we can summize that next time we dont really need to tune `C`. but we have to tune `n_components`. we also observe that the hyper-parameters do not scatter wildly, the curve is smooth, so the inner CV provided a good estimation of hyper-parameter effects.  \n\n\nSo for the logit (Logistic Regression) estimator, we have created and compared pipelines 1 and 2, and we have explored the best model of `pipe2`.  \nLets save the best logit-based model as `best_logit`:"},{"metadata":{"_kg_hide-input":true,"_uuid":"5ce186e8b365d94a9e2b6e8fe8d4f7e708ce7040","trusted":true},"cell_type":"code","source":"best_logit = inner.best_estimator_\n\n# clean workspace\ndel(inner, outer)\n\nprint('best_logit\\n', best_logit)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"58238326fa9051e18bc4508a055e26b06c7f4bd6"},"cell_type":"markdown","source":"Now lets evaluate pipelines with other estimators:\n\n### knc (k-nearest neighbours classifier):\nNote that `pipe1` fails because RFE wont work with knc since the knc does not expose \"coef_\" or \"feature_importances_\" attributes."},{"metadata":{"_kg_hide-input":true,"_uuid":"a0790518f669c33bf4cb726324f0acb27aaaf576","trusted":true},"cell_type":"code","source":"pipe1, param_dist1, pipe2, param_dist2 = createPipes(y_train, idx=1)  # create pipelines for knc model\n\n# inner1, outer1 = modelSel(pipe1, param_dist1, X_train, y_train)\n# this fails, because RFE wont work with knc:\n# RuntimeError: The classifier does not expose \"coef_\" or \"feature_importances_\" attributes\n\nprint('pipe2')\ninner2, outer2 = modelSel(pipe2, param_dist2, X_train, y_train)\n# lets name rename inner2 as:\nbest_knc = inner2.best_estimator_\n\nprint('\\nbest pipeline saved.')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5cc744fed18ba31b7d571f8eb88fb599ebe33d38"},"cell_type":"markdown","source":"### lsvc (linear support vector classifier):"},{"metadata":{"_kg_hide-input":true,"_uuid":"9298714f9ee15fc2fd31c520e7b4f53d3cf4456b","scrolled":false,"trusted":true},"cell_type":"code","source":"pipe1, param_dist1, pipe2, param_dist2 = createPipes(y_train, idx=2)  # create pipelines for lsvc model\nprint('pipe1')\ninner1, outer1 = modelSel(pipe1, param_dist1, X_train, y_train)\nprint('pipe2')\ninner2, outer2 = modelSel(pipe2, param_dist2, X_train, y_train)\n\n# lets name rename inner1 as:\nbest_lsvc = inner1.best_estimator_\nprint('\\nbest pipeline saved.')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"67adec011fdf969bc380c7ca4aa89928ab8364ec"},"cell_type":"markdown","source":"### svc (support vector classifier):\nNote that `pipe1` fails because RFE wont work with svc since the svc does not expose \"coef_\" or \"feature_importances_\" attributes."},{"metadata":{"_kg_hide-input":true,"_uuid":"76195992ed5370b43bf378fc3d01fa88f396bcbe","trusted":true},"cell_type":"code","source":"pipe1, param_dist1, pipe2, param_dist2 = createPipes(y_train, idx=3)  # create pipelines for svc model\n\n#inner1, outer1 = modelSel(pipe1, param_dist1, X_train, y_train)\n# this fails, because RFE wont work with svc:\n# RuntimeError: The classifier does not expose \"coef_\" or \"feature_importances_\" attributes\n\nprint('pipe2')\ninner2, outer2 = modelSel(pipe2, param_dist2, X_train, y_train)\n\n# lets name rename inner2 as:\nbest_svc = inner2.best_estimator_\nprint('\\nbest pipeline saved.')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dd4c1eea33e6b77fded59724383fff86c9a99786"},"cell_type":"markdown","source":"### dtc (decision tree classifier):"},{"metadata":{"_kg_hide-input":true,"_uuid":"681ac880043ec997a847bb412159716c07efbad1","scrolled":false,"trusted":true},"cell_type":"code","source":"pipe1, param_dist1, pipe2, param_dist2 = createPipes(y_train, idx=4)  # create pipelines for dtc model\nprint('pipe1')\ninner1, outer1 = modelSel(pipe1, param_dist1, X_train, y_train)\nprint('pipe2')\ninner2, outer2 = modelSel(pipe2, param_dist2, X_train, y_train)\n# lets name rename inner1 as:\nbest_dtc = inner1.best_estimator_\nprint('\\nbest pipeline saved.')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3d27d56a1d4c305c4aa57f664e1694e73ec9e54b"},"cell_type":"markdown","source":"### rfc (random forest classifier):"},{"metadata":{"_kg_hide-input":true,"_uuid":"fb1e58089b0c3b9955d344028ca486e2e3ac1529","trusted":true},"cell_type":"code","source":"pipe1, param_dist1, pipe2, param_dist2 = createPipes(y_train, idx=5)  # create pipelines for rfc model\n\nprint('pipe1')\ninner1, outer1 = modelSel(pipe1, param_dist1, X_train, y_train)\nprint('pipe2')\ninner2, outer2 = modelSel(pipe2, param_dist2, X_train, y_train)\n\n# lets name rename inner1 as:\nbest_rfc = inner1.best_estimator_\nprint('\\nbest pipeline saved.')\n\n# clean workspace\ndel(pipe1, param_dist1, pipe2, param_dist2)\ndel(inner1,outer1,inner2,outer2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fe9f5c0e2efe0870773881ad5f07868799ea609c"},"cell_type":"markdown","source":"# Model blending\nNext we can do model mixing / model blending  \nWe have many models now. we can see if combining them could perhaps produce an even stronger classifier!  \n\nWe will be using 2 'Voting Classifiers'\n*     one with Hard Vote or majority rules   (votingC_hard aka VCH)"},{"metadata":{"_kg_hide-input":true,"_uuid":"3f59eebb02e0efb3c3e0b8353ae9f6cd4b1cceda","trusted":true},"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\n\n# hard voting classifier\nvotingC_hard = VotingClassifier(\n        estimators=[('logit', best_logit), ('knc', best_knc), \n                    ('lsvc', best_lsvc), ('svc', best_svc), \n                    ('dtc',best_dtc), ('rfc',best_rfc)],\n        voting='hard', n_jobs=glbl['n_jobs'])\n#\nCV = ms.KFold(n_splits=glbl['n_splits'], shuffle = True, random_state=glbl['random_state'])\ncv_estimate2 = ms.cross_val_score(votingC_hard, X_train, y_train, cv = CV, n_jobs=glbl['n_jobs'])\nprint('Outcomes by cv fold')\nfor i, x in enumerate(cv_estimate2):\n    print('Fold %2d    %4.3f' % (i+1, x))\nprint('Mean performance metric = %4.3f' % np.mean(cv_estimate2))\nprint('stddev of the metric       = %4.3f' % np.std(cv_estimate2))\n#","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f8e8d50eca606aab418db5ba4fd290664cc15af"},"cell_type":"markdown","source":"*     one with Soft Vote or weighted probabilities   (votingC_soft aka VCS)"},{"metadata":{"_kg_hide-input":true,"_uuid":"85432c8d893bfcd119fdcf52c4f1152d893bba4e","trusted":true},"cell_type":"code","source":"# soft voting classifier\nvotingC_soft = VotingClassifier(\n        estimators=[('logit', best_logit), ('knc', best_knc),\n                    #('lsvc', best_lsvc),    # AttributeError: 'LinearSVC' object has no attribute 'predict_proba'. we need to take this out.\n                    ('svc', best_svc),      # predict_proba is not available when  probability=False   --- we will set this soon ...\n                    ('dtc',best_dtc), ('rfc',best_rfc)],\n        voting='soft', n_jobs=glbl['n_jobs'])\n#\n\n# for soft vote we need probabilities. the SVC can be set to have probabilities...\n# probability : boolean, optional (default=False)\n#    Whether to enable probability estimates. This must be enabled prior to calling fit, and will slow down that method.\n# lets see the parameters i can set in VotingC_soft:\nvotingC_soft.get_params().keys()\n# we need to set that 'svc__est__probability' to True\nvotingC_soft.set_params(svc__est__probability=True)\n\nCV = ms.KFold(n_splits=glbl['n_splits'], shuffle = True, random_state=glbl['random_state'])\ncv_estimate3 = ms.cross_val_score(votingC_soft, X_train, y_train, cv = CV)\nprint('Outcomes by cv fold')\nfor i, x in enumerate(cv_estimate3):\n    print('Fold %2d    %4.3f' % (i+1, x))\nprint('Mean performance metric = %4.3f' % np.mean(cv_estimate3))\nprint('stddev of the metric       = %4.3f' % np.std(cv_estimate3))\n#\n\ndel(cv_estimate2, cv_estimate3, i, x)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a0d03da8716b1e94c8bae882cb2c99860a637cd7"},"cell_type":"markdown","source":"# Evaluation on test set\n\nUntil now we have only used training set `X_train`.  \nLet us now evaluate each model on the entirely unseen `X_test`.   \n\nFirst, we fit each model with ENTIRE train-set...  \nNow lets score each model on the test set  \nThis is the best estimate of performance we have, without actually submitting to Kaggle   \n\nLets visualize the performance of each model:  \nWe create a column chart of the scores for each estimator used"},{"metadata":{"_kg_hide-input":true,"_uuid":"2c4d73bf7b36786bfdda7ce798116f6cd2cacc1b","trusted":true},"cell_type":"code","source":"# First, we fit each model with ENTIRE train-set.\nbest_logit.fit(X_train, y_train)\nbest_knc.fit(X_train, y_train)\nbest_lsvc.fit(X_train, y_train)\nbest_svc.fit(X_train, y_train)\nbest_dtc.fit(X_train, y_train)\nbest_rfc.fit(X_train, y_train)\nvotingC_hard.fit(X_train, y_train)\nvotingC_soft.fit(X_train, y_train)\n\n\n# now lets score each model on the test set\n\nscores=[]\nclassifiers=pd.DataFrame( ['logit', 'knc', 'lsvc', 'svc', 'dtc', 'rfc', 'vch', 'vcs'] )\nclassifiers.columns = ['classifier']\n\nscores.append( best_logit.score(X_test, y_test) )\nscores.append( best_knc.score(X_test, y_test) )\nscores.append( best_lsvc.score(X_test, y_test) )\nscores.append( best_svc.score(X_test, y_test) ) \nscores.append( best_dtc.score(X_test, y_test) ) \nscores.append( best_rfc.score(X_test, y_test) )\nscores.append( votingC_hard.score(X_test, y_test) ) \nscores.append( votingC_soft.score(X_test, y_test) )\n\nscores = pd.DataFrame( scores )\nscores.columns = ['score']\n\nscoresdf = pd.concat([classifiers, scores], axis = 1)   # concatenate columns\n# clean workspace\ndel(classifiers, scores)\n\n\n# Lets sort the `scoresdf` dataframe\nscoresdf.sort_values(by='score', inplace=True)\nscoresdf.score = (scoresdf.score*100000).astype(int)/1000   # give each 3 decimal points\n\n\n\n# we create a column chart of the scores for each estimator used\nfig = plt.figure(figsize=(8,6))   # define (new) plot area\nax = fig.gca()   # define axis\nsns.barplot(x = 'score', y='classifier', data = scoresdf, palette=\"Blues_d\", ax=ax)\nplt.title('Accuracy Score on the test-set by different classifiers \\n')\nplt.xlabel('Accuracy Score (%)')\nplt.ylabel('Classifier')\n\nfor i, v in enumerate( scoresdf.score ):  # give each 3 decimal points\n    ax.text(v + 0.5, i + .25, str(v), color='black', fontweight='bold')\n# clean workspace\ndel(i, v, scoresdf)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6ea5727861d637d925355c0d56afdb0c502b7432"},"cell_type":"markdown","source":"From this figure we see that our best classfier was the voting classifier with soft voting. From the non-blended models, the pipeline using the Decision Tree Classifier was the strongest model.\n \n # Feature importances\nIt is convenient that the Decision Tree Classifier scored so well. The DTC is a highly useful in that it ranks features on importance.  \nSuppose we wanted to know the relative importance of each feature. The DTC and RFC are highly useful in that it ranks features on importance. Lets consider the DTC:  \nRecall that we use RFE with our DTC. This means that the starting features are 'intact' -  they are not in PCA space. Some of them are, however, excluded from the DTC."},{"metadata":{"_kg_hide-input":true,"_uuid":"01402c303917fc0dd73d86e020b96087fbc9929c","trusted":true},"cell_type":"code","source":"# lets collect the names of the oringinal features\nfeatureRanks = pd.DataFrame(X_train.columns)\nfeatureRanks.columns=['feature']\n# first we need to figure out which features were deleted by the RFE\nfeatureRanks['support'] = best_dtc.named_steps['rfe'].support_\nfeatureRanks['ranking'] = best_dtc.named_steps['rfe'].ranking_\n\n# lets look at the feature importance ranking as per the RFC\nfeatureRanks['importance'] = 0     # initialize\n# now set the importance of features that were included\nfeatureRanks.loc[featureRanks.support==True,'importance'] = best_dtc.named_steps['est'].feature_importances_\n\n# lets sort by feature importances\nfeatureRanks.sort_values(by='importance', inplace=True)\nfeatureRanks.importance = (featureRanks.importance*100000).astype(int)/1000   # give each 3 decimal points\n\n# we create a column chart of the importances for each feature\nfig = plt.figure(figsize=(8,6))   # define (new) plot area\nax = fig.gca()   # define axis\nsns.barplot(x = 'importance', y='feature', data = featureRanks, palette=\"Blues_d\", ax=ax)\nplt.title('Importance as per Decision Tree Classifier with RFE \\n')\nplt.xlabel('Importance [%]')\nplt.ylabel('Feature')\n\nfor i, v in enumerate( featureRanks.importance ):  # give each 3 decimal points\n    ax.text(v + 0.5, i + .25, str(v), color='black', fontweight='bold')\n\ndel(i,v, featureRanks)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"670456cd78d16c352c640e0dddeeb22a21196421"},"cell_type":"markdown","source":"We will discuss the feature importance ranking soon...\n\n# Visualization\n\nIf we think about a potential product for this project, we may suggest a flow-chart that accurately summarizes the probability of a given passenger to survive. The classifier that is designed for such transparency and wonderful interpretability is the DTC (Decision Tree Classifier). Lets biuld a Decision Tree Graph to showcase the decision-making process behind the Decision Tree Classifier:"},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"_uuid":"674b403249b9e163ae6762d9a638a5e22c847e67","trusted":true},"cell_type":"code","source":"# we will need to name the features used by the DTC\n\nfeatureRanks2 = pd.DataFrame(X_train.columns)\nfeatureRanks2.columns=['feature']\n# first we need to figure out which features were deleted by the RFE\nfeatureRanks2['support'] = best_dtc.named_steps['rfe'].support_\nfeatureRanks2['ranking'] = best_dtc.named_steps['rfe'].ranking_\n# now isolate feature names taht were used\nnames = featureRanks2.loc[featureRanks2.support==True,['feature']]\nnames = list(names.feature)\n\nfrom sklearn.base import clone\ndtc = clone ( best_dtc.named_steps['est'] )     # copies estimator wihtout pointing to it\n\ndtc.fit(X_train.loc[:,names], y_train)   # refit the classifier with included data only\n# note that DTC does not need transforms on X_train, so we can use it directly\n\nimport graphviz \n\n# Create DOT data\ndot_data = skl.tree.export_graphviz(dtc, out_file=None,\n                                    max_depth = None,\n                                    feature_names = names,\n                                    class_names = True,     # in ascending numerical order\n                                    impurity=False,\n                                    proportion=True,\n                                    filled = True, rounded = True)\n#\n\n# Draw graph\ngraph = graphviz.Source(dot_data) \n# show graph\ngraph\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1e4a6c6a8c334cadfd922cdc90ad2d88b7e1feb7"},"cell_type":"markdown","source":"How to interpret:  \nFor example consider the first node:\n*  the conditional tells us where to go next\n*  the 'samples' indicate how many samples we have narrowed our search down to. Since this is the first node, we will always have samples=100%\n*  'value' indicate the proportion of the target taht have reached this node. so in this case Survived 0 and 1. In node 1 the proportion is 0.704 and 0.296. this means 70.4% of all passengers died. The ratio in 'value' sets the color intensity of the node.\n*  'class' indicate that at this node, what is the most likely result. in node 1 it is y[0]: Death. This can be derived from 'value'.\n*  if the condition is True, then go left. If the condition is False, then go right.\n\n\nWhat does the graph above tell us?  \nConsider the most important feature: `Title_Mr`.  \n\"Women and children first\" is a code of conduct, whereby the lives of women and children were to be saved first in a life-threatening situation, typically abandoning ship, when survival resources such as lifeboats were limited.  \nThis saying was in-fact made famous by the 1997 film 'Titanic'. From our examination, it seems that this saying had quite a lot of weight in reality!  \nThe most important factor in the survival of a given passenger was whether their title (barring royal, academic, and naval titles) was \"Mr\". We can clearly see from the Decision Tree above that if you were a 'Mr' (go right on the first node) then your prospects are dim at best... the colour of the right half of the Decision Tree is red compared to the left half.\n\nThe 2 next most important features are`Fare` and `Pclass`. For the majority of third-class passengers, the overwhelming problem they faced after the collision was navigating the labyrinth of passageways, staircases and public rooms to get up to the boat deck, which was mostly in the first-class area of the ship. There was no single staircase leading all the way up through the six to eight decks they would have to traverse to get near the lifeboats, and no handy maps of the ship they could use. Even the staff had trouble finding their way around.  \nThere is a myth that 3rd class passengers were even locked out of the boat deck - a fabrication by 1997's Titanic - however 3rd class passengers who survived declared these rumours to be false."},{"metadata":{"_uuid":"e0f8f6ef6de4cd728818c5fec0e1ea3c4c2d0ac8"},"cell_type":"markdown","source":"# Kaggle submission\nOur best model was the Voting Classifier with Soft voting (VCS). We will submit these results to Kaggle to score our method.  \nLets take a sample to see what our data looks  like"},{"metadata":{"_kg_hide-input":true,"_uuid":"0fb19122a37afc6bfcbd1a3d8aeac21bcbb48c14","trusted":true},"cell_type":"code","source":"# First, clean workspace\ndel(featureRanks2, dot_data, names)\n\n# we dont need the train-set or test-set data at all anymore\ndel(X_train, y_train, X_test, y_test)\n\n\n# we now use the testing data from Kaggle, which we have named 'X2'\n\ny2 = data2.Survived\nX2 = data2.drop(columns=['Survived'])\ndel(data2)\n\n# we use our best classifier: the VCS\ny_pred = votingC_soft.predict(X2)     # make the predictions based on X2\n\n# now lets make the submission\n\nsubmit = pd.DataFrame(y2)  # just quickly create a new dataFrame called submit\nsubmit['PassengerId'] = X2.index\nsubmit['Survived'] = y_pred\n\n# lets reorder our columns... just to make it look nice\nsubmit = submit[['PassengerId','Survived']]\n\n# lets take a look at what our data looks  like\nsubmit.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b04843f08801b63366d1a16d8a1e54b44a014953"},"cell_type":"markdown","source":"Recall that survived needs to be numeric 0 or 1  \nLets fix that and look at the data again:"},{"metadata":{"_kg_hide-input":true,"_uuid":"8120f8cd1d00b1b586e723401278b83a44719a0d","trusted":true},"cell_type":"code","source":"submit['Survived'] = submit.Survived.astype('uint8')\n# lets take a look at what our data looks  like\nsubmit.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e02ee3b405aff068a69880cdbd098317da8b5fd5"},"cell_type":"markdown","source":"Looks correct now   \n\nFinally we can output the results. The results will be saved under **Output**  \nScroll to the top of this notebook to see the headings:\nNotebook, Code, Data, **Output**, Comments, Log, Versions, Fork"},{"metadata":{"_kg_hide-input":true,"_uuid":"d50946007cffe159a7b8adfdd149addfbc594d9a","trusted":true},"cell_type":"code","source":"#submit file\nsubmit.to_csv(\"../working/submit.csv\", index=False)\n\nprint(\"Submitted to 'Output'\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cc3451534f207f5786f088a575c6bc9ff4cb11b8"},"cell_type":"markdown","source":"# Final comments\nMy intention with this project was to *tell a story using data*.  \nI hope that this kernel was insightful and enjoyable!  \n\nI have some 'TODO's in mind:\n* Implement parallel computing\n* Try different Dimensionality Reduction methods. For example: isomap\n* Try different classifiers. Gradient Boosting methods are popular on Kaggle, especially the XGB implementation.\n* I am currently only performing `n_iter` = 100 iterations in my cross-validated search. i would like to extend this to be more thorough.\n* Right now I am using RandomizedSearch. Instead, I would love to implement an optimization algorithm like Bayesian optimization / Gradient-based optimization / Evolutionary algorithm (Genetic algorithm). This would have the benefit of intelligently guiding the hyper-parameter search and implemting a search-stop criterion.\n* The `weights` parameter in the Voting Classifiers can be set. In fact, I wish to optimize the weights. This will exclude classfiers that hurt the Voting Classfier, and promote those that contribute more to its predictive power.\n\nI welcome comments and suggestions for improvement!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}