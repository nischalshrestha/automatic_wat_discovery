{"metadata": {"kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}, "language_info": {"nbconvert_exporter": "python", "codemirror_mode": {"name": "ipython", "version": 3}, "name": "python", "version": "3.6.3", "file_extension": ".py", "pygments_lexer": "ipython3", "mimetype": "text/x-python"}}, "nbformat_minor": 1, "nbformat": 4, "cells": [{"metadata": {"_uuid": "dabc9d95d508e7126c5d20b614a0528b775ca3b1", "_cell_guid": "94f68714-eadd-4225-83a6-591c3dfcf4e8"}, "cell_type": "markdown", "source": []}, {"metadata": {"_uuid": "f2dc9cbe8e6f1ebb9cd9ec702082e407b953aa6c", "collapsed": true, "_cell_guid": "5a85b65b-66a9-4042-b6a3-7fcbbb12bca5"}, "cell_type": "markdown", "source": ["**Justification **\n", "**Titanic Rescue through Random Forest**\n", "\n", "It seems like a paradox to imagine rescue from Titanic through Random Forest but as they say, anything is possible in the world of Machine Learning. Today we are going to see the usage of Random Forest classification algorithm while predicting survial of the Titanic passangers\n", "\n", "Radom Forest is a versatile Machine learning algorithm which can handle both numerical and categorical features nicely. You can leverage Random Forest when it comes to classification problem which has combination of Numeric and Categorical features\n", "\n", "Looking at the Titanic data, Random forest fits well. Data set is not a pure numeric in nature and it seems more aproprioate to avoid probability based classification algorithm like Naive Bayes, Logistic Regression as well as distance based algorithms like SVM and KNN for this problem.\n", "\n", "It looks like overall training data set is driven by some features which can be approached by asking precise questions and narrow down the results, just the way random forest works\n", "\n", "With all this background we are all set to see Randome Forest algorithm in action to solve this problem.\n", "\n"]}, {"metadata": {"_uuid": "ba64059f73ce22ce5404a012968673abed04b9b7", "_cell_guid": "05ca2fb9-db1f-4ce8-9963-7729c1fa1618"}, "cell_type": "markdown", "source": ["**Overall Solutioning**\n", "\n", "*While solving this problem, we will take following approach whihc can be genealized to any machine learning problem \n", "1. Feature Engineering\n", "2. Data Cleanup\n", "3. Data Visualization\n", "4. Finding weights of the feature\n", "5. Best Features Selection\n", "6. Training Model\n", "7. Judge model accuracy\n", "8. Predict Output*\n", "\n", "\n"]}, {"metadata": {"_uuid": "8dbfb50d164c2c54acef1a944eb435c0c4e84752", "_cell_guid": "79fc7253-c74b-46ff-9e9a-63df8945a58b"}, "cell_type": "markdown", "source": ["**1. Feature Engineering**"]}, {"metadata": {"_uuid": "9180d6c3bbe2a8933947557f59519c61fe17d936", "scrolled": true, "_cell_guid": "a01d6d09-f35d-4e80-b6d4-30879089c91a"}, "execution_count": null, "cell_type": "code", "source": ["# Imports\n", "\n", "# pandas\n", "import pandas as pd\n", "from pandas import Series,DataFrame\n", "\n", "# numpy, matplotlib, seaborn\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "sns.set_style('whitegrid')\n", "%matplotlib inline\n", "\n", "# machine learning\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.svm import SVC, LinearSVC\n", "from sklearn.ensemble import RandomForestClassifier\n", "from sklearn.neighbors import KNeighborsClassifier\n", "from sklearn.naive_bayes import GaussianNB\n", "from sklearn.ensemble import ExtraTreesClassifier\n", "from sklearn.model_selection import train_test_split\n", "from sklearn import metrics\n", "from sklearn.metrics import precision_recall_fscore_support\n", "\n", "# get titanic & test csv files as a DataFrame\n", "titanic_df = pd.read_csv(\"../input/train.csv\")\n", "test_df    = pd.read_csv(\"../input/test.csv\")\n", "\n", "# Get the info of the data frame\n", "titanic_df.info()\n", "# There are 12 features and 891 rows\n", "# Null Columns - Age, Cabin, Embarked\n", "\n", "# Add one more column called Salutation\n", "conditions = [\n", "    (titanic_df['Name'].str.contains('Master')),\n", "    (titanic_df['Name'].str.contains('Mrs.')),\n", "    (titanic_df['Name'].str.contains('Mr.')),\n", "    (titanic_df['Name'].str.contains('Miss'))]\n", "choices = ['Master', 'Mrs', 'Mr', 'Miss']\n", "titanic_df[\"Salutation\"] = np.select(conditions, choices, default='')\n", "\n", "means = titanic_df.groupby('Salutation').mean()\n", "print(means[\"Age\"][\"Master\"])\n", "titanic_df.loc[titanic_df.Salutation == 'Master','Age'] = titanic_df['Age'].fillna(means[\"Age\"][\"Master\"]) \n", "titanic_df.loc[titanic_df.Salutation == 'Mrs','Age'] = titanic_df['Age'].fillna(means[\"Age\"][\"Mrs\"]) \n", "titanic_df.loc[titanic_df.Salutation == 'Mr','Age'] = titanic_df['Age'].fillna(means[\"Age\"][\"Mr\"]) \n", "titanic_df.loc[titanic_df.Salutation == 'Miss','Age'] = titanic_df['Age'].fillna(means[\"Age\"][\"Miss\"]) \n", "\n", "# Change Sex to identify kids/teens info\n", "titanic_df.loc[titanic_df.Salutation == 'Master', 'Sex'] = 'master'\n", "titanic_df.loc[titanic_df.Salutation == 'Miss', 'Sex'] = 'miss'\n", "\n", "# Drop Name and Ticket columns\n", "titanic_df = titanic_df.drop(['Name','Ticket','Salutation'], axis=1)\n", "\n", "\n", "# Fill NA values of Age Cabin and Embarked\n", "#1] Calc kids average age\n", "\n", "titanic_df['Age'].fillna(titanic_df['Age'].mean(), inplace=True)\n", "titanic_df['Embarked'].fillna('B' , inplace=True)\n", "titanic_df['Cabin'].fillna('Z' , inplace=True)\n", "\n", "# Take only first letter of Cabin \n", "titanic_df['Cabin'] = titanic_df['Cabin'].astype(str).str[0]\n", "\n", "# Convert Cabin, Embarked and  Sex into Numeric\n", "titanic_df['Sex'].replace(['female','male','master','miss'], [0,1,2,3],inplace=True)\n", "titanic_df['Cabin'].replace(['Z','A', 'B', 'C', 'D', 'E', 'F', 'G', 'T'], [0,1,2,3,4,5,6,7,8],inplace=True)\n", "titanic_df['Embarked'].replace(['B','S','C','Q'], [0,1,2,3],inplace=True)\n", "\n", "# Change Age to float\n", "titanic_df['Age']  =  titanic_df['Age'].astype(float)\n", "#Check the clenaed dataframe\n", "titanic_df.info()\n", "titanic_df.head()\n"], "outputs": []}, {"metadata": {"_kg_hide-input": true, "_uuid": "7945795495eee2b1fd2f20d39b3952e15b83af15", "_cell_guid": "4e1ec511-d5ae-4d70-9171-1222d8f44fb8"}, "cell_type": "markdown", "source": ["Since we have cleaned up the train data, we can access the features weights using Extra Tree Classifier. Please note that this is just one of the methods. There are other feature selection techniques  like SelectKBestFeatures or RecursiveFeatureEllimination. Let's get rid of PassengerId and Parch. Based on weight analysis below Parch feature is not contributing in a lrge extent"]}, {"metadata": {"_uuid": "cfc2e8f1272a38b3d5e71373d2342f828800fe1a", "_cell_guid": "7354ce90-4d56-4e8b-8ed9-8d3c830a9eb0"}, "execution_count": null, "cell_type": "code", "source": ["# define training and testing sets\n", "X = titanic_df.drop([\"Survived\", \"PassengerId\"],axis=1)\n", "y = titanic_df[\"Survived\"]\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n", "\n", "#X = (X - X.mean()) / (X.max() - X.min())\n", "# Build a forest and compute the feature importances\n", "forest = ExtraTreesClassifier()\n", "forest.fit(X, y)\n", "\n", "importances = forest.feature_importances_\n", "std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n", "             axis=0)\n", "indices = np.argsort(importances)[::-1]\n", "\n", "# Print the feature ranking\n", "print(\"Feature ranking:\")\n", "\n", "for f in range(X.shape[1]):\n", "    print(\"feature %d. %s : (%f)\" % (indices[f],X.columns.values[indices[f]], importances[indices[f]]))\n", "\n", "# Plot the feature importances of the forest\n", "plt.figure()\n", "plt.title(\"Feature importances\")\n", "plt.bar(range(X.shape[1]), importances[indices],\n", "       color=\"g\", yerr=std[indices], align=\"center\")\n", "plt.xticks(range(X.shape[1]), indices)\n", "plt.xlim([-1, X.shape[1]])\n", "plt.show()\n", "\n", "# Note that features lile Sex, Fare and Age are very important followed by Pclass, cabin, parch, sibsp and Embarked"], "outputs": []}, {"metadata": {"_uuid": "7e914af4c1fdf776ba596c59468a2e481e99415c", "_cell_guid": "2e6e8cec-bbff-45f8-aff4-dbcb73a1c6e1"}, "cell_type": "markdown", "source": ["Let's validate these weights through data visualization\n", "\n", "1. SwarmPlot for Passengers with different genders clearly shows that female passengers have higher chance of survial compared to male passengers"]}, {"metadata": {"_uuid": "b8037a293fe4611f87543bb50a9843a5ae7a2f78", "collapsed": true, "_cell_guid": "f99a09b7-d3f9-4f14-8781-35655c95a132"}, "execution_count": null, "cell_type": "code", "source": [], "outputs": []}, {"metadata": {"_uuid": "54d4729763199e3aa80ca520157905fbce265a32", "scrolled": true, "_cell_guid": "f1565114-8230-4581-942c-dcc110814d11"}, "execution_count": null, "cell_type": "code", "source": ["sns.swarmplot(x='Sex', y='PassengerId',data=titanic_df, hue='Survived')\n", "\n"], "outputs": []}, {"metadata": {"_uuid": "cf3a22dbec31a9b7a8ab2189c240e23680d95da9", "_cell_guid": "fb604b38-5dd5-4066-8c26-d0089d84fbf9"}, "cell_type": "markdown", "source": ["Factor plot for Fare clearly indicates that higher the fare more is the chance of survival"]}, {"metadata": {"_uuid": "855deb119c759b79bf7977aa3655e4624dddb333", "_cell_guid": "2ecd3695-a4c3-4c28-9118-b449d90c4d02"}, "execution_count": null, "cell_type": "code", "source": ["facet = sns.FacetGrid(titanic_df, hue=\"Survived\",aspect=4)\n", "facet.map(sns.kdeplot,'Fare',shade= True)\n", "facet.set(xlim=(0, titanic_df['Fare'].max()))\n", "facet.add_legend()"], "outputs": []}, {"metadata": {"_uuid": "eaf4b229f690fb45e6c3967cd5867f963ed2389a", "_cell_guid": "b24d8080-421f-4c80-99f0-a6a4cd64fe32"}, "cell_type": "markdown", "source": ["Factor plot below suggests that passengers with age between 5 to 15 and 27 to 60 have higher chance of survival"]}, {"metadata": {"_uuid": "37fdb7f845a7b6f2b96e49098b80521b7979d48c", "_cell_guid": "3768de54-dc17-4110-a900-3d3efde545d7"}, "execution_count": null, "cell_type": "code", "source": ["facet = sns.FacetGrid(titanic_df, hue=\"Survived\",aspect=4)\n", "facet.map(sns.kdeplot,'Age',shade= True)\n", "facet.set(xlim=(0, titanic_df['Age'].max()))\n", "facet.add_legend()"], "outputs": []}, {"metadata": {"_uuid": "dafc617021a10ce322cb76eee6ef746c6db76679", "_cell_guid": "8df5ac06-d0d6-4f31-863c-edc3d87694e4"}, "cell_type": "markdown", "source": ["Looking at the Pclass swarm plot it is clear that 1st class passengers have higher chance of susrvial comared to 2nd class passengers. Similarly 2nd class passengers have higher chance of survival compare to passengers travelling in third class"]}, {"metadata": {"_uuid": "93f6778fe5fbbda8a972810c88d4985af46d1488", "collapsed": true, "_cell_guid": "56649251-0155-43aa-bccb-e781f2c227f0"}, "execution_count": null, "cell_type": "code", "source": ["sns.swarmplot(x='Pclass', y='PassengerId',data=titanic_df, hue='Survived')"], "outputs": []}, {"metadata": {"_uuid": "78cd1c25e95f2793303eb913d9fd9595777b5c59", "_cell_guid": "fe7bc358-3f1e-42df-b421-f3441a2947c9"}, "cell_type": "markdown", "source": ["Cabin feature is also  contributing to some extent. We can see that Cabin 2,3,4 and 5 have higher chance of survival compared to others. "]}, {"metadata": {"_uuid": "403ea1b6424c242ef80a63fa0215a186a8de965f", "collapsed": true, "_cell_guid": "95bca803-5e99-42b4-be14-c931b4c86ecd"}, "execution_count": null, "cell_type": "code", "source": ["sns.countplot(x='Cabin',data=titanic_df, hue='Survived')"], "outputs": []}, {"metadata": {"_uuid": "b07f7c820614b0e0385cf3ce25fc954c08b93039", "_cell_guid": "82f2924c-8311-4b8d-91d7-ecd79c930bb7"}, "cell_type": "markdown", "source": ["Similarly we can look at SibSp, Parch and Embarked"]}, {"metadata": {"_uuid": "ce5a4689c3823cceace48d283c048bcbb733afb0", "collapsed": true, "_cell_guid": "2250f8b7-8cb8-49c1-8097-b9b7d9a39a9e"}, "execution_count": null, "cell_type": "code", "source": ["sns.countplot(x='SibSp',data=titanic_df, hue='Survived')\n"], "outputs": []}, {"metadata": {"_uuid": "31901ab9d0ff806472f494466a67f8c52509c755", "collapsed": true, "_cell_guid": "1d355a38-02a0-44e2-b79b-8e69a1e69133"}, "execution_count": null, "cell_type": "code", "source": ["sns.countplot(x='Parch',data=titanic_df, hue='Survived')"], "outputs": []}, {"metadata": {"_uuid": "6ba05a5aaec0bdca6d9d8149793a583f91f1f294", "collapsed": true, "_cell_guid": "7cc427c3-9a94-4040-9be4-9289340b627e"}, "execution_count": null, "cell_type": "code", "source": ["sns.countplot(x='Embarked',data=titanic_df, hue='Survived')"], "outputs": []}, {"metadata": {"_uuid": "7b45ed637dafcde8a959720506491527cf890a7c", "_cell_guid": "9ec84018-5181-462a-a5aa-0a080f07cdd0"}, "cell_type": "markdown", "source": ["It's time to run our model and test the score. Let's remove the feature with least weight . It is not contributing to the model largly\n"]}, {"metadata": {"_uuid": "24fce5c14a8b08bde00a603fef1916dfd9bd77b3", "_cell_guid": "9a8ffada-b5c8-4fe0-8411-b9d3ac2fe75f"}, "execution_count": null, "cell_type": "code", "source": ["#Drop Embarked and PArch columns it is not contributing much\n", "X = X = titanic_df.drop([\"Survived\", \"PassengerId\",\"Parch\"],axis=1)\n", "\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n", "\n", "# Random Forests\n", "random_forest = RandomForestClassifier(max_depth=5, random_state=0)\n", "random_forest.fit(X_train, y_train)\n", "Y_pred = random_forest.predict(X_test)\n", "testdata = y_test\n", "predictions = Y_pred\n", "\n", "# save confusion matrix and slice into four pieces\n", "confusion = metrics.confusion_matrix(testdata, predictions)\n", "print('Confusion Metrics')\n", "print(confusion)\n", "print('Model Accuracy =', metrics.accuracy_score(testdata, predictions))\n", "\n", "#[row, column]\n", "TP = confusion[1, 1]\n", "TN = confusion[0, 0]\n", "FP = confusion[0, 1]\n", "FN = confusion[1, 0]\n", "precision = TP / float(TP + FP)\n", "recall = TP / float(FN + TP)\n", "print('Precision=',precision)\n", "print('Recall=',recall)\n"], "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": ["**Model Training and Test Data Cleanup**\n", "\n", "As seen above we are talking about 81% model accuracy with 82% Precision nd 71% Recall. Overall not a bad score."]}, {"metadata": {"_uuid": "58dea6415ed326dd24a6afebd88dc30914054bad", "_cell_guid": "605a52de-72c2-4138-a14d-bc3d4be84734"}, "execution_count": null, "cell_type": "code", "source": ["# Finally train the model on entire training data\n", "random_forest.fit(X, y)\n"], "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": ["Let's cleanup the test data\n"]}, {"metadata": {}, "execution_count": null, "cell_type": "code", "source": ["# Get the info of the data frame\n", "test_ToPred = test_df\n", "test_ToPred.info()\n", "# There are 12 features and 891 rows\n", "# Null Columns - Age, Cabin, Embarked\n", "\n", "# Add one more column called Salutation\n", "conditions = [\n", "    (test_ToPred['Name'].str.contains('Master')),\n", "    (test_ToPred['Name'].str.contains('Mrs.')),\n", "    (test_ToPred['Name'].str.contains('Mr.')),\n", "    (test_ToPred['Name'].str.contains('Miss'))]\n", "choices = ['Master', 'Mrs', 'Mr', 'Miss']\n", "test_ToPred[\"Salutation\"] = np.select(conditions, choices, default='')\n", "\n", "means = test_ToPred.groupby('Salutation').mean()\n", "\n", "test_ToPred.loc[test_ToPred.Salutation == 'Master','Age'] = test_ToPred['Age'].fillna(means[\"Age\"][\"Master\"]) \n", "test_ToPred.loc[test_ToPred.Salutation == 'Mrs','Age'] = test_ToPred['Age'].fillna(means[\"Age\"][\"Mrs\"]) \n", "test_ToPred.loc[test_ToPred.Salutation == 'Mr','Age'] = test_ToPred['Age'].fillna(means[\"Age\"][\"Mr\"]) \n", "test_ToPred.loc[test_ToPred.Salutation == 'Miss','Age'] = test_ToPred['Age'].fillna(means[\"Age\"][\"Miss\"]) \n", "\n", "# Change Sex to identify kids/teens info\n", "test_ToPred.loc[test_ToPred.Salutation == 'Master', 'Sex'] = 'master'\n", "test_ToPred.loc[test_ToPred.Salutation == 'Miss', 'Sex'] = 'miss'\n", "\n", "# Drop Name and Ticket columns\n", "test_ToPred = test_ToPred.drop(['PassengerId','Parch','Name','Ticket','Salutation'], axis=1)\n", "\n", "\n", "# Fill NA values of Age Cabin and Embarked\n", "#1] Calc kids average age\n", "\n", "test_ToPred['Age'].fillna(titanic_df['Age'].mean(), inplace=True)\n", "test_ToPred['Fare'].fillna(titanic_df['Fare'].mean(), inplace=True)\n", "test_ToPred['Embarked'].fillna('B' , inplace=True)\n", "test_ToPred['Cabin'].fillna('Z' , inplace=True)\n", "\n", "# Take only first letter of Cabin \n", "test_ToPred['Cabin'] = titanic_df['Cabin'].astype(str).str[0]\n", "\n", "# Convert Cabin, Embarked and  Sex into Numeric\n", "test_ToPred['Sex'].replace(['female','male','master','miss'], [0,1,2,3],inplace=True)\n", "test_ToPred['Cabin'].replace(['Z','A', 'B', 'C', 'D', 'E', 'F', 'G', 'T'], [0,1,2,3,4,5,6,7,8],inplace=True)\n", "test_ToPred['Embarked'].replace(['B','S','C','Q'], [0,1,2,3],inplace=True)\n", "\n", "# Change Age to float\n", "test_ToPred['Age']  =  titanic_df['Age'].astype(float)\n", "#Check the clenaed dataframe\n", "test_ToPred.info()\n", "test_ToPred.head()\n"], "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": ["Pedict the output"]}, {"metadata": {}, "execution_count": null, "cell_type": "code", "source": ["Y_pred = random_forest.predict(test_ToPred)\n"], "outputs": []}, {"metadata": {}, "execution_count": null, "cell_type": "code", "source": ["test_op = pd.DataFrame({\n", "        \"PassengerId\": test_df[\"PassengerId\"],\n", "        \"Survived\": Y_pred\n", "    })\n", "test_op.to_csv('titanicPrediction.csv', index=False)\n", "test_op.head()"], "outputs": []}, {"metadata": {"collapsed": true}, "execution_count": null, "cell_type": "code", "source": [], "outputs": []}]}