{"cells":[{"metadata":{"_uuid":"117dcb95800acf118631a3002b932dcc356519b0","collapsed":true,"_cell_guid":"d120c769-304b-47c3-b491-8750f5592225","trusted":false},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport lightgbm as lgbm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0c9ee29ab3b61ebebd4118a655bd61f4b4c9b644","_cell_guid":"efda3bb3-ab54-4fad-a33a-d7467d22afa6"},"cell_type":"markdown","source":"# An implementation of LightGBM for the Titanic problem\n\nLoad and check the data:"},{"metadata":{"_uuid":"be076ef24c07f8f2dd0e6b0b72fa47b48dd9a78f","collapsed":true,"_cell_guid":"0e19b3ff-06d7-49e2-9ecc-a1fd55dbd062","trusted":false},"cell_type":"code","source":"train_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')\ntrain_df.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"67cde09df6ce837d17099adaa045197b22e6d17e","_cell_guid":"f1d14c31-7ad8-4103-ac14-32a7fd324d43"},"cell_type":"markdown","source":"### Feature engineering\n\n- Add any extra features that may be handy\n- Encode and categoricals stores as strings\n- Drop features that look difficult to get use out of\n\nWant to make sure that everything I do to the training set happens to the test, so makes sense to run as a loop "},{"metadata":{"_uuid":"eb2ae68badb0920eb061ddf83d445bd47ee40751","collapsed":true,"_cell_guid":"f2e49c9c-b3ed-4fe4-a03a-9fea97eed707","trusted":false},"cell_type":"code","source":"# Not sure passenger ID is useful as a feature, but need to save it from the test set for the submission\ntest_passenger_ids = test_df.pop('PassengerId')\ntrain_df.drop(['PassengerId'], axis=1, inplace=True)\n\n# 'Embarked' is stored as letters, so fit a label encoder to the train set to use in the loop\nembarked_encoder = LabelEncoder()\nembarked_encoder.fit(train_df['Embarked'].fillna('Null'))\n\n# Dataframes to work on\ndf_list = [train_df, test_df]\n\nfor df in df_list:    \n    \n    # Record anyone travelling alone\n    df['Alone'] = (df['SibSp'] == 0) & (df['Parch'] == 0)\n    \n    # Transform 'Embarked'\n    df['Embarked'].fillna('Null', inplace=True)\n    df['Embarked'] = embarked_encoder.transform(df['Embarked'])\n    \n    # Transform 'Sex'\n    df.loc[df['Sex'] == 'female','Sex'] = 0\n    df.loc[df['Sex'] == 'male','Sex'] = 1\n    df['Sex'] = df['Sex'].astype('int8')\n    \n    # Drop features that seem unusable. Save passenger ids if test\n    df.drop(['Name', 'Ticket', 'Cabin'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9954df056296adc045381bf13f4c9c5cf3d431e4","_cell_guid":"89c73e20-547b-4e5c-96fa-6a5315206e5a"},"cell_type":"markdown","source":"### Prep the training set for learning"},{"metadata":{"_uuid":"240c379180a19cb4ce558429fbe8843aadc98a5f","collapsed":true,"_cell_guid":"1274f254-d769-408b-99b8-7bd929ca6882","trusted":false},"cell_type":"code","source":"# Separate the label\ny = train_df.pop('Survived')\n\n# Take a hold out set randomly\nX_train, X_test, y_train, y_test = train_test_split(train_df, y, test_size=0.2, random_state=42)\n\n# Create an LGBM dataset for training\ncategorical_features = ['Alone', 'Sex', 'Pclass', 'Embarked']\ntrain_data = lgbm.Dataset(data=X_train, label=y_train, categorical_feature=categorical_features, free_raw_data=False)\n\n# Create an LGBM dataset from the test\ntest_data = lgbm.Dataset(data=X_test, label=y_test, categorical_feature=categorical_features, free_raw_data=False)\n\n# Finally, create a dataset for the FULL training data to give us maximum amount of data to train on after \n# performance has been calibrate\nfinal_train_set = lgbm.Dataset(data=train_df, label=y, \n                               categorical_feature=categorical_features, free_raw_data=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f7ea3b59684b5f46a650e94b2ec51294f7886895","_cell_guid":"d66765f7-5e2d-4136-bb15-8d2837a42714"},"cell_type":"markdown","source":"### Define hyperparameters for LGBM"},{"metadata":{"_uuid":"1767b8459bf53a5cd7bcaf90ec4be28610a6f6d2","collapsed":true,"_cell_guid":"1996b68f-c614-41a3-9e41-8d64acacd8d6","trusted":false},"cell_type":"code","source":"lgbm_params = {\n    'boosting': 'dart',          # dart (drop out trees) often performs better\n    'application': 'binary',     #Â Binary classification\n    'learning_rate': 0.05,       # Learning rate, controls size of a gradient descent step\n    'min_data_in_leaf': 20,      # Data set is quite small so reduce this a bit\n    'feature_fraction': 0.7,     # Proportion of features in each boost, controls overfitting\n    'num_leaves': 41,            # Controls size of tree since LGBM uses leaf wise splits\n    'metric': 'binary_logloss',  # Area under ROC curve as the evaulation metric\n    'drop_rate': 0.15\n              }","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ba1b8b3a355f5b5379daab7c9ee9b656d9879a95","_cell_guid":"e946d2ed-991c-41f3-889d-fcc86b0256af"},"cell_type":"markdown","source":"### Train the model\n\nUse the held out training data to evaluate performance and early stop to control overfitting"},{"metadata":{"_uuid":"04a0d2dccc99bf687c22c547f1dd1a2e86af5824","collapsed":true,"_cell_guid":"29246c91-37d8-4317-a88c-5c5b2384bbaf","trusted":false},"cell_type":"code","source":"evaluation_results = {}\nclf = lgbm.train(train_set=train_data,\n                 params=lgbm_params,\n                 valid_sets=[train_data, test_data], \n                 valid_names=['Train', 'Test'],\n                 evals_result=evaluation_results,\n                 num_boost_round=500,\n                 early_stopping_rounds=100,\n                 verbose_eval=20\n                )\noptimum_boost_rounds = clf.best_iteration","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aaba53af1283fc35dad87698ed9f10aa877292fb","_cell_guid":"e81374b5-6dd5-47a1-b06c-8f7d60dfc89b"},"cell_type":"markdown","source":"### Visualise training performance"},{"metadata":{"_uuid":"d885fe78f705a0a831d3f074233c11ec7617d62d","collapsed":true,"_cell_guid":"1398e2f3-ad6f-41a4-b666-c613c4b073cc","trusted":false},"cell_type":"code","source":"fig, axs = plt.subplots(1, 2, figsize=[15, 4])\n\n# Plot the log loss during training\naxs[0].plot(evaluation_results['Train']['binary_logloss'], label='Train')\naxs[0].plot(evaluation_results['Test']['binary_logloss'], label='Test')\naxs[0].set_ylabel('Log loss')\naxs[0].set_xlabel('Boosting round')\naxs[0].set_title('Training performance')\naxs[0].legend()\n\n# Plot feature importance\nimportances = pd.DataFrame({'features': clf.feature_name(), \n                            'importance': clf.feature_importance()}).sort_values('importance', ascending=False)\naxs[1].bar(x=np.arange(len(importances)), height=importances['importance'])\naxs[1].set_xticks(np.arange(len(importances)))\naxs[1].set_xticklabels(importances['features'])\naxs[1].set_ylabel('Feature importance (# times used to split)')\naxs[1].set_title('Feature importance')\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d6a975a7fa6029d200a36b5cfda91d3e896d505","_cell_guid":"d2691fbf-a171-4edb-b572-569703dcc07b"},"cell_type":"markdown","source":"### Examine model performance \n\nAccuracy score can often be misleading for classifiers, so have a look at the precision and recall too"},{"metadata":{"_uuid":"9c8d5c427b0d6a9a943f3a183e077f6dbe109313","collapsed":true,"_cell_guid":"cb9d2a2c-2c9c-4249-a0e1-f6ed19e4d72d","trusted":false},"cell_type":"code","source":"preds = np.round(clf.predict(X_test))\nprint('Accuracy score = \\t {}'.format(accuracy_score(y_test, preds)))\nprint('Precision score = \\t {}'.format(precision_score(y_test, preds)))\nprint('Recall score =   \\t {}'.format(recall_score(y_test, preds)))\nprint('F1 score =      \\t {}'.format(f1_score(y_test, preds)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"43bd6fbe0691eeff8ee4cf8af4aefe8b3baf6000","_cell_guid":"db5ef966-af72-4dfb-b29b-9a993ad7fc07"},"cell_type":"markdown","source":"### Make predictions on the test set\n\nUsing the parameters optimised above, retrain on all the data so we don't miss anything. Then make predictions on the\ntest set."},{"metadata":{"_uuid":"53c55c58e83df8ae8711e10e9a93c458656d4d1b","collapsed":true,"_cell_guid":"478d6f05-c8e2-4ebf-b125-7d31dc4c8d5a","trusted":false},"cell_type":"code","source":"clf_final = lgbm.train(train_set=final_train_set,\n                      params=lgbm_params,\n                      num_boost_round=optimum_boost_rounds,\n                      verbose_eval=0\n                      )\n\ny_pred = np.round(clf_final.predict(test_df)).astype(int)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"38b73ef70c31e5bec3b74f6b9ab6c35851df9f1d","collapsed":true,"_cell_guid":"58ba556f-37f2-4c18-a9f6-cfca202951c3","trusted":false},"cell_type":"code","source":"output_df = pd.DataFrame({'PassengerId': test_passenger_ids, 'Survived': y_pred})","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"mimetype":"text/x-python","pygments_lexer":"ipython3","codemirror_mode":{"version":3,"name":"ipython"},"version":"3.6.4","name":"python","file_extension":".py","nbconvert_exporter":"python"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}