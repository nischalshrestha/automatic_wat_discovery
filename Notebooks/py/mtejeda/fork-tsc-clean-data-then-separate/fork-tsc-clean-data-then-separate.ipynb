{"cells":[{"metadata":{"_uuid":"f236db0cc0bf644a68cc6877201351baa67b8fa9"},"cell_type":"markdown","source":"Step 1: Import required libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n\nnp.random.seed(0)\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"Step 2:  Read in data and have a first look at it. This time we're going to merge the train and test datasets, do all the cleaning, and then separate. Note that since the survived column was added to the end of test, by setting sort to be false in the concatenate, it leaves survived as the last column for the rows from test but keeps it as the second column for the rows from train."},{"metadata":{"trusted":true,"_uuid":"3e3e269290921bb484495bb65743fc181322de07","collapsed":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\n# #Check for Columns with Missing Data\n# cols_with_missing = [col for col in train.columns if train[col].isnull().any()]\n# cols_with_missing\n\n# #Check for non-integer of float dtype columns\n# cols_with_object = [col for col in train.columns if train[col].dtype == object]\n# cols_with_object\n\ntest['Survived'] = \"\"\ncol_test = test.columns.tolist()\ncol_test.insert(1,col_test[-1])\ndel col_test[-1]\ntest = test[col_test]\n\nframes = [train,test]\nfull = train.append(test,ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1fb465e7f9a732fc85a1b31e42dce53424c27742","collapsed":true},"cell_type":"code","source":"test.iloc[-10:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8a1a2a23381f7238b65b6729058fcffb41fcff9e","collapsed":true},"cell_type":"code","source":"np.shape(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"873cdb960ce346eb35d459522591eda8f831f7fe","collapsed":true},"cell_type":"code","source":"train.iloc[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c0911ba1d028cf7f82bc5345f3b09cc82db3f2f1","collapsed":true},"cell_type":"code","source":"train.iloc[-10:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"83240011ba0a6950dade5b2ef23128822c7a8d96","collapsed":true},"cell_type":"code","source":"np.shape(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e1230fcfcf1a817aa9f9bd513b7be21f3fa4aefd","collapsed":true},"cell_type":"code","source":"full.iloc[0:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9468d9d1b2cbdf0f77dd06f9086abd6953b9589d","collapsed":true},"cell_type":"code","source":"full.iloc[-10:]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9871ba44d8478f133ff9f683e63cec449fac0e60"},"cell_type":"markdown","source":"Step 3: Create a function to clean the data"},{"metadata":{"trusted":true,"_uuid":"23a087af1f4e0769250df8cab1d1c3c8b224f512","collapsed":true},"cell_type":"code","source":"def data_cleanup(titanic_data):\n    sex_dummy = pd.get_dummies(titanic_data.Sex)\n    titanic_data['male'] = sex_dummy.male\n    titanic_data['female'] = sex_dummy.female\n    \n    titanic_data.Age = titanic_data.Age.fillna(titanic_data.Age.median())\n    titanic_data.Fare = titanic_data.Fare.fillna(titanic_data.Fare.median())\n    \n    titanic_data['Embarked'].fillna(method = 'bfill', axis = 0)\n    embarked_dummy = pd.get_dummies(titanic_data['Embarked'])\n    titanic_data['C'] = embarked_dummy.C\n    titanic_data['Q'] = embarked_dummy.Q\n    titanic_data['S'] = embarked_dummy.S\n    \n#     titanic_data['Salutation'] = titanic_data['Name'].apply(lambda x: x.split(\",\")[1:])\n#     titanic_data['Salutation'] = titanic_data['Salutation'].apply(lambda x: x[0])\n#     titanic_data['Salutation'] = titanic_data['Salutation'].apply(lambda x: x.split(\".\")[:1])\n#     titanic_data['Salutation'] = titanic_data['Salutation'].apply(lambda x: x[0])\n    \n#     mlb = MultiLabelBinarizer()\n#     # tt = pd.DataFrame(mlb.fit_transform(full.pop('Salutation')))\n# #     mlb.classes_\n                    \n#     titanic_data = titanic_data.join(pd.DataFrame(mlb.fit_transform(titanic_data.pop('Salutation')),\n#                                 columns = mlb.classes_))\n\n    return titanic_data","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fc8719989408a04953f9578f257bf2e0259acb0f"},"cell_type":"markdown","source":"Step 4: Pass the train and test data through the cleaning function"},{"metadata":{"trusted":true,"_uuid":"e505cc65e1a30e1ccc10e342302f3e5225c7f3d5","collapsed":true},"cell_type":"code","source":"full.index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"08ad9348102ebc17350fd04e5290f3e92811912b","collapsed":true},"cell_type":"code","source":"full = data_cleanup(full)\nfull.index\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"504388550b060a62be71e0053038466df4e21a30"},"cell_type":"markdown","source":"Step 5: Use one-hot encoding to encode categorical feature Salutation. MultiLabelBinarizer() is not working. After commenting out the last line of the data cleaning funciton where Salutation is converted from 1x1 array into an element, mlb works because it recognizes that inside of the 1x1 list is 1 element not a string of individual elements. I suspect that mlb.fit-transform works by looking at each element of a string based on the output of mlb.classes_ "},{"metadata":{"trusted":true,"_uuid":"27f24774edd2c83fd64bf380a9e659d7a859cb69","collapsed":true},"cell_type":"code","source":"# mlb = MultiLabelBinarizer()\n# # np.shape(mlb.fit_transform(train.pop('Salutation')))\n# # other = pd.DataFrame(mlb.fit_transform(train.pop('Salutation')),columns = mlb.classes_,index=train.index)\n# # mlb.classes_\n# # other['Mr']\n# # other.iloc[:,0]\n# train = train.join(pd.DataFrame(mlb.fit_transform(train.pop('Salutation')),\n#                                   columns = mlb.classes_,\n#                                   index=train.index),\n#                      on=train.index)\n# # train = train.merge(train,pd.DataFrame(mlb.fit_transform(train.pop('Salutation')),\n#                                        columns = mlb.classes_,\n#                                        index=train.index),\n#                     on=train.index)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8d56260988199af7873d59a4142c34467da26ef0"},"cell_type":"markdown","source":"Step 5.1: Identify the unique elements of the Salutation column and use those to build a one-hot encoding. Didn't have to do this because I cleaned the cleaning function. Instead check the names of the columns in the dataframe to know which one's we'll be passing to build the model."},{"metadata":{"trusted":true,"_uuid":"484df2082e43a9995b345d766b63856187bb7d6e","_kg_hide-output":false,"collapsed":true},"cell_type":"code","source":"test.index.size\ntrain.index.size\nfull.index.size","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"231dde6b774d3ca98b9de65f688a23334b249d20"},"cell_type":"markdown","source":"Step 6: Update the cleaning data function to include one-hot encoding of Salutation. See Updated function in step 3"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"aabfef2116ecf96ec2c7e5c93ef98af61ecb1c82"},"cell_type":"markdown","source":"Step 7: Added a few lines before building the model to extract the rows l"},{"metadata":{"trusted":true,"_uuid":"4f428c675f3b446c2c8d53720d2c0d7951ac27b8","collapsed":true},"cell_type":"code","source":"# df.loc[:, df.columns != 'b']\n# train.index\n# train.iloc[train.index]\n# np.shape(full)\nfull_test = full.copy()\nfull_train = full.copy()\n\n#train should be 0 to 890\nX_features_train = full_train.drop(full.index[891:],axis=0)\n# X_features_train = full.drop(labels=[891:1308],axis=0)\n# X_features_train = full.drop(test.index,axis=0)\nX_features_train = X_features_train.drop(['PassengerId','Survived','Name','Sex','Ticket','Cabin','Embarked'],axis=1)\nnp.shape(X_features_train)\n\n#currently this is returning 418 to 1308\nX_features_train.iloc[0:891]\n\n\n#test should be 891 to 1308\nX_features_test = full_test.drop(full.index[:891],axis=0)\n# X_features_test = full.drop(train.index,axis=0)\n# # full.iloc[train.index]\n# # full.iloc[test.index]\n# # # np.shape(X_features_test)\n# # # # np.shape(full)\nX_features_test = X_features_test.drop(['PassengerId','Survived','Name','Sex','Ticket','Cabin','Embarked'],axis=1)\n# np.shape(X_features_test)\nX_features_test.iloc[0:418]\n\n# # # X_features_train = train.iloc[train.index,train.columns != 'PassengerId','Survived','Name','Sex','Ticket','Cabin','Embarked']\n# # # X_features_test = test.loc[test.index,train.columns != 'PassengerId','Survived','Name','Sex','Ticket','Cabin','Embarked']\n# # # # X_features_train = train[['Age','SibSp','Parch','Fare','Pclass','C','Q','S','male','female']].copy()\n# # # # X_features_test  = test[['Age','SibSp','Parch','Fare','Pclass','C','Q','S','male','female']].copy()\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import KFold, cross_val_score\n\nrf = RandomForestClassifier(n_estimators=50,max_depth=20,n_jobs=-1)\nrf_model = rf.fit(X_features_train,train['Survived'])\ny_pred = rf_model.predict(X_features_test)\n# print(np.size(y_pred))\n# print(y_pred)\n# print(np.shape(X_features_test))\nres = pd.DataFrame(y_pred,index=test['PassengerId'])\nres.columns = ['Survived']\nprint(res.info)\nres.to_csv('result')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b55f5d2aeeae033df75421ff4082380c04c01ae1"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c2e35c2495cd15ded19a7fc66fba340330c9697d"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}