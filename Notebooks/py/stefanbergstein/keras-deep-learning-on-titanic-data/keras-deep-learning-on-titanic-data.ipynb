{"cells": [{"source": ["## Keras Deep Learning on Titanic data\n", "\n", "\n", "---\n", "With this notebook, I am trying to do two things in parallel:\n", "\n", "1. Applying my basic learnings from the book [Deep Learning With Python by Jason Brownlee](https://machinelearningmastery.com/deep-learning-with-python/) that I recently studied. \n", "2. Publish my first kaggle notebook for my own practice to become a Kaggler :-) \n", "\n", "\n", "There are several good Kaggel notebooks showing in-depth visualisations, data exploration and wrangling on the Titanic data:\n", "- [In-Depth Visualisations - Simple Methods](https://www.kaggle.com/jkokatjuhha/in-depth-visualisations-simple-methods)\n", "- [Titanic Data Exploration Starter](https://www.kaggle.com/neviadomski/titanic-data-exploration-starter)\n", "- [Titanic Data Science Solutions](https://www.kaggle.com/startupsci/titanic-data-science-solutions/notebook)\n", "\n", "I won't repeat any of the above work here and jump more or less directlty into the coding with Keras.\n", "\n", "@Jason, I would highly appreciate your feedback. Especially because the scores of the Keras model arn't that great (0.76-0.78).\n", "\n", "---\n", "\n", "### Overview:\n", "1. Simple Keras model with minimal cleansed data\n", "2. Predict the missing data in the Age feature (using Keras ;-)\n", "3. Wrangle, prepare, cleanse the titanic data manually. Source: [Titanic Data Science Solutions](https://www.kaggle.com/startupsci/titanic-data-science-solutions/notebook)\n", "4. Predict 'Survived' with Kears based on wrangled input data\n", "5. Summary\n", "---\n", "** My playground:**\n", "- conda 4.3.27\n", "- Python 3.5.4\n", "- Keras 2.0.8 using TensorFlow backend\n"], "cell_type": "markdown", "metadata": {"_cell_guid": "ed598c10-44fb-49fa-95ab-01fc405725e5", "_uuid": "70e87b309f2ed2ad2314942320b4047cd79d9bac"}}, {"source": ["### Part 1 - Simple Keras model with minimal cleansed data\n", "\n", "\n", "As usual, import modules, read data and display some data"], "cell_type": "markdown", "metadata": {"_cell_guid": "4295e855-c290-4fbf-9bb4-4d93afc996a5", "_uuid": "4bc35a5804458f6210e104868cf86171b5bf9f54"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["# data processing\n", "import numpy as np\n", "import pandas as pd \n", "\n", "# machine learning\n", "from keras.models import Sequential\n", "from keras.layers import Dense\n", "from keras.wrappers.scikit_learn import KerasClassifier\n", "from keras.wrappers.scikit_learn import KerasRegressor\n", "from sklearn.model_selection import GridSearchCV\n", "from sklearn.preprocessing import StandardScaler\n", "from sklearn.pipeline import Pipeline\n", "from sklearn.model_selection import cross_val_score\n", "from sklearn.model_selection import KFold\n", "\n", "# utils\n", "import time\n", "from datetime import timedelta\n", "\n", "# some configuratin flags and variables\n", "verbose=0 # Use in classifier\n", "\n", "# Input files\n", "file_train='../input/train.csv'\n", "file_test='../input/test.csv'\n", "\n", "# defeine random seed for reproducibility\n", "seed = 69\n", "np.random.seed(seed)\n", "\n", "# read training data\n", "train_df = pd.read_csv(file_train,index_col='PassengerId')\n", "\n"], "metadata": {"_cell_guid": "c1e06520-94ff-46a4-ac26-564c6fe3d4b4", "collapsed": true, "_uuid": "34bd97d22195804c4397e2bf2e4e07711a72f29f"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["# Show the columns\n", "train_df.columns.values"], "metadata": {"_cell_guid": "72e4a340-0620-471b-8285-c2f2725cadff", "collapsed": true, "_uuid": "325389cdb7e26469f0a6c3b38068b24e6c0ebe6a"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["# Show the shape\n", "train_df.shape"], "metadata": {"_cell_guid": "b94516bd-edf3-43ad-9310-5a139e55385b", "collapsed": true, "_uuid": "d80e7b9d3da054624d6b4c2403ed7f672c32de18"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["# preview the training dara\n", "train_df.head()"], "metadata": {"_cell_guid": "0ab7460e-1c7c-495a-bd65-851c7929b3ee", "collapsed": true, "_uuid": "0fb037a155c5f588e95cd157d9d744e3f061575e"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["# Show that there is NaN data (Age,Fare Embarked), that needs to be handled during data cleansing\n", "train_df.isnull().sum()"], "metadata": {"_cell_guid": "1bb49787-cf92-4e58-8dd9-3ae5fdabcabb", "collapsed": true, "_uuid": "cb3347790c40767ec7de9905522344d8cfd981a2"}}, {"source": ["#### A function for simple data cleansing\n", "- Drop unwanted features ['Name', 'Ticket', 'Cabin']\n", "- Fill missing data: Age and Fare with the mean, Embarked with most frequent value\n", "- Convert categorical features into numeric\n", "- Convert Embarked to one-hot"], "cell_type": "markdown", "metadata": {"_cell_guid": "766af68f-efaa-410c-8b19-775f8c528c7a", "_uuid": "fb1af8c7fb6ff23f6d54390dac6d1f70c3002962"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["def prep_data(df):\n", "    # Drop unwanted features\n", "    df = df.drop(['Name', 'Ticket', 'Cabin'], axis=1)\n", "    \n", "    # Fill missing data: Age and Fare with the mean, Embarked with most frequent value\n", "    df[['Age']] = df[['Age']].fillna(value=df[['Age']].mean())\n", "    df[['Fare']] = df[['Fare']].fillna(value=df[['Fare']].mean())\n", "    df[['Embarked']] = df[['Embarked']].fillna(value=df['Embarked'].value_counts().idxmax())\n", "    \n", "    # Convert categorical  features into numeric\n", "    df['Sex'] = df['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n", "      \n", "    # Convert Embarked to one-hot\n", "    enbarked_one_hot = pd.get_dummies(df['Embarked'], prefix='Embarked')\n", "    df = df.drop('Embarked', axis=1)\n", "    df = df.join(enbarked_one_hot)\n", "\n", "    return df"], "metadata": {"_cell_guid": "5c1b4c78-087f-43b5-9edb-298a7e622236", "collapsed": true, "_uuid": "92d6dfd08ac18a5e24b366181c520dd2e72700b9"}}, {"source": ["Prepare training data and show that there isn't any null data"], "cell_type": "markdown", "metadata": {"_cell_guid": "385d1c91-9f0b-425e-bebc-40de132e5f88", "_uuid": "8d336896a29d23ff0b738f31d442212869c8e1cb"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["train_df = prep_data(train_df)\n", "train_df.isnull().sum()"], "metadata": {"_cell_guid": "f4222a58-2d07-4d32-9866-77e483728bdd", "collapsed": true, "_uuid": "ed6a920a1a8a544181cca91669533481ed091182"}}, {"source": ["Split training data into input X and output Y"], "cell_type": "markdown", "metadata": {"_cell_guid": "ed428940-261a-41f5-a7b4-c6ed2c16300a", "_uuid": "8adc554c625c9a47070ec08cc102a26b7c972386"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["# X contains all columns except 'Survived'  \n", "X = train_df.drop(['Survived'], axis=1).values.astype(float)\n", "\n", "# It is almost always a good idea to perform some scaling of input values when using neural network models (jb).\n", "\n", "scale = StandardScaler()\n", "X = scale.fit_transform(X)\n", "\n", "# Y is just the 'Survived' column\n", "Y = train_df['Survived'].values"], "metadata": {"_cell_guid": "edf52f20-3fed-4ce7-9a70-6c59f061ed6f", "collapsed": true, "_uuid": "af715dcfc105d1fbd3526ba3ee1802abbb8f8b79"}}, {"source": ["#### Simple Network using Keras\n", "- Input lauer with 16 neuron (units/outputs).\n", "- Two hidden layers.\n", "- Output layer with a single neuron and sigmoid activation function to output a value between 0 and 1.\n", "- Optimizer and intit will be searched with GridSearch.\n"], "cell_type": "markdown", "metadata": {"_cell_guid": "26c8c9cb-6f85-4e24-acd2-acc2eada4afb", "_uuid": "168a34de2797c48a712f90390bba645552cf4ebc"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["def create_model(optimizer='adam', init='uniform'):\n", "    # create model\n", "    if verbose: print(\"**Create model with optimizer: %s; init: %s\" % (optimizer, init) )\n", "    model = Sequential()\n", "    model.add(Dense(16, input_dim=X.shape[1], kernel_initializer=init, activation='relu'))\n", "    model.add(Dense(8, kernel_initializer=init, activation='relu'))\n", "    model.add(Dense(4, kernel_initializer=init, activation='relu'))\n", "    model.add(Dense(1, kernel_initializer=init, activation='sigmoid'))\n", "    # Compile model\n", "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n", "    return model"], "metadata": {"_cell_guid": "cb0b5353-5a41-4918-820e-705ff978d0f8", "collapsed": true, "_uuid": "5d7699ff73573e3080a79d1f00d2c6a83214d533"}}, {"source": ["#### Run GridSearch, optionally\n", "GridSearch is very time consuming. \n", "Set `run_gridsearch = True ` in case you really want to run it.\n", "Or simply tune optimizers, inits, epochs and batches. \n"], "cell_type": "markdown", "metadata": {"_cell_guid": "819fcb9b-f035-4031-a660-dcd31c5a6288", "_uuid": "0692bd2341c2801e3575f8851b6efeb9a86e13e2"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["run_gridsearch = False\n", "\n", "if run_gridsearch:\n", "    \n", "    start_time = time.time()\n", "    if verbose: print (time.strftime( \"%H:%M:%S \" + \"GridSearch started ... \" ) )\n", "    optimizers = ['rmsprop', 'adam']\n", "    inits = ['glorot_uniform', 'normal', 'uniform']\n", "    epochs = [50, 100, 200, 400]\n", "    batches = [5, 10, 20]\n", "    \n", "    model = KerasClassifier(build_fn=create_model, verbose=verbose)\n", "    \n", "    param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=inits)\n", "    grid = GridSearchCV(estimator=model, param_grid=param_grid)\n", "    grid_result = grid.fit(X, Y)\n", "    \n", "    # summarize results\n", "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n", "    means = grid_result.cv_results_['mean_test_score']\n", "    stds = grid_result.cv_results_['std_test_score']\n", "    params = grid_result.cv_results_['params']\n", "    if verbose: \n", "        for mean, stdev, param in zip(means, stds, params):\n", "            print(\"%f (%f) with: %r\" % (mean, stdev, param))\n", "        elapsed_time = time.time() - start_time  \n", "        print (\"Time elapsed: \",timedelta(seconds=elapsed_time))\n", "        \n", "    best_epochs = grid_result.best_params_['epochs']\n", "    best_batch_size = grid_result.best_params_['batch_size']\n", "    best_init = grid_result.best_params_['init']\n", "    best_optimizer = grid_result.best_params_['optimizer']\n", "    \n", "else:\n", "    # pre-selected paramters\n", "    best_epochs = 200\n", "    best_batch_size = 5\n", "    best_init = 'glorot_uniform'\n", "    best_optimizer = 'rmsprop'"], "metadata": {"_cell_guid": "96cec510-5f63-4bc7-89e8-702651f8cc48", "collapsed": true, "_uuid": "77b2695ba714a03c48e8b4a946727c10b2187cff"}}, {"source": ["#### Build model and predit\n", "- Create a classifier with best parameters\n", "- Fit model \n", "- Predict 'Survived'"], "cell_type": "markdown", "metadata": {"_cell_guid": "68e4283f-da87-4fbc-b2cb-251ac0a44177", "collapsed": true, "_uuid": "74c5f39cc97a0ee29155245de69c545ed7bc143d"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["# Create a classifier with best parameters\n", "model_pred = KerasClassifier(build_fn=create_model, optimizer=best_optimizer, init=best_init, epochs=best_epochs, batch_size=best_batch_size, verbose=verbose)\n", "model_pred.fit(X, Y)\n", "\n", "# Read test data\n", "test_df = pd.read_csv(file_test,index_col='PassengerId')\n", "# Prep and clean data\n", "test_df = prep_data(test_df)\n", "# Create X_test\n", "X_test = test_df.values.astype(float)\n", "# Scaling\n", "X_test = scale.transform(X_test)\n", "\n", "# Predict 'Survived'\n", "prediction = model_pred.predict(X_test)\n", "\n"], "metadata": {"_cell_guid": "e4b29605-487c-4e90-8fbd-073c13ca79db", "_uuid": "0454dc566dd207a8cde6889ca5615702b25e8834", "collapsed": true, "_kg_hide-output": false}}, {"source": ["Save predictions"], "cell_type": "markdown", "metadata": {"_cell_guid": "14fcce59-2703-4a30-9952-1e0b2df30f77", "_uuid": "6eff3cc544162bd069fb3d1a4f0504ed38d16839"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["submission = pd.DataFrame({\n", "    'PassengerId': test_df.index,\n", "    'Survived': prediction[:,0],\n", "})\n", "\n", "submission.sort_values('PassengerId', inplace=True)    \n", "submission.to_csv('submission-simple-cleansing.csv', index=False)"], "metadata": {"_cell_guid": "d7bbeb3f-ed94-4a95-bc69-1c9976b72fcd", "collapsed": true, "_uuid": "d492365ec37721148d36602266587e0071ca7858"}}, {"source": ["#### Result part 1 - Simple Keras model with minimal cleansed data\n", "Submitting the prediction csv will have scroe of ~78%. Not bad and not that good."], "cell_type": "markdown", "metadata": {"_cell_guid": "1d1e494d-de88-4711-9f46-12dcc02e2796", "collapsed": true, "_uuid": "1d0aca111cd769a66110756c81acb3a6c0dff2fe"}}, {"source": ["### Part 2 - Predict the missing data in the Age feature \n", "\n", "The Age feature has a lot missing data. Let's try to predict the age where needed.\n", "\n", "#### Overview\n", "- Read the data (again) and put all data into one data frame (dfa)\n", "- Clean-up and prep data\n", "- Preview a few rows\n", "- Split data in to training set (Age not null) and 'to-be-predicted' set\n", "- Predict age"], "cell_type": "markdown", "metadata": {"_cell_guid": "069b75f2-f9d2-4d39-af00-b57a9001252f", "_uuid": "8050e34363e7d3141d539700ec552ef4eaa5cae7"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["# Read the data\n", "file_train='../input/train.csv'\n", "file_test='../input/test.csv'\n", "    \n", "df_train = pd.read_csv(file_train,index_col='PassengerId')\n", "df_test = pd.read_csv(file_test,index_col='PassengerId')  \n", "l = len(df_train.index)\n", "    \n", "\n", "## All data train and test in one dataframe \n", "dfa = df_train.append(df_test)\n", "\n", "# Drop unwanted features\n", "dfa = dfa.drop(['Name', 'Ticket', 'Cabin'], axis=1)\n", "    \n", "# Fill missing data: Fare with mean, Embarked with most frequent value\n", "dfa[['Fare']] = dfa[['Fare']].fillna(value=dfa[['Fare']].mean())\n", "dfa[['Embarked']] = dfa[['Embarked']].fillna(value=dfa['Embarked'].value_counts().idxmax())\n", "    \n", "# Convert categorical features into numeric\n", "dfa['Sex'] = dfa['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n", "\n", "# Convert 'Embarked' to one-hot\n", "enbarked_one_hot = pd.get_dummies(dfa['Embarked'], prefix='Embarked')\n", "dfa = dfa.drop('Embarked', axis=1)\n", "dfa = dfa.join(enbarked_one_hot)"], "metadata": {"_cell_guid": "8a45bf7b-2a2a-42ec-a825-b8bf0dc16c42", "collapsed": true, "_uuid": "86f949faf7c783d683fe6082e1682b39a9e7a481"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["dfa.head()"], "metadata": {"_cell_guid": "862d9369-2fa9-4626-ae4c-0be2aa492f5e", "collapsed": true, "_uuid": "7b60a45e4841ae543760b596fecadd52ee9f6b7d"}}, {"source": ["Split data in to training set (Age not null) and 'to-be-predicted' set (Age in nan)"], "cell_type": "markdown", "metadata": {"_cell_guid": "027f6594-e94a-4841-8aa7-d5dab157de26", "_uuid": "92923512607600042732973700cf29c019b7c116"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["# Split data in to training set (Age not null) and 'to-be-predicted' set (Age in nan)\n", "df_age_train = dfa[dfa.Age.notnull()]\n", "df_age_nan = dfa[dfa.Age.isnull()]"], "metadata": {"_cell_guid": "5c4a52b0-86bd-4709-9a5f-bff52b8074d6", "collapsed": true, "_uuid": "4e39b64edf8c39fe171448e248a515459f3ee2f2"}}, {"source": ["#### Predict age\n", "- Create input X, output Y and X_test\n", "- The model for the regression. The regression problem may have a single output neuron and the neuron may have no activation function (jb).\n", "- Create a pipeline, do cross-validation and predict\n", "- Generate new input files for the training and test data but with predicted age\n"], "cell_type": "markdown", "metadata": {"_cell_guid": "22a878ae-a951-43f7-8ee4-5e40bdbe5a39", "_uuid": "f3154463bb97603dd94b006c8ae2b39ef62572dd"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["# split data into input X and output Y\n", "X = df_age_train.drop(['Age', 'Survived'], axis=1).values.astype(float)\n", "Y = df_age_train['Age'].values.astype(float)\n", "\n", "X_test = df_age_nan.drop(['Age', 'Survived'], axis=1).values.astype(float)\n", "\n", "def age_model():\n", "    # create model\n", "    model = Sequential()\n", "    model.add(Dense(32, input_dim=X.shape[1], kernel_initializer='normal', activation='relu'))\n", "    model.add(Dense(16, kernel_initializer='normal', activation='relu'))\n", "    model.add(Dense(8, kernel_initializer='normal', activation='relu'))\n", "    model.add(Dense(1, kernel_initializer='normal'))\n", "    # Compile model\n", "    model.compile(loss='mean_squared_error', optimizer='adam')\n", "    return model\n"], "metadata": {"_cell_guid": "1369fc15-5da7-4063-a709-fc0c489badbf", "collapsed": true, "_uuid": "dd702c6c5f5634630955ce4ab28a967d7faa19e0"}}, {"source": ["**Create a pipeline**"], "cell_type": "markdown", "metadata": {"_cell_guid": "63821249-a90f-4362-8e4d-0fe7fe7ba51a", "_uuid": "41d74d20f9bbc2cb28d1a307d327bc4fd42ed9cf"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["estimators = []\n", "estimators.append(('standardize', StandardScaler()))\n", "estimators.append(('mlp', KerasRegressor(build_fn=age_model, epochs=100, batch_size=5, verbose=verbose)))\n", "pipeline = Pipeline(estimators)"], "metadata": {"_cell_guid": "80af653c-3510-4e38-8b4c-cb0a05f1a140", "collapsed": true, "_uuid": "2df9a0cc24852cea107f687cd6e819e97ced5679"}}, {"source": ["**Cross-validation**"], "cell_type": "markdown", "metadata": {"_cell_guid": "99a5c02e-fe4a-4d71-b6d0-de649e634fd8", "_uuid": "ee28a9937dcaffc5c02b47c067d0ef54645b6203"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["kfold = KFold(n_splits=2, random_state=seed)\n", "results = cross_val_score(pipeline, X, Y, cv=kfold)\n", "print(\"Result: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"], "metadata": {"_cell_guid": "f4e9cd2e-88ac-4b81-8f5d-ebfa922bd848", "collapsed": true, "_uuid": "2ee695164044fcae199ce99869157a49cf5899f4"}}, {"source": ["Ups, the mean square erroe is very poor.\n", "\n", "**Predict**"], "cell_type": "markdown", "metadata": {"_cell_guid": "a939c134-2b18-44b9-8582-3451ff724ad5", "_uuid": "80f616c6ab253afd3831f62aa9f313b03449089f"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["pipeline.fit(X, Y)\n", "prediction_train = pipeline.predict(X)\n", "prediction_test = pipeline.predict(X_test)"], "metadata": {"_cell_guid": "670e6a01-34e9-43f1-9cde-5d5bb881a8e0", "collapsed": true, "_uuid": "cb901b152eaf073fdd754b0e71670ede50006746"}}, {"source": ["** Generate new input files for the training and test data with predicted age **"], "cell_type": "markdown", "metadata": {"_cell_guid": "f00697f6-0098-46fb-b960-6de5f333a643", "_uuid": "554dc009cbfea574fbd97965fb2626cd57129b81"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["# Create a data frame with PassengerId and predicted age\n", "df_age_pred = pd.DataFrame({\n", "    'PassengerId': df_age_nan.index,\n", "    'Age_pred': prediction_test.astype(int)\n", "})\n", "df_age_pred.set_index('PassengerId', inplace=True)\n", "   \n", "\n", "# Add column with predicted age to the dataframe with all data (dfa)\n", "dfa2 = df_train.append(df_test) \n", "dfa_pred = pd.concat([dfa2, df_age_pred], axis=1)   \n", "\n", "# Update Age column with prediction where nan and remove Age_pred\n", "dfa_pred['Age'] = np.where(pd.isnull(dfa_pred['Age']), dfa_pred['Age_pred'] , dfa_pred['Age'])\n", "dfa_pred = dfa_pred.drop(['Age_pred'], axis=1)\n", "\n", "# Create new files\n", "l = len(df_train)\n", "df_train2 = dfa_pred[0:l] \n", "df_test2 = dfa_pred[l:] \n", "df_test2 = df_test2.drop(['Survived'], axis=1)\n", "\n", "df_train2.to_csv('train-age-predicted.csv')\n", "df_test2.to_csv('test-age-predicted.csv')"], "metadata": {"_cell_guid": "f4d26e5c-f303-4742-8fee-1c5b3453f5b3", "collapsed": true, "_uuid": "702fd99aa348ad8953d488eb1b221721b320d3db"}}, {"source": ["The two file above can be used instead of the original input data. Give it a try with the part 1 with the new file names. The result is still not that great (scroe of ~78%)."], "cell_type": "markdown", "metadata": {"_cell_guid": "9119fc26-3c53-43d6-bc64-dbbb5f06a278", "_uuid": "6331292b663cff88ff50509426d2ead99874deda"}}, {"source": ["### Part 3 - Wrangle, prepare, cleanse the titanic data manually\n", "Source: [Titanic Data Science Solutions](https://www.kaggle.com/startupsci/titanic-data-science-solutions/notebook) by Manav Sehgal\n", "\n", "Neither the simple cleaning nor the age prediction generated any score above 80%. Eventually manual data prep will help.\n", "\n", "\n", "**Re-read data, drop the Cabin and Ticket features**"], "cell_type": "markdown", "metadata": {"_cell_guid": "6bf76da7-041a-4e17-9003-0781e0ddca98", "_uuid": "18906373478352e39172fd93e6d7efd5155019f6"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["train_df = pd.read_csv('../input/train.csv')\n", "test_df = pd.read_csv('../input/test.csv')\n", "train_df = train_df.drop(['Ticket', 'Cabin'], axis=1)\n", "test_df = test_df.drop(['Ticket', 'Cabin'], axis=1)\n", "combine = [train_df, test_df]"], "metadata": {"_cell_guid": "4fb404d2-3011-41ea-a47e-7ed895b19046", "collapsed": true, "_uuid": "656e4918b7a3560289ffe5cb7192f277af8bf9fa"}}, {"source": ["**Creating new feature extracting from existing ...**"], "cell_type": "markdown", "metadata": {"_cell_guid": "73e10291-cd81-49b1-a84b-afa6598c2697", "_uuid": "6197719f369d3430283e349314ce2392483b26d3"}}, {"source": ["**New title featue**"], "cell_type": "markdown", "metadata": {"_cell_guid": "5702404e-f933-458c-b260-33de33d47eb6", "_uuid": "2b24e4aeb5691bd56d6a73e23dd311e25cedf717"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["for dataset in combine:\n", "    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n", "\n", "for dataset in combine:\n", "    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n", " \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n", "\n", "    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n", "    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n", "    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n", "    \n", "train_df[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()"], "metadata": {"_cell_guid": "ac8c3cc0-97c3-4e22-b71f-fa88513ec401", "collapsed": true, "_uuid": "d9abe52374fc95ade1869ec6bc572cbd07fa7611"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["# convert the categorical titles to ordinal.\n", "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n", "for dataset in combine:\n", "    dataset['Title'] = dataset['Title'].map(title_mapping)\n", "    dataset['Title'] = dataset['Title'].fillna(0)\n", "\n", "# Drop Name feature\n", "train_df = train_df.drop(['Name'], axis=1)\n", "test_df = test_df.drop(['Name'], axis=1)\n", "combine = [train_df, test_df]"], "metadata": {"_cell_guid": "10ffd8c4-8935-442b-9acb-67f1e31c8f2f", "collapsed": true, "_uuid": "77f02eebdcc54be0ce6891578ddec884625afcc7"}}, {"source": ["**Converting a categorical feature**"], "cell_type": "markdown", "metadata": {"_cell_guid": "f6c3c1a7-3a21-43ad-aa6e-115726f462d9", "_uuid": "d3f8b136a9a227015dcf5cf7e097a25a2468a973"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["for dataset in combine:\n", "    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)"], "metadata": {"_cell_guid": "69c7350b-9582-4b73-9f96-f620a6ffa141", "collapsed": true, "_uuid": "e10a47fb978909d67c8fd0041307df8275cf0973"}}, {"source": ["**Completing a numerical continuous feature**\n", "Guessing missing values is to use other correlated features. In our case we note correlation among Age, Gender, and Pclass. Guess Age values using [median](https://en.wikipedia.org/wiki/Median) values for Age across sets of Pclass and Gender feature combinations. So, median Age for Pclass=1 and Gender=0, Pclass=1 and Gender=1, and so on.."], "cell_type": "markdown", "metadata": {"_cell_guid": "dbde8748-399f-4e6f-a2ad-e0fcc33f62c3", "_uuid": "88e15bae7cc1e47512809e48e8b561b643143d48"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["train_df.head(10)"], "metadata": {"_cell_guid": "18ca18aa-a4cb-4d7a-838c-c905536c0016", "collapsed": true, "_uuid": "23964ecd8247da586f126c12cfbb3239c406a50e"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["guess_ages = np.zeros((2,3))\n", "for dataset in combine:\n", "    for i in range(0, 2):\n", "        for j in range(0, 3):\n", "            guess_df = dataset[(dataset['Sex'] == i) & \\\n", "                                  (dataset['Pclass'] == j+1)]['Age'].dropna()\n", "\n", "            age_guess = guess_df.median()\n", "\n", "            # Convert random age float to nearest .5 age\n", "            guess_ages[i,j] = int( age_guess/0.5 + 0.5 ) * 0.5\n", "            \n", "    for i in range(0, 2):\n", "        for j in range(0, 3):\n", "            dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == i) & (dataset.Pclass == j+1),\\\n", "                    'Age'] = guess_ages[i,j]\n", "\n", "    dataset['Age'] = dataset['Age'].astype(int)"], "metadata": {"_cell_guid": "50889aa1-79fd-4b6c-84cf-6939b136d455", "collapsed": true, "_uuid": "ea37a8821195761a1b46df4e25b673d96b09ad3f"}}, {"source": ["**Create age bands and save ordinals based on these bands**"], "cell_type": "markdown", "metadata": {"_cell_guid": "db5f751a-81da-4719-9462-ee2f3bc58d12", "_uuid": "3a6a2e9a14298bcbf4cd47832aa8bea942e763a7"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["for dataset in combine:    \n", "    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n", "    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n", "    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n", "    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n", "    dataset.loc[ dataset['Age'] > 64, 'Age']\n", "\n", "train_df.head()"], "metadata": {"_cell_guid": "698baf05-5aa8-436c-be27-4475c4f53d14", "collapsed": true, "_uuid": "08ba291d79a87430abdad32cd677942ab6dc4028"}}, {"source": ["**Create new feature for FamilySize which combines Parch and SibSp**"], "cell_type": "markdown", "metadata": {"_cell_guid": "18829816-0f09-4ddc-9381-7eb605df298a", "_uuid": "a359bfa5c2b717694da6f3945053a79ae1a91182"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["combine = [train_df, test_df]\n", "for dataset in combine:\n", "    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n", "\n", "train_df[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n"], "metadata": {"_cell_guid": "89557bcb-76e7-4719-a51e-5de4dcc43a82", "collapsed": true, "_uuid": "136599a894e9aa63790798d8228024e8be0e9f3f"}}, {"source": ["**Create another feature called IsAlone**"], "cell_type": "markdown", "metadata": {"_cell_guid": "ca5c6546-ec4f-470a-bae1-202fb3431c34", "_uuid": "d416995aedafc79ce304abbf56a75c5112a23fe0"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["for dataset in combine:\n", "    dataset['IsAlone'] = 0\n", "    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n", "\n", "train_df[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()"], "metadata": {"_cell_guid": "746d11fc-ad33-4e7f-90c8-d89529d3fcd2", "collapsed": true, "_uuid": "ed004e1927f2a7dac5c26d67a27533bb4a3f242a"}}, {"source": ["**Drop Parch, SibSp, and FamilySize features in favor of IsAlone**"], "cell_type": "markdown", "metadata": {"_cell_guid": "8816aa3c-5165-4c56-8ee4-e4b190143352", "_uuid": "6a5f2d2999d3206186f54d3b5556c544d3e5a687"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["train_df = train_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\n", "test_df = test_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\n", "combine = [train_df, test_df]"], "metadata": {"_cell_guid": "834217f2-10eb-4102-a6cf-b6193fb60961", "collapsed": true, "_uuid": "eebd7cb973868897f23e09acec8511cbe315c6bb"}}, {"source": ["**Create an artificial feature combining Pclass and Age**"], "cell_type": "markdown", "metadata": {"_cell_guid": "df5f5e3f-dd6b-4ee9-a55f-b4ea92b06842", "_uuid": "ce557b312eeca6994725e313f1f6b96e2a71d570"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["for dataset in combine:\n", "    dataset['Age*Class'] = dataset.Age * dataset.Pclass\n", "\n", "train_df.loc[:, ['Age*Class', 'Age', 'Pclass']].head(10)"], "metadata": {"_cell_guid": "a03d82a8-b21e-48f4-996f-2682ef61677c", "collapsed": true, "_uuid": "91b1b75e9394d8073f332e3c9bb206627381bb75"}}, {"source": ["**Completing a categorical feature**"], "cell_type": "markdown", "metadata": {"_cell_guid": "a02050b7-cf09-4e37-a7ab-7d42547907d5", "_uuid": "7cc011bf08a1db30aedc4b15d16777d28d5860d1"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["freq_port = train_df.Embarked.dropna().mode()[0]\n", "\n", "for dataset in combine:\n", "    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)\n", "    \n", "for dataset in combine:\n", "    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n", "\n", "test_df['Fare'].fillna(test_df['Fare'].dropna().median(), inplace=True)"], "metadata": {"_cell_guid": "429d1b4f-4e20-4fc9-9c6e-39dd1795eef5", "collapsed": true, "_uuid": "58842b049983edeea7a0474a8262a5f3ac7ce508"}}, {"source": ["**Fare feature to ordinal values based on the FareBand**\n"], "cell_type": "markdown", "metadata": {"_cell_guid": "3b0e0cdc-4d04-4619-8946-04d5d32729c1", "_uuid": "d0ec163c23a227a0ed5bf4008edb9d3263b9c8ca"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["for dataset in combine:\n", "    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n", "    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n", "    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n", "    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n", "    dataset['Fare'] = dataset['Fare'].astype(int)\n", "\n", "combine = [train_df, test_df]"], "metadata": {"_cell_guid": "9cfa08a9-6e01-473c-83df-0959e7070bf1", "collapsed": true, "_uuid": "d302da18b90a1bf66bb9e419c6eda5949002c39a"}}, {"source": ["**Save wrangled training and test data to files**"], "cell_type": "markdown", "metadata": {"_cell_guid": "3082f6fd-6fea-4e44-aa92-4f471e876e69", "_uuid": "8dcdb4e30ff15033e602f80823a3b5d1a1da0d9a"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["train_df.to_csv('train-wrangled.csv',index=False)\n", "test_df.to_csv('test-wrangled.csv',index=False)"], "metadata": {"_cell_guid": "db70fa48-c058-43cf-9d8b-b6b2235f8d1e", "collapsed": true, "_uuid": "886514bffad6b2f6bfef47b6974a842e54c5e596"}}, {"source": ["### Part 4 - Predict 'Survived' with Kears based on wrangled input data\n", "This part is more or less like part 1. Just with other input data."], "cell_type": "markdown", "metadata": {"_cell_guid": "41eed2a5-e5b4-4fc7-81b0-884047f7f4fd", "_uuid": "3d6d179db6f3acd9a3b4929437eff87a81caedf9"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["# Input files\n", "file_train='train-wrangled.csv'\n", "file_test='test-wrangled.csv'\n", "\n", "# read training data\n", "train_df = pd.read_csv(file_train,index_col='PassengerId')\n"], "metadata": {"_cell_guid": "c1e06520-94ff-46a4-ac26-564c6fe3d4b4", "collapsed": true, "_uuid": "34bd97d22195804c4397e2bf2e4e07711a72f29f"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["# Show the columns\n", "train_df.columns.values"], "metadata": {"_cell_guid": "11163a17-bd7d-4c46-9a76-f454767139ff", "collapsed": true, "_uuid": "6906012cf6ae2a6839e031ddfd0dbbb4270f4075"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["# Show the shape\n", "train_df.shape"], "metadata": {"_cell_guid": "3f31888e-0c8a-499a-9d1b-19747427849e", "collapsed": true, "_uuid": "5b833bf7d36edd164e85b045a485e43757785886"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["# preview the training dara\n", "train_df.head()"], "metadata": {"_cell_guid": "7093c3cc-ffb0-4311-815e-d6d41535e55b", "collapsed": true, "_uuid": "0d0d70f9a11bb6ed62ab3c305090580a4a49df77"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["# Show that there isn't any NaN data \n", "train_df.isnull().sum()"], "metadata": {"_cell_guid": "52f0ff74-42b2-46a8-bac9-b1e6fe2eb13e", "collapsed": true, "_uuid": "51541e17ad9622f51b0487b67ad1ac84eb8b932f"}}, {"source": ["Split training data into input X and output Y"], "cell_type": "markdown", "metadata": {"_cell_guid": "a2e0e609-5f36-4d8c-b236-b87f91ae3c71", "_uuid": "a578d1fab9df9ec0c1f73f385b7ecb60e7efea04"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["# X contains all columns except 'Survived'  \n", "X = train_df.drop(['Survived'], axis=1).values.astype(float)\n", "\n", "# It is almost always a good idea to perform some scaling of input values when using neural network models (jb).\n", "\n", "scale = StandardScaler()\n", "X = scale.fit_transform(X)\n", "\n", "# Y is just the 'Survived' column\n", "Y = train_df['Survived'].values"], "metadata": {"_cell_guid": "e63f3907-04a3-4fd4-b01a-e5d8ae7ddd90", "collapsed": true, "_uuid": "265ff04e2e10577efd5fe11eb9ab8a543ef0201a"}}, {"source": ["#### Run GridSearch, optionally\n", "GridSearch is very time consuming. \n", "Set `run_gridsearch = True ` in case you really want to run it.\n", "Or simply tune optimizers, inits, epochs and batches. \n"], "cell_type": "markdown", "metadata": {"_cell_guid": "61807a30-173f-453a-b580-9414844d4b47", "_uuid": "93a03018d85e2bfb5e738091306e28d20e13bb20"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["run_gridsearch = False\n", "\n", "if run_gridsearch:\n", "    \n", "    start_time = time.time()\n", "    if verbose: print (time.strftime( \"%H:%M:%S \" + \"GridSearch started ... \" ) )\n", "    optimizers = ['rmsprop', 'adam']\n", "    inits = ['glorot_uniform', 'normal', 'uniform']\n", "    epochs = [50, 100, 200, 400]\n", "    batches = [5, 10, 20]\n", "    \n", "    model = KerasClassifier(build_fn=create_model, verbose=verbose)\n", "    \n", "    param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=inits)\n", "    grid = GridSearchCV(estimator=model, param_grid=param_grid)\n", "    grid_result = grid.fit(X, Y)\n", "    \n", "    # summarize results\n", "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n", "    means = grid_result.cv_results_['mean_test_score']\n", "    stds = grid_result.cv_results_['std_test_score']\n", "    params = grid_result.cv_results_['params']\n", "    if verbose: \n", "        for mean, stdev, param in zip(means, stds, params):\n", "            print(\"%f (%f) with: %r\" % (mean, stdev, param))\n", "        elapsed_time = time.time() - start_time  \n", "        print (\"Time elapsed: \",timedelta(seconds=elapsed_time))\n", "        \n", "    best_epochs = grid_result.best_params_['epochs']\n", "    best_batch_size = grid_result.best_params_['batch_size']\n", "    best_init = grid_result.best_params_['init']\n", "    best_optimizer = grid_result.best_params_['optimizer']\n", "    \n", "else:\n", "    # pre-selected paramters\n", "    best_epochs = 200\n", "    best_batch_size = 5\n", "    best_init = 'glorot_uniform'\n", "    best_optimizer = 'rmsprop'"], "metadata": {"_cell_guid": "edcd66c3-7d93-417a-b9ce-0d7f1b6d3f35", "collapsed": true, "_uuid": "d90709c984203b44e22d762cd53584ba93258a48"}}, {"source": ["#### Build model and predit\n", "- Create a classifier with best parameters\n", "- Fit model \n", "- Predict 'Survived'"], "cell_type": "markdown", "metadata": {"_cell_guid": "e0add7d9-0ac4-4324-b725-b218f832face", "collapsed": true, "_uuid": "46ae33dcb659970ad144c0e8dbc005ee91e5ba61"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["# Create a classifier with best parameters\n", "model_pred = KerasClassifier(build_fn=create_model, optimizer=best_optimizer, init=best_init, epochs=best_epochs, batch_size=best_batch_size, verbose=verbose)\n", "model_pred.fit(X, Y)\n", "\n", "# Read test data\n", "test_df = pd.read_csv(file_test,index_col='PassengerId')\n", "\n", "# Create X_test\n", "X_test = test_df.values.astype(float)\n", "# Scaling\n", "X_test = scale.transform(X_test)\n", "\n", "# Predict 'Survived'\n", "prediction = model_pred.predict(X_test)\n", "\n"], "metadata": {"_cell_guid": "132aa84d-8a4d-4feb-a020-809b6373722e", "collapsed": true, "_uuid": "f117128b190dae0df5bc762a043a8c29d9b2f263"}}, {"source": ["Save predictions"], "cell_type": "markdown", "metadata": {"_cell_guid": "14c9a044-0c7b-49f5-b6c0-1da46ef304bd", "_uuid": "1382742e1362d7423d36099366de10302f6202b7"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": ["submission = pd.DataFrame({\n", "    'PassengerId': test_df.index,\n", "    'Survived': prediction[:,0],\n", "})\n", "\n", "submission.sort_values('PassengerId', inplace=True)    \n", "submission.to_csv('submission-manual-cleansing.csv', index=False)"], "metadata": {"_cell_guid": "f4b6db6a-ab36-4c86-8648-5a1a750fb037", "collapsed": true, "_uuid": "bbbae2912cb20f2c102192c85a3303106d9bdc9f"}}, {"source": ["#### Result part 4 - Predict 'Survived' with Kears based on wrangled input data\n", "Submitting the prediction csv will have scroe of ~78%. Not a good result :-("], "cell_type": "markdown", "metadata": {"_cell_guid": "fafc8eae-156e-4402-9505-58a3695ba249", "collapsed": true, "_uuid": "11d8d00c5fdb10754a1849ac7610cf24c0073d15"}}, {"source": ["## Summary\n", "The results are not really great. Either the simple multilayer perceptrons in this notebook are too simple for solving the prediction problem or the problem is not suited for deep learning and therefore other other algorithms (SVM, decision tree) are just better. At least it was fun playing with Keras.\n", "\n", "Your feedback and suggestions are very welcome."], "cell_type": "markdown", "metadata": {"_cell_guid": "3ea84fe7-8de9-4959-ae56-670e9ad68771", "_uuid": "460c43128592a17f328517b21efe2b63b24af893"}}, {"execution_count": null, "outputs": [], "cell_type": "code", "source": [], "metadata": {"_cell_guid": "f3eb3a47-6a12-44a8-9f9e-225da7a1a032", "collapsed": true, "_uuid": "c30bc17c3074c93094eb20934118f37bbae0210f"}}], "nbformat": 4, "metadata": {"kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "name": "python", "mimetype": "text/x-python", "file_extension": ".py", "nbconvert_exporter": "python", "version": "3.6.3", "pygments_lexer": "ipython3"}, "anaconda-cloud": {}}, "nbformat_minor": 1}