{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true,"_kg_hide-output":true},"cell_type":"markdown","source":"##################################################################\n#   Titanic - Learning From Disaster\n<br>\n###############################################################\n\n<br>\n\n\n* autor   : Alexandre Gazagnes\n* date    : 25/08/2018\n* commit  : v11\n\n<br>\n\nThought as a complete study of a data science project, this kernel does not only provide a turnkey solution but also provides a detailed study of the different steps needed to provide a good answer. Thus in the image of a mathematical demonstration, the approach and logic interest us more than the final solution (accuracy score of 0.89).\n\n<br>\n\nThe Titanic dataset is a very interesting dataset for several reasons:\n* if \"good\" results can be achieved easily (accuracy score 0.75-0.80), the improvement of these results appears very difficult\n* the dataset provided has a very low number of features, some of which are very complex to deal with\n* the pushing feature engineering approach is the key (as always) in this project, but the complexity and number of feature engineering strategies clearly poses a problem for having an accuracy score greater than 0.85\n\n<br>\n\nHere we will discuss the steps involved in a true Data Science project.\n* import and management of datasets\n* visualization\n* cleaning\n* outlier's management\n* not so basic and advanced engineering feature\n* how to implement a rigorous approach to navigate the feature engineering strategies\n* how to set up a rigorous approach for model selection and meta parametres selection\n* dummy / naive models\n* Logistic regression (and how to increase the accuracy score of 7-10% thanks to the feature engineering)\n* ML tricks as result clipping, pseudo-encoding, data increase (very common in DL btw)\n* Random Forests (and how to increase the accuracy score by 10-15% thanks to the feature engineering).\n"},{"metadata":{"_uuid":"121c022adc2c1b9f9115761b784bbf7b74d034ea"},"cell_type":"markdown","source":"**Import**\n--------------------------------------------------------------\n"},{"metadata":{"trusted":true,"_uuid":"35cffcd922ebef744bce4cd2e40401ce9306ee25","collapsed":true},"cell_type":"code","source":"import os, sys, logging, random, time\nfrom math import ceil\nimport itertools as it\nfrom collections import OrderedDict, Iterable\n\nimport pandas as pd\nimport numpy as np\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler, Normalizer, MinMaxScaler\n\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import Perceptron, RidgeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import LinearSVC, NuSVC\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.utils import shuffle\n\nprint(os.listdir(\"../input\"))\nPATH = \"../input/\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9060f687fb2aabe8a44475ad5169288834c7d68a"},"cell_type":"markdown","source":"**Logging and warning**\n--------------------------------------------------------------\n"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"72be3b33098384e7f0f6a75d722696532347484c"},"cell_type":"code","source":"# logger = logging.getLogger()\n# logger.setLevel(logging.CRITICAL)\nl = logging.INFO\nlogging.basicConfig(level=l, format=\"%(levelname)s : %(message)s\")\ninfo = logging.info\n\n# import warnings\n# warnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0cc5d4b4b2a472904fd4714f9ecd3d79896c09ce"},"cell_type":"markdown","source":"**Constants**\n--------------------------------------------------------------\n"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"31c02f1089d0aa6cf726184a8e2db4c2e6a47bcb"},"cell_type":"code","source":"PROJECT     = \"Kaggle-Titanic_Machine_Learning_From_Disaster\"\nDATA        = \"data\"\nSUBMISSIONS = \"submissions\"\nTRAIN_FILE  = \"train.csv\"\nTEST_FILE   = \"test.csv\"\nN_1         = 20\nN_2         = 10\nN_3         = 30\nN_4         = 50 \nTEST_SIZE   = 25\nCV          = 5","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0999db64020f0fb7899879101b963c22b39b9cd4"},"cell_type":"markdown","source":"**Graph settings**\n--------------------------------------------------------------\n"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"dd3ce6c0c63850c2501c5a7c7d8b2b2ee5499b38"},"cell_type":"code","source":"%matplotlib inline\n# get_ipython().magic('matplotlib inline')\nsns.set()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ef85a90851d2fe13d874bbb13f85264772b4e504"},"cell_type":"markdown","source":"**Code conventions**\n--------------------------------------------------------------\n"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7ee75c333383b021b17e0da8949e5cdab42f4682"},"cell_type":"markdown","source":"## About functions\nWherever it will be possible, we will use function rather than just normal command lines  it is not the dominant practice, sorry for that :)\n\n## About name scopes\nWhatever the scope of a variable, as far as possible, if a variable is named \"x\" in any scope and we want to work on a copy of \"x\" it will be called \"_x\"\nas far as possible, inside a function, if it is a question of modifying the iniltial dataframe, an internal copy will be made:\n```python\ndef do_something_to(df) : \n    _df = df.copy()\n    _df = _df.map(my_function)\n    return _df\n    \ndf = do_something_to(df)\n```\n\n## About 'for loops'\nFor obvious reasons 'for loops' ('for i in list :' AND list comprehensions) sould not be used with a pd.DataFrame object but considering code readability, some 'for loops' can be found in the lines bellow. You are free to delete these awful - but readable - lines :) "},{"metadata":{"_uuid":"1ab89976db3cd996222ab513745db7d4363ffd85"},"cell_type":"markdown","source":"**00-first_dataset_tour.py**\n--------------------------------------------------------------\n"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"703e68b2770037221417bdfec50b5a0ba587f74c"},"cell_type":"code","source":"################################################################################\n\n# ----------------------------------------------------------\n# 00-first_dataset_tour.py\n# ----------------------------------------------------------\n\n################################################################################\n\n\n\n# Find here a first study of the dataset, in which we try to understand and\n# give meaning to the dataset.\n\n# We are not trying to solve our problem but to be focused on visualization,\n# clenaning and all feature engineering improvements.\n\n# At first we will just study the corelations, the links, the quality and the\n# meaning of our dataset. External research and more general considerations may \n# be included in this work","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"29258ec670bee0f174a6fde62b963fff06a11275"},"cell_type":"markdown","source":"**Dataframe creation **\n---------------------------------------\n"},{"metadata":{"trusted":true,"_uuid":"d8c8422e0e585d17a3a301a07bc04a268375f52f","collapsed":true},"cell_type":"code","source":"# our first function designed to init a dataframe form a file\n\ndef init_df(path, file, precast=False) : \n\n    # init dataframe\n    df = pd.read_csv(path+file, index_col=0)\n\n    # if train df\n    if len(df.columns)  == 11 : \n        df.columns  = pd.Index( [   \"target\", \"pclass\",\"name\", \"sex\", \"age\",\n                                    \"sibsp\",\"parch\",\"ticket\",\"fare\",\"cabin\",\n                                    \"embarked\"], dtype=\"object\")\n    # if test df \n    elif len(df.columns )  == 10 : \n        df.columns  = pd.Index( [   \"pclass\",\"name\", \"sex\", \"age\",\n                                    \"sibsp\",\"parch\",\"ticket\",\"fare\",\"cabin\",\n                                    \"embarked\"], dtype=\"object\")\n    else : \n        raise ValueError(\"invalid numb of columns\")\n\n    # if needed, change sex and embarled feature in int dtype\n    if precast : \n        sex_dict        = {\"male\":1, \"female\":0}\n        embarked_dict   = {\"S\":2, \"C\":1, \"Q\":0}\n\n        df[\"sex\"]        = df.sex.map(sex_dict)\n        df[\"embarked\"]   = df.embarked.apply(lambda x : x if x not in [\"S\", \"C\", \"Q\"] else embarked_dict[x] )\n\n    return df\n\n####\n\n# train and test\ntrain_df = init_df(PATH, TRAIN_FILE)\ntest_df = init_df(PATH, TEST_FILE)\n\n# for nas : concat train and test df\nboth_df = train_df.copy().append(test_df)\n\n# we will work on train to have target feature\ndf = train_df.copy()\n\n# it could be good to keep a copy of our original df\nDF = df.copy()\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"52892bec7f0c44b9c435a0ef3a8e047220656cfd"},"cell_type":"markdown","source":"**Data exploration and visualization**\n---------------------------------------------------\n"},{"metadata":{"trusted":true,"_uuid":"5e01eac28e77a6415e4512161b4b85eb30c85ad4","collapsed":true},"cell_type":"code","source":"# let's have a first tour of our dataframe with some old print functions\ndef study_global_df(df) :     \n    print(\"data frame dimension :       \")\n    print(df.ndim)\n    print(\"\\n\\ndata frame shape :       \")\n    print(df.shape)\n    print(\"\\n\\ndata frame types :      \")\n    print(df.dtypes)\n    print(\"\\n\\ndata frame index :       \") \n    print(df.index)\n    print(\"\\n\\ndata frame columns :     \")\n    print(df.columns)\n    print(\"\\n\\ndata frame info :     \")\n    print(df.info())\n\n\n    \n####\n\nstudy_global_df(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fb14152b1f27ca2c8806169eee56266a45b1de79","collapsed":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9eea9756508eff9a543dd88251a52a3fbbf2045f","collapsed":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"56b2db1b5f5e3addbcff8db13711295ded8871f6","collapsed":true},"cell_type":"code","source":"# visualisation\ndef visualize_global(df) : \n    df.hist(grid=True,bins=50, figsize=(10,10))\n    # sns.pairplot(df)\n\n    \n####\n\nvisualize_global(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d8cd8e42644e90ea0ed0dbc8feaaa978b1f62d2b","collapsed":true},"cell_type":"code","source":"# in order to have sex and embarked \ndef precast_df(df) : \n        # DO NOT FORGET TO MAKE A COPY :):)\n        _df = df.copy()\n        sex_dict        = {\"male\":1, \"female\":0}\n        embarked_dict   = {\"S\":2, \"C\":1, \"Q\":0}\n\n        _df[\"sex\"]        = _df.sex.map(sex_dict)\n        _df[\"embarked\"]   = _df.embarked.apply(lambda x : x if x not in [\"S\", \"C\", \"Q\"] else embarked_dict[x] )\n\n        return _df        \n    \n####\n\n_df = precast_df(df)\nvisualize_global(_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6179c4470fc928f3bbe5b4ad48cd03d8fb401b9b","collapsed":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cfb6704b19bfb5ab62ff3bf6d3f2ac0d78bacbb8","collapsed":true},"cell_type":"code","source":"# pclass, sex, and embarqued are definitively categorcial features, so let's work on this\ndef visualize_global_cat(df) : \n    for feat in [\"embarked\", \"sex\", \"pclass\"] : \n        sns.factorplot(feat,'target', data=df,size=4,aspect=3)\n    \n    # other way : \n    # fig, (ax1,ax2, ax3) = plt.subplots(1,3,figsize=(15,5))\n    # for ax, feat in zip((ax1,ax2, ax3), [\"embarked\", \"sex\", \"pclass\"]) :\n    #     data = df[[feat, \"target\"]].groupby([feat],as_index=False).mean()\n    #     sns.barplot(x=feat, y='target', data=data, size=4,aspect=3, ax=ax)\n\n####\n\nvisualize_global_cat(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"68ab9d2e6edfa8576a28793b7a0b4a92b57dd21a","collapsed":true},"cell_type":"code","source":"# can we learn something with continuous features ?\ndef visualize_global_continuous(df) : \n    for feat in [\"age\", \"fare\", \"sibsp\", \"parch\" ] : \n        data = pd.concat([pd.cut(df[feat], 11, labels=range(11)), df[\"target\"]], axis=1)\n        # data.columns = [feat, \"target\"]\n        sns.factorplot(feat, \"target\", data=data, size=4,aspect=3)\n\n####\n\nvisualize_global_continuous(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1afbe8900b62f9a96c2b86eb7e2e8ab962b6b6a3","collapsed":true},"cell_type":"code","source":"# let's go deeper and visualize our categorical features in depth\ndef visualize_depth_cat_1(df) : \n    for feat in [\"embarked\", \"sex\", \"pclass\"] : \n        fig, axs = plt.subplots(1,3,figsize=(15,5))\n        sns.countplot(x=feat, data=df, ax=axs[0])\n        sns.countplot(x='target', hue=feat, data=df, order=[1,0], ax=axs[1])\n        data = df[[feat, \"target\"]].groupby([feat],as_index=False).mean()\n        sns.barplot(x=feat, y='target', data=data,ax=axs[2])\n\n####\n\nvisualize_depth_cat_1(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"33447e6e40c79492d7f50a007188d39b441fd63b","collapsed":true},"cell_type":"code","source":"def visualize_depth_cat_2(df) :\n    for feat in [\"sex\", \"pclass\", \"embarked\"] : \n        sns.factorplot(\"target\", col=feat, col_wrap=4,\n                    data=df, kind=\"count\", size=3.5, aspect=.8)\n        \n####\n\nvisualize_depth_cat_2(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e84db0153613e3dbbffe01139d1d3739b21cb728","collapsed":true},"cell_type":"code","source":"def visualize_depth_continuous_2(df) : \n    for feat in  [\"fare\", \"age\", \"sibsp\", \"parch\"] : \n        facet = sns.FacetGrid(df, hue=\"target\",aspect=4)\n        facet.map(sns.kdeplot,feat,shade= True)\n        facet.set(xlim=(0, df[feat].max()))\n        facet.add_legend()\n\n####\n\nvisualize_depth_continuous_2(df)      ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"95fc8615ca8e6f95c75d4bf581a17dd799bc6516","collapsed":true},"cell_type":"code","source":"def just_another_fancy_graph_1(df) : \n    fig, axs = plt.subplots(1,3, figsize = (20,5))\n    feats = [\"embarked\", \"sex\", \"pclass\"]\n\n    for feat, ax in zip(feats, axs) : \n        for f in df[feat].unique() : \n            age = df[~df[\"age\"].isnull()]\n            age = age[age[feat] == f]\n            sns.distplot(age[\"age\"], bins=50, ax=ax)\n            plt.legend(df[feat].unique(),loc='best')\n            plt.title(feat)\n\n####\n\njust_another_fancy_graph_1(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ce2508fbba110b27bb0a4ed897faada87f89d167","collapsed":true},"cell_type":"code","source":"def just_another_fancy_graph_2(df) :\n    g = sns.FacetGrid(df, col=\"sex\", row=\"target\", margin_titles=True)\n    g.map(plt.hist, \"age\",color=\"purple\");\n\n####\n\njust_another_fancy_graph_2(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"859132695c5800f6029ff0846254fc228127fea8","collapsed":true},"cell_type":"code","source":"def just_another_fancy_graph_3(df) :\n    fig, ax = plt.subplots(1,1, figsize=(20,5))\n    sns.boxplot(x=\"embarked\", y=\"age\", hue=\"pclass\", data=df, ax=ax);\n\n####\n\njust_another_fancy_graph_3(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"41df2f42057166ef08b7c611758939f00d0e881e","collapsed":true},"cell_type":"code","source":"# finally let's see the correlation matrix\n\ndef graph_corr_matrix(df) : \n    \n    fig, ax = plt.subplots(1,1,figsize=(15,15))\n    corr_mat = df.corr()\n    sns.heatmap(corr_mat, cmap=\"coolwarm\", annot=True, fmt='.3g', ax=ax)\n    plt.title(\"correlation matrix\")\n    \n####\n\ngraph_corr_matrix(df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"834afed02cb32b6bc502dc0c48fa6afdefe60566"},"cell_type":"markdown","source":"**Data cleaning**\n--------------------------------------------\n"},{"metadata":{"trusted":true,"_uuid":"4f2f4e63093782f8b6074345d57d38ac9fe7014b","collapsed":true},"cell_type":"code","source":"def study_nas(df): \n    total = df.isnull().sum().sort_values(ascending=False)\n    percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False).round(3)\n    return pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n\n####\n\nstudy_nas(both_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2ad98575d70beea7748ea58fa60e451f71587ae7","collapsed":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0c5c8da3361ab208c6c733667d8d608a98807e99","collapsed":true},"cell_type":"code","source":"# 2 nas for embarked and 1 for fare, easy...\n\ndef fill_embarked(df) :\n    _df = df.copy()\n    _df.loc[_df.embarked.isna(), : ]\n    try : \n        _embarked = _df.embarked.value_counts().sort_values(ascending=False).index[0]\n        _df[\"embarked\"] = _df.embarked.fillna(_embarked)\n    except: \n        pass\n    return _df\n\ndef fill_fare(df) : \n    _df = df.copy()\n    _pclass =  int(_df.loc[_df.fare.isna(),\"pclass\"].values)\n    try : \n        val = _df.loc[_df.pclass == _pclass, \"fare\"].median()\n        _df[\"fare\"] = _df.fare.fillna(val)\n    except : \n        pass\n    return _df\n\n####\n\n_both_df = fill_embarked(fill_fare(both_df))\nstudy_nas(_both_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"05f34567d96748c3e20b4630b677527e7ffb185e","collapsed":true},"cell_type":"code","source":"# but for age it is a much more complex problem! \n# we 20% of nas and know due to obvious logic and due to our data visualization that this a \n# very important feature\n# first thing to do could be a statistical fillin strategy based of pclass, sex and embarked\n\ndef fill_age_easy(df) : \n    _df = df.copy()\n    \n    idxs = _df.loc[_df[\"age\"].isna(), : ].index\n\n    # using 'for loop' with df is (strongly)  recommanded\n    # this 'for  loop' is just here to increase code readability \n    for i in idxs : \n        pers = _df.loc[i, :]\n\n        mask_1 = _df[\"pclass\"]    == pers[\"pclass\"]\n        mask_2 = _df[\"embarked\"]  == pers[\"embarked\"]\n        mask_3 = _df[\"sex\"]       == pers[\"sex\"]\n\n        mask = mask_1 & mask_2 & mask_3\n        sub_df = _df.loc[mask, :]\n\n        if len(sub_df) > 100 : \n            age_mean = sub_df.age.mean()\n            age_std = sub_df.age.std()\n        \n        else : \n            mask = mask_1 & mask_3\n            sub_df = _df.loc[mask, :]\n\n            if len(sub_df) > 100 : \n                age_mean = sub_df.age.mean()\n                age_std = sub_df.age.std()\n\n            else : \n                mask = mask_1 \n                sub_df = _df.loc[mask, :]\n                age_mean = sub_df.age.mean()\n                age_std = sub_df.age.std()\n\n        # define a random age based on the specific norma distr of our filtered samples\n        age = np.random.randint(age_mean - age_std, age_mean + age_std)\n        _df.loc[i, \"age\"] = int(age)\n\n    return _df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"91cf3dd4e397523f08f8a0e74af58a4a11db6422"},"cell_type":"code","source":"# a much more complex way to fill age - but maybe much more\n# correct - could to use a ML algo to predict good values of age\n# we will not implement this right now but it could be something like this \n\ndef fill_age_hard(df) : \n\n    _df = df.copy()\n\n    # separate target and other features \n    features_list = [\"target\", \"cabin\", \"name\", \"ticket\", \"embarked\"]\n    droped_features = df.loc[:,features_list ]\n    _df = df.drop(features_list, axis=1)\n\n    # sep train and test df\n    _train_df = _df.loc[df[\"age\"].isna(), :]\n    _test_df  = _df.loc[~df[\"age\"].isna(), :]\n    print(_train_df.shape, _test_df.shape)\n\n    # split X_train, X_test...\n    X, y = _train_df.drop(\"age\", axis=1), _train_df.age\n    X_train, X_test, y_train, y_test = train_test_split(X,y)\n    print([i.shape for i in (X_train, X_test, y_train, y_test) ])\n\n    model_list = [LogisticRegression]\n    for m in model_list : \n        grid = GridSearchCV(m(), {}, cv=5)\n        grid.fit(X_train, y_train)\n        acc = accuracy_score(grid.predict(X_test), y_test)\n        print(acc)\n\n    pred_ages = grid.predict(_test_df).astype(np.uint32)\n    _test_df[\"age\"] =  pred_ages\n\n\n    # merge df with droped features \n    for f in droped_features.columns : \n        _df[f] = droped_features[f] \n\n    return _df\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e1a1eea983b6f2ddee45f881a7edfa2e4c5d33a8","collapsed":true},"cell_type":"code","source":"# finally we can define our fill_nas function, keeping in mind that our fill_age could be strongly improved\n# and that we should have our train df and our test df in args\n\ndef fill_nas(df, df2=None,  \n                 embarked_meth=fill_embarked, \n                 fare_meth=fill_fare,\n                 age_meth=fill_age_easy) :\n\n    # merge 2 df if needed\n    if isinstance(df2, Iterable) : \n        idx_1 = df.index\n        idx_2 = df2.index\n        _df = df.copy().append(df2)\n    else : \n        _df = df.copy()\n        \n    #  fill embarked, age, and fare\n    _df = embarked_meth(_df)\n    _df = fare_meth(_df)\n    _df = age_meth(_df)\n    \n    # if needed re-split train_df and test_df\n    if isinstance(df2, Iterable) : \n        df_1 = _df.loc[idx_1, :]\n        df_2 = _df.loc[idx_2, :]\n        return df_1, df_2\n    else :\n        return _df\n####\n\n# please not the 2 examples are the same\n_train_df, _test_df =  fill_nas(train_df, test_df)\n_both_df = fill_nas(both_df)\n# you just need to re-split both_df in train_df and test_df\n\ndf = _train_df.copy()\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"52a8d4074bc950e22e8315ba209d1e454015a09a","collapsed":true},"cell_type":"code","source":"study_nas(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"43b1668a3ed95844f2b2fdd98b718fea97dfdc28","collapsed":true},"cell_type":"code","source":"# how many non unique values for each feature?\ndef count_unique(df) : \n    data = pd.DataFrame([len(df[feat].unique()) for feat in df.columns], columns=[\"unique values\"], index=df.columns)\n    return data.sort_values(by=\"unique values\", ascending=False)\n\n####\n\ncount_unique(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8f742c9f3e0d0231934ed23b90a609f4890db7d0","collapsed":true},"cell_type":"code","source":"# let's have a look to unique - non continuous- values\n\ndef study_unique(df) : \n\n    col = [i for i in df.columns if i not in (\"age\", \"ticket\", \"name\", \"cabin\", \"fare\")]\n    ser = pd.DataFrame( \n            [ (df[i].unique() if len(df[i].unique())<20 else \"too many values\", \n              df[i].dtype) for i in col], index=col, columns=[\"unique\", \"dtype\"])\n    return ser\n\n\n####\n\nstudy_unique(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cab045f774ef93ab421fb46bf6ccdb4926ce2711","collapsed":true},"cell_type":"code","source":"def detect_outliers(df) : \n\n    # see global fare info\n    print(df.describe().fare)\n\n    print(df.fare.sort_values(ascending=True).iloc[:15])\n\n    # plot info\n    fig, axs = plt.subplots(2,1, figsize=(20, 5))\n    df.fare.hist(bins=100, ax=axs[0])\n    _df = df.loc[df.fare <=50.0, :]\n    _df.fare.hist(bins=20, ax=axs[1])\n\n    # regul fare >250\n    print(df.fare.sort_values(ascending=False).iloc[:10])\n\n\n####\n\ndetect_outliers(both_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d59f16a33a8c826629ffe3d9522f0197cfb127b2","collapsed":true},"cell_type":"code","source":"# handle outliers\ndef manage_fare_outliers(df, small=True, high=False) : \n    _df = df.copy()\n\n    if small : \n        # regul fare == 0\n        idxs = _df.loc[df[\"fare\"]<1.0 , : ].index\n        \n        for i in idxs : \n            pers = _df.loc[i, :]\n            _pclass = int(pers.pclass)\n            sub_df = _df.loc[_df.pclass == _pclass, :]\n            fare_mean = sub_df.fare.mean()\n            fare_std = sub_df.fare.std()\n\n            _fare = np.random.randint(fare_mean - fare_std, fare_mean + fare_std)\n            _df.loc[i, \"fare\"] = int(_fare)\n\n    if high : \n        _df.loc[_df.fare >260, \"fare\"] = 260\n\n    return _df\n\n####\n\n_df = manage_fare_outliers(df)\n_df.loc[_df.fare == 0, :]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"659f76fea841e64f23928e25f5e0e4997a46bdff","collapsed":true},"cell_type":"code","source":"detect_outliers(_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3ceeab73f7b5c37de954ed1fc11f8d01568614c4","collapsed":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a418bde8fd42839897aa2e54ab12fa4fbad6dedd"},"cell_type":"markdown","source":"**Feature engineering**\n---------------------------------------\n"},{"metadata":{"trusted":true,"_uuid":"e59a37ad5639c291c974425dce1e6fa96fa98ded","collapsed":true},"cell_type":"code","source":"# we now are going to group every people with the same ticket\n# we can consider that there are together, as family or as goup of friends\n# this is a very important information because if we are able to \"regroup\" families / freinds we could enhance \n# significatively our level of information\n\ndef group_ticket(df, threshold=2, reg_fare=False) : \n    \n    _df = df.copy() \n\n    # handle wierd tickets \n    _df.loc[df.ticket == \"LINE\", \"ticket\"] = 'LINE -1'\n      \n    _df[\"group_id\"] = np.nan\n    _df[\"group_count\"] = np.nan\n\n    # sep nb and letters\n    _df[\"ticket_nb\"]  = _df.ticket.apply(lambda i : int(i) if \" \" not in i else int(i[i.rfind(\" \")+1 :]))\n    _df[\"ticket_let\"] = _df.ticket.apply(lambda i : \"Nan\" if \" \" not in i else i[ : i.rfind(\" \")])\n    # len(df[\"_ticket_nb\"]) == len(df[\"_ticket_nb\"].unique()) \n    # ticket nb non unique!!! snif snif grrr grrrrr\n    \n    # we need to simplify this feature\n    print(_df.ticket_let.unique().sort())\n    _df[\"ticket_let\"] = _df.ticket_let.apply(lambda i : i.replace(\".\", \"\"))\n\n    ticket_dict = { 'A/4'          : \"A4\",\n                    'A/5'          : \"A5\",\n                    'A/S'          : \"AS\",\n                    'A4'           : \"A4\",\n                    'A5'           : \"A5\",\n                    'C'            : \"C\",\n                    'CA'           : \"CA\",\n                    'CA/SOTON'     : \"CA\",\n                    'FC'           : \"FC\",\n                    'FCC'          : \"FCC\",\n                    'Fa'           : \"FA\",\n                    'LINE'         : \"LINE\",\n                    'Nan'          : np.nan,\n                    'P/PP'         : \"PP\",\n                    'PC'           : \"PC\",\n                    'PP'           : \"PP\",\n                    'SC'           : \"SC\",\n                    'SC/A4'        : \"A4\",\n                    'SC/AH'        : \"AH\",\n                    'SC/AH Basle'  : \"Basle\",\n                    'SC/PARIS'     : \"PARIS\",\n                    'SC/Paris'     : \"PARIS\",\n                    'SCO/W'        : \"SCO\",\n                    'SO/C'         : \"SOC\",\n                    'SO/PP'        : \"PP\",\n                    'SOC'          : \"SOC\",\n                    'SOP'          : \"SOP\",\n                    'SOTON/O2'     : \"STON\",\n                    'SOTON/OQ'     : \"STON\",\n                    'SP'           : \"SP\",\n                    'STON/O 2'     : \"STON\",\n                    'STON/O2'      : \"STON\",\n                    'SW/PP'        : \"PP\",\n                    'W/C'          : \"WC\",\n                    'WE/P'         : \"WEP\",\n                    'WEP'          : \"WEP\"}\n\n    _df[\"ticket_let\"] = _df.ticket_let.map(ticket_dict)\n\n    # how many tickets per pers\n    group_count = _df.ticket.value_counts().to_dict(ticket_dict)\n    _group_count = [(i,j) for i,j in group_count.items() if j>=threshold]\n    _group_count.sort(key=lambda i : i[1], reverse=True)\n\n    # add group id and group count\n    for i, tup in enumerate(_group_count) : \n        t, c = tup\n        idxs = _df.loc[df.ticket == t, :].index\n        _df.loc[idxs,\"group_id\"] = i\n        _df.loc[idxs,\"group_count\"] = c\n\n    # regularize fare if needed\n    if reg_fare : \n        idxs = _df.loc[~_df.group_count.isna(), :].index        \n        _df.loc[idxs, \"fare\"] =  _df.loc[idxs, \"fare\"] / _df.loc[idxs, \"group_count\"] \n\n    # fill na group_count = 1\n    _df[\"group_count\"] = _df.group_count.fillna(1)\n    \n    _df = _df.drop(\"ticket\", axis=1)\n    \n    return _df\n\n####\n\n_df = group_ticket(df)\n_df.ticket_let.value_counts(dropna=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"65d9b7b973c64631e8b0c1531f90e342b0f33134","collapsed":true},"cell_type":"code","source":"print(DF.ticket.head())\n_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f1a003bd32a7ca85f2af72324a977c6c31457280","collapsed":true},"cell_type":"code","source":"_df.loc[_df.group_count.isna(), : ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"490f89a2413fff53e98efa2e9810aadc8c55deca","collapsed":true},"cell_type":"code","source":"sub_df = df.ticket.value_counts().sort_values(ascending=False)\npd.DataFrame(sub_df[sub_df>2], columns=[\"ticket\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d9156c398dd794b029aec18f2332afb4f8d5de0","collapsed":true},"cell_type":"code","source":"def vizualize_group_count_1(df) : \n    fig, (axis1, axis2, axis3) = plt.subplots(1,3,figsize=(15,5))\n    feat = \"group_count\"\n    sns.countplot(x=feat, data=df, ax=axis1)\n    sns.countplot(x='target', hue=feat, data=df, order=[1,0], ax=axis2)\n    data = df[[feat, \"target\"]].groupby(df[feat],as_index=False).mean()\n    sns.barplot(x=feat, y='target', order=sorted(df.group_count.unique()), data=data,ax=axis3)\n    \n####\nprint(_df.columns)\nvizualize_group_count_1(_df) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"99b3af89ad6770e236289608c1d4b954be175375","collapsed":true},"cell_type":"code","source":"def vizualize_group_count_2(df): \n    feat = \"group_count\"\n    facet = sns.FacetGrid(df, hue=\"target\",aspect=4)\n    facet.map(sns.kdeplot,feat,shade= True)\n    facet.set(xlim=(0, df[feat].max()))\n    facet.add_legend()\n    \n####\n\nvizualize_group_count_2(_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2f7f1072f8237c00a17a2121b2112afd298c18b7","collapsed":true},"cell_type":"code","source":"def vizualize_group_count_3(df): \n    feat= \"group_count\"\n    g = sns.factorplot(\"target\", col=feat, col_wrap=4,\n                        data=df[df[feat].notnull()],\n                        kind=\"count\", size=2.5, aspect=.8)\n    \n####\n\nvizualize_group_count_3(_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"80bad6422c17bd041b40416b18f38859eea86633","collapsed":true},"cell_type":"code","source":"# what about names? \n# obiouvsly we can separate first name, last name and name title\n\ndef sep_names(df) :\n    _df = df.copy()\n    \n    # create a tmp feat\n    _df[\"name_len\"] = _df.name.apply(lambda i : len(i))\n    _df[\"_name\"]    = _df.name.apply(lambda i : i.split(\", \"))\n    _df[\"_name\"]    = _df[\"_name\"].apply(lambda i : [    i[0],\n                                                    i[1][: i[1].find(\".\")], \n                                                    i[1][i[1].find(\" \"):]   ]   )\n\n    # split last, first, title\n    _df[\"name_last\"]     = _df[\"_name\"].apply(lambda i : i[0])\n    _df[\"title\"]         = _df[\"_name\"].apply(lambda i : i[1])\n    _df[\"name_first\"]    = _df[\"_name\"].apply(lambda i : i[2])\n\n    # countness ?\n    idx = _df.loc[_df.title == \"the Countess\", :].index\n    _df.loc[idx, \"title\"] = \"Countess\"\n    _df.loc[idx, \"name_first\"] = \"Lucy Noel Martha Dyer-Edwards\"\n\n    # sep spouce/second name : \n    _df[\"name_second\"]   = _df.name_first.apply(lambda i : i[i.find(\"(\")+1 : ] if \"(\" in i else np.nan)\n    _df[\"name_first\"]    = _df.name_first.apply(lambda i : i[: i.find(\" (\")] if \"(\" in i else i)\n\n    # #################################   \n    # df[\"name_last_count\"] = np.nan\n    ###################################   \n    \n    # clean :\n    items = [\"name_first\", \"name_second\", \"name_last\"]\n    def clean(i) : \n        try     : return i.replace(\")\", \"\").replace('\"\"', \"\").replace('\"', \"\").replace(\"'\", '')\n        except  : return i\n\n    for item in items : \n        _df[item] = _df[item].apply(lambda i : clean(i))\n        # more pandastic \n        # _df[item] = _df[item].map(clean)\n        \n    _df = _df.drop([\"name\", \"_name\"], axis= 1)\n\n    return _df\n\n####\n\n_df = sep_names(df)\n_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b04a8ed34fab15326a571dac715a94729f404071","collapsed":true},"cell_type":"code","source":"# 20th most poular names\ndef most_popular_names(df) : \n    _df = sep_names(df)\n    ser = _df.name_last.value_counts().sort_values(ascending=False)[:20]\n    return pd.DataFrame({\"20th most poular names\":ser})\n\n####\n\nmost_popular_names(both_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1563a0e8d7049e384e53df9e292224800119a525","collapsed":true},"cell_type":"code","source":"def visualize_name_len_1(df) : \n    _df = sep_names(df)\n    feat = \"name_len\"\n    facet = sns.FacetGrid(_df, hue=\"target\",aspect=4)\n    facet.map(sns.kdeplot,feat,shade= True)\n    facet.set(xlim=(0, _df[feat].max()))\n    facet.add_legend()\n    \n####\n\nvisualize_name_len_1(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fc2c51b885982a9a21772b0683ddf16014a4e931","collapsed":true},"cell_type":"code","source":"def visualize_name_count(df) : \n    _df = sep_names(df)\n    feat = \"name_count\"\n    facet = sns.FacetGrid(_df, hue=\"target\",aspect=4)\n    facet.map(sns.kdeplot,feat,shade= True)\n    facet.set(xlim=(0, _df[feat].max()))\n    facet.add_legend()\n    \n####\n#_df = group_ticket(df)\n# visualize_name_count(_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3dac85bea924196ec96b374748bb8893a65c3fd6","collapsed":true},"cell_type":"code","source":"# titles\ndef study_titles(df) : \n    _df = sep_names(df)\n    return pd.DataFrame({\"title_freq\": _df.title.value_counts(normalize=True).round(2), \"title_nb\": _df.title.value_counts(),})\n\n####\n\nstudy_titles(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"653feff9ae0b57e838042b0d18cbb3483e684286","collapsed":true},"cell_type":"code","source":"# second names?\ndef study_second_names(df) : \n    _df = sep_names(df)\n    fem = pd.Series(_df.loc[_df.sex == \"female\", \"name_second\"].isna().value_counts(), name=\"female\")\n    mal = pd.Series(_df.loc[_df.sex == \"male\", \"name_second\"].isna().value_counts(), name=\"male\")\n    print(\"name_second count by sex\")\n    return pd.DataFrame(dict(male=mal, female=fem))\n\n####\n\nstudy_second_names(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ea61194621265e8cf5893043719f47a0cf6babe4","collapsed":true},"cell_type":"code","source":"# with name, let's build a fancy name, easy to read\ndef fancy_names(df) :  \n    _df = df.copy()\n    names = list()\n    for i in df.index :\n        second = \"\" if _df.loc[i, \"name_second\"] is np.nan else str(_df.loc[i, \"name_second\"]) \n        txt =  _df.loc[i, \"title\" ] + \" \"+ _df.loc[i, \"name_first\"] + \" \" + second + \" \" + _df.loc[i, \"name_last\"]\n        txt = txt.replace(\"  \", \" \").replace('\"', \"\").replace(\"'\", \"\")\n        names.append(txt)\n    _df[\"fancy_name\"] = names\n    return _df\n\n####\n\n_df = fancy_names(sep_names(df))\npd.DataFrame(dict(fancy_name= _df.sort_values(by=\"name_len\", ascending=False).fancy_name.iloc[:20]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7333ccbda82883b9ddae4ed0993cd9ce82825fa6","collapsed":true},"cell_type":"code","source":"# let's regroup titles in a smaller list\ndef group_title(df) : \n\n    _df = df.copy()\n    \n    # we do not touch to Mr, Mrs, Miss, Master, Rev, Dr\n    group_dict_1 = {    \"Don\"       : \"Nobl\",  \n                        \"Mme\"       : \"Mrs\",\n                        \"Ms\"        : \"Miss\",\n                        \"Major\"     : \"Mil\",\n                        \"Lady\"      : \"Nobl\",\n                        \"Sir\"       : \"Nobl\",\n                        \"Mlle\"      : \"Miss\",\n                        \"Col\"       : \"Mil\",\n                        \"Capt\"      : \"Mil\",\n                        \"Dona\"      : \"Nobl\",\n                        \"Countess\"  : \"Nobl\",\n                        \"Jonkheer\"  : \"Nobl\",  }\n    \n    group_dict_2 = {\"Capt\":       \"Officer\",\n                    \"Col\":        \"Officer\",\n                    \"Major\":      \"Officer\",\n                    \"Jonkheer\":   \"Royalty\",\n                    \"Don\":        \"Royalty\",\n                    \"Sir\" :       \"Royalty\",\n                    \"Dr\":         \"Officer\",\n                    \"Rev\":        \"Officer\",\n                    \"Countess\":   \"Royalty\",\n                    \"Dona\":       \"Royalty\",\n                    \"Mme\":        \"Mrs\",\n                    \"Mlle\":       \"Miss\",\n                    \"Ms\":         \"Mrs\",\n                    \"Mr\" :        \"Mr\",\n                    \"Mrs\" :       \"Mrs\",\n                    \"Miss\" :      \"Miss\",\n                    \"Master\" :    \"Master\",\n                    \"Lady\" :      \"Royalty\"\n                    }\n\n    group_dict = group_dict_2\n    _df[\"title\"] = _df.title.apply(lambda i : group_dict[i] if i in group_dict.keys() else i )\n\n    return _df  \n\n####\n\n_df = group_title(sep_names(df))\n_df.title.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"becb3fabc2dca97e145dc3702d961edbade9db30","collapsed":true},"cell_type":"code","source":"def visualize_titles_1(df):\n    _df = group_title(sep_names(df))\n    fig, (axis1, axis2, axis3) = plt.subplots(1,3,figsize=(15,5))\n    feat = \"title\"\n\n    sns.countplot(x=feat, data=_df, ax=axis1)\n    sns.countplot(x='target', hue=feat, data=_df, order=[1,0], ax=axis2)\n    data = _df[[feat, \"target\"]].groupby([feat],as_index=False).mean()\n    sns.barplot(x=feat, y='target', data=data,ax=axis3)\n\n####\n\nvisualize_titles_1(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4115aecfea58918a0af34b3c42e388b3add969aa","collapsed":true},"cell_type":"code","source":"def visualize_titles_2(df):\n    feat = \"title\"\n    _df = group_title(sep_names(df))\n    g = sns.factorplot(\"target\", col=feat, col_wrap=4,\n                    data=_df[_df[feat].notnull()],\n                    kind=\"count\", size=2.5, aspect=.8)\n####\n\nvisualize_titles_2(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"24335b484b2d10e1e6cb0d37883864243f3c133f","collapsed":true},"cell_type":"code","source":"# we now have a tricky topic \n# what about people who are together, a family for ex, but with various tickets nb/id?\n# we need to tackle this problem, let's do this...\n\ndef group_name(df, threshold=2) : \n\n    try     : \n        _df = sep_names(group_ticket(df))\n    except : \n        _df = df.copy()\n\n    # add company\n    _df[\"company\"] = _df.parch + _df.sibsp\n\n    # add a specific mask\n    name_count = _df.name_last.value_counts().to_dict()\n    _df[\"name_count\"] = _df.name_last.apply(lambda i : name_count[i])\n    mask = (_df.name_count >1) & ((_df.parch +_df.sibsp) >= threshold)\n    sub_df = _df.loc[mask, :].copy()\n    sub_df.sort_values(by=[\"name_count\", \"name_last\", \"pclass\", \"embarked\", \"age\", \"group_id\"], ascending=False, inplace=True)\n    names_list = sub_df.name_last.unique()\n\n    # identify wierd / fake family\n    manual_check = list()\n    for i, n in enumerate(names_list) : \n\n        fam = _df.loc[_df.name_last == n , :]\n        idxs = fam.index\n        nb = len(fam)\n\n        is_wierd = True if (   (len(fam.embarked.unique()) != 1) \n                            or (len(fam.pclass.unique())   != 1)\n                            or (len(fam.group_id.unique())   != 1) \n                            or (len(fam.company.unique())  != 1) \n                            or (((fam.company.sum() + nb) / nb) != nb )) else False\n        if is_wierd :  manual_check.append(idxs)\n\n    # uncomment if you want to print it \n    # for idxs in manual_check : \n    #     sub_df = _df.loc[idxs, :]\n    #     sub_df = sub_df.sort_values(by=[\"name_last\", \"group_id\", \"company\", \"pclass\", \"embarked\", \"age\", \"sex\", ], ascending=False)\n    #     print(sub_df.loc[:, [  \"name_last\", \"name_first\", \"name_second\", \n    #                         \"pclass\", \"embarked\", \"age\",\"sex\",\n    #                         \"cabin_nb\", \"cabin_let\", \"parch\", \"sibsp\", \"company\", \"group_id\"]])\n    #     input()\n    #     print(50 * \"\\n\")\n\n    # manualy correct group id \n    try : \n        _df.loc[  1079, \"group_id\"] = 40.0\n        _df.loc[  1025, \"group_id\"] = 211.0\n        _df.loc[ [39, 334], \"group_id\"] = 163.0\n        _df.loc[  530, \"group_id\"] = 92.0\n        _df.loc[ [705, 624], \"group_id\"]  = 209.0\n        _df.loc[ [393,105], \"group_id\"]  = 1000.0\n        _df.loc[ [353, 533, 1229, 774], \"group_id\"] = 1001.0\n        _df.loc[  176, \"group_id\"] = 201.0\n        _df.loc[  1296, \"group_id\"] = 153.0\n        _df.loc[  1197,\"group_id\"] = 149.0\n        _df.loc[  594,\"group_id\"] = 214.0\n        _df.loc[ [1268, 70],\"group_id\"] = 1002.0\n    except: \n        print(\"both_df expected, method failed\")\n\n    # drop useless features\n    _df = _df.drop([\"name_count\", \"name_first\", \"name_second\", \"name_last\"], axis=1)\n\n    return df\n\n####\n\n_df = group_name(both_df)\n_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0e70b8d01117d5f7037775116c6b7429f534907e","collapsed":true},"cell_type":"code","source":"# finally we have so separate cabin number and cabin letter\n\ndef sep_cabins(df) : \n\n    _df = df.copy()\n    # replace cabin by list of str if various cabs for on pers\n    _df[\"_cabin\"]    = _df.cabin.apply(lambda i : [str(i), ])\n    _df[\"_cabin\"]    = _df._cabin.apply(lambda i : i[0].split(\" \"))\n    _df[\"_cabin\"]    = _df._cabin.apply(lambda i : sorted(i))\n\n    # count how many cabs by pers\n    _df[\"cabin_count\"]  = _df[\"_cabin\"].apply(lambda i : len(i) if i[0] != \"nan\" else 0)\n\n    # drop various cabs and take dirst cab for every people \n    # the split letter (deck ? ) from numb\n    _df[\"_cabin\"]    = _df[\"_cabin\"].apply(lambda i : i[0]) \n    _df[\"cabin_let\"] = _df[\"_cabin\"].apply(lambda i : i[0] if i != \"nan\" else np.nan) \n    _df[\"cabin_nb\"]  = _df[\"_cabin\"].apply(lambda i : int(i[1:]) if ((i != \"nan\") and (len(i)>1)) else np.nan) \n\n    # encode letter (deck) as an int\n    cabin_let_list  = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"T\"]\n    cabin_let_dict  = {j:i for i,j in enumerate(cabin_let_list)}\n    _df[\"cabin_let\"] = _df[\"cabin_let\"].apply(lambda i :cabin_let_dict[i] if i is not np.nan else np.nan)\n\n    # drop useless features\n    _df = _df.drop([\"cabin\", \"_cabin\"], axis=1, inplace=False)\n\n    return _df\n\n_df = sep_cabins(both_df)\n_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"90f362a4193023761bfb5b8769797c857b7d56d4","collapsed":true},"cell_type":"code","source":"def visualize_cabins_1(df) : \n    _df = sep_cabins(df)\n    fig, (axis1, axis2, axis3) = plt.subplots(1,3,figsize=(15,5))\n    feat = \"cabin_let\"\n\n    sns.countplot(x=feat, data=_df, ax=axis1)\n    sns.countplot(x='target', hue=feat, data=_df, order=[1,0], ax=axis2)\n    data = _df[[feat, \"target\"]].groupby([feat],as_index=False).mean()\n    sns.barplot(x=feat, y='target', order=sorted(_df[feat].unique()), data=data,ax=axis3)\n\n####\n\nvisualize_cabins_1(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5c193bd6ed27e2b5b8a0ced438fbd74591ba92cf","collapsed":true},"cell_type":"code","source":"def visualize_cabins_2(df) : \n    _df = sep_cabins(df)\n    feat = \"cabin_let\"\n\n    facet = sns.FacetGrid(_df, hue=\"target\",aspect=4)\n    facet.map(sns.kdeplot,feat,shade= True)\n    facet.set(xlim=(0, _df[feat].max()))\n    facet.add_legend()\n    \n####\n\nvisualize_cabins_2(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9be8625fc5132a2d50c6f7a7c3251e363edc7075","collapsed":true},"cell_type":"code","source":"# Not for now, but find bellow some very useful features we will be happy to find to upgrade accuracy score to 86+%\n\ndef is_alone(df) : \n    return None\n\ndef din_not_pay_histicket(df) : \n    return None\n\ndef is_the_dominant_of_the_family(df) : \n    return None\n\ndef is_child(df):\n    return None\n\ndef is_child_with_mother_survivor(df) : \n    return None\n\ndef is_man_just_with_his_wife(df) : \n    return None\n\ndef is_wife_with_men_survivor(df):\n    return None\n\ndef is_child_with_brother_sister_survivor(df):\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"15135baa2f6748d6975cf43655b21d6a9b0d370a"},"cell_type":"code","source":"# of course we also can separate age into categories : is_baby (-3), is_little_child (3-7), is_child (7-12), is_pre_ado (12-14), is_ado (15-18), is_young (18-25) is_adult_1 (25-35) \n# is_adult_2 (35-45), is_adult_3 (45-55), is pre_old (55-65), is old (65-75) and is_very_old (75-200)...","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"353e912adfdb6a86d2e7445488c5551993294c73","collapsed":true},"cell_type":"code","source":"# in order to have a good feature engineering strategy, we have to enhance our dataframe with dummy features\n\ndef add_noises(df) : \n    df[\"n_noise\"] = np.random.randn(len(df))\n    df[\"u_noise\"] = np.random.rand(len(df))\n\n    return df \n\n####\n\n_df = add_noises(df)\n_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"10daef95f1724cfa81ef2fc9a9b7965e04be408a"},"cell_type":"code","source":"# remember to retype all num?\ndef retype_all_num(df) : \n    _df = df.copy()\n    sex_dict        = {\"male\":1, \"female\":0}\n    embarked_dict   = {\"S\":2, \"C\":1, \"Q\":0}\n    _df[\"sex\"]        = _df.sex.map(sex_dict)\n    _df[\"embarked\"]   = _df.embarked.apply(lambda x : x if x not in [\"S\", \"C\", \"Q\"] else embarked_dict[x] )\n    return _df\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e3a97fee4d1e241b8149bf697c50f0d069b617e7","collapsed":true},"cell_type":"code","source":"# it will be also helpfull to retype all our features into categorical values\n\ndef retype_all_cat_1(df) :  \n    \n    # add name_count, group_count, name_len, cabin_count\n    # is_alone, is_with_survivor, is_dominant_male\n    \n    # is_baby (-3), is_little_child (3 years - 7) is child (7-12), is_preado(12 - 14) is_ado(15-18), is_young(18-25) is_adult1(25-35) is_adult1(35-45) is_adult1(45-55)\n    #  is pre_old(55-65) is old(65-75) very old(75-200)\n    \n    _df = df.copy()\n    \n    # equivalent to hot encoding / dummy encoding\n    features_list = (\"cabin_nb\", \"cabin_let\", \"group_id\", \"group_count\")\n    features_list = [(\"as_\"+i, i) for i in features_list]\n    \n    for new, old in features_list : \n        _df[new] = 1\n        idxs = _df.loc[_df[old].isna(), :].index\n        df.loc[idxs, new] = 0\n\n    for new, old in features_list : \n        cat          = pd.Categorical(df[new].unique())\n        df[new]      = df[new].astype(cat)\n\n    # fill nas\n    df[\"cabin_nb\"]      = df[\"cabin_nb\"].fillna(-1)\n    df[\"cabin_let\"]     = df[\"cabin_let\"].fillna(-1)\n    df[\"group_id\"]      = df[\"group_id\"].fillna(1234)\n    df[\"group_count\"]   = df[\"group_count\"].fillna(0)\n\n    # numerize str features\n    group_dict = {j:i for i, j in enumerate(df.title.unique())} \n    df[\"title\"] = df.title.apply(lambda i : group_dict[i] if i in group_dict.keys() else i )                       \n    \n    sex_dict = {j:i for i, j in enumerate(df.sex.unique())} \n    df[\"sex\"] = df.sex.apply(lambda i : sex_dict[i] if i in sex_dict.keys() else i )                       \n\n    embarked_dict = {j:i for i, j in enumerate(df.embarked.unique())}\n    df[\"embarked\"] = df.embarked.apply(lambda i : embarked_dict[i] if i in embarked_dict.keys() else i )                       \n\n    # columns\n    columns = df.columns\n\n    # categories\n    # bool_cat          = pd.Categorical([0,1])\n    pclass_cat          = pd.Categorical(df.pclass.unique(), ordered=True) \n    sex_cat             = pd.Categorical(df.sex.unique())\n    embarked_cat        = pd.Categorical(df.embarked.unique())\n    group_count_cat     = pd.Categorical(df.group_count.unique(), ordered=True)\n    title_cat           = pd.Categorical(df.title.unique())\n    # cabin_count_cat     = pd.Categorical(df.cabin_count.unique(), ordered=True)\n    cabin_let_cat       = pd.Categorical(df.cabin_let.unique(), ordered=True)\n    parch_cat           = pd.Categorical(df.parch.unique(), ordered=True)\n    sibsp_cat           = pd.Categorical(df.sibsp.unique(), ordered=True)\n    company_cat         = pd.Categorical(df.company.unique(), ordered=True)\n    group_id_cat        = pd.Categorical(df.group_id.unique())\n\n    # discrete\n    if \"pclass\" in columns : \n        df['pclass']        = df['pclass'].astype(pclass_cat)\n    if \"sex\" in columns : \n        df['sex']           = df['sex'].astype(sex_cat)\n    if \"embarked\" in columns : \n        df['embarked']      = df['embarked'].astype(embarked_cat)\n    if \"group_count\" in columns :     \n        df['group_count']   = df['group_count'].astype(group_count_cat)\n    if \"title\" in columns :     \n        df['title']         = df['title'].astype(title_cat)\n    # if \"cabin_count\" in columns :     \n    #    df['cabin_count']   = df['cabin_count'].astype(cabin_count_cat)\n    if \"cabin_let\" in columns :    \n        df['cabin_let']     = df['cabin_let'].astype(cabin_let_cat)\n    if \"parch\" in columns :  \n        df['parch']         = df['parch'].astype(parch_cat)\n    if \"sibsp\" in columns :      \n        df['sibsp']         = df['sibsp'].astype(sibsp_cat)\n    if \"company\" in columns :      \n        df['company']       = df['company'].astype(company_cat)\n    if \"group_id\" in columns :  \n        df['group_id']      = df['group_id'].astype(group_id_cat)\n\n    # continous\n    if \"age\" in columns :  \n        df['age']           = pd.cut(df.age,12, labels=range(12))\n    if \"fare\" in columns :  \n        df['fare']          = pd.cut(df.fare,12, labels=range(12))\n    # if \"cabin_nb\" in columns :  \n    #    df['cabin_nb']      = pd.cut(df.cabin_nb,12, labels=range(12))\n    if \"u_noise\" in columns :  \n        df['u_noise']       = pd.cut(df.u_noise,11, labels=range(11))\n    if \"n_noise\" in columns :  \n        df['n_noise']       = pd.cut(df.n_noise,11, labels=range(11))\n\n    return df\n\n# _df = retype_all_cat_1(group_name(df))\n# _df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0293caa755be9aabe01f15153874020fa88d8de3","collapsed":true},"cell_type":"code","source":"def retype_all_cat2(df) :\n    pass\n\n    #####\n    \n    # all in one hot dummy var\n    \n    #####","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1588cc024e33840f166d58c2c8497054548a05f8","collapsed":true},"cell_type":"code","source":"# let's now draw our first feature engineering startegy\n# a very basic one, just for naive models\n\ndef num_feature_eng_1(path, train_file, test_file) : \n    \n    # init 2 df\n    _train_df = init_df(PATH, TRAIN_FILE)\n    _test_df = init_df(PATH, TEST_FILE)\n    _train_idxs = train_df.index.copy()\n    _test_idxs = test_df.index.copy()\n    \n    # merge them\n    _both_df = _train_df.copy().append(_test_df)\n    \n    # handle them\n    _both_df = fill_nas(_both_df)\n    _both_df = manage_fare_outliers(_both_df)\n\n    # delete str or nas features\n    _both_df = _both_df.drop([\"cabin\", \"name\", \"ticket\"],axis=1)\n\n    # all numerical\n    _both_df = retype_all_num(_both_df)\n    \n    # re split \n    _train_df, _test_df = _both_df.loc[_train_idxs, :], _both_df.loc[_test_idxs, :]\n    \n    return _train_df, _test_df \n\n####\n\n# train and test\n_train_df, _test_df = num_feature_eng_1(PATH, TRAIN_FILE, TEST_FILE) \ndf = _train_df.copy()\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"395b19f3b489cd7edf2cabba2f62c00006f52b39","collapsed":true},"cell_type":"code","source":"# samething but with more complex strategy\n\ndef num_feature_eng_2(path, train_file, test_file) : \n    \n    # init 2 df\n    _train_df = init_df(PATH, TRAIN_FILE)\n    _test_df = init_df(PATH, TEST_FILE)\n    _train_idxs = train_df.index.copy()\n    _test_idxs = test_df.index.copy()\n    \n    # merge them\n    _both_df = _train_df.copy().append(_test_df)\n    _both_df = fill_nas(_both_df)\n    _both_df = manage_fare_outliers(_both_df)\n    \n    # handle them\n    _both_df = group_ticket(_both_df)\n    _both_df = sep_names(_both_df)\n    _both_df = group_title(_both_df)\n    _both_df = group_name(_both_df)\n    _both_df = sep_cabins(_both_df)\n    \n    # drop useless features\n    _both_df = _both_df.drop([\"group_id\", \"ticket_nb\", \"ticket_let\", \"name_last\", \"title\", \"name_first\",\"name_second\", \"cabin_nb\", \"cabin_let\"], axis=1)\n\n    # all numerical\n    _both_df = retype_all_num(_both_df)\n    \n    # re split \n    _train_df, _test_df = _both_df.loc[_train_idxs, :], _both_df.loc[_test_idxs, :]\n    \n    return _train_df, _test_df \n\n####\n\n_train_df, _test_df = num_feature_eng_2(PATH, TRAIN_FILE, TEST_FILE)\ndf = _train_df.copy()\n_train_df.dtypes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9a34008a413e85f1d936986e042a5fa7b17a3645","collapsed":true},"cell_type":"code","source":"# we now are building a much more complex feature engineering strategy\n\n####\n# we will join train_df and test_df to apply best feat eng strategy and the we will split train_df and test_df\n####\n\ndef cat_feature_eng_1(train_df, test_df) : \n    train_index = train_df.index\n    test_index  = test_df.index\n\n    df = train_df.copy().append(test_df)\n\n    \"\"\"\n    df = fill_nas(df)\n    df = sep_names(df)\n    # df = fancy_names(df)\n    df = sep_cabins(df)\n    df = group_title(df)\n    df = group_ticket(df)\n    df = group_name(df)\n    df = add_noises(df)\n    df = retype_all_cat(df)\n    \"\"\"\n    \n    train_df = df.loc[train_index, :]\n    test_df = df.loc[test_index, :]\n    \n    return train_df, test_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"76d4f72ff1ce86a40c96a411e5b9753c9943c394","collapsed":true},"cell_type":"code","source":"# _df, _ = cat_feature_eng_1(train_df, test_df)\n# _df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"19c4e34cfbefc91c0b597f4415e00327412ef8b1"},"cell_type":"code","source":"def cat_feature_eng_2(df) : \n    \"\"\"\n    dzdzad\n    zdazdazd\n    azdzaazdza\n    dadazdzad  \n    \"\"\"\n    return df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1eb5f46615e310950b321a16188fdd1027f632e2"},"cell_type":"markdown","source":"**01-first_naive_models.py**\n---------------------------------------\n"},{"metadata":{"trusted":true,"_uuid":"f06fc781b662c6784ab828f1574fc752c1ce1bb6","collapsed":true},"cell_type":"code","source":"################################################################################\n\n# ----------------------------------------------------------\n# 01-first_naive_models.py\n# ----------------------------------------------------------\n\n################################################################################\n\n\n\n# In this second part, we will implement our first logistic regression model.\n\n# We will first implement by hand a naive classifier, then a dummy classifier \n# (who does the same job), and finally a basic logistic regression model.\n\n# Rather than looking at the results of a regression we will implement a \n# function that will test the model x times and that will average the results\n# obtained\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"66be29da5a9e4532c3229be7ced3219cbbaa305b"},"cell_type":"markdown","source":"**Features-target and test-train slipt**\n---------------------------------------\n"},{"metadata":{"trusted":true,"_uuid":"7f36014af6afbff7cba8dce722921ed830b492c2","collapsed":true},"cell_type":"code","source":"# split our features from our target\n\ndef return_X_y(df) : \n    if \"target\" in df.columns : \n        X = df.drop(\"target\", axis=1)\n        y = df.target\n        return X, y  \n    else : \n        return df \n    \n####\n\n_train_df, _test_df = num_feature_eng_1(PATH, TRAIN_FILE, TEST_FILE) \ndf = _train_df.copy()\nDF = df.copy()\nX,y = return_X_y(df)\nprint(X.columns)\nprint(y.name)\nprint(X.head())\nprint(y.head())\nprint(N_1)\nprint(CV)\nprint(TEST_SIZE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b19a96aaecf9c32fbac937686392878a98e38a90","collapsed":true},"cell_type":"code","source":"# split test and train df/target\n\ndef split(X,y=None, size=0.33) :\n    if isinstance(y, Iterable) : \n        return train_test_split(X, y, test_size=size)\n    else : \n        X,y = return_X_y(X)\n        return train_test_split(X, y, test_size=size)\n\n####\n\nX_tr, X_te, y_tr, y_te = tup = split(X,y)\nX_train, X_test, y_train, y_test = X_tr, X_te, y_tr, y_te \nfor i in tup : print(i.shape)\n    \n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d2697dddfc28cc5576851a5f6c4a093da30f636"},"cell_type":"markdown","source":"**Dummy and naive models**\n-----------------------------------------------------------------\n"},{"metadata":{"trusted":true,"_uuid":"854470bb1c82a59726a3c9e0dde50c13e53aacdb","collapsed":true},"cell_type":"code","source":"# rather than coding a dummy model from scratch, use sklearn DummyClassifier \n\ndef dummy_model(df, param=None) : \n\n    X,y     = return_X_y(df)\n    X_train, X_test, y_train, y_test = split(X,y)\n\n    model = DummyClassifier()\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test) \n\n    acc = accuracy_score(y_test, y_pred).round(3)\n\n    return acc, model\n\n####\n\nacc, mod = dummy_model(df)\nprint(round(acc,4))\n\nser = pd.Series([dummy_model(df)[0] for i in range(N_2)])\nprint(round(ser.mean(),4))\nprint(round(ser.median(),4))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8acf31007229368dded877695649d49bcde3cb10","collapsed":true},"cell_type":"code","source":"# just for fun, trying to make predictions with a very basic model (no meta \n# params, no features engineering) this one will be our model prediction base\n# it is suposed to be better than our DummyClassifier. If not there is a major\n# issue...\n\ndef basic_model(df, param=None) : \n\n    X,y     = return_X_y(df)\n\n    X_train, X_test, y_train, y_test = split(X,y)\n\n    model = LogisticRegression()\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n\n    acc = accuracy_score(y_test, y_pred).round(3)\n    \n    return acc, model\n\n####\n\nacc, mod = basic_model(df)\nprint(round(acc,4))\n\nser = pd.Series([basic_model(df)[0] for i in range(N_2)])\nprint(round(ser.mean(),4))\nprint(round(ser.median(),4))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b96a466c55aefa5310db5c387a30200b387aef3a"},"cell_type":"markdown","source":"**Parsing various models**\n---------------------------------------"},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"99917e27ca574957db7c905a4654a06066629267","collapsed":true},"cell_type":"code","source":"# find here an high level function wich is charged of all basic tasks of a GridSearchCV\n\ndef GSCV_basic(model=None,  params=None, df=None) : \n\n    if not isinstance(df, pd.DataFrame): \n        df = DF.copy()\n        \n    if not model   : model = LogisticRegression()\n    if not params  : params = dict() \n    try            : model = model()\n    except         : pass\n    \n    X,y     = return_X_y(df)\n    X_train, X_test, y_train, y_test = split(X,y)\n\n    grid = GridSearchCV(model, params, \n                        n_jobs=6, \n                        scoring=\"accuracy\",\n                        cv=10)\n\n    grid.fit(X_train, y_train)\n    y_pred = grid.predict(X_test)\n    acc = accuracy_score(y_test, y_pred).round(3)\n    \n    return acc, grid\n\n\n####\n\nacc, mod = GSCV_basic(df=df)\nser = pd.Series([GSCV_basic(df=df)[0] for i in range(N_2)])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b5c6da795c54e9c0050fd97bc4d46904467d15a9","collapsed":true},"cell_type":"code","source":"print(round(acc,4))\nprint(round(ser.mean(),4))\nprint(round(ser.median(),4))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"51d37dc188ccb566a47406454e895a76a1f9c29b","collapsed":true},"cell_type":"code","source":"# we now are studying quickly wich model is the best\n\nCOLUMNS = [     \"LR\",       \"RC\",\n                \"SVC\",      # \"Nu\",\n                \"KNN\",\n                \"DT\",\n                \"RF\", \n                \"Ada\", # \"Per\",      \n                \"MLP\"   ]\n\nMODELS = [      LogisticRegression, RidgeClassifier,\n                LinearSVC, # NuSVC,\n                KNeighborsClassifier,\n                DecisionTreeClassifier, \n                RandomForestClassifier,\n                AdaBoostClassifier, # Perceptron, \n                MLPClassifier ]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"99757d8f70f738aa0fd87b9a8c585ff271b0ac8b","collapsed":true},"cell_type":"code","source":"# as GSCV_basic function, we build a 'meta ML handler' (overkill you said???)\n\ndef parse_various_models(  n, df=None, params=None,\n                                    models = MODELS, columns= COLUMNS) : \n\n    if not isinstance(df, pd.DataFrame): \n        df = DF.copy()\n\n    if len(models) != len(columns) : \n        raise ValueError(\"lens not goods\")\n\n    if not params : params = dict()    \n\n    results = [     pd.Series([GSCV_basic(m, df=df)[0] for m in models], \n                        index=columns) for i in range(n)]\n    \n    results = pd.DataFrame(results, columns=columns)\n\n    return results\n\n####\n\nresults = parse_various_models(N_2, df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"87b0a50fe0c39c7ed91ad46c992a84cb36fee260","collapsed":true},"cell_type":"code","source":"# print out raw values\nresults.iloc[:10, :]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fdece479c6b0523e136fef6c74c5cd7b4366c495","collapsed":true},"cell_type":"code","source":"# lets have fancy representation of our results\n_results = results.describe().T.sort_values(by=\"50%\", ascending=False)\n_results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8772afe6d2d8579aa424c85c9d709e321e0401b3","collapsed":true},"cell_type":"code","source":"# graph it \nfig, ax = plt.subplots(1,1, figsize=(20,8))\nresults.boxplot(ax=ax)\nplt.xlabel(\"models\")\nplt.ylabel(\"log_loss score\")\nplt.title(\"benchmark various models, without feat eng or meta params\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cef40111ec6cf151bfb45ab4b1d596aeb49a7d54"},"cell_type":"markdown","source":"**02-playing_with_LR.py**\n---------------------------------------"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"67ba344442ce67cb3347b9665f76bb3ae1c2f095"},"cell_type":"code","source":"################################################################################\n\n# ----------------------------------------------------------\n# 02-playing_with_LR..py\n# ----------------------------------------------------------\n\n################################################################################\n\n\n\n# In this third part we will finally start to make real machine learning. We will first parse various feature engineering\n# strategies helped by an other magic function\n\n# We will then benchmark the different classification models as well as the impact of the different meta \n# parametres on the relevance of the basic model: number of folds, preprocessing, scoring method, clip\n# of the predicted values, etc.\n\n# This work is clearly laborious, but its successful execution depends on our ability to really push \n# our model at best.\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"78fe8fec350b43edc57d03493eeaecccc0ddfaae"},"cell_type":"markdown","source":"**About feature engineering impact**\n---------------------------------------\n"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"042c372f4787364760f77af96c7ac8adbcb5cf7a"},"cell_type":"code","source":"# here we have various first considerations about feature engineering \n\ndef merge_age_sex(df, age=14) : \n    _df = df.copy() \n    _df[\"childness\"]  = _df.age.apply(lambda x : (0 if x > 17 else (1 if x > 14 else 2)))  \n    _df[\"status\"]     = _df.childness.apply(lambda x : 2 if x > 0 else -1)\n    mask = _df[\"status\"] == -1\n    _df.loc[mask, \"status\"] = (_df.loc[mask, \"sex\"] - 1).abs()\n    _df.drop([\"age\", \"sex\", \"childness\"], axis=1, inplace=True)\n    return _df\n\n\ndef add_childness(df) : \n    _df = df.copy() \n    _df[\"childness\"] = _df.age.apply(lambda x : (0 if x > 17 else (1 if x > 14 else 2)))\n    return _df\n\n\ndef convert_age(df, ages=None, coefs=None) :\n    _df = df.copy() \n    if not ages :  ages     = [ 3, 7,14,15,16,17,20,30,40,50,60,70,80,100]\n    if not coefs : coefs    = [10,10,10, 5, 3, 1, 0, 0, 0, 0, 0, 0, 0,  0]     \n    def converter(x) : \n        for a, c in zip(ages, coefs) : \n            if x <= a :return c\n        raise ValueError(\"error\")\n    _df[\"age\"] = _df.age.apply( lambda x : converter(x))\n    return _df\n\n\ndef regularize_fare(df) : \n    _df = df.copy() \n    # gérer les fare à 0 et les fares à 200+\n    _df.loc[_df.fare >200,\"fare\"] = 220\n    for i in range(1,4) : \n        val = _df.loc[_df[\"pclass\"] == i, \"fare\"].mean()\n        _df.loc[(_df.fare == 0.0) & (_df[\"pclass\"] == i), \"fare\"] = val\n    return _df\n\n\ndef add_family(df) : \n    _df = df.copy() \n    try : \n        _df[\"familly\"] = _df.sibsp + _df.parch\n    except : \n        ValueError(\"impossible\")\n    return _df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"bfc34632fa9d3f0e9e81ce188cddaf9db3e27fd6"},"cell_type":"code","source":"# various lambda methods \n\nnothing             = lambda df : df\ndrop_age            = lambda df : df.drop([\"age\"], axis=1)\nchildness_del_age   = lambda df : drop_age(add_childness(df))\ndrop_embarked       = lambda df : df.drop([\"embarked\"], axis=1)\ndrop_fare           = lambda df : df.drop([\"fare\"], axis=1)\ndrop_fare_embarked  = lambda df : drop_fare(drop_embarked(df))\ndrop_sibsp          = lambda df : df.drop([\"sibsp\"], axis=1)\ndrop_parch          = lambda df : df.drop([\"parch\"], axis=1)\nfamily_del_both     = lambda df : drop_parch(drop_sibsp(add_family(df)))\nfamily_del_sibsp    = lambda df : drop_sibsp(add_family(df))\nfamily_del_parch    = lambda df : drop_parch(add_family(df))\ndel_sibsp_parch     = lambda df : drop_parch(drop_sibsp(df))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b7c202e00f141dbc79269aa837580b64f1857220","collapsed":true},"cell_type":"code","source":"# and here our scenarii to benchmark\n\nMETHOD_LIST = []\nCOLUMN_LIST = []\n\n# about age\n# METHOD_LIST = [nothing, convert_age, merge_age_sex, drop_age, add_childness, childness_del_age]\n# COLUMN_LIST = [\"nothing\", \"convert_age\", \"merge_age_sex\", \"drop_age\", \"add_childness\", \"childness_del_age\"]\n\n# about embarked\n# METHOD_LIST =     [nothing, drop_embarked]\n# COLUMN_LIST =     [\"nothing\", \"drop_embarked\"]\n\n#  about fare\n# METHOD_LIST =   [nothing,   drop_fare,     regularize_fare]\n# COLUMN_LIST =   [\"nothing\", \"drop_fare\",   \"regul_fare\",  ]\n\n# # about family \n# METHOD_LIST = [nothing, add_family, drop_sibsp, drop_parch, family_del_both, family_del_sibsp, family_del_parch, del_sibsp_parch]\n# COLUMN_LIST = [\"nothing\", \"add_family\", 'drop_sibsp', 'drop_parch', 'family_del_both', 'family_del_sibsp', 'family_del_parch', \"del_sibsp_parch\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5886b8d83a9578db5ba24d9f96ddcd21c3ad3afc"},"cell_type":"code","source":"# we need to have a tool to compare basic model, vs feature engineered model\n# our goal is to select the method wich offers us the best accuracy gain\n# of course it will be impossible to gain 5 or 10%, but 0.5, or 1% will be fine \n\ndef df_enhance_gain(method, df=None, model=None, params=None) : \n    if not isinstance(df, pd.DataFrame) : \n        df = DF.copy()\n    if not model :  model = LogisticRegression()\n    if not params : params = dict()\n        \n    grid = GridSearchCV(model, params, cv=3, n_jobs=6, scoring=\"accuracy\")\n\n    # init acc\n    X_tr, X_te, y_tr, y_te = split(*return_X_y(df))  \n    grid.fit(X_tr, y_tr)   \n    y_pred = grid.predict(X_te)\n    init_acc = accuracy_score(y_te, y_pred)\n\n    # new acc\n    _df = df.copy()\n    _df = method(_df)\n    X_tr, X_te, y_tr, y_te = split(*return_X_y(_df))\n    grid.fit(X_tr, y_tr)   \n    y_pred = grid.predict(X_te)\n    new_acc = accuracy_score(y_te, y_pred)\n    \n    return round((new_acc - init_acc) / init_acc,3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ece63e45547cead995d8deec81513e843c95e1b9","collapsed":true},"cell_type":"code","source":"# we now need a tool to handle multiple tests and to give us back a global result dataframe\n\ndef benchmark_various_df_enhance(  n, df=None, params=None, model=None,\n                                methods = METHOD_LIST, cols = COLUMN_LIST) : \n    if not isinstance(df, pd.DataFrame): \n        df = DF.copy()\n\n    if not model : model =  LogisticRegression()\n    if not params : params = dict() \n    if len(methods) != len(cols) : raise ValueError(\"len do not match\")\n\n    results = [ [df_enhance_gain(m, df, model, params) for m in methods] \n                    for i in range(n) ]\n    results = pd.DataFrame(results, columns=cols)\n    return results\n\n####\n\n# let's try this\n\nMETHOD_LIST =   [nothing,   drop_fare,     regularize_fare]\nCOLUMN_LIST =   [\"nothing\", \"drop_fare\",   \"regul_fare\",  ]\n\nres = benchmark_various_df_enhance(N_1, None, methods=METHOD_LIST, cols=COLUMN_LIST)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6b851b33158d28f83609c4bf74c124d7cc59bc55","collapsed":true},"cell_type":"code","source":"# print sorted describe()\nres.describe().T.sort_values(by=\"50%\", ascending=False)\nres.describe().T.sort_values(by=\"mean\", ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c80d2134c469af789cf66142198736d2450f964d"},"cell_type":"code","source":"# ok it works! even if results are not good, we have a tool to track each feature engineering impact\n# in this case we can se that 'drop_fare' method help us to increase gain mean of 1% and median of ... 0.4% \n# Q1 and Q3 are also better with this method, but the std of our results is about 3-4%.\n# our accuracy gain is not so good enouth... \n\n# why the hell is there a 'nothing' method wich do nothing? \n# it is supposed to give us a scale of natural std of our results... ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7cc4eab8edd94b9a608970f65da26122e06ad166","collapsed":true},"cell_type":"code","source":"# boxplot\nfig, ax = plt.subplots(1,1, figsize=(20,8))\nres.boxplot(ax=ax)\nplt.xlabel(\"fare strategy\")\nplt.ylabel(\"accuracy gain in % (0.01 means 1%)\")\nplt.title(\"benchmark various feat eng regarding fare feature\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7b7c4bf243284fa70cce946ae9691d604b5ccb4a","collapsed":true},"cell_type":"code","source":"# plot mean med, Q1, Q3\nfig, ax = plt.subplots(1,1, figsize=(20,8))\nres.describe().T.loc[:, [\"mean\", \"50%\", \"75%\", \"25%\"]].plot(ax=ax)\nplt.xlabel(\"fare strategy\")\nplt.ylabel(\"accuracy gain in % (0.01 means 1%)\")\nplt.title(\"benchmark various feat eng regarding fare feature\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e764b1595eb4d4da34a544517009136bffa538ca"},"cell_type":"code","source":"# about embarked\n# same method...\n\nMETHOD_LIST =     [nothing, drop_embarked]\nCOLUMN_LIST =     [\"nothing\", \"drop_embarked\"]\n                   \nres = benchmark_various_df_enhance(N_1, None, methods=METHOD_LIST, cols=COLUMN_LIST)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"300b4160ebccab33de3a73f6ab5fa880a08e2cce","collapsed":true},"cell_type":"code","source":"# print sorted describe()\nres.describe().T.sort_values(by=\"50%\", ascending=False)\nres.describe().T.sort_values(by=\"mean\", ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"013050a801008f86679b7b2e05966ed01316d0ca","collapsed":true},"cell_type":"code","source":"# boxplot\nfig, ax = plt.subplots(1,1, figsize=(20,8))\nres.boxplot(ax=ax)\nplt.xlabel(\"embarked strategy\")\nplt.ylabel(\"accuracy gain in % (0.01 means 1%)\")\nplt.title(\"benchmark various feat eng regarding emabrked feature\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6895d3a85434a126dd7f261a23db6723851492d1","collapsed":true},"cell_type":"code","source":"# plot mean med, Q1, Q3\nfig, ax = plt.subplots(1,1, figsize=(20,8))\nres.describe().T.loc[:, [\"mean\", \"50%\", \"75%\", \"25%\"]].plot(ax=ax)\nplt.xlabel(\"embarked strategy\")\nplt.ylabel(\"accuracy gain in % (0.01 means 1%)\")\nplt.title(\"benchmark various feat eng regarding emabrked feature\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2551abf802ce6658f260c7243f9920c5bcfe0585"},"cell_type":"code","source":"# about age\nMETHOD_LIST = [nothing, convert_age, merge_age_sex, drop_age, add_childness, childness_del_age]\nCOLUMN_LIST = [\"nothing\", \"convert_age\", \"merge_age_sex\", \"drop_age\", \"add_childness\", \"childness_del_age\"]\n\nres = benchmark_various_df_enhance(N_1, None, methods=METHOD_LIST, cols=COLUMN_LIST)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"50c9d8b020a4cf4fbb2c866aec46311dc55b2546","collapsed":true},"cell_type":"code","source":"# print sorted describe()\nres.describe().T.sort_values(by=\"50%\", ascending=False)\nres.describe().T.sort_values(by=\"mean\", ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b444faa9dd554e19af41ba8921311f3aee8e8b97","collapsed":true},"cell_type":"code","source":"# boxplot\nfig, ax = plt.subplots(1,1, figsize=(20,8))\nres.boxplot(ax=ax)\nplt.xlabel(\"age strategy\")\nplt.ylabel(\"accuracy gain in % (0.01 means 1%)\")\nplt.title(\"benchmark various feat eng regarding age feature\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"48a7d901381b8cb3726f36d03ae7508e0e20d68c","collapsed":true},"cell_type":"code","source":"# plot mean med, Q1, Q3\nfig, ax = plt.subplots(1,1, figsize=(20,8))\nres.describe().T.loc[:, [\"mean\", \"50%\", \"75%\", \"25%\"]].plot(ax=ax)\nplt.xlabel(\"age strategy\")\nplt.ylabel(\"accuracy gain in % (0.01 means 1%)\")\nplt.title(\"benchmark various feat eng regarding age feature\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6b0124be15ffc85a875565ba1df4f9af19d61ec4"},"cell_type":"markdown","source":"**About result's clipping impact**\n---------------------------------------"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d476e82ced1e966a569dc5f2df9289a96a864a30"},"cell_type":"code","source":"# we now will try to ehance our output with thresholding our predictions \n\ndef clipping_results(y_pred, x=0.5) : \n    if not isinstance(y_pred, Iterable) : \n        raise ValueError(\"y_pred has to be a pd.Series\")\n    if not(0 <= x <= 1 ) :\n        raise ValueError(\"threshold must be 0.00 --> 0.5\")\n    y_pred = pd.Series(y_pred)\n    y_pred = y_pred.apply(lambda i : 1 if i>=x else 0)\n    return y_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"436b8503f245be7902a3257aff929cd7a324b73d"},"cell_type":"code","source":"# compute accuracy gain for one threshold\n\ndef clipping_results_gain(k, df, model=None, params=None) :     \n    if not isinstance(df, pd.DataFrame) : \n        df = DF.copy()\n    if not model  : model = LogisticRegression()\n    if not params : params = dict()\n    #  info(k)\n    X,y = return_X_y(df)\n    X_tr, X_te, y_tr, y_te = split(X, y)\n    y_test = y_te\n    \n    grid = GridSearchCV(model, params, \n                        cv = 10, \n                        n_jobs=6,\n                        scoring=\"accuracy\")\n    \n    grid.fit(X_tr, y_tr)   \n    y_pred = grid.predict_proba(X_te)[:, 1]\n\n    init_acc = accuracy_score(y_test, clipping_results(y_pred))\n\n    y_pred = clipping_results(y_pred, k)\n    new_acc = accuracy_score(y_test, y_pred)\n\n    return round((new_acc - init_acc) / init_acc,3)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"5f05368bf1d541d3ff10d10e14fc7beed9401f67","collapsed":true},"cell_type":"code","source":"# and benchmark every threshold between 0.0 and 0.5\n\ndef benchmark_various_clipping(   n , df=None, params=None, \n                                        model=None, threshold_list=None) :     \n    if not isinstance(df, pd.DataFrame) : \n        df = DF.copy()\n    if not model :  model = LogisticRegression()\n    if not params : params = dict()\n\n    if not threshold_list : \n        threshold_list = np.arange(0.2,0.8, 0.05).round(2)\n        # threshold_list = np.arange(0.44,0.66, 0.02).round(2)\n        # threshold_list = [round(i/1000, 3) for i in range(10,101)]\n        # threshold_list = [round(i/1000, 3) for i in range(10,500, 5)]\n    results = [ [clipping_results_gain(k, df, model, params) for k in threshold_list]\n                     for _ in range(n)]\n    results = pd.DataFrame(results, columns=threshold_list)\n    return results\n\n####\n\nres = benchmark_various_clipping(N_1, df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"21878bf2a65628d75f9753b3da779cff72952f5f","collapsed":true},"cell_type":"code","source":"# print sorted results describe()\nres.describe().T.sort_values(by=\"50%\", axis=0, ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1cfe85a1b8abd557e5d9473452d45b723448a625","collapsed":true},"cell_type":"code","source":"# boxplot \nfig, ax = plt.subplots(1,1, figsize=(20,8))\nres.boxplot(ax=ax)\nplt.xlabel(\"clipping levels\")\nplt.ylabel(\"accuracy gain in % (0.01 means 1%)\")\nplt.title(\"benchmark various clipping strategies\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6f9ff9b8730bc8e251091469155e539b1b17c445","collapsed":true},"cell_type":"code","source":"# plot mean, med, Q1,Q3\nfig, ax = plt.subplots(1,1, figsize=(20,8))\nres.describe().T.loc[:,  [\"mean\", \"50%\", \"75%\", \"25%\"]].plot(ax=ax)\nplt.xlabel(\"clipping levels\")\nplt.ylabel(\"accuracy gain in % (0.01 means 1%)\")\nplt.title(\"benchmark various clipping strategies\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0506532020d7e9c6e57ab860bdb6695de423ef12"},"cell_type":"markdown","source":"**About meta parametres impact**\n---------------------------------------\n"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ecb289ddfef0045229af31eea984a27058c2b0ff"},"cell_type":"code","source":"# find here a new version of our GSCV function\n# but with clipping option\n\ndef GSCV_basic(     model, params,  \n                    X_train, X_test, y_train, y_test, \n                    clipping=None,\n                    n_jobs=6, scoring=\"accuracy\", cv=10): \n\n    try    : model = model()\n    except : pass \n\n    grid = GridSearchCV(model, params, \n                        n_jobs=n_jobs, \n                        scoring=scoring,\n                        cv=cv)\n\n    try : grid.fit(X_train, y_train)\n    except Exception as e : print(e)\n\n    if not clipping : \n        try : y_pred = grid.predict(X_test)\n        except  Exception as e : print(e)\n    else : \n        try :\n            y_pred = grid.predict_proba(X_test)[:, 1]\n            y_pred = clipping_results(y_pred, clipping)\n        except Exception as e: \n            raise e\n\n    try : acc = accuracy_score(y_test, y_pred).round(3)\n    except  Exception as e : print(e)\n\n    return acc, grid\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"15ca190ea3b85760be64c325492a933d68a88aa2","collapsed":true},"cell_type":"code","source":"# find here 3 dict with meta parametres \n\ndefault_params  = {     \"penalty\":[\"l2\"],\n                        \"dual\":[False],\n                        \"tol\":[0.0001],\n                        \"C\":[1.0],\n                        \"fit_intercept\":[True],\n                        \"intercept_scaling\":[1],\n                        \"class_weight\":[None],\n                        \"solver\":[\"liblinear\"],\n                        \"max_iter\":[100],\n                        \"multi_class\":[\"ovr\"],\n                        \"warm_start\":[False],   }\n\nall_params          = { \"penalty\":[\"l1\", \"l2\"],\n                        \"dual\":[True, False],\n                        \"tol\":[0.0001, 0.001, 0.1, 1],                   # consider also np.logspace(-6, 2, 9)\n                        \"C\":[0.0001, 0.001, 0.01, 0.1, 1, 10, 100],      # consider also np.logspace(-3, 1, 40)\n                        \"fit_intercept\":[True],\n                        \"intercept_scaling\":[1],\n                        \"class_weight\":[None],\n                        \"solver\":[\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\", \"saga\"],\n                        \"max_iter\":[100, 1000],   # consider also np.logspace(3, 5, 3)\n                        \"multi_class\":[\"ovr\", \"multinomial\"],\n                        \"warm_start\":[False, True],   }\n\nall_params2     = { \"penalty\":[\"l1\", \"l2\"],\n                        \"dual\":[True, False],\n                        \"tol\":[0.0001, 0.001, 0.01],            # consider also np.logspace(-6, 2, 9)\n                        \"C\":[0.001, 0.01, 0.1, 1, 10],      # consider also np.logspace(-3, 1, 40)\n                        \"fit_intercept\":[True],\n                        \"intercept_scaling\":[1],\n                        \"class_weight\":[None],\n                        \"solver\":[\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\", \"saga\"],\n                        \"max_iter\":[100],                   # consider also np.logspace(3, 5, 3)\n                        \"multi_class\":[\"ovr\", \"multinomial\"],\n                        \"warm_start\":[True, False],   }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"22cfb7387e4f8cf807f429b560688653f6b08edb"},"cell_type":"code","source":"def _mean(x) : \n    if not isinstance(x, Iterable) : \n        raise ValueError(\"x must be iter\")\n    return round(float(sum(x) / len(x)), 3)\n\n\ndef _med(x) : \n    x = sorted(x)\n    if not (len(x) % 2) : \n        idx     = len(x) /2\n        idx_u   = ceil(idx)\n        idx_d   = ceil(idx) - 1\n        med = _mean([x[idx_u], x[idx_d]])\n    else :\n        idx = int(len(x)/2)\n        med = x[idx]\n    return round(med, 3)\n\n\ndef _mix(x) : \n    mea_x = _mean(x)\n    med_x = _med(x)\n    return _mean([mea_x, med_x]) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ff8b2f6e4bc8cddcd67d3bd4074f8fd12a099306","collapsed":true},"cell_type":"code","source":"# in order to be able to parse easly various meta parametres, without wondering if there are\n# correct, we need a function which is able to combine a dict of list and one\n# other which is able to  select only good parametres\n\ndef combine_param_dict(d) : \n    d = OrderedDict(d)\n    combinations = it.product(*(d[feat] for feat in d))\n    combinations = list(combinations)\n    d = [{i:[j,] for i,j in zip(d.keys(), I)} for I in combinations ]\n    return d\n\n\ndef valid_param_dict(model, list_of_dicts, X_train, X_test, y_train, y_test) : \n    good_dicts = list()\n    try : _model = model()\n    except : _model = model\n    for d in list_of_dicts : \n        try : \n            m = GridSearchCV(_model, d, cv=3)\n            m.fit( X_train, y_train)\n            good_dicts.append(d)\n        except : \n            print(\"params!\", end=\"**\")\n    return good_dicts\n\n####\n\nd = {\"a\" : [\"a\",\"b\",\"c\"], \"b\": [0,1,2,3,4]}\nd = combine_param_dict(d)\nd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c4041468c5e5486d3a5a8f32d4884e7ebad014c1"},"cell_type":"code","source":"# and here we have the heart of our meta parametres search\n# is job is to parse all feeded params, to combine them,\n# to select only good ones, and to compute the average accuracy score\n\ndef parse_various_params(   n, model, params, df=None, \n                             meth=None, save=True, feat_ing=None,\n                             clipping = None, \n                             n_file=4, name=\"benchmark_various_params\",\n                             path=\"benchmarks/params/\",\n                             n_jobs=6, scoring=\"accuracy\",cv=5) : \n\n    if not isinstance(df, pd.DataFrame): \n        df = DF.copy()\n    if      meth == None   : meth = _mix\n    elif    meth == \"mean\" : meth = _mean\n    elif    meth == \"med\"  : meth = _med\n    elif    meth == \"mix\"  : meth = _mix\n    else                   : raise ValueError(\"not good method\") \n\n    if not feat_ing : feat_ing = \"no feat_ing\"\n    if not name : name = \"benchmark_various_params\"\n    if not path : path = \"benchmarks/params/\"\n    if not n_file :  n_file =  1\n\n    name = path+name+str(n_file)+\".csv\"\n\n    if save : \n        txt =   \"init file         \\n\"\n        txt +=  \"model     : {}      \\n\".format(model)\n        txt +=  \"params    : {}      \\n\".format(params)\n        txt +=  \"n         : {}      \\n\".format(n)\n        txt +=  \"clipping  : {}      \\n\".format(clipping)\n        txt +=  \"meth      : {}      \\n\".format(meth)\n        txt +=  \"feat_ing  : {}      \\n\".format(feat_ing)\n        txt +=  \"\\n\\n  ********************************************** \\n\\n\"\n\n        with open(name, \"w\") as f : f.write(txt)\n\n    X,y     = return_X_y(df)\n    X_train, X_test, y_train, y_test = split(X,y)\n    columns = list(params.keys())\n    columns.append(\"acc\")\n    results = list()\n\n    param_dict = combine_param_dict(params)\n    param_dict = valid_param_dict(model, param_dict, X_train, X_test, y_train, y_test)\n\n    for param in param_dict : \n        info(\"testing param : \" + str(param))\n        accs = [GSCV_basic(   model, param, \n                                X_train, X_test, y_train, y_test, \n                                clipping=clipping, \n                                n_jobs=n_jobs, scoring=scoring,cv=cv )[0] \n                                for i in range(n)]\n\n        acc = round(meth(accs), 3)\n        # grid_param = grid.get_params()\n        if save : \n            txt = str(acc) + \",\" + str(param) + \"\\n\"\n            with open(name, \"a\") as f : f.write(txt)\n                \n        serie = {i: j[0] for i,j in param.items()}\n        serie[\"acc\"] = acc\n        results.append(pd.Series(serie))\n\n        info(\"done\")\n\n    results = pd.DataFrame(results, columns =columns )\n    results.sort_values(by=\"acc\", ascending=False, inplace=True)\n    return results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5cc5ba23e4e7a702690fc8f024de0ba3232f248e"},"cell_type":"code","source":"# let's try a dict params \n\nsmall_dict_params    = { \"penalty\":[\"l1\", \"l2\"],\n                        \"dual\":[False, True],\n                        \"tol\":[0.0001, 0.001, 0.01],            # consider also np.logspace(-6, 2, 9)\n                        \"C\":[0.001, 0.01, 0.1, 1, 10],      # consider also np.logspace(-3, 1, 40)\n                        \"fit_intercept\":[True],\n                        \"intercept_scaling\":[1],\n                        \"class_weight\":[None],\n                        \"solver\":[\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\", \"saga\"],\n                        \"max_iter\":[100],                   # consider also np.logspace(3, 5, 3)\n                        \"multi_class\":[\"ovr\",\"multinomial\"],\n                        \"warm_start\":[True],   }","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"scrolled":true,"_uuid":"bc01abdc0b29befdb3b2fa56ec976037fd5dc57d","collapsed":true},"cell_type":"code","source":"res = parse_various_params( N_1, # should be 10, 20, 30 or 50\n                           LogisticRegression, \n                           small_dict_params, \n                           save=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5092149b5b76f31f08dc6d3d114df053772e74cd","collapsed":true},"cell_type":"code","source":"# print sorted results describe()\n_res = res.sort_values(by=\"acc\", axis=0, ascending=False).iloc[:20, :]\nprint(len(res))\n_res","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4749b4a347bf4ae7234ad5e2166fb40214ff53f9","collapsed":true},"cell_type":"code","source":"_res.sort_values(by=\"acc\", axis=0, ascending=False).iloc[20: , :]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"28eae6cd41ec061abd7d9cbee60baa1b93dc0c81","collapsed":true},"cell_type":"code","source":"# scatter various sub parametres\nfig, ax = plt.subplots(1,1, figsize=(10,10))\nax.scatter(res.tol,res.acc )\nplt.xlabel(\"tol levels\")\nplt.ylabel(\"accuracy gain in % (0.01 means 1%)\")\nplt.title(\"benchmark various accuracy gain for tol levels\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"09dfd095b54dd8b3ad3cae93d053e3920dd2a825","collapsed":true},"cell_type":"code","source":"# scatter various sub parametres\nfig, ax = plt.subplots(1,1, figsize=(10,10))\nax.scatter(res.C, res.acc)\nplt.xlabel(\"C levels\")\nplt.ylabel(\"accuracy gain in % (0.01 means 1%)\")\nplt.title(\"benchmark various accuracy gain for C levels\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4636b70ed28510aea4490420886d5782629aa515"},"cell_type":"code","source":"# we now are going to check if our best params dict is 'really' can \n# strongly impact our results\n# we now are going to check if our best params dict is 'really' can \n# strongly impact our results\n\nbest_params_top_5 = list()\nfor i in range(5) : \n    best_params = _res.drop(\"acc\",axis=1).iloc[i,:].to_dict()\n    best_params = {i:[j] for i, j in best_params.items()}\n    best_params_top_5.append(best_params)\n\nBEST_PARAMS = dict(     dual            = [False], \n                        penalty         = [\"l2\"], \n                        tol             = [0.001], \n                        multi_class     = [\"multinomial\"], \n                        warm_start      = [True], \n                        solver          = [\"lbfgs\"],\n                        C               = [0.1], \n                        fit_intercept   = [True]    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e86267505dc164bb263faa5e466ff555a4a57bae"},"cell_type":"code","source":"# compute accuracy gain : with vs without params \n\ndef meta_params_results_gain(model, params, df, re_split=False) : \n    \n    try     : model = model()\n    except  : pass \n\n    grid = GridSearchCV(model, dict(), \n                        cv = 10, \n                        n_jobs=6,\n                        scoring=\"accuracy\")\n\n    grid2 = GridSearchCV(model, params, \n                        cv = 10, \n                        n_jobs=6,\n                        scoring=\"accuracy\")\n    \n    # init acc\n    X_tr, X_te, y_tr, y_te = split(*return_X_y(df))  \n    grid.fit(X_tr, y_tr)   \n    y_pred = grid.predict(X_te)\n    init_acc = accuracy_score(y_te, y_pred)\n\n    # new acc\n    if re_split : \n        X_tr, X_te, y_tr, y_te = split(*return_X_y(df))  \n    \n    grid2.fit(X_tr, y_tr)   \n    y_pred = grid2.predict(X_te)\n    new_acc = accuracy_score(y_te, y_pred)\n\n    return round((new_acc - init_acc) / init_acc,3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"deb9d0cc717bb2c0602f4d1909a31fba3c39a357","collapsed":true},"cell_type":"code","source":"# benchmark various params, or just one\n\ndef benchmark_various_meta_params(   n, model, params, df=None) : \n    if not isinstance(df, pd.DataFrame) : \n        df = DF.copy()\n    if not model : model= LogisticRegression\n    if not params : params = BEST_PARAMS\n    model = model()\n    results = [meta_params_results_gain(model, params, df) for _ in range(n)]\n    # results = [[params_results_gain(model, p, df) for _ in range(n)]for p in params]\n    results = pd.DataFrame({\"params\" : results})   \n    # results = pd.DataFrame(results, columns=map(str,params))\n    return results\n\n####\n\nres = benchmark_various_meta_params(N_1, LogisticRegression, BEST_PARAMS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cd6729da1224b5020f208602704225fd5fd62c6b","collapsed":true},"cell_type":"code","source":"# print sorted results describe()\nres.describe().T.sort_values(by=\"mean\", ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d4f81a7e3d9b1d3fd133e46ee18240d4bfb0601b","collapsed":true},"cell_type":"code","source":"# boxplot \nfig, ax = plt.subplots(1,1, figsize=(20,8))\nres.boxplot(ax=ax)\nplt.xlabel(\"best meta parmas\")\nplt.ylabel(\"accuracy gain in % (0.01 means 1%)\")\nplt.title(\"benchmark best meta params\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0d8493b16a7c60d1da41f500b13cde30261cbdff","collapsed":true},"cell_type":"code","source":"# very famous in DL, the pseudo labelling can be a good method to upgrade the accuracy rate\n# it works for multi class problems, let's try for our dataset\n\ndef pseudo_labelling(df, model=None, params=None) : \n    if not isinstance(df, pd.DataFrame) : \n        df = DF.copy()\n\n    if not model : model = LogisticRegression()\n        \n    if not params : params = dict()\n\n    X,y = return_X_y(df)\n    X_tr, X_te, y_tr, y_te = split(X, y)\n    y_test = y_te\n    \n    grid = GridSearchCV(model, params, \n                        cv = 10, \n                        n_jobs=6,\n                        scoring=\"accuracy\")\n    \n    grid.fit(X_tr, y_tr)   \n    y_pred = grid.predict(X_te)\n\n    init_acc = accuracy_score(y_test, y_pred)\n\n    TR = X_tr.copy()\n    TR[\"target\"] = y_tr\n    TE = X_te.copy()\n    TE[\"target\"] = y_pred\n    \n    new_df = TR.append(TE)\n    new_X,new_y = return_X_y(new_df)\n\n    grid.fit(new_X,new_y)\n\n    y_pred = grid.predict(X_te)\n    new_acc = accuracy_score(y_test, y_pred)\n\n    return round((new_acc - init_acc) / init_acc,3)\n\n####\n\nres = pd.Series([pseudo_labelling(df) for i in range(N_3)], name=\"pseudo_labelling\")\nres.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e12aeb57a7fa3926b362cf3610fc84353eb0d3cf"},"cell_type":"code","source":"default_params  =     { \"alpha\":[1.0],\n                        \"normalize\":[False],\n                        \"tol\":[0.001],\n                        \"fit_intercept\":[True],\n                        \"class_weight\":[None],\n                        \"solver\":[\"auto\"],\n                        \"max_iter\":[None],  }\n\nall_params      =     { \"alpha\":np.logspace(-5, 3, 9), \n                        \"normalize\":[False, True],\n                        \"tol\":[1, 0.1, 0.001, 0.0001],\n                        \"fit_intercept\":[True],\n                        \"class_weight\":[None],\n                        \"solver\":['svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'],\n                        \"max_iter\":[None],  }\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4cd43cfca1555b901edfcdc1551d22f0c040aef2","scrolled":true,"_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"res = parse_various_params( N_1, # should be 10, 20, 30 or 50 \n                           RidgeClassifier, \n                           all_params, \n                           save=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d742774ccac3136d7398046767eb76fc4e10d7ff","collapsed":true},"cell_type":"code","source":"_res = res.sort_values(by=\"acc\", axis=0, ascending=False).iloc[:20, :]\nprint(len(_res))\n_res","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8ae28e1152d4fa157f326bc88fca544c8afa9999","collapsed":true},"cell_type":"code","source":"_res.sort_values(by=\"acc\", axis=0, ascending=False).iloc[20: , :]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0fc4f9bd5686903b136c9f0359b91e0ade48de57","collapsed":true},"cell_type":"code","source":"# select our best params \nBEST_PARAMS_2 = _res.drop(\"acc\",axis=1).iloc[0,:].to_dict()\nBEST_PARAMS_2 = {i:[j] for i, j in BEST_PARAMS_2.items()}\nBEST_PARAMS_2\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1e3eb77ad02c28817e0b50e706e0a3911fa4e15b","collapsed":true},"cell_type":"code","source":"res = benchmark_various_meta_params(N_1, RidgeClassifier, BEST_PARAMS_2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8a74febe963a0fa6aa770cdf074ee0e3303bc996","collapsed":true},"cell_type":"code","source":"# print sorted results \nres.describe().T.sort_values(by=\"mean\", ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0df7bea2cc1c18f77ecfb89b6fc635cbcdebeac9","collapsed":true},"cell_type":"code","source":"# boxplot \nfig, ax = plt.subplots(1,1, figsize=(20,8))\nres.boxplot(ax=ax)\nplt.xlabel(\"best meta parmas\")\nplt.ylabel(\"accuracy gain in % (0.01 means 1%)\")\nplt.title(\"benchmark best meta params\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"8784c768fe1447752aa167e8ecedb00adc62b02e"},"cell_type":"code","source":"# ok we can see that our meta params gain (+1.3%) is very good compared to other meta params gain","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"d745e858e99ec22bbedd2b3a4f40641bdf7377fa"},"cell_type":"markdown","source":"**About global feature eng. strategies impact**\n-------------------------------------------\n"},{"metadata":{"trusted":true,"_uuid":"a484958737c4c6b0c215f402c52258e30aac6ce0","collapsed":true},"cell_type":"code","source":"_train_df, _test_df = num_feature_eng_1(PATH, TRAIN_FILE, TEST_FILE) \ndf = _train_df.copy()\nDF = df.copy()\nX,y = return_X_y(df)\nprint(X.columns)\nprint(y.name)\nprint(X.head())\nprint(y.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"6199f76df47d94186e770b540dab36f16624073c"},"cell_type":"code","source":"_train_df, _test_df = num_feature_eng_2(PATH, TRAIN_FILE, TEST_FILE) \ndf = _train_df.copy()\nDF = df.copy()\nX,y = return_X_y(df)\nprint(X.columns)\nprint(y.name)\nprint(X.head())\nprint(y.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"efb099a8b6f22df6615ddaf093dd6903070048a1","_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"def feat_strat_gain(feat_eng_1, feat_eng_2, path=None, \n                    train_file=None, test_file=None, model=None, params=None) : \n    if not path : path = PATH\n    if not train_file : train_file = TRAIN_FILE\n    if not test_file : test_file = TEST_FILE\n    if not model :  model = LogisticRegression()\n    if not params : params = dict()\n        \n    grid = GridSearchCV(model, params, cv=3, n_jobs=6, scoring=\"accuracy\")\n\n    # init acc\n    _train_df, _test_df = feat_eng_1(path, train_file, test_file)\n    X_tr, X_te, y_tr, y_te = split(*return_X_y(_train_df))\n    grid.fit(X_tr, y_tr)   \n    y_pred = grid.predict(X_te)\n    init_acc = accuracy_score(y_te, y_pred)\n\n    # new acc\n    _train_df, _test_df = feat_eng_2(path, train_file, test_file)\n    X_tr, X_te, y_tr, y_te = split(*return_X_y(_train_df))\n    grid.fit(X_tr, y_tr)   \n    y_pred = grid.predict(X_te)\n    new_acc = accuracy_score(y_te, y_pred)\n    \n    return round((new_acc - init_acc) / init_acc,3)\n\n####\n\nresults = [feat_strat_gain(num_feature_eng_1, num_feature_eng_2) for _ in range(30)]\nresults = pd.DataFrame({\"feat. strat\" : results})  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1532f7092445a0e52079211d35a55fa3e6d7bc4f","collapsed":true},"cell_type":"code","source":"results.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"08217266242028c9c0155af52690e00d00b42b0a","collapsed":true},"cell_type":"code","source":"results.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0b451275ebb4845da752c806f2aa7178817c97a1","collapsed":true},"cell_type":"code","source":"results.boxplot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5fa38483bf09350eef3d2b8c19d586a001b24f05"},"cell_type":"markdown","source":"**03-playing_with_RF.py**\n---------------------------------------"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f8f0c0b5418005491f5c88d96c5ac5137e52db09"},"cell_type":"code","source":"################################################################################\n\n# ----------------------------------------------------------\n# 03-playing_with_RF..py\n# ----------------------------------------------------------\n\n################################################################################\n\n\n\n# In this 4st part we will play with or Random Forest Classifier and with all feature engineering strategies we can to \n# improve our accuracy score. \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"4c3b06553dbd71b91a02e43bb0a22d3d8aa28767"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5e97d7d2ed5b0d613d517e7202aa100583066c15"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ef1644c21bb7dca466a4a72dd1d5868b085e585f"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}