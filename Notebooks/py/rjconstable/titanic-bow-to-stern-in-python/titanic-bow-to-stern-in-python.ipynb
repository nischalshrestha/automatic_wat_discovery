{"cells":[{"cell_type":"markdown","metadata":{"_uuid":"0a5860d351c42d75613640d5c3b4ef0008caf4bf"},"source":"## **Table of Contents** \n* ** [Exploratory data analysis](#Exploratory-data-analysis)**\n    + Read in data to a Pandas dataframe and inspect the features\n    + Feature correlation visualisation\n* ** [Data cleaning and feature engineering](#Data-cleaning-and-feature-engineering)**\n    + Dealing with missing values and dropping unwanted features\n    + Converting categorical features to numerical representations\n    + Assigning continuous features to discrete bins\n    + Extracting new features\n* ** [Training ML algorithms to predict survival with scikit-learn](#Training-ML-algorithms-to-predict-survival-with-scikit-learn)**\n    + K-fold cross validation to train the optimal classification algorithm\n    + What is XGBoost and how does it work?\n    + Feature importance\n    + Hyperparameter tuning with GridSearch CV\n* ** [Model evaluation](#Model-evaluation) **\n    + Learning curves: bias vs variance\n    + Confusion matrix\n    + Precision vs recall\n    + Precision/recall trade off\n    + ROC-AUC \n* ** [Submission](#Submission) **\n* ** [Conclusions](#Conclusions) **"},{"cell_type":"markdown","metadata":{"_uuid":"b89d508d15f3e8273f962af199e536ac97250e49"},"source":"# The Titanic dataset and binary classification challenge\n"},{"cell_type":"markdown","metadata":{"_uuid":"f2032a193230b00eeaff94ded0a0c77f3f3d67f0"},"source":"In this challenge we're provided with some tabulated data about passengers on the RMS Titanic and how they faired in the infamous maritime disaster. The challenge is to explore the data, identify and prepare predictive features correlating with survival probability and train a binary classifier to successfully predict survival in an unlabelled dataset, using those features. "},{"cell_type":"markdown","metadata":{"_uuid":"d9983569bcc2cd93a6b1693fe2bb4a4b0e8df92e"},"source":"# Exploratory data analysis\n\n## Read in data to a Pandas dataframe and inspect the features\nWe are provided with passenger manifest data from the Titanic in two csv files. A train.csv file containing features and survival target label and a test.csv with features but no survival column. Firstly, I should read in the data to a pandas dataframe to allow me to explore and manipulate the data effectively."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"fb966eba5bc63c47429ff542b12ec6a85b125697"},"outputs":[],"source":"# Load dependencies\n\nimport pandas as pd    # data cleaning and preparation\nimport numpy as np     # arrays, linear algebra \nimport matplotlib.pyplot as plt  #visualisation libraries\nimport seaborn as sns    #pretty plots by default\nimport re              # regular expressions for pattern matching in strings\n\nsns.set(style='whitegrid', rc={\"grid.linewidth\": 0.1})\nsns.set_context(\"paper\", font_scale=2)\n%matplotlib inline"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"0d5d72d9941c0e0f8e62d70b795871bec4a01143"},"outputs":[],"source":"#Read in the titanic survival dataset CSV files\n\ndf_train = pd.read_csv('../input/train.csv')\ndf_test = pd.read_csv('../input/test.csv')"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"9d6b7de0998a6534c09cfbf1f4493af61824883c"},"outputs":[],"source":"#Take a look at the training data frame \ndf_train.head()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"0cc329713ccbc3dc009c91ab42f81b40e021cfe5"},"outputs":[],"source":"#Use df.info() to get a count of the non-null rows in each column. We can also see the data type of each column\ndf_train.info()"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"f906f1766b90aefc89161a43912b3aa2d0664103"},"outputs":[],"source":"#Use df.describe() to see a summary statistics table for the dataframe\n\ndf_train.describe()"},{"cell_type":"markdown","metadata":{"_uuid":"f407772e78761b7394c75a77f8414d212bb207b9"},"source":"* From the df.describe() output we see that the mean for the survived column (0 = died, 1 = survived) is 0.38, therefore a mere 38% of the passengers in the training set survived. The two classes in our target variable are not evenly distributed in the data, something to consider during modelling.\n\n* If the test set features are similarly distributed to the training set, a predictor predicting death no matter the input feature values would be 62% accurate\n\n* We can also see the count, mean, std, min and max values for the various features giving a sense of their distributions\n\n* Looks like we have some missing values in the Age and Cabin columns in the training set, which will have to be dealt with before model building\n\n* Many of the feature columns are categorical variables which will need to be converted to numerically encoded representations in order to feed them into machine learning models in sklearn"},{"cell_type":"markdown","metadata":{"_uuid":"2311c728e33655ca40e3a4e60810ea436278931b"},"source":"## Feature correlation visualisation\nBy plotting features against the target label we draw some initial conclusions about the correlation between the existing numerical features and survival."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"add46bd59fad8bc93b07d912b12d662c005029e1"},"outputs":[],"source":"# More women surived than men\nsns.factorplot(x='Sex', col='Survived', kind='count', data=df_train);"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"59f1a7d1de968caa4e2ccf7443918673106b0b26"},"outputs":[],"source":"# First class passengers > second class passengers > third class passengers, in percentage survival\nsns.factorplot('Pclass','Survived', data=df_train)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"f4e01e4baf1c5277b4033b2f1c56541915eec761"},"outputs":[],"source":"# The bulk of the dead were men in Pclass 3\npd.crosstab([df_train.Sex, df_train.Survived], df_train.Pclass, margins=True).style.background_gradient(cmap='winter_r')"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"1460f10c93dd198b676d9c373389107cfec2932d"},"outputs":[],"source":"# More people embarked at S than the other ports and the majority of those embarking at S died\nsns.factorplot(x='Survived', col='Embarked', kind='count', data=df_train);"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"80d5972e60ea43599b7b8495b64b8d8b0ea1c9d5"},"outputs":[],"source":"# Most of the men who died were in their prime, between 18-40\ngrid = sns.FacetGrid(df_train, col='Survived', row='Sex', size=3.5, aspect=1.6, hue = 'Survived')\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend();"},{"cell_type":"markdown","metadata":{"_uuid":"32d9a78bd324ea842d8f55fc3d930a6cc41b7d81"},"source":"Looking at the correlation matrix, Pclass and Fare are most clearly correlated with survival whilst Age, SibSp(no. siblings or spouse) and Parch (number of parents or children aboard) also show some correlation"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"9462b50b98545d56f1f89549d4309aef125f2e0f"},"outputs":[],"source":"# df.corr() calculates Pearsons correlation coefficient by default for each feature pair and target-feature pair\ncorr = df_train.corr()\nsns.heatmap(corr, cmap = 'RdBu_r', center = 0)"},{"cell_type":"markdown","metadata":{"_uuid":"062606573e12c4570e4dfd827f864c44118e73f5"},"source":"So it looks like Sex, Pclass, Fare, Age, Parch, SibSp and Embarked are useful predictive features. \nIt's possible that the Name and Cabin information could also be useful but work needs to be done to extract the useful content from these columns (i.e title and deck) and then encode them numerically."},{"cell_type":"markdown","metadata":{"_uuid":"8166222cc59ae74990ae70d78697f50d96c73975"},"source":"# Data cleaning and feature engineering\n\nHere I will drop unwanted features, deal with missing values, encode categorical variables as numerics, assign continuous variables to discrete bins and generate new features. All transformations have to be carried out on both training and testing data in order to train the model and generate the predictions for Kaggle submission."},{"cell_type":"markdown","metadata":{"_uuid":"7ab5c93e35df3ce71e61b83e268d575e9f25b220"},"source":"## Dealing with missing values and dropping unwanted features"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"c0ebd664947b11be566432c789bec50c5c24fbd7"},"outputs":[],"source":"# Drop the passenger ID and Ticket columns, they're unlikely to have any predictive utility\n\ndf_train = df_train.drop(['PassengerId', 'Ticket'], axis = 1)\ndf_test = df_test.drop(['Ticket'], axis = 1)"},{"cell_type":"markdown","metadata":{"_uuid":"d22168ffd05341a4c271ba0cd8f2cb55f871b0cc"},"source":"Time to address the missing values in the Age column, which will caused problems for sklearn using .fillna(method =\"ffil\") substitutes the empty values with the last value and in this way doesn't effect the distribution, provided empty Age values aren't located next to one another"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"fdd7117b7d71532186f297130554709871fb88aa"},"outputs":[],"source":"df_train = df_train.fillna(method='ffill')\ndf_test = df_test.fillna(method='ffill')"},{"cell_type":"markdown","metadata":{"_uuid":"6ff46ea2cb723ba3a6541cd7694967dce60f2a1d"},"source":"## Converting categorical features to numerical representations"},{"cell_type":"markdown","metadata":{"_uuid":"f46634e8c0debcb7b28f946cf67645a9f7c55f3c"},"source":"Let's convert the Sex column from categorical to a one-hot encoded numeric representation, as dtype int64, so that sklearn can work with it"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"48fa9c04584f8529a2409a1b16c3a6792a1bffdb"},"outputs":[],"source":"df_train.Sex = df_train.Sex.map({'male':0, 'female':1}).astype(int)\ndf_test.Sex = df_test.Sex.map({'male':0, 'female':1}).astype(int)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"3d5d26f90b8f9cf5b2a593a07d862e6f88e4e224"},"outputs":[],"source":"df_train.Sex.value_counts()"},{"cell_type":"markdown","metadata":{"_uuid":"4d21760786e681157449dae89ac434cc2467e089"},"source":"Convert the embarked column to a numeric mapping"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"f5b9bae0f6c64f5fce73154177531443235c8082"},"outputs":[],"source":"data = [df_train, df_test]\n\nfor dataset in data:\n    dataset.Embarked = dataset.Embarked.map({'S':0, 'C':1, 'Q':2}).astype(int)"},{"cell_type":"markdown","metadata":{"_uuid":"919e4a169b892c14975aaf783d826265655b686b"},"source":"## Assigning continuous features to discrete bins"},{"cell_type":"markdown","metadata":{"_uuid":"a9c5853a4ed1bc8b502799f83cebeff12153aaf1"},"source":"Now to decide upon suitable Age bins I'll have a look at the Age distribution"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"97b3e3546d5574900cc4f354256e99883499595d"},"outputs":[],"source":"plt.subplots(figsize=(10,7))\nplt.xticks(np.arange(min(df_train.Age), max(df_train.Age)+1, 10.0))\nsns.distplot(df_train.Age)"},{"cell_type":"markdown","metadata":{"_uuid":"8c18e75f142cd89e7b913d549ff78084fbe6ef79"},"source":"The Age feature is somewhat normally distributed/slightly bimodal so I'll just assign the ages to 8 evenly spaced bins"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"d020ef239c30d69bab844686ffa4eb3b161744db"},"outputs":[],"source":"data = [df_train, df_test]\n\nfor df in data:\n    df['Age_bin']=np.nan\n    for i in range(8,0,-1):\n        df.loc[df['Age'] <= i*10, 'Age_bin'] = i"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"937b778ba09ce7e1cc80a5f1ad075ea712a23f72"},"outputs":[],"source":"data = [df_train, df_test]\n\nfor df in data:\n    df_train.Age_bin = df_train.Age_bin.astype(int)\n    df_test.Age_bin = df_test.Age_bin.astype(int)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"a94d9a3e7227c9f508957141576c24d2a19307df"},"outputs":[],"source":"sns.distplot(df_train.Age_bin)"},{"cell_type":"markdown","metadata":{"_uuid":"63acbe9fbc1b53aff185231f1b0c02eafe1f529b"},"source":"Time to assign the Fare values to suitable bins"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"44c6e28a24ba0db393e01d9d82ce4513e001bf9e"},"outputs":[],"source":"# Plot the Fares to take a look at their distributions\nplt.subplots(figsize=(30,10))\nplt.xticks(np.arange(min(df_train.Fare), max(df_train.Fare)+1, 10.0))\nsns.distplot(df_train.Fare)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"b27eed3331bc001f074a6ba0d88b3529c7ad4a20"},"outputs":[],"source":"# Based on the distribution of the Fares, assign them to the following bins corresponding to roughly each peak in \n# the distplot kde profile (gaussian kernel density estimate) in an attempt to capture the distribution\ndata = [df_train, df_test]\n\nfor dataset in data:\n    dataset.loc[ dataset['Fare'] <= 5, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 5) & (dataset['Fare'] <= 10), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 10) & (dataset['Fare'] <= 20), 'Fare'] = 2\n    dataset.loc[(dataset['Fare'] > 20) & (dataset['Fare'] <= 40), 'Fare']   = 3\n    dataset.loc[(dataset['Fare'] > 40) & (dataset['Fare'] <= 99), 'Fare']   = 4\n    dataset.loc[(dataset['Fare'] > 99) & (dataset['Fare'] <= 180), 'Fare']   = 5\n    dataset.loc[(dataset['Fare'] > 180) & (dataset['Fare'] <= 280), 'Fare']   = 6\n    dataset.loc[ dataset['Fare'] > 280, 'Fare'] = 7\n    dataset['Fare'] = dataset['Fare'].astype(int)"},{"cell_type":"markdown","metadata":{"_uuid":"b75465d33d21847fde3e5c7a941aa775a6c3f004"},"source":"## Extracting new features"},{"cell_type":"markdown","metadata":{"_uuid":"f7d5ed20f1c24e09eeec6b703ee364e81a334b63"},"source":"Use regular expressions to extract the Title and deck number from the Name and Cabin columns respectively and assign them one hot encodings"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"d5b87249c2d6ab90fcae51aab54d7e6d2c3be420"},"outputs":[],"source":"data = [df_train, df_test]\ntitles = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n\nfor dataset in data:\n    # use the regex ' ([A-Za-z]+)\\.' to extract the titles from the names\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n    \n    # Assign rare titles as Rare and convert Mme and Ms to the more common Mrs and Miss \n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr',\n                                            'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    # convert titles into numbers\n    dataset['Title'] = dataset['Title'].map(titles)\n    # fill NaN with 0 to keep the dimensions the same as the other features\n    dataset['Title'] = dataset['Title'].fillna(0)\n\n# Extract the Deck feature from the Cabin column    \n    \ndeck = {'A':1, 'B':2, 'C':3, 'D':4, 'E':5,'F':6,'G':7,'U':8}    \n    \nfor dataset in data:\n    dataset['Cabin'] = dataset['Cabin'].fillna('U0')\n    dataset['Deck'] = dataset['Cabin'].map(lambda x: re.compile('([a-zA-Z]+)').search(x).group())\n    dataset['Deck'] = dataset['Deck'].map(deck)\n    dataset['Deck'] = dataset['Deck'].fillna(0)\n    dataset['Deck'] = dataset['Deck'].astype(int)"},{"cell_type":"markdown","metadata":{"_uuid":"6f6e339665e889027f244e740eac2b65edba127b"},"source":"Create a not_alone binary feature. Perhaps those with families were more likely to survive, as Parch and SibSp were both positively correlated with survival."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"08e8590870633e4d34985b481d43bb1fea34d7ae"},"outputs":[],"source":"data = [df_train, df_test]\nfor dataset in data:\n    dataset['Relatives'] = dataset['SibSp'] + dataset['Parch']\n    dataset.loc[dataset['Relatives']>0, 'not_alone'] = 0\n    dataset.loc[dataset['Relatives']== 0 , 'not_alone'] = 1\n    dataset['not_alone'] = dataset['not_alone'].astype(int)"},{"cell_type":"markdown","metadata":{"_uuid":"dc49acf17289200d56c76ac47e04346486e9c0c3"},"source":"Males in Pclass 3 didn't fair well on the Titanic based on the plots above, so I'll generate an indicator variable to capture this correlation."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"e934ee23f9c84749f4a1d9e7230e86b156cf4fd1"},"outputs":[],"source":"data = [df_train, df_test]\n\nfor dataset in data:\n    dataset['Male_P3'] = (dataset['Sex'] == 0) & (dataset['Pclass'] == 3).astype(int)\n"},{"cell_type":"markdown","metadata":{"_uuid":"7f98c6a4872745bf14938820571a6b5b6a573bc5"},"source":"Multiplying features together, generates new features which may be useful. Age and Pclass showed the same correlation with Survival, perhaps multiplying them together will amplify their contribution to the classifers I'm about to train"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"26f1804bc30eb52696a52e0cb9d751b49274d1b3"},"outputs":[],"source":"data = [df_train, df_test]\nfor dataset in data:\n    dataset['Age_Class']= dataset['Age_bin']* dataset['Pclass']"},{"cell_type":"markdown","metadata":{"_uuid":"95f799e1db1c8dab590943ca17bbaa857d0a5ece"},"source":"Fare per person is an additional socio-economic indicator derived from the Fare and Relatives features, which correlates positively with Survival"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"066b86a86c285f41f221af9d84f25f19deb14086"},"outputs":[],"source":"for dataset in data:\n    dataset['Fare_Per_Person'] = dataset['Fare']/(dataset['Relatives']+1)\n    dataset['Fare_Per_Person'] = dataset['Fare_Per_Person'].astype(int)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"8725c8cce0f1421012cdfd404e5cf1a2ec6618a8"},"outputs":[],"source":"# Now drop the old continuous or text features which aren't required\ndata = [df_train, df_test]\n\nfor dataset in data:\n    dataset.drop(['Age','Name', 'Cabin'],axis=1,inplace=True)"},{"cell_type":"markdown","metadata":{"_uuid":"3caaa32bc178611e1662604752948feff9da95de"},"source":"Plotting the correlation matrix now that feature engineering is complete shows how much additional predictive signal we have generated ready for modelling"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"13bc05e1dc59712dc5fcd2faf3f33c3c3a1dda08"},"outputs":[],"source":"plt.subplots(figsize=(20,10))\ncorr = df_train.corr()\nsns.heatmap(corr, cmap = 'RdBu_r', center = 0)"},{"cell_type":"markdown","metadata":{"_uuid":"71efcb0432a53138932958a3507fdfda49976e5f"},"source":"# Training ML algorithms to predict survival with scikit-learn"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"6db88322bb68332ca99bc1e20436dc2a746c9a90"},"outputs":[],"source":"# Prepare dataframes for sklearn, so that the feature columns and target label are in separate dataframes\nX_training = df_train.drop('Survived', axis = 1).copy()\ny_training = df_train.Survived"},{"cell_type":"markdown","metadata":{"_uuid":"77ca91fde623a0b640ba0c679334bd14db1a2dd1"},"source":"Time to raid the sklearn library for some suitable classification algorithms, training tools and validatory metrics. For the binary classification task of predicting survival based on several features we could try some classic classifiers such as: support vector classifier, decision tree, random forests, K nearest neighbors and logistic regression.\n\nAlso worth trying are classifiers from the ensemble module of sklearn which implement tree \"boosting\", such as: adaptive boosting (AdaBoost), gradient boosting, exta trees and extreme gradient boosting (XGBoost) classifiers. "},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"4a47124568da60a639d6b6e47b327cf165f4d661"},"outputs":[],"source":"from sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, learning_curve, StratifiedShuffleSplit\n\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n"},{"cell_type":"markdown","metadata":{"_uuid":"52f3b1a7685dcb0d1a192216e6f7f518ade99ae8"},"source":"I could perform a train test split of the training dataframe using train_test_split...."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"bdd084713a0af17218287de582417dcd990cfcc2"},"outputs":[],"source":"X_train, X_test, y_train, y_test = train_test_split(df_train.drop('Survived',axis=1),\n                                                    df_train['Survived'], test_size=0.2, random_state=2)"},{"cell_type":"markdown","metadata":{"_uuid":"c64a5267d5ee8a152592c26e0fc780f41abbe01a"},"source":"...then train each classifier using the X_train and y_train subsets..."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"697bea167dbf55ef28844522780e5babc76fafbd"},"outputs":[],"source":"rfc = RandomForestClassifier()\n\nrfc.fit(X_train, y_train)"},{"cell_type":"markdown","metadata":{"_uuid":"b8c486b6fce2224679496d8af95bd1a416a14b59"},"source":"... and then assess the performance of the classifier against the test set, which the classifier hasn't been trained on, hopefully giving a sensible estimate of the classifier's out of sample performance via sklearn's accuracy score, which is just the fraction of correct predictions."},{"cell_type":"markdown","metadata":{"_uuid":"f3f867d085f4e1e0c12cf5ff48cdcb0c3015c40a"},"source":"![accuracy_score](http://scikit-learn.org/stable/_images/math/cd4bea15b385d15cceb8e24f68976da7d8510290.png)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"7ead52154f1baea9cc5ceabaddd52f27904f0c01"},"outputs":[],"source":"rfc_prediction = rfc.predict(X_test) \naccuracy_score(rfc_prediction, y_test)"},{"cell_type":"markdown","metadata":{"_uuid":"f03dac831cdc6ed04c6294066aaa093498b35b5a"},"source":"## K-fold cross validation to train the optimal classification algorithm"},{"cell_type":"markdown","metadata":{"_uuid":"b817b4313c70182c85cae33ca2773a867791a4aa"},"source":"However, in order to make the most out of the training data available, maximising the amount of signal gathered from each feature, a better way to train and assess the classifier is via cross validation, a process of iteratively splitting the training data into different training and testing sets so that all of the data is used to both train and test the classifier, taking an average of the performance to give a more realistic impression of the model's performance. This also provides a more reliable accuracy metric by which to compare rival algorithms on a given problem.\n\nThe image below illustrates the process of splitting a dataset into K-folds (K = 5) for cross validation."},{"cell_type":"markdown","metadata":{"_uuid":"daeb7a40c98f794f106acecbc5098d7f1f4d6b97"},"source":"![cv](http://tomaszkacmajor.pl/wp-content/uploads/2016/05/cross-validation.png)"},{"cell_type":"markdown","metadata":{"_uuid":"bc74b47da7b60f68602e403c0d5daf6ddbe1499e"},"source":"https://www.edureka.co/blog/implementation-of-decision-tree/"},{"cell_type":"markdown","metadata":{"_uuid":"4ffc7b177f10ddeb1c70e03966ed3145cf9eb0d5"},"source":"To ensure the cross validation folds are not derived from chunks of the dataset which are unrepresentative of the distributions seen in the dataset as a whole, shuffling and stratification can be specified in the cross validation options."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"1d1ff786322981d8fe1cfe98524e55f6f43be153"},"outputs":[],"source":"# stratified shuffle split of the data into 10 folds, with 20% of the data used in testing\nsss = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=2)"},{"cell_type":"markdown","metadata":{"_uuid":"b30842952cdb8328f3655b75b236159f85433d4e"},"source":"Using the cell below I can perform the same training and testing as outlined above using train_test_split but instead training and testing on all available data via cross validation, whilst testing multiple classifiers in parallel."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"def8ee0763d05af7e57e10bce5c5ba10fb03d239"},"outputs":[],"source":"random_state=2\n\n# store all the classifiers in a list\nclassifiers=[]\nclassifiers.append(SVC(random_state=random_state))\nclassifiers.append(DecisionTreeClassifier(random_state=random_state))\nclassifiers.append(RandomForestClassifier(random_state=random_state))\nclassifiers.append(KNeighborsClassifier())\nclassifiers.append(LogisticRegression(random_state=random_state))\nclassifiers.append(AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state),random_state=random_state))\nclassifiers.append(ExtraTreesClassifier(random_state=random_state))\nclassifiers.append(GradientBoostingClassifier(random_state=random_state))\nclassifiers.append(XGBClassifier(random_state=random_state))\n\n# store the cross validation accuracy results in a list\ncv_results=[]\nfor classifier in classifiers:\n    cv_results.append(cross_val_score(estimator=classifier,X=X_training,y=y_training,\n                                      cv=sss,scoring='accuracy', n_jobs=-1))\n\n# store the mean accuracy and standard deviation of the accuracy for each cross validation fold in lists    \ncv_means=[]\ncv_std=[]\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n\n# Combine the results lists in a dataframe\ncv_results=pd.DataFrame({'CV_mean':cv_means , 'CV_std':cv_std , 'Algorithm':['SVC','DTC','RFC','KNN','LR',\n                                                                             'ADA','XT','GBC','XGB']})\n# sort the results by score\ncv_results = cv_results.sort_values('CV_mean',ascending=False)\n\n# plot the values for swift visual assessment of classifier performance\nplt.subplots(figsize=(8,5))\nsns.barplot('Algorithm','CV_mean',data=cv_results)"},{"cell_type":"markdown","metadata":{"_uuid":"5341f05307c70613fc94282f323878eeb58add88"},"source":"Looks like XGBoost gives the best mean accuracy score from cross validation"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"a9d45b6f71b86c915f0463504b1c21c5924c9473"},"outputs":[],"source":"cv_results"},{"cell_type":"markdown","metadata":{"_uuid":"a664dcb4982ff4fa4a11adf6044ed80a155cdbfd"},"source":"# What is XGBoost and how does it work?"},{"cell_type":"markdown","metadata":{"_uuid":"b6728754d9c6ec4aec7a741aeba2abcfd30e32f6"},"source":"Decision tree classifiers are supervised machine learning algorithms, which are trained by maximising the separation of classes achieved by decision bounderies in labelled data and thereby allow you to classify previously unseen data. XGBoost is a **tree ensemble** based classification algorithm which uses many shallow decision trees to classify the input features, which individually are only slightly better than random guessing. These are  known as **weak learners** and have **high bias and low variance**. The predictions of these weak learners are summed to give an overall prediction for a given test example's set of feature values, in a similar manner to a random forest classifier but with much shallower trees. The tree boosting of XGBoost refers to the **minimisation of the bias** of the model, by optimisation of the individual tree structures (number of leaves) over many boosting steps, with each step attempting to add a new tree to the model.\n\nTL;DR\n\nXGBoost is a tree boosting alogrithm using ensembles of shallow decision trees."},{"cell_type":"markdown","metadata":{"_uuid":"0127da70aab4df796c14e5667e21d1bc44ae5b71"},"source":"![decisiontreeforest](http://cdn-ak.f.st-hatena.com/images/fotolife/T/TJO/20150603/20150603222152.png)"},{"cell_type":"markdown","metadata":{"_uuid":"a9b920c5f9fa2530bd5b2e22f7667880b41799f7"},"source":"http://tjo-en.hatenablog.com/entry/2015/06/04/190000"},{"cell_type":"markdown","metadata":{"_uuid":"b81fe9d97f86c70107de25cf2e08764afabcbd5e"},"source":"The weak learners work something like the diagram from the XGBoost documentation below. A given tree in the model will be optimised to draw decision boundaries through distributions of the features I've engineered, which maximise the separation of the Survived 0 and 1 classes. The scores for a given terminal node, will then be based on the probability that an example satisfying that criteria/threshold, belongs to either the Survived = 0 or Survived = 1 class. The scores for all the trees are combined to generate a prediction for a given testing example's set of feature values. The boosting procedure optimises the number of trees, leaf scores and structure of these trees, to minimise the classifier's bias and maximise it's predictive accuracy."},{"cell_type":"markdown","metadata":{"_uuid":"e440416bd3583fc477bac464d27b72608245f8f5"},"source":"![weaklearners](https://raw.githubusercontent.com/dmlc/web-data/master/xgboost/model/twocart.png)"},{"cell_type":"markdown","metadata":{"_uuid":"36a84444b1446a937b09da50045811ec11c1167e"},"source":"A detailed description of XGBoost is given here http://xgboost.readthedocs.io/en/latest/model.html"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"6bee261032c371f67490302a23f6be9e8c7ce4b2"},"outputs":[],"source":"xgbc = XGBClassifier(random_state=random_state)\nxgbc.fit(X_training, y_training)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"043d264c57de5ec8cdc78945337471dabeac8f8c"},"outputs":[],"source":"scores = cross_val_score(xgbc, X_training, y_training, cv=sss, scoring = \"accuracy\")\n\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())\n"},{"cell_type":"markdown","metadata":{"_uuid":"467948065eb59d46b98ba1d45c72eec0ad174abe"},"source":"The XGBoost classifier has a mean CV accuracy of 83.07% on the training data with default parameters. This can be improved by tweaking features and model parameters."},{"cell_type":"markdown","metadata":{"_uuid":"d682272f5edf4a477eb263f21fd46a75fa2c4980"},"source":"## Feature importance\n\nWhich of the features I engineered is XGBoost using most effectively to generate trees which minimise it's error function?"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"c1850aede106cc3508c46d06f112adb5bf64c907"},"outputs":[],"source":"importances = pd.DataFrame({'feature':X_training.columns,'importance':np.round(xgbc.feature_importances_,3)})\n\nimportances = importances.sort_values('importance',ascending=False).set_index('feature')\n"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"scrolled":false,"_uuid":"a611f7c4ee9ce9d3636759a57d551054cff4e800"},"outputs":[],"source":"importances.plot.bar(figsize=(10,5))"},{"cell_type":"markdown","metadata":{"_uuid":"0a3ba71947ccf280f30482e9c4d7472d554a7fea"},"source":"If I drop a few of the less important features will the cv accuracy increase? Will I increase the overall signal to noise of my predictive features?"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"514dfbe8bbb22d361d479255b73e30c68187ed74"},"outputs":[],"source":"X_training = X_training.drop(['not_alone', 'Male_P3','Parch'], axis=1)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"d50f65c7dca66068e9f47debdff7751cf03571da"},"outputs":[],"source":"xgbc = XGBClassifier(random_state=random_state)\nxgbc.fit(X_training, y_training)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"47aaccf647081abd5e4305538c8a3e51f7b486d8"},"outputs":[],"source":"scores = cross_val_score(xgbc, X_training, y_training, cv=sss, scoring = \"accuracy\")\n\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())\n"},{"cell_type":"markdown","metadata":{"_uuid":"7f9bf6d9742dac2705688a19704d8027fcc39534"},"source":"Looks like dropping the **not_alone, Male_P3 and Parch** features improved the mean CV accuracy score slightly. "},{"cell_type":"markdown","metadata":{"_uuid":"913e572b6fe3c5dff3942381bc279ba3d97c677b"},"source":"## Hyperparameter tuning with GridSearch CV\nNow to reduce overfitting by tuning the model parameters relevant to model complexity "},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"9136f07ca0d0e7ee4ba34c56dff67991a0786e28"},"outputs":[],"source":"# GridSearch CV automatically tests a series of model parameters through the cross validation specified and outputs\n# the parameters which generated the best cross validation score\n\nparams = {\n          'max_depth' : range(3,10,1),       # maximum depth of the trees - deeper = more likely to overfit\n          'min_child_weight' : range(1,5,1), # minimum child weight - The lower the more likely to overfit\n          'gamma' : [1,2,3,4],               # gamma - regularisation parameter\n         }\n\nxgbc = XGBClassifier()\n\nclf = GridSearchCV(estimator=xgbc, param_grid=params, cv = sss, n_jobs=-1)\n\nclf.fit(X_training, y_training)\n\nclf.best_params_"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"5ec30aed901cb931cae7b7b8c4b73c91f1d37516"},"outputs":[],"source":"xgbc = XGBClassifier(random_state=random_state, gamma = 2, max_depth = 8, min_child_weight = 2)\nxgbc.fit(X_training, y_training)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"cce0e0f7026683234b5aced3609852eb3375c831"},"outputs":[],"source":"scores = cross_val_score(xgbc, X_training, y_training, cv=sss, scoring = \"accuracy\")\n\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())"},{"cell_type":"markdown","metadata":{"_uuid":"9997c47bd1d523b9786b5c74511e5c9d43e7d41a"},"source":"So parameter tuning has also improved the mean cross validation accuracy of the classifier slightly. I can now generate some plots and other metrics to get a sense of how well the XGBoost classifier is working."},{"cell_type":"markdown","metadata":{"_uuid":"ffdf8a48415a6d5303736cb66ed11ef054a3d1f9"},"source":"# Model evaluation"},{"cell_type":"markdown","metadata":{"_uuid":"aa2d855d4fcb72642e7a5928497bb6c39b2f95cb"},"source":"## Learning curves: bias vs variance\n\n\nError = Bias + Variance. Bias being the difference between predicitions and target labels and variance being the spread of predictions from each other.  Learning curves of error vs training set size can be used to assess the **bias and variance** of your model"},{"cell_type":"markdown","metadata":{"_uuid":"248afac9a2d9371206907261614bde5d77fe2ac2"},"source":"![biasvariance](https://i.imgur.com/I1rzmAY.png?1)"},{"cell_type":"markdown","metadata":{"_uuid":"bf10d260f1a0d9a6d8461db597434f5b3c61ed75"},"source":"Andrew Ng Stanford ML course slides"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"scrolled":false,"_uuid":"8a6c76d9c067da73e788787057fea3d8483df60e"},"outputs":[],"source":"train_sizes, train_scores, test_scores = learning_curve(xgbc, X_training, y_training, n_jobs=-1, cv=sss, train_sizes=np.linspace(.1, 1.0, 5), verbose=0)\n\n# calculate mean and standard deviation for training and testing scores\ntrain_scores_mean = np.mean(train_scores, axis=1)\ntrain_scores_std = np.std(train_scores, axis=1)\ntest_scores_mean = np.mean(test_scores, axis=1)\ntest_scores_std = np.std(test_scores, axis=1)\n\n# setup the plot\nplt.figure(figsize = (8,5))\nplt.title('XGBoost classifier')\nplt.xlabel(\"Training examples\")\nplt.ylabel(\"Score\")\nplt.plot([1,760],[1,1], linewidth=2, color = 'r')\n\nplt.grid()\n    \n# shade the area +/- one standard deviation of the mean scores\nplt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"b\")\nplt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    \n# plot the mean training and test scores vs training set size\nplt.plot(train_sizes, train_scores_mean, 'o-', color=\"b\", label=\"Training score\")\nplt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n\nplt.ylim(-.1,1.1)"},{"cell_type":"markdown","metadata":{"_uuid":"ca0772240cc1d95acb179e5e2ff83806886a241d"},"source":"Looking at my learning curve it looks like the the XGBoost classifier is definitely of the **high bias low variance** type. The curves are also quite flat suggesting adding more data is unlikely to improve performance substantially."},{"cell_type":"markdown","metadata":{"_uuid":"d65ac3a7c9bcfcfeb10b043dc753b40bc000994d"},"source":"## Confusion matrix\nThe confusion matrix is a plot of the predictions generated by the classifier, separated into True predictions (true positives (TP) and true negatives (TN)) and False predictions (false positives (FP) and false negatives (FN)), allowing you to visualise which predictions your model is getting wrong most often"},{"cell_type":"markdown","metadata":{"_uuid":"15e323cefd2b643feaa2c1d85459947272196b5d"},"source":"![confusion_matrix](http://revolution-computing.typepad.com/.a/6a010534b1db25970b01bb08c97955970d-pi)"},{"cell_type":"markdown","metadata":{"_uuid":"60b08d49c3d83fd8618a9649e1b9c6cf13176f27"},"source":"http://revolution-computing.typepad.com/.a/6a010534b1db25970b01bb08c97955970d-pi"},{"cell_type":"markdown","metadata":{"_uuid":"39d6e5d0a422bba140714ffd32dd45df16cab57a"},"source":"In the confusion matrix plot below you can see that the classifier predicts survived = 0 more than survived = 1 and the false positives and false negatives look comparatively low. Nevertheless it looks to have more false survived = 0 predictions than false survived = 1 predictions, perhaps due to the greater number of dead individuals in the training data."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"a54541da75e468f75a88aace6d1c0ae3b71d4d11"},"outputs":[],"source":"from sklearn.model_selection import cross_val_predict\n# from pandas_ml import ConfusionMatrix\npredictions = cross_val_predict(xgbc, X_training, y_training, cv=10)\n# cm = ConfusionMatrix(y_training, predictions)\n# cm.plot()"},{"cell_type":"markdown","metadata":{"_uuid":"9ef2b8c38f41d2981e3b094eaf85e85a6d7b421d"},"source":"![pandas_ml_cm](https://i.imgur.com/pWeT9w7.png)"},{"cell_type":"markdown","metadata":{"_uuid":"69720e6edfe4a5cdf66a3c6e046da5b8f385691f"},"source":"## Precision and recall"},{"cell_type":"markdown","metadata":{"_uuid":"45e737d97fa61a5a8c0fedcb4d3ee11ae3cc1b38"},"source":"Precision and recall are useful for assessing model performance in instances where the distributions of classes in the training data are imbalanced. \n\nPrecision = of the times positive is predicted, what fraction of those predictions were correct (dependent on bias).\n\n\n$$ Precision = \\dfrac {TP} {TP+FP} $$ \n\nRecall = of the positive class examples present in the data, what fraction were predicted successfully (dependent on variance).\n\n\n$$ Recall = \\dfrac {TP} {TP+FN} $$\n\nPrecision and recall can be used to calculate the F1 score, the harmonic mean of precision and recall and another performance metric to look at:\n\n$$ F1 = 2 * \\dfrac {(Precision * Recall)} {(Precision + Recall)} $$"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"d6966aa319d3d9617ab09badd6ba04dfd933cac8"},"outputs":[],"source":"from sklearn.metrics import f1_score\nf1_score(y_training, predictions)"},{"cell_type":"markdown","metadata":{"_uuid":"919ed9340ea653a8240f803725516a3cd68117ac"},"source":"Plotting precision vs recall gives an idea of overall model performance. Curves for perfect binary classifiers shown below. Notice how the baseline model (50/50 random prediction of either class) shifts dependent on the distribution of classes in the dataset."},{"cell_type":"markdown","metadata":{"_uuid":"e8db90b5917c7ed45f19f49da737fa4be282977c"},"source":"![precision_recall](https://classeval.files.wordpress.com/2015/06/perfect-precision-recall-curve1.png?w=840&h=448)"},{"cell_type":"markdown","metadata":{"_uuid":"6027651d59518ea2d4fd5b617098f65390d2c09e"},"source":"https://classeval.wordpress.com/introduction/introduction-to-the-precision-recall-plot/"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"e6133549dd22e071602b14a63fd6ceb9f6fc5237"},"outputs":[],"source":"from sklearn.metrics import precision_recall_curve\n\n# generate predictions from the training data\ny_scores = xgbc.predict_proba(X_training)\ny_scores = y_scores[:,1]\n\nprecision, recall, threshold = precision_recall_curve(y_training, y_scores)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"bee8e8b04cb4c13b7011239a36507f2d28a5582f"},"outputs":[],"source":"def plot_precision_vs_recall(precision, recall):\n    plt.plot(recall, precision, \"g\", linewidth=2)\n    plt.ylabel(\"Recall\")\n    plt.xlabel(\"Precision\")\n    plt.axis([0, 1.5, 0, 1.5])\n    plt.xlim(0,1)\n    plt.ylim(0,1.1)\n    \nplt.figure(figsize=(8, 5))\nplot_precision_vs_recall(precision, recall)"},{"cell_type":"markdown","metadata":{"_uuid":"7647efc2a91e7e501f3db7985c1134f917be08a2"},"source":"Looks like my classifier is not perfect but not too bad."},{"cell_type":"markdown","metadata":{"_uuid":"eee583e0b167cbf2377bf5800589015b935e72ce"},"source":"## Precision vs recall trade-off\n\nDepending on the application of the classifier you may want to adjust the model for high precision lower recall or higher recall and lower precision. In this case I just want to make the most accurate predictor but if for example the consequences of subsequent decisions based on false positives were worse than false negatives, you would want to adjust the threshold to increase the precision at the expense of the recall of the classifier. You can calculate the precision and recall for a range of threshold values (cut off values generating a decision boundary between the two classes in the binary classifier) and plot them to visualise the precison/recall trade-off of the model."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"2eb070b55decb557719cdb80a6d9df3eee9ad771"},"outputs":[],"source":"def plot_precision_and_recall(precision, recall, threshold):\n    plt.plot(threshold, precision[:-1], \"r-\", label=\"Precision\", linewidth=2)\n    plt.plot(threshold, recall[:-1], \"b\", label=\"Recall\", linewidth=2)\n    plt.xlabel(\"Threshold\")\n    plt.legend(loc=\"upper right\")\n    plt.ylim([0, 1])\n\nplt.figure(figsize=(8, 5))\nplot_precision_and_recall(precision, recall, threshold)"},{"cell_type":"markdown","metadata":{"_uuid":"71cedaca989440803ff27246dda35754f8e8640e"},"source":"Based on the curve above if a precision of ~90% was desired a threshold of 0.6 could be used, which would come at the cost of a recall of ~75%"},{"cell_type":"markdown","metadata":{"_uuid":"082883981be04824c30cf40bdc621f06f9b5696b"},"source":"## Reciever operator characteristic - area under curve (ROC AUC)\nAnother visualisation of model performance is the ROC curve, a plot of true positive rate vs false positive rate, which also gives you the ROC-AUC performance metric. A random classifier would give a plot like the red line whilst a perfect classifier would follow the green line."},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"b9596bb30011f6741fcdbafefdcc89919dffc353"},"outputs":[],"source":"from sklearn.metrics import roc_curve\n\n# calculate rates of false +ve and true +ve predictions\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_training, y_scores)\n\ndef plot_roc_curve(false_positive_rate, true_positive_rate, label=None):\n    plt.plot(false_positive_rate, true_positive_rate, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], 'r', linewidth=2)\n    plt.plot([0, 1], [1, 1], 'g', linewidth=1.5)\n    plt.plot([0, 0], [1, 0], 'g', linewidth=1.5)\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.ylim(0,1.02)\n    plt.xlim(-0.005,1)\n\nplt.figure(figsize=(8, 5))\nplot_roc_curve(false_positive_rate, true_positive_rate)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"73adcdf11bcb4035f6114239e48f9bb54b37d635"},"outputs":[],"source":"from sklearn.metrics import roc_auc_score\nr_a_score = roc_auc_score(y_training, y_scores)\nprint(\"ROC-AUC Score:\", r_a_score)"},{"cell_type":"markdown","metadata":{"_uuid":"10b9c38cfd25337bd5f568ee09426aff1e47d9ba"},"source":"The curve looks to be closer to the top left hand corner than the red line and so the AUC score is close approaching 1. The classifier is doing a reasonable job!"},{"cell_type":"markdown","metadata":{"_uuid":"d8ff15824a7a96354023e7e0227e0155a7081c7a"},"source":"# Submission"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"ee757d5ac3f773d69fbff103b6a7f6cace9a2722"},"outputs":[],"source":"submission_testing = df_test.drop(['PassengerId','not_alone', 'Male_P3','Parch'], axis = 1).copy()\nY_prediction = xgbc.predict(submission_testing)"},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"_uuid":"44bba37aff478846b1a28bb6259f331e0b42786f"},"outputs":[],"source":"submission = pd.DataFrame({\n        \"PassengerId\": df_test[\"PassengerId\"],\n        \"Survived\": Y_prediction\n    })\nsubmission.to_csv('submission.csv', index=False)"},{"cell_type":"markdown","metadata":{"_uuid":"d98033dfe5c6018923b51c97ea39299ad2b666df"},"source":"# Conclusions\nExploration of the data shows many features correlate with survival. Interestingly the XGBoost classifier which cross validation indicated was the most accurate algorithm, didn't rely heavily on all of the engineered features  according to the importance plot. The relatively low importance of the Sex feature, which seemed a very strongly correlated feature with survival was suprising. The accuracy of the XGBoost classifier I have trained could definitely be improved through systematic feature selection and parameter tuning. Nevertheless with the minimal tweaking demonstrated here it is reasonably accurate. \n\n** Many thanks to the countless Kaggle kernel authors and blog writers whose work helped me in developing this kernel.**"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.1"}},"nbformat":4,"nbformat_minor":2}