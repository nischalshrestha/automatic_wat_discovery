{"cells":[{"metadata":{"_cell_guid":"2134091f-1b4b-47aa-a0de-64938d3ea201","_uuid":"740e4edb3c044ff542d9f80638dfce68e0a6f320"},"cell_type":"markdown","source":"# **Introduction**\n\nThis is a demonstration the Pandas and Scikit-learn libraries. Our goal is to create a model that predicts which passengers onboard the titanic survived the disaster. "},{"metadata":{"_uuid":"cb4395e866f268470e39e7b772cd1bc99f7cccf6"},"cell_type":"markdown","source":"# Obtaining Data\n\nFirst, let's import the tools we will be using, as well as loading in the data provided: "},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"2ee954407a1cdfd7c61503f4c0f956a48ed06333"},"cell_type":"code","source":"import numpy as np\nimport re\nimport pandas\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import linear_model\nfrom sklearn import preprocessing\nfrom sklearn.neural_network import MLPClassifier\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neighbors import NearestNeighbors\n\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n\noriginal = pandas.read_csv(\"../input/train.csv\")","execution_count":126,"outputs":[]},{"metadata":{"_uuid":"e4037dd9a4e3a9ba2e1b9801a969e650ca7cd0d8"},"cell_type":"markdown","source":"# Features"},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"89a5aae160b4c37975e3442668d59fc86dafe05d"},"cell_type":"code","source":"print(original.columns.values)","execution_count":79,"outputs":[]},{"metadata":{"_uuid":"cb2bbf618b9e426d01dc7d1b494b6de79dd9770a"},"cell_type":"markdown","source":"Let's break down immedieately signifant features, from an intuitive point of view: \n\n**Pclass, Sex, Age**\n\nImmedieately, we know that this is probably important: the rich, young, and female were prioritized spots on lifeboats. \n\n**Name**\n\nInitally, it is unclear how we can use the passenger's name in our model. However, we notice that the *title* of the passenger is also included here. Perhaps that will be useful. \n\n**Parch, Sibsp**\n\nPerhaps large families were priortized for spots on lifeboats? \n\n** Fare**\n\nThis is a great indication of the passenger's socio-economic status. Again, the rich were favoured for spots on the lifeboats. \n\n** Cabin** \n\nDoes the first letter represent the level their rooms were on? If so, this could allude to their social status. However, since we are already given ticket fare and class, cabin number does not seem to offer anything new. "},{"metadata":{"_uuid":"1137b5ad7375e71ba57b70878459bd9499fe8ae9"},"cell_type":"markdown","source":"# Missing and Restoring Data\n\nSome passengers are missing some data entries. In particular, age, cabin, and embarked are missing values. While age and embarked all have a good number of entries, we have less than a quarter of the cabin numbers. \n\n**Age**\n\nThe simplest solution is to simply use the mean value of the column. It is indeed possible to use the other features of a passenger to make a better prediction of their age, but for this demonstration I will limit to using the mean: "},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"f1eabb843923448183e0abd92f75379842e9aeed"},"cell_type":"code","source":"original['Age'] = original['Age'].replace(np.nan, original['Age'].mean(), regex=True)","execution_count":128,"outputs":[]},{"metadata":{"_uuid":"5a9a345331a99bf6e7d3f05bd6dd63e2a636882f"},"cell_type":"markdown","source":"**Embarked**\n\nWe replace missing values with M(issing):"},{"metadata":{"trusted":false,"_uuid":"8327b14e1cf4c00737f75485f11136e6b0c622dd"},"cell_type":"code","source":"original['Embarked'] = original['Embarked'].replace(np.nan, \"M\", regex=True)\n#We fill in missing values with M\n\noriginal.sample(5)","execution_count":130,"outputs":[]},{"metadata":{"_uuid":"f4047d4c894f1d5975af0786247ea8931a9812a8"},"cell_type":"markdown","source":"# Extracting Titles\n\nTitles will be useful to us! Create a new column in our dataframe only for the column. Note that they are preceeded by \", \", and ends with \".\": "},{"metadata":{"_kg_hide-output":true,"trusted":false,"_uuid":"6b6209cb00532e9874365449f70f3bf030c9b5b3"},"cell_type":"code","source":"name = original['Name']\ntitles = []\n\nfor i in range(len(name)):\n\ts = name[i]\n\ttitle = re.search(', (.*)\\.', s)\n\ttitle = title.group(1)\n\ttitles.append(title)\n\noriginal['Titles'] = titles","execution_count":131,"outputs":[]},{"metadata":{"_uuid":"e91b5e534e35f558abb02fe435ec09ff1504900e"},"cell_type":"markdown","source":"We now have a new column in our dataframe called Titles: "},{"metadata":{"trusted":false,"_uuid":"ba622789cbd62caa1018b7d669d8f771e6a679ba"},"cell_type":"code","source":"original.sample(5)","execution_count":98,"outputs":[]},{"metadata":{"_uuid":"c7ef4ab1e1fc07ff41b5d1d03096fa93192c1536"},"cell_type":"markdown","source":"Credit to Manav Sehgal for totalling the number of occurances of the titles we have just generated. We notice some very rare titles (e.g. Countess). Let's group these titles together. Also, we can group together titles that have equvilent meanings, or ones that may have simply been spelling errors (e.g. Mme and Mrs, Mlle and Miss, Ms and Miss ... etc): "},{"metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"trusted":false,"_uuid":"5d205465c2c7dea14fa627cdf0c8dcc323cca464"},"cell_type":"code","source":"original['Titles'].replace(['Sir', 'Rev', 'Major', 'Lady', 'Jonkheer', 'Dr', 'Don', 'Countess', 'Col', 'Capt'], 'Name')\n\noriginal['Titles'].replace(['Ms', 'Mme', 'Mlle'], ['Miss', 'Mrs', 'Miss'])","execution_count":133,"outputs":[]},{"metadata":{"_uuid":"1fc5ed525285cf366fcbf8f6dcd5e986baf643ea"},"cell_type":"markdown","source":"# One Hot Encoding\n\nSome features are given to us in numerical form, such as age, may not have the same ordinal significance that their numerical value may imply.  Hence, we categorize these features usling the Sklearn LabelBinarizer: "},{"metadata":{"trusted":false,"_uuid":"4c8011485207b418f68197775b771a1ec469e0de"},"cell_type":"code","source":"Sex_binarized = pandas.DataFrame(preprocessing.LabelBinarizer().fit_transform(original.Sex))\nTicket_binarized = pandas.DataFrame(preprocessing.LabelBinarizer().fit_transform(original.Ticket))\nEmbarked_binarized = pandas.DataFrame(preprocessing.LabelBinarizer().fit_transform(original.Embarked))\nTitles_binarized = pandas.DataFrame(preprocessing.LabelBinarizer().fit_transform(original.Titles))","execution_count":135,"outputs":[]},{"metadata":{"_uuid":"18c14ba751caa484ff3afbc60d543f490f05a674"},"cell_type":"markdown","source":"# Collecting Variables\n\nWe have managed to isolate some variables that seem reasonable for our purpose, Let's collect the features that we will be using into a new dataframe. Also, we know the result of our prediction: whether or not the particular passenger survives or not (1 and 0, respectively). While we're here, let's split our data into two sets, one for training and one for testing our model. We use Sklearn's train_test_split to do so:"},{"metadata":{"trusted":false,"_uuid":"ea5a58932246afe554334f12bee391a3871e98d7"},"cell_type":"code","source":"prediction_params = pandas.concat([Sex_binarized, Ticket_binarized, Embarked_binarized, Titles_binarized, original.Pclass, original.Age, original.SibSp, original.Fare, original.Parch], axis=1)\nprediction_result = original.Survived\n\nx_train, x_test, y_train, y_test = train_test_split(prediction_params, prediction_result, test_size = 0.15, random_state = 10)","execution_count":136,"outputs":[]},{"metadata":{"_uuid":"bd9148b7c654694ba57579e0f0d2bfc251f56000"},"cell_type":"markdown","source":"# Creating our Models: \n\nThis is an obvious classification problem. Our first instinct would be to use a **logistic model**. This does surprisingly well: "},{"metadata":{"trusted":false,"_uuid":"63ce8e4911bc81c4f04be0b813219f99986e656b"},"cell_type":"code","source":"logistic_model = linear_model.LogisticRegression().fit(x_train, y_train.values.ravel())\nlogistic_prediction = logistic_model.predict(x_test)\n\naccuracy_score(logistic_prediction, y_test)","execution_count":137,"outputs":[]},{"metadata":{"_uuid":"927ea3f8727d6b1223a022c8c75688e090a5df22"},"cell_type":"markdown","source":"Another obvious one is using a **decision tree**: "},{"metadata":{"trusted":false,"_uuid":"ae346b632006d6209a19983e92c12680b11c003e"},"cell_type":"code","source":"dt = DecisionTreeClassifier()\ndt.fit(x_train, y_train)\ndt_prediction = dt.predict(x_test)\n\naccuracy_score(dt_prediction, y_test)","execution_count":43,"outputs":[]},{"metadata":{"_uuid":"314abf2ed4faf214889136b0ecf6509c6881452f"},"cell_type":"markdown","source":"**Random Forests**:  "},{"metadata":{"trusted":false,"_uuid":"0b874938652d3eba83e814a2e9c474403222801d"},"cell_type":"code","source":"rf = RandomForestClassifier()\nrf.fit(x_train, y_train)\nrf_prediction = rf.predict(x_test)\n\naccuracy_score(rf_prediction, y_test)","execution_count":44,"outputs":[]},{"metadata":{"_uuid":"1317be85ba0733583710b4df96357b3cc5c798a6"},"cell_type":"markdown","source":"Finally, we can consider the use of a perceptron classifier. We use a relu activation function, and a single hidden layer about 1/5 the size of the input layer. "},{"metadata":{"trusted":false,"_uuid":"1f8cea3c2c0aaa214e2861a9f81d1b0400eadd8a"},"cell_type":"code","source":"clf = MLPClassifier(activation = 'relu', solver='lbfgs', hidden_layer_sizes=(150), random_state=10)\nclf.fit(x_train, y_train)\nneural_prediction = clf.predict(x_test)\n\naccuracy_score(neural_prediction, y_test)","execution_count":139,"outputs":[]},{"metadata":{"_uuid":"c6b29982994eceba9017ceb75e564e2df39d9816"},"cell_type":"markdown","source":"# Conclusion\n\nWhile some have achieved more accurate results using a decision tree or a random forest sampling, it would seem that our logistic regression produced the most accurate result. We submit at another day  because I have a calculus exam tomorrow and am very sleep deprived.. "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}