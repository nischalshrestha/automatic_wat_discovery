{"cells":[{"metadata":{"_cell_guid":"8819d5f7-ae64-4481-ac59-076227a1896e","_uuid":"108afaba1e52e81faf8991652ffcb70a0672bea3"},"cell_type":"markdown","source":"**First Kaggle kernel - Building a sensible, robust solution to the Titanic problem**\n\n*This is by no means an exhaustive study, however it should serve as a good overview of building a simple first model which performs well without succumbing to overfitting.\nThis method resulted in a public score of 0.80382 using a random forest model.\nInspiration for much of the feature engineering came from [this](https://www.kaggle.com/erikbruin/titanic-2nd-degree-families-and-majority-voting) very thorough kernel.\n*\n1. Reading the data and an initial look\n2. Dealing with missing age data\n3. Intuitions and high level feature analysis\n4. Calculating group size, family or friends?\n5. Building our features and fitting a model\n6. Potential improvements\n\n"},{"metadata":{"_cell_guid":"04cb3c48-446b-41cf-9b77-a41a7ba23e1d","_uuid":"adf6647d15fd4aaca8b6274e6a95faca38a95e0b"},"cell_type":"markdown","source":"**1. Reading the data and an initial look**\n\nFirst, we can glance at the data frames to check they were read correctly\n"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","scrolled":false,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":false,"collapsed":true},"cell_type":"code","source":"\nimport numpy as np \nimport pandas as pd \n\ndf = pd.read_csv('../input/train.csv')\ndf_test = pd.read_csv('../input/test.csv')\ndf_full = pd.concat([df,df_test])\n\n\n\ndf.head()\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4ae3d49e-3d55-437a-b923-2e069a1c83e3","_uuid":"a0b63ade5f41d84f0abfca888b574d94262294c6","collapsed":true,"trusted":false},"cell_type":"code","source":"df_test.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1e52e769-162e-475b-8988-03f3e93a78bb","_uuid":"b37e78f8d7045e3295cc339d98057b1feff43ba5"},"cell_type":"markdown","source":"Check for missing data by counting the nulls in each dataframe"},{"metadata":{"_cell_guid":"5471d5e9-7038-4913-ab54-cdfad6a412bf","_uuid":"f8f0aa8c2e0fa76dd2ed54d49a4fa139df8c1866","collapsed":true,"trusted":false},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e20e4af4-0631-4426-8dc1-3ed3e47ce356","_uuid":"a7726f5a33320d1598ef70c085215c2d5e8d3d40","collapsed":true,"trusted":false},"cell_type":"code","source":"df_test.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5006a9cd-9948-4a44-9e62-9b37f1bebc01","_uuid":"8e22d40e1da12ed8d733172233864829cd6c6f03"},"cell_type":"markdown","source":"The cabin sparsity is the biggest concern here, with 687 out of 891 records missing there's very little chance that it will be of interest to us compared to other features. \n\nAge has a significant rate of missing data (~20%) so some care will need to be taken when imputing this feature.\n\nIf embarked is to be used then it would most likely be sufficient to just use the most frequent entry as there is no missing data in the test set and only 2 missing rows in the training set. "},{"metadata":{"_cell_guid":"8f478024-670e-4f64-8831-1108cda0483f","_uuid":"c629dfe500afa6e5f68d042da119146dc4a26fcc"},"cell_type":"markdown","source":"**2. Dealing with missing age data**\n\nWith age likely a significant indicator of survival, we need to impute missing data with a little more care than a blunt mean/median approach. \nUsing a bit of intuition one would assume that:\n- Males with the title 'Master' are children\n- Females with the title 'Miss' would be younger than the title 'Mrs'\n- Those in first class were more likely to be older than those in second and third\n\nThe first step will be extracting and simplifying the titles of the passengers."},{"metadata":{"_cell_guid":"b44c1446-ce5a-4890-8b01-66b3c806fc85","_uuid":"468c5e7b6e687f059eea3fd7639accb653443e03","collapsed":true,"trusted":false},"cell_type":"code","source":"#Title list was extracted using the substrings in string method and combined into a unique list.\ntitle_list=['Mrs', 'Mr', 'Master', 'Miss', 'Major', 'Rev',\n                'Dr', 'Ms', 'Mlle','Col', 'Capt', 'Mme', 'Countess',\n                'Don', 'Jonkheer']\n\ndef substrings_in_string(string, substrings):\n\n    for substring in substrings:\n        if str(string).find(substring) != -1:\n            return substring\n    \n    return np.nan\n\n\n#replacing all titles with mr, mrs, miss, master\ndef replace_titles(x):\n    title=x['Title']\n    if title in ['Don', 'Major', 'Capt', 'Jonkheer', 'Rev', 'Col']:\n        return 'Mr'\n    elif title in ['Countess', 'Mme']:\n        return 'Mrs'\n    elif title in ['Mlle', 'Ms']:\n        return 'Miss'\n    elif title =='Dr':\n        if x['Sex']=='Male':\n            return 'Mr'\n        else:\n            return 'Mrs'\n    else:\n        return title\n    \n    \ndf_full['Title']=df_full['Name'].map(lambda x: substrings_in_string(x, title_list))\ndf_full['Title']=df_full.apply(replace_titles, axis=1)\n\ndf['Title']=df['Name'].map(lambda x: substrings_in_string(x, title_list))\ndf['Title']=df.apply(replace_titles, axis=1)\n\ndf_test['Title']=df_test['Name'].map(lambda x: substrings_in_string(x, title_list))\ndf_test['Title']=df_test.apply(replace_titles, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4ce101cc-71a4-44b3-880a-5541323f7293","_uuid":"d494c9286e1e88c3b65a12feaba24dde6605080e"},"cell_type":"markdown","source":"With the titles of passengers aggregated into a simple list, we can now inspect the spread of age across these titles for each class to confirm our intuition."},{"metadata":{"_cell_guid":"8cbb457a-d345-4f1b-9aba-2e5a486902a1","_uuid":"385c3147b1961b29edd7a0d6d57464ed651a1b77","trusted":false,"collapsed":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ntitles = ['Mrs','Mr','Miss','Master']\nplt.figure(figsize=(12,12))\nfor title in titles:\n    data = []\n    for Pclass in [1,2,3]:\n        data.append(df_full.loc[(df_full['Title'] == title) & \n                                (df_full['Pclass'] == Pclass) & \n                                ~np.isnan(df_full['Age'])]['Age'].values)\n    plt.subplot(2,2,titles.index(title)+1)\n    plt.boxplot(data)\n    plt.title(title)\n    plt.xlabel('Class')\n    plt.ylabel('Age')\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"be7ea119-3365-46bb-b462-5e897e13e7be","_uuid":"4510d28dcacf51da644302ad833e3e6586f86e88","trusted":false,"collapsed":true},"cell_type":"code","source":"df[['Title','Pclass','Age']].groupby(['Title','Pclass']).median()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3457d5f8-a4f2-45a7-a43b-9bf9c116be9a","_uuid":"42c8bbef176c8f23c3f8c8607293a61108ea85fe"},"cell_type":"markdown","source":"From the above summary and plots we can see there is indeed an impact of class on age, as well as title.\n\nIn the final model, passengers were categorised as 'Male, Female, Child' , so rather than perform a regression it should be sufficient for now to just take the median age for each title / class as shown in the table above. \n\nThe only problem that may arise from this method is labelling those with the title 'Miss' as adults when they are children. \n\n\n\n"},{"metadata":{"_cell_guid":"6e87ba56-10b4-4341-b046-6c0addd26a99","_uuid":"f767fceb0a8251709e9523f4eed4dcd47a73d4eb","trusted":false,"collapsed":true},"cell_type":"code","source":"#Add a boolean feature to flag whether this passenger's age was originally missing. \ndef age_missing(x):\n    if (np.isnan(x['Age'])):\n        return 1\n    else:\n        return 0\n\ndf['Age_Missing'] = df.apply(age_missing,axis=1)\ndf_test['Age_Missing'] = df_test.apply(age_missing,axis=1)\n\n#Age imputation\ndef age_imputation(x, age_lookup):\n    age = x['Age']\n    pclass = x['Pclass']\n    title = x['Title']\n\n    if np.isnan(age):\n        lookup_age = age_lookup['Age'][age_lookup['Title'] == title][age_lookup['Pclass'] == pclass].values\n        \n        return lookup_age[0].astype(int)\n    else:\n        return age\n    \nage_lookup = df_full[['Pclass','Title','Age']].groupby(['Pclass','Title'], as_index=False).median().sort_values(by='Age', ascending=False)\ndf['Age'] = df.apply(lambda x: age_imputation(x,age_lookup), axis=1)\ndf_test['Age'] = df_test.apply(lambda x: age_imputation(x,age_lookup), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"18c37faa-ddf1-4c58-8cb8-a620f764b73f","_uuid":"f7b233cce9a6f5a49579dc0ce4ff5f87894d9655"},"cell_type":"markdown","source":"**3. Intuitions and high level feature analysis**\n\nWith age data imputed in we can look at some of our assumptions about who is more likely to survive or perish.\n\n* We expect those in higher classes are more likely to survive\n* Females are more likely to survive than males\n* Children are more likely to survive than adults\n\n* Port of embarkation shouldn't have a significant impact on survival (unless different ports had different socio-economic levels in which case it could just be another way of phrasing class)\n* Title shouldn't make a significant difference to survival rates (especially with our simplified set of titles)\n* Fare paid should also have a minimal impact (as this would most likely just map back to class)\n\nThe last significant factors to examine are the Sibsp and Parch counts (i.e. group size), and our new feature indicating whether the age was missing from the passenger record.\n\nThese can all be combined on a pearson correlation plot to see how they impact survival and how we might simplify things further by combining features. "},{"metadata":{"_cell_guid":"9b07654b-06e3-4af8-95c7-bf4fd7e2229e","_uuid":"f026141cba7fea6e13f3dc41d07b401554fa6ab9","trusted":false,"collapsed":true},"cell_type":"code","source":"import seaborn as sns\ndf_interesting = df.loc[:,['Age','SibSp','Parch','Sex','Pclass','Age_Missing','Survived']]\ndf_interesting['Sex'] = df_interesting['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n\n\n\nplt.figure(figsize=(10,10))\nsns.heatmap(df_interesting.astype(float).corr(),linewidths=0.1,vmax=1.0,square=True, linecolor='white', annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3158919d-2970-4e1c-9d3a-ccc03003e25b","_uuid":"4a7004f9ef14555984a5f1ae591286748e60b592"},"cell_type":"markdown","source":"As expected, Pclass and Sex have a strong correlation to survival. \n\n\nParch appears to have a stronger correlation than SibSp, though both are still somewhat weak compared to Sex and Pclass. It would be worth simplifying these into a single group size feature (more on that in the next section). \n\nAge *appears* to have a weaker than expected correlation to survival, (in fact it is more strongly correlated to other features than it is to survival.)\n\nWe can investigate this surprising Age correlation a little further by looking at the age distribution among those who survived and did not. \n\n"},{"metadata":{"_cell_guid":"4f37a8b4-7af4-46d2-a221-a4e5c3ded50c","_uuid":"7b4b731d160fcd86a83d4860218689173394f8d5","trusted":false,"collapsed":true},"cell_type":"code","source":"plt.figure(figsize=(20,12))\nplt.title('Age distribution among survived and perished')\nsns.distplot(df.loc[df['Survived'] == 1]['Age'].values,label = 'Survived')\nsns.distplot(df.loc[df['Survived'] == 0]['Age'].values,label = 'Perished')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"afd5c317-20e2-4e85-961d-70770b0ece2e","_uuid":"6e6b189516d6b98caaf544035b68e39e980ca596"},"cell_type":"markdown","source":"From the above distribution plot we can assume that the survival rate is much higher for children, but otherwise age does not really factor in (ignoring the peak around 25-30 which I suspect would be the males in 2nd and 3rd class as they made up a significant proportion of the ship and had a much lower rate of survival.) \n\nWhat we really want is a feature which identifies children (say under the age of 14), while we're there we might was well make this feature identify men and women too. \n\nSo what we can do now is simplifiy our model and introduce a new categorical feature representing 'Person Class' (i.e. male/female/child in 1st, 2nd, or 3rd class). \n\n*NB: this feature initially did not have Pclass included (i.e. Pclass stayed as its own feature) but this lead to a significantly lower public score of 0.77990*"},{"metadata":{"_cell_guid":"c8bfceda-172b-4d70-b770-7c9f1b95a284","_uuid":"e32da91a25370cc6c6b4239e1f46085891b4d1bb","trusted":false,"collapsed":true},"cell_type":"code","source":"#Apply person class to encompass gender, whether the passenger is a child, and class\ndef person_class(x):\n    sex = x['Sex']\n    age = x['Age']\n    pclass = str(x['Pclass'])\n    if (age <= 14):\n        return 'child'+'_'+pclass\n    else:\n        return sex+'_'+pclass\n\ndf['Person_Class'] = df.apply(person_class,axis=1)\n\ndf_test['Person_Class'] = df_test.apply(person_class,axis=1)\n\n#Rebuild pearson correlation plot\nfrom sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\ndf_interesting = df.loc[:,['SibSp','Parch','Person_Class','Age_Missing','Survived']]\ndf_interesting['Person_Class'] = encoder.fit_transform(df_interesting['Person_Class'])\nplt.figure(figsize=(10,10))\nsns.heatmap(df_interesting.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, linecolor='white', annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bf44234c-a1da-4d57-89f1-49c26c45089c","_uuid":"6faf4c3cf40bdb2279c938eeaa3310e8b5c9cf6f"},"cell_type":"markdown","source":"**4. Calculating group size, family or friends?**\n\nAs mentioned previously, the SibSp and Parch factors weren't amazingly helpful on their own, but could perhaps be combined into a 'group size' feature. \nHowever, it is entirely possible that there are people on board travelling together who would not show up with anything in those columns (i.e. friends, extended family).\n\nOne way to modify the group size variable to include these cases is to count the number of people on each unique ticket number, then take the maximum of this and SibSp + Parch. \n\n\n"},{"metadata":{"_cell_guid":"912faa6c-73dd-4907-8cd2-33ee8ca8b920","_uuid":"dbadd8a77a24221c3e138438822ff0a4585cfd7a","trusted":false,"collapsed":true},"cell_type":"code","source":"#Adding group size feature\ndef modify_group_size(x,ticket_numbers):\n    ticket = x['Ticket']\n    num_on_ticket = ticket_numbers['count'][ticket_numbers['Ticket'] == ticket].values[0]\n    group_size = x['Group_Size']\n    return max(num_on_ticket,group_size)\n\nticket_numbers = df_full.groupby(['Ticket']).size().reset_index(name='count')\n\ndf['Group_Size'] = df['Parch'] + df['SibSp'] + 1\ndf['Group_Size'] = df.apply(lambda x: modify_group_size(x,ticket_numbers), axis=1)\n\ndf_test['Group_Size'] = df_test['Parch'] + df['SibSp'] + 1\ndf_test['Group_Size'] = df_test.apply(lambda x: modify_group_size(x,ticket_numbers), axis=1)\n\n#Look at the survival rate for each group size\ndf[['Group_Size','Survived']].groupby(['Group_Size']).mean().reset_index()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ed679708-44f8-4492-947a-f9e598c1d926","_uuid":"eee5a2d5eadb26c0498baf485be138f3e483afc4"},"cell_type":"markdown","source":"We can probably make this group size feature a bit simpler by grouping into 3  categories: \n* Solo\n* Small group (2-4)\n* Large group (5+)\n"},{"metadata":{"_cell_guid":"954ddacc-4b35-4faf-87cc-e32f58dff645","_uuid":"8ab405fbafcdaf8347d396bc1e2a0fc03711c0bc","trusted":false,"collapsed":true},"cell_type":"code","source":"#Categorizing group size for simplicity\ndef categorize_group(x):\n    size = x['Group_Size']\n    if(size== 1):\n        return 'solo'\n    \n    elif (size >= 2 and size <= 4):\n        return 'small_group'\n    else:\n        return 'large_group'\n\n   \ndf['Group_Category'] = df.apply(categorize_group,axis=1)\n\ndf_test['Group_Category'] = df_test.apply(categorize_group,axis=1)\n\n\n#Rebuild pearson correlation plot\ndf_interesting = df.loc[:,['Group_Category','Person_Class','Age_Missing','Survived']]\ndf_interesting['Person_Class'] = encoder.fit_transform(df_interesting['Person_Class'])\ndf_interesting['Group_Category'] = encoder.fit_transform(df_interesting['Group_Category'])\nplt.figure(figsize=(10,10))\nsns.heatmap(df_interesting.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, linecolor='white', annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e812fc99-4ee7-4f06-9858-f0dd9edf1442","_uuid":"ac519bf116d5c6df4602b5a352cde606ae39f850"},"cell_type":"markdown","source":"**5. Building our features and fitting a model**\n\nBefore fitting our model and analysing the accuracy, we can take a look at survival rates within combinations of our new (entirely categorical) features. \n"},{"metadata":{"_cell_guid":"4b636124-aad8-46e0-871c-c78d5356db14","_uuid":"0f0e9441321566e6780853f9c4a7c241f14df8c0","trusted":false,"collapsed":true},"cell_type":"code","source":"df[['Person_Class','Group_Category','Survived']].groupby(['Person_Class','Group_Category']).mean()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f835ac12-ec27-4cd8-9ae4-025d8c3cc24e","_uuid":"b7f53bed861ce409518de40d67b4968228266270"},"cell_type":"markdown","source":"There's a couple of  interesting things to note here that may impact how accurate we can get our model:\n\n* Females in 3rd class fare a lot worse than females in 1st or 2nd class\n* Males in 1st class fare much better than males in 2nd or 3rd class (with the exception of large groups but I would take this with a grain of salt given how few large groups there are)\n\n\nSeveral classifiers were trialled, with features tuned using grid search and k-fold cross validation, resulting in a random forest with 20 decision trees. (Scores were very slightly lower for a single decision tree classifier, and overall there was not a significant difference in average accuracy - I assume this is due to the relatively small number of entirely categorical features).\n"},{"metadata":{"_cell_guid":"214d3366-3d04-42d3-b507-4938cf39bfde","_uuid":"a943621bc5c6683921be1dec2607f25d2edc60f7","trusted":false,"collapsed":true},"cell_type":"code","source":"#Building feature matrices from relevant dataframe columns\nX = df[['Group_Category','Person_Class','Age_Missing']].values\nX_test= df_test[['Group_Category','Person_Class','Age_Missing']].values\n\n#Label encode textual features\nfrom sklearn.preprocessing import OneHotEncoder\n\n\nX[:, 0] = encoder.fit_transform(X[:, 0])\nX[:, 1] = encoder.fit_transform(X[:, 1])\n\nX_test[:, 0] = encoder.fit_transform(X_test[:, 0])\nX_test[:, 1] = encoder.fit_transform(X_test[:, 1])\n\n\"\"\"One hot encode categorical features (this does not make a significant difference to average 10-fold cross validation score, but has been left in \nas it improved public leaderboard score.\n\"\"\"\n\nonehotencoder = OneHotEncoder(categorical_features = [0,1])\nX = onehotencoder.fit_transform(X).toarray()\nX_test = onehotencoder.fit_transform(X_test).toarray()\n\n#Set y variable for fitting\ny = df['Survived'].values\n\n# Fitting Random Forest to the Training set\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 20, \n                                    criterion = 'entropy', \n                                    max_features = 10,\n                                    min_samples_split = 2,\n                                    min_samples_leaf = 1,\n                                    max_depth = 6,\n                                    random_state = 2018)\nclassifier.fit(X, y)\n\n# Applying 10-Fold Cross Validation\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = classifier, X = X, y = y, cv = 10)\nmeanAccuracy = accuracies.mean()\nstdAccuracy = accuracies.std()\n\nprint(str(accuracies) + '\\nMean: ' + str(meanAccuracy) + '\\nStDev: ' + str(stdAccuracy))\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"81c2ff14-ccd0-4d2c-a457-2e1fc0a5a92d","_uuid":"c369b92ef88b333a61037651bc8a946b1c8005c7"},"cell_type":"markdown","source":"**6. Potential improvements**\n\nOverall this is not too bad an accuracy given the simplicity of the model, with a public score of 0.80382 (a touch outside the top 10% at time of writing). \n   \nHowever, there is one limitation of the current model that becomes quite obvious when plotting the actual vs predicted survival rates on the training set.\n    \n"},{"metadata":{"_cell_guid":"ef78de21-d0b3-4353-9841-f490cefcf883","_uuid":"faad434fb045b88ad72a130bd5e9b486c06f3d60","trusted":false,"collapsed":true},"cell_type":"code","source":"#Append the prediction of survival back on to the training set\ny_pred = classifier.predict(X)\ndf['Pred_survived'] = y_pred\n\nincorrect_predictions = df.loc[~(df['Survived'] == df['Pred_survived'])]\ncorrect_predictions = df.loc[(df['Survived'] == df['Pred_survived'])]\n\ngroup_counts = df[['Person_Class','Group_Category']].groupby(['Person_Class','Group_Category']).size().reset_index(name = 'Count')\nsurvived_by_group = df[['Person_Class','Group_Category','Survived','Pred_survived']].groupby(['Person_Class','Group_Category']).mean().reset_index()\ngroup_counts['Survived'] = survived_by_group['Survived']\ngroup_counts['Pred_survived'] = survived_by_group['Pred_survived']\ngroup_counts['Diff'] = group_counts['Pred_survived']-group_counts['Survived']\n\ngroup_counts\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8a6c7c41-3de5-4322-8f23-6746a841fe4f","_uuid":"7678f1a13f4175dd6a22502795a69480fbf90512"},"cell_type":"markdown","source":"Essentially the way the features and model have been set up means that within each unique combination, the model will either predict 0 or 100% survival rate, so our biggest rate of error as a percentage comes when one of our groups has a 50% rate of survival (essentially a coin toss when we only consider the features we have.) \n\nAny group with a true survival rate below 0.5 has a predicted survival rate of 0. Interestingly this completely binary result for the shown groups indicates that the \"Age_Missing\" feature is having no impact on our predictions.\n\nIf we assume the training set's true survival rate was the same in the test set, we can produce a ballpark estimate for our error rate when using all 418 test data records.\n"},{"metadata":{"_cell_guid":"48f606cb-141f-4ad1-9df5-3cd364e3811f","scrolled":false,"_uuid":"6e5a1da9362440c3cd0ffc9caeda424ee66216c1","trusted":false,"collapsed":true},"cell_type":"code","source":"group_counts_test = df_test[['Person_Class','Group_Category']].groupby(['Person_Class','Group_Category']).size().reset_index(name = 'Count')\n\ngroup_counts_test\n\ndef count_potential_errors(x,train_group_counts):\n    person_class = x['Person_Class']\n    group_category = x['Group_Category']\n    error_rate = train_group_counts['Diff'][train_group_counts['Person_Class'] == person_class][train_group_counts['Group_Category'] == group_category].values\n    if (len(error_rate) == 0):\n        return x['Count']\n    else:\n        return abs((x['Count']*error_rate[0]).astype(int))\n    \ngroup_counts_test['Est_Num_Errors'] = group_counts_test.apply(lambda x:count_potential_errors(x,group_counts),axis=1)\n\nprint('Estimated maximum accuracy (based on 418 entries in test set): ')\nprint(1 - group_counts_test['Est_Num_Errors'].sum()/418)\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0ad75704-1ecf-49be-a640-fe1cccd11713","_uuid":"dc2312b5efe275c7adb82f813e31536b677d439b"},"cell_type":"markdown","source":"This seems to be the logical conclusion for a simple one classifier model based on as few features as possible (in the end only really 2 features were required, but the creation of these features was the challenging part). \n\nThat being said there could be some things to try next if I revisit this competition after some more experience:\n* Stacking of models that behave in *different* ways (I did play around with some stacking / voting ensembles but the models behaved too similary to have an impact)\n* Introducing new features to further subdivide groups until the rate of survival for each unique combination is closer to 0 or 1 (not likely given the number of features we have / random noise / the risk of overfitting) \n* Splitting data and applying different models to different subsets"}],"metadata":{"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}