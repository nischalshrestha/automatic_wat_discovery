{"cells":[{"metadata":{"_uuid":"9e6719a6421df61ae40f061fe78575882c1031a7"},"cell_type":"markdown","source":"# Titanic Competition \n\nBy Atwine Mugume Twinamatsiko"},{"metadata":{"_uuid":"f05d4a38e8eaad6e96c03fb284c7e595c8cad95e"},"cell_type":"markdown","source":"### Competition Description:\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\nIn this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\n\nThis is a supervised problem because we already have the labeled data in which we want to classify the information\n\n### Evaluation is based on accuracy:\n\nThe evaluation of the hand in  result files is based on the accuracy of the predictions:\nfrom sklearn we can import the accuracy score and use it to test our  predictions.\n\nHand in format is:\nPassenger ID, Survived"},{"metadata":{"trusted":true,"_uuid":"30a222bee8643cdef41e96c7b5f24f0b519005a1"},"cell_type":"code","source":"#importing necessary libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2b51b8ece4604905024c60ccc7f7f980200cc902"},"cell_type":"code","source":"#let us import the data we have from the competition\ntest = pd.read_csv('../input/test.csv')\ntrain =  pd.read_csv('../input/train.csv')\ngendr = pd.read_csv('../input/gender_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2592b5471529bfad595e78d0aeb98c3b033ec149"},"cell_type":"markdown","source":"### Now that the data has been imported we look at initial exploration\n\nLet's begin with the train dataset"},{"metadata":{"trusted":true,"_uuid":"904d6a555120d10f15247210495c925399719f56"},"cell_type":"code","source":"#what does our dataset look like\ntrain.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4c98d47a244ff8c55b389eabe7b44fb03ab79e67"},"cell_type":"code","source":"#let us see the kind of dat types we are workng with\ntrain.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5703708bcfedf31fdd7c0e4e284ad66aa3b5b317"},"cell_type":"markdown","source":"We have numeric and non numeric objects that we have to deal with since the machine learning algorithm only deals with numeric data"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"cfd1163681e6dd30b6489101c192b95f657cc3e8"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2c2c9c5a40fed476ea20aec370411e11440cee63"},"cell_type":"markdown","source":"### Since the train and test data are at times similar its easier to work on them at the same time"},{"metadata":{"trusted":true,"_uuid":"22ae1405c0d2d4bfdb0ce83401b6daeb85e29b21"},"cell_type":"code","source":"test.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bdc78b5aad73098f6f5f56543c818465bcd49a77"},"cell_type":"markdown","source":"As we can possibly see that the test info has only one column less than the training info and that is the survived column:"},{"metadata":{"trusted":true,"_uuid":"9b94b5a27e1cd02bb3063ed36955159920fb6f75"},"cell_type":"code","source":"#lets first drom the survived column in the train dataset and then merge the two of them\nsurvived = train['Survived']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"733c02718eea4397e88e50af78667fe94f916644"},"cell_type":"code","source":"train.drop(columns=('Survived'), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f1e2bc52f16d2722eebde6ce795b1fe1840f5a70"},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"84f2151994aa9a9741f8651faa4fffe257f986f1"},"cell_type":"code","source":"#first we create two differentiating columns in the two data frames\ntrain['Tag']='train'\ntest['Tag']= 'test'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"99d352d347b6f3d675c98ad5327b1b3b243fecab"},"cell_type":"code","source":"#we now concetenate the fromes\nframes = [train, test]\ndf = pd.concat(frames)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"918040bf531f08d8f187d1490abbb258163343d6"},"cell_type":"code","source":"#now we have the two data frames in one it is easier to apply the innitial exploration functions\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9181faa66d86ea26e599291d6ae44a2719517f8d"},"cell_type":"code","source":"#there are some values that may not be necessary while we are doing our analysis such as name\ndrop_columns = ['PassengerId','Name','Ticket']\ndf.drop(columns=drop_columns, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"6333440588c6876819c33754e9b561b44c4b581b"},"cell_type":"code","source":"#how many null values do we have\n#4 seem like a few\ndf.isnull().any().sum()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"3b24b9014bbf7ae92bd0c167d37069e1ee496c06"},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ca2b819ee560a347bbcc893b02318d34945fa441"},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5fe264f3a07cc6ded46f87893a07263a05402a05"},"cell_type":"code","source":"#now to separate the two data sets based on the tag columns\ntest_df = df[df['Tag']== 'test']\ntrain_df = df[df['Tag']== 'train']","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"3ceaac7d93ecd9833b8e1d12665c97bb7da95856"},"cell_type":"code","source":"train_df.drop(columns=('Tag'), inplace= True)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"dc6429785fb1c2a3f410254ae0598324179e2715"},"cell_type":"code","source":"test_df.drop(columns=('Tag'), inplace= True)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"281e4b856378b992e38d591b06c0344a0bdcf97d"},"cell_type":"code","source":"#these are the numeric data frames we have formed and are easy to look at\ntrain_df = pd.get_dummies(train_df)\ntest_df = pd.get_dummies(test_df)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"8b0452d83ddb9984f67c756f6f60c57a2e52d739"},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"94df91ebb89c15e85c2b706dfd395aab51445858"},"cell_type":"code","source":"#one hot encoding adds variables that we did not create so we have to align the data frames\ntrain_df,test_df = train_df.align(test_df, join= 'inner', axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"915453223f51aa1f27349a461c80544b15998803"},"cell_type":"code","source":"print('The remaining features', train_df.shape)\nprint('The remaining test features', test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0e08e10c6c4e7a63be7c09e685d1b01c32bbddfc"},"cell_type":"code","source":"#now lets return the survived to the training data set\ntrain_df['Survived'] = survived","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ac1fa178a9e49fa36c451bce65ecc2eee10a6a63"},"cell_type":"code","source":"train_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8828ebd74a359c810726bbf315a345c4eb83cdd2"},"cell_type":"code","source":"#replace the NaN with 0 so that i can use the mean values to impute\ntrain_df['Age']= train_df['Age'].replace(0, np.nan)\ntrain_df.fillna(train_df.mean(), inplace=True)\n\ntest_df['Age']= test_df['Age'].replace(0, np.nan)\ntest_df.fillna(test_df.mean(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"66415907590b50542b86c61d5e35ac1d1f98028f"},"cell_type":"markdown","source":"### Now that I have mostly numeric values lets explore"},{"metadata":{"trusted":true,"_uuid":"807996e0646060b458a949c9990939be1cbc88da"},"cell_type":"code","source":"#lets have a loot at the survived variable\ntrain_df['Survived'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5abb763e3f9ce5be462e118c03b108deee1ccde4"},"cell_type":"code","source":"#lets display it\ntrain_df['Survived'].astype(int).hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"881c8371139ff8acc0cf30565363d0b6feec3a7e"},"cell_type":"code","source":"#lets see if we have any missing values\n# we have none of those\ntrain_df.isnull().any().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"86407ee9c5e72c6fb914c22abf347eed8237b403"},"cell_type":"code","source":"#column types\ntrain_df.dtypes.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a98aa52b46fe35e1f387042324515235218b6cb9"},"cell_type":"code","source":"#what is the correlation of the variables to the target variable?","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af239898cdf5a1d2b0772825a08df335b0408456"},"cell_type":"code","source":"Corr = train_df.corr()['Survived'].sort_values()\n\n#Print them\nprint('Most positive correlations', Corr.tail(10))\nprint('-'*20)\nprint('Most negative correlations', Corr.head(10))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"01603663e8b84965215da66d727e0caf0f4d1a0b"},"cell_type":"markdown","source":"### Manual feature Engineering:"},{"metadata":{"_uuid":"b6c93967429caf6941809528c2d4ea8b605e001c"},"cell_type":"markdown","source":"### Train features"},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"e64b8272ec8174b61046c7ec17dc6e2741f5a6aa"},"cell_type":"code","source":"#we need to group by the client id therefore we need to add it back inthe df\ntrain_df['ID'] = train['PassengerId']\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3b3058fe6fe9be5fb2b018f26f826c6025d5cb5e"},"cell_type":"code","source":"#in order to create features we remove survived first\ntrain_df.drop(columns= ('Survived'), inplace= True)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"67debf3c6a071c5ccb05dc4d0c790486257ed510"},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3fbe3b144a09d7a02b95d1a8efa75821b8ef2928"},"cell_type":"code","source":"#we are going to make new features by the aggregation functions those arefunctions like mean and others\ntrain_agg = train_df.groupby('ID',as_index= False).agg(['count', 'mean', 'max', 'min', 'sum']).reset_index()\ntrain_agg.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5e7078b350be89ba6e4ceceb22c1fdb863386f85"},"cell_type":"markdown","source":"We need to create new names for each of these columns. The following code makes new names by appending the stat to the name. Here we have to deal with the fact that the dataframe has a multi-level index. I find these confusing and hard to work with, so I try to reduce to a single level index as quickly as possible."},{"metadata":{"trusted":true,"_uuid":"1aef7f641260e1a96a22ec367ee498689565fd5e"},"cell_type":"code","source":"# List of column names\ncolumns = ['ID']\n\n# Iterate through the variables names\nfor var in train_agg.columns.levels[0]:\n    # Skip the id name\n    if var != 'ID':\n        \n        # Iterate through the stat names\n        for stat in train_agg.columns.levels[1][:-1]:\n            # Make a new column name for the variable and stat\n            columns.append('train_%s_%s' % (var, stat))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c710c17132449a533aaf89b536435aad01348b9f"},"cell_type":"code","source":"train_agg.columns = columns\ntrain_agg.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c4eb675b3899e81c8b11b36885bba4d470170bdc"},"cell_type":"code","source":"#we have now to merge the train_df with the train agg\ntrain_df = train_df.merge(train_agg, on = 'ID', how = 'left')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"288843be903a952584cfbc08217c6eba37f7711d"},"cell_type":"code","source":"# List of new correlations\nnew_corrs = []\n\n# Iterate through the columns \nfor col in columns:\n    # Calculate correlation with the target\n    corr = train_df['ID'].corr(train_df[col])\n    \n    # Append the list as a tuple\n\n    new_corrs.append((col, corr))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1d4c08cae6dd40e4dfe1b6e12ba21d95064e06fd"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4f8af939e363face4bde7e4178686d9f91420186"},"cell_type":"markdown","source":"We have created many more columns but they are not all useful. We have to test the correlation if they need to be used in this instance"},{"metadata":{"trusted":true,"_uuid":"dfdebe9fcbca1cf14bee2f818b63e6e47930c83e"},"cell_type":"code","source":"# Sort the correlations by the absolute value\n# Make sure to reverse to put the largest values at the front of list\nnew_corrs = sorted(new_corrs, key = lambda x: abs(x[1]), reverse = True)\nnew_corrs[:15]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7dac14e2e874c0fefbeeac5e4d1306674ed0563d"},"cell_type":"markdown","source":"### Test Features"},{"metadata":{"trusted":true,"_uuid":"79b15914b00eb4a7c4e4d155bd1774d273be1ec0"},"cell_type":"code","source":"#we now do the same thing for the test data\n#we need to group by the client id\ntest_df['ID'] = train['PassengerId']","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"5678beec108e69359486c0092cafdfeea566b1c4"},"cell_type":"code","source":"#we are going to make new features by the aggregation functions those arefunctions like mean and others\ntest_agg = test_df.groupby('ID',as_index= False).agg(['count', 'mean', 'max', 'min', 'sum']).reset_index()\ntest_agg.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e75e06db9fc9c4ff4d43268c827bfb7b3194e8b"},"cell_type":"code","source":"# List of column names\ncolumns2 = ['ID']\n\n# Iterate through the variables names\nfor var2 in test_agg.columns.levels[0]:\n    # Skip the id name\n    if var2 != 'ID':\n        \n        # Iterate through the stat names\n        for stat in test_agg.columns.levels[1][:-1]:\n            # Make a new column name for the variable and stat\n            columns2.append('test_%s_%s' % (var2, stat))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"82783227d244b2e324fab7bdaccb035d08599f38"},"cell_type":"code","source":"test_agg.columns = columns2\ntest_agg.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d54af08a48002efc706ffee8357d59f3334f060"},"cell_type":"code","source":"#we have now to merge the train_df with the train agg\ntest_df = test_df.merge(test_agg, on = 'ID', how = 'left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"739bf48d73fed77db532a6efd2651093ed21efa9"},"cell_type":"code","source":"train_df.shape , test_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fae507d30ec4a9399deab056bde77071478d66ea"},"cell_type":"code","source":"#because the data was made into two data frames now we combine them back to get the full number of peple on the boat\nadd = [train_df,test_df]\ntest_fin = pd.concat(add)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"56c1666e9790b7c4dbc091e7bfee48f03094246f"},"cell_type":"code","source":"#returning the survived column to train\ntrain_df['Survived'] = survived","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"1d42487c411456ed06a33fdfd724a57a8e0d3354"},"cell_type":"code","source":"train_df.shape , test_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9787dfad0a63f993200ceb5184f2b22236da68c6"},"cell_type":"code","source":"# train_df.to_csv('train_df.csv')\n# test_df.to_csv('test_df.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1a49f936feac87e1c5343fc09f0fb496f9218e8b"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e74294d7a8aaff57f3542c897cb5bfb022220981"},"cell_type":"markdown","source":"## Modeling"},{"metadata":{"trusted":true,"_uuid":"365055a67607430ac0698494f295fac412749752"},"cell_type":"code","source":"import lightgbm as lgb\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import LabelEncoder\n\nimport gc\n\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"542cf7335d9ee75fbe785f7b892588028e23e8ac"},"cell_type":"code","source":"def model(features, test_features, encoding = 'ohe', n_folds = 5):\n    \n    \"\"\"Train and test a light gradient boosting model using\n    cross validation. \n    \n    Parameters\n    --------\n        features (pd.DataFrame): \n            dataframe of training features to use \n            for training a model. Must include the TARGET column.\n        test_features (pd.DataFrame): \n            dataframe of testing features to use\n            for making predictions with the model. \n        encoding (str, default = 'ohe'): \n            method for encoding categorical variables. Either 'ohe' for one-hot encoding or 'le' for integer label encoding\n            n_folds (int, default = 5): number of folds to use for cross validation\n        \n    Return\n    --------\n        submission (pd.DataFrame): \n            dataframe with `SK_ID_CURR` and `TARGET` probabilities\n            predicted by the model.\n        feature_importances (pd.DataFrame): \n            dataframe with the feature importances from the model.\n        valid_metrics (pd.DataFrame): \n            dataframe with training and validation metrics (ROC AUC) for each fold and overall.\n        \n    \"\"\"\n    \n    # Extract the ids\n    train_ids = features['ID']\n    test_ids = test_features['ID']\n    \n    # Extract the labels for training\n    labels = features['Survived']\n    \n    # Remove the ids and target\n    features = features.drop(columns = ['ID', 'Survived'])\n    test_features = test_features.drop(columns = ['ID'])\n    \n    \n    # One Hot Encoding\n    if encoding == 'ohe':\n        features = pd.get_dummies(features)\n        test_features = pd.get_dummies(test_features)\n        \n        # Align the dataframes by the columns\n        features, test_features = features.align(test_features, join = 'inner', axis = 1)\n        \n        # No categorical indices to record\n        cat_indices = 'auto'\n    \n    # Integer label encoding\n    elif encoding == 'le':\n        \n        # Create a label encoder\n        label_encoder = LabelEncoder()\n        \n        # List for storing categorical indices\n        cat_indices = []\n        \n        # Iterate through each column\n        for i, col in enumerate(features):\n            if features[col].dtype == 'object':\n                # Map the categorical features to integers\n                features[col] = label_encoder.fit_transform(np.array(features[col].astype(str)).reshape((-1,)))\n                test_features[col] = label_encoder.transform(np.array(test_features[col].astype(str)).reshape((-1,)))\n\n                # Record the categorical indices\n                cat_indices.append(i)\n    \n    # Catch error if label encoding scheme is not valid\n    else:\n        raise ValueError(\"Encoding must be either 'ohe' or 'le'\")\n        \n    print('Training Data Shape: ', features.shape)\n    print('Testing Data Shape: ', test_features.shape)\n    \n    # Extract feature names\n    feature_names = list(features.columns)\n    \n    # Convert to np arrays\n    features = np.array(features)\n    test_features = np.array(test_features)\n    \n    # Create the kfold object\n    k_fold = KFold(n_splits = n_folds, shuffle = False, random_state = 50)\n    \n    # Empty array for feature importances\n    feature_importance_values = np.zeros(len(feature_names))\n    \n    # Empty array for test predictions\n    test_predictions = np.zeros(test_features.shape[0])\n    \n    # Empty array for out of fold validation predictions\n    out_of_fold = np.zeros(features.shape[0])\n    \n    # Lists for recording validation and training scores\n    valid_scores = []\n    train_scores = []\n    \n    # Iterate through each fold\n    for train_indices, valid_indices in k_fold.split(features):\n        \n        # Training data for the fold\n        train_features, train_labels = features[train_indices], labels[train_indices]\n        # Validation data for the fold\n        valid_features, valid_labels = features[valid_indices], labels[valid_indices]\n        \n        # Create the model\n        model = lgb.LGBMClassifier(n_estimators=10000, objective = 'binary', \n                                   class_weight = 'balanced', learning_rate = 0.05, \n                                   reg_alpha = 0.1, reg_lambda = 0.1, \n                                   subsample = 0.8, n_jobs = -1, random_state = 50)\n        \n        # Train the model\n        model.fit(train_features, train_labels, eval_metric = 'auc',\n                  eval_set = [(valid_features, valid_labels), (train_features, train_labels)],\n                  eval_names = ['valid', 'train'], categorical_feature = cat_indices,\n                  early_stopping_rounds = 100, verbose = 200)\n        \n        # Record the best iteration\n        best_iteration = model.best_iteration_\n        \n        # Record the feature importances\n        feature_importance_values += model.feature_importances_ / k_fold.n_splits\n        \n        # Make predictions\n        test_predictions += model.predict_proba(test_features, num_iteration = best_iteration)[:, 1] / k_fold.n_splits\n        \n        # Record the out of fold predictions\n        out_of_fold[valid_indices] = model.predict_proba(valid_features, num_iteration = best_iteration)[:, 1]\n        \n        # Record the best score\n        valid_score = model.best_score_['valid']['auc']\n        train_score = model.best_score_['train']['auc']\n        \n        valid_scores.append(valid_score)\n        train_scores.append(train_score)\n        \n        # Clean up memory\n        gc.enable()\n        del model, train_features, valid_features\n        gc.collect()\n        \n    # Make the submission dataframe\n    submission = pd.DataFrame({'ID': test_ids, 'Survived': test_predictions})\n    \n    # Make the feature importance dataframe\n    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importance_values})\n    \n    # Overall validation score\n    valid_auc = roc_auc_score(labels, out_of_fold)\n    \n    # Add the overall scores to the metrics\n    valid_scores.append(valid_auc)\n    train_scores.append(np.mean(train_scores))\n    \n    # Needed for creating dataframe of validation scores\n    fold_names = list(range(n_folds))\n    fold_names.append('overall')\n    \n    # Dataframe of validation scores\n    metrics = pd.DataFrame({'fold': fold_names,\n                            'train': train_scores,\n                            'valid': valid_scores}) \n    \n    return submission, feature_importances, metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"68a587a569b3d09f44bae961f52fe55a8d88bf11"},"cell_type":"code","source":"def plot_feature_importances(df):\n    \"\"\"\n    Plot importances returned by a model. This can work with any measure of\n    feature importance provided that higher importance is better. \n    \n    Args:\n        df (dataframe): feature importances. Must have the features in a column\n        called `features` and the importances in a column called `importance\n        \n    Returns:\n        shows a plot of the 15 most importance features\n        \n        df (dataframe): feature importances sorted by importance (highest to lowest) \n        with a column for normalized importance\n        \"\"\"\n    \n    # Sort features according to importance\n    df = df.sort_values('importance', ascending = False).reset_index()\n    \n    # Normalize the feature importances to add up to one\n    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n\n    # Make a horizontal bar chart of feature importances\n    plt.figure(figsize = (10, 6))\n    ax = plt.subplot()\n    \n    # Need to reverse the index to plot most important on top\n    ax.barh(list(reversed(list(df.index[:15]))), \n            df['importance_normalized'].head(15), \n            align = 'center', edgecolor = 'k')\n    \n    # Set the yticks and labels\n    ax.set_yticks(list(reversed(list(df.index[:15]))))\n    ax.set_yticklabels(df['feature'].head(15))\n    \n    # Plot labeling\n    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n    plt.show()\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ebe641b73a32d70b12e85438c3bf284399f4775f"},"cell_type":"markdown","source":"### Let us now employ the functions above to run the model"},{"metadata":{"trusted":true,"_uuid":"381f9ee8a08cd447e974039007c2e8263de259e4"},"cell_type":"code","source":"submission, correlations, metrics = model(train_df,test_fin)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"07a262a2276d1f5b62c78f6927534c0a41918083"},"cell_type":"code","source":"metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ad227269ad47e900e10945614b3aa46be5ac63fc"},"cell_type":"code","source":"#lets plot the feature importances\ncorr = plot_feature_importances(correlations)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ccb218b4de88a5fb3a145d4ea50beb4fa47974fe"},"cell_type":"code","source":"submission['Survived'] = submission['Survived'].round(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"7c7228ec43234fce0a513e7711edebff2d9acd19"},"cell_type":"code","source":"# submission.to_csv('Titanic_Submission2.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"937a73812aaa99e694ad1e4dc59c31c63772e250"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}