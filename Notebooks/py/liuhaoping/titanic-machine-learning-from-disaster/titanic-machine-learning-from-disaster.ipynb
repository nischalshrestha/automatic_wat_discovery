{"cells":[{"metadata":{"trusted":true,"_uuid":"4cd5978e36a374af5acb2269671435114643397e"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Going to use these 5 base models for the stacking\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n                              GradientBoostingClassifier, ExtraTreesClassifier)\nfrom sklearn.svm import SVC\nfrom sklearn.cross_validation import KFold\n\nimport os\nprint(os.listdir(\"../input\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"scrolled":true},"cell_type":"code","source":"train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\n# Store our passenger ID for easy access\nPassengerId = test['PassengerId']\n\ndisplay(train.head(2))\ndisplay(test.head(2))\nprint(train.shape)\nprint(test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"4222c4cdb069ea8d8982f9b8b26d9b3e9122685d"},"cell_type":"code","source":"train.info()\nprint(\"*\"*50)\ntest.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"75c3298d3d6e05d6d0173fe3cbe273bfe1af07c0"},"cell_type":"markdown","source":"## What is the distribution of numerical feature values across the samples?\n\n\n- Total samples are 891 or 40% of the actual number of passengers on board the Titanic (2,224).\n- Survived is a categorical feature with 0 or 1 values.\n- Around 38% samples survived representative of the actual survival rate at 32%.\n- Most passengers (> 75%) did not travel with parents or children.\n- Nearly 30% of the passengers had siblings and/or spouse aboard.\n- Fares varied significantly with few passengers (<1%) paying as high as $512.\n- Few elderly passengers (<1%) within age range 65-80."},{"metadata":{"trusted":true,"_uuid":"04114c35f1483261b2e3e03aaa7f11d9fdc89d59"},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"176deff3f47259b42838e2f4a8e61903f2b98678"},"cell_type":"markdown","source":"## What is the distribution of categorical features?\n\n- Names are unique across the dataset (count=unique=891)\n- Sex variable as two possible values with 65% male (top=male, freq=577/count=891).\n- Cabin values have several dupicates across samples. Alternatively several passengers shared a cabin.\n- Embarked takes three possible values. S port used by most passengers (top=S)\n* - Ticket feature has high ratio (22%) of duplicate values (unique=681)."},{"metadata":{"trusted":true,"_uuid":"1cabfb2943598787e81f6937785222d1088da2b0"},"cell_type":"code","source":"train.describe(include=['O'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6f266006d80867522667e2dc36546555e537c000"},"cell_type":"markdown","source":"## Feature Enginerring\n\nTitanic Best Working Classfier : by [Sina](https://www.kaggle.com/sinakhorami/titanic-best-working-classifier)"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ac8190f9ca5fd4a686a44bc75293a3de177aec03"},"cell_type":"code","source":"full_data = [train, test]\n\n# Some features of my own that I have added in\n# Gives the length of the name\ntrain['Name_length'] = train['Name'].apply(len)\ntest['Name_length'] = test['Name'].apply(len)\n# Feature that tells whether a passenger had a cabin on the Titanic\ntrain['Has_Cabin'] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\ntest['Has_Cabin'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n\n# Feature engineering steps taken from Sina\n# Create new feature FamilySize as a combination of SibSp and Parch\nfor dataset in full_data:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n# Create new feature IsAlone from FamilySize\nfor dataset in full_data:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n# Remove all NULLS in the Embarked column\nfor dataset in full_data:\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n# Remove all NULLS in the Fare column and create a new feature CategoricalFare\nfor dataset in full_data:\n    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\ntrain['CategoricalFare'] = pd.qcut(train['Fare'], 4)\n# Create a New feature CategoricalAge\nfor dataset in full_data:\n    age_avg = dataset['Age'].mean()\n    age_std = dataset['Age'].std()\n    age_null_count = dataset['Age'].isnull().sum()\n    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n    dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\n    dataset['Age'] = dataset['Age'].astype(int)\ntrain['CategoricalAge'] = pd.cut(train['Age'], 5)\n# Define function to extract titles from passenger names\ndef get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    # If the title exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    return \"\"\n# Create a new feature Title, containing the titles of passenger names\nfor dataset in full_data:\n    dataset['Title'] = dataset['Name'].apply(get_title)\n# Group all non-common titles into one single grouping \"Rare\"\nfor dataset in full_data:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n\nfor dataset in full_data:\n    # Mapping Sex\n    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n    \n    # Mapping titles\n    title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n    \n    # Mapping Embarked\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n    \n    # Mapping Fare\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] \t\t\t\t\t\t        = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] \t\t\t\t\t\t\t        = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n    \n    # Mapping Age\n    dataset.loc[ dataset['Age'] <= 16, 'Age'] \t\t\t\t\t       = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4 ;","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"80432a87ebc978bc48379e93178dc719bb00c972"},"cell_type":"code","source":"# Feature selection\ndrop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']\ntrain = train.drop(drop_elements, axis = 1)\ntrain = train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\ntest  = test.drop(drop_elements, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9e19ba5793cef340c1c3f180490c6462516da762"},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3ebde98810b06cd82ff1e3ff8d4e6557f0fd9c57"},"cell_type":"markdown","source":"## Pearson Correlation Heatmap"},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"ccb1d3a1a8c21806eae338f619d7fc212a1f43d1"},"cell_type":"code","source":"colormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"580509eb937890a49a93e523f0f7174f77504ca5","scrolled":true,"collapsed":true},"cell_type":"code","source":"# Create Numpy arrays of train, test and target ( Survived) dataframes to feed into our models\n# Y_train = train['Survived'].ravel()\n# train = train.drop(['Survived'], axis=1)\n\ntrain = train.values # Creates an array of the train data\ntest = test.values # Creats an array of the test data","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2375d37fbf1ffb28727f6e77d8fcd6683f71c5b2"},"cell_type":"markdown","source":"## Random split train data set into taining and validation"},{"metadata":{"trusted":true,"_uuid":"32c4a844deb374086dfb79ffcadabcbfbe25653c"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train,X_validate,Y_train,Y_validate=train_test_split(train[0::, 1::],train[0::, 0],random_state=7,train_size=0.7)\nprint(X_train.shape)\nprint(Y_train.shape)\nprint(X_validate.shape)\nprint(Y_validate.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6ec271859a5058a62a315d2ad2ad0288b8664406"},"cell_type":"markdown","source":"# Models"},{"metadata":{"_uuid":"b0629e8c15def9e7328ae3397d048d878f61c25f"},"cell_type":"markdown","source":"## Linear regression model (sklearn)"},{"metadata":{"trusted":true,"_uuid":"2016db60bfbadf752b9616e0c8b17f56c6ec5afd"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import mean_squared_error\n\nclf = LogisticRegression()\nclf.fit(X_train,Y_train)\npredictions=clf.predict(X_validate)\nprint(mean_squared_error(Y_validate,predictions))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df1e6fd684ae29182b37e628697c49f43edbfa38"},"cell_type":"markdown","source":"### Cross Validation\n    - http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html\n    - http://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"47fd80c5d59ba790a75bd57684437998334b0831"},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\nfrom sklearn import linear_model\nlg = linear_model.LogisticRegression()\nprint(cross_val_score(lg, X_train, Y_train, cv= 5))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"50142c8a163e05d23baa08e3f9e9488e2a321651"},"cell_type":"markdown","source":"## Tree-Based model (xgboost)\n - refer to https://www.kaggle.com/simulacra/titanic-with-xgboost"},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"1d474ddf8a10a3f92b5878c4ae6207b3b123c086"},"cell_type":"code","source":"from xgboost import XGBClassifier\n\nxgb = XGBClassifier()\nxgb.fit(X_train, Y_train)\nxgb.score(X_validate, Y_validate)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"7e19550ea24c1c479f9bf63ddd39b019dd3b22ae"},"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Create the parameter grid: gbm_param_grid \ngbm_param_grid = {\n    'n_estimators': range(8, 20),\n    'max_depth': range(6, 10),\n    'learning_rate': [.4, .45, .5, .55, .6],\n    'colsample_bytree': [.6, .7, .8, .9, 1]\n}\n\n# Instantiate the regressor: gbm\ngbm = XGBClassifier(n_estimators=10)\n\n# Perform random search: grid_mse\nxgb_random = RandomizedSearchCV(param_distributions=gbm_param_grid, \n                                    estimator = gbm, scoring = \"accuracy\", \n                                    verbose = 1, n_iter = 50, cv = 4)\n\n\n# Fit randomized_mse to the data\nxgb_random.fit(X_train, Y_train)\n\n# Print the best parameters and lowest RMSE\nprint(\"Best parameters found: \", xgb_random.best_params_)\nprint(\"Best accuracy found: \", xgb_random.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6efef61a55636f28ffbbadc827e79c9cb0f16192"},"cell_type":"markdown","source":"### XG-boost Predition"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5db78646643328efe197a8552e64aba42be3be53"},"cell_type":"code","source":"xgb_pred = xgb_random.predict(test)\nsubmission = pd.concat([PassengerId, pd.DataFrame(xgb_pred)], axis = 'columns')\nsubmission.columns = [\"PassengerId\", \"Survived\"]\nsubmission.to_csv('prediction-xg.csv', header = True, index = False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"118a99a1137841c93c951cf216002e8049dafea7"},"cell_type":"markdown","source":"## Many Classifers \n    - SVC, DecisionTreeClassifier, ensemble, GaussianNB, LogisticRegression\n    - refer to https://www.kaggle.com/sinakhorami/titanic-best-working-classifier"},{"metadata":{"trusted":true,"_uuid":"1f9f2ebc3fd06dc1d8c51e70034ccd3220a89029"},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\n\nclassifiers = [\n    KNeighborsClassifier(3),\n    SVC(probability=True),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    AdaBoostClassifier(),\n    GradientBoostingClassifier(),\n    GaussianNB(),\n    LinearDiscriminantAnalysis(),\n    QuadraticDiscriminantAnalysis(),\n    LogisticRegression()]\n\nlog_cols = [\"Classifier\", \"Accuracy\"]\nlog  = pd.DataFrame(columns=log_cols)\n\nsss = StratifiedShuffleSplit(n_splits=10, test_size=0.1, random_state=0)\n\nX = train[0::, 1::]\ny = train[0::, 0]\n\nacc_dict = {}\n\nfor train_index, test_index in sss.split(X, y):\n\tx_train, x_test = X[train_index], X[test_index]\n\ty_train, y_test = y[train_index], y[test_index]\n\t\n\tfor clf in classifiers:\n\t\tname = clf.__class__.__name__\n\t\tclf.fit(x_train, y_train)\n\t\ttrain_predictions = clf.predict(x_test)\n\t\tacc = accuracy_score(y_test, train_predictions)\n\t\tif name in acc_dict:\n\t\t\tacc_dict[name] += acc\n\t\telse:\n\t\t\tacc_dict[name] = acc\n\nfor clf in acc_dict:\n\tacc_dict[clf] = acc_dict[clf] / 10.0\n\tlog_entry = pd.DataFrame([[clf, acc_dict[clf]]], columns=log_cols)\n\tlog = log.append(log_entry)\n\nplt.xlabel('Accuracy')\nplt.title('Classifier Accuracy')\n\nsns.set_color_codes(\"muted\")\nsns.barplot(x='Accuracy', y='Classifier', data=log, color=\"b\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e7509078a5077822f0f06ec48528eb976ddc7837"},"cell_type":"markdown","source":"### Predition"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ca3efde8bf7b6357444ce28fb0433fdad0e3e15d"},"cell_type":"code","source":"candidate_classifier = SVC()\ncandidate_classifier.fit(train[0::, 1::], train[0::, 0])\nresult = candidate_classifier.predict(test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ff435afe906f5d2450ea9e1e12940937a559c33b"},"cell_type":"code","source":"display(PassengerId.head(2))\ndf=pd.DataFrame(data=result,index=PassengerId,columns=['Survived'])\nprint(df.head(2))\ndf.to_csv('prediction-svc.csv',header=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aaf36e3968cd480fd42d7959a5b033da7f3bfca5"},"cell_type":"markdown","source":"## NN Model\n- refer to https://www.kaggle.com/liyenhsu/titanic-neural-network"},{"metadata":{"trusted":true,"_uuid":"091730dc600d48d588a3aebf1ec417fe34c0ef2c"},"cell_type":"code","source":"import keras \nfrom keras.models import Sequential # intitialize the ANN\nfrom keras.layers import Dense      # create layers\n\n# Initialising the NN\nmodel = Sequential()\n\n# layers\nmodel.add(Dense(units = 9, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11))\nmodel.add(Dense(units = 9, kernel_initializer = 'uniform', activation = 'relu'))\nmodel.add(Dense(units = 5, kernel_initializer = 'uniform', activation = 'relu'))\nmodel.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n\n# Compiling the ANN\nmodel.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n# Train the ANN\nmodel.fit(np.concatenate((X_train, X_validate), axis=0), np.concatenate((Y_train, Y_validate), axis=0), batch_size = 32, epochs = 200)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b60c72fca3322724e35ddca08b531e9545fa7223","collapsed":true},"cell_type":"code","source":"y_pred = model.predict(test)\ny_final = (y_pred > 0.5).astype(int).reshape(test.shape[0])\n\noutput = pd.DataFrame({'PassengerId': PassengerId, 'Survived': y_final})\noutput.to_csv('prediction-ann.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}