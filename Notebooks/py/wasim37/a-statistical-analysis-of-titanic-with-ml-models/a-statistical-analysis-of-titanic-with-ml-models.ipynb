{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n\n#load packages\nimport sys\nprint(\"Python version: {}\". format(sys.version))\n\nimport pandas as pd\nprint(\"pandas version: {}\". format(pd.__version__))\n\nimport matplotlib\nprint(\"matplotlib version: {}\". format(matplotlib.__version__))\n\nimport numpy as np\nprint(\"NumPy version: {}\". format(np.__version__))\n\nimport scipy as sp\nprint(\"SciPy version: {}\". format(sp.__version__)) \n\nimport IPython\nfrom IPython import display\nprint(\"IPython version: {}\". format(IPython.__version__)) \n\nimport sklearn\nprint(\"scikit-learn version: {}\". format(sklearn.__version__))\n\nimport random\nimport time\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\nprint('-'*25)\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\nfrom pandas.tools.plotting import scatter_matrix\n\n%matplotlib inline\nmpl.style.use('ggplot')\nsns.set_style('white')\npylab.rcParams['figure.figsize'] = 12,8","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6b8c250aeecfc7f7d0b38797b4c18220a305fc63","collapsed":true},"cell_type":"code","source":"# 加载数据源\ndata_raw = pd.read_csv('../input/train.csv')\ndata_val = pd.read_csv('../input/test.csv')\n\n# 玩数据前，先copy一个备份\ndata1 = data_raw.copy(deep = True)\n# 训练集和测试集都需要数据清洗，此处操作是为了后面清洗方便\ndata_cleaner = [data1, data_val]\n\n# 预览数据\nprint (data1.info())\ndata1.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fa52e28e68fff7212678081309c84a3102a67415"},"cell_type":"markdown","source":"## 字段说明\n- survival\t\n是否存活\t0 = No, 1 = Yes\n- pclass\t\n票类，可以反映乘客的社会经济地位\t1 = 1st, 2 = 2nd, 3 = 3rd\n- sex\t\n性别\t\n- Age\t\n年龄 Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n- sibsp 数据集用这样的形式来定义这样的家庭关系...... \n - Sibling =兄弟，姐妹，同父异母的弟弟，义妹\n - Spouse = 丈夫，妻子（包二奶和未婚夫被忽略）\n- parch\t 数据集用这样的形式来定义这样的家庭关系...... \n - Parent =母亲，父亲\n - Child = 女儿，儿子，继女，继子\n - 有些孩子只带着保姆旅行，因此parch = 0。\n- ticket\n票号\n- fare\t\n乘客票价\n- cabin\t\n客舱号码\n- embarked\t\n登船的港口\tC = Cherbourg, Q = Queenstown, S = Southampton\n"},{"metadata":{"trusted":true,"_uuid":"201aa8b553a8621db5926eab682e6566e6cb8f3d","collapsed":true},"cell_type":"code","source":"print('训练集缺失值统计:\\n', data1.isnull().sum())\nprint(\"-\"*10)\n\nprint('测试集缺失值统计:\\n', data_val.isnull().sum())\nprint(\"-\"*10)\n\ndata_raw.describe(include = 'all')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cacb6d7f67df57919d878120ef3f84c2ab691d83"},"cell_type":"markdown","source":"** 开发者文档: **\n* [pandas.DataFrame](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html)\n* [pandas.DataFrame.info](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.info.html)\n* [pandas.DataFrame.describe](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.describe.html)\n* [Indexing and Selecting Data](https://pandas.pydata.org/pandas-docs/stable/indexing.html)\n* [pandas.isnull](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.isnull.html)\n* [pandas.DataFrame.sum](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sum.html)\n* [pandas.DataFrame.mode](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.mode.html)\n* [pandas.DataFrame.copy](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.copy.html)\n* [pandas.DataFrame.fillna](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html)\n* [pandas.DataFrame.drop](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html)\n* [pandas.Series.value_counts](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.value_counts.html)\n* [pandas.DataFrame.loc](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.loc.html)"},{"metadata":{"trusted":true,"_uuid":"f3dffcb80f30b0ead8080f1d4d0f378af260409a","_kg_hide-output":true,"collapsed":true},"cell_type":"code","source":"### 缺失值处理\nfor dataset in data_cleaner:    \n    #用中位数填充缺失年龄\n    dataset['Age'].fillna(dataset['Age'].median(), inplace = True)\n\n    #complete embarked with mode\n    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)\n\n    #中位数填充票价\n    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)\n    \n# 删除乘客id、客舱号码与票号，因为这些数据感觉对预测乘客最终是否存活没有很大帮助\ndrop_column = ['PassengerId','Cabin', 'Ticket']\ndata1.drop(drop_column, axis=1, inplace = True)\n\nprint(data1.isnull().sum())\nprint(\"-\"*10)\nprint(data_val.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"74f624238f52456bad517c7475f5442dcd0eb82e","collapsed":true},"cell_type":"code","source":"### 特征工程\nfor dataset in data_cleaner:    \n    #Discrete variables 离散变量\n    dataset['FamilySize'] = dataset ['SibSp'] + dataset['Parch'] + 1\n\n    dataset['IsAlone'] = 1 #初始化为1\n    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0 #如果家庭人数大于1，更新为0\n\n    # http://www.pythonforbeginners.com/dictionary/python-split\n    dataset['Title'] = dataset['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n\n    # qcut vs cut: https://stackoverflow.com/questions/30211923/what-is-the-difference-between-pandas-qcut-and-pandas-cut\n    # https://pandas.pydata.org/pandas-docs/stable/generated/pandas.qcut.html\n    dataset['FareBin'] = pd.qcut(dataset['Fare'], 4)\n\n    # https://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html\n    dataset['AgeBin'] = pd.cut(dataset['Age'].astype(int), 5)\n\n#cleanup rare title names\n#print(data1['Title'].value_counts())\nstat_min = 10 #while small is arbitrary, we'll use the common minimum in statistics: http://nicholasjjackson.com/2012/03/08/sample-size-is-10-a-magic-number/\ntitle_names = (data1['Title'].value_counts() < stat_min) #this will create a true false series with title name as index\n\n#apply and lambda functions are quick and dirty code to find and replace with fewer lines of code: https://community.modeanalytics.com/python/tutorial/pandas-groupby-and-python-lambda-functions/\ndata1['Title'] = data1['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)\nprint(data1['Title'].value_counts())\nprint(\"-\"*10)\n\n#再次预览数据\ndata1.info()\nprint(\"-\"*10)\ndata_val.info()\nprint(\"-\"*10)\ndata1.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"20e5b8e5b072c7644e62a6ed05ddbdd91507b615"},"cell_type":"markdown","source":"## 数据格式转换\n\n将分类数据转换为虚拟变量进行数学分析。有多种方法可以对分类变量进行编码; 这里使用sklearn和pandas函数。\n\n** 开发者文档: **\n* [Categorical Encoding](http://pbpython.com/categorical-encoding.html)\n* [Sklearn LabelEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)\n* [Sklearn OneHotEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)\n* [Pandas Categorical dtype](https://pandas.pydata.org/pandas-docs/stable/categorical.html)\n* [pandas.get_dummies](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)"},{"metadata":{"trusted":true,"_uuid":"2d6e5dab6c1da9909c0eb1373c19a265ac2a59ef","collapsed":true},"cell_type":"code","source":"# 使用 Label Encoder 对数据集对象进行类别转换\n\nlabel = LabelEncoder()\nfor dataset in data_cleaner:    \n    dataset['Sex_Code'] = label.fit_transform(dataset['Sex'])\n    dataset['Embarked_Code'] = label.fit_transform(dataset['Embarked'])\n    dataset['Title_Code'] = label.fit_transform(dataset['Title'])\n    dataset['AgeBin_Code'] = label.fit_transform(dataset['AgeBin'])\n    dataset['FareBin_Code'] = label.fit_transform(dataset['FareBin'])\n\n\n# 定义 y\nTarget = ['Survived']\n\n# 定义x\n# data1_x 表格展示\n# data1_x_calc 模型算法的输入\ndata1_x = ['Sex','Pclass', 'Embarked', 'Title','SibSp', 'Parch', 'Age', 'Fare', 'FamilySize', 'IsAlone']\ndata1_x_calc = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code','SibSp', 'Parch', 'Age', 'Fare']\ndata1_xy =  Target + data1_x\nprint('Original X Y: ', data1_xy, '\\n')\n\n#define x variables for original w/bin features to remove continuous variables\ndata1_x_bin = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code', 'FamilySize', 'AgeBin_Code', 'FareBin_Code']\ndata1_xy_bin = Target + data1_x_bin\nprint('Bin X Y: ', data1_xy_bin, '\\n')\n\n\n#define x and y variables for dummy features original\ndata1_dummy = pd.get_dummies(data1[data1_x])\ndata1_x_dummy = data1_dummy.columns.tolist()\ndata1_xy_dummy = Target + data1_x_dummy\nprint('Dummy X Y: ', data1_xy_dummy, '\\n')\n\n\ndata1_dummy.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"553a1496185a072c8d8671e09f41752da7c68445","collapsed":true},"cell_type":"code","source":"# 数据已经清洗完毕，现在重新检查下数据\nprint('训练集缺失值统计: \\n', data1.isnull().sum())\nprint(\"-\"*10)\nprint (data1.info())\nprint(\"-\"*10)\n\nprint('验证集缺失值统计: \\n', data_val.isnull().sum())\nprint(\"-\"*10)\nprint (data_val.info())\nprint(\"-\"*10)\n\ndata_raw.describe(include = 'all')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9d5d1643e662055b89fbeabcc93f24c9c4e979f7"},"cell_type":"markdown","source":"## 划分数据集"},{"metadata":{"trusted":true,"_uuid":"197fa43d421655cb07357f397dfaeee1c3521253","collapsed":true},"cell_type":"code","source":"# https://www.quora.com/What-is-seed-in-random-number-generation\ntrain1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data1[data1_x_calc], data1[Target], random_state = 0)\ntrain1_x_bin, test1_x_bin, train1_y_bin, test1_y_bin = model_selection.train_test_split(data1[data1_x_bin], data1[Target] , random_state = 0)\ntrain1_x_dummy, test1_x_dummy, train1_y_dummy, test1_y_dummy = model_selection.train_test_split(data1_dummy[data1_x_dummy], data1[Target], random_state = 0)\n\n\nprint(\"Data1 Shape: {}\".format(data1.shape))\nprint(\"Train1 Shape: {}\".format(train1_x.shape))\nprint(\"Test1 Shape: {}\".format(test1_x.shape))\n\ntrain1_x_bin.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b16ad175ecb48a6608d60e0bdca237366551edc6"},"cell_type":"markdown","source":"## 用统计进行探索性分析\n数据清理已经完毕，现在需要用描述性和图形化的统计数据来进行探索，进而描述和总结变量。在这个阶段，可能需要对特征进行分类并确定它们与目标变量的相关性"},{"metadata":{"trusted":true,"_uuid":"3b2a621ae8872daa206bad892c81b62c06fbbe1b","collapsed":true},"cell_type":"code","source":"#Discrete Variable Correlation by Survival using\n#描述变量的相关性通过 \n#group by aka pivot table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\nfor x in data1_x:\n    if data1[x].dtype != 'float64' :\n        print('Survival Correlation by:', x)\n        print(data1[[x, Target[0]]].groupby(x, as_index=False).mean())\n        print('-'*10, '\\n')\n        \n\n#using crosstabs: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.crosstab.html\nprint(pd.crosstab(data1['Title'],data1[Target[0]]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e97fd4f5be130d49dfee332f6b3b865d330d192","collapsed":true},"cell_type":"code","source":"#注意: 特意用不同的形式进行绘画仅仅是基于学习的目的\n\n#可供选择的绘图方式: https://pandas.pydata.org/pandas-docs/stable/visualization.html\n#各种图形介绍：https://blog.csdn.net/suzyu12345/article/details/69029106\n\n#这里将使用 matplotlib.pyplot: https://matplotlib.org/api/pyplot_api.html\n\n#将使用 figure去组织图形: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.figure.html#matplotlib.pyplot.figure\n#subplot: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplot.html#matplotlib.pyplot.subplot\n#and subplotS: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplots.html?highlight=matplotlib%20pyplot%20subplots#matplotlib.pyplot.subplots\n\n#graph distribution of quantitative data\nplt.figure(figsize=[16,12])\n\n# 箱型图\nplt.subplot(231)\nplt.boxplot(x=data1['Fare'], showmeans = True, meanline = True)\nplt.title('Fare Boxplot')\nplt.ylabel('Fare ($)')\n\nplt.subplot(232)\nplt.boxplot(data1['Age'], showmeans = True, meanline = True)\nplt.title('Age Boxplot')\nplt.ylabel('Age (Years)')\n\nplt.subplot(233)\nplt.boxplot(data1['FamilySize'], showmeans = True, meanline = True)\nplt.title('Family Size Boxplot')\nplt.ylabel('Family Size (#)')\n\n# 直方图\nplt.subplot(234)\nplt.hist(x = [data1[data1['Survived']==1]['Fare'], data1[data1['Survived']==0]['Fare']], \n         stacked=True, color = ['g','r'],label = ['Survived','Dead'])\nplt.title('Fare Histogram by Survival')\nplt.xlabel('Fare ($)')\nplt.ylabel('# of Passengers')\nplt.legend()\n\nplt.subplot(235)\nplt.hist(x = [data1[data1['Survived']==1]['Age'], data1[data1['Survived']==0]['Age']], \n         stacked=True, color = ['g','r'],label = ['Survived','Dead'])\nplt.title('Age Histogram by Survival')\nplt.xlabel('Age (Years)')\nplt.ylabel('# of Passengers')\nplt.legend()\n\nplt.subplot(236)\nplt.hist(x = [data1[data1['Survived']==1]['FamilySize'], data1[data1['Survived']==0]['FamilySize']], \n         stacked=True, color = ['g','r'],label = ['Survived','Dead'])\nplt.title('Family Size Histogram by Survival')\nplt.xlabel('Family Size (#)')\nplt.ylabel('# of Passengers')\nplt.legend()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2cc9c9cfbb60d544027ae1859d9b934b4b1a42f4","collapsed":true},"cell_type":"code","source":"#使用 seaborn 进行多变量的比较\n\n#graph individual features by survival  通过生存来绘制个体特征\nfig, saxis = plt.subplots(2, 3,figsize=(16,12))\n\n# 柱状图\nsns.barplot(x = 'Embarked', y = 'Survived', data=data1, ax = saxis[0,0])\nsns.barplot(x = 'Pclass', y = 'Survived', order=[1,2,3], data=data1, ax = saxis[0,1])\nsns.barplot(x = 'IsAlone', y = 'Survived', order=[1,0], data=data1, ax = saxis[0,2])\n\nsns.pointplot(x = 'FareBin', y = 'Survived',  data=data1, ax = saxis[1,0])\nsns.pointplot(x = 'AgeBin', y = 'Survived',  data=data1, ax = saxis[1,1])\nsns.pointplot(x = 'FamilySize', y = 'Survived', data=data1, ax = saxis[1,2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fb6e7bc68be40bba596e942e64130d8b5fe2da5d","collapsed":true},"cell_type":"code","source":"#graph distribution of qualitative data: Pclass\n#we know class mattered in survival, now let's compare class and a 2nd feature\nfig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(14,12))\n\nsns.boxplot(x = 'Pclass', y = 'Fare', hue = 'Survived', data = data1, ax = axis1)\naxis1.set_title('Pclass vs Fare Survival Comparison')\n\n# 小提琴图\nsns.violinplot(x = 'Pclass', y = 'Age', hue = 'Survived', data = data1, split = True, ax = axis2)\naxis2.set_title('Pclass vs Age Survival Comparison')\n\nsns.boxplot(x = 'Pclass', y ='FamilySize', hue = 'Survived', data = data1, ax = axis3)\naxis3.set_title('Pclass vs Family Size Survival Comparison')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bb32e74065f76d7ea1cd426a6c74b4ca39de361c","collapsed":true},"cell_type":"code","source":"#graph distribution of qualitative data: Sex\n#we know sex mattered in survival, now let's compare sex and a 2nd feature\nfig, qaxis = plt.subplots(1,3,figsize=(14,12))\n\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'Embarked', data=data1, ax = qaxis[0])\naxis1.set_title('Sex vs Embarked Survival Comparison')\n\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'Pclass', data=data1, ax  = qaxis[1])\naxis1.set_title('Sex vs Pclass Survival Comparison')\n\nsns.barplot(x = 'Sex', y = 'Survived', hue = 'IsAlone', data=data1, ax  = qaxis[2])\naxis1.set_title('Sex vs IsAlone Survival Comparison')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2f01fea22afddf43b4761c9fddcb59bda546c605","collapsed":true},"cell_type":"code","source":"#more side-by-side comparisons\n#更多的比较\nfig, (maxis1, maxis2) = plt.subplots(1, 2,figsize=(14,12))\n\n#how does family size factor with sex & survival compare\nsns.pointplot(x=\"FamilySize\", y=\"Survived\", hue=\"Sex\", data=data1,\n              palette={\"male\": \"blue\", \"female\": \"pink\"},\n              markers=[\"*\", \"o\"], linestyles=[\"-\", \"--\"], ax = maxis1)\n\n#how does class factor with sex & survival compare\nsns.pointplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=data1,\n              palette={\"male\": \"blue\", \"female\": \"pink\"},\n              markers=[\"*\", \"o\"], linestyles=[\"-\", \"--\"], ax = maxis2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"93f70d28350e5caf61346205d4b4b9410afc05fe","collapsed":true},"cell_type":"code","source":"#how does embark port factor with class, sex, and survival compare\n#facetgrid: https://seaborn.pydata.org/generated/seaborn.FacetGrid.html\ne = sns.FacetGrid(data1, col = 'Embarked')\ne.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', ci=95.0, palette = 'deep')\ne.add_legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ed4a877e888c75f52285d7d9837e8e151d00b64d","collapsed":true},"cell_type":"code","source":"#plot distributions of age of passengers who survived or did not survive\n# 绘制年龄的分布\na = sns.FacetGrid( data1, hue = 'Survived', aspect=4 )\na.map(sns.kdeplot, 'Age', shade= True )\na.set(xlim=(0 , data1['Age'].max()))\na.add_legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1dab7b1697d571f7d9923cd08c3812cbb022e25e","collapsed":true},"cell_type":"code","source":"#histogram comparison of sex, class, and age by survival\nh = sns.FacetGrid(data1, row = 'Sex', col = 'Pclass', hue = 'Survived')\nh.map(plt.hist, 'Age', alpha = .75)\nh.add_legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"991da0ae79810fe9e6a2fb1b7efff37f76a1225b","collapsed":true},"cell_type":"code","source":"#pair plots of entire dataset\n# 整个数据集的 pair plots\npp = sns.pairplot(data1, hue = 'Survived', palette = 'deep', size=1.2, diag_kind = 'kde', diag_kws=dict(shade=True), plot_kws=dict(s=10) )\npp.set(xticklabels=[])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e2e47b416d75287ed740b49f32f5ff3a07bfe450","collapsed":true},"cell_type":"code","source":"#数据集的相关热图\ndef correlation_heatmap(df):\n    _ , ax = plt.subplots(figsize =(14, 12))\n    colormap = sns.diverging_palette(220, 10, as_cmap = True)\n    \n    _ = sns.heatmap(\n        df.corr(), \n        cmap = colormap,\n        square=True, \n        cbar_kws={'shrink':.9 }, \n        ax=ax,\n        annot=True, \n        linewidths=0.1,vmax=1.0, linecolor='white',\n        annot_kws={'fontsize':12 }\n    )\n    \n    plt.title('Pearson Correlation of Features', y=1.05, size=15)\n\ncorrelation_heatmap(data1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2f1c5ed416ad8af80abf20e354538637ae0a610b"},"cell_type":"markdown","source":"## 建模\n\n\n**Machine Learning Selection:**\n* [Sklearn Estimator Overview](http://scikit-learn.org/stable/user_guide.html)\n* [Sklearn Estimator Detail](http://scikit-learn.org/stable/modules/classes.html)\n* [Choosing Estimator Mind Map](http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)\n* [Choosing Estimator Cheat Sheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Scikit_Learn_Cheat_Sheet_Python.pdf)\n\n本次建模属于监督学习分类算法。\n\n**Machine Learning Classification Algorithms:**\n* [Ensemble Methods](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble)\n* [Generalized Linear Models (GLM)](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model)\n* [Naive Bayes](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.naive_bayes)\n* [Nearest Neighbors](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.neighbors)\n* [Support Vector Machines (SVM)](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.svm)\n* [Decision Trees](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.tree)\n* [Discriminant Analysis](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.discriminant_analysis)"},{"metadata":{"trusted":true,"_uuid":"1c052fe52b61df59db9999e5ac15f04b916e1be8","collapsed":true},"cell_type":"code","source":"# 机器学习算法选择及初始化\nMLA = [\n    #集成算法\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n\n    #高斯\n    gaussian_process.GaussianProcessClassifier(),\n    \n    #GLM\n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n    \n    #贝叶斯\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    #Nearest Neighbor\n    neighbors.KNeighborsClassifier(),\n    \n    #SVM\n    svm.SVC(probability=True),\n    svm.NuSVC(probability=True),\n    svm.LinearSVC(),\n    \n    #Trees    \n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(),\n    \n    #Discriminant Analysis 判别分析\n    discriminant_analysis.LinearDiscriminantAnalysis(),\n    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n\n    \n    #xgboost: http://xgboost.readthedocs.io/en/latest/model.html\n    XGBClassifier()    \n    ]\n\n\n#split dataset in cross-validation with this splitter class: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit\n# ShuffleSplit 是 train_test_split 的替换方案\ncv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0 ) # run model 10x with 60/30 split intentionally leaving out 10%\n\n#create table to compare MLA metrics\nMLA_columns = ['MLA Name', 'MLA Parameters','MLA Train Accuracy Mean', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD' ,'MLA Time']\nMLA_compare = pd.DataFrame(columns = MLA_columns)\n\n#create table to compare MLA predictions\nMLA_predict = data1[Target]\n\n#index through MLA and save performance to table\n# 通过MLA进行索引，并将表现保存到表格中\nrow_index = 0\nfor alg in MLA:\n\n    #set name and parameters\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n    \n    #score model with cross validation: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate\n    cv_results = model_selection.cross_validate(alg, data1[data1_x_bin], data1[Target], cv  = cv_split)\n\n    MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n    MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean()\n    MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()   \n    #if this is a non-bias random sample, then +/-3 standard deviations (std) from the mean, should statistically capture 99.7% of the subsets\n    MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3   #let's know the worst that can happen!\n    \n\n    #save MLA predictions - see section 6 for usage\n    alg.fit(data1[data1_x_bin], data1[Target])\n    MLA_predict[MLA_name] = alg.predict(data1[data1_x_bin])\n    \n    row_index+=1\n\n    \n#print and sort table: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_values.html\nMLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\n# 算法比对\nMLA_compare\n\n# 预测结果\n# MLA_predict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"110862618205058197e6e601adf0d814a06e6618","collapsed":true},"cell_type":"code","source":"#barplot using https://seaborn.pydata.org/generated/seaborn.barplot.html\nsns.barplot(x='MLA Test Accuracy Mean', y = 'MLA Name', data = MLA_compare, color = 'm')\n\n#使用 pyplot 进行美化: https://matplotlib.org/api/pyplot_api.html\nplt.title('Machine Learning Algorithm Accuracy Score \\n')\nplt.xlabel('Accuracy Score (%)')\nplt.ylabel('Algorithm')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6a28082876b866f0949b06e13b26ab1d8548df96"},"cell_type":"markdown","source":"## 通过网格搜索继续调整模型\n\n直接调用使用的都是模型的默认参数，我们可以使用 [ParameterGrid](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ParameterGrid.html#sklearn.model_selection.ParameterGrid), [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV) 和自定义  [sklearn scoring](http://scikit-learn.org/stable/modules/model_evaluation.html) 继续调整模型提高精准率[](http://); [点击这里了解更多 ROC_AUC scores 的信息](http://www.dataschool.io/roc-curves-and-auc-explained/)."},{"metadata":{"trusted":true,"_uuid":"25a1b3674ae1fcfa137a44f69361cb934182d3e4","collapsed":true},"cell_type":"code","source":"#这里以决策树为基准模型\ndtree = tree.DecisionTreeClassifier(random_state = 0)\nbase_results = model_selection.cross_validate(dtree, data1[data1_x_bin], data1[Target], cv  = cv_split)\ndtree.fit(data1[data1_x_bin], data1[Target])\n\nprint('BEFORE DT Parameters: ', dtree.get_params())\nprint(\"BEFORE DT Training w/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \nprint(\"BEFORE DT Test w/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\nprint(\"BEFORE DT Test w/bin score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\n#print(\"BEFORE DT Test w/bin set score min: {:.2f}\". format(base_results['test_score'].min()*100))\nprint('-'*10)\n\n#tune hyper-parameters: http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier\nparam_grid = {'criterion': ['gini', 'entropy'],  #scoring methodology; two supported formulas for calculating information gain - default is gini\n              #'splitter': ['best', 'random'], #splitting methodology; two supported strategies - default is best\n              'max_depth': [2,4,6,8,10,None], #max depth tree can grow; default is none\n              #'min_samples_split': [2,5,10,.03,.05], #minimum subset size BEFORE new split (fraction is % of total); default is 2\n              #'min_samples_leaf': [1,5,10,.03,.05], #minimum subset size AFTER new split split (fraction is % of total); default is 1\n              #'max_features': [None, 'auto'], #max features to consider when performing split; default none or all\n              'random_state': [0] #seed or control random number generator: https://www.quora.com/What-is-seed-in-random-number-generation\n             }\n\n#print(list(model_selection.ParameterGrid(param_grid)))\n\n#choose best model with grid_search: #http://scikit-learn.org/stable/modules/grid_search.html#grid-search\n#http://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html\ntune_model = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, scoring = 'roc_auc', cv = cv_split)\ntune_model.fit(data1[data1_x_bin], data1[Target])\n\n#print(tune_model.cv_results_.keys())\n#print(tune_model.cv_results_['params'])\nprint('AFTER DT Parameters: ', tune_model.best_params_)\n#print(tune_model.cv_results_['mean_train_score'])\nprint(\"AFTER DT Training w/bin score mean: {:.2f}\". format(tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100)) \n#print(tune_model.cv_results_['mean_test_score'])\nprint(\"AFTER DT Test w/bin score mean: {:.2f}\". format(tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))\nprint(\"AFTER DT Test w/bin score 3*std: +/- {:.2f}\". format(tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3))\nprint('-'*10)\n\n#duplicates gridsearchcv\n#tune_results = model_selection.cross_validate(tune_model, data1[data1_x_bin], data1[Target], cv  = cv_split)\n\n#print('AFTER DT Parameters: ', tune_model.best_params_)\n#print(\"AFTER DT Training w/bin set score mean: {:.2f}\". format(tune_results['train_score'].mean()*100)) \n#print(\"AFTER DT Test w/bin set score mean: {:.2f}\". format(tune_results['test_score'].mean()*100))\n#print(\"AFTER DT Test w/bin set score min: {:.2f}\". format(tune_results['test_score'].min()*100))\n#print('-'*10)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"38c2a34b751a6528636731efc2e7edc941a3de6e","collapsed":true},"cell_type":"code","source":"Y_pred = tune_model.predict(data_val[data1_x_bin])\nsubmission = pd.DataFrame({\n        \"PassengerId\": data_val[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\nsubmission.to_csv('titanic.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a8ca7f0113283ba68604d2e61f20793634a2e883"},"cell_type":"markdown","source":"## 使用交叉验证进行特征选择\n\n可以将特征选择和网格搜索联合使用\n\n关于特征选择，[Sklearn](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection)有好几种可供选择，这里将使用[交叉验证（CV）的递归特征消除（RFE）](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html#sklearn.feature_selection.RFECV)。"},{"metadata":{"trusted":true,"_uuid":"df6ed7a90308607cfbb46481ef60739f4586db61","collapsed":true},"cell_type":"code","source":"#base model\nprint('BEFORE DT RFE Training Shape Old: ', data1[data1_x_bin].shape) \nprint('BEFORE DT RFE Training Columns Old: ', data1[data1_x_bin].columns.values)\n\nprint(\"BEFORE DT RFE Training w/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \nprint(\"BEFORE DT RFE Test w/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\nprint(\"BEFORE DT RFE Test w/bin score 3*std: +/- {:.2f}\". format(base_results['test_score'].std()*100*3))\nprint('-'*10)\n\n\n\n#feature selection\ndtree_rfe = feature_selection.RFECV(dtree, step = 1, scoring = 'accuracy', cv = cv_split)\ndtree_rfe.fit(data1[data1_x_bin], data1[Target])\n\n#transform x&y to reduced features and fit new model\n#alternative: can use pipeline to reduce fit and transform steps: http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\nX_rfe = data1[data1_x_bin].columns.values[dtree_rfe.get_support()]\nrfe_results = model_selection.cross_validate(dtree, data1[X_rfe], data1[Target], cv  = cv_split)\n\n#print(dtree_rfe.grid_scores_)\nprint('AFTER DT RFE Training Shape New: ', data1[X_rfe].shape) \nprint('AFTER DT RFE Training Columns New: ', X_rfe)\n\nprint(\"AFTER DT RFE Training w/bin score mean: {:.2f}\". format(rfe_results['train_score'].mean()*100)) \nprint(\"AFTER DT RFE Test w/bin score mean: {:.2f}\". format(rfe_results['test_score'].mean()*100))\nprint(\"AFTER DT RFE Test w/bin score 3*std: +/- {:.2f}\". format(rfe_results['test_score'].std()*100*3))\nprint('-'*10)\n\n\n#tune rfe model\nrfe_tune_model = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, scoring = 'roc_auc', cv = cv_split)\nrfe_tune_model.fit(data1[X_rfe], data1[Target])\n\n#print(rfe_tune_model.cv_results_.keys())\n#print(rfe_tune_model.cv_results_['params'])\nprint('AFTER DT RFE Tuned Parameters: ', rfe_tune_model.best_params_)\n#print(rfe_tune_model.cv_results_['mean_train_score'])\nprint(\"AFTER DT RFE Tuned Training w/bin score mean: {:.2f}\". format(rfe_tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100)) \n#print(rfe_tune_model.cv_results_['mean_test_score'])\nprint(\"AFTER DT RFE Tuned Test w/bin score mean: {:.2f}\". format(rfe_tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))\nprint(\"AFTER DT RFE Tuned Test w/bin score 3*std: +/- {:.2f}\". format(rfe_tune_model.cv_results_['std_test_score'][tune_model.best_index_]*100*3))\nprint('-'*10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"9eed16a006ccf6e60ea9a1bc4afb0540795674a9","collapsed":true},"cell_type":"code","source":"#Graph MLA version of Decision Tree: http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\n#决策树可视化\nimport graphviz \ndot_data = tree.export_graphviz(dtree, out_file=None, \n                                feature_names = data1_x_bin, class_names = True,\n                                filled = True, rounded = True)\ngraph = graphviz.Source(dot_data) \n# graph","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"759272ccd254e775c48fc912c6979af12ac8ab0d"},"cell_type":"markdown","source":"## Validate and Implement\n使用验证数据进行提交"},{"metadata":{"trusted":true,"_uuid":"ee7f66abea3fb14b06df89bd01d342c8002d53a9","collapsed":true},"cell_type":"code","source":"#compare algorithm predictions with each other, where 1 = exactly similar and 0 = exactly opposite\n#there are some 1's, but enough blues and light reds to create a \"super algorithm\" by combining them\n# 将算法的预测结果两两比对\ncorrelation_heatmap(MLA_predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e1d4788b0b9907422bb79bdc8e9a9dd93677498","collapsed":true},"cell_type":"code","source":"#why choose one model, when you can pick them all with voting classifier\n#http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html\n#removed models w/o attribute 'predict_proba' required for vote classifier and models with a 1.0 correlation to another model\nvote_est = [\n    #Ensemble Methods: http://scikit-learn.org/stable/modules/ensemble.html\n    ('ada', ensemble.AdaBoostClassifier()),\n    ('bc', ensemble.BaggingClassifier()),\n    ('etc',ensemble.ExtraTreesClassifier()),\n    ('gbc', ensemble.GradientBoostingClassifier()),\n    ('rfc', ensemble.RandomForestClassifier()),\n\n    #Gaussian Processes: http://scikit-learn.org/stable/modules/gaussian_process.html#gaussian-process-classification-gpc\n    ('gpc', gaussian_process.GaussianProcessClassifier()),\n    \n    #GLM: http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n    ('lr', linear_model.LogisticRegressionCV()),\n    \n    #Navies Bayes: http://scikit-learn.org/stable/modules/naive_bayes.html\n    ('bnb', naive_bayes.BernoulliNB()),\n    ('gnb', naive_bayes.GaussianNB()),\n    \n    #Nearest Neighbor: http://scikit-learn.org/stable/modules/neighbors.html\n    ('knn', neighbors.KNeighborsClassifier()),\n    \n    #SVM: http://scikit-learn.org/stable/modules/svm.html\n    ('svc', svm.SVC(probability=True)),\n    \n    #xgboost: http://xgboost.readthedocs.io/en/latest/model.html\n   ('xgb', XGBClassifier())\n\n]\n\n\n#Hard Vote or majority rules\nvote_hard = ensemble.VotingClassifier(estimators = vote_est , voting = 'hard')\nvote_hard_cv = model_selection.cross_validate(vote_hard, data1[data1_x_bin], data1[Target], cv  = cv_split)\nvote_hard.fit(data1[data1_x_bin], data1[Target])\n\nprint(\"Hard Voting Training w/bin score mean: {:.2f}\". format(vote_hard_cv['train_score'].mean()*100)) \nprint(\"Hard Voting Test w/bin score mean: {:.2f}\". format(vote_hard_cv['test_score'].mean()*100))\nprint(\"Hard Voting Test w/bin score 3*std: +/- {:.2f}\". format(vote_hard_cv['test_score'].std()*100*3))\nprint('-'*10)\n\n\n#Soft Vote or weighted probabilities\nvote_soft = ensemble.VotingClassifier(estimators = vote_est , voting = 'soft')\nvote_soft_cv = model_selection.cross_validate(vote_soft, data1[data1_x_bin], data1[Target], cv  = cv_split)\nvote_soft.fit(data1[data1_x_bin], data1[Target])\n\nprint(\"Soft Voting Training w/bin score mean: {:.2f}\". format(vote_soft_cv['train_score'].mean()*100)) \nprint(\"Soft Voting Test w/bin score mean: {:.2f}\". format(vote_soft_cv['test_score'].mean()*100))\nprint(\"Soft Voting Test w/bin score 3*std: +/- {:.2f}\". format(vote_soft_cv['test_score'].std()*100*3))\nprint('-'*10)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"2469895e07dbc8a66e3eb257a4ebb928d19b25cc"},"cell_type":"code","source":"# #IMPORTANT: THIS SECTION IS UNDER CONSTRUCTION!!!! 12.24.17\n# #UPDATE: This section was scrapped for the next section; as it's more computational friendly.\n\n# #WARNING: Running is very computational intensive and time expensive\n# #code is written for experimental/developmental purposes and not production ready\n\n\n# #tune each estimator before creating a super model\n# #http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n# grid_n_estimator = [50,100,300]\n# grid_ratio = [.1,.25,.5,.75,1.0]\n# grid_learn = [.01,.03,.05,.1,.25]\n# grid_max_depth = [2,4,6,None]\n# grid_min_samples = [5,10,.03,.05,.10]\n# grid_criterion = ['gini', 'entropy']\n# grid_bool = [True, False]\n# grid_seed = [0]\n\n# vote_param = [{\n# #            #http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n#             'ada__n_estimators': grid_n_estimator,\n#             'ada__learning_rate': grid_ratio,\n#             'ada__algorithm': ['SAMME', 'SAMME.R'],\n#             'ada__random_state': grid_seed,\n    \n#             #http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier\n#             'bc__n_estimators': grid_n_estimator,\n#             'bc__max_samples': grid_ratio,\n#             'bc__oob_score': grid_bool, \n#             'bc__random_state': grid_seed,\n            \n#             #http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier\n#             'etc__n_estimators': grid_n_estimator,\n#             'etc__criterion': grid_criterion,\n#             'etc__max_depth': grid_max_depth,\n#             'etc__random_state': grid_seed,\n\n\n#             #http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier\n#             'gbc__loss': ['deviance', 'exponential'],\n#             'gbc__learning_rate': grid_ratio,\n#             'gbc__n_estimators': grid_n_estimator,\n#             'gbc__criterion': ['friedman_mse', 'mse', 'mae'],\n#             'gbc__max_depth': grid_max_depth,\n#             'gbc__min_samples_split': grid_min_samples,\n#             'gbc__min_samples_leaf': grid_min_samples,      \n#             'gbc__random_state': grid_seed,\n    \n#             #http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n#             'rfc__n_estimators': grid_n_estimator,\n#             'rfc__criterion': grid_criterion,\n#             'rfc__max_depth': grid_max_depth,\n#             'rfc__min_samples_split': grid_min_samples,\n#             'rfc__min_samples_leaf': grid_min_samples,   \n#             'rfc__bootstrap': grid_bool,\n#             'rfc__oob_score': grid_bool, \n#             'rfc__random_state': grid_seed,\n        \n#             #http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV\n#             'lr__fit_intercept': grid_bool,\n#             'lr__penalty': ['l1','l2'],\n#             'lr__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n#             'lr__random_state': grid_seed,\n            \n#             #http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB\n#             'bnb__alpha': grid_ratio,\n#             'bnb__prior': grid_bool,\n#             'bnb__random_state': grid_seed,\n    \n#             #http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier\n#             'knn__n_neighbors': [1,2,3,4,5,6,7],\n#             'knn__weights': ['uniform', 'distance'],\n#             'knn__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n#             'knn__random_state': grid_seed,\n            \n#             #http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC\n#             #http://blog.hackerearth.com/simple-tutorial-svm-parameter-tuning-python-r\n#             'svc__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n#             'svc__C': grid_max_depth,\n#             'svc__gamma': grid_ratio,\n#             'svc__decision_function_shape': ['ovo', 'ovr'],\n#             'svc__probability': [True],\n#             'svc__random_state': grid_seed,\n    \n    \n#             #http://xgboost.readthedocs.io/en/latest/parameter.html\n#             'xgb__learning_rate': grid_ratio,\n#             'xgb__max_depth': [2,4,6,8,10],\n#             'xgb__tree_method': ['exact', 'approx', 'hist'],\n#             'xgb__objective': ['reg:linear', 'reg:logistic', 'binary:logistic'],\n#             'xgb__seed': grid_seed    \n\n#         }]\n\n\n\n\n# #Soft Vote with tuned models\n# #grid_soft = model_selection.GridSearchCV(estimator = vote_soft, param_grid = vote_param, cv = 2, scoring = 'roc_auc')\n# #grid_soft.fit(data1[data1_x_bin], data1[Target])\n\n# #print(grid_soft.cv_results_.keys())\n# #print(grid_soft.cv_results_['params'])\n# #print('Soft Vote Tuned Parameters: ', grid_soft.best_params_)\n# #print(grid_soft.cv_results_['mean_train_score'])\n# #print(\"Soft Vote Tuned Training w/bin set score mean: {:.2f}\". format(grid_soft.cv_results_['mean_train_score'][tune_model.best_index_]*100)) \n# #print(grid_soft.cv_results_['mean_test_score'])\n# #print(\"Soft Vote Tuned Test w/bin set score mean: {:.2f}\". format(grid_soft.cv_results_['mean_test_score'][tune_model.best_index_]*100))\n# #print(\"Soft Vote Tuned Test w/bin score 3*std: +/- {:.2f}\". format(grid_soft.cv_results_['std_test_score'][tune_model.best_index_]*100*3))\n# #print('-'*10)\n\n\n# #credit: https://rasbt.github.io/mlxtend/user_guide/classifier/EnsembleVoteClassifier/\n# #cv_keys = ('mean_test_score', 'std_test_score', 'params')\n# #for r, _ in enumerate(grid_soft.cv_results_['mean_test_score']):\n# #    print(\"%0.3f +/- %0.2f %r\"\n# #          % (grid_soft.cv_results_[cv_keys[0]][r],\n# #             grid_soft.cv_results_[cv_keys[1]][r] / 2.0,\n# #             grid_soft.cv_results_[cv_keys[2]][r]))\n\n\n# #print('-'*10)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"637e7f6d9af56259147530f6ef3f723714b91871"},"cell_type":"code","source":"# #WARNING: Running is very computational intensive and time expensive.\n# #Code is written for experimental/developmental purposes and not production ready!\n\n\n# #Hyperparameter Tune with GridSearchCV: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n# grid_n_estimator = [10, 50, 100, 300]\n# grid_ratio = [.1, .25, .5, .75, 1.0]\n# grid_learn = [.01, .03, .05, .1, .25]\n# grid_max_depth = [2, 4, 6, 8, 10, None]\n# grid_min_samples = [5, 10, .03, .05, .10]\n# grid_criterion = ['gini', 'entropy']\n# grid_bool = [True, False]\n# grid_seed = [0]\n\n\n# grid_param = [\n#             [{\n#             #AdaBoostClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n#             'n_estimators': grid_n_estimator, #default=50\n#             'learning_rate': grid_learn, #default=1\n#             #'algorithm': ['SAMME', 'SAMME.R'], #default=’SAMME.R\n#             'random_state': grid_seed\n#             }],\n       \n    \n#             [{\n#             #BaggingClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier\n#             'n_estimators': grid_n_estimator, #default=10\n#             'max_samples': grid_ratio, #default=1.0\n#             'random_state': grid_seed\n#              }],\n\n    \n#             [{\n#             #ExtraTreesClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier\n#             'n_estimators': grid_n_estimator, #default=10\n#             'criterion': grid_criterion, #default=”gini”\n#             'max_depth': grid_max_depth, #default=None\n#             'random_state': grid_seed\n#              }],\n\n\n#             [{\n#             #GradientBoostingClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier\n#             #'loss': ['deviance', 'exponential'], #default=’deviance’\n#             'learning_rate': [.05], #default=0.1 -- 12/31/17 set to reduce runtime -- The best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300, 'random_state': 0} with a runtime of 264.45 seconds.\n#             'n_estimators': [300], #default=100 -- 12/31/17 set to reduce runtime -- The best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300, 'random_state': 0} with a runtime of 264.45 seconds.\n#             #'criterion': ['friedman_mse', 'mse', 'mae'], #default=”friedman_mse”\n#             'max_depth': grid_max_depth, #default=3   \n#             'random_state': grid_seed\n#              }],\n\n    \n#             [{\n#             #RandomForestClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier\n#             'n_estimators': grid_n_estimator, #default=10\n#             'criterion': grid_criterion, #default=”gini”\n#             'max_depth': grid_max_depth, #default=None\n#             'oob_score': [True], #default=False -- 12/31/17 set to reduce runtime -- The best parameter for RandomForestClassifier is {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'oob_score': True, 'random_state': 0} with a runtime of 146.35 seconds.\n#             'random_state': grid_seed\n#              }],\n    \n#             [{    \n#             #GaussianProcessClassifier\n#             'max_iter_predict': grid_n_estimator, #default: 100\n#             'random_state': grid_seed\n#             }],\n        \n    \n#             [{\n#             #LogisticRegressionCV - http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html#sklearn.linear_model.LogisticRegressionCV\n#             'fit_intercept': grid_bool, #default: True\n#             #'penalty': ['l1','l2'],\n#             'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'], #default: lbfgs\n#             'random_state': grid_seed\n#              }],\n            \n    \n#             [{\n#             #BernoulliNB - http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB\n#             'alpha': grid_ratio, #default: 1.0\n#              }],\n    \n    \n#             #GaussianNB - \n#             [{}],\n    \n#             [{\n#             #KNeighborsClassifier - http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier\n#             'n_neighbors': [1,2,3,4,5,6,7], #default: 5\n#             'weights': ['uniform', 'distance'], #default = ‘uniform’\n#             'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n#             }],\n            \n    \n#             [{\n#             #SVC - http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC\n#             #http://blog.hackerearth.com/simple-tutorial-svm-parameter-tuning-python-r\n#             #'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n#             'C': [1,2,3,4,5], #default=1.0\n#             'gamma': grid_ratio, #edfault: auto\n#             'decision_function_shape': ['ovo', 'ovr'], #default:ovr\n#             'probability': [True],\n#             'random_state': grid_seed\n#              }],\n\n    \n#             [{\n#             #XGBClassifier - http://xgboost.readthedocs.io/en/latest/parameter.html\n#             'learning_rate': grid_learn, #default: .3\n#             'max_depth': [1,2,4,6,8,10], #default 2\n#             'n_estimators': grid_n_estimator, \n#             'seed': grid_seed  \n#              }]   \n#         ]\n\n\n\n# start_total = time.perf_counter() #https://docs.python.org/3/library/time.html#time.perf_counter\n# for clf, param in zip (vote_est, grid_param): #https://docs.python.org/3/library/functions.html#zip\n\n#     #print(clf[1]) #vote_est is a list of tuples, index 0 is the name and index 1 is the algorithm\n#     #print(param)\n    \n    \n#     start = time.perf_counter()        \n#     best_search = model_selection.GridSearchCV(estimator = clf[1], param_grid = param, cv = cv_split, scoring = 'roc_auc')\n#     best_search.fit(data1[data1_x_bin], data1[Target])\n#     run = time.perf_counter() - start\n\n#     best_param = best_search.best_params_\n#     print('The best parameter for {} is {} with a runtime of {:.2f} seconds.'.format(clf[1].__class__.__name__, best_param, run))\n#     clf[1].set_params(**best_param) \n\n\n# run_total = time.perf_counter() - start_total\n# print('Total optimization time was {:.2f} minutes.'.format(run_total/60))\n\n# print('-'*10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c1ce8e584e8e3e8b10daf343a3be30c3478708c9"},"cell_type":"code","source":"# #Hard Vote or majority rules w/Tuned Hyperparameters\n# grid_hard = ensemble.VotingClassifier(estimators = vote_est , voting = 'hard')\n# grid_hard_cv = model_selection.cross_validate(grid_hard, data1[data1_x_bin], data1[Target], cv  = cv_split)\n# grid_hard.fit(data1[data1_x_bin], data1[Target])\n\n# print(\"Hard Voting w/Tuned Hyperparameters Training w/bin score mean: {:.2f}\". format(grid_hard_cv['train_score'].mean()*100)) \n# print(\"Hard Voting w/Tuned Hyperparameters Test w/bin score mean: {:.2f}\". format(grid_hard_cv['test_score'].mean()*100))\n# print(\"Hard Voting w/Tuned Hyperparameters Test w/bin score 3*std: +/- {:.2f}\". format(grid_hard_cv['test_score'].std()*100*3))\n# print('-'*10)\n\n# #Soft Vote or weighted probabilities w/Tuned Hyperparameters\n# grid_soft = ensemble.VotingClassifier(estimators = vote_est , voting = 'soft')\n# grid_soft_cv = model_selection.cross_validate(grid_soft, data1[data1_x_bin], data1[Target], cv  = cv_split)\n# grid_soft.fit(data1[data1_x_bin], data1[Target])\n\n# print(\"Soft Voting w/Tuned Hyperparameters Training w/bin score mean: {:.2f}\". format(grid_soft_cv['train_score'].mean()*100)) \n# print(\"Soft Voting w/Tuned Hyperparameters Test w/bin score mean: {:.2f}\". format(grid_soft_cv['test_score'].mean()*100))\n# print(\"Soft Voting w/Tuned Hyperparameters Test w/bin score 3*std: +/- {:.2f}\". format(grid_soft_cv['test_score'].std()*100*3))\n# print('-'*10)\n\n\n# #12/31/17 tuned with data1_x_bin\n# #The best parameter for AdaBoostClassifier is {'learning_rate': 0.1, 'n_estimators': 300, 'random_state': 0} with a runtime of 33.39 seconds.\n# #The best parameter for BaggingClassifier is {'max_samples': 0.25, 'n_estimators': 300, 'random_state': 0} with a runtime of 30.28 seconds.\n# #The best parameter for ExtraTreesClassifier is {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'random_state': 0} with a runtime of 64.76 seconds.\n# #The best parameter for GradientBoostingClassifier is {'learning_rate': 0.05, 'max_depth': 2, 'n_estimators': 300, 'random_state': 0} with a runtime of 34.35 seconds.\n# #The best parameter for RandomForestClassifier is {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'oob_score': True, 'random_state': 0} with a runtime of 76.32 seconds.\n# #The best parameter for GaussianProcessClassifier is {'max_iter_predict': 10, 'random_state': 0} with a runtime of 6.01 seconds.\n# #The best parameter for LogisticRegressionCV is {'fit_intercept': True, 'random_state': 0, 'solver': 'liblinear'} with a runtime of 8.04 seconds.\n# #The best parameter for BernoulliNB is {'alpha': 0.1} with a runtime of 0.19 seconds.\n# #The best parameter for GaussianNB is {} with a runtime of 0.04 seconds.\n# #The best parameter for KNeighborsClassifier is {'algorithm': 'brute', 'n_neighbors': 7, 'weights': 'uniform'} with a runtime of 4.84 seconds.\n# #The best parameter for SVC is {'C': 2, 'decision_function_shape': 'ovo', 'gamma': 0.1, 'probability': True, 'random_state': 0} with a runtime of 29.39 seconds.\n# #The best parameter for XGBClassifier is {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 300, 'seed': 0} with a runtime of 46.23 seconds.\n# #Total optimization time was 5.56 minutes.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"1755364a2c7531e782d0a15fde4c73dd83769af5"},"cell_type":"code","source":"# #prepare data for modeling\n# print(data_val.info())\n# print(\"-\"*10)\n# #data_val.sample(10)\n\n\n\n# #handmade decision tree - submission score = 0.77990\n# data_val['Survived'] = mytree(data_val).astype(int)\n\n\n# #decision tree w/full dataset modeling submission score: defaults= 0.76555, tuned= 0.77990\n# #submit_dt = tree.DecisionTreeClassifier()\n# #submit_dt = model_selection.GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, scoring = 'roc_auc', cv = cv_split)\n# #submit_dt.fit(data1[data1_x_bin], data1[Target])\n# #print('Best Parameters: ', submit_dt.best_params_) #Best Parameters:  {'criterion': 'gini', 'max_depth': 4, 'random_state': 0}\n# #data_val['Survived'] = submit_dt.predict(data_val[data1_x_bin])\n\n\n# #bagging w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77990\n# #submit_bc = ensemble.BaggingClassifier()\n# #submit_bc = model_selection.GridSearchCV(ensemble.BaggingClassifier(), param_grid= {'n_estimators':grid_n_estimator, 'max_samples': grid_ratio, 'oob_score': grid_bool, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\n# #submit_bc.fit(data1[data1_x_bin], data1[Target])\n# #print('Best Parameters: ', submit_bc.best_params_) #Best Parameters:  {'max_samples': 0.25, 'n_estimators': 500, 'oob_score': True, 'random_state': 0}\n# #data_val['Survived'] = submit_bc.predict(data_val[data1_x_bin])\n\n\n# #extra tree w/full dataset modeling submission score: defaults= 0.76555, tuned= 0.77990\n# #submit_etc = ensemble.ExtraTreesClassifier()\n# #submit_etc = model_selection.GridSearchCV(ensemble.ExtraTreesClassifier(), param_grid={'n_estimators': grid_n_estimator, 'criterion': grid_criterion, 'max_depth': grid_max_depth, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\n# #submit_etc.fit(data1[data1_x_bin], data1[Target])\n# #print('Best Parameters: ', submit_etc.best_params_) #Best Parameters:  {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'random_state': 0}\n# #data_val['Survived'] = submit_etc.predict(data_val[data1_x_bin])\n\n\n# #random foreset w/full dataset modeling submission score: defaults= 0.71291, tuned= 0.73205\n# #submit_rfc = ensemble.RandomForestClassifier()\n# #submit_rfc = model_selection.GridSearchCV(ensemble.RandomForestClassifier(), param_grid={'n_estimators': grid_n_estimator, 'criterion': grid_criterion, 'max_depth': grid_max_depth, 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\n# #submit_rfc.fit(data1[data1_x_bin], data1[Target])\n# #print('Best Parameters: ', submit_rfc.best_params_) #Best Parameters:  {'criterion': 'entropy', 'max_depth': 6, 'n_estimators': 100, 'random_state': 0}\n# #data_val['Survived'] = submit_rfc.predict(data_val[data1_x_bin])\n\n\n\n# #ada boosting w/full dataset modeling submission score: defaults= 0.74162, tuned= 0.75119\n# #submit_abc = ensemble.AdaBoostClassifier()\n# #submit_abc = model_selection.GridSearchCV(ensemble.AdaBoostClassifier(), param_grid={'n_estimators': grid_n_estimator, 'learning_rate': grid_ratio, 'algorithm': ['SAMME', 'SAMME.R'], 'random_state': grid_seed}, scoring = 'roc_auc', cv = cv_split)\n# #submit_abc.fit(data1[data1_x_bin], data1[Target])\n# #print('Best Parameters: ', submit_abc.best_params_) #Best Parameters:  {'algorithm': 'SAMME.R', 'learning_rate': 0.1, 'n_estimators': 300, 'random_state': 0}\n# #data_val['Survived'] = submit_abc.predict(data_val[data1_x_bin])\n\n\n# #gradient boosting w/full dataset modeling submission score: defaults= 0.75119, tuned= 0.77033\n# #submit_gbc = ensemble.GradientBoostingClassifier()\n# #submit_gbc = model_selection.GridSearchCV(ensemble.GradientBoostingClassifier(), param_grid={'learning_rate': grid_ratio, 'n_estimators': grid_n_estimator, 'max_depth': grid_max_depth, 'random_state':grid_seed}, scoring = 'roc_auc', cv = cv_split)\n# #submit_gbc.fit(data1[data1_x_bin], data1[Target])\n# #print('Best Parameters: ', submit_gbc.best_params_) #Best Parameters:  {'learning_rate': 0.25, 'max_depth': 2, 'n_estimators': 50, 'random_state': 0}\n# #data_val['Survived'] = submit_gbc.predict(data_val[data1_x_bin])\n\n# #extreme boosting w/full dataset modeling submission score: defaults= 0.73684, tuned= 0.77990\n# #submit_xgb = XGBClassifier()\n# #submit_xgb = model_selection.GridSearchCV(XGBClassifier(), param_grid= {'learning_rate': grid_learn, 'max_depth': [0,2,4,6,8,10], 'n_estimators': grid_n_estimator, 'seed': grid_seed}, scoring = 'roc_auc', cv = cv_split)\n# #submit_xgb.fit(data1[data1_x_bin], data1[Target])\n# #print('Best Parameters: ', submit_xgb.best_params_) #Best Parameters:  {'learning_rate': 0.01, 'max_depth': 4, 'n_estimators': 300, 'seed': 0}\n# #data_val['Survived'] = submit_xgb.predict(data_val[data1_x_bin])\n\n\n# #hard voting classifier w/full dataset modeling submission score: defaults= 0.75598, tuned = 0.77990\n# #data_val['Survived'] = vote_hard.predict(data_val[data1_x_bin])\n# data_val['Survived'] = grid_hard.predict(data_val[data1_x_bin])\n\n\n# #soft voting classifier w/full dataset modeling submission score: defaults= 0.73684, tuned = 0.74162\n# #data_val['Survived'] = vote_soft.predict(data_val[data1_x_bin])\n# #data_val['Survived'] = grid_soft.predict(data_val[data1_x_bin])\n\n\n#submit filesubmit = data_val[['PassengerId','Survived']]\n# submit.to_csv(\"../working/submit.csv\", index=False)\n\n# print('Validation Data Distribution: \\n', data_val['Survived'].value_counts(normalize = True))\n# submit.sample(10)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f9c3f4aeb98a0153f2a43e0dc0ba77f85a586b6f"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}