{"cells":[{"metadata":{"trusted":false,"_uuid":"e4f7016e052d583b3a8d291942fcc11f0b23ed73"},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nimport numpy\nfrom matplotlib import pyplot\nfrom sklearn.ensemble.partial_dependence import partial_dependence, plot_partial_dependence\nimport pandas as pd\nimport sklearn as sk\nfrom xgboost import XGBClassifier\nfrom sklearn.preprocessing.imputation import Imputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9951054ea2a4a260181e52c72fdb14e07d8b35ef"},"cell_type":"markdown","source":"# Load Dataset (training and test)"},{"metadata":{"trusted":false,"_uuid":"cd56d3272453d31834487686c74badfdf7835ae4"},"cell_type":"code","source":"dataset = pd.read_csv('../input/train.csv')\nvalidation_dataset = pd.read_csv('../input/test.csv')\ndataset.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"621057de012d186d7d0bc515f664e48ce4e2b800"},"cell_type":"markdown","source":"# Preprocess training dataset"},{"metadata":{"trusted":false,"_uuid":"c4ee62da8cbae2cd16067fdbca40dba9efc50642"},"cell_type":"code","source":"# helper function to map one-hot encoding\ndef try_iter(x):\n    try:\n        return sum([ord(z) for z in x])\n    except TypeError:\n        return float('NaN')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"7a77c66555ce2fbac52986befc1c9fd6ccba7937"},"cell_type":"code","source":"# feature engineering\n# extract person status (Mr., Ms., Dr.... etc.)\nstatus = [name.split(',')[1].split('.')[0].split()[-1] for name in dataset['Name'].values.tolist()]\n# extract person Cabin Letter (only first class ticket has separate cabin)\nroom = []\nfor letter in dataset['Cabin']:\n    if isinstance(letter, str):\n        room.append(letter[0])\n    else:\n        room.append('Unknown')\n# count family members\nfamily_members = dataset['SibSp'] + dataset['Parch']\n\n# drop columns that cannot be processed\ndata = dataset.drop(['Name', 'Ticket', 'Cabin'], axis=1)\n\ndata['Status'] = status\ndata['Room'] = room\ndata['Family_members'] = family_members\n\n# change data in object columns\n# two methods\n# Map string values to equivalent integer values\nfor col in data.columns.values.tolist():\n    if data[col].dtypes == 'O':\n        data[col] = list(map(lambda x: try_iter(x), data[col].values.tolist()))\ncolumns = data.columns[2:].tolist()\ndata = data[columns + ['Survived']]\n        \n# Create additional columns containing true or false (1 or 0) in rows where that value was\n# one-hot encoding (cathegorical data)\n## data = pd.get_dummies(data)\n\n# split training data into training and validation datasets\n# this step is important, avoid fitting whole training dataset anywhere and then splitting into train and valid\narray = data.values\nX = array[:, 0:-1].astype(float)\nY = array[:, -1]\nvalidation_size = 0.30\nseed = 7\nX_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=validation_size, random_state=seed)\nX_train = pd.DataFrame(data=X_train, columns=columns)\nX_validation = pd.DataFrame(data=X_validation, columns=columns)\n\n# handling missing values (NaN, Null)\n# creates additonal new columns based on calumns where missing data was (fill those columns with 1 and 0)\n# True where missing value was, False where not (1 or 0)\nmissing_columns = [col for col in X_train.columns if X_train[col].isnull().any()]\nfor col in missing_columns:\n    X_train[col + '_missing_data'] = X_train[col].isnull()\noriginal_data = X_train\n# fill missing values with mean values\nimputer = Imputer()\nX_train = pd.DataFrame(data=imputer.fit_transform(X_train))\nX_train.columns = original_data.columns\n# make one column indicating where wasmissing point, drop missing_columns\nX_train['missing_values'] = numpy.zeros((len(X_train),1))\nfor col in missing_columns:\n    X_train['missing_values'] += X_train[col + '_missing_data']\n    X_train = X_train.drop([col + '_missing_data'], axis=1)\nX_train['Age'] = X_train['Age'].values.round()\nX_train = X_train.values\n\n# validation dataset\nmissing_columns = [col for col in X_validation.columns if X_validation[col].isnull().any()]\nfor col in missing_columns:\n    X_validation[col + '_missing_data'] = X_validation[col].isnull()\noriginal_data = X_validation\n# fill missing values with mean values\nimputer = Imputer()\nX_validation = pd.DataFrame(data=imputer.fit_transform(X_validation))\nX_validation.columns = original_data.columns\n# make one column indicating where wasmissing point, drop missing_columns\nX_validation['missing_values'] = numpy.zeros((len(X_validation),1))\nfor col in missing_columns:\n    X_validation['missing_values'] += X_validation[col + '_missing_data']\n    X_validation = X_validation.drop([col + '_missing_data'], axis=1)\nX_validation['Age'] = X_validation['Age'].values.round()\nX_validation = X_validation.values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ce227deaf3404e03422296b8ae676b0576419edd"},"cell_type":"markdown","source":"# Preprocess test dataset"},{"metadata":{"trusted":false,"_uuid":"6ed897d24e18cba12d5264230fc7473a47b5e933"},"cell_type":"code","source":"# feature engineering\n# extract person status (Mr., Ms., Dr.... etc.)\nstatus = [name.split(',')[1].split('.')[0].split()[-1] for name in validation_dataset['Name'].values.tolist()]\n# extract person Cabin Letter (only first class ticket has separate cabin)\nroom = []\nfor letter in validation_dataset['Cabin']:\n    if isinstance(letter, str):\n        room.append(letter[0])\n    else:\n        room.append('Unknown')\n# count family members\nfamily_members = validation_dataset['SibSp'] + validation_dataset['Parch'] \n\ntest_data = validation_dataset.drop(['Name', 'Ticket', 'Cabin'], axis=1)\ntest_data['Status'] = status\ntest_data['Room'] = room\ntest_data['Family_members'] = family_members\n\nfor col in test_data.columns.values.tolist():\n    if test_data[col].dtypes == 'O':\n        test_data[col] = list(map(lambda x: try_iter(x), test_data[col].values.tolist()))\n## test_data = pd.get_dummies(test_data)\n\n# only for further result saving\npass_id = test_data[['PassengerId']]\n\noriginal_data = test_data\nmissing_columns = [col for col in test_data.columns if test_data[col].isnull().any()]\nfor col in missing_columns:\n    test_data[col + '_missing_data'] = test_data[col].isnull()\nimputer = Imputer()\ntest_data = pd.DataFrame(imputer.fit_transform(test_data))\ntest_data.columns = original_data.columns\ntest_data['missing_values'] = numpy.zeros((len(test_data),1))\nfor col in missing_columns:\n    test_data['missing_values'] += test_data[col + '_missing_data']\n    test_data = test_data.drop([col + '_missing_data'], axis=1)\ncolumns = test_data.columns[1::].tolist()\ntest_data = test_data[columns]\ntest_data['Age'] = test_data['Age'].values.round()\ntest_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"95686417ea6108b12a809c97455f652efa6a0018"},"cell_type":"markdown","source":"# Prepare test dataset"},{"metadata":{"trusted":false,"_uuid":"f7e175a6b73f984bff5ce12e972a47b0035e94b4"},"cell_type":"code","source":"test_X = test_data.values[:, :].astype(float)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4c082afeeae00da492f9950f38177a246ab41d33"},"cell_type":"markdown","source":"# Prepare few ML algorithms for learning"},{"metadata":{"trusted":false,"_uuid":"5596df4340a96c0fc2588101e5fbeb5c8dcdec39"},"cell_type":"code","source":"num_fold = 10\nseed = 7\nscoring = 'accuracy'\npipelines = []\n# you can choose which type of algorithms you want to use,\n# better to check correlation between their predictions later and modify your choice\n# if we are using all algorithms we can score 79% to 80% accuracy\npipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()), ('LR', LogisticRegression())])))\npipelines.append(('ScaledLDA', Pipeline([('Scaler', StandardScaler()), ('LDA', LinearDiscriminantAnalysis())])))\npipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()), ('KNN', KNeighborsClassifier())])))\npipelines.append(('ScaledCART', Pipeline([('Scaler', StandardScaler()), ('CART', DecisionTreeClassifier())])))\npipelines.append(('ScaledNB', Pipeline([('Scaler', StandardScaler()), ('NB', GaussianNB())])))\npipelines.append(('ScaledSVM', Pipeline([('Scaler', StandardScaler()), ('SVM', SVC())])))\npipelines.append(('AB', AdaBoostClassifier()))\npipelines.append(('GB', GradientBoostingClassifier()))\npipelines.append(('RF', RandomForestClassifier()))\npipelines.append(('ET', ExtraTreesClassifier()))\npipelines.append(('XGB', XGBClassifier()))\nresults = []\nnames = []\n# k-fld test, produce extimated score per algorithm based on training and validation datasets\nfor name, model in pipelines:\n    kfold = KFold(n_splits=num_fold, random_state=seed)\n    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"{}: {} ({})\".format(name, cv_results.mean(), cv_results.std())\n    print(msg)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5415025a2e771f9bef4726fb41e094a1a7a845a8"},"cell_type":"markdown","source":"# Visualize K-fold veryfication for each model, use boxplot"},{"metadata":{"trusted":false,"_uuid":"652f5e69eae05413f85a5c44931f9fbd0e61a9ee"},"cell_type":"code","source":"# usage of 25,50,75 percentile and min/max values from each distribution of k-fold result per algorithm\nfig = pyplot.figure(figsize=(15,6))\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\npyplot.boxplot(results)\nax.set_xticklabels(names)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"655f92a12a84a644b96e8cc84c5ca68aff77d89a"},"cell_type":"markdown","source":"# Train each model based on training dataset, predict based on validation and test dataset (results store in two new DataFrames)\n\nTrain on: training dataset (bigger part of train.csv)\n\nPredict on: validation dataset (smaller part of train.csv) and test dataset\n\nProduce two new DataFrames: \n\n    (new_train_dataset - contains results after predicting based on validation dataset\n                            \\+ Survived column from validation dataset)\n                            \n    (new_test_dataset - contains results after predicting based on test dataset)"},{"metadata":{"trusted":false,"_uuid":"11e517568455d715e58e5758b9dd21ecb342b54e"},"cell_type":"code","source":"importance = []\nplots = []\nnew_train_data = pd.DataFrame(data=Y_validation, columns=['Survived'])\nnew_test_data = pd.DataFrame(data=test_data['Age'], columns=['Age'])\n# scaling is only needed when we are using based algorithms\nscaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nfor name, model in pipelines:\n    if 'Scaled' not in name:\n        model.fit(X_train, Y_train)\n        # stores feature importances per model\n        importance.append((name, model.feature_importances_))\n        # plot partial dependencies for GradientBoosting (in this version of sklearn GB only available)\n        # we can look at particular feature dependencies\n        if name == 'GB':\n            plot_partial_dependence(model, X=X_train, features=[2], feature_names=data.columns.values.tolist(), grid_resolution=10)\n        predictions = model.predict(X_validation)\n        new_train_data[name] = predictions\n\n        predictions = model.predict(test_X)\n        new_test_data[name] = predictions\n    else:\n        model.fit(rescaledX, Y_train)\n        # stores feature importances per model\n        try:\n            importance.append((name, model.steps[1][1].feature_importances_))\n        except:\n            pass\n        rescaledValidX = scaler.transform(X_validation)\n        predictions = model.predict(rescaledValidX)\n        new_train_data[name] = predictions\n\n        rescaledTestX = scaler.transform(test_X)\n        predictions = model.predict(rescaledTestX)\n        new_test_data[name] = predictions\nnew_test_data = new_test_data.drop(['Age'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"45829aa9232903cc1b89f7882e11a7b7d8619f88"},"cell_type":"markdown","source":"# Plot feature importance per used algorithm"},{"metadata":{"trusted":false,"_uuid":"a744e3a6c78913b94ff0bb4764f377da60949a43"},"cell_type":"code","source":"cols = test_data.columns.values\n# Create a dataframe with features\nalgs = {'features': cols}\nfor i, imp in enumerate(importance):\n    algs['{} feature importances'.format(imp[0])] = imp[1]\nfeature_dataframe = pd.DataFrame(algs)\nfor model in importance:\n    # Scatter plot \n    trace = go.Scatter(\n        y = feature_dataframe['{} feature importances'.format(model[0])].values,\n        x = feature_dataframe['features'].values,\n        mode='markers',\n        marker=dict(\n            sizemode = 'diameter',\n            sizeref = 1,\n            size = 25,\n            color = feature_dataframe['{} feature importances'.format(model[0])].values,\n            colorscale='Portland',\n            showscale=True\n        ),\n        text = feature_dataframe['features'].values\n    )\n    data = [trace]\n\n    layout= go.Layout(\n        autosize= True,\n        title= '{} feature importances'.format(model[0]),\n        hovermode= 'closest',\n        yaxis=dict(\n            title= 'Feature Importance',\n            ticklen= 5,\n            gridwidth= 2\n        ),\n        showlegend= False\n    )\n    fig = go.Figure(data=data, layout=layout)\n    py.iplot(fig,filename='scatter2010')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"0341ecde93cd165202e76cf4856758882bd5c2aa"},"cell_type":"code","source":"# calculate mean value of each particular feature importance\nfeature_dataframe['mean'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise\nfeature_dataframe.head(3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"25b31e784b22e4718b2520e98567e48dc90b8a97"},"cell_type":"markdown","source":"# Plot barplot of each feature avarage importance"},{"metadata":{"trusted":false,"_uuid":"6abd16c97b432080b4a45563293ec8d743c9dbe6"},"cell_type":"code","source":"y = feature_dataframe['mean'].values\nx = feature_dataframe['features'].values\ndata = [go.Bar(\n            x= x,\n             y= y,\n            width = 0.5,\n            marker=dict(\n               color = feature_dataframe['mean'].values,\n            colorscale='Portland',\n            showscale=True,\n            reversescale = False\n            ),\n            opacity=0.6\n        )]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Barplots of Mean Feature Importance',\n    hovermode= 'closest',\n    yaxis=dict(\n        title= 'Feature Importance',\n        ticklen= 5,\n        gridwidth= 2\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='bar-direct-labels')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"91b2d02ca21b9af67f6b43039e1e15f482f486a0"},"cell_type":"markdown","source":"# Create heatmap for every used algorithm (we now can see correlations between algorithm predictions)"},{"metadata":{"trusted":false,"_uuid":"9bfd99bf20c0e81c9697214d026cdd7bb82cef98"},"cell_type":"code","source":"data = [\n    go.Heatmap(\n        z= new_train_data[new_train_data.columns.values.tolist()[1:]].astype(float).corr().values ,\n        x= new_train_data.columns.values.tolist()[1:],\n        y= new_train_data.columns.values.tolist()[1:],\n          colorscale='Viridis',\n            showscale=True,\n            reversescale = True\n    )\n]\npy.iplot(data, filename='labelled-heatmap')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e7b9338e3f7a8953cadb86d7264830022520a235"},"cell_type":"markdown","source":"# Train meta classifier based on new training dataset, predict based on new test dataset\n\nRandomForest as a meta-classifier (default parameters)"},{"metadata":{"trusted":false,"_uuid":"5939ab160d15770d8a4e85afca586d544489f0c0"},"cell_type":"code","source":"array = new_train_data.values\nX = array[:, 0:-1].astype(float)\nY = array[:, -1]\nscaler = StandardScaler().fit(X)\nrescaledX = scaler.transform(X)\nmodel = RandomForestClassifier()\nmodel.fit(rescaledX, Y)\nval = model.feature_importances_\nt_X = new_test_data.values[:, :].astype(float)\nrescaledTestX = scaler.transform(t_X)\npredictions = model.predict(rescaledTestX)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ccf75a545a8b70265cf70e20fef3a7cd506be823"},"cell_type":"markdown","source":"Plot feature importance from first-level algorithms predictions"},{"metadata":{"trusted":false,"_uuid":"e419fcfcb3df5b49da58f6c347de500dcd30bb29"},"cell_type":"code","source":"cols = new_train_data.columns.values\n# Create a dataframe with features\nfeature_dataframe = pd.DataFrame( {'features': cols,\n                                   'Random Forest feature importances': val[1]\n                                  })\n# Scatter plot \ntrace = go.Scatter(\n    y = feature_dataframe['Random Forest feature importances'].values,\n    x = feature_dataframe['features'].values,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n        color = feature_dataframe['Random Forest feature importances'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = feature_dataframe['features'].values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Random Forest feature importances',\n    hovermode= 'closest',\n    yaxis=dict(\n        title= 'Feature Importance',\n        ticklen= 5,\n        gridwidth= 2\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c3c79d07041cd8f4fc3948bcd9f36ef3f67eb16b"},"cell_type":"markdown","source":"Save results to file"},{"metadata":{"trusted":false,"_uuid":"eb2a4d504aa1723893e50214b2cdeb2d866be00d"},"cell_type":"code","source":"temp_p = []\nfor val in predictions:\n    temp_p.append(int(val))\npredictionsDF = pd.DataFrame({'Survived': temp_p})\ntemp = []\nfor val in pass_id.values:\n    temp.append(val[0])\npass_idDF = pd.DataFrame({'PassengerId': temp})\nresult = pd.concat([pass_idDF, predictionsDF], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"fef51012c648cc97ab4f4c9c2264f7b61013e84d"},"cell_type":"code","source":"print(result)\nresult.to_csv('result.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}