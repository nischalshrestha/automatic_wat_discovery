{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "129eb2ee-c93f-264d-3759-f37dadca752c"
      },
      "source": [
        "I'm new to Keras and neural networks in general, so I thought I would try out Keras for this problem. Neural Networks probably aren't a good choice for this particular problem, going through the discussions it looks like there aren't any solutions that get over 80% accuracy. I'm slowly updating this, my goal is to get 90% accuracy with this solution, please let me know if you have any suggestions for making this model better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "4e1609f3-d966-8b5f-10fd-b4dcc03c2932"
      },
      "source": [
        "## Setup some potential models ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "21645771-c670-c0fd-c23e-be37a3b51c63"
      },
      "source": [
        "I have no idea what form of model I should use, so I'm making some methods that use various architectures to see if any of them give better performance. So far, the simple model has performed the best, and the multiple layers model is close behind. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "038781c9-0b3e-3151-150a-7d60e3f39913"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "\n",
        "# Simpel model that only has one Dense input layer and one output layer. \n",
        "def create_model_simple(input_size):\n",
        "    model = Sequential([\n",
        "        Dense(512, input_dim=input_size),\n",
        "        Activation('relu'),\n",
        "        Dense(1),\n",
        "        Activation('sigmoid'),\n",
        "    ])\n",
        "\n",
        "    # For a binary classification problem\n",
        "    model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Slightly more complex model with 1 hidden layer\n",
        "def create_model_multiple_layers(input_size):\n",
        "    model = Sequential([\n",
        "        Dense(512, input_dim=input_size),\n",
        "        Activation('relu'),\n",
        "        Dense(128),\n",
        "        Activation('relu'),\n",
        "        Dense(1),\n",
        "        Activation('sigmoid'),\n",
        "    ])\n",
        "\n",
        "    # For a binary classification problem\n",
        "    model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Simple model with a dropout layer thrown in\n",
        "def create_model_dropout(input_size):\n",
        "    model = Sequential([\n",
        "        Dense(512, input_dim=input_size),\n",
        "        Activation('relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(1),\n",
        "        Activation('sigmoid'),\n",
        "    ])\n",
        "\n",
        "    # For a binary classification problem\n",
        "    model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Slightly more complex model with 2 hidden layers and a dropout layer\n",
        "def create_model_complex(input_size):\n",
        "    model = Sequential([\n",
        "        Dense(512, input_dim=input_size),\n",
        "        Activation('relu'),\n",
        "        Dense(256),\n",
        "        Activation('relu'),\n",
        "        Dense(128),\n",
        "        Dropout(0.5),\n",
        "        Activation('relu'),\n",
        "        Dense(64),\n",
        "        Dropout(0.5),\n",
        "        Activation('relu'),\n",
        "        Dense(1),\n",
        "        Activation('sigmoid'),\n",
        "    ])\n",
        "\n",
        "    # For a binary classification problem\n",
        "    model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "aeb38dc4-3e46-38b4-e622-c5b25b1070eb"
      },
      "source": [
        "## Format the data ##\n",
        "I'm using some tips found in [this helpful kernel](https://www.kaggle.com/omarelgabry/titanic/a-journey-through-titanic). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e323c7f5-3d08-1887-5548-4ab4e2cce523"
      },
      "outputs": [],
      "source": [
        "def format_data(dataframe):\n",
        "    # drop unnecessary columns\n",
        "    # PassengerId is always different for each passenger, not helpful\n",
        "    # Name is different for each passenger, not helpful (maybe last names would be helpful?)\n",
        "    # Ticket information is different for each passenger, not helpful\n",
        "    # Embarked does not have any strong correlation for survival rate. \n",
        "    # Cabin data is very sparse, not helpful\n",
        "    dataframe = dataframe.drop(['PassengerId','Name','Ticket','Embarked','Cabin'], axis=1)\n",
        "\n",
        "    # Instead of having two columns Parch & SibSp, \n",
        "    # we can have only one column represent if the passenger had any family member aboard or not,\n",
        "    # Meaning, if having any family member(whether parent, brother, ...etc) will increase chances of Survival or not.\n",
        "    dataframe['Family'] = dataframe[\"Parch\"] + dataframe[\"SibSp\"]\n",
        "    dataframe['Family'].loc[dataframe['Family'] > 0] = 1\n",
        "    dataframe['Family'].loc[dataframe['Family'] == 0] = 0\n",
        "\n",
        "    # drop Parch & SibSp\n",
        "    dataframe = dataframe.drop(['SibSp','Parch'], axis=1)\n",
        "\n",
        "    # get average, std, and number of NaN values in titanic_df\n",
        "    average_age_titanic   = dataframe[\"Age\"].mean()\n",
        "    std_age_titanic       = dataframe[\"Age\"].std()\n",
        "    count_nan_age_titanic = dataframe[\"Age\"].isnull().sum()\n",
        "\n",
        "    # generate random numbers between (mean - std) & (mean + std)\n",
        "    rand_1 = np.random.randint(average_age_titanic - std_age_titanic, average_age_titanic + std_age_titanic, \n",
        "                               size = count_nan_age_titanic)\n",
        "\n",
        "    dataframe[\"Age\"][np.isnan(dataframe[\"Age\"])] = rand_1\n",
        "    \n",
        "    return dataframe\n",
        "\n",
        "def string_to_numbers(data, dataframe, encoder):\n",
        "    # assign labels for all the non-numeric fields\n",
        "    headings = list(dataframe.columns.values)\n",
        "    for heading_index in range(len(headings)):\n",
        "        dataframe_type = dataframe[headings[heading_index]].dtype\n",
        "        column = data[:,heading_index]\n",
        "        if dataframe_type == np.int64 or dataframe_type == np.float64:\n",
        "            data[:,heading_index] = column.astype(float)\n",
        "        else :\n",
        "            data[:,heading_index] = encoder.fit(column).transform(column).astype(float)\n",
        "            \n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bda9e026-039b-9797-e8b4-9469c7bfb997"
      },
      "outputs": [],
      "source": [
        "import pandas\n",
        "import numpy as np\n",
        "import sklearn.preprocessing as preprocessing\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# load dataset\n",
        "titanic_df = pandas.read_csv('../input/train.csv')\n",
        "# format the data\n",
        "titanic_df = format_data(titanic_df)\n",
        "\n",
        "# attempt to remove all of the \"outliers\", things like high class female passengers who died (likely to live) or \n",
        "# low class males surviving (likely to die)\n",
        "#titanic_df = titanic_df.drop(titanic_df[(titanic_df[\"Pclass\"] == 1) & (titanic_df[\"Survived\"] == 0)].index)\n",
        "#titanic_df = titanic_df.drop(titanic_df[(titanic_df[\"Pclass\"] == 3) & (titanic_df[\"Survived\"] == 1)].index)\n",
        "\n",
        "# pull out the correct answers (survived or not)\n",
        "Y_train = titanic_df[\"Survived\"].values\n",
        "# drop the survived column for the training data\n",
        "titanic_df = titanic_df.drop(\"Survived\",axis=1)\n",
        "X_train = titanic_df.values\n",
        "\n",
        "# assign labels for all the non-numeric fields\n",
        "encoder = LabelEncoder()\n",
        "X_train = string_to_numbers(X_train, titanic_df, encoder)\n",
        "        \n",
        "# Extract a small validation set\n",
        "validation_set_size = 200\n",
        "random_indices = np.random.randint(low=0,high=len(X_train)-1,size=validation_set_size)\n",
        "X_valid = X_train[random_indices]\n",
        "Y_valid = Y_train[random_indices]\n",
        "X_train = np.delete(X_train, random_indices, axis=0)\n",
        "Y_train = np.delete(Y_train, random_indices, axis=0)\n",
        "               \n",
        "# normalize the data\n",
        "preprocessing.scale(X_train)\n",
        "preprocessing.scale(X_valid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "16d337f7-fbaf-99ba-7d16-2d1ca5b863a0"
      },
      "source": [
        "## Train the model ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "172fc06d-f6a1-9f4e-3f01-450e3bdeb533"
      },
      "outputs": [],
      "source": [
        "# Train the model, iterating on the data in batches of 64 samples\n",
        "model = create_model_simple(len(X_train[0]))\n",
        "#model = create_model_multiple_layers(len(X_train[0]))\n",
        "#model = create_model_dropout(len(X_train[0]))\n",
        "#model = create_model_complex(len(X_train[0]))\n",
        "\n",
        "model.optimizer.lr = 0.01\n",
        "model.fit(X_train, Y_train, epochs=100, batch_size=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b5a13e55-26f9-7cdb-6901-a7cca3e19dc0"
      },
      "source": [
        "## Train XGBoost ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a96336e7-aea1-c7dc-3c02-53498a1267c3"
      },
      "outputs": [],
      "source": [
        "F_train = model.predict(X_train, batch_size=64)\n",
        "F_val = model.predict(X_valid, batch_size=64)\n",
        "\n",
        "dTrain = xgb.DMatrix(F_train, label=Y_train)\n",
        "dVal = xgb.DMatrix(F_val, label=Y_valid)\n",
        "\n",
        "xgb_params = {\n",
        "    'objective': 'binary:logistic',\n",
        "    'booster': 'gbtree',\n",
        "    'eval_metric': 'logloss',\n",
        "    'eta': 0.1, \n",
        "    'max_depth': 9,\n",
        "    'subsample': 0.9,\n",
        "    'colsample_bytree': 1 / F_train.shape[1]**0.5,\n",
        "    'min_child_weight': 5,\n",
        "    'silent': 1\n",
        "}\n",
        "best = xgb.train(xgb_params, dTrain, 1000,  [(dTrain,'train'), (dVal,'val')], \n",
        "                verbose_eval=10, early_stopping_rounds=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "f309524a-caf9-424c-d02e-534fab99b654"
      },
      "source": [
        "## Visualize Predictions on Validation Set ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "28cca234-c644-eb2b-b8f3-7032f8e84ca0"
      },
      "outputs": [],
      "source": [
        "# run predictions on our validation data (the small subset we removed before training)\n",
        "train_preds = best.predict(dVal, ntree_limit=best.best_ntree_limit)\n",
        "rounded_preds = np.round(train_preds).astype(int).flatten()\n",
        "correct_preds = np.where(rounded_preds==Y_valid)[0]\n",
        "print(\"Accuracy: {}%\".format(float(len(correct_preds))/float(len(rounded_preds))*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "0193eaec-c1b1-e7ce-2295-afc9c3c8c6ee"
      },
      "source": [
        "## Get ready for rendering plots ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "70bc1834-226a-6c6b-4ea2-ee8db3367f5b"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def render_value_frequency(dataframe, title):\n",
        "    fig, ax = plt.subplots()\n",
        "    dataframe.value_counts().plot(ax=ax, title=title, kind='bar')\n",
        "    plt.show()\n",
        "    \n",
        "def render_plots(dataframes):\n",
        "    headings = dataframes.columns.values\n",
        "    for heading in headings:\n",
        "        data_type = dataframes[heading].dtype\n",
        "        if data_type == np.int64 or data_type == np.float64:\n",
        "            dataframes[heading].plot(kind='hist',title=heading)\n",
        "            plt.show()\n",
        "        else:\n",
        "            render_value_frequency(dataframes[heading],heading)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "34eff5c2-1a04-98a3-7432-ca0381cab35d"
      },
      "source": [
        "## Correct ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bae51ee3-840e-868d-503f-7417d1943909"
      },
      "outputs": [],
      "source": [
        "correct = np.where(rounded_preds==Y_valid)[0]\n",
        "print(\"Found {} correct labels\".format(len(correct)))\n",
        "render_plots(titanic_df.iloc[correct])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "1551d5ce-14e6-876f-80fa-f4a98ccc1671"
      },
      "source": [
        "## Incorrect ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6b7efe3e-44e1-5aec-d911-7e0e6666584e"
      },
      "outputs": [],
      "source": [
        "incorrect = np.where(rounded_preds!=Y_valid)[0]\n",
        "print(\"Found {} incorrect labels\".format(len(incorrect)))\n",
        "render_plots(titanic_df.iloc[incorrect])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "de712615-fc40-4360-bde9-76be7e5d20f8"
      },
      "source": [
        "## Confident Survived and Survived ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6c65570e-78d6-3e8f-0c0e-b90c1a73bd0f"
      },
      "outputs": [],
      "source": [
        "confident_survived_correct = np.where((rounded_preds==1) & (rounded_preds==Y_valid))[0]\n",
        "print(\"Found {} confident correct survived labels\".format(len(confident_survived_correct)))\n",
        "render_plots(titanic_df.iloc[confident_survived_correct])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "e014d8b9-c01d-25b2-a857-974a63bc64c2"
      },
      "source": [
        "## Confident Died and Died ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9082fd39-2f90-c4e5-71e9-955944dfddb8"
      },
      "outputs": [],
      "source": [
        "confident_died_correct = np.where((rounded_preds==0) & (rounded_preds==Y_valid))[0]\n",
        "print(\"Found {} confident correct died labels\".format(len(confident_died_correct)))\n",
        "render_plots(titanic_df.iloc[confident_died_correct])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "8d082ce4-fa13-4da1-4edd-3875bd24bc98"
      },
      "source": [
        "## Confident Survived and Died ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "954fa125-ae40-272c-34a0-529b3e58627e"
      },
      "outputs": [],
      "source": [
        "confident_survived_incorrect = np.where((rounded_preds==1) & (rounded_preds!=Y_valid))[0]\n",
        "print(\"Found {} confident incorrect survived labels\".format(len(confident_survived_incorrect)))\n",
        "render_plots(titanic_df.iloc[confident_survived_incorrect])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "7d0d56e5-f6a2-7b09-be34-e2060e912e68"
      },
      "source": [
        "## Confident Died and Survived ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "51ce42ca-e7d1-f380-dc63-74a4f0c76e56"
      },
      "outputs": [],
      "source": [
        "confident_died_incorrect = np.where((rounded_preds==0) & (rounded_preds!=Y_valid))[0]\n",
        "print(\"Found {} confident incorrect died labels\".format(len(confident_died_incorrect)))\n",
        "render_plots(titanic_df.iloc[confident_died_incorrect])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9f9e0d14-a63e-37c7-381f-1f78e579bebe"
      },
      "source": [
        "## Uncertain ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "11ec3a70-ecb0-530e-42e1-0eebe6f0a097"
      },
      "outputs": [],
      "source": [
        "most_uncertain = np.argsort(np.abs(train_preds.flatten()-0.5))[:10]\n",
        "render_plots(titanic_df.iloc[most_uncertain])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b36a3d58-749d-8568-f5d4-7277510cbefd"
      },
      "source": [
        "## Test the model ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5efbcc3e-c41d-d337-0305-c143305f5a9e"
      },
      "outputs": [],
      "source": [
        "# load test dataset\n",
        "test_df = pandas.read_csv('../input/test.csv')\n",
        "# get the passenger IDs\n",
        "passenger_ids = test_df['PassengerId'].values\n",
        "# format the data\n",
        "test_df = format_data(test_df)\n",
        "\n",
        "# only for test_df, since there is a missing \"Fare\" value\n",
        "test_df[\"Fare\"].fillna(test_df[\"Fare\"].median(), inplace=True)\n",
        "\n",
        "X_test = test_df.values\n",
        "\n",
        "# assign labels for all the non-numeric fields\n",
        "encoder = LabelEncoder()\n",
        "X_test = string_to_numbers(X_test, test_df, encoder)\n",
        "               \n",
        "#normalize the data\n",
        "#preprocessing.normalize(dataset)\n",
        "preprocessing.scale(X_test)\n",
        "\n",
        "F_test = model.predict(X_test, batch_size=64)\n",
        "\n",
        "dTest = xgb.DMatrix(F_test)\n",
        "\n",
        "preds = best.predict(dTest, ntree_limit=best.best_ntree_limit)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "79466082-9355-8d89-048e-0f7f9421cb1f"
      },
      "source": [
        "## Output the submission ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "02682c3e-68a6-e806-3f19-4e1438281cee"
      },
      "outputs": [],
      "source": [
        "preds = np.round(preds).astype(int).flatten()\n",
        "    \n",
        "submission = pandas.DataFrame({\n",
        "        \"PassengerId\": passenger_ids,\n",
        "        \"Survived\": preds\n",
        "    })\n",
        "submission.to_csv('titanic.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "58fdae2d-9f16-a775-7344-73ee73e1eec1"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}