{"cells":[{"metadata":{"_cell_guid":"7e65685c-5d7b-4b32-a423-5f77fd9d024a","_uuid":"8ffb1147380cd4b1cc50b6fad1a2a464e1471ba3"},"cell_type":"markdown","source":"**INTRODUCTION**\n\nThis is my first attempt at writing a Machine learning code using existing libraries independently. This kernel is suitable for begineers to learn the different portions of a typical Machine learning Pipeline. In this kernel i will explicitly annotate all the different steps and provide reasons for choices."},{"metadata":{"_cell_guid":"30be9a24-c126-457f-a131-c89d6ab4b1a7","_uuid":"fccd6f59c5a4afe675cdeb7cfbdc29f503467ce7"},"cell_type":"markdown","source":"**ML algorithm pipeline**\n1. Importing modules, loading/unloading data\n2. Data Cleansing & Data Exploration\n    * Filling in NaN values\n    *Binning values into categories\n    *Scaling features\n3.  Feature Engineering\n4. Building the ML model\n5. Assessing the Model\n6. Predicting results\n7. Improvement Areas"},{"metadata":{"_cell_guid":"60aa35b3-bd2c-422c-a309-fe4606fb5ad4","_uuid":"933984ebf9228b26a335ce8d420c026c8d1ca1b0","trusted":true},"cell_type":"code","source":"'''IMPORTING MODULES/LOADING/UNLOADING DATA'''\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.\n\ntrain = pd.read_csv('../input/titanic/train.csv')\ntest = pd.read_csv('../input/titanic/test.csv')\n\n'''DATA CLEANSING/DATA EXPLORATION'''\n\nprint(train.Embarked.value_counts())\nix  = train.Embarked.isnull().nonzero()\ntrain.loc[ix[0],\"Embarked\"] = 'S' #since the number of S is the most we will fill the NaN values with S\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import Imputer,LabelBinarizer, StandardScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n\n\n#function deletes column according to what you feed in\nclass data_cleanse(BaseEstimator,TransformerMixin):\n    def __init__(self,string_param='None'):\n        self.string_param = string_param\n    def fit(self,X,y=None):\n        try:\n            type(self.string_param) == list\n        except TypeError:\n            print('Please input the string_params as a list')\n        return self\n    def transform(self,X):\n        X = X.drop(self.string_param,axis=1)\n        return X\n\n#this code does a one hot encoding and adds into the training data\ndef one_hot_into_df (df,column_name=None):\n\n    def zero_to_one_array(array):\n        intermediate= np.equal(array, np.zeros(len(df_encode)).reshape(-1,1))\n        return 1*intermediate\n\n    #from IPython.core.debugger import set_trace\n    #set_trace()\n    if type(df) != pd.DataFrame:\n        return 'Only datatypes of dataframe is allowed'\n    if type(column_name) != str:\n        return \"only column_name of type str is allowed\"\n    \n    from sklearn.preprocessing import LabelBinarizer \n    encoder = LabelBinarizer()\n    df_encode = encoder.fit_transform(df[column_name])\n    uniq_col= sorted(df[column_name].unique())\n    num_uniq_col = len(uniq_col)\n    \n    #add the binarized into he training data\n    if num_uniq_col>2:\n        for i in range(num_uniq_col):\n            df[uniq_col[i]]= df_encode[:,i]\n\n    elif num_uniq_col==2:\n        df[uniq_col[0]]=zero_to_one_array(df_encode) \n        df[uniq_col[1]]=df_encode\n        \n        \n    '''\n    Embarked_encode = pd.DataFrame(df)\n    for x in [uniq_col]:\n        df[x] = Embarked_encode[x]'''\n        \n    df= df.drop([column_name],axis=1)\n    return df\n\nclass one_hot_encode(BaseEstimator,TransformerMixin):\n    def __init__(self,column_name=None):\n        self.column_name = column_name\n    def fit(self,X,y=None):\n        try:\n            type(self.column_name) == str\n        except TypeError:\n            print('Please input the column name as a str')\n        return self\n    def transform(self,X):\n        for term in self.column_name:\n            X=one_hot_into_df (X,term)\n        return X\n\nestimator = Pipeline([('data_cleanse',data_cleanse(['Cabin','Ticket','Name'])),\n    ('one_hot_encode',one_hot_encode(['Embarked','Sex'])),\n    ('imputer',Imputer(strategy=\"median\")) #imputer returns a numpy array\n                     ])\n\nresult = estimator.fit_transform(train)\n\n#imput the headings into the numpy array(imputer function converts it into an numpy array)\nheadings = train.columns.values\nheadings = np.delete(headings,[3,4,8,10,11],0)\nheadings = np.append(headings,[\"C\",'Q','S',\"female\",'male'])\nresult_df =pd.DataFrame(result)\nresult_df.columns = headings\n\n'''\nfrom sklearn.preprocessing import StandardScaler\nfor x in ['Age','Fare']:\n    #import pdb;pdb.set_trace()\n    scaled = StandardScaler().fit_transform(result_df[x].reshape(-1,1))\n    result_df[x] = scaled\n'''\n\nresult_df.drop('PassengerId',axis=1)","execution_count":1,"outputs":[]},{"metadata":{"_cell_guid":"84181e63-47cf-4269-bcfa-bef0812e6ef6","_uuid":"77b48746b1b5d8e35110ae318a128638ac9c9ddb"},"cell_type":"markdown","source":"**> Feature Engineering**\n\n\nNow we will look at the features and try to hand select some interesting features\n"},{"metadata":{"_cell_guid":"1a5c4624-a4b8-41c8-8a35-04386a769e23","_uuid":"9ee94a488e015f39a0fffb7e98f4fc5d6864dc91","trusted":true},"cell_type":"code","source":"train.Survived.value_counts()\n#the average survival rate is 0.3838 , any combination of results which produce a higher \n#survival rate than that is worth looking at\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.figure(1)\nplt.subplot(1,2,1)\nsns.set_style(\"whitegrid\")\ng_1 = sns.countplot(x=train.Pclass,hue=train.Survived,palette=\"Set3\")\n\nplt.subplot(1,2,2)\ng_2 = sns.countplot(x=train.Parch,hue=train.Survived,palette=\"Set3\")\n\n\n#assuming rich female woman will survive etc,\ncombinator = {\"Pclass\":[1],'Sex':['female']}\nric_fem = train.isin(combinator).sum(axis=1)\nix = (ric_fem == 2).nonzero()\ntrain.iloc[ix]\n\nresult_df.drop(\"Survived\",axis=1)","execution_count":2,"outputs":[]},{"metadata":{"_cell_guid":"039e700f-0520-422e-94bc-c2d4d75d9e84","_uuid":"607bb9380956442adcbb8e924a60ca277cdbed7d","collapsed":true},"cell_type":"markdown","source":"Building the model\n"},{"metadata":{"_cell_guid":"2255a891-c538-4650-be60-441cc23e1ac1","_uuid":"8cd71b8a8e3293da85c8bf5a0c4ac281a5068fbb","collapsed":true,"trusted":true},"cell_type":"code","source":"#splitting the train dataset into train and cv\nfrom sklearn.model_selection import train_test_split\nx_df = result_df.drop('Survived',axis=1)\ny_df = result_df.Survived\n\n#x_train, x_cv , y_train , y_cv = train_test_split(x,y,test_size=0.3)\n\n\n\nfrom sklearn import linear_model\nfrom sklearn.model_selection import GridSearchCV\nfrom time import time\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import uniform as sp_rand\nimport scipy.stats as sp","execution_count":3,"outputs":[]},{"metadata":{"_cell_guid":"ef70cc19-7b33-447a-a55e-91d6ed6baa94","_uuid":"2cf1e32fc7391768323b519d5e0e5763a91c246f","collapsed":true,"trusted":true},"cell_type":"code","source":"# Utility function to report best scores\ndef report(results, n_top=20):\n    for i in range(1, n_top + 1):\n        candidates = np.flatnonzero(results['rank_test_score'] == i)\n        for candidate in candidates:\n            print(\"Model with rank: {0}\".format(i))\n            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n                  results['mean_test_score'][candidate],\n                  results['std_test_score'][candidate]))\n            print(\"Parameters: {0}\".format(results['params'][candidate]))\n            print(\"\")","execution_count":4,"outputs":[]},{"metadata":{"_cell_guid":"db1a20a3-13ef-4602-9040-3018d1eb63cd","_uuid":"6fbb201b04413dd024592603febed90cf5e3da66","collapsed":true,"trusted":false},"cell_type":"code","source":"logistic = linear_model.LogisticRegression(penalty = 'l2')\n\n#the hyper-parameters include the number of features,  type of regularization \n#and the alpha parameter of the regularizer\nparam_grid= {'penalty':['l1','l2'],'C':sp_rand()}\n\nnum_iter = 2000\nrand_search_cv = RandomizedSearchCV(logistic,param_distributions = param_grid\n                                    ,n_iter = num_iter)\n\nstart = time()\nrand_search_cv.fit(x_df, y_df)\nprint(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n      \" parameter settings.\" % ((time() - start), num_iter))\nreport(rand_search_cv.cv_results_)\n\nlogistic.set_params(**rand_search_cv.best_params_)\n#note the ** means to unpack all the iterations in the named argument \n#(either in dictionary form or in named pair eg \"c\" = 0.1231)\n#note the * means to unpack all the positional arguments","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ef9623d7-ddb3-467a-8fcc-86fd26a3f347","_uuid":"ea1cc95bddfebfffd9367d33533fc3948304d558"},"cell_type":"markdown","source":"Obtaining the results in order to submit prediction outcome"},{"metadata":{"_cell_guid":"902befe5-214f-4159-ac0a-50c55c0d3bfa","_uuid":"99a6d37a95849f625a6a8c498accbb8d9150f9c5","trusted":true},"cell_type":"code","source":"#applying data cleansing on the test data too\ntest_copy = estimator.fit_transform(test)\n\ntemp = list(headings)\ntemp.remove('Survived')\nheadings = np.asarray(temp)\n\ntest_copy =pd.DataFrame(test_copy)\ntest_copy.columns = headings\n\n'''\nfor x in ['Age','Fare']:\n    #import pdb;pdb.set_trace()\n    scaled = StandardScaler().fit_transform(test_copy[x].values.reshape(-1,1))\n    test_copy[x] = scaled\n'''\ntest_copy.drop('PassengerId',axis=1)","execution_count":5,"outputs":[]},{"metadata":{"_cell_guid":"80378e5d-3ea7-4142-9142-ba98b0cce664","_uuid":"aff58a09558dece1ae8af7a3fe6256e975e2ad92","collapsed":true,"trusted":false},"cell_type":"code","source":"#submitting the results\ny_pred = rand_search_cv.predict(test_copy).astype(int)\n\n\nsubmission = pd.DataFrame({\n        \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": y_pred\n    })\nsubmission.to_csv('titaniclogistic.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f79fd334-c774-4dc8-b521-6b21f866aae3","_uuid":"409592f4a336719b67502a174caff701d99f5798"},"cell_type":"markdown","source":"This script produces a score of 74% accuracy which is pretty low relative to other scripts, so we will now plot the learning curve in hopes of obtaining a clearer picture as to why the score is so low.\n"},{"metadata":{"_cell_guid":"5aa136c0-1624-4b2a-8b5c-af41cf4784d6","_uuid":"070ece53bbceedc2f556e018d53191c13276bba4","collapsed":true},"cell_type":"markdown","source":"Note from the figure, we can see that the training error and the validation score is quite close and hovering at 80, this signifies that the model is not overfitting (a overfitting model will have a huge difference between training and valdiaation error).\n\nHence if we want to further improve our results there is 3 ways we can go about doing so:\n\n1.  Increase the number of data collected\n2. Use a different model\n3. Increase the number of features to capture more complexity \n\nSince the current model we are using now is a basic logistic regression, the model might be unable to capture the complexity of the data and we will now view the results using different models (SVM, random forest)\n"},{"metadata":{"_cell_guid":"b67d326e-fd00-4acb-bfed-51f39200dea9","_uuid":"8574b734690458ba67fe0eabd48550a10411b9d1","collapsed":true,"trusted":false},"cell_type":"code","source":"#we will now train a SVM\nfrom sklearn import svm\nsvm_model = svm.LinearSVC(dual=False)\nparam_grid_svm= {'penalty':['l1','l2'],'C':sp_rand()}\n\nnum_iter_svm = 2000\nrand_search_svm = RandomizedSearchCV(svm_model,param_distributions = param_grid_svm\n                                    ,n_iter = num_iter_svm)\nstart = time()\nrand_search_svm.fit(x_df, y_df)\nprint(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n      \" parameter settings.\" % ((time() - start), num_iter_svm))\nreport(rand_search_svm.cv_results_)\n\nsvm_model.set_params(**rand_search_svm.best_params_)\n\n#submitting the results\ny_pred = rand_search_svm.predict(test_copy).astype(int)\nsubmission = pd.DataFrame({\n        \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": y_pred\n    })\nsubmission.to_csv('titanicsvm.csv', index=False)\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ddd0046f-e609-47fb-86e5-2fed877fdcd7","_uuid":"5e32aa8659bce29f2041c2a989132f7252311f5b","trusted":true,"collapsed":true},"cell_type":"code","source":"#we will now train with a random tree forest algorithm\nfrom sklearn.ensemble import RandomForestClassifier\nparams_grid = {'n_estimators':sp.randint(1,200),\n              'criterion':['gini','entropy'],\n               'max_depth':sp.randint(1,8),\n               'min_samples_leaf':sp.randint(10,50),\n               'max_features':np.arange(0.1,0.7,0.1)\n               }\nnum_iterations = 2000\nran_forest_mnist = RandomForestClassifier(random_state = 42, verbose =1,n_jobs=-1)\nran_forest_mnist_cv = RandomizedSearchCV(ran_forest_mnist,param_distributions=params_grid,\n                                         n_iter=num_iterations,verbose=1,n_jobs=-1)","execution_count":10,"outputs":[]},{"metadata":{"_cell_guid":"18e80330-6da2-46fd-b765-000e908e6200","_uuid":"cc94ef2498cc98d565502d41a7bf881d92a58ea1","trusted":true},"cell_type":"code","source":"start = time()\nran_forest_mnist_cv.fit(x_df,y_df)\nprint(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n      \" parameter settings.\" % ((time() - start), num_iterations))\n","execution_count":11,"outputs":[]},{"metadata":{"_cell_guid":"41e2fd7e-31f8-41b9-bd14-34585932bcb9","_uuid":"988087f213cee6cea03afc513ea440e4df8ac632","trusted":true},"cell_type":"code","source":"report(ran_forest_mnist_cv.cv_results_)\n\nran_forest_mnist.set_params(**ran_forest_mnist_cv.best_params_)\n\nran_forest_mnist.fit(x_df,y_df)\n\nran_forest_mnist.score(x_df,y_df)","execution_count":12,"outputs":[]},{"metadata":{"_cell_guid":"f55742c4-3235-4774-80cb-0a4abea3b7ae","_uuid":"cd4d36d6a05510a3c77839ef676623b27a5a4595","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import learning_curve\nstart = time()\ntrain_size,train_score,cv_score = learning_curve(ran_forest_mnist,x_df,y_df,\n                                                 train_sizes=np.linspace(0.2,1,num=21,dtype= float),\n                                                 cv = 10)\n#the train_score,cv_score are all arra of number of different train size x number of cv folds\nprint('The process took {0}seconds'.format(time()-start))\n\ntrain_score_mean , cv_score_mean = np.mean(train_score,axis=1) , np.mean(cv_score,axis=1)\n\nplt.plot(train_size,train_score_mean,label='Training score')\n\nplt.plot(train_size,cv_score_mean,label=\"Cross validation score\")\nplt.xlabel('number of training examples')\nplt.ylabel('Accuracy score')\nplt.title(\"Learning curve\")\nplt.legend()","execution_count":13,"outputs":[]},{"metadata":{"_cell_guid":"fd3ab67c-ed4c-45db-aad0-0231ad7871bd","_uuid":"e7334172fe99738260087860fa744fd067def262","trusted":true},"cell_type":"code","source":"#submitting the results\ny_pred_ran_forest = ran_forest_mnist.predict(test_copy).astype(int)\nsubmission = pd.DataFrame({\n        \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": y_pred_ran_forest\n    })\nsubmission.to_csv('titanicran1.csv', index=False)","execution_count":15,"outputs":[]},{"metadata":{"_cell_guid":"42aa5787-639c-405d-ad9b-8593de766a4f","_uuid":"885e4901a62acd8e9b4f34193de53e88cce6e40a","collapsed":true,"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}