{"cells":[{"metadata":{"_uuid":"5adae90bcaed7a0bf4725a7c82fa216e8099f7bd"},"cell_type":"markdown","source":"# Titanic : Machine Learning from Disaster"},{"metadata":{"_uuid":"25b5c0086ed0b380daec0b2b378f21d7f09b0ae7"},"cell_type":"markdown","source":"In this notebook, we are going to create an algorithm which will predict  if a passenger of the titanic survived or not. This algorithm will learn from train dataset and then it will predict if each passenger of the test dataset survived. \n\nThe plan to do that is the following one :\n\n1. Exploration of data\n2. Features engineering\n3. Cross validation testing of the model\n4. Prediction on test data & submission on kaggle"},{"metadata":{"_uuid":"36ce2f5c196e4773aff267a85a1a908b92c6168f"},"cell_type":"markdown","source":"## Imports and useful functions"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport matplotlib\nimport pydot\nimport re\n\n%matplotlib inline\nimport matplotlib\nimport matplotlib.pyplot as plt\nplt.rcParams['axes.labelsize'] = 14\nplt.rcParams['xtick.labelsize'] = 12\nplt.rcParams['ytick.labelsize'] = 12\n\nimport numpy as np\nimport sklearn\nimport seaborn as sns\n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn import metrics\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, ExtraTreesClassifier, AdaBoostClassifier\nfrom sklearn.svm import SVC  \nfrom sklearn.tree import export_graphviz, DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB\nfrom sklearn.preprocessing import StandardScaler\n\n\nimport os\nos.environ['KMP_DUPLICATE_LIB_OK']='True'\nfrom xgboost import XGBClassifier\n\nfrom IPython.display import display","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#path of datasets\npath_train = '../input/train.csv'\npath_test = '../input/test.csv'\n\ndef display_confusion_matrix(sample_test, prediction, score=None):\n    cm = metrics.confusion_matrix(sample_test, prediction)\n    plt.figure(figsize=(9,9))\n    sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square=True, cmap='Blues_r')\n    plt.ylabel('Actual label')\n    plt.xlabel('Predicted label')\n    if score:\n        all_sample_title = 'Accuracy Score: {0}'.format(score)\n        plt.title(all_sample_title, size = 15)\n    print(metrics.classification_report(sample_test, prediction))\n    \ndef visualize_tree(tree, feature_names):\n    \"\"\"Create tree png using graphviz\"\"\"\n    with open(\"dt.dot\", 'w') as f:\n        export_graphviz(tree, out_file=f,\n                        feature_names=feature_names)\n\n    command = [\"dot\", \"-Tpng\", \"dt.dot\", \"-o\", \"dt.png\"]\n    try:\n        subprocess.check_call(command)\n    except:\n        exit(\"Could not run dot, ie graphviz, to \"\n             \"produce visualization\")\n        \ndef model_tuning(model, param_grid):\n    \n    gc_cv = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n    gc_cv.fit(X_train_sample_train, Y_train_sample_train)\n    \n    return gc_cv.best_params_, gc_cv.best_score_, gc_cv.cv_results_\n\ndef standardize(df):\n    \n    standardize_df = df.copy()\n    \n    target = None\n    if 'Survived' in standardize_df.columns.tolist():\n        target = standardize_df['Survived'] # Separating out the target before standardizing\n        standardize_df = standardize_df.drop(['Survived'],  axis=1)\n\n    # Standardizing the features\n    scaled_values = StandardScaler().fit_transform(standardize_df.values)\n    standardize_df = pd.DataFrame(scaled_values, index=standardize_df.index, columns=standardize_df.columns)\n    if target is not None:\n        standardize_df = standardize_df.join(target)\n    \n    return standardize_df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"789084c52a3741a4bdab8c1feff3bbf81be5fc8d"},"cell_type":"markdown","source":"## 1. Data exploration"},{"metadata":{"_uuid":"87e46869bf36e52e65b540541789129ee6aed3d8","trusted":true},"cell_type":"code","source":"#create dataframe for training dataset and print ten first rows as preview\ntrain_df_raw = pd.read_csv(path_train)\ntrain_df_raw.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"996505836b43d054f175c48e2229993930d6de3d","trusted":true},"cell_type":"code","source":"# Compute some basical statistics on the dataset\ntrain_df_raw.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"191dd9ee84b0dcdd283b71bfc53e84f8d49f60da","trusted":true},"cell_type":"code","source":"# Let's plot some histograms to have a previzualisation of some of the data ...\ntrain_df_raw.drop(['PassengerId'], 1).hist(bins=50, figsize=(20,15))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"840c201359244d09dbcd378dafd8ebf0a42427c5"},"cell_type":"markdown","source":"With this first exploration, we can see that :\n\n* Only aproximately 35% of passengers survived ...\n* More than the half of passengers are in the lowest class (pclass = 3)\n* Most of the fare tickets are below 50\n* Majority of passengers are alone (sibsp and parch)"},{"metadata":{"_uuid":"c86c5e54aeeb83b785747f2cdd7d276a694b691c"},"cell_type":"markdown","source":"## 2. Features engineering"},{"metadata":{"_uuid":"0002426c7784d72821b2654013587fff35ca5be6","trusted":true},"cell_type":"code","source":"def preprocess_data(df):\n    \n    # Replace string data by numeric data\n    df['Cabin'].fillna('U0', inplace=True)\n    df['Sex'] = df['Sex'].replace('male', 1)\n    df['Sex'] = df['Sex'].replace('female', 0)\n    df['Embarked'] = df['Embarked'].replace('S', 0)\n    df['Embarked'] = df['Embarked'].replace('C', 1)\n    df['Embarked'] = df['Embarked'].replace('Q', 2)\n    df['Embarked'].fillna(0, inplace=True) # because there is approximately 80% of 0 in embarked column\n\n    # Replace NaN data in age column by mean age of passengers\n    mean_age = df['Age'].mean()\n    df['Age'].fillna(mean_age, inplace=True)\n    \n    df['Fare'] = df['Fare'].interpolate()\n\n    # Let's work on 'Name' column : we find the title in the name and extract it on a new column 'Title'\n    mapping_title = {title: pos for pos, title in enumerate(set([name.split('.')[0].split(',')[1].strip() for name in df['Name']]))}\n    df['Title'] = pd.Series((mapping_title[name2.split('.')[0].split(',')[1].strip()] for name2 in df['Name']), index=df.index)\n    df = df.drop(columns=['Name'])\n\n    #Creation of a deck column corresponding to the letter contained in the cabin value\n    mapping_deck = {title: pos for pos, title in enumerate(set([cab[:1] for cab in df['Cabin']]))}\n    df['Deck'] = pd.Series((mapping_deck[cab2[:1]] for cab2 in df['Cabin']), index=df.index)\n\n    # Modify the cabin column to keep only the cabin number\n    cabin_numbers = list()\n    for cab3 in df['Cabin']:\n        if len(cab3) != 1:\n            cabin_numbers.append(int(cab3[1:].strip()) if len(cab3[1:]) <= 3 else int(cab3[len(cab3)-2:].strip()))\n        else:\n            cabin_numbers.append(0)\n\n    # df['Cabin'] = pd.Series(cabin_numbers, index=df.index)\n    df = df.drop(['Cabin'], 1)\n\n    # Modify the ticket column to keep only the ticket number\n    ticket_numbers = list()\n    for ticket in df['Ticket']:\n        try:\n            ticket_numbers.append(int(ticket))\n        except ValueError:\n            splitted = ticket.split(' ')\n            if len(splitted) == 1:\n                ticket_numbers.append(0)\n            else:\n                ticket_numbers.append(int(splitted[len(splitted)-1]))\n\n    df['Ticket'] = pd.Series(ticket_numbers, index=df.index)\n        \n    return df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"75a781ff2a645d7a68489debed35e4e8e71a5b7e"},"cell_type":"markdown","source":"## 3. Cross validation of model on train dataset"},{"metadata":{"_uuid":"5938ee5f0e21505fe0766caceab370c3be0cfd3f","trusted":true},"cell_type":"code","source":"# Let's divide the train dataset in two datasets to evaluate perfomance of machine learning models used\ntrain_df = train_df_raw.copy()\nX_train = train_df.drop(['Survived'], 1)\nY_train = train_df['Survived']\n\n# Split dataset for prediction\nX_train_sample_train, X_train_sample_test, Y_train_sample_train, Y_train_sample_test = sklearn.model_selection.train_test_split(X_train,\n                                                                                                                                Y_train, \n                                                                                                                                test_size=0.3, \n                                                                                                                                random_state=42)\n\nX_train_sample_train = preprocess_data(X_train_sample_train)\nX_train_sample_test = preprocess_data(X_train_sample_test)\nX_train_sample_train = standardize(X_train_sample_train)\nX_train_sample_test = standardize(X_train_sample_test)\n\nX_train_sample_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"967eb31ccbac7df5e78acb6a972a0f8dda9568e2"},"cell_type":"markdown","source":"### Try several models"},{"metadata":{"trusted":true,"_uuid":"344101a216db654142f8a300ae6c4589ae4e839f"},"cell_type":"code","source":"# Create and train model on train data sample\nlogisticRegr = LogisticRegression(random_state=42)\nlogisticRegr.fit(X_train_sample_train, Y_train_sample_train)\n\n# Predict for test data sample\nlogistic_prediction_train = logisticRegr.predict(X_train_sample_test)\n\n# Compute error between predicted data and true response and display it in confusion matrix\nscore = metrics.accuracy_score(Y_train_sample_test, logistic_prediction_train)\ndisplay_confusion_matrix(Y_train_sample_test, logistic_prediction_train, score=score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d2845e6bbc40cd5ba5bdad43c5b00609567f5ff4"},"cell_type":"code","source":"# Create and train model on train data sample\ndt = DecisionTreeClassifier(min_samples_split=15, min_samples_leaf=8, random_state=42)\ndt.fit(X_train_sample_train, Y_train_sample_train)\n\n# Predict for test data sample\ndt_prediction_train = dt.predict(X_train_sample_test)\n\n# Compute error between predicted data and true response and display it in confusion matrix\nscore = metrics.accuracy_score(Y_train_sample_test, dt_prediction_train)\ndisplay_confusion_matrix(Y_train_sample_test, dt_prediction_train, score=score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"99d559c2f68bbf700452c822e8e352303e7e3991"},"cell_type":"code","source":"visualize_tree(dt, X_train_sample_test.columns)\n! dot -Tpng dt.dot > dt.png","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"677c9bb2fc42ad37029fbb02b85099a72283173c"},"cell_type":"markdown","source":"![title](dt.png)"},{"metadata":{"trusted":true,"_uuid":"f5a40cfda0a8d632ef1daf3476c6d474ee366ec1"},"cell_type":"code","source":"# Create and train model on train data sample\nrf = RandomForestClassifier(n_estimators=50, random_state = 42)\nrf.fit(X_train_sample_train, Y_train_sample_train)\n\n# Predict for test data sample\nrf_prediction_train = rf.predict(X_train_sample_test)\n\n# Compute error between predicted data and true response and display it in confusion matrix\nscore = metrics.accuracy_score(Y_train_sample_test, rf_prediction_train)\ndisplay_confusion_matrix(Y_train_sample_test, rf_prediction_train, score=score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8703951d1626be0bb68ad78bce74d44841d27a3e"},"cell_type":"code","source":"# Create and train model on train data sample\nvot = VotingClassifier([('dt', dt), ('lr', logisticRegr), ('rf', rf)], voting='soft')\nvot.fit(X_train_sample_train, Y_train_sample_train)\n\n# Predict for test data sample\nvot_prediction_train = vot.predict(X_train_sample_test)\n\n# Compute error between predicted data and true response and display it in confusion matrix\nscore = metrics.accuracy_score(Y_train_sample_test, vot_prediction_train)\ndisplay_confusion_matrix(Y_train_sample_test, vot_prediction_train, score=score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0afb4e138eb209e982c579994458c887d8c07902"},"cell_type":"code","source":"# First try with a random forest with all default parameters\nboost = XGBClassifier(random_state=42) # replace the sckitlearn class here to try other models\nboost.fit(X_train_sample_train, Y_train_sample_train)\n\n# Predict for test data sample\nboost_prediction_train = boost.predict(X_train_sample_test)\n\n# Compute error between predicted data and true response and display it in confusion matrix\nscore = metrics.accuracy_score(Y_train_sample_test, boost_prediction_train)\n\ndisplay_confusion_matrix(Y_train_sample_test, boost_prediction_train, score=score)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"06c3acf429d8d09968c07e8c1c06f88875bb62db"},"cell_type":"markdown","source":"## 4. Apply on test dataset and submit on kaggle "},{"metadata":{"_uuid":"962229f11da6b327557f53e048f0a55068f6dc4a","trusted":false},"cell_type":"code","source":"test_df_raw = pd.read_csv(path_test)\ntest_df = test_df_raw.copy()\nX_test = preprocess_data(test_df)\nX_test = standardize(X_test)\nX_train = preprocess_data(X_train)\nX_train = standardize(X_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"499d60687a2a8ce30edbdf17e2f3ed89d7d9e7bc","trusted":false},"cell_type":"code","source":"# Create and train model on train data sample\nmodel_test = XGBClassifier(random_state=42) # replace the sckitlearn class here to try other models\nmodel_test.fit(X_train, Y_train)\n\n# Predict for test data sample\nmodel_test_prediction_test = model_test.predict(X_test)\n\nresult_df_xgb = test_df_raw.copy()\nresult_df_xgb['Survived'] = model_test_prediction_test.astype(int)\n\nresult_df_xgb.head()\nresult_df_xgb.to_csv('submission.csv', columns=['PassengerId', 'Survived'], index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"33860af24a5a19833ae74f1014baa293e857264f"},"cell_type":"markdown","source":"#### *Precision obtained on kaggle : 0.79425 !** ðŸŽ‰*"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":1}