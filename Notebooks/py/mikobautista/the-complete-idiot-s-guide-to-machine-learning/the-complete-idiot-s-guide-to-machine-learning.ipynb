{"nbformat_minor": 1, "metadata": {"language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython3", "codemirror_mode": {"name": "ipython", "version": 3}, "version": "3.6.3"}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}, "cells": [{"source": ["# The Complete Idiot's Guide to Machine Learning\n", "This notebook serves a beginner's guide to the basics of Machine Learning workflow. It is written in Python, utilizing libraries such as pandas, numpy, and scikit-learn. I will attempt to explain the decisions I made throughout this experiment in layman's terms. As a data science newbie, I would greatly appreciate any feedback you might have regarding this kernel. If you find some value in this notebook, please consider upvoting as well.\n", "\n", "Happy coding! :)\n", "\n", "## Table of Contents\n", "1. Importing Libraries\n", "2. Previewing the Data\n", "3. Data Analysis and Wrangling\n", "4. Feature Encoding\n", "5. Model Evaluation\n", "6. K-Fold Cross-Validation\n", "7. Hyper-Parameter Tuning\n", "8. Uploading Submission to Kaggle"], "cell_type": "markdown", "metadata": {"_uuid": "7c772e2d9f8d0ac44da7ddb03f9f83b445b82c56", "_cell_guid": "ef91380c-2fe0-4b2c-9495-441cd3ddbf01"}}, {"source": ["## 1. Importing Libraries"], "cell_type": "markdown", "metadata": {"_uuid": "6c1dd35bfa1f6ccd1018484fd7171b39c7527487", "_cell_guid": "2fb3d846-fe5b-47c0-a7f4-56fe9c56d3f1"}}, {"source": ["# Information Processing\n", "import numpy as np\n", "import pandas as pd\n", "import re\n", "\n", "# Data Visualization\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "sns.set_style('darkgrid')\n", "sns.set_palette(sns.color_palette('pastel'))\n", "\n", "# Machine Learning\n", "from sklearn.ensemble import RandomForestClassifier\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.metrics import accuracy_score\n", "from sklearn.model_selection import cross_val_score\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.model_selection import GridSearchCV\n", "from sklearn.model_selection import KFold\n", "from sklearn.naive_bayes import GaussianNB\n", "from sklearn.neighbors import KNeighborsClassifier\n", "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n", "from sklearn.svm import SVC\n", "from sklearn.tree import DecisionTreeClassifier"], "cell_type": "code", "metadata": {"_uuid": "074afbea0a960918df1c27fc60d39f692d417238", "collapsed": true, "_cell_guid": "2701983f-a123-4644-9b7e-3495698bc724"}, "execution_count": null, "outputs": []}, {"source": ["## 2. Previewing the Data"], "cell_type": "markdown", "metadata": {"_uuid": "48851d32df514592e8251f4db66d018d458a8506", "_cell_guid": "f85c2de5-a25a-461c-93e6-8d7cf5e0dacc"}}, {"source": ["# Import the dataset\n", "train = pd.read_csv('../input/train.csv')\n", "test = pd.read_csv('../input/test.csv')\n", "train.head()"], "cell_type": "code", "metadata": {"_uuid": "c6bb0ebbb2e413f5ec94735944f0f44e4957df25", "scrolled": true, "_cell_guid": "3014c53a-cf76-4ed0-887d-92cec77240df"}, "execution_count": null, "outputs": []}, {"source": ["train.info()\n", "print(\"======================================\")\n", "test.info()"], "cell_type": "code", "metadata": {"_uuid": "685aa9a58eebc04e36bc4dab83e21eb57aa6e7b8", "_cell_guid": "978ea9f6-84fa-4816-97e9-a23c45944ad7"}, "execution_count": null, "outputs": []}, {"source": ["print(train.isnull().sum()) # Null values for Age, Cabin, Embarked\n", "print(\"===================\")\n", "print(test.isnull().sum()) # Null values for Age, Fare, Cabin"], "cell_type": "code", "metadata": {"_uuid": "0bb18a854c723142fb12ae7454440b2b2041d3e1", "_cell_guid": "83b2e0ee-bbb2-4d8e-8017-2632ade33cdc"}, "execution_count": null, "outputs": []}, {"source": ["## 3. Data Analysis and Wrangling\n", " "], "cell_type": "markdown", "metadata": {"_uuid": "4f602aa56050f0d5751c826515ea1ba8b97f2ffa", "_cell_guid": "4ce3dff2-c09d-477d-b164-0b8811e5b12f"}}, {"source": ["# Ticket is an alphanumeric string indicating the passenger's ticket\n", "# number. There does not seem to be any inherent pattern in them.\n", "print(train['Ticket'].sample(10))\n", "\n", "# Let's drop this feature. It is unlikely to contain any useful data\n", "# for our purposes.\n", "def drop_ticket(df):\n", "    df.drop('Ticket', axis=1, inplace=True)\n", "    \n", "drop_ticket(train)\n", "drop_ticket(test)"], "cell_type": "code", "metadata": {"_uuid": "c6a18cecb3b7cb8298cd2acbe3da355bac918a86", "_cell_guid": "f25f50fc-28bb-4e27-ad4d-a98fee0cc177"}, "execution_count": null, "outputs": []}, {"source": ["# Sex is a no-brainer feature to keep. Females have a significantly better chance\n", "# of survival than males. The entire population's survival rate is only 38.4%, but\n", "# it jumps all the way to 74.2% for females!\n", "_, ax = plt.subplots(1, 2, figsize=(20,10))\n", "train['Survived'].value_counts().plot.pie(explode=[0,0.1], autopct='%1.1f%%', ax=ax[0])\n", "ax[0].set_title('Survival Rate of Population')\n", "sns.barplot('Sex', 'Survived', data=train, ax=ax[1])\n", "ax[1].set_title('Survival Rate by Sex')\n", "plt.show()"], "cell_type": "code", "metadata": {"_uuid": "a549a7c6523ac0f0cc32d1d95a89871fbb4e1179", "_cell_guid": "a04e3042-24ed-4ad7-bbaf-6ca4a8f09cfe"}, "execution_count": null, "outputs": []}, {"source": ["# Embarked seems to have a strong correlation with survival.\n", "# Those who departed from Cherbourg (\"C\"), for example, were more\n", "# likely to survive than those who departed from Queenstown (\"Q\").\n", "_, ax = plt.subplots(1, 2, figsize=(20,10))\n", "sns.countplot('Embarked', data=train, ax=ax[0]);\n", "ax[0].set_title('Passenger Count by Embarked')\n", "ax[0].set_ylabel('Passenger Count')\n", "sns.countplot('Embarked', hue='Survived', data=train, ax=ax[1])\n", "ax[1].set_title('Survival Count by Embarked')\n", "ax[1].set_ylabel('Survival Count')\n", "plt.show()"], "cell_type": "code", "metadata": {"_uuid": "c225341266f2e367ca46d65be526c96ca332cb29", "_cell_guid": "22b3df76-beeb-4f47-8d6d-3456dc653052"}, "execution_count": null, "outputs": []}, {"source": ["# From the chart above, we observe that a vast majority of the passengers\n", "# embarked from Southampton (\"S\"). It would be reasonable to use this value\n", "# to fill the 2 entries with null Embarked.\n", "def fill_null_embarked(df):\n", "    df.Embarked.fillna('S', inplace=True)\n", "    \n", "fill_null_embarked(train)\n", "fill_null_embarked(test)"], "cell_type": "code", "metadata": {"_uuid": "554cacddd3754a64ffb5bbddc9172211cced2eae", "collapsed": true, "_cell_guid": "db043deb-1291-4a67-87e5-8c573692be91"}, "execution_count": null, "outputs": []}, {"source": ["# Age is a very powerful indicator. The probability of survival is\n", "# the highest for young passengers. On the flipside, it\n", "# is the lowest for the elderly and those in their prime years of\n", "# strength (perhaps because they were preoccupied helping other people).\n", "_, ax = plt.subplots(figsize=(20,5))\n", "sns.distplot(train[train['Survived'] == 0]['Age'].dropna(), hist=False, kde_kws={\"shade\": True})\n", "sns.distplot(train[train['Survived'] == 1]['Age'].dropna(), hist=False, kde_kws={\"shade\": True})\n", "ax.set_title(\"Population Distribution by Age and Survival Rate\")\n", "ax.set_ylabel('% of Population')\n", "ax.legend(['Survived = 0', 'Survived = 1'])\n", "ax.set_xlim(0, 85)\n", "plt.show()"], "cell_type": "code", "metadata": {"_uuid": "8aa2a2d3f3a8030d8b3fcbdaaa6418e3316a866e", "_cell_guid": "46a03bb3-2b36-4f06-b32f-56ac5fdba05f"}, "execution_count": null, "outputs": []}, {"source": ["# Unlike Embarked, there are a substantial number of missing values for Age. It\n", "# would be ill-advised for us to blindly fill these in. Fortunately, we can\n", "# utilize the salutations in the Name feature to make a more educated guess.\n", "def fill_null_age(df_train, df_test):\n", "    # Calculate the average age for all passengers with the same salutation.\n", "    df_combined = pd.concat([train[['Age', 'Name']], test[['Age', 'Name']]])\n", "    df_combined['Salutation'] = df_combined.Name.str.extract(', ([A-Za-z]+)\\.', expand=False)\n", "    average_ages = df_combined.groupby('Salutation')['Age'].mean()\n", "    df_combined.drop('Salutation')\n", "    # Fill in null Age values using these averages. \n", "    df_train.loc[df_train.Age.isnull(), 'Age'] = df_train[df_train.Age.isnull()].apply(lambda row: average_ages[re.search(', ([A-Za-z]+)\\.', row.Name).group(1)], axis=1)\n", "    df_test.loc[df_test.Age.isnull(), 'Age'] = df_test[df_test.Age.isnull()].apply(lambda row: average_ages[re.search(', ([A-Za-z]+)\\.', row.Name).group(1)], axis=1)\n", "    return df_train, df_test\n", "\n", "train, test = fill_null_age(train, test)"], "cell_type": "code", "metadata": {"_uuid": "274b0bfad5f665fc1aa8f3652a33449ed29bbcbc", "collapsed": true, "_cell_guid": "d948f08c-e3ea-4257-a5b4-7e117a6912ae"}, "execution_count": null, "outputs": []}, {"source": ["# Age bands simplify the data we pass into the model. They\n", "# allow us to capture core information with lower overhead.\n", "def create_age_bands(df):\n", "    bins = [np.NINF, 15, 30, 60, np.inf]\n", "    labels = ['0_Young', '1_Prime', '2_Mature', '3_Elderly']\n", "    df['AgeBand'] = pd.cut(df.Age, bins, labels = labels)\n", "    df.drop('Age', axis=1, inplace=True)\n", "    \n", "create_age_bands(train)\n", "create_age_bands(test)\n", "\n", "_, ax = plt.subplots(1, 2, figsize=(20,10))\n", "sns.countplot(x=\"AgeBand\", data=train, ax=ax[0]);\n", "ax[0].set_title('Passenger Count by AgeBand')\n", "ax[0].set_ylabel('Passenger Count')\n", "sns.countplot('AgeBand', hue='Survived', data=train, ax=ax[1])\n", "ax[1].set_title('Survival Count by AgeBand')\n", "ax[1].set_ylabel('Survival Count')\n", "plt.show()"], "cell_type": "code", "metadata": {"_uuid": "51ab0d0a48341bea51b1627609563deca39612ce", "_cell_guid": "ab4edb9f-9890-414b-afbb-6b1c8f57a8a4"}, "execution_count": null, "outputs": []}, {"source": ["# Money talks. Pclass, a proxy for socio-economic status, is another feature\n", "# that we should keep. 1 denotes upper class, 2 denotes middle, 3 denotes lower.\n", "# Clearly, those with more money were likelier to survive the tragedy.\n", "# In fact, nearly *all* females from upper class survived!\n", "_, ax = plt.subplots(1, 2, figsize=(20,10))\n", "train['Pclass'].value_counts().plot.pie(explode=[0.025,0.025,0.025], autopct='%1.1f%%', ax=ax[0])\n", "ax[0].set_title('Population Pclass Breakdown')\n", "sns.pointplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=train, ax=ax[1]);\n", "ax[1].set_title('Survival Rate by Pclass and Sex')\n", "plt.show()"], "cell_type": "code", "metadata": {"_uuid": "ff46667895535758b1f4245b80a8c6f6458a67f3", "_cell_guid": "1f7d59e5-d6aa-4945-bf0b-cb29301b9923"}, "execution_count": null, "outputs": []}, {"source": ["# Although a person's name might not be very indicative of his/her survival,\n", "# the Name field also contains the passenger's salutation. Individuals with\n", "# titles such as Don, Rev, Dr, etc. have a low likelihood of survival. These\n", "# salutations are typically found in occupations where having a special love for\n", "# community is essential (medical/military/royalty). I hypothesize that they\n", "# felt compelled to save others before themselves, thus explaining the low\n", "# survival rate.\n", "train['Salutation'] = train.Name.str.extract(', ([A-Za-z\\s]+)\\.', expand=False)\n", "_, ax = plt.subplots(figsize=(20,5))\n", "sns.barplot('Salutation', 'Survived', data=train, ax=ax)\n", "ax.set_title('Survival Rate by Salutation')\n", "plt.show()"], "cell_type": "code", "metadata": {"_uuid": "3b672cc214dd4f0ca806e2e12b17e408b44caedc", "_cell_guid": "25008707-3cd1-466f-8b22-2d2515812b78"}, "execution_count": null, "outputs": []}, {"source": ["# Create a new Salutation feature, combining all of the \"Rescuer\" prefixes.\n", "# In addition, we also group synonymous prefixes together.\n", "def simplify_name(df):\n", "    df['Salutation'] = df.Name.str.extract(', ([A-Za-z\\s]+)\\.', expand=False)\n", "    df['Salutation'].replace(['Dr', 'Rev', 'Major', 'Col','Capt', 'Sir', 'Don', 'Jonkheer'], 'Rescuer', inplace = True)\n", "    df['Salutation'].replace(['Lady', 'Dona', 'the Countess', 'Mme'], 'Mrs', inplace = True)\n", "    df['Salutation'].replace(['Mlle', 'Ms'], 'Miss', inplace = True)\n", "    df.drop('Name', axis=1, inplace=True)\n", "\n", "simplify_name(train)\n", "simplify_name(test)"], "cell_type": "code", "metadata": {"_uuid": "16e57eeabda0d9cd5fc2f141aa7c7572207f8c93", "collapsed": true, "_cell_guid": "0bf78a79-5721-4c41-85a6-0bd7bbdfcb7d"}, "execution_count": null, "outputs": []}, {"source": ["# The data indicates that the Cabin feature did have an impact on survival.\n", "# It didn't matter too much which type of cabin you had though. It seems that\n", "# as long as you *had* one, you were already much better off than those without.\n", "train['CabinLetter'] = train['Cabin'].str[0]\n", "train['CabinLetter'].fillna('None', inplace=True)\n", "_, ax = plt.subplots(figsize=(20,5))\n", "sns.barplot('CabinLetter', 'Survived', data=train, ax=ax)\n", "ax.set_title(\"Survival Rate by CabinLetter\")\n", "train.drop('CabinLetter', axis=1, inplace=True)\n", "plt.show()\n", "\n", "# Hence, we can simplify the cabin feature by converting it to a WithCabin bool.\n", "def simplify_cabin(df):\n", "    df.loc[df.Cabin.notnull(), 'WithCabin'] = True\n", "    df.loc[df.Cabin.isnull(), 'WithCabin'] = False\n", "    df.Cabin.unique()\n", "    df.drop('Cabin', axis=1, inplace=True)\n", "\n", "simplify_cabin(train)\n", "simplify_cabin(test)"], "cell_type": "code", "metadata": {"_uuid": "0c4a3f557565bbe3d1c73063678762cbe392e40b", "_cell_guid": "56fcffc0-363a-4e0d-b27c-c7160643784d"}, "execution_count": null, "outputs": []}, {"source": ["# The trends in SibSp (sibling + spouse) and Parch (parent + children) are identical.\n", "# Someone who is traveling alone is less likely to survive than those with 1-3\n", "# companions. However, he/she would be better off than those with 4+ companions.\n", "# To simplify both features together, we will create a new one called FamilySize.\n", "_, ax = plt.subplots(figsize=(20,5))\n", "sns.pointplot('SibSp', 'Survived', data=train, ax=ax, errwidth=0, color='lightskyblue',)\n", "sns.pointplot('Parch', 'Survived', data=train, ax=ax, errwidth=0, color='mediumaquamarine')\n", "ax.set_title('Survival Rate by SibSp and Parch')\n", "ax.legend(['SibSp', 'Parch'])\n", "\n", "def create_family_size(df):\n", "    bins = [np.NINF, 0, 3, np.inf]\n", "    labels = ['0_Alone', '1_Few', '2_Many']\n", "    df['FamilySize'] = pd.cut(df.SibSp + df.Parch, bins, labels = labels)\n", "    df.drop('SibSp', axis=1, inplace=True)\n", "    df.drop('Parch', axis=1, inplace=True)\n", "\n", "create_family_size(train)\n", "create_family_size(test)\n", "\n", "_, ax = plt.subplots(figsize=(20,5))\n", "sns.barplot('FamilySize', 'Survived', data=train, ax=ax)\n", "ax.set_title(\"Survival Rate by FamilySize\")\n", "plt.show()"], "cell_type": "code", "metadata": {"_uuid": "120f820db1e93929673dd2d00898287fa1885c8e", "_cell_guid": "9244d932-9ea6-444b-a287-1035011dc188"}, "execution_count": null, "outputs": []}, {"source": ["# Did I mention money talks? Very few passengers who paid over $80 died. Meanwhile,\n", "# your odds of survival were terrible if your ticket cost less than $20.\n", "_, ax = plt.subplots(figsize=(20,5))\n", "sns.distplot(train[train['Survived'] == 0]['Fare'].dropna(), hist=False, kde_kws={\"shade\": True})\n", "sns.distplot(train[train['Survived'] == 1]['Fare'].dropna(), hist=False, kde_kws={\"shade\": True})\n", "ax.set_title(\"Population Distribution by Fare and Survival Rate\")\n", "ax.set_ylabel(\"% of Population\")\n", "ax.set_xlim(0, 160)\n", "ax.legend(['Survived = 0', 'Survived = 1'])\n", "plt.show()"], "cell_type": "code", "metadata": {"_uuid": "d1238456889d9b971ba0e3557f9043e3089ab211", "_cell_guid": "7a431046-ad71-4efd-8809-fbc68c247905"}, "execution_count": null, "outputs": []}, {"source": ["# In the entire dataset, there is only one null value for Fare.\n", "# We can fill this in with the average without much concern.\n", "def fill_null_fare(df_train, df_test):\n", "    df_combined = pd.concat([train['Fare'], test['Fare']])\n", "    avg_fare = df_combined.mean()\n", "    df_train.Fare.fillna(avg_fare, inplace=True)\n", "    df_test.Fare.fillna(avg_fare, inplace=True)\n", "\n", "fill_null_fare(train, test)"], "cell_type": "code", "metadata": {"_uuid": "ef0c7694fbcd9ebc0a2eed49cea3a1fa2aa7766d", "collapsed": true, "_cell_guid": "7a737a67-253c-485c-84e6-dc99c5172b12"}, "execution_count": null, "outputs": []}, {"source": ["# FareBands will simplify the processing of this feature.\n", "def create_fare_bands(df):\n", "    bins = [np.NINF, 20, 80, np.inf]\n", "    labels = ['0_Low', '1_Medium', '2_High']\n", "    df['FareBand'] = pd.cut(df.Fare, bins, labels = labels)\n", "    df.drop('Fare', axis=1, inplace=True)\n", "    \n", "create_fare_bands(train)\n", "create_fare_bands(test)\n", "\n", "_, ax = plt.subplots(figsize=(20,5))\n", "sns.barplot('FareBand', 'Survived', data=train, ax=ax)\n", "ax.set_title(\"Survival Rate by FareBand\")\n", "plt.show()"], "cell_type": "code", "metadata": {"_uuid": "a7538064dc6591b120b97347ca8fe279a064cf7e", "_cell_guid": "8c287c74-d74d-489b-b69d-8e7819443099"}, "execution_count": null, "outputs": []}, {"source": ["## 4. Feature Encoding"], "cell_type": "markdown", "metadata": {"_uuid": "1a8ce286fabbe0ba6af12ec20a8c98ac8db0af45", "_cell_guid": "40f8fb67-4c06-4cc9-afae-00edd2b6078c"}}, {"source": ["train.head()"], "cell_type": "code", "metadata": {"_uuid": "5b698a38ce93553cda56516cd3f767fdd1d294ac", "_cell_guid": "259d8b5a-15b8-4715-b8cd-ff17ea8d4f9c"}, "execution_count": null, "outputs": []}, {"source": ["# Integer encode the ordinal features.\n", "def encode_ordinal_features(df_train, df_test):\n", "    ordinal_features = ['FamilySize', 'AgeBand', 'FareBand']\n", "    df_combined = pd.concat([df_train[ordinal_features], df_test[ordinal_features]])\n", "    \n", "    for feature in ordinal_features:\n", "        le = LabelEncoder()\n", "        le = le.fit(df_combined[feature])\n", "        df_train[feature] = le.transform(df_train[feature])\n", "        df_test[feature] = le.transform(df_test[feature])\n", "    return df_train, df_test\n", "    \n", "train, test = encode_ordinal_features(train, test)\n", "train.head()"], "cell_type": "code", "metadata": {"_uuid": "1afda202eec4ac6930e05b7d7b091a57280c9b21", "_cell_guid": "ee1c2bb4-0952-4842-b539-64304d61469a"}, "execution_count": null, "outputs": []}, {"source": ["# Binary encode the categorical features.\n", "def encode_categorical_features(df_train, df_test):\n", "    categorical_features = ['Salutation', 'Sex', 'Embarked', 'WithCabin']\n", "    df_combined = pd.concat([df_train[categorical_features], df_test[categorical_features]])\n", "    for feature in categorical_features:\n", "        # First, perform integer encoding.\n", "        le = LabelEncoder()\n", "        le = le.fit(df_combined[feature])\n", "        df_train[feature] = le.transform(df_train[feature])\n", "        df_test[feature] = le.transform(df_test[feature])\n", "        df_combined[feature] = le.transform(df_combined[feature])\n", "        combined_integer_encoded = df_combined[feature].values.reshape(len(df_combined[feature]), 1)\n", "        # Then, perform binary encoding.\n", "        ohe = OneHotEncoder(sparse=False)\n", "        ohe = ohe.fit(combined_integer_encoded)\n", "        train_binary_encoded = ohe.transform(df_train[feature].values.reshape(len(df_train[feature]), 1))\n", "        test_binary_encoded = ohe.transform(df_test[feature].values.reshape(len(df_test[feature]), 1))\n", "        num_unique_values = len(df_combined[feature].unique())\n", "        for i in range(num_unique_values):\n", "            if (i > 0): # Avoid the dummy variable trap.\n", "                col_name = feature + \"_\" + str(le.inverse_transform(i))\n", "                train_col_data = train_binary_encoded[:, i].astype(int)\n", "                test_col_data = test_binary_encoded[:, i].astype(int)\n", "                df_train[col_name] = train_col_data\n", "                df_test[col_name] = test_col_data\n", "        df_train.drop(feature, axis=1, inplace=True)\n", "        df_test.drop(feature, axis=1, inplace=True)\n", "    return df_train, df_test\n", "\n", "train, test = encode_categorical_features(train, test)\n", "\n", "train.head()"], "cell_type": "code", "metadata": {"_uuid": "d4de61e39f8a98cda77828717f11519c19ee0b42", "_cell_guid": "229c4ade-6920-4d5b-be84-cdbc6343b401"}, "execution_count": null, "outputs": []}, {"source": ["## 5. Model Evaluation"], "cell_type": "markdown", "metadata": {"_uuid": "5b87133981d94b985d2ab1eba958b16b98e19e4f", "_cell_guid": "b4fc9aa4-14b8-4a4a-91be-e53e7c63b87c"}}, {"source": ["# Split the training data among X_train, X_val, Y_train, and Y_val.\n", "predictors = train.drop(['Survived', 'PassengerId'], axis=1)\n", "target = train[\"Survived\"]\n", "X_train, X_val, Y_train, Y_val = train_test_split(predictors, target, test_size = 0.2, random_state = 0)"], "cell_type": "code", "metadata": {"_uuid": "2acc213a9bcedc5fda00cf5c8bb5de364569a9c1", "collapsed": true, "_cell_guid": "3cd34ce0-392d-4dc0-bb7b-ba83879ab17d"}, "execution_count": null, "outputs": []}, {"source": ["# Logistic Regression\n", "logreg = LogisticRegression()\n", "logreg.fit(X_train, Y_train)\n", "Y_pred = logreg.predict(X_val)\n", "acc_log = round(accuracy_score(Y_pred, Y_val) * 100, 2)\n", "print('Logistic Regression: ' + str(acc_log))\n", "\n", "# KNN\n", "knn = KNeighborsClassifier(n_neighbors = 5)\n", "knn.fit(X_train, Y_train)\n", "Y_pred = knn.predict(X_val)\n", "acc_knn = round(accuracy_score(Y_pred, Y_val) * 100, 2)\n", "print('KNN: ' + str(acc_knn))\n", "\n", "# Support Vector Machines (SVM)\n", "svc = SVC(kernel = 'linear', random_state = 0)\n", "svc.fit(X_train, Y_train)\n", "Y_pred = svc.predict(X_val)\n", "acc_svc = round(accuracy_score(Y_pred, Y_val) * 100, 2)\n", "print('SVC: ' + str(acc_svc))\n", "\n", "# Kernel SVM\n", "kernel_svc = SVC(kernel = 'rbf', random_state = 0)\n", "kernel_svc.fit(X_train, Y_train)\n", "Y_pred = kernel_svc.predict(X_val)\n", "acc_kernel_svc = round(accuracy_score(Y_pred, Y_val) * 100, 2)\n", "print('Kernel SVC: ' + str(acc_kernel_svc))\n", "\n", "# Naive Bayes\n", "gaussian = GaussianNB()\n", "gaussian.fit(X_train, Y_train)\n", "Y_pred = gaussian.predict(X_val)\n", "acc_gaussian = round(accuracy_score(Y_pred, Y_val) * 100, 2)\n", "print('Gaussian: ' + str(acc_gaussian))\n", "\n", "# Decision Tree Classification\n", "decision_tree = DecisionTreeClassifier(random_state = 0)\n", "decision_tree.fit(X_train, Y_train)\n", "Y_pred = decision_tree.predict(X_val)\n", "acc_decision_tree = round(accuracy_score(Y_pred, Y_val) * 100, 2)\n", "print('Decision Tree: ' + str(acc_decision_tree))\n", "\n", "# Random Forest Classification\n", "random_forest = RandomForestClassifier(n_estimators=100, random_state = 0)\n", "random_forest.fit(X_train, Y_train)\n", "Y_pred = random_forest.predict(X_val)\n", "random_forest.score(X_train, Y_train)\n", "acc_random_forest = round(accuracy_score(Y_pred, Y_val) * 100, 2)\n", "print('Random Forest: ' + str(acc_random_forest))"], "cell_type": "code", "metadata": {"_uuid": "eb1716d50d5e1a3429b9178c995f8b358f14a549", "_cell_guid": "95be39d4-487d-480d-8ffb-32717451752f"}, "execution_count": null, "outputs": []}, {"source": ["## 6) K-Fold Cross-Validation"], "cell_type": "markdown", "metadata": {"_uuid": "2244c316b0d72b784bfcda9a323d4963b4a0647f", "_cell_guid": "e26e276d-0db0-4757-a01d-6110b5335a3e"}}, {"source": ["# Perform K-Fold Cross-Validation with 10 splits.\n", "kfold = KFold(n_splits=10, random_state=0)\n", "mean=[]\n", "accuracy=[]\n", "std=[]\n", "classifiers=['Logistic Regression',\n", "             'KNN',\n", "             'SVC',\n", "             'Kernel SVC',\n", "             'Gaussian',\n", "             'Decision Tree',\n", "             'Random Forest']\n", "models=[logreg,\n", "        knn,\n", "        svc,\n", "        kernel_svc,\n", "        gaussian,\n", "        decision_tree,\n", "        random_forest]\n", "for i in models:\n", "    model = i\n", "    cv_result = cross_val_score(model, X_train, Y_train, cv = kfold, scoring = \"accuracy\")\n", "    mean.append(cv_result.mean())\n", "    std.append(cv_result.std())\n", "    accuracy.append(cv_result)\n", "    \n", "print(pd.DataFrame({'CV Mean': mean,'Std': std}, index=classifiers))\n", "_, ax = plt.subplots(figsize=(20,10))\n", "sns.boxplot(classifiers, accuracy, ax=ax)\n", "ax.set_title(\"Model Accuracy\")\n", "ax.set_xlabel(\"Model\")\n", "ax.set_ylabel(\"Accuracy\")"], "cell_type": "code", "metadata": {"_uuid": "ca59ce10d2f6a5ce07aa374f32d62677395a012b", "_cell_guid": "6a8d9911-e194-408f-b038-1c0619a0ae71"}, "execution_count": null, "outputs": []}, {"source": ["## 7. Hyper-Parameter Tuning"], "cell_type": "markdown", "metadata": {"_uuid": "000c8b47e080c1da0bbb0555cefd6843d3e00eb0", "_cell_guid": "02a7afde-3707-4e00-9690-aa6708044522"}}, {"source": ["# Tune the hyper-parameters for Kernel SVC, our best performing model.\n", "C = np.logspace(-6, 5, 12)\n", "gamma = np.logspace(-6, 5, 12)\n", "param_grid = {'C': C, 'gamma': gamma}\n", "gd = GridSearchCV(estimator=kernel_svc, param_grid=param_grid, verbose=True)\n", "gd.fit(X_train, Y_train)\n", "print('Hyper-Parameter SVC (Best Score): ' + str(gd.best_score_))\n", "print('Hyper-Parameter SVC (Best Estimator): ' + str(gd.best_estimator_))"], "cell_type": "code", "metadata": {"_uuid": "6a20580040639d53f97fb06cb5039771f717735c", "_cell_guid": "28ecd9f6-24c7-400a-8877-bc11be6f4581"}, "execution_count": null, "outputs": []}, {"source": ["# For good measure, do the same for Logistic Regression, our second best performing model.\n", "C = np.logspace(-6, 5, 12)\n", "param_grid = {'C': C, 'penalty': ['l1', 'l2']}\n", "gd = GridSearchCV(estimator=logreg, param_grid=param_grid, verbose=True)\n", "gd.fit(X_train, Y_train)\n", "print('Hyper-Parameter Logistic Regression (Best Score): ' + str(gd.best_score_))\n", "print('Hyper-Parameter Logistic Regression (Best Estimator): ' + str(gd.best_estimator_))"], "cell_type": "code", "metadata": {"_uuid": "d1e0fd5dbb452b0c484816b08f25b589636ce627", "_cell_guid": "1399e8b2-e765-455e-bfdb-4f1a7d2f377a"}, "execution_count": null, "outputs": []}, {"source": ["## 8. Uploading Submission to Kaggle"], "cell_type": "markdown", "metadata": {"_uuid": "cd0c6cbeb95affb8ebd6b19b96a743817d144c5f", "_cell_guid": "9d88de1f-b87b-40b4-a504-2bfbd67f3a36"}}, {"source": ["# Make our final predictions and export the CSV file.\n", "ids = test['PassengerId']\n", "hyper_param_kernel_svc = SVC(C=100.0, cache_size=200, class_weight=None, coef0=0.0,\n", "  decision_function_shape='ovr', degree=3, gamma=0.01, kernel='rbf',\n", "  max_iter=-1, probability=False, random_state=0, shrinking=True,\n", "  tol=0.001, verbose=False)\n", "hyper_param_kernel_svc.fit(X_train, Y_train)\n", "predictions = hyper_param_kernel_svc.predict(test.drop('PassengerId', axis=1))\n", "output = pd.DataFrame({ 'PassengerId' : ids, 'Survived': predictions })\n", "output.to_csv('submission.csv', index=False)"], "cell_type": "code", "metadata": {"_uuid": "641e4ad40282c2435b7ff4d7d7735675dcfc9c29", "collapsed": true, "_cell_guid": "43634cf6-6501-4c78-90ce-7786f36dfba6"}, "execution_count": null, "outputs": []}], "nbformat": 4}