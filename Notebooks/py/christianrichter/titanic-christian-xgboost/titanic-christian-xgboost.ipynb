{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This script shows you how to make a submission using a few\n# useful Python libraries.\n# It gets a public leaderboard score of 0.76077.\n# Maybe you can tweak it and do better...?\n\nimport pandas as pd\nimport xgboost as xgb\nfrom sklearn.preprocessing import LabelEncoder\nimport numpy as np\n\n# Load the data\ntrain_df = pd.read_csv('../input/train.csv', header=0)\ntest_df = pd.read_csv('../input/test.csv', header=0)\n\n# We'll impute missing values using the median for numeric columns and the most\n# common value for string columns.\n# This is based on some nice code by 'sveitser' at http://stackoverflow.com/a/25562948\nfrom sklearn.base import TransformerMixin\nclass DataFrameImputer(TransformerMixin):\n    def fit(self, X, y=None):\n        self.fill = pd.Series([X[c].value_counts().index[0]\n            if X[c].dtype == np.dtype('O') else X[c].median() for c in X],\n            index=X.columns)\n        return self\n    def transform(self, X, y=None):\n        return X.fillna(self.fill)\n\nfeature_columns_to_use = ['Pclass','Sex','Age','Fare','Parch']\nnonnumeric_columns = ['Sex']\n\n# Join the features from train and test together before imputing missing values,\n# in case their distribution is slightly different\nbig_X = train_df[feature_columns_to_use].append(test_df[feature_columns_to_use])\nbig_X_imputed = DataFrameImputer().fit_transform(big_X)\n\n# XGBoost doesn't (yet) handle categorical features automatically, so we need to change\n# them to columns of integer values.\n# See http://scikit-learn.org/stable/modules/preprocessing.html#preprocessing for more\n# details and options\nle = LabelEncoder()\nfor feature in nonnumeric_columns:\n    big_X_imputed[feature] = le.fit_transform(big_X_imputed[feature])\n\n# Prepare the inputs for the model\ntrain_X = big_X_imputed[0:train_df.shape[0]].as_matrix()\ntest_X = big_X_imputed[train_df.shape[0]::].as_matrix()\ntrain_y = train_df['Survived']\n\n# You can experiment with many other options here, using the same .fit() and .predict()\n# methods; see http://scikit-learn.org\n# This example uses the current build of XGBoost, from https://github.com/dmlc/xgboost\ngbm = xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05).fit(train_X, train_y)\npredictions = gbm.predict(test_X)\n\n\n# Kaggle needs the submission to have a certain format;\n# see https://www.kaggle.com/c/titanic-gettingStarted/download/gendermodel.csv\n# for an example of what it's supposed to look like.\nsubmission = pd.DataFrame({ 'PassengerId': test_df['PassengerId'],\n                            'Survived': predictions })\nsubmission.to_csv(\"submission.csv\", index=False)# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"acc_xgboost = round(gbm.score(train_X, train_y) * 100, 2)\nprint(acc_xgboost)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c713d11b549c3e8c620e0f9ff9db04368c42b317"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}