{"cells":[{"metadata":{"_uuid":"569eac03ac4451f2780571863e51f67995e02a83"},"cell_type":"markdown","source":"**Welkom to my kernel, to play around**\n\nThis kernel was made for Google Colab., it will auto-number submission files   \nIt contains an easy to use structure for column functions and also a handy parameterised gridsearch.    \nSo far my best result has been around 0.79906, and i'm sharing it because i'm curious if any one can improve it.    \n\nThis net does not contain a lot of plotting, because you probaply did allready a lot of comparrisons    \nAnd since neural nets are kind of a blackboxes, why investigate it using diagrams ?.   \n\nThough i did include future engineering, and you could alter the code to make use of it or not (simply drop such a column if you dont want it).   \nVarious tricks are commented out (like advanced dropout methods, and you could easily markout dropout too if you wish.   \n\nIf you can get over **0.79906** please let me know, i'm curious about improvements.   \nThe code was written to have it easily adjustable, so i have no doubts small changes can get higher results.   \nEspecialy future engineering, might get you higher.    \nThere is a family names trick in it, and a a few more, like unknown age and unknown fare.      \nBTW you can drop Age its not a big problem ..   \n\n\n*(PS i'm not a native English species)*"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# if you want to know what kaggle comes whidth \n! pip list  ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4c554b4a9c8870407f3897b67fcf5c11177c4138"},"cell_type":"markdown","source":"**Bind to google colab file system or kaggle**     \nI think this will work on both i tested it with google though."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false,"_kg_hide-input":true},"cell_type":"code","source":"folderpath =\" \"\ntry:\n  # Load the Drive helper and mount\n  from google.colab import drive\n  # This will prompt for authorization.\n  drive.mount('/content/drive')\n  folderpath=\"drive/My Drive/\"\nexcept:\n  import os\n  folderpath =\"../input/\"\nprint(os.listdir(folderpath))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"999e718edb540b7e414fc42958580da38216a189"},"cell_type":"markdown","source":"Data loading and future engering"},{"metadata":{"trusted":true,"_uuid":"fdd10a136a945f23c80e38222b9f2d72be5fb0e4"},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n#header =['PassengerId','Survived','Pclass','Name','Sex','Age','SibSp','Parch','Ticket','Fare','Cabin','Embarked']\ntrainpath = folderpath+\"train.csv\"\ntestpath = folderpath + \"test.csv\"\ndf = pd.read_csv(trainpath)  #to learn from\ndt = pd.read_csv(testpath)   #to test upon later\n\n\nDataSet_Train = df.values\n\ny = DataSet_Train[:,1].astype(int)  #the result truth to learn \nDataset_Final = dt.values\nids = Dataset_Final[:,0].astype(int)# the passsenger ID column of the chalange\n\n\n\n# helper functions for viewing and testing the data :\n\ndef show_table(data,x):\n    print('Sample data entry :',x)\n    for col in df.columns:\n        dol = col #+ \"            \"\n       # dol = dol[:16] \n        print (dol, data[col].values[x-1])\n    print() \n\ndef find_Nan(data,name):\n    print ('Missing values:',name)\n    print(data.isnull().sum(axis=0)) # list columns with Nan values. (axis=1 will show rows)\n    print(data.isnull().sum(axis=1).sum()) \n    print()\n\n    \n\n##--------------- Helper functions for data Engineering------------------------\n# helper functions \ndef checkforindex(text,array): # return array index of match or else -1 in case nan -1 as well\n    i=-1\n    if (pd.isna(text)):\n        #print(\"empty\")\n        return i\n    else:\n        #print(\"found\",text)\n        for item in array:\n            i=i+1\n            if (text.find(item)>=0):\n                return i\n    return -1\n\ndef checkfor(text,array):  # a simple check does a string contain a word from the array\n    if (pd.isna(text)):\n        return False    \n    for item in array:\n        if (text.find(item)>=0):\n            return True\n    return False\n\ndef containsAll(str, set):\n    for c in set:\n        if c not in str: return False;\n    return True \n  \n  \ndef stringbetween(s,a,b):\n  try:\n    named= (s.split(a))[1].split(b)[0]\n    named = named.replace(' ', '')+'.'\n    return named\n  except:\n    return \"?\"\n\n\n##--------------------Data engineering (can we exctract more information from the data..\n#data enginering functions\ndef dating_rank(x): \n    title=x['Name']\n    if (checkfor(title,['Mr.','Mrs.'])):\n        return 2\n    if (checkfor(title,['Don', 'Major', 'Capt', 'Jonkheer', 'Rev', 'Col'])):\n        return 3\n    if (checkfor(title,['Master','Sir'])):\n        return 4\n    if (checkfor(title,['Countess', 'Mme'])):  \n        return 5\n    if (checkfor(title,['Mlle', 'Ms','Miss'])):       #non maried\n        return 6\n    if (checkfor(title,['Dr'])):\n        return 3\n    else:\n        return 4\n      \ndef Named_Title(x):\n    title=x['Name']\n    try:\n      named = stringbetween(title,',','.')\n      return named\n    except:\n      print(title)\n    return '?'\n\n\ndef relations_onboard(x):\n    result = x['SibSp']+x['Parch']\n    return result\n\ndef fam_ideal_team_work(x):\n    result = x['SibSp']+x['Parch']\n    if ((result>1)&(result<5)):\n        return 1\n    return 0\n\n\ndef getdeck(x):\n    deck = x['Cabin']\n    return checkforindex(deck,['A', 'B', 'C', 'D', 'E', 'F', 'G', 'T'])\n\ndef hascabin(x):\n    xn=x['Cabin']\n    c = checkforindex(xn,['A', 'B', 'C', 'D', 'E', 'F', 'G', 'T'])\n#     if (c==0):\n#         return 0\n    if (c==-1):\n        return 0\n    return 1\n\n\ndef getembarkedA(x):\n    harbor=x['Embarked']\n    if (checkforindex(harbor,['C','S','Q'])==0):\n        return 1\n    return 0\ndef getembarkedB(x):\n    harbor=x['Embarked']\n    if (checkforindex(harbor,['C','S','Q'])==1):\n        return 1\n    return 0\ndef getembarkedC(x):\n    harbor=x['Embarked']\n    if (checkforindex(harbor,['C','S','Q'])==2):\n        return 1\n    return 0\n\n\n\n\ndef getsex(x):\n    sex = x['Sex']\n    \n    if (sex==\"female\"):\n        return 1 \n    else:\n        return 0\n    return 1\n\ndef get_survived(x):\n        xo=x['Survived']\n        if (pd.isna(xo)):\n            return 0\n        if (xo==0):\n            return 0\n        return 1\ndef upperclass(x):\n    xo=x['Pclass']\n    if (xo==1):\n        return 1\n    return 0\n\ndef midclass(x):\n    xo=x['Pclass']\n    if (xo==2):\n        return 1\n    return 0\n\ndef lowerclass(x):\n    xo=x['Pclass']\n    if (xo==3):\n        return 1\n    return 0\n\ndef age_fix(x):\n    xn=x['Age']\n    if (xn==0):\n        return 1\n    else:      \n        return 0\n    return 0\n    \ndef cabin3thclass(x):\n    xx=x['Fare_Per_Person']  \n    if (xx<12):\n        return 1\n    return 0\n\ndef cabin2ndclass(x):\n    xx= x['Fare_Per_Person']  \n    if ((xx>=12)&(xx<57)):\n        return 1\n    return 0\ndef cabin1stclass(x):\n    xx= x['Fare_Per_Person']  \n    if (xx>=57):\n        return 1\n    return 0\ndef cabinluxclass(x):\n    xx= x['Fare_Per_Person']  \n    if (xx>=100):\n        return 1\n    return 0        \n\ndef Unknown_Fare(x):\n    xx =x['Fare']\n    if (pd.isna(xx)):\n      return 0\n    if (xx>0):\n      return 0\n    return 1\n  \ndef UnKnownAge(x):\n    xx =x['Age']\n    zz=x['female']\n    try:   #NAN\n      if (xx>0):\n        if(zz==1):\n          return 0\n        else:\n          return -1\n    except:\n      nothing =True\n      \n    if (zz!=1):\n      return 1 \n    else:\n      return 0 # male 'sure dead' is less effective on female\n      \n  \ndef SureName(x):\n  xx=x['Name']\n  sep = ','\n  xx = xx.split(sep, 1)[0]\n  return xx\n  \ndef Survivingfam(x):\n  # https://www.kaggle.com/cdeotte/titanic-using-name-only-0-81818  (Here I converted it to python logic)\n  female = x['female']\n  alive_list = ['Baclini', 'Becker', 'Brown', 'Caldwell', 'Collyer', 'Coutts', 'Doling', 'Fortune', 'Goldsmith', 'Graham', 'Hamalainen',\n             'Harper', 'Hart', 'Hays', 'Herman', 'Hippach', 'Johnson', 'Kelly', 'Laroche', 'Mellinger', 'Moor', 'Moubarek', 'Murphy',\n             'Navratil', 'Newell', 'Nicola-Yarred', 'Peter', 'Quick', 'Richards', 'Ryerson', 'Sandstrom', 'Taussig', 'West', 'Wick']\n  \n  dead_list = ['Barbara' ,'Boulos', 'Bourke', 'Ford', 'Goodwin', 'Jussila', 'Lefebre', 'Palsson',\n             'Panula', 'Rice', 'Sage' 'Skoog', 'Strom', 'Van Impe', 'Vander Planke', 'Zabour']\n\n  if (female!=1):        #male\n    xx=x['Title']\n    if (xx=='Master'):\n      xx=x['SureName']\n      if xx in alive_list:\n        return 1\n  else:                  #female\n    xx=x['SureName']       \n    if xx in dead_list:\n      return 0\n    else:\n      return 1\n  return 0\n\ndef DeadFam(x):\n  dead_list = ['Barbara' ,'Boulos', 'Bourke', 'Ford', 'Goodwin', 'Jussila', 'Lefebre', 'Palsson',\n             'Panula', 'Rice', 'Sage' 'Skoog', 'Strom', 'Van Impe', 'Vander Planke', 'Zabour']\n  xx=x['SureName']\n  if xx in dead_list:\n    return 1\n  return 0\n\ndef travel_alone(x):\n  xx=x['SibSp']\n  xy=x['Parch']\n  if (xx==0):\n    if(xy==0):\n      return 1\n  return 0\n\ndef inverseAge(x):  # just in doubt how a neural net would handle this, a stronger signal for low values maybe?\n  xx=x['Age']\n  inv = 1-xx\n  return inv\n\n\n  \n  \ndef prepare_data(data):\n    data['RelationsOnBoard']=  data.apply(relations_onboard, axis=1)\n    data['DatingRank']= data.apply(dating_rank, axis=1)\n    data['Deck'] = data.apply(getdeck, axis=1)\n    data['female'] = data.apply(getsex,axis=1)\n  \n    data['harborA'] = data.apply(getembarkedA,axis=1)\n    data['harborB'] = data.apply(getembarkedB,axis=1)\n    data['harborC'] = data.apply(getembarkedC,axis=1)\n    data['ClassUp'] = data.apply(upperclass,axis=1)\n    data['ClassMid'] = data.apply(midclass,axis=1)\n    data['ClassLow'] = data.apply(lowerclass,axis=1)\n    data['HasCabin'] = data.apply(hascabin,axis=1)\n    \n    #there are 1 or 2 fare prices missing, i assume they travel cheap\n\n    try :\n        data['Survived'] = data.apply(get_survived,axis=1)\n    except:\n        noSurvivaldata=True\n        \n    data['KnownAge']=data.apply(UnKnownAge,axis=1) \n    data['Parch'].astype('int64')\n    data['SibSp'].astype('int64')\n    \n    data['Family_Size']= data['Parch']+ data['SibSp']\n    \n    data['Fare_Per_Person']=data['Fare']/(data['Family_Size']+1)\n    \n    #https://www.dummies.com/education/history/suites-and-cabins-for-passengers-on-the-titanic/\n    data['Cabin3thclass']= data.apply(cabin3thclass,axis=1)\n    data['Cabin2ndClass']= data.apply(cabin2ndclass,axis=1)\n    data['Cabin1stClass']= data.apply(cabin1stclass,axis=1)\n    data['CabinLuxeryClass']=data.apply(cabinluxclass,axis=1)\n    data['IdealFamSize'] = data.apply(fam_ideal_team_work ,axis=1)    \n    #remove any NAN values left out of safity (shouldnt be there by now)\n    #data.replace(np.nan, 0, inplace=True)\n    \n    #normalization shouldnt cause difference between train and test data, so lets do it manual   >> Later i normalized certain columns (you can remove that if you whish)\n    \n    data['Age']=data['Age']*0.01\n    data['Ageinv']=data.apply(inverseAge,axis=1)\n    \n    data['Fare_Per_Person']=data['Fare_Per_Person']*0.001\n    data['DatingRank']=data['DatingRank']*0.15\n    data['Family_Size']=data['Family_Size']*0.02  # max 50\n    data['Deck']=data['Deck']*0.025 + 0.025\n    data['Fare']=data['Fare']*0.0001\n    data['RelationsOnBoard']=data['RelationsOnBoard']*0.02\n    data['SibSp']=data['SibSp']*0.02\n    data['Parch']=data['Parch']*0.02\n    \n    data['Title'] = data.Name.apply(lambda name: name.split(',')[1].split('.')[0].strip())\n    \n    data['SureName']= data.apply(SureName,axis=1)\n    \n    data['Survivingfam']=data.apply(Survivingfam,axis=1)\n    data['DeadFam']=data.apply(DeadFam,axis=1)\n    data['Travel_Alone']=data.apply(travel_alone,axis=1) \n    data['Unknown_Fare']= data.apply(Unknown_Fare,axis=1)\n    \n    data.drop(columns=['Name','Sex','Cabin','Embarked','Pclass'], inplace=True)\n    data.drop(columns=['Ticket'], inplace=True)\n    \n    data['Deck']=data['Deck'].round(4)\n    data['Age']=data['Age'].round(3)    \n    return data\n##\n\ndt= prepare_data(data=dt)        \ndf =prepare_data(data=df)\nshow_table(df,2)\ndf.dtypes","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"720a59d76d9861400f29e4c99e8ba47c630f9ba1"},"cell_type":"markdown","source":"**Future engineering, and data optimization.**     \nSeveral tricks are inside this code some are outmarked while others are not.    \nI find the Age estimating interesting, its averaged on similair people, ea males of same harbor, same class, and same title.   \nSince not all groups contained usedfull info the group splitting gets reduced in a few steps untill all ages are estimated.    \nDespite that tough i'd like to point out that none of the males widht unknown ages survived, but i kept track of that allready in the upper code block.   \nA small plot is included about age distributin, despite i didnt want to overwhelm with plots here.  "},{"metadata":{"trusted":true,"_uuid":"b10e60a3ddec2acb7b126c2641ac091e18164755"},"cell_type":"code","source":"#find_Nan(df,'dc nan entries')\n\ntry:\n    df.drop('Survived', axis=1, inplace=True)\n    dc =  df.append(dt) \nexcept:\n    print('Survived likely deleted in previou run of this cell')\n\n\ndc['Age'].replace(0,np.NaN)\ndc['Age']  = dc.groupby(['female','ClassUp','ClassMid','ClassLow','SibSp','Parch','harborA','harborB','harborC','Title'])['Age'].transform(lambda x: x.fillna(x.median()))\ndc['Age'].replace(0,np.NaN)\ndc['Age']  = dc.groupby(['female','ClassUp','ClassMid','ClassLow','SibSp','Parch','Title'])['Age'].transform(lambda x: x.fillna(x.median()))\ndc['Age'].replace(0,np.NaN)\ndc['Age']  = dc.groupby(['female','ClassUp','ClassMid','ClassLow','SibSp','Parch'])['Age'].transform(lambda x: x.fillna(x.median()))\ndc['Age'].replace(0,np.NaN)\ndc['Age']  = dc.groupby(['female','Title'])['Age'].transform(lambda x: x.fillna(x.median()))\n\n# The fact that we dont know age information from some people, and not from others, is that fact itself informative too ?.\n# I made allready an AgeUnknown column, now where that is 1 or 0 i'll move value of dc['Age'] into it and make age zero.\n# The thinking is some people who survived we know age of, so knowing age might be also hide a survival factor information.\n\n# for index, row in dc.iterrows():\n#   u = row['AgeUnknown']\n#   if (u>0):\n#     w = dc.at[index,'Age']\n#     dc.at[index,'AgeUnknown']=w\n#     dc.at[index,'Age']=0\n\n# for index in dc.index:\n#   rr = dc.at[index,'AgeUnknown']\n#   r=rr.item(0)\n#   if (r>0):\n#       w= dc.at[index,'Age'].item(0)\n#       dc.at[index,'AgeUnknown']=w\n#       dc.at[index,'Age']=0\n\n# # indexes where column AgeUnknown is >0\n# inds = dc[dc['AgeUnknown'] > 0].index.tolist()\n# # change the indexes of AgeUnknown to to the Age column\n# df.loc[inds, 'AgeUnknown'] = dc.loc[inds, 'Age']\n# # change the Age to 0 at those indexes\n# dc.loc[inds, 'Age'] = 0\n\ndc['Ageinv']=dc.apply(inverseAge,axis=1)\n\n\ndc['Fare'].replace(0,np.NaN)\ndc['Fare'] =            dc.groupby(['female','DatingRank','harborA','harborB','harborC',\n                                    'ClassUp','ClassMid','ClassLow','Family_Size','Title' ])['Fare'].transform(lambda x: x.fillna(x.median()))\n\ndc['Fare_Per_Person'].replace(0,np.NaN)\ndc['Fare_Per_Person'] = dc.groupby(['female','DatingRank','harborA','harborB','harborC',\n                                    'ClassUp','ClassMid','ClassLow','Family_Size','Title' ])['Fare_Per_Person'].transform(lambda x: x.fillna(x.median()))\n\ndc['Fare'].replace(0,np.NaN)\ndc['Fare'] =            dc.groupby(['ClassUp','ClassMid','ClassLow','Family_Size' ])['Fare'].transform(lambda x: x.fillna(x.median()))\n\ndc['Fare_Per_Person'].replace(0,np.NaN)\ndc['Fare_Per_Person'] = dc.groupby(['ClassUp','ClassMid','ClassLow','Family_Size'])['Fare_Per_Person'].transform(lambda x: x.fillna(x.median()))\n\n\n#dc['Fare_Per_Person_Rounded'] = dc['Fare_Per_Person'].round(3)  \n\nfor index in dc.index:\n    rr = dc.at[index,'Unknown_Fare']\n    r=rr.item(0)\n    if (r>0):\n      w= dc.at[index,'Fare_Per_Person']\n      dc.at[index,'Unknown_Fare']=w\n      dc.at[index,'Fare_Per_Person']=0\n\ndc.drop(['PassengerId'], axis=1, inplace=True) #they're kinda redundant now    \n    \nnormalized_titles = {\n    \"Capt\":       \"Officer\",\n    \"Col\":        \"Officer\",\n    \"Major\":      \"Officer\",\n    \"Jonkheer\":   \"Royalty\",\n    \"Don\":        \"Royalty\",\n    \"Sir\" :       \"Royalty\",\n    \"Dr\":         \"Officer\",\n    \"Rev\":        \"Officer\",\n    \"the Countess\":\"Royalty\",\n    \"Dona\":       \"Royalty\",\n    \"Mme\":        \"Mrs\",\n    \"Mlle\":       \"Miss\",\n    \"Ms\":         \"Mrs\",\n    \"Mr\" :        \"Mr\",\n    \"Mrs\" :       \"Mrs\",\n    \"Miss\" :      \"Miss\",\n    \"Master\" :    \"Master\",\n    \"Lady\" :      \"Royalty\"\n}\n    \ndc.Title = dc.Title.map(normalized_titles)\n\n# turn titles into boolean columns\n# https://stackoverflow.com/questions/36544089/pandas-create-boolean-columns-from-categorical-column/36544125\n\ndef normalize_column (data, name):\n  data[name]=((data[name]-data[name].min())/(data[name].max()-data[name].min()))\n  return data\n\ndc =normalize_column(dc,'Fare')\ndc =normalize_column(dc,'Age')\ndc =normalize_column(dc,'Deck')\ndc =normalize_column(dc,'Fare_Per_Person')\ndc =normalize_column(dc,'SibSp')\ndc =normalize_column(dc,'Parch')\ndc =normalize_column(dc,'RelationsOnBoard')\n\ndr = dc.Title.str.get_dummies()\ndr.columns = ['is_'+col for col in dr.columns]\n\ndc.reset_index(drop=True, inplace=True)\ndr.reset_index(drop=True, inplace=True)\ndc = pd.concat([dc,dr],axis=1)\n\nimport math\ndef age_inverted(x):\n  a = x['Age']\n  return 1-math.sqrt(a)\n  \n  \n\ndc['Age']= dc.apply(age_inverted,axis=1)\n\nplotdata =dc['Age']\nplt.hist(plotdata*100)\nplt.title(\"Gaussian Histogram\")\nplt.xlabel(\"Age-group\")\nplt.ylabel(\"Frequency\")\nplt.hist(plotdata*100, bins=40)\nplt.hist(plotdata*100, bins=160)\nplt.show()\n\n# creating all family names (is a bit to much try to do it only for certain family size.)\n\n\n# fam = pd.get_dummies(dc.SureName)\n# dc.reset_index(drop=True, inplace=True)\n# fam.reset_index(drop=True, inplace=True)\n# dc = pd.concat([dc,fam],axis=1)\n\n# Nice function but it resulted in a terrible score..\n# dc['CatFare']=pd.qcut(dc.Fare_Per_Person,q=8,labels=False)\n# dc['CatAge']=pd.qcut(dc.Age,q=15,labels=False)\n\n\ndc.drop(['Title','DatingRank','SureName'], axis=1, inplace=True) \n\nX = dc.iloc[:891]\nT  = dc.iloc[891:]\n# y is allready created in the first jupyter cell.\n\ntry:\n    dc.to_csv(\"drive/My Drive/corrected.csv\",index=False)  # to see all missing files\nexcept:\n    print('Warning its likely corrected.csv file is in use and so it cannot be updated')\n\nfind_Nan(dc,'dc nan entries')\nprint(dc.shape)\nfrom IPython.core.display import HTML\ndisplay(HTML(df[0:8].to_html()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"61f272e8d5068af95b93ab584cb75bff9c326f56"},"cell_type":"markdown","source":"**Dealing with Test and Train data**    \nSplit the data in X train and T test  ( y = Test results from earlier code above.     \nI also provide a X_train and X_test and y_train and y_test.    \n\nYou're free to use that, instead though i rather use all the Test data \nSo to fit a model width all known data, and then later compare various neural nets against to get the best match."},{"metadata":{"trusted":true,"_uuid":"3990bf5bd3b3f20a38b334b189c45d9fec1af99e"},"cell_type":"code","source":"dc.drop(['SibSp','RelationsOnBoard'], axis=1, inplace=True) #if you want you can drop more columns here, droping Age is no problem !\n\nLzero =[len((dc.columns))]\nprint (Lzero)\n\nX = dc.iloc[:891]\nT  = dc.iloc[891:]\nfrom sklearn.model_selection import train_test_split\n\n# Despite that i wont use this, essentially i will choose best network by as much known data (to fit the best classifier).\n# I keep it in here in case you would like to use it.\nX_train, X_test, y_train, y_test = train_test_split(X, y,  random_state=41)   \nprint (X_train.shape)\nprint (X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)\npd.set_option('display.max_columns', 40)\npd.set_option('display.width', 400)\ndisplay(dc[:17])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c8e91871bb41d18d5760f7f5ee6bd43feff20b24"},"cell_type":"markdown","source":"**A simple deep neural net**\n2 hidden layers, usually thats enough for small problems, and so far it seamed L1 works best with 17 and L2 width 7   \nBut you can change all kind of paramaters and compare the best against eachother  \n"},{"metadata":{"trusted":true,"_uuid":"013020f3faeca4b38fa91aa0694ca9199fe44a17"},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense,Dropout,advanced_activations\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.optimizers import Adam\nfrom sklearn.model_selection import GridSearchCV, KFold\n\n# Define a random seed, so all nets start the same (so you can compare them better)\nseed = 6  \nnp.random.seed(seed)\n\n# Start defining the model\ndef create_model(L0,L1,L2,loss,optimizer,dropout_rate,activationfunction):\n    # create model\n    model = Sequential()\n    model.add(Dense(L1, input_dim =L0, kernel_initializer='normal', activation='relu'))\n    \n#     act = advanced_activations.PReLU(weights=None, alpha_initializer=\"zero\")\n#     model.add(Dense(L1, input_dim=31, kernel_initializer=\"uniform\"))\n#     model.add(act)\n    \n    model.add(Dropout(dropout_rate))\n    model.add(Dense(L2, input_dim = L1, kernel_initializer='normal', activation='relu'))\n#     act = advanced_activations.PReLU(weights=None, alpha_initializer=\"zero\")\n#     model.add(Dense(L2, input_dim=31, kernel_initializer=\"uniform\"))\n#     model.add(act)\n    \n    model.add(Dense(1, activation='sigmoid')) # Use sigmoid for classification answers (ea 1 or 0 )\n    \n    # compile the model\n    adam = Adam(lr = 0.01)\n    model.compile(loss = loss, optimizer = optimizer, metrics = ['accuracy'])\n    return model\n\n# create the model\nmodel = KerasClassifier(build_fn = create_model)\n\n\n#sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n#model.compile(loss='binary_crossentropy', optimizer='rmsprop',  metrics=['accuracy'],n_jobs=-1)   #n-jobs-1  seems to crash google colab, unsure for kaggle\n\n# in an array diffrent parameters can be givven to compare different neural net settings\n\nLzero =[len((dc.columns))]\nparameters_set2 ={'L0': Lzero,             # dont alter it automatically adjust to the data column size\n                  'L1': [17],              # [7,11,15,17,25]\n                  'L2': [7],               # [3,5,7,9,11]\n                  'dropout_rate':[0.250],  #[0.0,2,2.25,0.5],\n                  'batch_size': [32],      #[32,64,128 ], \n                  'epochs': [200,50],       #[2000,1000,800000]\n                  'loss': ['binary_crossentropy'],   #['rmsprop','Nadam','Adamax','Adadelta','Adam'], \n                  'optimizer': ['rmsprop'],          #note usually relu is best.\n                  'activationfunction' : ['relu']    # ['relu','tanh','logistic'] \n                  \n                  #'verbose': [3]\n                 }\n\n#if you got something good you can create new parameter sets easily \ngrid = GridSearchCV(model, parameters_set2)  \n\n# how long is your coding running... you might want to know so display the current time\nimport time\nlocaltime = time.asctime( time.localtime(time.time()) )\nprint (\"Local current time :\", localtime)\ngrid.fit(X,y,verbose=0)   # verbose info is nice, but it slows down a lot as well.\n\nbest_est = grid.best_estimator_\n\nlocaltime = time.asctime( time.localtime(time.time()) )\nprint (\"Local current time :\", localtime)\nprint(best_est)\n\ntrainscore = best_est.predict(X)\npredictions = best_est.predict(T)  #remind that T is our Kaggle chalange our unknown set...\n\nsubmissions = pd.DataFrame({'PassengerId' : ids })\nsubmissions['Survived']=predictions[:,0]\n\ni=1\n# Incremental file saving of results\nimport os\noutputname = folderpath+\"submit_v\"\nfname = outputname+str(i)+\".csv\"\nprint(\"prepare saving..\")\nwhile os.path.exists(fname):\n    i += 1\n    fname = outputname+str(i)+\".csv\"\nsubmissions.to_csv(fname,index=False)\n\n\n# Showing results\nprint (\"Done, results are in \",fname)\nprint(grid.best_params_)   #showing best parameters so you can refine it later.\nprint()\nkk=dc.columns.get_values().tolist()\nmakeitastring = ' '.join(map(str, kk))\nprint(\"Used :\",makeitastring)\nprint()\nprint(submissions)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}