{"cells":[{"metadata":{"_cell_guid":"91a3eeb0-6f64-ec2b-9eab-47d28cf72264","_uuid":"08c96401d6f554df6ac407acb668bce0f258a6d8","trusted":false},"cell_type":"code","source":"import pandas as pd\nimport xgboost as xgb\nfrom sklearn.preprocessing import LabelEncoder\nimport numpy as np\n\n# Load the data\ntrain_df = pd.read_csv('../input/train.csv', header=0)\ntest_df = pd.read_csv('../input/test.csv', header=0)\n\nfeature_columns_to_use = ['Pclass','Sex','Age','Fare','Parch']\n\n# Join the features from train and test together before imputing missing values,\n# in case their distribution is slightly different\nbig_X = train_df[feature_columns_to_use].append(test_df[feature_columns_to_use])\n\n# Fare column is just one value empty so we will fill it with median value\nbig_X['Fare'] = big_X['Fare'].fillna(big_X['Fare'].median())\n\n# Creating probabilty distribution for ages from the existing column values\nages_probabilities = big_X['Age'].value_counts().to_frame()\nages_probabilities['index1'] = ages_probabilities.index\nages_probabilities = ages_probabilities.rename(columns={'Age': 'Count', 'index1': 'Age'})\nages_probabilities = ages_probabilities.reindex_axis(['Age','Count'], axis=1)\nages_probabilities = ages_probabilities.reset_index()\nages_probabilities = ages_probabilities.drop([\"index\"],axis=1)\nages_probabilities['Probability'] = ages_probabilities['Count'] / big_X['Age'].value_counts().sum()\n\ninput_ages_list = ages_probabilities['Age'].values.tolist()\nprops_ages_list = ages_probabilities['Probability'].values.tolist()\nnewAges = np.random.choice(input_ages_list, big_X['Age'].isnull().sum(), props_ages_list)\n\n# fill Ages null values with this distribution\nAgeNulls = big_X[pd.isnull(big_X['Age'])]\nfor i, ni in enumerate(AgeNulls.index[:len(newAges)]):\n    big_X['Age'].loc[ni] = newAges[i]\n\nbig_X_imputed = big_X\n\n# XGBoost doesn't (yet) handle categorical features automatically, so we need to change\n# them to columns of integer values.\n# See http://scikit-learn.org/stable/modules/preprocessing.html#preprocessing for more\n# details and options\nle = LabelEncoder()\nbig_X_imputed['Sex'] = le.fit_transform(big_X_imputed['Sex'])\n\n# Prepare the inputs for the model\ntrain_X = big_X_imputed[0:train_df.shape[0]].as_matrix()\ntest_X = big_X_imputed[train_df.shape[0]::].as_matrix()\ntrain_y = train_df['Survived']\n\n# You can experiment with many other options here, using the same .fit() and .predict()\n# methods; see http://scikit-learn.org\n# This example uses the current build of XGBoost, from https://github.com/dmlc/xgboost\ngbm = xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05).fit(train_X, train_y)\npredictions = gbm.predict(test_X)\n\n# Kaggle needs the submission to have a certain format;\n# see https://www.kaggle.com/c/titanic-gettingStarted/download/gendermodel.csv\n# for an example of what it's supposed to look like.\nsubmission = pd.DataFrame({ 'PassengerId': test_df['PassengerId'],\n                            'Survived': predictions })\nsubmission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"_change_revision":0,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}