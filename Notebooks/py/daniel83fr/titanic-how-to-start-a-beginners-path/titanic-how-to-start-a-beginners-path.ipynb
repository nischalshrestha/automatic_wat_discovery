{"cells":[{"metadata":{"_cell_guid":"4d7098ce-32d6-4c0e-9aa5-be7d115a537d","_uuid":"eb1a269cb49f8c9b20d3325e407bcb0aa3b46498"},"cell_type":"markdown","source":"# Introduction\nThe purpose of this kernel is to formalize the different steps I've been through to understand the Titanic dataset,\nand few firsts submissions into Kaggle competition.\n*Note that i'm newbie, and it may not be the best way to achieve my goal (even maybe not the right way...)*\n\n(work in progress... all comments are welcome,.)\n\n# Data Overview\nFirst we will analyse the different files that are given, to see what is expected.\n\n## Loading Data\nWe have three files:\n- train.csv\n- test.csv\n- gender_submission.csv (a sample submission file)"},{"metadata":{"collapsed":true,"_cell_guid":"27fe5a63-4840-4b2b-86c2-f19aa9a1610c","_uuid":"9c616f63124ab568ef4725772ffa19c22642c9a4","trusted":true},"cell_type":"code","source":"import pandas as pd\npd.options.mode.chained_assignment = None #To hide some warnings\n# Supress unnecessary warnings so that presentation looks clean\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport seaborn as sns\n\n# Print all rows and columns\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\n\nTRAIN_FILE = \"../input/train.csv\"\nTEST_FILE = \"../input/test.csv\"\nSUBMISSION_FILE = \"../input/gender_submission.csv\"","execution_count":7,"outputs":[]},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5"},"cell_type":"markdown","source":"\n\n### Train Data\n\n"},{"metadata":{"collapsed":true,"_cell_guid":"10ff7e86-4dec-4658-b0a3-5031dd6d66cd","_uuid":"ed4d2350272da197afc6be9616d5e2cfe81bab3e","trusted":false},"cell_type":"code","source":"train_data = pd.read_csv(TRAIN_FILE)\ntrain_data.info()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"8954cd84-4283-47d0-9c08-82108d1f0954","_uuid":"f1dd9a91c22f16283f79d6728a03a9495442f971","trusted":false},"cell_type":"code","source":"train_data = pd.read_csv(TRAIN_FILE)\n\nprint(\"# 5 First lines\")\nprint(train_data[1:6].to_csv(index=False))\n\nprint(\"# Data Infos\")\ntrain_data.info()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"72569d66-170d-4ed5-a2cb-0d71b8eace76","_uuid":"f422364650080a0b1483aa72be0e734373a16dfd"},"cell_type":"markdown","source":"- The following columns contains null values :** Age, Cabin, Embarked**\n- The following columns contains text values : ** Name, Sex, Ticket, Cabin, Embarked **\nThese data will have to be cleaned before we can use them\n\n### Test Data\nwe will do the same analyse with the test data"},{"metadata":{"collapsed":true,"_cell_guid":"bff742d2-bf09-4ede-ab9a-5dc07eef3d95","_uuid":"eb97b1713cca14726c77e91e4e34717a053ef7aa","trusted":false},"cell_type":"code","source":"test_data = pd.read_csv(TEST_FILE)\n\nprint(\"# 5 First lines\")\nprint(test_data[1:6].to_csv(index=False))\n\nprint(\"# Data Infos\")\ntest_data.info()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bf468820-65cf-405e-8e3f-d7ed662a0e1c","_uuid":"ff53ff479e48d0001a9649e9aa098592808c7b7a"},"cell_type":"markdown","source":"Test data doesn't contain the Survived column as it is the result.\nElse data is in the same format.\n- ** Fare** column contains a null value in test data, but not in train data."},{"metadata":{"_cell_guid":"201622cb-8d11-4ec6-a8ed-441d165ea9a7","_uuid":"22bcbe85adf1113202a7a1ee38536c23381f6094"},"cell_type":"markdown","source":"### Submisson Data"},{"metadata":{"collapsed":true,"_cell_guid":"17020019-eb79-40f3-b8cc-ed8707ddc31c","_uuid":"bc886bc4d4f59e64c01a431e58cbb7b5d20bedad","trusted":false},"cell_type":"code","source":"submission_data = pd.read_csv(SUBMISSION_FILE)\n\nprint(\"# 5 First lines\")\nprint(submission_data[1:6].to_csv(index=False))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8a13f21c-95fc-466e-8cac-77a64870ffdc","_uuid":"a9dd4e07f95de6e866f08fde022de8224aad9b00"},"cell_type":"markdown","source":"The expected format is straight forward, for a list of passenger (passengerId) we want to know if they survived (1) or not (0)."},{"metadata":{"_cell_guid":"a9b87310-a9b7-43ab-80bf-0adb4f6c3b0f","_uuid":"4ef54d0efa45c550016fa17709bd03394ab78de4"},"cell_type":"markdown","source":"# Data Prediction\nNow we can start to arrange the data in order to do the prediction.\nFor this purpose, we will use Scikit-learn library.\nFor the moment, we will choose one of the model that sounds suitable for the exerice => **KNeighborsClassifier**\nThis page can be useful : http://scikit-learn.org/stable/tutorial/machine_learning_map/index.html\n\nNote: This model requires all data to be not null and numerical values. We will also need the data to be normalized.\n\nwe will split our train data using train_test_split and validate them using cross_val_score.\nFirst we will add a useless column with same value (0), and check how the model performs.\n"},{"metadata":{"collapsed":true,"_cell_guid":"29dd9244-8f02-4f7d-b80a-52f355ed2849","_uuid":"54789b6c82f6d46112ae31f2862b0eb8fa1b0cec","trusted":true},"cell_type":"code","source":"# Required imports\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn import preprocessing","execution_count":13,"outputs":[]},{"metadata":{"_cell_guid":"da1ea20a-086f-4866-8a47-b3f2c8ed60bd","_uuid":"37f2453858614ff192bc838c962baefc757c82ab","trusted":true},"cell_type":"code","source":"def create_useless_column(data):\n    data[\"UselessColumn\"] = 0\n    data_new = data[[\"UselessColumn\"]]\n    return data_new\n\ndef extract_survived(data):\n    return data[\"Survived\"]\n\ndef apply_model(data, data_label):\n    model = KNeighborsClassifier(n_neighbors=2)\n    scores = cross_val_score(model, data, data_label, cv=5, verbose=1, scoring='accuracy')\n    print(scores.mean())\n\ndata_label = extract_survived(train_data)\ndata = create_useless_column(train_data)\napply_model(data, data_label)","execution_count":14,"outputs":[]},{"metadata":{"_cell_guid":"70ebd71f-5ef2-4dc0-a94e-5d88f58ab7ab","_uuid":"4a0cf23d239bf286c293429a47d549e71df18268"},"cell_type":"markdown","source":"## PassengerId\n"},{"metadata":{"trusted":true,"_uuid":"1f0ebebc9eb8e238cb3693dd4df8ab657d91e17e"},"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\ntrain_data = pd.read_csv(TRAIN_FILE)\ntest_data = pd.read_csv(TEST_FILE)\n\n\nprint(\"Train: {} null (on {})\".format(test_data[\"PassengerId\"].isnull().sum(), len(test_data)))\nprint(\"Test: {} null (on {})\".format(train_data[\"PassengerId\"].isnull().sum(), len(train_data)))\nprint(\"\")\n\nprint(\"Correlation:\")\ncorr = train_data[[\"PassengerId\", \"Survived\"]].corr()\n\nprint(corr)\nsns.heatmap(corr, \n            xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values, vmin=0, center=0)","execution_count":45,"outputs":[]},{"metadata":{"_uuid":"fa2b2e3cfc66951f7f8ae7158af0c410e2d8b09e"},"cell_type":"markdown","source":"There is no correlation between PassengerId and Survived.\nWe can drop the feature"},{"metadata":{"trusted":true,"_uuid":"67c4f7fb156b6aba665d4b6e36e94449bfd520a1"},"cell_type":"code","source":"def drop_survived(data):\n    return data.drop(\"Survived\", axis=1, errors=\"ignore\")\n\ndef drop_passenger_id(data):\n    return data.drop(\"PassengerId\", axis=1, errors=\"ignore\")\n\ntrain_data = pd.read_csv(TRAIN_FILE)\ndata = train_data\ndata = drop_survived(data)\ndata = drop_passenger_id(data)[[\"Fare\"]]\napply_model(data, data_label)","execution_count":38,"outputs":[]},{"metadata":{"_cell_guid":"804cc80c-0081-4b40-9f71-54bb9dc404eb","_uuid":"2cd1ad326259bebd4a81776ad6f2db73a2ecbe98"},"cell_type":"markdown","source":"## Pclass\nPclass represent the ticket class for the passengers\n> - TRAIN: Pclass       891 non-null int64\n> - TEST: Pclass         418 non-null int64"},{"metadata":{"trusted":true,"_uuid":"9af18aab099750a8859505d293e62d23e2718319"},"cell_type":"code","source":"print(\"Train: {} null (on {})\".format(test_data[\"Pclass\"].isnull().sum(), len(test_data)))\nprint(\"Test: {} null (on {})\".format(train_data[\"Pclass\"].isnull().sum(), len(train_data)))\nprint(\"\")\n\nprint(\"Correlation:\")\npclass = train_data[[\"Pclass\", \"Survived\"]]\npclass[\"Class1\"] = (pclass[\"Pclass\"] == 1).astype(int)\npclass[\"Class2\"] = (pclass[\"Pclass\"] == 2).astype(int)\npclass[\"Class3\"] = (pclass[\"Pclass\"] == 3).astype(int)\npclass = pclass.drop(\"Pclass\", axis=1)\ncorr = pclass.corr()\nprint(corr)\n\nsns.heatmap(corr, \n            xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values, center=0)","execution_count":62,"outputs":[]},{"metadata":{"_cell_guid":"02e976a1-72b8-43e4-9522-644e95a2227f","_uuid":"cedc630e42e82c86a23743ccf57dd8028ff8be88","trusted":true},"cell_type":"code","source":"groups = test_data.groupby(['Pclass']).size()\ngroups.plot.bar()","execution_count":46,"outputs":[]},{"metadata":{"_cell_guid":"acc7c01b-ac4a-408e-a529-f1b63cdd9c88","_uuid":"dd2d1aa3ef4ca19e2c31e1f830be2864448dae46"},"cell_type":"markdown","source":"models expects data to be normalized in order to perform better.\nSo we will try to normalize each features.\n"},{"metadata":{"collapsed":true,"_cell_guid":"ed95e978-6314-441d-8767-27bff7f5c2e5","_uuid":"6ec747a2fbe4cd65542808ac708abaf27f7f4a90","trusted":false},"cell_type":"code","source":"data = train_data.copy()\ndata[\"Pclass\"] = data[\"Pclass\"] - 1\ndata[\"Pclass\"] =  preprocessing.maxabs_scale(data[\"Pclass\"])\nprint(data[\"Pclass\"].value_counts())","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"84786c9d-d1db-4073-b0ac-678cd7fca848","_uuid":"720befb8cbf2381565155520301581a97e4866b0","trusted":false},"cell_type":"code","source":"def handle_pclass(data):\n    new_data = data\n    new_data[\"Pclass\"] = new_data[\"Pclass\"] -1\n    new_data[\"Pclass\"] = preprocessing.maxabs_scale(data[\"Pclass\"])\n    return new_data\n\ndata = train_data.copy()\ndata = drop_survived(data)\ndata = drop_passenger_id(data)\ndata = handle_pclass(data)[[\"Pclass\"]] #Note: We will activate features one by one to avoid errors with columns that are not numerical\napply_model(data, data_label)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"fb19bf2a-a74a-4218-b38d-24ad3644d5d9","_uuid":"e0d7d36ec19e6ddbdbf4bf867f27351680a6f88c"},"cell_type":"markdown","source":"## Name\n"},{"metadata":{"_cell_guid":"07e6ab42-0155-4015-94f0-7beba3d38587","_uuid":"6f2327871d05a8f0841bfcb040f6340b42af899e","trusted":true},"cell_type":"code","source":"names = train_data[\"Name\"]\nprint(names[1:10])\n\nprint(\"Train: {} null (on {})\".format(test_data[\"Name\"].isnull().sum(), len(test_data)))\nprint(\"Test: {} null (on {})\".format(train_data[\"Name\"].isnull().sum(), len(train_data)))\nprint(\"\")","execution_count":108,"outputs":[]},{"metadata":{"_cell_guid":"4234448a-f69f-423f-94fd-3d659bd2541a","_uuid":"790939dd1eb44e959a5268e3494740bc97aa2acf"},"cell_type":"markdown","source":"The format seem to be *LastName, Title. firstname (maiden name)*\nFamily name could be interesting to guess origins, or group passengers by family. But it would be pretty difficult to use.\nThe title could be an intersting information to extract"},{"metadata":{"trusted":true,"_uuid":"68e3b62e1587976dab9d44c834195aeeb481d109"},"cell_type":"code","source":"data = train_data.copy()\ndata[\"Name\"] = data[\"Name\"].str.replace(\".\",\";\")\ndata[\"Name\"] = data[\"Name\"].str.replace(\",\",\";\")\ndata[\"Name\"] = data[\"Name\"].str.split(';', expand=True)[1]\n\nunique1 = data[\"Name\"].unique()\nprint(unique1)\n\ndata2 = test_data.copy()\ndata2[\"Name\"] = data2[\"Name\"].str.replace(\".\",\";\")\ndata2[\"Name\"] = data2[\"Name\"].str.replace(\",\",\";\")\ndata2[\"Name\"] = data2[\"Name\"].str.split(';', expand=True)[1]\nunique2 = data2[\"Name\"].unique()\nprint(unique2)\n\nfor i in unique2:\n    if not unique1.__contains__(i):\n        print(\"Missing:\" + i)\n\ncorr = data[[\"Name\",\"Survived\"]].corr()\nprint(corr)\ndumies = pd.get_dummies(data[\"Name\"])\ndumies[\"Survived\"] = data[\"Survived\"]\n\nsns.heatmap(corr, \n            xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values, center=0)\n\ndata[[\"Survived\"]].groupby([data[\"Name\"], data[\"Pclass\"], data[\"Sex\"], data[\"Survived\"]]).count()","execution_count":124,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"2be9f2dd-ec04-4067-8962-59d2b1d5bf39","_uuid":"01a01def6edc49575c2997e7a35f4eafd08d418b","trusted":false},"cell_type":"code","source":"data = test_data.copy()\ndata[\"Name\"] = test_data[\"Name\"].str.replace(\".\",\";\")\ndata[\"Name\"] = data[\"Name\"].str.replace(\",\",\";\")\ndata[\"Name\"] = data[\"Name\"].str.split(';', expand=True)[1]\ndata[\"Name\"] = data[\"Name\"].str.replace(\"Capt\",\"Mr\")\ndata[\"Name\"] = data[\"Name\"].str.replace(\"Col\",\"Mr\")\ndata[\"Name\"] = data[\"Name\"].str.replace(\"Don\",\"Mr\")\ndata[\"Name\"] = data[\"Name\"].str.replace(\"Dr\",\"Mr\")\ndata[\"Name\"] = data[\"Name\"].str.replace(\"Jonkheer\",\"Mr\")\ndata[\"Name\"] = data[\"Name\"].str.replace(\"Rev\",\"Mr\")\ndata[\"Name\"] = data[\"Name\"].str.replace(\"Sir\",\"Mr\")\ndata[\"Name\"] = data[\"Name\"].str.replace(\"Mme\",\"Mrs\")\ndata[\"Name\"] = data[\"Name\"].str.replace(\"Mlle\",\"Miss\")\ndata[\"Name\"] = data[\"Name\"].str.replace(\"the Countess\",\"Mme\")\ndata[\"Name\"] = data[\"Name\"].str.replace(\"Mme\",\"Mrs\")\ndata[\"Name\"] = data[\"Name\"].str.replace(\"Ms\",\"Mrs\")\ndata[\"Name\"] = data[\"Name\"].str.replace(\"Major\",\"Mr\")\ndata[\"Name\"] = data[\"Name\"].str.replace(\"Master\",\"Mr\")\ndata[\"Name\"] = data[\"Name\"].str.replace(\"Lady\",\"Miss\")\ngroups = data.groupby(\"Name\").size()\ngroups.plot.bar()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"dc90e24c-aacb-4d55-8c72-1bf78f783671","_uuid":"4845e7128cfc94dac7efb6cd8b65bf188dd38a1f","trusted":false},"cell_type":"code","source":"def handle_name(data):\n    new_data = data\n    new_data[\"Name\"] = data[\"Name\"]\n    new_data[\"Name\"] = new_data[\"Name\"].str.replace(\".\",\";\")\n    new_data[\"Name\"] = new_data[\"Name\"].str.replace(\",\",\";\")\n    new_data[\"Name\"] = new_data[\"Name\"].str.split(';', expand=True)[1]\n    new_data[\"Name\"] = new_data[\"Name\"].str.replace(\"Capt\",\"Mr\")\n    new_data[\"Name\"] = new_data[\"Name\"].str.replace(\"Col\",\"Mr\")\n    new_data[\"Name\"] = new_data[\"Name\"].str.replace(\"Don\",\"Mr\")\n    new_data[\"Name\"] = new_data[\"Name\"].str.replace(\"Dr\",\"Mr\")\n    new_data[\"Name\"] = new_data[\"Name\"].str.replace(\"Jonkheer\",\"Mr\")\n    new_data[\"Name\"] = new_data[\"Name\"].str.replace(\"Rev\",\"Mr\")\n    new_data[\"Name\"] = new_data[\"Name\"].str.replace(\"Sir\",\"Mr\")\n    new_data[\"Name\"] = new_data[\"Name\"].str.replace(\"Mme\",\"Mrs\")\n    new_data[\"Name\"] = new_data[\"Name\"].str.replace(\"Mlle\",\"Miss\")\n    new_data[\"Name\"] = new_data[\"Name\"].str.replace(\"the Countess\",\"Mme\")\n    new_data[\"Name\"] = new_data[\"Name\"].str.replace(\"Mme\",\"Mrs\")\n    new_data[\"Name\"] = new_data[\"Name\"].str.replace(\"Ms\",\"Mrs\")\n    new_data[\"Name\"] = new_data[\"Name\"].str.replace(\"Major\",\"Mr\")\n    new_data[\"Name\"] = new_data[\"Name\"].str.replace(\"Master\",\"Mr\")\n    new_data[\"Name\"] = new_data[\"Name\"].str.replace(\"Lady\",\"Miss\")\n    new_data[\"Miss\"] = new_data[\"Name\"].str.contains(\"Miss\").astype(int)\n    new_data[\"Mr\"] = new_data[\"Name\"].str.contains(\"Mr\").astype(int)\n    new_data[\"Mrs\"] = new_data[\"Name\"].str.contains(\"Mrs\").astype(int)\n    new_data = new_data.drop(\"Name\", axis=1)\n    return new_data\n\ndata = train_data\ndata = drop_survived(data)\ndata = drop_passenger_id(data)\ndata = handle_pclass(data)\ndata= handle_name(data)[[\"Pclass\", \"Miss\", \"Mr\", \"Mrs\"]]\napply_model(data, data_label)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7383c7eb-22bb-45cd-8a11-0a1a3fd2e6d9","_uuid":"715e7db4eff86c9057823e93d168215c10db340a"},"cell_type":"markdown","source":"## Sex\nData is either male or female\nwe can simply convert to int\n\n"},{"metadata":{"collapsed":true,"_cell_guid":"b12e7c57-ca3f-4578-8c87-2b5c76691770","_uuid":"3df0e67d256b771bd42f85cd255e908dd144382c","trusted":false},"cell_type":"code","source":"def handle_sex(data):\n    new_data = data\n    new_data[\"Sex\"] = data[\"Sex\"].str.contains(\"female\").astype(int)\n    return new_data\n\ntest_data = pd.read_csv(TRAIN_FILE)\ngroups = handle_sex(test_data).groupby(\"Sex\").size()\ngroups.plot.bar()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"93be9bd9-71e8-4845-b626-1cf3d436cb8e","_uuid":"ee9d28c2633723e36874b1e67f395fd060d1e4ba","trusted":false},"cell_type":"code","source":"data = train_data\ndata = drop_survived(data)\ndata = drop_passenger_id(data)\ndata = handle_pclass(data)\ndata= handle_name(data)\ndata= handle_sex(data)[[\"Pclass\", \"Sex\",  \"Miss\", \"Mr\", \"Mrs\"]]\napply_model(data, data_label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"304157fd713792d62f4b1874ee7e2581313c44ed"},"cell_type":"code","source":"print(\"Train: {} null (on {})\".format(train_data[\"Sex\"].isnull().sum(), len(train_data)))\nprint(\"Test: {} null (on {})\".format(test_data[\"Sex\"].isnull().sum(), len(test_data)))\nprint(\"\")\n\nprint(\"Correlation:\")\nsex = train_data[[\"Sex\", \"Survived\"]]\nsex[\"Sex\"] = (sex[\"Sex\"] != \"female\").astype(int)\ncorr = sex.corr()\nprint(corr)\n\nsns.heatmap(corr, \n            xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values, center=0)","execution_count":131,"outputs":[]},{"metadata":{"_cell_guid":"2a4930e6-023d-442e-a7f5-353ff38daff9","_uuid":"afb4b9a41c4df3c869d3aaa7a418c31eb82bbb75"},"cell_type":"markdown","source":"## Age\nFor the moment we will just replace null values by the mean"},{"metadata":{"_cell_guid":"25be46de-3387-45b6-b951-3a75937dee5a","_uuid":"db7e89452bb62d3d037e6416b20c950d40926620","trusted":true},"cell_type":"code","source":"def handle_age(data):\n    new_data = data\n    new_data[\"Age\"] = new_data[\"Age\"].fillna(new_data[\"Age\"].mean())\n    new_data[\"Age\"] = new_data[\"Age\"]/15\n    new_data[\"Age\"] = new_data[\"Age\"].astype(int)\n    new_data[\"Age\"] = preprocessing.maxabs_scale(data[\"Age\"])\n    return new_data\n\ntest_data = pd.read_csv(TRAIN_FILE)\ngroups.plot.bar()\ngroups = handle_age(test_data).groupby(\"Age\").size()\ngroups.plot.bar()\n\nprint(\"Train: {} null (on {})\".format(train_data[\"Age\"].isnull().sum(), len(train_data)))\nprint(\"Test: {} null (on {})\".format(test_data[\"Age\"].isnull().sum(), len(test_data)))\nprint(\"\")\n\nprint(\"Correlation:\")\nage = train_data[[\"Age\", \"Survived\"]]\nage[\"Age\"] = age[\"Age\"].fillna(age[\"Age\"].mean())\nage[\"LessThan5\"] = (age[\"Age\"] < 5).astype(int)\nage[\"Between5And12\"] = ((age[\"Age\"] >= 5) & (age[\"Age\"] < 12)).astype(int)\nage[\"Between12And16\"] = ((age[\"Age\"] >= 12) & (age[\"Age\"] < 16)).astype(int)\nage[\"Between16And45\"] = ((age[\"Age\"] >= 16) & (age[\"Age\"] < 45)).astype(int)\nage[\"Between45And60\"] = ((age[\"Age\"] >= 45) & (age[\"Age\"] < 60)).astype(int)\nage[\"MoreThan60\"] = (age[\"Age\"] > 60).astype(int)\nage = age.drop(\"Age\", axis=1)\n\ncorr = age.corr()\nprint(corr)\n\nsns.heatmap(corr, \n            xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values)","execution_count":142,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"3841b35b-89f0-46a7-851d-2779d069156d","_uuid":"9ef1f5488d827bf043ad035adb00ab3f61081034","trusted":false},"cell_type":"code","source":"data = train_data.copy()\ndata = drop_survived(data)\ndata = drop_passenger_id(data)\ndata = handle_pclass(data)\ndata= handle_name(data)\ndata= handle_sex(data)\ndata= handle_age(data)[[\"Pclass\", \"Miss\", \"Mr\", \"Mrs\", \"Sex\", \"Age\"]]\napply_model(data, data_label)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9ecc14c8-da9b-4e65-8eb1-4418cfd58741","_uuid":"9ebbc787e3fa7a19bdaa7579de3164897923ec7d"},"cell_type":"markdown","source":"## SibSp\nFor the moment we will use this feature as is.\nIt would be intersting to split this"},{"metadata":{"_cell_guid":"fa168e4b-ff23-4669-9cd0-9e6066199c93","_uuid":"f7b7ab2fa60b29928a36634ff41b1f52e646674e","trusted":true},"cell_type":"code","source":"groups.plot.bar()\ngroups = test_data.groupby(\"SibSp\").size()\ngroups.plot.bar()\n\nprint(\"Train: {} null (on {})\".format(train_data[\"SibSp\"].isnull().sum(), len(train_data)))\nprint(\"Test: {} null (on {})\".format(test_data[\"SibSp\"].isnull().sum(), len(test_data)))\nprint(\"\")\n\nprint(\"Correlation:\")\nsib = train_data[[\"SibSp\", \"Survived\"]]\ncorr = sib.corr()\nprint(corr)\n\nsns.heatmap(corr, \n            xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values)","execution_count":145,"outputs":[]},{"metadata":{"_cell_guid":"fe5f4f81-6051-4d89-87f3-c2be49fb20c6","_uuid":"8bf440664e321200290f048c95c7c5f28f75f0e9"},"cell_type":"markdown","source":"## Parch"},{"metadata":{"_cell_guid":"031b95b3-e9b8-4871-82fc-efc8427b1b0f","_uuid":"b88662d6d5d1a979504608f521cfba685f0ec8d5","trusted":true},"cell_type":"code","source":"groups.plot.bar()\ngroups = test_data.groupby(\"Parch\").size()\ngroups.plot.bar()\n\nprint(\"Train: {} null (on {})\".format(train_data[\"Parch\"].isnull().sum(), len(train_data)))\nprint(\"Test: {} null (on {})\".format(test_data[\"Parch\"].isnull().sum(), len(test_data)))\nprint(\"\")\n\nprint(\"Correlation:\")\nsib = train_data[[\"Parch\", \"Survived\"]]\ncorr = sib.corr()\nprint(corr)\n\nsns.heatmap(corr, \n            xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values)","execution_count":146,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"ae44f812-b84a-482b-8aa0-2c752acd4b1d","_uuid":"1d42da53388ff96f6a3a4299dcf61f8f10eab832","trusted":false},"cell_type":"code","source":"def handle_sibsp(data):\n    new_data = data\n    new_data[\"SibSp\"] = preprocessing.maxabs_scale(data[\"SibSp\"])\n    return new_data\n    \ndef handle_parch(data):\n    new_data = data\n    new_data[\"Parch\"] = preprocessing.maxabs_scale(data[\"Parch\"])\n    return new_data\n                     \ndata = train_data.copy()\ndata = drop_survived(data)\ndata = drop_passenger_id(data)\ndata = handle_pclass(data)\ndata = handle_name(data)\ndata = handle_sex(data)\ndata = handle_age(data)\ndata = handle_sibsp(data)\ndata = handle_parch(data)\ndata = data[[\"Pclass\", \"Miss\", \"Mr\", \"Mrs\", \"Sex\", \"Age\", \"SibSp\", \"Parch\"]]\napply_model(data, data_label)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f2f1d5d9-40fe-4166-9826-d54f08fab719","_uuid":"185d07e014e78a09df7806868e3681e4b3e6ffc0"},"cell_type":"markdown","source":"## Ticket\nWill be excluded for now\n\n"},{"metadata":{"_cell_guid":"6dd0b9a6-c3ca-41f6-8487-d5fa06c87814","_uuid":"3ccca5ec3dd06420761518b81ba57dc203190d6e","trusted":true},"cell_type":"code","source":"data = train_data.copy()\nprint(data[\"Ticket\"].head(10))","execution_count":147,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"33b2752f-2c5e-4788-b4ed-cdbfd23ab8db","_uuid":"34b07f47c4e7e914abe275b47888dddeb9275fb6","trusted":false},"cell_type":"code","source":"def drop_ticket(data):\n    return data.drop([\"Ticket\"], axis=1)\n                        \ndata = train_data\ndata = drop_survived(data)\ndata = drop_passenger_id(data)\ndata = handle_pclass(data)\ndata = handle_name(data)\ndata = handle_sex(data)\ndata = handle_age(data)\ndata = handle_sibsp(data)\ndata = handle_parch(data)\ndata = drop_ticket(data)\ndata = data[[\"Pclass\",  \"Miss\", \"Mr\", \"Mrs\", \"Sex\", \"Age\", \"SibSp\", \"Parch\"]]\napply_model(data, data_label)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"eb8122ff-7990-41d7-a9ba-79cad4275da4","_uuid":"6d1163ff976f0832646142e1d067422ebf37c56f"},"cell_type":"markdown","source":"## Fare\n\n"},{"metadata":{"collapsed":true,"_cell_guid":"702031b3-6a5f-4812-aa22-9815cec5bcba","_uuid":"79357fcf05cda1e52dcbd23e765ddf2c8b94db69","trusted":false},"cell_type":"code","source":"def handle_fare(data):\n    new_data = data\n    new_data[\"Fare\"] = new_data[\"Fare\"].fillna(new_data[\"Fare\"].mean()) #some null values in test_data\n    new_data[\"Fare\"] = new_data[\"Fare\"]/ 20\n    new_data[\"Fare\"] = new_data[\"Fare\"].astype(int)\n    new_data[\"Fare\"] = preprocessing.maxabs_scale(data[\"Fare\"])\n    return new_data\n\ntest_data = pd.read_csv(TRAIN_FILE)\ngroups = handle_fare(test_data).groupby(\"Fare\").size()\ngroups.plot.bar()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"f51e526d-c7ec-4440-b907-0d8b67243899","_uuid":"424ff61019294134e894b4e3626b793127a77985","trusted":false},"cell_type":"code","source":"data = train_data\ndata = drop_survived(data)\ndata = drop_passenger_id(data)\ndata = handle_pclass(data)\ndata = handle_name(data)\ndata = handle_sex(data)\ndata = handle_age(data)\ndata = handle_sibsp(data)\ndata = handle_parch(data)\ndata = drop_ticket(data)\ndata = handle_fare(data)\ndata = data[[\"Pclass\", \"Mr\", \"Mrs\", \"Miss\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\"]]\napply_model(data, data_label)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ca8087e7-c58a-4825-ba8f-98c54a500e55","_uuid":"abc82af99989d821d162643288dedb8e6686f471"},"cell_type":"markdown","source":"## Cabin\nfor the moment we will just check if cabin number is known or not"},{"metadata":{"collapsed":true,"_cell_guid":"e231a757-2865-411b-a151-249e6fa8963e","_uuid":"31b3d7cfbfd597c77242745437dd23f75bb56ae8","trusted":false},"cell_type":"code","source":"data = train_data.copy()\nprint(data[\"Cabin\"].head(15))\n","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"78cc425b-7968-4752-a098-8444c6c2eeab","_uuid":"f2fb5afba0bc0f5f3a011bd233a3625bc29e7ce4","trusted":false},"cell_type":"code","source":"def handle_cabin(data):\n    new_data = data\n    new_data[\"Cabin\"] = new_data[\"Cabin\"].isna().astype(int)\n    return new_data\n                       \ndata = train_data\ndata = drop_survived(data)\ndata = drop_passenger_id(data)\ndata = handle_pclass(data)\ndata = handle_name(data)\ndata = handle_sex(data)\ndata = handle_age(data)\ndata = handle_sibsp(data)\ndata = handle_parch(data)\ndata = drop_ticket(data)\ndata = handle_fare(data)\ndata = handle_cabin(data)\ndata = data[[\"Pclass\", \"Mr\", \"Mrs\", \"Miss\", \"Sex\", \"Age\", \"SibSp\", \"Parch\",\"Fare\", \"Cabin\"]]\napply_model(data, data_label)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6f74362e-fa28-4f20-bb39-7b95f9689c1c","_uuid":"22b5b93740df9611dd1e4e323f737a53d071c773"},"cell_type":"markdown","source":"\n\n## Embarked"},{"metadata":{"trusted":true,"_uuid":"9a73ac4630760aa26e890a4d94edcff1b91ef60a"},"cell_type":"code","source":"print(\"Train: {} null (on {})\".format(train_data[\"Embarked\"].isnull().sum(), len(train_data)))\nprint(\"Test: {} null (on {})\".format(test_data[\"Embarked\"].isnull().sum(), len(test_data)))\nprint(\"\")\n\nprint(\"Correlation:\")\nembarked = train_data[[\"Embarked\", \"Survived\"]]\nembarked[\"NotEmbarked\"] = embarked[\"Embarked\"].isna().astype(int)\nembarked[\"Embarked\"] = embarked[\"Embarked\"].fillna(\"\")\nembarked['Southampton'] = embarked[\"Embarked\"].str.contains(\"S\").astype(int)\nembarked['Queenstown'] = embarked[\"Embarked\"].str.contains(\"Q\").astype(int)\nembarked['Cherbourg'] = embarked[\"Embarked\"].str.contains(\"C\").astype(int)\nembarked = embarked.drop(\"Embarked\", axis=1)\ncorr = embarked.corr()\nprint(corr)\n\nsns.heatmap(corr, \n            xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values)","execution_count":148,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"7aaa432a-9f28-4653-bde4-685e09786917","_uuid":"dced37e19eaf3cf258201a8a8312e50bf0f9ec02","trusted":false},"cell_type":"code","source":"def handle_embarked(data):\n    new_data = data\n    new_data[\"NotEmbarked\"] = new_data[\"Embarked\"].isna().astype(int)\n    new_data[\"Embarked\"] = new_data[\"Embarked\"].fillna(\"\")\n    new_data['Southampton'] = new_data[\"Embarked\"].str.contains(\"S\").astype(int)\n    new_data['Queenstown'] = new_data[\"Embarked\"].str.contains(\"Q\").astype(int)\n    new_data['Cherbourg'] = new_data[\"Embarked\"].str.contains(\"C\").astype(int)\n    new_data = new_data.drop(\"Embarked\", axis=1)\n    return new_data\n\nprint(handle_embarked(train_data)[[\"NotEmbarked\", \"Southampton\", \"Queenstown\", \"Cherbourg\"]].head())","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_cell_guid":"e904916f-b911-410f-9499-84a7827aed7d","_uuid":"b671ae01f331085523b5ca85e608fce3118258af","trusted":false},"cell_type":"code","source":"def process_data(data):\n    data = drop_survived(data)\n    data = drop_passenger_id(data)\n    data = handle_pclass(data)\n    data = handle_name(data)\n    data = handle_sex(data)\n    data = handle_age(data)\n    data = handle_sibsp(data)\n    data = handle_parch(data)\n    data = drop_ticket(data)\n    data = handle_fare(data)\n    data = handle_cabin(data)\n    data = handle_embarked(data)\n    return data\n\ndata = train_data.copy()\ndata = process_data(data)\nprint(data.head())\napply_model(data, data_label)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"400a161e-d056-4cc8-a79d-9fca88aac786","_uuid":"677cbafc56deeafe1061ce77565f109d181ce189"},"cell_type":"markdown","source":"# Generate Output\nwe have create a first implementation for our prediction.\nNow we can generate the output file.\n\nSo far we have only used part of the train_data to fit our model (as we are using one part for crossvalidation).\nNow we can fit our model with the the whole set, and run the prediction on the test_data.\nFormat the result as expected in submission format.\n\n\n\n"},{"metadata":{"collapsed":true,"_cell_guid":"9b418a49-602e-440f-ae07-ec09be45e921","_uuid":"7fc48381282fb69ea0dd7583363aa4698edb0ad8","trusted":false},"cell_type":"code","source":"    model = KNeighborsClassifier(n_neighbors=2)\n    X_train = pd.read_csv(TRAIN_FILE)   \n    y_train = X_train[\"Survived\"]\n    X_train = process_data(X_train) \n    \n    X_test = pd.read_csv(TEST_FILE)  \n    test_labels = X_test[[\"PassengerId\"]]\n    X_test = process_data(X_test) \n    \n    model.fit(X_train, y_train)\n    result = model.predict(X_test)\n    print(len(result))\n    df = pd.DataFrame()\n    df['PassengerId'] = test_labels.astype(int)\n    df['Survived'] = result.astype(int)\n    print(df.head())\n    df.to_csv(\"submission.csv\", index=False)\n    print(\"Done\")\n    ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"69d193eb-b7dd-42fa-81cc-15d459a349fa","_uuid":"4aadc0330a7375932858dd497167e38cf22983de"},"cell_type":"markdown","source":"# Tuning the model\n"},{"metadata":{"_cell_guid":"efa54060-a3a9-4c19-80cf-f24c83f754ca","_uuid":"75df2dd8f150e939f3dc0aab259e4e37644f4b27"},"cell_type":"markdown","source":"First lets put all the code together:"},{"metadata":{"collapsed":true,"_cell_guid":"238b5eb1-58a7-4719-beca-e9b7976534b6","_uuid":"f1db1d6dcd513315d7fd90724c54b743168afcd1","trusted":false},"cell_type":"code","source":"import pandas as pd\nimport sys\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn import preprocessing\n\npd.options.mode.chained_assignment = None #To hide some warnings\n\nTRAIN_FILE = \"../input/train.csv\"\nTEST_FILE = \"../input/test.csv\"\ntrain_data = pd.read_csv(TRAIN_FILE)\ntest_data = pd.read_csv(TEST_FILE)\n\ndef drop_survived(data):\n    return data.drop(\"Survived\", axis=1, errors=\"ignore\")\n\ndef drop_passenger_id(data):\n    return data.drop(\"PassengerId\", axis=1, errors=\"ignore\")\n\ndef apply_model(data, data_label):\n    model = KNeighborsClassifier(n_neighbors=2)\n    scores = cross_val_score(model, data, data_label, cv=5, verbose=1, scoring='accuracy')\n    print(scores.mean())\n    \ndef extract_survived(data):\n    return data[\"Survived\"]\n\ndef handle_pclass(data):\n    new_data = data\n    new_data[\"Pclass\"] = new_data[\"Pclass\"] -1\n    new_data[\"Pclass\"] = preprocessing.maxabs_scale(data[\"Pclass\"])\n    return new_data\n\ndef handle_name(data):\n    new_data = data\n    new_data[\"Name\"] = data[\"Name\"]\n    new_data[\"Name\"] = new_data[\"Name\"].str.replace(\".\",\";\")\n    new_data[\"Name\"] = new_data[\"Name\"].str.replace(\",\",\";\")\n    new_data[\"Name\"] = new_data[\"Name\"].str.split(';', expand=True)[1]\n    new_data[\"Name\"] = new_data[\"Name\"].str.replace(\"Capt\",\"Mr\")\n    new_data[\"Name\"] = new_data[\"Name\"].str.replace(\"Col\",\"Mr\")\n    new_data[\"Name\"] = new_data[\"Name\"].str.replace(\"Don\",\"Mr\")\n    new_data[\"Name\"] = new_data[\"Name\"].str.replace(\"Dr\",\"Mr\")\n    new_data[\"Name\"] = new_data[\"Name\"].str.replace(\"Jonkheer\",\"Mr\")\n    new_data[\"Name\"] = new_data[\"Name\"].str.replace(\"Rev\",\"Mr\")\n    new_data[\"Name\"] = new_data[\"Name\"].str.replace(\"Sir\",\"Mr\")\n    new_data[\"Name\"] = new_data[\"Name\"].str.replace(\"Mme\",\"Mrs\")\n    new_data[\"Name\"] = new_data[\"Name\"].str.replace(\"Mlle\",\"Miss\")\n    new_data[\"Name\"] = new_data[\"Name\"].str.replace(\"the Countess\",\"Mme\")\n    new_data[\"Name\"] = new_data[\"Name\"].str.replace(\"Mme\",\"Mrs\")\n    new_data[\"Name\"] = new_data[\"Name\"].str.replace(\"Ms\",\"Mrs\")\n    new_data[\"Name\"] = new_data[\"Name\"].str.replace(\"Major\",\"Mr\")\n    new_data[\"Name\"] = new_data[\"Name\"].str.replace(\"Master\",\"Mr\")\n    new_data[\"Name\"] = new_data[\"Name\"].str.replace(\"Lady\",\"Miss\")\n    new_data[\"Miss\"] = new_data[\"Name\"].str.contains(\"Miss\").astype(int)\n    new_data[\"Mr\"] = new_data[\"Name\"].str.contains(\"Mr\").astype(int)\n    new_data[\"Mrs\"] = new_data[\"Name\"].str.contains(\"Mrs\").astype(int)\n    new_data = new_data.drop(\"Name\", axis=1)\n    return new_data\n\ndef handle_sex(data):\n    new_data = data\n    new_data[\"Sex\"] = data[\"Sex\"].str.contains(\"female\").astype(int)\n    return new_data\n\ndef handle_age(data):\n    new_data = data\n    new_data[\"Age\"] = new_data[\"Age\"].fillna(new_data[\"Age\"].mean())\n    new_data[\"Age\"] = new_data[\"Age\"]/15\n    new_data[\"Age\"] = new_data[\"Age\"].astype(int)\n    new_data[\"Age\"] = preprocessing.maxabs_scale(data[\"Age\"])\n    return new_data\n\ndef handle_sibsp(data):\n    new_data = data\n    new_data[\"SibSp\"] = preprocessing.maxabs_scale(data[\"SibSp\"])\n    return new_data\n    \ndef handle_parch(data):\n    new_data = data\n    new_data[\"Parch\"] = preprocessing.maxabs_scale(data[\"Parch\"])\n    return new_data\n\ndef drop_ticket(data):\n    return data.drop([\"Ticket\"], axis=1)\n\ndef handle_fare(data):\n    new_data = data\n    new_data[\"Fare\"] = new_data[\"Fare\"].fillna(new_data[\"Fare\"].mean()) #some null values in test_data\n    new_data[\"Fare\"] = new_data[\"Fare\"]/ 20\n    new_data[\"Fare\"] = new_data[\"Fare\"].astype(int)\n    new_data[\"Fare\"] = preprocessing.maxabs_scale(data[\"Fare\"])\n    return new_data\n\ndef handle_cabin(data):\n    new_data = data\n    new_data[\"Cabin\"] = new_data[\"Cabin\"].isna().astype(int)\n    return new_data\n\ndef handle_embarked(data):\n    new_data = data\n    new_data[\"NotEmbarked\"] = new_data[\"Embarked\"].isna().astype(int)\n    new_data[\"Embarked\"] = new_data[\"Embarked\"].fillna(\"\")\n    new_data['Southampton'] = new_data[\"Embarked\"].str.contains(\"S\").astype(int)\n    new_data['Queenstown'] = new_data[\"Embarked\"].str.contains(\"Q\").astype(int)\n    new_data['Cherbourg'] = new_data[\"Embarked\"].str.contains(\"C\").astype(int)\n    new_data = new_data.drop(\"Embarked\", axis=1)\n    return new_data\n\ndef process_data(data):\n    data = drop_survived(data)\n    data = drop_passenger_id(data)\n    data = handle_pclass(data)\n    data = handle_name(data)\n    data = handle_sex(data)\n    data = handle_age(data)\n    data = handle_sibsp(data)\n    data = handle_parch(data)\n    data = drop_ticket(data)\n    data = handle_fare(data)\n    data = handle_cabin(data)\n    data = handle_embarked(data)\n    return data\n\ndata = train_data.copy()\ndata = process_data(data)\napply_model(data, data_label)\n\nmodel = KNeighborsClassifier(n_neighbors=2)\nX_train = pd.read_csv(TRAIN_FILE)   \ny_train = X_train[\"Survived\"]\nX_train = process_data(X_train) \n    \nX_test = pd.read_csv(TEST_FILE)  \ntest_labels = X_test[[\"PassengerId\"]]\nX_test = process_data(X_test) \n    \nmodel.fit(X_train, y_train)\nresult = model.predict(X_test)\nprint(len(result))\ndf = pd.DataFrame()\ndf['PassengerId'] = test_labels.astype(int)\ndf['Survived'] = result.astype(int)\nprint(df.head())\ndf.to_csv(\"submission.csv\", index=False)\n  ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"22f2722d-22ee-4a5a-8370-548b1cbc2c2c","_uuid":"f4b839b983e89f2a8ff9876d1760b7bf3907f6b7"},"cell_type":"markdown","source":"Some refactoring before continuing...\nThen we will try several models to have an idea about how accurate they are."},{"metadata":{"collapsed":true,"_cell_guid":"14afc02c-82cf-4fa4-b199-f43df9638f41","_uuid":"6424996e9444a908bc16b04b9de8453959cccb30","trusted":false},"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn import preprocessing\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.options.mode.chained_assignment = None #To hide some warnings\n\nTRAIN_FILE = \"../input/train.csv\"\nTEST_FILE = \"../input/test.csv\"\ntrain_data = pd.read_csv(TRAIN_FILE)\ntest_data = pd.read_csv(TEST_FILE)\n\n   \ndef extract_survived(data):\n    return data[\"Survived\"]\n\ndef drop_survived(data):\n    return data.drop(\"Survived\", axis=1, errors=\"ignore\")\n\ndef drop_passenger_id(data):\n    return data.drop(\"PassengerId\", axis=1, errors=\"ignore\")\n\ndef handle_pclass(data):\n    new_data = data\n    new_data[\"Pclass\"] = new_data[\"Pclass\"] -1\n    new_data[\"Pclass\"] = preprocessing.maxabs_scale(data[\"Pclass\"])\n    return new_data\n\ndef replace_multi(string, separators, new_separator):\n    for s in separators:\n        string = string.replace(s, new_separator)\n        \ndef filter_data_contains(data, column, contain, target_column=None):\n    new_data = data\n    if target_column == None:\n        new_data[column] = new_data[column].str.contains(contain).astype(int)\n    else:\n        new_data[target_column] = new_data[column].str.contains(contain).astype(int)\n    return new_data\n\ndef extract_title(x):\n    return x.replace(\".\",\";\").replace(\",\",\";\").split(\";\")[1]\n\ndef handle_name(data):\n    new_data = data\n    new_data[\"Name\"] = new_data[\"Name\"].apply(extract_title)\n    to_replace = {\n        \"Mr\": [\"Capt\", \"Col\", \"Don\", \"Dr\", \"Jonkheer\", \"Rev\", \"Sir\", \"Major\", \"Master\" ],\n        \"Miss\": [\"Mlle\", \"Lady\" ],\n        \"Mrs\" :  [\"Mme\", \"the Countess\", \"Ms\" ]\n    }\n    \n    for title in to_replace.keys():\n        for t in to_replace[title]:\n            new_data[\"Name\"] = new_data[\"Name\"].str.replace(t, title)\n        new_data = filter_data_contains(new_data, \"Name\", title, title)\n        \n    new_data = new_data.drop(\"Name\", axis=1)\n    return new_data\n\ndef fill_na_with_mean(data, column):\n    new_data = data\n    new_data[column] = new_data[column].fillna(new_data[column].mean())\n    return new_data\n\ndef fill_na_with_mean(data, column):\n    new_data = data\n    new_data[column] = new_data[column].fillna(new_data[column].mean())\n    return new_data\n\ndef is_na(data, column):\n    new_data = data\n    new_data[column] = new_data[column].isna().astype(int)\n    return new_data\n    \ndef handle_sex(data):\n    return filter_data_contains(data, \"Sex\", \"female\")\n\ndef handle_age(data):\n    new_data = data\n    new_data = fill_na_with_mean(new_data, \"Age\")\n    new_data[\"Age\"] =new_data[\"Age\"] / 15\n    new_data[\"Age\"] = preprocessing.maxabs_scale(data[\"Age\"])\n    return new_data\n\ndef handle_sibsp(data):\n    new_data = data\n    new_data[\"SibSp\"] = preprocessing.maxabs_scale(data[\"SibSp\"])\n    return new_data\n    \ndef handle_parch(data):\n    new_data = data\n    new_data[\"Parch\"] = preprocessing.maxabs_scale(data[\"Parch\"])\n    return new_data\n\ndef drop_ticket(data):\n    return data.drop([\"Ticket\"], axis=1)\n\ndef handle_fare(data):\n    new_data = data\n    new_data[\"Fare\"] = fill_na_with_mean(new_data, \"Fare\")\n    new_data[\"Fare\"] = new_data[\"Fare\"]/ 20\n    new_data[\"Fare\"] = preprocessing.maxabs_scale(data[\"Fare\"])\n    return new_data\n\ndef handle_cabin(data):\n    new_data = data\n    new_data = is_na(new_data,\"Cabin\")\n    return new_data\n\ndef handle_embarked(data):\n    new_data = data\n    new_data[\"NotEmbarked\"] =  new_data[\"Embarked\"].isna().astype(int)\n    new_data[\"Embarked\"] = new_data[\"Embarked\"].fillna(\"\")\n    new_data = filter_data_contains(new_data, \"Embarked\", \"S\", \"Southampton\")\n    new_data = filter_data_contains(new_data, \"Embarked\", \"Q\", \"Queenstown\")\n    new_data = filter_data_contains(new_data, \"Embarked\", \"C\", \"Cherbourg\")\n    new_data = new_data.drop(\"Embarked\", axis=1)\n    return new_data\n\ndef process_data(data):\n    data = drop_survived(data)\n    data = drop_passenger_id(data)\n    data = handle_pclass(data)\n    data = handle_name(data)\n    data = handle_sex(data)\n    data = handle_age(data)\n    data = handle_sibsp(data)\n    data = handle_parch(data)\n    data = drop_ticket(data)\n    data = handle_fare(data)\n    data = handle_cabin(data)\n    data = handle_embarked(data)\n    return data\n\ndata = train_data.copy()\ndata_label = extract_survived(data)\ndata = process_data(data)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_moons, make_circles, make_classification\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn import linear_model\n\nclassifiers = {\n    \"Nearest Neighbors\" : KNeighborsClassifier(3),\n    \"LinearRegression\": linear_model.LinearRegression(),\n    \"Ridge\": linear_model.Ridge(alpha = .5),\n    \"Lasso\": linear_model.Lasso(alpha = 0.1),\n    \"ElasticNet\": linear_model.ElasticNet(random_state=0),\n    \"Lars\": linear_model.Lars(n_nonzero_coefs=1),\n    \"LassoLars\": linear_model.LassoLars(alpha=.1),\n    \"Omp\": linear_model.OrthogonalMatchingPursuit(n_nonzero_coefs=1),\n    \"BayesianRidge\":linear_model.BayesianRidge(),\n    \"ARDRegression\":linear_model.ARDRegression(),\n    \"LogisitcRegression\":linear_model.LogisticRegression(),\n    \"SGDClassifier\":linear_model.SGDClassifier(),\n    \"Perceptron\": linear_model.Perceptron(),\n    \"PassiveAggressiveClassifier\": linear_model.PassiveAggressiveClassifier(),\n    \"Theil-Sen\": linear_model.TheilSenRegressor(random_state=42),\n    \"RANSAC\": linear_model.RANSACRegressor(random_state=42),\n    \"Huber\": linear_model.HuberRegressor(),\n    \"SVC linear\": SVC(kernel=\"linear\", C=0.025),\n    \"SVC\": SVC(gamma=2, C=1, probability=True),\n    \"GuassianProcess\":GaussianProcessClassifier(1.0 * RBF(1.0)),\n    \"DecisionTree\":DecisionTreeClassifier(max_depth=5),\n    \"RandomForest\":RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n    \"NeutraNet\":MLPClassifier(alpha=1),\n    \"ADABoost\":AdaBoostClassifier(),\n    \"GaussianNB\":GaussianNB(),\n    \"QDA\":QuadraticDiscriminantAnalysis()\n}\n\nbest_model_names = {}\nfor model_name in classifiers.keys():\n    try:\n        model = classifiers[model_name]\n        scores = cross_val_score(model, data, data_label, cv=5, verbose=1, scoring='accuracy')\n        score = scores.mean()\n        if score > .8:\n            best_model_names[model_name] = scores.mean()\n            print(\"{} {}\".format(model_name, scores.mean()))\n    except:\n        pass\n            \n\nprint(best_model_names)\n\n\nres = pd.DataFrame()\nX_train = pd.read_csv(TRAIN_FILE)   \ny_train = X_train[\"Survived\"]\nX_train = process_data(X_train) \nX_test = pd.read_csv(TEST_FILE)  \ntest_labels = X_test[[\"PassengerId\"]]\nX_test = process_data(X_test) \nres[\"PassengerId\"] = test_labels[\"PassengerId\"]\n\nfor model_name in best_model_names.keys():\n    model =  classifiers[model_name]\n    model.fit(X_train, y_train)\n    result = model.predict_proba(X_test)[:,1]\n    print(\"{}: {} rows\".format(model_name, len(result)))\n    res[model_name] = result\n\nmodels_list = list(best_model_names.keys())\nres['ProbaMin'] = res[models_list].min(axis=1)\nres['ProbaMax'] = res[models_list].max(axis=1)\nres['Accurate'] = (res['ProbaMin']< .20) | (res['ProbaMax']> .80)\nres['Survived'] = (res['ProbaMax']-0.5) > (0.5-res['ProbaMin'])\nres['Survived'] = res['Survived'].astype(int)\nres.to_csv(\"submission_detail.csv\", index=False)\n\nres_filtered = res[[\"PassengerId\",\"Survived\"]]\nres_filtered.to_csv(\"submission.csv\", index=False)\nres.head(20)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}