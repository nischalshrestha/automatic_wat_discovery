{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"markdown","source":"# First attempt at machine learning using Kaggle.\n\nFamiliar with python but not from a machine learning perspective so here goes first attempt using the Titanic data!\n\n## 1) Establishing notebook environment"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# data handling libraries\nimport numpy as np\nimport pandas as pd\n\n# data visualisation libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"7154e7449216476f2c0ef2c95981e74678d81272"},"cell_type":"markdown","source":"## 2) Loading data"},{"metadata":{"trusted":true,"_uuid":"bee9dc95afec10397f81749bf1cca287b6c6aa2e","collapsed":true},"cell_type":"code","source":"# import dataset for training model\ntrain_df = pd.read_csv('../input/train.csv')\n\ntest_df = pd.read_csv('../input/test.csv')\n\n# check its right\ntrain_df.head()","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"df3f5906d610f8ffbcc9145085141841f76b9035"},"cell_type":"markdown","source":"## 3) Exploring data"},{"metadata":{"trusted":true,"_uuid":"2054a92aa7d6ce6c24908861cb9587bb6dca0dce","collapsed":true},"cell_type":"code","source":"print('Non null data info')\ntrain_df.info()\nprint('-------------')\ntest_df.info()","execution_count":3,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a8d6b78009cbb36128fc0be960dcff34821aa5be","collapsed":true},"cell_type":"code","source":"# understand distribution of null values\ntrain_df.isnull().sum(), print('------'),test_df.isnull().sum()","execution_count":4,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a4b24b71b2455d4d8ba609ecbbab9bcb4889e682","collapsed":true},"cell_type":"code","source":"print(train_df.describe())","execution_count":5,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee17f7f78d9f04a624848f661eb6e8f2e1f95f33","collapsed":true},"cell_type":"code","source":"plt.figure(figsize=(6,12))\nplt.subplot(5,1,1)\nsns.barplot(x='Sex',y='Survived', data=train_df)\n# fares versus survived\ntrain_df['Fare'] = train_df['Fare'].fillna(-0.5)\nbins = [-1, 0, 20 ,41 , 60, 81, 100, np.inf]\nlabels = ['Unknown','1-20','21-41','42-60','61-81','82-100','101+']\ntrain_df['FareGroup'] = pd.cut(train_df['Fare'],bins,labels = labels)\nplt.subplot(5,1,2)\nsns.barplot(x='FareGroup',y='Survived', data=train_df)\nplt.subplot(5,1,3)\nsns.barplot(x='Embarked',y='Survived', data=train_df)\nplt.subplot(5,1,4)\nsns.barplot(x='SibSp',y='Survived', data=train_df)\n\n\n\n# parents/children aboard\nplt.subplot(5,1,5)\nsns.barplot(x='Parch',y='Survived', data=train_df)\nplt.tight_layout()","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"e47b889c5979d3ff3d51d5d719c658bf3e051c80"},"cell_type":"markdown","source":"## Feature Engineering\n\nLets simplify a number of the features and compare the model with the raw features versus simplified."},{"metadata":{"trusted":true,"_uuid":"790235621fd61df67ad9b3f7befd62f0addf188d","collapsed":true},"cell_type":"code","source":"train_df['SibSpBool'] = (train_df['SibSp'].apply(lambda x: 1 if x>0 else 0))\n\ntrain_df['ParchBool'] = (train_df['Parch'].apply(lambda x: 1 if x>0 else 0))\n\ntrain_df['CabinBool'] = (train_df['Cabin'].notnull().astype('int'))\n\nplt.figure(figsize=(4,6))\nplt.subplot(3,1,1)\nsns.barplot(x='ParchBool',y='Survived', data=train_df)\nplt.subplot(3,1,2)\nsns.barplot(x='SibSpBool',y='Survived', data=train_df)\nplt.subplot(3,1,3)\nsns.barplot(x='CabinBool',y='Survived', data=train_df)\nplt.tight_layout()","execution_count":7,"outputs":[]},{"metadata":{"_uuid":"46feaffbaa03d49eb6f0d42ad44c53550f818b38"},"cell_type":"markdown","source":"Convert Sex and FareGroup to numeric values for inclusion in the model."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"e6a71019ea2477dc5604daff1bea19bae7b3fc60"},"cell_type":"code","source":"# dictionary to map values in column to numbers\nsex_map = {'male' : 0, 'female' : 1}\n# replace using the map dictionary\ntrain_df['Sex'] = train_df['Sex'].replace(sex_map)\n\n# map for fare groups\nfare_map = {'Unknown' : 0,'1-20' : 1,'21-41' : 2,'42-60' :3 ,'61-81' : 4,'82-100' : 5,'101+' : 6}\n# replace using the map dictionary\ntrain_df['FareGroup'] = train_df['FareGroup'].replace(fare_map)","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"4681fcb707c0c4d7f4e6e69af89692cd7dc2017c"},"cell_type":"markdown","source":"## Functions for imputing missing ages\n\nBoth the test and train set have missing age values.  \n\nSo here i've defined two functions to help impute age based on the mean age of persons with a specific title in their name. \n\ntitle_maker() simply extracts the titles from the Name column (it throws a warning regarding the method of indexing)\n\"A value is trying to be set on a copy of a slice from a DataFrame\"\nIt's because I haven't yet quite got round to reading the pandas documentation regarding the error (sorry!)\n\nI've taken this approach from one of the first Titanic Kernels I read ([https://www.kaggle.com/nadintamer/titanic-survival-predictions-beginner](http://))\nSo props!"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"46d0cb30924a6a08aa12885bcc1a9a6abf9f78ee"},"cell_type":"code","source":"# age imputer\ndef age_imputer(dataf_to_impute,dataf_to_ref):\n    # create groupby table of titles and mean age\n    title_age = dataf_to_ref[['Title','Age']][dataf_to_ref['Age'].notnull()].groupby('Title').mean()\n\n    # for loop to impute mean values from above table\n    for Id in dataf_to_impute['PassengerId'][dataf_to_impute['Age'].isnull()]:\n        for tle in dataf_to_impute['Title'][dataf_to_impute['PassengerId'] == Id]:\n            dataf_to_impute['Age'][dataf_to_impute['PassengerId'] == Id] = title_age.loc[tle].sum()\n\n# function for returning titles\ndef title_maker(dataframe):\n    dataframe['Title'] = (dataframe['Name'][dataframe['Name'].notnull()]).apply(lambda x: list(list(x.split(','))[1].split(' '))[1])","execution_count":9,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"440935a2cc1f49c284f09c417cca765ee6cc0ba5","collapsed":true},"cell_type":"code","source":"# check their are currently NA values in age\ntrain_df['Age'].isna().sum()","execution_count":10,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1bc4defd52efb0760b424f2379f4c4254abeff53","collapsed":true},"cell_type":"code","source":"# run the functions and impute the NAs\ntitle_maker(train_df)\n\nage_imputer(train_df, train_df)\n\n# check the NAs are gone\ntrain_df['Age'].isna().sum()","execution_count":11,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c61804581f79e88fdb8fe64a5997046125a0a976","collapsed":true},"cell_type":"code","source":"# lets plot it versus survived\nbins = [-1,0 ,5 ,12 , 18, 24, 35, 60, np.inf]\nlabels = ['Unknown','Baby','Child','Student','Teenager','Young Adult','Adult','Senior']\ntrain_df['AgeGroup'] = pd.cut(train_df['Age'],bins,labels = labels)\n\nsns.barplot(x='AgeGroup',y='Survived', data=train_df)","execution_count":12,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f125f39a1f640845e1aa77de7452ab64542a16f8"},"cell_type":"code","source":"age_map = {'Unknown' : 0,\n           'Baby' : 1,\n           'Child' : 2,\n           'Student' : 3,\n           'Teenager' : 4,\n           'Young Adult' : 5,\n           'Adult' : 6,\n           'Senior' : 7}\n\ntrain_df['AgeGroup'] = train_df['AgeGroup'].replace(age_map)","execution_count":13,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"735357c5a97a3d39a54ff70a43806b8dece00663","collapsed":true},"cell_type":"code","source":"# using seaborn to see correlations (plots both negative and positive)\ncorrmap = train_df[['PassengerId','Survived','Pclass','FareGroup','AgeGroup','SibSpBool','ParchBool','Sex','CabinBool']].corr()\nf, ax = plt.subplots(figsize=(12,9))\nsns.heatmap(corrmap, vmax=.8, square=True)","execution_count":14,"outputs":[]},{"metadata":{"_uuid":"ce5833c3103da1086cef063a442de9f7a10b0ddc"},"cell_type":"markdown","source":"## Produce the models\n\nLets produce some models comparing the simplified features versus the original features."},{"metadata":{"trusted":true,"_uuid":"65efe962264fc36771d4c4b85d50f30aed923076","collapsed":true},"cell_type":"code","source":"train_df.columns","execution_count":15,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"78e0c01f7da7e1370a88d4743a4d1d0542e9b559","collapsed":true},"cell_type":"code","source":"Y = train_df['Survived'].values.ravel()\n\n# all simplified variables\nX_new = train_df[['Pclass','FareGroup','AgeGroup','SibSpBool','ParchBool','Sex','CabinBool']]\n\n# original variables that are numeric\nX_orig = train_df[['Pclass','Sex','Age','SibSp','Parch','Fare','CabinBool']]","execution_count":16,"outputs":[]},{"metadata":{"_uuid":"64a50564a5ec2ae0336586ce2228084934ec32fa"},"cell_type":"markdown","source":"### Using a random forest to compare original and simplified features."},{"metadata":{"trusted":true,"_uuid":"f2201afbe0379e7a7fb12d3ed993a3611b5f4678","collapsed":true},"cell_type":"code","source":"# using cross validation\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_val_predict\n\nmodel_RF = RandomForestClassifier(random_state=0)\n\nmy_pipeline = make_pipeline(model_RF)\n\nscores1_RF = cross_val_score(my_pipeline,X_orig,Y,scoring = 'accuracy',cv=5)\n\nscores2_RF = cross_val_score(my_pipeline,X_new,Y,scoring = 'accuracy', cv=5)\n\nY_preds1 = cross_val_predict(my_pipeline,X_orig,Y)\n\nprint('Score for model with original features : {}'.format(scores1_RF*100))\n\nprint('Score for model with simplified features : {}'.format(scores2_RF*100))","execution_count":49,"outputs":[]},{"metadata":{"_uuid":"fffc03f322346dfc650ef09f0c2fe8cc5785793f"},"cell_type":"markdown","source":"### Using a XGBoost to compare original and simplified features."},{"metadata":{"trusted":true,"_uuid":"d066db1d8f8df2a20ffb2e75cd3edf266a6477aa","collapsed":true},"cell_type":"code","source":"# using cross validation\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_val_predict\n\nmodel = XGBClassifier(random_state=0)\n\nmy_pipeline = make_pipeline(model)\n\nscores1_XG = cross_val_score(my_pipeline,X_orig,Y,scoring = 'accuracy',cv=5)\n\nscores2_XG = cross_val_score(my_pipeline,X_new,Y,scoring = 'accuracy', cv=5)\n\nY_preds1 = cross_val_predict(my_pipeline,X_orig,Y)\n\nprint('Score for model with original features : {}'.format(scores1_XG*100))\n\nprint('Score for model with simplified features : {}'.format(scores2_XG*100))","execution_count":50,"outputs":[]},{"metadata":{"_uuid":"43bbcfbe8548cfd693390a2b2b3eb3dd96336366"},"cell_type":"markdown","source":"### Using SVC to compare original and simplified features."},{"metadata":{"trusted":true,"_uuid":"d272f195b4dca7927ccb40074527b7102194d0d7","collapsed":true},"cell_type":"code","source":"# using cross validation\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_val_predict\n\nmodel = SVC(random_state=0, gamma=\"auto\")\n\nmy_pipeline = make_pipeline(model)\n\nscores1_SV = cross_val_score(my_pipeline,X_orig,Y,scoring = 'accuracy',cv=5)\n\nscores2_SV = cross_val_score(my_pipeline,X_new,Y,scoring = 'accuracy', cv=5)\n\nY_preds1 = cross_val_predict(my_pipeline,X_orig,Y)\n\nprint('Score for model with original features : {}'.format(scores1_SV*100))\n\nprint('Score for model with simplified features : {}'.format(scores2_SV*100))","execution_count":51,"outputs":[]},{"metadata":{"_uuid":"f804918887d813a49b191d53a3cdfa5f6e93c11d"},"cell_type":"markdown","source":"### Using a ANN to compare original and simplified features."},{"metadata":{"trusted":true,"_uuid":"12a83f859a531f9431221cf0e2aecfeab0f11320","collapsed":true},"cell_type":"code","source":"# attempt at using ANN to solve this problem\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_val_predict\n\nmy_pipeline = make_pipeline(StandardScaler(),MLPClassifier(hidden_layer_sizes=(7,7,7),random_state=0,max_iter=1000))\n\nscores1_ANN = cross_val_score(my_pipeline,X_orig,Y,scoring = 'accuracy',cv=5)\n\nscores2_ANN = cross_val_score(my_pipeline,X_new,Y,scoring = 'accuracy', cv=5)\n\nY_preds1 = cross_val_predict(my_pipeline,X_orig,Y)\n\nprint('Score for model with original features : {}'.format(scores1_ANN*100))\n\nprint('Score for model with simplified features : {}'.format(scores2_ANN*100))","execution_count":52,"outputs":[]},{"metadata":{"_uuid":"54c31317e1a8863983bb628a9c165ad3f15a3358"},"cell_type":"markdown","source":"We'll make a quick boxplot of all our scores to help us visualise the consistency of the models."},{"metadata":{"trusted":true,"_uuid":"958225bdc703a16a2492d9bf9a4fd0bb1c887273","collapsed":true},"cell_type":"code","source":"plt.figure(figsize=(12,6))\nplt.boxplot([scores1_RF,scores2_RF,scores1_XG,scores2_XG,scores1_SV,scores2_SV,scores1_ANN,scores2_ANN],\n           labels=['Original RF','Simplified RF',\n                  'Original XGBoost','Simplified XGBoost',\n                  'Original SVC','Simplified SVC',\n                  'Original ANN','Simplified ANN'])","execution_count":54,"outputs":[]},{"metadata":{"_uuid":"6a0f4c85c883872c6464ceec7f114b0ce82b2976"},"cell_type":"markdown","source":"## Preparing for submission"},{"metadata":{"_uuid":"28c7eb1ea921d09328089f5a1d4b5df20bf59659"},"cell_type":"markdown","source":"Lets start by preparing the test set in the same way we've engineered the train set.\n\n(In future i'd look to combine the sets, it's well explained why in this kernel https://www.kaggle.com/pliptor/how-am-i-doing-with-my-score)"},{"metadata":{"trusted":true,"_uuid":"b1da1269a0509169949340f414da78940924e36f","collapsed":true},"cell_type":"code","source":"# initially check how many NA in Age\n# make a copy\ntest_df1 = test_df\n# check NA\ntest_df1['Age'].isna().sum()","execution_count":55,"outputs":[]},{"metadata":{"_uuid":"589e531764063385f7916893ee1c657355b07750"},"cell_type":"markdown","source":"We'll impute the missing ages in our test set using our pre defined function."},{"metadata":{"trusted":true,"_uuid":"7c4d5d892550f9f63566444e9e98abce3cad81ca","collapsed":true},"cell_type":"code","source":"# make title column to help age imputer\ntitle_maker(test_df1)\n# impute age with function\nage_imputer(test_df1,train_df)\n# check NAs are gone!\ntest_df1['Age'].isna().sum()","execution_count":56,"outputs":[]},{"metadata":{"_uuid":"dbbac096692744e7738b5386713ef2e711c6d7d1"},"cell_type":"markdown","source":"Then we'll do all subsequent feature engineering in the cell below.\n\nThis creates the simplified feature ranges as performed above."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"dad32cdea4e5b684c2c4a489e4091c7909896f50"},"cell_type":"code","source":"# Engineer features as done on test set\n\n# age to AgeGroups\nbins = [-1,0 ,5 ,12 , 18, 24, 35, 60, np.inf]\n\nlabels = ['Unknown','Baby','Child','Student','Teenager','Young Adult','Adult','Senior']\n\ntest_df1['AgeGroup'] = pd.cut(test_df1['Age'],bins,labels = labels)\n\nage_map = {'Unknown' : 0,\n           'Baby' : 1,\n           'Child' : 2,\n           'Student' : 3,\n           'Teenager' : 4,\n           'Young Adult' : 5,\n           'Adult' : 6,\n           'Senior' : 7}\n\ntest_df1['AgeGroup'] = test_df1['AgeGroup'].replace(age_map)\n\n# Sex to binary\nsex_map = {'male' : 0, 'female' : 1}\n\ntest_df1['Sex'] = test_df1['Sex'].replace(sex_map)\n\n#Cabin to CabinBool\ntest_df1['CabinBool'] = (test_df1['Cabin'].notnull().astype('int'))\n\n# fare to FareGroup\ntest_df1['Fare'] = test_df1['Fare'].fillna(-0.5)\nbins = [-1, 0, 20 ,41 , 60, 81, 100, np.inf]\nlabels = ['Unknown','1-20','21-41','42-60','61-81','82-100','101+']\ntest_df1['FareGroup'] = pd.cut(test_df1['Fare'],bins,labels = labels)\nfare_map = {'Unknown' : 0,'1-20' : 1,'21-41' : 2,'42-60' :3 ,'61-81' : 4,'82-100' : 5,'101+' : 6}\n\ntest_df1['FareGroup'] = test_df1['FareGroup'].replace(fare_map)\n\n# ParchBool\ntest_df1['ParchBool'] = (test_df1['Parch'].apply(lambda x: 1 if x>0 else 0))\n\n#SibSp to SibSpBool\ntest_df1['SibSpBool'] = (test_df1['SibSp'].apply(lambda x: 1 if x>0 else 0))","execution_count":57,"outputs":[]},{"metadata":{"_uuid":"354e46b49bee337caeedb00cce109e6c5e2f6827"},"cell_type":"markdown","source":"Let's check it looks correct."},{"metadata":{"trusted":true,"_uuid":"49aa19482aa059cf9d6547d7b711349801bc0839","collapsed":true},"cell_type":"code","source":"test_df1.head()","execution_count":58,"outputs":[]},{"metadata":{"_uuid":"2d2db71ee9f7e96337f19a5c4b000d08d6c8e896"},"cell_type":"markdown","source":"## Submission file generation\n\nSimplified SVC appears to give the highest most consistent score so we'll try it with that."},{"metadata":{"trusted":true,"_uuid":"e5630d69db668150ccac17690f20056646ef1d6a","collapsed":true},"cell_type":"code","source":"test_df1_pred = test_df1[X_new.columns]\n\ntest_df1_pred.head()","execution_count":59,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"39899496890580ea1c658e53760a897df51a6a2e","collapsed":true},"cell_type":"code","source":"ids = test_df1['PassengerId']\n\nmodel = SVC(random_state=0, gamma=\"auto\")\n\nmodel.fit(X_new,Y)\n\ntest_df1_pred = test_df1[X_new.columns]\n\npredictions = model.predict(test_df1_pred)\n\noutput = pd.DataFrame({'PassengerId' : ids,\n                      'Survived' : predictions})\n\noutput.to_csv('submission.csv', index=False)","execution_count":64,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c8dd009edf3786a2a5f46c7cfcf1013e29491663"},"cell_type":"markdown","source":"# Reflection\nThis kernel ran through to give a score of 0.78947. Which isn't too bad.\n\nI'd highly recommend checking out this discussion on possible scores ([https://www.kaggle.com/c/titanic/discussion/56254](http://)) which highlights some key things to think about when building your model with this dataset and how a lot of luck comes into play here not necessarily true predictive power.\n\nAnyway, its been a great opportunity to get to grips with concepts from the Learn Kernels and develop my understanding of machine learning.\n\nLove to get some feedback, especially on my clunky way of extracting titles!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}