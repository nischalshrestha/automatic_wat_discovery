{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"Well hello there!\nI'm a 17 year old student who was dumb enough to go into data science with not prior coding experience and here we are.\nI'll be trying my best to go along the code and explain it as we get into graphs and all that cool stuff.   \nLet's begin then!"},{"metadata":{"_uuid":"fbd70d8bb1883163a098ec1086b4a712a9af0e0f"},"cell_type":"markdown","source":"#Contents\n    1. Import Necessary Libraries\n    2. Read In and Explore the Data\n    3. Data Analysis\n    4. Data Visualization\n    5. Cleaning Data\n    6. Choosing the Best Model\n    7. Creating Submission File\nAny and all feedback is welcome!"},{"metadata":{"_kg_hide-input":true,"_uuid":"e5c9667a1172344a5271a09dd9492ef3d5d24451"},"cell_type":"markdown","source":"First things first. Let's import the required libraries and tools."},{"metadata":{"trusted":true,"_uuid":"b257abcffe6b31ec7b0b8511920f04008b6c6f04"},"cell_type":"code","source":"#data analysis libraries \nimport numpy as np\nimport pandas as pd\n\n#visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aba51fe55b744657ee5350c8f97516b0dd9602e9"},"cell_type":"markdown","source":"Now that we have all of the libraries in place, let's take a look at our data. \nWe will first import it and read it with the commands: 'pd.csv_read' and 'describe()' "},{"metadata":{"trusted":true,"_uuid":"0973c85996bae125a7ea5ef6adfb0b9557beb986"},"cell_type":"code","source":"#Reading and keep the tables in easy variables\ntrain = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\n#Looking at the closer details\ntrain.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eeca079ee1b18dcbb0b9cd43472f597b3dc11c07"},"cell_type":"markdown","source":"#Data Analysis\nWe have two choices now.\nWe can either take a look at the graph as a whole to see the collumns and some variables with either the 'head()' or the '.sample()' functions,\nor we can use the '.collumns' function to see the collumns on the screen\n"},{"metadata":{"trusted":true,"_uuid":"bcdd014d3a3a25ee6c541bd58ac9506838edb4ba"},"cell_type":"code","source":"#To see the collumn names only\nprint(train.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5baa4afcc6ff1f83a0d93770ccd114bbf44d522f"},"cell_type":"code","source":"#To see it on a graph\ntrain.sample(20)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7e7153f8958fbf45a08a9fc631b2037ba8c47d58"},"cell_type":"markdown","source":"Let's take a look at how our values were stored shall we?\nPandas has a simple built in function, 'dtypes', allowing us to very easily look at the data types."},{"metadata":{"trusted":true,"_uuid":"678e783bd59dc55ed366429e3990151b0f80f309"},"cell_type":"code","source":"print(train.dtypes)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"018970074853567c25f6150c75e51b744f59c731"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"05b1f743b8623bb70d2f59cc4a1814bb750f2dcf"},"cell_type":"markdown","source":"Numeric values: \"PassengerId\", \"Survived\", \"Pclass\", \"Age\", \"SibSp\", \"Parch\", \"Fare\"  \nString values: \"Name\", \"Sex\", \"Embarked\"  \nAlphanumeric values(Numbers and strings together): \"Cabin\", \"Ticket\""},{"metadata":{"_uuid":"64e9df62d63fd1adf8a52dea3b8f79bc2043eadf"},"cell_type":"markdown","source":"Let's see if we have any missing values in any of the columns"},{"metadata":{"trusted":true,"scrolled":false,"_uuid":"74f7270d24e67dd85e1366ef957b06f18180b127"},"cell_type":"code","source":"print(train.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2e3224c53662132f0e4b1c8cc8c62fcf20b5d85c"},"cell_type":"markdown","source":"Yikes!  \nIt seems like we have many missing values from our DataSet.  \n19.86% of the age column is missing.  \nA whooping 70% of the cabin values are missing.   \nOnly 0.22% of the embarked column is missing which shouldn't hurt our graph too much.\nAge factor is important for the survival rate, so we should try and fill in those values as much as we can.\nCabin values are mostly missing so dropping the table may be the wisest option. We can however figure that higher \"fares\" would equal to a higher cabin, therefore making a dependence graph can still give us an idea. It is not necessary but could be certainly interesting.\n    "},{"metadata":{"_uuid":"ce581cf20a9bdf47677c1c9a5bb12af8ad28018b"},"cell_type":"markdown","source":"##Time for predictions!  \nSex: Since people have the conception of \"Women and children first\", it's likely that more women survived than men.  \nAge: Just thinking rationally would let us see that people who were younger (Not  so young that they have to be carried by someone of course) than the most were likely to survive more aswell.  \npclass: This is an interesting one. Higher fares may have let people get cabins from a higher part of the Titanic, which would've allowed easier acces to lifeboats.  \nSibSp/Parch: Who knows, maybe the people travelling alone had better chances. Maybe people's sheer willpower allowed them to survive the harsh event in the end."},{"metadata":{"_uuid":"a75a32058dba8656b03cfd7ab8654f7f7f969888"},"cell_type":"markdown","source":"#Graphing  \nIf you were bored from all the numbers and tables, this part may be more fun for you.\n##Sex Factor:"},{"metadata":{"trusted":true,"_uuid":"ef3abd7ad759feedf7e507cc7f86b303f327f110"},"cell_type":"code","source":"sns.barplot(data= train, x=\"Sex\", y=\"Survived\")\nprint(\"Percentage of females surviving:\", train[\"Survived\"][train.Sex == \"female\"].value_counts(normalize=True)*100)\nprint(\"Number males surviving:\", train[\"Survived\"][train.Sex == \"male\"].value_counts(normalize = True)*100)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"14eaf91ccd8ca781347ed0e977a39c4567cab6bd"},"cell_type":"markdown","source":"As predicted, females had a way higher chance of survival compared to males.  \n\n   ##Age Factor:"},{"metadata":{"trusted":true,"_uuid":"ff45b7dc2cb35c2c9a6fd9643823e0031c88f43f"},"cell_type":"code","source":"train[\"Age\"] = train[\"Age\"].fillna(-0.5)\ntest[\"Age\"]= test[\"Age\"].fillna(-0.5)\nbins = [-1, 0, 5, 12, 18, 24, 35, 60,np.inf]\nlabels = [\"Unknown\",\"Small children\",\"Children\",\"Teens\",\"Adolescent\",\"Young Adults\",\"Adults\",\"Elders\"]\ntrain[\"AgeGroups\"] = pd.cut(train.Age,labels=labels, bins=bins)\ntest[\"AgeGroups\"] = pd.cut(test.Age,labels=labels, bins=bins)\nsns.barplot(data = train,x= \"AgeGroups\",y= \"Survived\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cfc979fb6e67adda82fddcdda35beaeb99315b13"},"cell_type":"code","source":"#pclass Factor\nsns.barplot(data= train, x=\"Pclass\", y=\"Survived\")\nprint(\"Percentage of High earners surviving:\", train[\"Survived\"][train.Pclass == 1].value_counts(normalize=True)*100)\nprint(\"Percentage of middle class earners surviving:\", train[\"Survived\"][train.Pclass == 2].value_counts(normalize = True)*100)\nprint(\"Percentage of low class earners surviving:\", train[\"Survived\"][train.Pclass == 3].value_counts(normalize = True)*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c092fab2fd5f2fc609344ab20ea14e28bbc7ebbb"},"cell_type":"code","source":"#Family factors\nsns.barplot(data =train, x=\"SibSp\",y=\"Survived\")\nprint(\"Survivors with one sibling/Spouse:\", train[\"Survived\"][train[\"SibSp\"] == 1].value_counts(normalize = True)[1]*100)\nprint(\"Survivors with two sibling/Spouse:\", train[\"Survived\"][train[\"SibSp\"] == 2].value_counts(normalize = True)[1]*100)\nprint(\"survivors with three sibling/Spouse:\", train[\"Survived\"][train[\"SibSp\"] == 3].value_counts(normalize = True)[1]*100)\nprint(\"Survivors with four sibling/Spouse:\", train[\"Survived\"][train[\"SibSp\"] == 4].value_counts(normalize = True)[1]*100)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"07dba4ae6e96f5062ccb107be88d56a950a822c4"},"cell_type":"markdown","source":"As more siblings or spouses were present, the probability of survival was also lower. However it is interesting to note that people with 1 sibling/spouse was mos likely to survive is a very interesting  that people with no siblings or sposes were less likely to survive than the ones who had one or two."},{"metadata":{"trusted":true,"_uuid":"ee3af68aebea5553e899542de62857d438bff688"},"cell_type":"code","source":"#Parch Factor\nsns.barplot(data = train,x=\"Parch\",y=\"Survived\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0fe7eaad42522e8f578c53e31e6071a6c51884cb"},"cell_type":"markdown","source":"Cabin Feature\nCabin features are a little bit tricky. They don't really mean anything, unless we assume that the ones recorded were people that were more important, or of a higher economic class."},{"metadata":{"trusted":true,"_uuid":"64f7a3361d71f97331690ebd3064d15a95a1ba29"},"cell_type":"code","source":"train[\"CabinBool\"] = (train.Cabin.notnull().astype(\"int\"))\ntest[\"CabinBool\"] =  (train.Cabin.notnull().astype(\"int\"))\nprint(\"Percentage of recorded Cabins that survived:\",train[\"Survived\"][train[\"CabinBool\"] == 1].value_counts(normalize = True)[1]*100)\nprint(\"Percentage of recorded Cabins that didn't survive:\",train[\"Survived\"][train[\"CabinBool\"] == 0].value_counts(normalize = True)[1]*100)\nsns.barplot(data = train,x=\"CabinBool\",y=\"Survived\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7eb2454239b8f89ee69e3d6a6eaee8045660e38a"},"cell_type":"markdown","source":"Time to clean up the dataset\nWe should take a look at out test dataset aswell for this one"},{"metadata":{"trusted":true,"_uuid":"f633377027cf0a3671919009507b75d287512a15"},"cell_type":"code","source":"test.describe(include=\"all\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fb0131de58ce47e86ea52f1570ea36d4fbb65409"},"cell_type":"markdown","source":"By using our own intuition and the results of our tests and graphs, let's drop some of the columns that don't affect the results or fill in the columns with the appropriate values.\n(Don't forget to drop the same columns from the two datasets)"},{"metadata":{"trusted":true,"_uuid":"4156e065977a0f2f83e0487aa41982f13b40a661"},"cell_type":"code","source":"train = train.drop([\"Ticket\"],axis = 1)\ntest = test.drop(['Ticket'], axis = 1)\n\ntrain = train.drop([\"Cabin\"],axis = 1)\ntest = test.drop([\"Cabin\"],axis = 1)\n\ntrain = train.drop([\"CabinBool\"],axis = 1)\ntest = test.drop([\"CabinBool\"],axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f4c564a359174ee6ee42071f86e53f5ba3001887"},"cell_type":"markdown","source":"Embarked"},{"metadata":{"trusted":true,"_uuid":"c76529261801bcc1f32a75d71e5980ae4fec4589"},"cell_type":"code","source":"print(\"Number of people embarking in Southampton (S):\",train[train[\"Embarked\"] == \"S\"].shape[0])\n\nprint(\"Number of people embarking in Cherbourg (C):\",train[train[\"Embarked\"] == \"C\"].shape[0])\n\nprint(\"Number of people embarking in Queenstown (Q):\",train[train[\"Embarked\"] == \"Q\"].shape[0])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2297a566a605765f72509d30a50c2b7af3a55c63"},"cell_type":"markdown","source":"We see that most embarked from Southhampton, so we can fill in the empty values with \"S\""},{"metadata":{"trusted":true,"_uuid":"da8fe742e6c73e08b1ffde1c3e42fb20c6e3f643"},"cell_type":"code","source":"#replacing the missing values in the Embarked feature with S\ntrain = train.fillna({\"Embarked\": \"S\"})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"449be2960af558a3351362f881f6e532b71f864d"},"cell_type":"code","source":"#create a combined group of both datasets\ncombine = [train, test]\n\n#extract a title for each Name in the train and test datasets\nfor dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train['Title'], train['Sex'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"52da47d6b190de4bc4ddabb04c1d524bad31ee6b"},"cell_type":"code","source":"#Replace the titles with numbers that would let out computer understand\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Capt', 'Col',\n    'Don', 'Dr', 'Major', 'Rev', 'Jonkheer', 'Dona'], 'Rare')\n    \n    dataset['Title'] = dataset['Title'].replace(['Countess', 'Lady', 'Sir'], 'Royal')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n\ntrain[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c4540abde445bdb2d11b4d56ca604e8bae82bf58"},"cell_type":"code","source":"title_map = {\"Master\":1,\"Miss\":2,\"Mr\":3,\"Mrs\":4,\"Rare\":5,\"Royal\":5}\nfor dataset in combine:\n    dataset[\"Title\"] = dataset[\"Title\"].map(title_map)\n    dataset[\"Title\"] = dataset[\"Title\"].fillna(0)\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1d7f0b86193736f0c4bb1ece3ea49b351a9a4c79"},"cell_type":"code","source":"Master_age = train[train[\"Title\"]==1][\"AgeGroups\"].mode()#Small children (Somehow, don't ask me)\nMs_age = train[train[\"Title\"]==2][\"AgeGroups\"].mode() #Teens\nMr_age = train[train[\"Title\"]==3][\"AgeGroups\"].mode() #Young Adults\nMrs_age = train[train[\"Title\"]==4][\"AgeGroups\"].mode() #Adults\nRare_age = train[train[\"Title\"]==5][\"AgeGroups\"].mode() #Adult\nRoyal_age = train[train[\"Title\"]==6][\"AgeGroups\"].mode() #Adults\n#Demonstrating what I'm doing when using .mode\nprint(Master_age)\nprint(Ms_age)\nprint(Mrs_age)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b778a41e673df4184e9a43741c539bd13f0d97b1"},"cell_type":"code","source":"title_age_map = {1: \"Small children\", 2: \"Teens\", 3: \"Young Adults\", 4: \"Adults\", 5: \"Adults\", 6: \"Adults\"}\n\nfor x in range(len(train[\"AgeGroups\"])):\n    if train[\"AgeGroups\"][x] == \"Unknown\":\n        train[\"AgeGroups\"][x] = title_age_map[train[\"Title\"][x]]\n\nfor x in range(len(test[\"AgeGroups\"])):\n    if test[\"AgeGroups\"][x] == \"Unknown\":\n        test[\"AgeGroups\"][x] = title_age_map[train[\"Title\"][x]]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d1e7223229cd86fa837ccf960c9e4e4ca9f03344"},"cell_type":"markdown","source":"Now that we filled in the missing values, let's turn those into numbers that our computer can process easily."},{"metadata":{"trusted":true,"_uuid":"73778d08394fd98417160e791b9efcf863db995c"},"cell_type":"code","source":"Age_map = {\"Small children\": 1, \"Children\": 2, \"Teens\": 3, \"Adolescent\": 4, \"Young Adults\": 5, \"Adults\": 6, \"Elders\": 7}\n\ntrain[\"AgeGroups\"] = train[\"AgeGroups\"].map(Age_map)\ntest[\"AgeGroups\"] = test[\"AgeGroups\"].map(Age_map)\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"304ae9ae14d373057740340ed30e06a58d72a67e"},"cell_type":"markdown","source":"Now that we are finally extracted everything we could from the name and the age columns, we can now get rid of them."},{"metadata":{"trusted":true,"_uuid":"ce36de6f3ab69b2552a2b96018a5f410b5a64cff"},"cell_type":"code","source":"#drop the name feature since it contains no more useful information.\ntrain = train.drop([\"Name\"], axis = 1)\ntest = test.drop([\"Name\"], axis = 1)\ntrain = train.drop([\"Age\"], axis = 1)\ntest = test.drop([\"Age\"], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bd115fe8a74d8792896bbfc5103b74376ee793d2"},"cell_type":"markdown","source":"Sex Feature"},{"metadata":{"trusted":true,"_uuid":"b00623be2d5e4c83ae8fc5de538e83c7e2fe20bd"},"cell_type":"code","source":"#map each Sex value to a numerical value\nsex_mapping = {\"male\": 0, \"female\": 1}\ntrain['Sex'] = train['Sex'].map(sex_mapping)\ntest['Sex'] = test['Sex'].map(sex_mapping)\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"97071b4002d8080ef93c6d0678c4495a7141a5ad"},"cell_type":"markdown","source":"Embarked Feature"},{"metadata":{"trusted":true,"_uuid":"3954f0f71732143a50f9b1eb868fc84b467196bd"},"cell_type":"code","source":"#map each Embarked value to a numerical value that can be read by the computer. Almost there!\nembarked_mapping = {\"S\": 1, \"C\": 2, \"Q\": 3}\ntrain['Embarked'] = train['Embarked'].map(embarked_mapping)\ntest['Embarked'] = test['Embarked'].map(embarked_mapping)\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f6c77651b01cd86ad102250db1a4eda4e1efc505"},"cell_type":"markdown","source":"Finally, the fare feature.  \nWe'll try and categorize this into more logical \"bins\""},{"metadata":{"trusted":true,"_uuid":"b45f6f265ad8c5a50861adaed456d8fe0bb1a8d4"},"cell_type":"code","source":"print(train.Fare.max())\nprint(train.Fare.min())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"60f4e88293c8bee57f369e71174c11d92a2e927f","trusted":true},"cell_type":"code","source":"#fill in missing Fare value in test set based on mean fare for that Pclass \nfor x in range(len(test[\"Fare\"])):\n    if pd.isnull(test[\"Fare\"][x]):\n        pclass = test[\"Pclass\"][x] #Pclass = 3\n        test[\"Fare\"][x] = round(train[train[\"Pclass\"] == pclass][\"Fare\"].mean(), 4)\n        \n#map Fare values into groups of numerical values\ntrain['FareBand'] = pd.qcut(train['Fare'], 4, labels = [1, 2, 3, 4])\ntest['FareBand'] = pd.qcut(test['Fare'], 4, labels = [1, 2, 3, 4])\n\n#drop Fare values\ntrain = train.drop(['Fare'], axis = 1)\ntest = test.drop(['Fare'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c863a96e1f676e086050a8600e201f49908acd6d"},"cell_type":"code","source":"#Check train data to see what we ended up with\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"af40a5b7c2ec7c247c0e06093f5c549b21152423"},"cell_type":"code","source":"#Check test data\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"13a766f01f7fea940c40e87cd4bd20d153013e87"},"cell_type":"markdown","source":"6) Choosing the Best Model"},{"metadata":{"_uuid":"9748731e32570447413f93e2c3d62484a813d48a"},"cell_type":"markdown","source":"Splitting the Training Data  \nWe will use part of our training data (22% in this case) to test the accuracy of our different models."},{"metadata":{"trusted":true,"_uuid":"c1e2317ac4ed15e682d05df1414782e32c044adc"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\npredict = train.drop(['Survived', 'PassengerId'], axis=1)\ntarget = train[\"Survived\"]\nx_train, x_val, y_train, y_val = train_test_split(predict, target, test_size = 0.22, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3811ac3839a9897333871ab619c1c52ef7914f72"},"cell_type":"markdown","source":"Testing Different Models  \nI will be testing the following models with my training data (got the list from [here](http://www.kaggle.com/nadintamer/titanic-survival-predictions-beginner/notebook)): \n  \n*   Gaussian Naive Bayes  \n*   Logistic Regression  \n*   Support Vector Machines  \n*   Perceptron  \n*   Decision Tree Classifier  \n*   Random Forest Classifier  \n*   KNN or k-Nearest Neighbors  \n*   Stochastic Gradient Descent  \n*   Gradient Boosting Classifier  \n*   For each model, we set the model, fit it with 80% of our training data, predict for 20% of the training data and check the accuracy."},{"metadata":{"trusted":true,"_uuid":"5182be4f3681080d7ab2e1e1247e85e626aba3ba"},"cell_type":"code","source":"# Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\ngaussian = GaussianNB()\ngaussian.fit(x_train, y_train)\ny_pred = gaussian.predict(x_val)\nacc_gaussian = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_gaussian)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d313ffa735fac84ab9d9c5d908ab321f3d9cfee2"},"cell_type":"code","source":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\nlogreg.fit(x_train, y_train)\ny_pred = logreg.predict(x_val)\nacc_logreg = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_logreg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ed8d1e19fe7e1ad813b8491b47340a25fc066749"},"cell_type":"code","source":"# Support Vector Machines\nfrom sklearn.svm import SVC\n\nsvc = SVC()\nsvc.fit(x_train, y_train)\ny_pred = svc.predict(x_val)\nacc_svc = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_svc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5ffa242c7f6b0322b87d734ec5f2d521a3ab85ae"},"cell_type":"code","source":"# Linear SVC\nfrom sklearn.svm import LinearSVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(x_train, y_train)\ny_pred = linear_svc.predict(x_val)\nacc_linear_svc = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_linear_svc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b8bc22d106ae434aff26955250955b5a86210e0e"},"cell_type":"code","source":"# Perceptron\nfrom sklearn.linear_model import Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(x_train, y_train)\ny_pred = perceptron.predict(x_val)\nacc_perceptron = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_perceptron)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6457528a824987c4ad5065f6c3fe46407b98412b"},"cell_type":"code","source":"#Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\n\ndecisiontree = DecisionTreeClassifier()\ndecisiontree.fit(x_train, y_train)\ny_pred = decisiontree.predict(x_val)\nacc_decisiontree = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_decisiontree)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6b15501d0a0568398f2b7c5c4dbe1a007884ce9d"},"cell_type":"code","source":"# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n\nrandomforest = RandomForestClassifier()\nrandomforest.fit(x_train, y_train)\ny_pred = randomforest.predict(x_val)\nacc_randomforest = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_randomforest)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"74053d9c62d2ee0553f5b85381df7a32fff071bb"},"cell_type":"code","source":"# KNN or k-Nearest Neighbors\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\nknn.fit(x_train, y_train)\ny_pred = knn.predict(x_val)\nacc_knn = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_knn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"24101a8c853b098609904d4da3f1c59327c2cf67"},"cell_type":"code","source":"# Stochastic Gradient Descent\nfrom sklearn.linear_model import SGDClassifier\n\nsgd = SGDClassifier()\nsgd.fit(x_train, y_train)\ny_pred = sgd.predict(x_val)\nacc_sgd = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_sgd)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"60aea31879f892d22f09e52bcd10111056b83954"},"cell_type":"code","source":"# Gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngbk = GradientBoostingClassifier()\ngbk.fit(x_train, y_train)\ny_pred = gbk.predict(x_val)\nacc_gbk = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_gbk)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2784cd53dbd3515f44a1b3ff879c72e54d70770d"},"cell_type":"code","source":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', 'Linear SVC', \n              'Decision Tree', 'Stochastic Gradient Descent', 'Gradient Boosting Classifier'],\n    'Score': [acc_svc, acc_knn, acc_logreg, \n              acc_randomforest, acc_gaussian, acc_perceptron,acc_linear_svc, acc_decisiontree,\n              acc_sgd, acc_gbk]})\nmodels.sort_values(by='Score', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"300c4cf6d2c87d003b80a36dcf5298da1d3f25cb"},"cell_type":"markdown","source":"I decided to use the Gradient Boosting Classifier since it has a higher score overall."},{"metadata":{"_uuid":"0b51429dd0058321ab0edc7eb996b12d4342f10f"},"cell_type":"markdown","source":"7) Creating Submission File  \nIt's time to create a submission.csv file to upload to the Kaggle competition!"},{"metadata":{"trusted":true,"_uuid":"3488129ee8c0654c25e590c514a4722b53e68350"},"cell_type":"code","source":"#set ids as PassengerId and predict survival \nids = test['PassengerId']\npredictions = gbk.predict(test.drop('PassengerId', axis=1))\n\n#set the output as a dataframe and convert to csv file named submission.csv\noutput = pd.DataFrame({ 'PassengerId' : ids, 'Survived': predictions })\noutput.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4175dcd094084828c945477a37486a9e2b4cd402"},"cell_type":"markdown","source":"That was one hell of a ride!  \nIf you made it all the way here, congrats you're doing amazing :)  \nIf anyone has ay questions or suggestions regarding the process, I'd be more than happy to help!  \nHope you have a wonderfull day and keep on coding!"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}