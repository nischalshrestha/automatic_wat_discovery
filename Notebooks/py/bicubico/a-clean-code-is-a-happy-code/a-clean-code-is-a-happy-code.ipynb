{"cells":[{"metadata":{"_uuid":"54db0b849bab881ea2de0436eea61db44f7c06d4"},"cell_type":"markdown","source":"First: Prepare the environment, import all relevant libraries"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c53a9fa354d546668c1a0e029a0c384eeb8271cd"},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import make_pipeline\nfrom xgboost import XGBRegressor\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","execution_count":51,"outputs":[]},{"metadata":{"_uuid":"aeb7467853af8dd687fae3b987a246363cc48a6f"},"cell_type":"markdown","source":"Load the data for train and test sets. Check columns (later we'll need their names)"},{"metadata":{"trusted":true,"_uuid":"7fb87a36cc7e08b0682e30c80ee33ee6bb3a8beb","collapsed":true},"cell_type":"code","source":"# Datasources\ntrain_data = pd.read_csv('../input/train.csv')\ntest_data = pd.read_csv('../input/test.csv')","execution_count":52,"outputs":[]},{"metadata":{"_uuid":"31a50d60066c5e6ed707628a1995dbce64f20704"},"cell_type":"markdown","source":"**Data Analytics**"},{"metadata":{"_uuid":"92cb07d39e97330799e78f78280bb038f20986fe"},"cell_type":"markdown","source":"Lets take a look at the train data"},{"metadata":{"trusted":true,"_uuid":"ab43f1f4a14d450b9f6c54f2592698906d9843b5"},"cell_type":"code","source":"train_data.head(20)","execution_count":53,"outputs":[]},{"metadata":{"_uuid":"e31bf1b171a1754aabc90a5da81bb5516e82d354"},"cell_type":"markdown","source":"Store the Passenger Id of the test set in an array, so later we can submit survival predictions linked to each Passenger Id"},{"metadata":{"trusted":true,"_uuid":"165ac3030bd69aefa6d5d77ecca14309fc59bd74","collapsed":true},"cell_type":"code","source":"test_id = test_data.PassengerId","execution_count":54,"outputs":[]},{"metadata":{"_uuid":"3c9e055fba73f003fc886385aa78e0c7c5aba8f6"},"cell_type":"markdown","source":"Store the survival status, so later we can fit the model using the train data (as X) and the target (as y)."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"34167bb461314fcf0d54afc7e39c35b26b6684d5"},"cell_type":"code","source":"#Prediction Target\n#Single column on train data that contains the prediction\ntrain_y = train_data.Survived","execution_count":55,"outputs":[]},{"metadata":{"_uuid":"0d2937cdb52d0f8b405b40e7775d7ef6277c5553"},"cell_type":"markdown","source":"Cleaning: Remove some columns"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"6b3e969ab87d7bcd607c6c366665fd8e49018e12"},"cell_type":"code","source":"#cols_with_missing_values = [col for col in train_data.columns\n#                                   if train_data[col].isnull().any()]\ncols_with_missing_values=['Age','Cabin']\ntrain_X = train_data.drop(['PassengerId','Survived']+cols_with_missing_values, axis=1)\ntest_X = test_data.drop(['PassengerId']+cols_with_missing_values, axis=1)","execution_count":56,"outputs":[]},{"metadata":{"_uuid":"e04da6e8be1f61021de4bf9fbc10eb1fab98a5b5"},"cell_type":"markdown","source":"Handling Categorical Values:  Selecting columns for One Hot Enconding depending on how many different values they have. If they have fewer than 10 different values, then we'll use it for One Hot Encoding"},{"metadata":{"trusted":true,"_uuid":"a2eb18b9976eed8fac14dada29bb1dd321da288c","collapsed":true},"cell_type":"code","source":"# Categorical values. \n# Choosing only those columns for on hot encodding where the categorical value for any attribute is not more than 10\nlow_cardinality_cols = [cname for cname in train_X.columns\n                                       if train_X[cname].nunique()< 10 and\n                                       train_X[cname].dtype==\"object\"]\nnumeric_cols = [cname for cname in train_X.columns\n                               if train_X[cname].dtype in ['int64', 'float64']]\n\nuseful_cols = low_cardinality_cols + numeric_cols\ntrain_X = train_X[useful_cols]\ntest_X = test_X[useful_cols]","execution_count":57,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5cec58c15f83a804ac3b5abe7de528e59e35343e","collapsed":true},"cell_type":"code","source":"def pairplot(X,y):\n    X['y'] = y\n    sns.pairplot(X,hue='y')\n    X=X.drop(['y'],axis=1)\n    return ","execution_count":58,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fda5224cf153d2c0f0c5ed49dcb1c0c404e1a42a","collapsed":true},"cell_type":"code","source":"#pairplot(train_X,train_y)\n#train_X=train_X.drop(['y'],axis=1)","execution_count":59,"outputs":[]},{"metadata":{"_uuid":"6c8544e3e00bb29e3c555d57c6b7af8ad4449e76"},"cell_type":"markdown","source":"Lets add some extra info, like family size"},{"metadata":{"trusted":true,"_uuid":"2149cdf9bd4602bc4f393539fbaf5f912d505d4c","collapsed":true},"cell_type":"code","source":"def data_transform(df):\n    df['FamilySize'] = df['SibSp']+df['Parch']\n    df['CuicoHijoUnico'] = (4-df['Pclass'])/(df['Parch']+1)\n    df = pd.get_dummies(df)\n    return df\n\ntrain_X = data_transform(train_X)\ntest_X = data_transform(test_X)","execution_count":60,"outputs":[]},{"metadata":{"_uuid":"f79bb0ef0862be464d37dddbc0ae61b839c3b671"},"cell_type":"markdown","source":"Ready for fitting the model."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"my_pipeline=make_pipeline(SimpleImputer(),XGBRegressor())\nmy_pipeline.fit(train_X, train_y)\n\n#Get Predictions\npredictions = np.around(my_pipeline.predict(test_X),0).astype(np.int64)","execution_count":65,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"099c60afb85a10a348f6bea376b4c333433a3e89"},"cell_type":"code","source":"","execution_count":63,"outputs":[]},{"metadata":{"_uuid":"28436dfba964103df00dbb90d5fef8c3a530b966"},"cell_type":"markdown","source":"Preparing submission file, just joining PassengerID and Predictions and putting them in a single dataframe."},{"metadata":{"trusted":true,"_uuid":"10c980951b8308b4def861f4980b089bded0f4d0"},"cell_type":"code","source":"#Submit predictions\nmy_submission = pd.DataFrame({'PassengerId': test_id, 'Survived': predictions})\nmy_submission.describe()\n","execution_count":43,"outputs":[]},{"metadata":{"_uuid":"d3a0014e92dfdaf8b69adf6cd5cdfc0b03c50ce1"},"cell_type":"markdown","source":"Check if the submision data seems to be ok"},{"metadata":{"trusted":true,"_uuid":"9b1f19c7dd2c62b1273fc3d12590b8a07d01349b"},"cell_type":"code","source":"my_submission.head(10)","execution_count":44,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"734fd5e6a0c5836a1e546dc4f02290031a32fb75"},"cell_type":"code","source":"my_submission.to_csv('submission.csv', index=False)","execution_count":45,"outputs":[]},{"metadata":{"_uuid":"e4a05d5735bde9beb222c503699d005aa9944ead"},"cell_type":"markdown","source":"149919 = 0.7942\n\n151210 = 0.786"},{"metadata":{"trusted":true,"_uuid":"e7bf891461878e8645fe67384ee951d4a4601f93"},"cell_type":"code","source":"my_submission['producto']=my_submission['PassengerId']*my_submission['Survived']\nmy_submission['producto'].values.sum()","execution_count":46,"outputs":[]},{"metadata":{"_uuid":"e7de6db638070346cbe9345293833984650ed85c"},"cell_type":"markdown","source":"Generate the submission file"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"cb36ae642f03d81442a8e91ea6f3a38dc272964f"},"cell_type":"code","source":"","execution_count":15,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c450a3eeaef53284c445ea2c2dfd74fc7b38c4e4"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}