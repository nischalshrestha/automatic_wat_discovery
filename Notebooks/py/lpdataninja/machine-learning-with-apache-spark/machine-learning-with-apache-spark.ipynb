{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"### Apache Spark : The Unified Analytics Engine\nThe largest open source project in data processing framework that can do ETL, analytics, machine learning and graph processing on large volumes of data at rest (batch processing) or in motion (streaming processing) with rich high-level APIs for the programming languages like Scala, Python, Java and R.\n\nSpark has seen immense growth over the past several years. Hundreds of contributors working collectively have made Spark an amazing piece of technology powering the de facto standard for big data processing and data sciences across all industries. \n\n![](http://)![](http://)Internet powerhouses such as Netflix, Yahoo, and eBay have deployed Spark at massive scale, collectively processing multiple petabytes of data on clusters of over 8,000 nodes. "},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"### Why Spark ?\n\nTypically when you think of a computer you think about one machine sitting on your desk at home or at work. \nThis machine works perfectly well for applying machine learning on small dataset . However, when you have huge dataset(in tera bytes or giga bytes), there are some things that your computer is not powerful enough to perform.\nOne particularly challenging area is data processing. Single machines do not have enough power and resources to perform\ncomputations on huge amounts of information (or you may have to wait for the computation to finish).\n\nA cluster, or group of machines, pools the resources of many machines together allowing us to use all the cumulative\nresources as if they were one. Now a group of machines alone is not powerful, you need a framework to coordinate\nwork across them. Spark is a tool for just that, managing and coordinating the execution of tasks on data across a\ncluster of computers."},{"metadata":{"_uuid":"a29617b6d56d5a1f4780368714dda4a80d426174"},"cell_type":"markdown","source":"### Spark Architecture\n\nApache Spark allows you to treat many machines as one machine and this is done via a master-worker type architecture where there is a driver or master node in the cluster, accompanied by worker nodes. The master sends work to the workers and either instructs them to pull to data from memory or from disk (or from another data source).\n"},{"metadata":{"_uuid":"43ea8e15be98ad9dfd46cffdd0dce79994e3d059"},"cell_type":"markdown","source":"### Read more about Architecture\nhttps://spark.apache.org/docs/latest/cluster-overview.html"},{"metadata":{"trusted":true,"_uuid":"b74808f1ee70b232b446345087aad23ca071e59f"},"cell_type":"markdown","source":"### Spark Applications\n\nSpark Applications consist of a driver process and a set of executor processes. The driver process runs your main()\nfunction, sits on a node in the cluster, and is responsible for three things: maintaining information about the Spark\nApplication; responding to a user’s program or input; and analyzing, distributing, and scheduling work across the\nexecutors (defined momentarily). The driver process is absolutely essential - it’s the heart of a Spark Application and\nmaintains all relevant information during the lifetime of the application.\n\nThe executors are responsible for actually executing the work that the driver assigns them. This means, each\nexecutor is responsible for only two things: executing code assigned to it by the driver and reporting the state of the\ncomputation, on that executor, back to the driver node."},{"metadata":{"_uuid":"bb9d5ec16793002b06ece8480df110b897e04d1f"},"cell_type":"markdown","source":"### Running Code\n\nI am using freely available databricks stand alone community edition server (https://community.cloud.databricks.com) as Spark library currently not available in Kaggle directly.\n\nHere is Spark code for LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, Gradient-boosted tree classifier, NaiveBayes & Support Vector Machine on Titanic dataset\n\n##### Databricks Notebook :\nhttps://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/5722190290795989/3865595167034368/8175309257345795/latest.html\n\n##### Github :\nhttps://github.com/lp-dataninja/SparkML/blob/master/kaggle-titanic-pyspark.ipynb\n"},{"metadata":{"_uuid":"9cbfbcc68c734a1fd822344dd34c53b9cef580f0"},"cell_type":"markdown","source":"### Reference \n\nhttps://docs.databricks.com/spark/latest/gentle-introduction/gentle-intro.html\n\nhttps://docs.databricks.com/spark/latest/gentle-introduction/gentle-intro.html#gentle-introduction-to-apache-spark\n\nhttps://docs.databricks.com/spark/latest/gentle-introduction/for-data-scientists.html"},{"metadata":{"trusted":true,"_uuid":"095c1fae8ec410b2eeacc6504164a71cb399569b"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}