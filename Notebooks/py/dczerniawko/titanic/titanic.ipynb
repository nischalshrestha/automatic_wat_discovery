{"cells":[{"metadata":{"collapsed":true,"_uuid":"1531e7c0e154fc672ba5a3b1115dbb3acff91c45"},"cell_type":"markdown","source":"# Titanic: Machine Learning from Disaster\n\"Titanic: Machine Learning from Disaster\" is [Kaggle competition](https://www.kaggle.com/c/titanic),\nwhich is starting my adventure with machine learning.\nThe goal is to predict if a passenger survived the sinking of the Titanic or not.\n\n\nCompetition contain dataset presented in table:\n\nVariable|Definition|Key\n:-------|:--------:|---\nSurvival|Survival|0 = No<br>1 = Yes\nPclass|Ticket class|1 = 1st<br>2 = 2nd<br>3 = 3rd\nSex|Sex|\nAge|Age in years|\nSibSp|# of siblings / spouses<br>aboard the Titanic|\nParch|# of parents / children<br>aboard the Titanic|\nTicket|Ticket number|\nFare|Passenger fare|\nCabin|Cabin number|\nEmbarked|Port of Embarkation|C = Cherbourg<br>Q = Queenstown<br>S = Southampton\n\n\nAt the beggining, we need import libraries which I used later:"},{"metadata":{"scrolled":true,"trusted":true,"collapsed":true,"_uuid":"ae6ce22b76dc8c568f8cc62e59bf23a798bc45c5"},"cell_type":"code","source":"# data manipulation \nimport numpy as np\nimport pandas as pd\n\n# data visualization \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import learning_curve\n%matplotlib inline\n\n# preprocesing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_validate\n\n# algorithms\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nimport xgboost as xgb\n\n# metrics\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# optimization\nfrom functools import partial\nfrom hyperopt import hp, fmin, tpe\n\n# ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"8a6795833da39d80727497341a2b05a49ab28711"},"cell_type":"markdown","source":"Next I defined required functions to analyze, train and predict model:"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"b0f3241e608ff8a9fd247afec681efedb65e4845"},"cell_type":"code","source":"# Look for missing values in DataFrame\ndef missing_values(df):\n    for column in df.columns:\n        null_rows = df[column].isnull()\n        if null_rows.any() == True:\n            print('%s: %d nulls' % (column, null_rows.sum()))\n\ndef survived_plot(feature):\n    all = df_train.groupby(('Survived', feature)).size().unstack()\n    all.index = ['Survived', 'Dead']\n    all.plot(kind='bar', stacked=True,\n             title = 'Who survived?', figsize = (12, 6))\n\ndef good_feats(df):\n    feats_from_df = set(df.select_dtypes([np.int, np.float]).columns.values)\n    bad_feats = {'PassengerId', 'Survived', 'SibSp', 'Parch'}\n    return list(feats_from_df - bad_feats)\n\ndef factorize(df, *columns):\n    for column in columns:\n        df[column + '_cat'] = pd.factorize(df[column])[0]\n\ndef plot_learning_curve(model, title, X, y, ylim=None, cv = None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \n    plt.figure(figsize=(12,8))\n    plt.title(title)\n    if ylim is not None:plt.ylim(*ylim)\n\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    \n    train_sizes, train_scores, test_scores = learning_curve(\n        model, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Testing score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\ndef title_feat(*dfs):\n    for df in dfs:\n        title = df['Name'].apply(lambda x:\n                                       x.split(',')[1].split('.')[0].strip())\n        # title mapping\n        title_map = {'Mr':1, 'Miss':2, 'Mrs':3, 'Master': 2, 'Rev': 5,\n                     'Dr':6, 'Col':8, 'Mlle':2, 'Major':8, 'Ms':1,\n                     'the Countess':3, 'Don':1, 'Capt':8, 'Lady':4, 'Mme':4,\n                     'Sir':4, 'Jonkheer':7, 'Dona':3}\n        df['Title_cat'] = title.apply(lambda x: title_map[x])\n    \ndef age_feat(*dfs):\n    for df in dfs:\n        age_title = df.groupby(['Title_cat'])['Age'].median().to_dict()\n        df['Age'] = df.apply(lambda row: age_title[row['Title_cat']]\n                                         if pd.isnull(row['Age'])\n                                         else row['Age'], axis=1)\n        df['Age_norm'] = pd.cut(df['Age'],[0,5,18,35,60,80])\n        factorize(df, 'Age_norm')\n    \ndef family_feat(*dfs):\n    for df in dfs:\n        df['Family'] = df['SibSp'] + df['Parch'] + 1\n        \ndef model_train_predict(model, X, y, success_metric=accuracy_score):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    return success_metric(y_test, y_pred)","execution_count":2,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"38b8383b2b68d93923e20f0f93ebe3a6a7c1e881"},"cell_type":"code","source":"# load data\ndf_train = pd.read_csv('../input/train.csv')\ndf_test = pd.read_csv('../input/test.csv')\ndf_test['Survived'] = np.nan","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"25107800a55ba7730a84ff70fc59f220bbbc02cb"},"cell_type":"markdown","source":"# Collect some important information"},{"metadata":{"trusted":false,"_uuid":"f41f96795de23e2ce6e0867454739915b1cea1f2","collapsed":true},"cell_type":"code","source":"# simple info about train dataset\ndf_train.info()","execution_count":4,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c843aed94fbcc1ef41498a16d485406406cf8422","collapsed":true},"cell_type":"code","source":"# get five first rows\ndf_train.head()","execution_count":5,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"415539b74c4748642f5769beca1a610191c5017e","collapsed":true},"cell_type":"code","source":"# get five random rows\ndf_train.sample(5)","execution_count":6,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c9bcc04beb1dd7146e2b8cfbbcccb006869aefcd","collapsed":true},"cell_type":"code","source":"# statistic information about training set\ndf_train.describe()","execution_count":7,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1a40aeb42a09d212c3167ed12574ac6081191038","collapsed":true},"cell_type":"code","source":"# columns names\ndf_train.columns","execution_count":8,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"5ee16f831d8c9fe51f3ab4075df0c606b006c1cd","collapsed":true},"cell_type":"code","source":"# columns correlation\ndf_train.corr()","execution_count":9,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"43a8966b05a689d44e92d1a29d45b7852cc029fe","collapsed":true},"cell_type":"code","source":"# visualization correlation\nplt.rcParams['figure.figsize']=(15,7)\nsns.heatmap(df_train.corr().abs(), annot=True, linewidths=.5, cmap=\"Blues\");","execution_count":10,"outputs":[]},{"metadata":{"_uuid":"ef878ad8daaf1b82b46c17b37435ad4331dd45fd"},"cell_type":"markdown","source":"Two important things: \n    - correlation between two features (should be removed from model)\n    - correlation between feature and target variable (should be added to model)\n    \nIn this case we have correlated two features: Parch and SibSp, so we can deduce, that marrieds had children.\n\nNext correlation, which is worth paying attention is correlation between target variable and feature Fare. I think, that it is main point to study. If someone was payed to much for, a ticket, that had better chance of survival."},{"metadata":{"collapsed":true,"_uuid":"c46c6f0b8655c8d7616e0c1b355e40018cb874f7"},"cell_type":"markdown","source":"# How Titanic sank?\n\n![Sinking of the RMS Titanic animation](images/Sinking_of_the_RMS_Titanic_animation.gif)\n\nhttps://en.wikipedia.org/wiki/April_1912#/media/File:Sinking_of_the_RMS_Titanic_animation.gif\n\n# Where was the lifeboats?\n\n![Titanic_Boat_Deck_plan_with_lifeboats.png](images/Titanic_Boat_Deck_plan_with_lifeboats.png)\n\nhttps://en.wikipedia.org/wiki/Lifeboats_of_the_RMS_Titanic#/media/File:Titanic_Boat_Deck_plan_with_lifeboats.png\n\nI think it's very important questions, because depending on where someone had a cabin, they had a different chance of survival. Some cabins were sunk at the very beginning. Other cabins were far to the lifeboats.\n\n# Visualization\nI used groupby() method to group survivors with feature, but I found another method in someone repository, so I compared them:"},{"metadata":{"trusted":false,"_uuid":"626201ec68a5fa919058f67fbecee7859fd83c85","collapsed":true},"cell_type":"code","source":"# curiosity about function\ndef func():\n    together = df_train.groupby(['Sex', 'Survived']).size()\n    \ndef func2():\n    survived = df_train[df_train.Survived == 1]['Sex'].value_counts()\n    dead = df_train[df_train.Survived == 0]['Sex'].value_counts()\n\nprint('Calculation time for groupby():')\n%time for i in range(10000): func()\nprint('Calculation time for another method:')\n%time for i in range(10000): func2()","execution_count":11,"outputs":[]},{"metadata":{"_uuid":"1c8490434b9802a56d1150ff1b5fac23741320e5"},"cell_type":"markdown","source":"As you can see, function with groupby() is three times faster than alternative. Why? The second function browses dataset twice, but groupby() in first function browse it only once."},{"metadata":{"trusted":false,"_uuid":"aa2a0192fe76d9f65ebec4ba6e3237217630845d","collapsed":true},"cell_type":"code","source":"# who survived? per sex\nsurvived_plot('Sex')","execution_count":12,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"26b2ac3aa8579ee8ec65b9e2f0c0b4fc53c9ecc8","collapsed":true},"cell_type":"code","source":"# who survived? per pclass\nsurvived_plot('Pclass')","execution_count":13,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b76ece06800b1aabfbbd01f3749b4d664173d7d0","collapsed":true},"cell_type":"code","source":"# survival possibility dependent on age\nsns.set(style=\"darkgrid\")\n\nsns.FacetGrid(df_train, hue = 'Survived', size = 2.5,\n              aspect = 5, palette = 'Blues') \\\n  .map(sns.kdeplot, 'Age', shade=True) \\\n  .set(xlim = (0, df_train['Age'].max())) \\\n  .add_legend();","execution_count":14,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"31512703b0ce0a7fd4bfebd43612d7c280ceb836","collapsed":true},"cell_type":"code","source":"# how survival possibility depend on class \nsns.set(rc = {'figure.figsize':(11.7,8.27)})\nsns.barplot(x = 'Pclass', y = 'Survived', hue = 'Sex', data = df_train);","execution_count":15,"outputs":[]},{"metadata":{"scrolled":true,"trusted":false,"_uuid":"df68ab0da99a271cf55327d7028a08c851409402","collapsed":true},"cell_type":"code","source":"# dependency between Pclass and Sex - who had change of survived?\nsns.pointplot(x = 'Sex', y = 'Survived', hue = 'Pclass', data = df_train);","execution_count":16,"outputs":[]},{"metadata":{"_uuid":"901cbd888bcafe1e66adee0322599ab782701bd5"},"cell_type":"markdown","source":"# Basic model\n\nFirst I choose features from dataset. I search it with select_dtypes() method. I choose numerical variable \nbecause classification model works only on numerical variables. Frequently objects cointains categorial variable, but we can't use it in this form - we must change it to numerical. We could do this for example by use factorization function.\n\n```feats_from_dataset = df.select_dtypes([np.int, np.float]).columns.values```\n\n\nVariable 'Survived' is our target so we mustn't put it to models's features. 'PassengerId' is id of passenger \non titanic. It isn't necessary for us.\n\n```bad_feats = ['PassengerId', 'Survived']```\n\n```good_feats = [ feats for feats in feats_from_dataset if feats not in bad_feats ]```"},{"metadata":{"trusted":false,"_uuid":"9d0f08fb13cf48af5d73b66a7c1b655f7c989732","collapsed":true},"cell_type":"code","source":"df_train = df_train[pd.notnull(df_train['Embarked'])]\n\nfactorize(df_train, 'Sex', 'Embarked')\nfactorize(df_test, 'Sex', 'Embarked')\ndf_train.head(5)","execution_count":17,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c817feb8647c5bea6a1a575c276e519d8f73e7de","collapsed":true},"cell_type":"code","source":"# we have two new columns with factorized variables: 'Sex_cat', 'Embarked_cat'\ngood_feats(df_train)","execution_count":18,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"aa6728d9a29afacf9eec1c34bca1de84bfc269d1","collapsed":true},"cell_type":"code","source":"# basic model\nX = df_train[good_feats(df_train)].values\ny = df_train['Survived']\n\nmodel = DummyClassifier()\nscore = model_train_predict(model, X, y)\nprint(\"Score: %.2f\" % score)","execution_count":19,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"b8b64d758204023cac62ab4d009b964040b03f2d","collapsed":true},"cell_type":"code","source":"plt = plot_learning_curve(model, \"Learning Curves (Dummy Classifier)\", X, y, ylim=(0.5, 1.0), cv=10, n_jobs=4)\nplt.show()","execution_count":20,"outputs":[]},{"metadata":{"_uuid":"65df32a1ba010cf2dfad2dff446ac4ae4c2faeab"},"cell_type":"markdown","source":"Red line shows result for train dataset. This line must be better than green line, because we learn on train dataset.\nHowever, we want that green line also have hight score."},{"metadata":{"_uuid":"49dc0b2c9104b76996599bdcbf2bd6d321083bf6"},"cell_type":"markdown","source":"# Feature engineering"},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"550081aff599afbc59d058a3e57b1bc9bd4ba697"},"cell_type":"code","source":"title_feat(df_train, df_test)\nage_feat(df_train, df_test)\nfamily_feat(df_train, df_test)","execution_count":21,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"6e91267ca7b63e42b8e75bf5c7cafd307e54db4d"},"cell_type":"markdown","source":"# Advanced model"},{"metadata":{"trusted":false,"_uuid":"06855ae5825416ecc545f95f53599ea12cc3ee44","collapsed":true},"cell_type":"code","source":"X = df_train[good_feats(df_train)].values\ny = df_train['Survived']\n\nmodels = [\n    LogisticRegression(),\n    DecisionTreeClassifier(max_depth=10),\n    RandomForestClassifier(max_depth=10),\n    ExtraTreesClassifier(max_depth=20)\n]\n\nfor model in models:\n    print(str(model) + \": \")\n    %time score = model_train_predict(model, X, y)\n    print(str(score) + \"\\n\")\n    plt = plot_learning_curve(model, \"Learning Curves\", X, y, ylim=(0.5, 1.0), n_jobs=4)\n    plt.show()","execution_count":22,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ba972af89f6d2dcc5707adf9f45103c38782b20a","collapsed":true},"cell_type":"code","source":"for model in models:\n    print(str(model) + \": \")\n    %time cross_validate(model, X, y, scoring='accuracy', cv=3)\n    print(str(score) + \"\\n\")\n    plt = plot_learning_curve(model, \"Learning Curves\", X, y, ylim=(0.5, 1.0), cv=3, n_jobs=4)\n    plt.show()","execution_count":23,"outputs":[]},{"metadata":{"_uuid":"145d9705769ba4fe6775716023282451fcd093d0"},"cell_type":"markdown","source":"Hmmmm... Logistic Regresion is the best. The rest have overfitting."},{"metadata":{"trusted":false,"_uuid":"70485133a2373afca1317064d18f82622a810e80","collapsed":true},"cell_type":"code","source":"# Feature importances\n# graphs show feature importances\n\nmodels = [\n    DecisionTreeClassifier(max_depth=10),\n    RandomForestClassifier(max_depth=10),\n    ExtraTreesClassifier(max_depth=20)\n]\n\nfor model in models:\n    model.fit(X, y)\n    importances = model.feature_importances_\n    indices = np.argsort(importances)[::-1]\n    plt.figure(figsize=(10, 5))\n    plt.title('Feature importances: ' + str(model).split('(')[0])\n    plt.bar(range(X.shape[1]), model.feature_importances_[indices],\n           color = 'g', align = 'center')\n    plt.xticks(range(X.shape[1]), [ good_feats(df_train)[x] for x in indices])\n    plt.xticks(rotation=90)\n    plt.xlim([-1, X.shape[1]])\n    plt.show()","execution_count":24,"outputs":[]},{"metadata":{"_uuid":"dc83b78c0d727cff50c331b326042dc7b73620cb"},"cell_type":"markdown","source":"# xgboost"},{"metadata":{"trusted":false,"_uuid":"d4267fff7a910084aeb146186f64e9028943ad30","collapsed":true},"cell_type":"code","source":"model = xgb.XGBClassifier()\nmodel.fit(X, y)\n\ny_pred = model.predict(X)\nscore = accuracy_score(y, y_pred)\nprint(\"Score: %.2f\" % score)","execution_count":25,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"0f30c8ba4424fe8cc40abac36375d18be29c06fd","collapsed":true},"cell_type":"code","source":"X = df_train[good_feats(df_train)].values\ny = df_train['Survived']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n\ndef compute(params):\n    model = xgb.XGBClassifier(**params)\n    model.fit(X_train, y_train)\n\n    y_pred = model.predict(X_test)\n    score = accuracy_score(y_test, y_pred)\n    #print(\"Score: %.2f\" % score)\n    #print(params)\n    return (1 - score)\n\nspace = {\n        'max_depth':  hp.choice('max_depth', range(4,6)),\n        'min_child_weight': hp.uniform('min_child_weight', 0, 10),\n        'subsample': hp.quniform('subsample', 0.5, 1, 0.05),\n        'gamma': hp.quniform('gamma', 0.5, 1, 0.05)\n    }\n\nbest = fmin(compute, space, algo=tpe.suggest, max_evals=250)\nprint(best)","execution_count":26,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"34a8145fb9054ed0fd6dcc472b6e95da40c9ca1e","collapsed":true},"cell_type":"code","source":"X_train = df_train[good_feats(df_train)].values\ny_train = df_train['Survived']\nX_test = df_test[good_feats(df_test)].values\n\nmodel = xgb.XGBClassifier(**best)\nmodel.fit(X_train, y_train)","execution_count":27,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"cc12c21082acc632cefec45b9372b7ad9db4a6e1","collapsed":true},"cell_type":"code","source":"print(classification_report(y, \n                            y_pred, \n                            target_names=['Not Survived', 'Survived']))","execution_count":28,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}