{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Thu Nov 22 12:00:30 2018\n\n@author: bhgajula\n\"\"\"\n\n#importing the libraries \nimport pandas as pd\n\n#importing the dataset\ntraining_dataset = pd.read_csv('../input/train.csv')\ntest_dataset = pd.read_csv('../input/test.csv')\nsurvival = training_dataset.iloc[:,1].values\n\n# defining a function to find out to which deck the passenger belongs to from cabin \ndef Deck(x): \n    if str(x) != 'nan':\n        return str(x)[0] \n    else:\n        return \n  \n###\n#dropping the Survived from training set and appending the test set and removing the passengerid and ticket to make  finest dataset\ndataset = training_dataset.drop(columns=[\"Survived\"]).append(test_dataset).drop(columns=[\"PassengerId\", \"Ticket\"])\n\n#cleaning the data of our dataset\ndataset[\"hasParents\"] = dataset[\"Parch\"].apply(lambda x: (x>0)*1)  #making the Parch Column to having Parents and Children or not\ndataset[\"hasSiblings\"] = dataset[\"SibSp\"].apply(lambda x: (x>0)*1)  #making the Siblings Column to having Siblings and Spouse or not\ndataset[\"Deck\"] = dataset[\"Cabin\"].apply(Deck) #extracting the deck of the passenger by cabin\ndataset[\"Title\"] = dataset[\"Name\"].str.extract( ' ([A-Za-z]+\\.)', expand= False) #extrcting the title of the passenger\ndataset = dataset.drop(columns=[\"Parch\",\"SibSp\",\"Cabin\",\"Name\"], axis=1) #dropping the duplicates columns that are extracted\n\n#Encoding the categorical variable Sex to a binary variable by LabelEncoder\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndataset[\"Sex\"] = le.fit_transform(dataset[\"Sex\"])\n\n#Fillimg out the unavailable port of embarkation as Southhampton(S)\ndataset[\"Embarked\"].fillna('S' , inplace = True)\n\n#filling the unknown age values with the median values of age\ndataset[\"Age\"].fillna(dataset[\"Age\"].median(), inplace = True )\n\n#filling the unknown fare values with the median values of fare\ndataset[\"Fare\"].fillna(dataset[\"Fare\"].median(), inplace = True )\n\n#Dividing the age in to various sets for different age ranges\ndataset.loc[(dataset[\"Age\"]<=18), \"Age\"] = 0\ndataset.loc[(dataset[\"Age\"]>18) & (dataset[\"Age\"]<=30),\"Age\"] = 1\ndataset.loc[(dataset[\"Age\"]>30) & (dataset[\"Age\"]<=50),\"Age\"] = 2\ndataset.loc[(dataset[\"Age\"]>50) & (dataset[\"Age\"]<=65),\"Age\"] = 3\ndataset.loc[(dataset[\"Age\"]>65), \"Age\"]\n\n#Dividing the fare column in to various sets for fare ranges\ndataset.loc[(dataset[\"Fare\"]<=7.91), \"Fare\"]=0\ndataset.loc[(dataset[\"Fare\"]>7.91) & (dataset[\"Fare\"]<=14.454), \"Fare\"]=1\ndataset.loc[(dataset[\"Fare\"]>14.454) & (dataset[\"Fare\"]<=31), \"Fare\"]=2\ndataset.loc[(dataset[\"Fare\"])>31, \"Fare\"]=3\n\n#Converting the fare as int datatype\ndataset[\"Fare\"] = dataset[\"Fare\"].astype(int)\n\n#Converting the Pclass as string\ndataset[\"Pclass\"] = dataset[\"Pclass\"].astype(\"str\")\n    \n#OneHotEncoding the dataset\ndataset = pd.get_dummies(dataset)\n\n#Taking only 891 records of training set to build a model \nml_model=training_dataset.shape[0]\nX = dataset[:ml_model]\ny = survival\n\n#Dividing the dataset into training set and test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test ,y_train ,y_test = train_test_split(X, y, train_size=0.9, test_size=0.1)\n\n\n#Building the DecisionTreeClassifier model\nfrom sklearn.tree import DecisionTreeClassifier\nclassifier0 = DecisionTreeClassifier(criterion=\"entropy\", splitter=\"best\", random_state = 0)\nclassifier0.fit(X_train,y_train)\n#predicting the output\ny_pred0 = classifier0.predict(X_test)\n#knowing the confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm0=confusion_matrix(y_test,y_pred0)\n#accuracy of decision tree classifier\nacc_dtc = round(classifier0.score(X_train, y_train) * 100, 2)\nacc_dtc\n##91.39\n\n\n###\n# Fitting Logistic Regression to the Training set\nfrom sklearn.linear_model import LogisticRegression\nclassifier1 = LogisticRegression(solver='liblinear', random_state = 0)\nclassifier1.fit(X_train, y_train)\n#predicting the output\ny_pred1 = classifier1.predict(X_test)\n#knowing the confusion matrix\ncm1=confusion_matrix(y_test,y_pred1)\n#Accuracy of logistic regression\nacc_log = round(classifier1.score(X_train, y_train) * 100, 2)\nacc_log\n##82.65\n\n\n###\n# Fitting K-NN to the Training set\nfrom sklearn.neighbors import KNeighborsClassifier\nclassifier2 = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\nclassifier2.fit(X_train, y_train)\n#predicting the output\ny_pred2 = classifier2.predict(X_test)\n#knowing the confusion matrix\ncm2=confusion_matrix(y_test,y_pred0)\n#accuracy of k nearest neighbour\nacc_knn = round(classifier2.score(X_train, y_train) * 100, 2)\nacc_knn\n##86.27\n\n\n###\n# Fitting SVM to the Training set\nfrom sklearn.svm import SVC\nclassifier3 = SVC(kernel = 'linear',gamma='auto', random_state = 0)\nclassifier3.fit(X_train, y_train)\n#predicting the output\ny_pred3 = classifier3.predict(X_test)\n#knowing the confusion matrix\ncm3=confusion_matrix(y_test,y_pred3)\n#accuracy of support vector machine classifier\nacc_svc = round(classifier3.score(X_train, y_train) * 100, 2)\nacc_svc\n##79.53\n\n\n###\n# Fitting Kernel SVM to the Training set\nfrom sklearn.svm import SVC\nclassifier4 = SVC(kernel = 'rbf', gamma='auto', random_state = 0)\nclassifier4.fit(X_train, y_train)\n#predicting the output\ny_pred4 = classifier4.predict(X_test)\n#knowing the confusion matrix\ncm4=confusion_matrix(y_test,y_pred4)\n#acuracy of kernel svm\nacc_ksvm = round(classifier4.score(X_train, y_train) * 100, 2)\nacc_ksvm\n##79.28\n\n\n###\n# Fitting Naive Bayes to the Training set\nfrom sklearn.naive_bayes import GaussianNB\nclassifier5 = GaussianNB()\nclassifier5.fit(X_train, y_train)\n#predicting the output\ny_pred5 = classifier5.predict(X_test)\n#knowing the confusion matrix\ncm5=confusion_matrix(y_test,y_pred5)\n#accuracy of naive bayes\nacc_nb = round(classifier5.score(X_train, y_train) * 100, 2)\nacc_nb\n\n###\n# Fitting Random Forest Classification to the Training set\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier6 = RandomForestClassifier(n_estimators = 50, criterion = 'entropy', random_state = 0)\nclassifier6.fit(X_train, y_train)\n#predicting the output\ny_pred6 = classifier6.predict(X_test)\n#knowing the confusion matrix\ncm6=confusion_matrix(y_test,y_pred6)\n#accuracy of random forest classifier\nacc_rfc = round(classifier6.score(X_train, y_train) * 100, 2)\nacc_rfc\n##91.39\n\n\n###\n# Fitting XGBoost to the Training set\nfrom xgboost import XGBClassifier\nclassifier7 = XGBClassifier(booster='gbtree', silent=1, seed=0, base_score=0.5, subsample=0.75)\nclassifier7.fit(X_train, y_train)\n#predicting the output\ny_pred7 = classifier7.predict(X_test)\n#knowing the confusion matrix\ncm7=confusion_matrix(y_test,y_pred7)\n#accuracy of the xg boost\nacc_xgb = round(classifier7.score(X_train, y_train) * 100, 2)\nacc_xgb\n##85.02\n\n\n#Now after knowing the best model for predicting the accuracy of survival using it on train.csv\n#building the model for getting the results on the train.csv\nfrom xgboost import XGBClassifier\nCLASSIFIER = XGBClassifier(booster='gbtree', silent=1, seed=0, base_score=0.5, subsample=0.75)\nCLASSIFIER.fit(X,y)\n\nZ = dataset[ml_model:]\n\nZ_pred=CLASSIFIER.predict(Z)\n\ntitanic_submission = pd.DataFrame({\n        \"PassengerId\": test_dataset[\"PassengerId\"],\n        \"Survived\": Z_pred\n    })\n\ntitanic_submission.to_csv('titanic_submission.csv', index=False)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}