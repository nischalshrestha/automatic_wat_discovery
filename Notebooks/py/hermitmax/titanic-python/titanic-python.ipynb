{"nbformat": 4, "metadata": {"language_info": {"nbconvert_exporter": "python", "pygments_lexer": "ipython3", "name": "python", "version": "3.6.3", "codemirror_mode": {"version": 3, "name": "ipython"}, "mimetype": "text/x-python", "file_extension": ".py"}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}, "nbformat_minor": 1, "cells": [{"cell_type": "code", "source": ["# data analysis and wrangle\n", "import pandas as pd \n", "import numpy as np\n", "from collections import Counter\n", "from sklearn import preprocessing\n", "\n", "# visualisation \n", "import seaborn as sns\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline\n", "\n", "# machine learning\n", "from sklearn.cluster import KMeans\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.svm import SVC, LinearSVC\n", "from sklearn.ensemble import RandomForestClassifier\n", "from sklearn.neighbors import KNeighborsClassifier\n", "from sklearn.naive_bayes import GaussianNB\n", "from sklearn.linear_model import Perceptron\n", "from sklearn.linear_model import SGDClassifier\n", "from sklearn.tree import DecisionTreeClassifier\n", "from sklearn.pipeline import Pipeline\n", "from sklearn.model_selection import GridSearchCV"], "outputs": [], "metadata": {"_uuid": "ba528074be66f6a4cf89b056c1db8a107b064c43", "collapsed": true, "_cell_guid": "4c6a405f-e261-4723-8ee9-3fbd53958819"}, "execution_count": null}, {"cell_type": "code", "source": ["# step1: load data \n", "train_data = pd.read_csv(\"../input/train.csv\")\n", "test_data = pd.read_csv(\"../input/test.csv\")\n", "combine = [train_data,test_data]"], "outputs": [], "metadata": {"_uuid": "595bb44499cdd43042bad50e7f8c20adf7c12e2c", "_cell_guid": "b446cf6f-c815-44b5-bc30-743f9a8f887c"}, "execution_count": null}, {"cell_type": "markdown", "source": ["## Data Analysis"], "metadata": {"_uuid": "2593fbc83c6a04b89f7de14c05e0dadfbe1b8696", "_cell_guid": "37fca26a-7e16-487b-a5d5-39db648c966e"}}, {"cell_type": "code", "source": ["# divide data into categorical and numeric variables\n", "# -> so we know how to visualize them later\n", "train_data.head()\n", "#print (pd.unique(train_data.loc[:,\"Embarked\"]))\n", "#print (pd.unique(test_data.loc[:,\"Embarked\"]))\n", "# categorical variable\n", "#     nominal var: PassengeId[int], Name[char], Sex[male/female], Ticket[char+int], cabin[NaN]\n", "#     ordinal var: Pclass[int], Embarked[char]\n", "# numerical variable\n", "#     discrete var: Age, SibSp, Parch\n", "#     continuous var: Fare\n", "# Label\n", "#     survived: 0 -- not survived, 1 -- survived"], "outputs": [], "metadata": {"_uuid": "364cfacaf9c5c1b095eb3e5ad4818ba72d5d042c", "_cell_guid": "17325488-013f-4a91-92b1-f04db3eab628", "_kg_hide-output": true}, "execution_count": null}, {"cell_type": "code", "source": ["# any null, outlier or abnormal feature or pattern or representative\n", "# -> the reason why we need to find them is because\n", "# -> ML models do not like missing values and will be confused with outlier\n", "#    and also if features are representative, it might contribute a lot for our model\n", "print (train_data.info())\n", "print (\"-\"*40)\n", "print (test_data.info())\n", "# train - cabin(a lot of null values -> think if drop them.)\n", "#       - Embarked (2 null values -> replace with similar values)\n", "#       - Age (200+ null values --> replace with similar values)\n", "# test - cabin (a lot of null values -> think if drop them.)\n", "#      - Fare (1 null values -> replace with median or mean)"], "outputs": [], "metadata": {"_uuid": "95d51fc4fb0d3ea357c1b8457845564a2ce27466", "_cell_guid": "dce83785-66ea-49f5-ad11-6e50b485bc69"}, "execution_count": null}, {"cell_type": "code", "source": ["# check representativeness of features\n", "# my purpose: if the feature is representative (-> feature is useful)\n", "#             e.g. like Pclass, if Pclass = 1 has high survival (> average), and others have lower rates, \n", "#             then this feature may have correlations with survival rates --> so keep it. \n", "train_data.head()\n", "train_data.describe(percentiles=[.1, .2, .3, .4, .5, .6, .7, .8, .9, 1.])\n", "## PassengerId --> unique ID from 1 to 891 --> not representative --> drop them\n", "# Survived --> 38% survival rate \n", "# Pclass --> most of people (50%+) from Pclass 3 --> survial rate for different Pclass\n", "# Age --> has missing values & 80% people below 41 years old --> survival rate for different ages\n", "# SibSp --> most of people (60%+) does not have brothers & sisters together -> look correlations\n", "# Parch --> more than 70% does have parents together -> look correlations\n", "# Fare --> most of people (80%) pay below 39 but there are weired values like 512(?) & 0(staff).\n", "#      --> see correlations first & may build new ratio variables based on it\n", "#      --> too many unique Fares --> need to build ratio variables"], "outputs": [], "metadata": {"_uuid": "5384a04b547973a38826d8f6301282d5c492f850", "_cell_guid": "7c425381-f063-4ba0-b4e3-9fbb5c8a3379"}, "execution_count": null}, {"cell_type": "code", "source": ["train_data.describe(include =[\"O\"])\n", "# Name -> unique -> it is useless now --> may group similar name together\n", "# Sex -> most of people (577/891) are male --> see correlations\n", "# Ticket -> some ticket number are same --> why?\n", "## Cabin -> some Cabin are same -> some people share one room\n", "#        -> not only has too many missing value and also it might have no correlations with labels\n", "# Embarked -> people from three ports & 644/889 from S"], "outputs": [], "metadata": {"_uuid": "ffe799e8818a5dced49f56956becfbc457d6126a", "_cell_guid": "0fce2716-30d1-4344-9066-3cb2b93c9b60"}, "execution_count": null}, {"cell_type": "code", "source": ["# Assumption\n", "# group one: high level customers --> [Pclass, Fare] --> high survival rate\n", "# group two: have relatives on boat --> [SibSp, Parch] --> high survival rate\n", "# group three: lady first --> Sex --> lady has high survival rate\n", "# group four: child, elder first --> Age --> have high survival rate\n", "# group five: from different ports --> Embarked --> surival rate varying"], "outputs": [], "metadata": {"_uuid": "82030b3d6b16397bb5a6fa9c6d8016a7e7806584", "collapsed": true, "_cell_guid": "795a2ece-3fc8-474e-86f2-170e92b3cddf"}, "execution_count": null}, {"cell_type": "code", "source": ["# group one - true\n", "train_data[[\"Pclass\", \"Survived\"]].groupby(\"Pclass\", as_index = False).mean().sort_values(by = \"Survived\", ascending = False)\n", "#train_data[[\"Fare\", \"Survived\"]].groupby(\"Fare\", as_index = False).mean().sort_values(by = \"Survived\", ascending = False)\n", "# try later about Fare - maybe make a range variable"], "outputs": [], "metadata": {"_uuid": "1b2dd0d93e7a17317b3cb3bd518dde8aa4993a37", "_cell_guid": "9bc39b73-46dd-4e6c-9e00-a812040e3550"}, "execution_count": null}, {"cell_type": "code", "source": ["#g = sns.FacetGrid(train_data)\n", "#g.map(sns.pointplot, \"Pclass\", \"Survived\", palette='deep')"], "outputs": [], "metadata": {"_uuid": "f8da1884a69f95e83ae69bba82165608c3c8efcc", "collapsed": true, "_cell_guid": "336c11a7-627f-4bce-94ac-4d04a2e8507c", "_kg_hide-output": false, "_kg_hide-input": false}, "execution_count": null}, {"cell_type": "code", "source": ["# group two - true\n", "print (train_data[[\"SibSp\", \"Survived\"]].groupby(\"SibSp\", as_index = False).mean().sort_values(by = \"Survived\", ascending = False))\n", "print (\"-\"*40)\n", "print (train_data[[\"Parch\", \"Survived\"]].groupby(\"Parch\", as_index = False).mean().sort_values(by = \"Survived\", ascending = False))\n", "# it seems like when you have too high or no SibSp (>=3 or =0) -> below average survival rate\n", "# it seems like when you have too high or no Parch (>=4 or =0) -> below average survival rate\n", "# if you have too many family memeber, you dont have enough power to help every one and they too\n", "# and also, if you do have family memeber, nobody help you\n", "# also, maybe we can combine those two variables later"], "outputs": [], "metadata": {"scrolled": true, "_uuid": "8f26b85350c935a7febda2c91969ea91c739aeaa", "_cell_guid": "17a20953-0c9f-4e35-ae4a-6c6f1fef3c5b"}, "execution_count": null}, {"cell_type": "code", "source": ["# group three - true\n", "train_data[[\"Sex\", \"Survived\"]].groupby(\"Sex\", as_index = False).mean().sort_values(by = \"Survived\", ascending = False)"], "outputs": [], "metadata": {"_uuid": "99fdfe5a904cedd54efdd565ca245060a9c900cb", "_cell_guid": "77e3e971-9195-4730-a908-d6eec7162630"}, "execution_count": null}, {"cell_type": "code", "source": ["# group four - later -> need to re-design variable\n", "g = sns.FacetGrid(train_data, col = \"Survived\")\n", "g.map(plt.hist, \"Age\")\n", "# not obvious -> create Age range\n", "train_data[\"AgeRange\"] = pd.cut(train_data[\"Age\"], 5)\n", "train_data[[\"AgeRange\", \"Survived\"]].groupby(\"AgeRange\", as_index = False).mean().sort_values(by = \"Survived\", ascending = False)"], "outputs": [], "metadata": {"_uuid": "89fd89e49c539e98ebae75b3d1f576439b1bd9a2", "_cell_guid": "b747ee99-58e1-4ef2-b0be-735d095de934"}, "execution_count": null}, {"cell_type": "code", "source": ["# group five - true assumption\n", "train_data[[\"Embarked\", \"Survived\"]].groupby(\"Embarked\", as_index = False).mean().sort_values(by = \"Survived\", ascending = False)\n", "# people from C port have high survial rate"], "outputs": [], "metadata": {"_uuid": "88d8edd429cefdd948544aa67a2fc15fd60487a6", "_cell_guid": "66fbb058-2f96-47af-bdef-b20823299ff8"}, "execution_count": null}, {"cell_type": "markdown", "source": ["## Wrangle Data"], "metadata": {"_uuid": "b0827adc60a81fefec31c49d764d76b76e22fd19", "_cell_guid": "621a7f37-2fa3-429a-b3b9-12fa1557e825"}}, {"cell_type": "code", "source": ["train_data = pd.read_csv(\"../input/train.csv\")\n", "train_label = train_data[\"Survived\"]\n", "train_data = train_data.drop(\"Survived\", axis = 1)\n", "test_data = pd.read_csv(\"../input/test.csv\")\n", "combine = [train_data,test_data]\n", "train_data.head()"], "outputs": [], "metadata": {"_uuid": "33866eda92386766c57c717594454826544bcf5e", "_cell_guid": "2676f9af-e411-41d5-b500-44fed282bd62"}, "execution_count": null}, {"cell_type": "code", "source": ["# complete incomplete features\n", "# train \n", "#       - Embarked (2 null values -> replace with similar values)\n", "#       - Age (200+ null values --> replace with similar values)\n", "# test \n", "#      - Fare (1 null values -> replace with median or mean)\n", "\n", "# correct - if outlier drop them / remove abnormal data\n", "#         - PassengerId[int] \u2014\u2014\u2014\u2014\u2014 drop \n", "#         - Name[char] \u2014\u2014\u2014\u2014\u2014 drop\n", "#         - cabin[NaN] \u2014\u2014\u2014\u2014\u2014 drop\n", "\n", "# convert categorical variables into numeric variables/dummy variables/one-hot\n", "#         - Sex[male/female] \u2014\u2014\u2014\u2014\u2014 dummy \n", "#         - Ticket[char+int] \u2014\u2014\u2014\u2014\u2014 dummy \n", "#         - Pclass[int] \u2014\u2014\u2014\u2014\u2014 dummy \n", "#         - Embarked[char] \u2014\u2014\u2014\u2014\u2014 dummy \n", "\n", "# create  - build new variables based on existing varialbes"], "outputs": [], "metadata": {"_uuid": "e9746b5d62f81d6dc907c523422c98cd2d5fb18a", "collapsed": true, "_cell_guid": "e68e72eb-b4c6-44c6-99dc-7b6faa4036d5"}, "execution_count": null}, {"cell_type": "code", "source": ["# correct features\n", "print (\"Before\", train_data.shape, test_data.shape, combine[0].shape, combine[1].shape)\n", "\n", "train_df = train_data.drop([\"PassengerId\", \"Name\", \"Cabin\", \"Ticket\"], axis = 1)\n", "test_df = test_data.drop([\"PassengerId\", \"Name\", \"Cabin\", \"Ticket\"], axis = 1)\n", "del train_data \n", "del test_data\n", "combine = [train_df, test_df]\n", "\n", "print (\"After\", train_df.shape, test_df.shape, combine[0].shape, combine[1].shape)"], "outputs": [], "metadata": {"_uuid": "65c2e7901c75e0afe887b9cbe4f1678dab340758", "_cell_guid": "bd302420-c128-4f74-967a-5093aecc2775"}, "execution_count": null}, {"cell_type": "code", "source": ["# convert features\n", "categories_to_dummies = [\"Sex\", \"Pclass\", \"Embarked\"]\n", "for i, df in enumerate(combine):\n", "    for j in categories_to_dummies:\n", "        # train\n", "        # categories to dummies \n", "        tmp = pd.get_dummies(df[j], prefix=j)\n", "        df = df.join(tmp)\n", "        df = df.drop(j, axis = 1)\n", "        combine[i] = df\n", "del tmp\n", "del df\n", "print (\"After\", combine[0].shape, combine[1].shape)\n", "combine[0].head()"], "outputs": [], "metadata": {"_uuid": "d39d8ac0d58396db107ad80c31931e7139111c6b", "_cell_guid": "a23b4962-cbed-4c00-b0d2-ec0d298293dd"}, "execution_count": null}, {"cell_type": "code", "source": ["# handle missing value\n", "# Fare - test \n", "# better solution\n", "# test_df['Fare'].fillna(test_df['Fare'].dropna().median(), inplace=True)\n", "tmp_test = combine[1]\n", "tmp_test.loc[tmp_test[\"Fare\"].isnull(), \"Fare\"] = tmp_test[\"Fare\"].median()\n", "pd.isnull(tmp_test[\"Fare\"]).sum() > 0"], "outputs": [], "metadata": {"_uuid": "9e8b82403bcf0cba063b2ae83d2cae4a61a16c17", "collapsed": true, "_cell_guid": "930ee7e3-a806-4557-b3bd-15c6f30fbb4f"}, "execution_count": null}, {"cell_type": "code", "source": ["train_df[\"AgeRange\"] = pd.cut(train_df[\"Age\"], 5)"], "outputs": [], "metadata": {"_uuid": "8b13d351a6ab0b72c1eb2c0cf44400d9163ee37a", "collapsed": true, "_cell_guid": "f738fc7a-94ef-40a1-b5e3-492048e0a7b8"}, "execution_count": null}, {"cell_type": "code", "source": ["for i, df in enumerate(combine):\n", "    df.loc[df[\"Age\"] <= 16.336, \"Age\"] = 0\n", "    df.loc[(df[\"Age\"] > 16.336) & (df[\"Age\"] <=32.252), \"Age\"] = 1\n", "    df.loc[(df[\"Age\"] > 32.252) & (df[\"Age\"] <=48.168), \"Age\"] = 2\n", "    df.loc[(df[\"Age\"] > 48.168) & (df[\"Age\"] <=64.084), \"Age\"] = 3\n", "    df.loc[(df[\"Age\"] > 64.084) & (df[\"Age\"] <=80.0), \"Age\"] = 4\n", "    combine[i] = df\n", "del df\n", "print (combine[0][\"Age\"])\n", "print (combine[1][\"Age\"])"], "outputs": [], "metadata": {"_uuid": "409d4ab33196c6e1f84600d5a62ab143f6075e17", "collapsed": true, "_cell_guid": "4dca8a3b-9ba1-4fb3-a3a0-8dc082390fe7"}, "execution_count": null}, {"cell_type": "code", "source": ["# clustering\n", "# build temporary train & test sets\n", "tmp_train = combine[0].copy()\n", "tmp_test = combine[1].copy()\n", "tmp_train = tmp_train.drop(\"Age\", axis = 1)\n", "tmp_test = tmp_test.drop(\"Age\", axis = 1)\n", "# kmeans clustering for train & test sets\n", "# & assign \"kmeans_labels\" back to \"combine sets\"\n", "kmeans = KMeans(n_clusters=5, random_state=0).fit(tmp_train)\n", "combine[0][\"kmeans_labels\"] = pd.Series(kmeans.labels_)\n", "combine[1][\"kmeans_labels\"] = pd.Series(kmeans.predict(tmp_test))\n", "# verify success or not\n", "combine[1].head()"], "outputs": [], "metadata": {"_uuid": "69e2b9cd63bf3be079bc3ede75a8dd8d5992f64a", "collapsed": true, "_cell_guid": "f2853bfe-25b0-4ea2-aaca-626abee08ebe"}, "execution_count": null}, {"cell_type": "code", "source": ["# replace age's missing value \n", "tmp_train = combine[0].copy()\n", "tmp_test = combine[1].copy()\n", "tmp_age_train = []\n", "tmp_age_test = []\n", "for i in range(5):\n", "    tmp_age_train.append(Counter(tmp_train.loc[tmp_train[\"kmeans_labels\"] == i, \"Age\"]).most_common()[0][0])\n", "    tmp_age_test.append(Counter(tmp_test.loc[tmp_test[\"kmeans_labels\"] == i, \"Age\"]).most_common()[0][0])\n", "for j in range(5):\n", "    tmp_train.loc[(tmp_train[\"kmeans_labels\"] == j) & (tmp_train[\"Age\"].isnull()), \"Age\"] = tmp_age_train[j]\n", "    tmp_test.loc[(tmp_test[\"kmeans_labels\"] == j) & (tmp_test[\"Age\"].isnull()), \"Age\"] = tmp_age_test[j]\n", "combine[0][\"Age\"] = tmp_train[\"Age\"]\n", "combine[1][\"Age\"] = tmp_test[\"Age\"]\n", "print (pd.isnull(combine[0][\"Age\"]).sum() >0, pd.isnull(combine[1][\"Age\"]).sum() >0)"], "outputs": [], "metadata": {"_uuid": "bbb2451e101aa1eeb10add717e1f495c1ff0778b", "collapsed": true, "_cell_guid": "246098e6-9d94-47d8-889e-6d1d7c3239f2"}, "execution_count": null}, {"cell_type": "code", "source": ["# modeling\n", "# drop, kmeans, since it is not original features\n", "X_train = combine[0].drop(\"kmeans_labels\", axis = 1)\n", "Y_train = train_label\n", "X_test = combine[1].drop(\"kmeans_labels\", axis = 1)\n", "print (\"After\", X_train.shape, Y_train.shape, X_test.shape)\n", "X_train.head()"], "outputs": [], "metadata": {"_uuid": "36a75ecf5439618292459210698cff16daafb6ed", "collapsed": true, "_cell_guid": "9ed4868d-d0ab-42bd-a71e-20a184460de5"}, "execution_count": null}, {"cell_type": "code", "source": ["# split original train dataset into train & dev sets\n", "from sklearn.model_selection import train_test_split\n", "X_train_t, X_train_dev, y_train_t, y_train_dev = train_test_split(\n", "     X_train, Y_train, test_size=0.3, random_state=2)\n", "print (\"train_train\", X_train_t.shape, y_train_t.shape)\n", "print (\"train_dev\", X_train_dev.shape, y_train_dev.shape)\n", "#X_train.iloc[8, :]\n", "#Y_train[8]\n", "#y_train_t[9]\n", "#X_train.iloc[9, :]\n", "#Y_train.iloc[9]"], "outputs": [], "metadata": {"_uuid": "e1767852cc4e23863643693d5195af712299e9e4", "collapsed": true, "_cell_guid": "570fdb4f-ce4a-4aa0-a71b-8a4cca5184a2"}, "execution_count": null}, {"cell_type": "code", "source": ["# group 1: perceptual, logistic regression, SVM, neural network, kNN"], "outputs": [], "metadata": {"_uuid": "b3f7a6cd61656593a7909d6bee81467bc4f35727", "collapsed": true, "_cell_guid": "d09ce555-153a-449f-9ab2-c4041869a457"}, "execution_count": null}, {"cell_type": "code", "source": ["X_train_t.head()"], "outputs": [], "metadata": {"_uuid": "879658e6d3c1765e8814cef8f6b422c51888e3bc", "collapsed": true, "_cell_guid": "d7589a21-df0c-4397-8bdf-3e22dd60113b"}, "execution_count": null}, {"cell_type": "code", "source": ["from sklearn import preprocessing\n", "# build \n", "min_max_scaler = preprocessing.MinMaxScaler()\n", "# scale train data\n", "copy_that_t = X_train_t.copy()\n", "scaled_X_t = min_max_scaler.fit_transform(X_train_t[[\"Age\", \"SibSp\", \"Parch\", \"Fare\"]])\n", "copy_that_t[[\"Age\", \"SibSp\", \"Parch\", \"Fare\"]] = pd.DataFrame(scaled_X_t, columns=[\"Age\", \"SibSp\", \"Parch\", \"Fare\"], index = copy_that_t.index)\n", "X_train_t = copy_that_t\n", "# scale train dev data\n", "copy_that_dev = X_train_dev.copy()\n", "scaled_X_dev = min_max_scaler.transform(copy_that_dev[[\"Age\", \"SibSp\", \"Parch\", \"Fare\"]])\n", "copy_that_dev[[\"Age\", \"SibSp\", \"Parch\", \"Fare\"]] = pd.DataFrame(copy_that_dev, columns=[\"Age\", \"SibSp\", \"Parch\", \"Fare\"], index = copy_that_dev.index)\n", "X_train_dev = copy_that_dev\n", "#scaled_X_dev = min_max_scaler.transform(X_train_dev[[\"Age\", \"SibSp\", \"Parch\", \"Fare\"]])\n", "#X_train_dev[[\"Age\", \"SibSp\", \"Parch\", \"Fare\"]] = pd.DataFrame(scaled_X_dev, columns=[\"Age\", \"SibSp\", \"Parch\", \"Fare\"], index = X_train_dev.index)\n", "X_train_t.head()\n", "X_train_dev.head()"], "outputs": [], "metadata": {"_uuid": "67cc1c9b9f1f21bdbfdc7838812f419b9ad07e22", "collapsed": true, "_cell_guid": "3b483650-41f1-4be4-89e6-29cbb24deb79"}, "execution_count": null}, {"cell_type": "code", "source": ["# logistic regression\n", "# tune parameters\n", "# keep cv, dual, penalty, solver, refit, random_state same\n", "# --> then tune Cs to reach the highest train_cv_acc with very large max_iter\n", "# --> then tune max_iter to reach the highest train_cv_acc with optimal Cs\n", "from sklearn.linear_model import LogisticRegressionCV\n", "\n", "\n", "# build pipe\n", "# use pipe, you can only use two operations: fit and transform. \n", "# --> it is not suitable for my sitation, i need to use fit_transforms for first four columns\n", "# --> then to do the fit for all the data\n", "logCV = LogisticRegressionCV(Cs=100, cv=5, dual=False, penalty='l2', solver='lbfgs', max_iter=50,\n", "                            refit=True, random_state=1)\n", "# set parameters\n", "parameters = dict(\n", "Cs = [100, 120, 130], \n", "cv = [5], \n", "max_iter = [10, 15]\n", ")\n", "\n", "# estimators\n", "estimator = GridSearchCV(logCV,parameters)\n", "estimator.fit(X_train_t, y_train_t)\n", "#logistic.predict(X_test)\n", "print (\"best estimator\", estimator.best_estimator_, estimator.best_params_)\n", "#print (\"best coef\", pipe.named_steps[\"logCVCV\"].coef_)\n", "#print (\"train_cv_acc\", pipe.named_steps[\"logCVCV\"].scores_)\n", "print (\"train_cv_acc\", estimator.score(X_train_t, y_train_t)*100)\n", "print (\"dev_acc\", estimator.score(X_train_dev, y_train_dev)*100)\n", "# TO DO\n", "# best estimator from GridSearchCV\n", "# --> LogisticRegressionCV(best_parameters)\n", "# --> logCV.fit(X_train_t, y_trian_t)\n", "#names = ['Age', 'SibSp', 'Parch', 'Fare', 'Sex_female', 'Sex_male', 'Pclass_1',\n", "#       'Pclass_2', 'Pclass_3', 'Embarked_C', 'Embarked_Q', 'Embarked_S']\n", "#plot_coef_df = pd.DataFrame({\"names\":np.array(names), \n", "#                             \"logCV_coef\":np.array(logCV.coef_.reshape(12,1))\n", "#                            })\n", "# pipe\n", "#pipe = Pipeline(steps=[(\"logCVCV\", logCV)])\n", "#pipe.set_params(logCVCV__Cs = [4, 5, 6, 7, 8, 9,10], logCVCV__cv = 5, \n", "                #logCVCV__max_iter = 30, logCVCV__random_state=1).fit(X_train_t, y_train_t)\n", "#logCV.fit(X_train_t, y_train_t)"], "outputs": [], "metadata": {"_uuid": "1108c0260468dae018b4da06023bdf4b1edcc1dd", "collapsed": true, "_cell_guid": "ef30a6f7-bb88-46b1-b97a-3657191d9c41"}, "execution_count": null}, {"cell_type": "code", "source": ["# SVM\n", "from sklearn.svm import SVC\n", "svm = SVC(C=1.0, kernel='rbf', degree=5, gamma='auto', coef0=0.0, \n", "           shrinking=True, probability=False, tol=0.001, \n", "           cache_size=200, class_weight=None, verbose=False, max_iter=-1, \n", "           decision_function_shape='ovr', random_state=2)\n", "\n", "# set parameters\n", "parameters_svm = dict(\n", "C = [1.0, 1.5, 2.0, 2.5]\n", ")\n", "\n", "# build estimators\n", "estimator_svm = GridSearchCV(svm,parameters_svm, cv = 5)\n", "estimator_svm.fit(X_train_t, y_train_t)\n", "print (\"best estimator_svm\", estimator_svm.best_estimator_, estimator_svm.best_params_)\n", "print (\"train_cv_acc\", estimator_svm.score(X_train_t, y_train_t)*100)\n", "print (\"dev_acc\", estimator_svm.score(X_train_dev, y_train_dev)*100)"], "outputs": [], "metadata": {"_uuid": "795f99cfb1fca0b4e3cb9ebf938edb17f5672d5c", "collapsed": true, "_cell_guid": "0756e0bd-48f7-4b75-bfbf-ddca27a164dc"}, "execution_count": null}, {"cell_type": "code", "source": ["# NN\n", "from sklearn.neural_network import MLPClassifier\n", "NN = MLPClassifier(hidden_layer_sizes=(100, ), activation='relu', solver='lbfgs', \n", "                   alpha=0.1, batch_size='auto', learning_rate='constant', \n", "                   learning_rate_init=0.001, power_t=0.5, \n", "                   max_iter=500, shuffle=True, random_state=3, tol=0.0001, \n", "                   verbose=False, warm_start=False, momentum=0.9, \n", "                   nesterovs_momentum=True, early_stopping=True, validation_fraction=0.1, \n", "                   beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n", "# set parameters\n", "parameters_NN = {\n", "    'hidden_layer_sizes':[(10, )],\n", "    'activation':['relu'],\n", "    'alpha':[0.5, 0.4, 1.0, 1.2, 1.5],\n", "    'solver':['lbfgs']\n", "}\n", "# build estimators\n", "estimator_NN = GridSearchCV(NN,parameters_NN, cv = 5)\n", "estimator_NN.fit(X_train_t, y_train_t)\n", "print (\"best estimator_svm\", estimator_NN.best_estimator_, estimator_NN.best_params_)\n", "print (\"train_cv_acc\", estimator_NN.score(X_train_t, y_train_t)*100)\n", "print (\"dev_acc\", estimator_NN.score(X_train_dev, y_train_dev)*100)"], "outputs": [], "metadata": {"_uuid": "61b5bf74bf6449db850d6cd1010619aa11b34cbe", "collapsed": true, "_cell_guid": "de7a79eb-f421-49e3-9b5f-817a72409690"}, "execution_count": null}, {"cell_type": "code", "source": ["from sklearn.model_selection import StratifiedKFold\n", "skf = StratifiedKFold(n_splits=10, shuffle=False, random_state=1)\n", "KFoldSplit = skf.split(X_train_t, y_train_t)"], "outputs": [], "metadata": {"_uuid": "51dada13cdcb502692bde46aec129d5a3a114b0f", "collapsed": true, "_cell_guid": "12dd0bea-0960-4285-8353-f1eb6bbb54b7"}, "execution_count": null}, {"cell_type": "code", "source": ["print (X_train_t.shape, y_train_t.shape)\n", "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\n", "for i, j in skf.split(X_train_t, y_train_t):\n", "    #print (i)\n", "    #print (j)\n", "    #print (np.unique(y_train_t.iloc[i]))\n", "    break\n", "#X_train_dev.iloc[test_index,:]"], "outputs": [], "metadata": {"_uuid": "ad73b337aef902d28f3712affe128048ed048a35", "collapsed": true, "_cell_guid": "34285b9e-ffcc-4268-8ae4-c408a56351a6"}, "execution_count": null}, {"cell_type": "code", "source": ["# kNN\n", "from sklearn.neighbors import KNeighborsClassifier\n", "\n", "scores = []\n", "tmpScore = 0\n", "for j in range(1,20, 1):\n", "    for train_index, test_index in skf.split(X_train_t, y_train_t):\n", "        kNN = KNeighborsClassifier(n_neighbors=j, weights='uniform', \n", "                           algorithm='auto', leaf_size=30, p=1, \n", "                           metric='minkowski', metric_params=None, \n", "                           n_jobs=1)\n", "        X = X_train_t[train_index,:]\n", "        y = y_train_t[train_index]\n", "        kNN.fit(X, y)\n", "        tmpScore += (kNN.score(X_train_dev[test_index,:], y_train_dev[test_index])/10.0)\n", "    scores.append(tmpScore)\n", "    tmpScore = 0\n", "x = range(1, 20, 1)\n", "plt.plot(x, scores)\n", "plt.show()\n", "#parameters_kNN = {\n", "#    'n_neighbors':[3, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16], \n", "#    'weights': ['distance', 'uniform'], \n", "#    'algorithm':['auto'], \n", "#    'leaf_size':[30],\n", "#    'p': [1, 2, 3],\n", "#}\n", "# build estimators\n", "#estimator_kNN = GridSearchCV(kNN,parameters_kNN, cv = 5)\n", "#estimator_kNN.fit(X_train_t, y_train_t)\n", "#print (\"best estimator_svm\", estimator_kNN.best_estimator_, estimator_kNN.best_params_)\n", "#print (\"train_cv_acc\", estimator_kNN.score(X_train_t, y_train_t)*100)\n", "#print (\"dev_acc\", estimator_kNN.score(X_train_dev, y_train_dev)*100)\n", "#????"], "outputs": [], "metadata": {"_uuid": "5db9ab3397e2730488013b1ada5c964285114464", "collapsed": true, "_cell_guid": "78ce025a-e5a5-4db4-8c3e-eb3ca167b8c7"}, "execution_count": null}, {"cell_type": "code", "source": ["# perceptual \n", "from sklearn.linear_model import Perceptron\n", "per = Perceptron(penalty='l1', alpha=0.0001, fit_intercept=True, \n", "                 max_iter=500, tol=0.00001, shuffle=True, verbose=0, \n", "                 eta0=1.0, n_jobs=1, random_state=5, class_weight=None, \n", "                 warm_start=False, n_iter=None\n", ")\n", "# set parameters\n", "parameters_per = {\n", "    'penalty':['l2', 'l1'], \n", "    'alpha':[0.0005, 0.001, 0.01, 0.5],\n", "}\n", "# build estimators\n", "estimator_per = GridSearchCV(per,parameters_per, cv = 10)\n", "estimator_per.fit(X_train_t, y_train_t)\n", "print (\"best estimator_svm\", estimator_per.best_estimator_, estimator_per.best_params_)\n", "print (\"train_cv_acc\", estimator_per.score(X_train_t, y_train_t)*100)\n", "print (\"dev_acc\", estimator_per.score(X_train_dev, y_train_dev)*100)"], "outputs": [], "metadata": {"_uuid": "ec4376739b816f55d99ed9c48e91aa4369875536", "collapsed": true, "_cell_guid": "d92163f0-cae9-4856-aff7-9ca08e301cb4"}, "execution_count": null}, {"cell_type": "markdown", "source": ["## ensemble models"], "metadata": {"_uuid": "fe480b8f87966192d66d5f8d638af4a908b63ef0", "_cell_guid": "859b2c92-e93b-4aee-9237-2ac607627b22"}}, {"cell_type": "code", "source": ["# randome forest \n", "from sklearn.ensemble import RandomForestClassifier\n", "rf = RandomForestClassifier(n_estimators=10, criterion='gini', \n", "                            max_depth=None, min_samples_split=2, \n", "                            min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n", "                            max_features='auto', max_leaf_nodes=None, \n", "                            min_impurity_decrease=0.0, min_impurity_split=None, \n", "                            bootstrap=True, oob_score=False, n_jobs=1, \n", "                            random_state=6, verbose=0, warm_start=False, \n", "                            class_weight=None)\n", "# set parameters\n", "params_rf = {\n", "    'n_estimators':[10,20, 25,30],\n", "    'max_depth':[None, 3, 4],\n", "    'min_samples_split':[2],\n", "    'min_samples_leaf':[1],\n", "    'max_features':[5, 10],\n", "    'max_leaf_nodes':[None]\n", "}\n", "# build estimators\n", "estimator_rf = GridSearchCV(rf,params_rf, cv = 5)\n", "estimator_rf.fit(X_train_t, y_train_t)\n", "print (\"best estimator_rf\", estimator_rf.best_estimator_, estimator_rf.best_params_)\n", "print (\"train_cv_acc\", estimator_rf.score(X_train_t, y_train_t)*100)\n", "print (\"dev_acc\", estimator_rf.score(X_train_dev, y_train_dev)*100)"], "outputs": [], "metadata": {"_uuid": "36a11ad4e8080259d16226b1bcbe243bb8110c65", "collapsed": true, "_cell_guid": "862046ad-6c1f-40c9-91fc-9411ab9f5774"}, "execution_count": null}]}