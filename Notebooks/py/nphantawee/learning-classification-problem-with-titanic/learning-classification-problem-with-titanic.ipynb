{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true},"cell_type":"markdown","source":"# About me and this notebook\n_About me_ : I am a person who has no degree in math or computer science. I work in data management area and learn Python by myself in last 2 years, but very small chance to use it in my work. I preferred to use Excel + VBA to complete all data related taks. I hope that keep practicing would make me become a data scientist someday.\n\n_About notebook_ : This notebook is to conclude what I've learned from various source, I am so new to this area please feel free to comment & suggest. I will learn from you guys.\n\nThanks,<br>\n**Nopp**\n\n-----\n**Credit/Reference**:\n- https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python\n- https://www.kaggle.com/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy\n- https://www.kaggle.com/ash316/eda-to-prediction-dietanic\n- https://www.kaggle.com/yassineghouzam/titanic-top-4-with-ensemble-modeling\n- https://www.kaggle.com/sshadylov/titanic-solution-using-random-forest-classifier"},{"metadata":{"_uuid":"0ba3c40bb1d6a4e5c64d5fa9445ba32f126ee84f"},"cell_type":"markdown","source":"# 0. Import some required dependencies"},{"metadata":{"_uuid":"24b614cf65dad6913ec27114274d12236112b542","trusted":false},"cell_type":"code","source":"import sys\nprint('Python version: {}'.format(sys.version))\n\nimport pandas as pd\nprint('pandas version: {}'.format(pd.__version__))\n\nimport numpy as np\n\nprint('numpy version: {}'.format(np.__version__))\n\nimport matplotlib as mlp\nimport matplotlib.pyplot as plt\n%matplotlib inline\nprint('matplotlib version: {}'.format(mlp.__version__))\n\nimport seaborn as sns\nprint('seaborn version: {}'.format(sns.__version__))\n\nimport os\nprint('\\nFile list:',os.listdir('../input'))\n\nimport time\nstart_time = time.time()\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e3175fb527c8c4f49313a5744491850a064930ed"},"cell_type":"markdown","source":"# 1. What is this dataset all about\nThe Titanic was the largest ship afloat at the time of her maiden voyage and carried 2,224 people on that maiden voyage from Southampton on 10th April 1912. Her destination was New York City, America but her last port of call was at Cobh (Queenstown), Ireland on 11th April 1912.\n\nThis dataset contain all passengers record with their surviability."},{"metadata":{"_uuid":"f3d4e30a3cada8cb53948154e979d22f9bfe97db","scrolled":true,"trusted":false},"cell_type":"code","source":"df = pd.read_csv('../input/train.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cc44055171b26f1ca8a7ae2f5a0ae051a1afcbb3"},"cell_type":"markdown","source":"### Data dictionary\n- **survival** : Survival\t: 0 = No, 1 = Yes\n- **pclass**\tTicket class\t1 = 1st, 2 = 2nd, 3 = 3rd\n- **sex**: gender of the passenger\n- **Age**: age in year unit\n- **sibsp**: number of siblings / spouses aboard the Titanic\t\n- **parch**: number of parents / children aboard the Titanic\t\n- **ticket**: Ticket number\t\n- **fare**: Passenger fare\t\n- **cabin**: Cabin number\t\n- **embarked**:\tPort of Embarkation\tC = Cherbourg, Q = Queenstown, S = Southampton\n-----\n### Variable note\n**pclass**: A proxy for socio-economic status (SES)\n1st = Upper\n2nd = Middle\n3rd = Lower\n\n**age**: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\n**sibsp**: The dataset defines family relations in this way...\n\n**Sibling** = brother, sister, stepbrother, stepsister\n\n**Spouse** = husband, wife (mistresses and fianc√©s were ignored)\n\n**parch**: The dataset defines family relations in this way...\n\n**Parent** = mother, father\n\n**Child** = daughter, son, stepdaughter, stepson, some children travelled only with a nanny, therefore parch=0 for them."},{"metadata":{"_uuid":"5b1499c7579774b56c9905bb12701c58902e70ea"},"cell_type":"markdown","source":"# 2. Ask some intesting questions/Create hypothesis \n- Male or Female has higher survival rate ?\n- Passenger who in higher class would have higher rate of survive since the staff will help them first.\n- Age 20-30 should have highest survive rate since they are in the healthiest condition."},{"metadata":{"_uuid":"e33c2d8defa923a9c1d10988d936041ce459d685"},"cell_type":"markdown","source":"# 3. Clean up process"},{"metadata":{"_uuid":"42ceb9804ecae959c0bf641e75a8d19d9851df85"},"cell_type":"markdown","source":"## 3.1 Overview\nBefore make any change to the input data, we should quickly see through input data and gain some basic understanding first"},{"metadata":{"_uuid":"f8f5b31a69d56e53311924b69c6f31e187ad2bcb","trusted":false},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c2d200a413b97715c83c44577a6e4d347944cf91","trusted":false},"cell_type":"code","source":"df.columns.to_series().groupby(df.dtypes).groups","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cf9e7f0c693213ac78149d1c4d6530c4199d2369"},"cell_type":"markdown","source":"### From `.info()` we know that \n- Numerical feature are ['PassengerId', 'Survived', 'Pclass', 'SibSp', 'Parch','Age', 'Fare'] \n- Categorical feature are ['Name', 'Sex', 'Ticket', 'Cabin', 'Embarked']"},{"metadata":{"_uuid":"9efe28dd9b39d90d0e1676c3639ba240c9312259","trusted":false},"cell_type":"code","source":"#show few first rows\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c75de6709b6a29f442bef58f836bd9faffc76143","trusted":false},"cell_type":"code","source":"#Overview\ndf.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"18bacc3d83e57dd745e9ed73554d5015b42fde8f"},"cell_type":"markdown","source":"### Now we know that\n- Age, Cabin and Embarked have some missing data.\n- Suvived rate is 0.3(30%)\n- Age has quite high variance compare to other features, 14.52 and 49.69\n- Oldest passenger is 80 years old\n- Highest fare is 512.32 USD\n- Sib, Parch is maximum at 8 and 6 accordingly."},{"metadata":{"_uuid":"12dc7176cd86553b78d30b699fce7bd48a02301d"},"cell_type":"markdown","source":"## 3.2 Take care missing data"},{"metadata":{"_uuid":"6cded5efe8cd2094894fd5242935a56188d57703","trusted":false},"cell_type":"code","source":"df.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2e3d2a13744e87be5b95d7b81c65819d62b0c357","trusted":false},"cell_type":"code","source":"sns.heatmap(data=df.isna(),yticklabels=False,cmap='coolwarm',cbar=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9640f7d913bea1789508dc7a77e4d73f0d62ecff"},"cell_type":"markdown","source":"**Let's fill them up one by one**\n- Age: I think passengers in different classes may have different income rate and age, older may be the one who earn more salary and can spend more. So I will group by `Pclass` then fill missing value with mean.\n- Cabin: There are too many missing data, so I will remove this feature\n- Embarked: Since only 2 of them are missing, and they are not numerical, So I can not replace missing value with mean value. I will fill it by most appear category"},{"metadata":{"_uuid":"b9bb9457f1ecfb3221e713fa2474d96f7f513e6c","trusted":false},"cell_type":"code","source":"#Group them by age and find the mean of each Pclass\nplt.figure(figsize=(12, 5))\nax = sns.boxplot(data=df,x=df['Pclass'],y=df['Age'],palette='coolwarm') # create plot object.\nmedians = df.groupby(['Pclass'])['Age'].median().values #get median values\nmedian_labels = [str(np.round(s, 2)) for s in medians] #create label from median values\npos = range(len(medians)) # get range of median values\n#Loop to put value label\nfor tick,label in zip(pos,ax.get_xticklabels()):\n    ax.text(pos[tick], medians[tick] + 0.5, median_labels[tick], \n            horizontalalignment='center', size=13, color='r', weight='semibold')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eb2fe9cc2660009c2f2ea3206198053f028bd1a4","trusted":false},"cell_type":"code","source":"#create function to fill age\ndef fill_age_na(cols):\n    Age = cols[0]\n    Pclass = cols[1]\n    if pd.isnull(Age):\n        if Pclass == 1:\n            return 37\n        elif Pclass == 2:\n            return 29\n        else:\n            return 24\n    else:\n        return Age","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3052369ae892d85b01276a604c6eada61bf6723b","trusted":false},"cell_type":"code","source":"df['Age'] = df[['Age','Pclass']].apply(fill_age_na,axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"821d5bdcaa4c2fc21a7c1e4343c87f2a6e2cc3ee","trusted":false},"cell_type":"code","source":"df['Age'].isna().sum() # no more missing value","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"18da94beeea101a642da0ee42039293f63705ac5"},"cell_type":"markdown","source":"**Cabin** since there is too many missing value, I will remove it from dataframe\nHowever, I saw some kernel use this data as representative of passenger location before disaster"},{"metadata":{"_uuid":"cc468818bc711c6cc9bbd74edad635cfaf774878","trusted":false},"cell_type":"code","source":"col_to_drop = ['Cabin']\ndf.drop(columns=col_to_drop,axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"05cc90c2a9823d66021732bc913d09c466f1cb54","trusted":false},"cell_type":"code","source":"df.columns # 'Cabin' is now removed.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4bc178e000620c68caec3aa06d1da88ef4f22e86"},"cell_type":"markdown","source":"**Embarked** I will fill missing value by most frequent appear value"},{"metadata":{"_uuid":"743a122f140b5025e3376ea23dafbd46d6bf8c26","trusted":false},"cell_type":"code","source":"sns.countplot(x=df['Embarked'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7e41c85cfe20fbcd1889ab985b62733ea9bf621a","trusted":false},"cell_type":"code","source":"#or use mode\ndf['Embarked'].mode()[0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"47cb23ebf7b16156880d5ee6a5394f55e6016194","trusted":false},"cell_type":"code","source":"#Let's fill it\ndf['Embarked'].fillna(df['Embarked'].mode()[0], inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d65e303f4c0a76e63d014738f52fd324afbe053c","trusted":false},"cell_type":"code","source":"#check if all are filled\ndf['Embarked'].isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b4286e63947fc6dd22e16ad8f95cf13564d60d6c"},"cell_type":"markdown","source":"## 3.3 Correct data\nI will skip this part since I don't see any value to correct"},{"metadata":{"_uuid":"c67a89b7f9d334a4d6432b2d4fa27fa4f6488bc4"},"cell_type":"markdown","source":"## 3.4 Feature engineering/Create new usful feature"},{"metadata":{"_uuid":"413a5d64ce48efbddddb9d1ef50aae9c153f128f","trusted":false},"cell_type":"code","source":"# create name length feature, since I think longer name may harder to call by staff and lead to death\n# you may improve this by removing those initial first(remove Mr. Mrs, Ms, Dr. etc)\ndf['NameLength'] = df['Name'].apply(len)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"838c8844354097a05e150ffe1a1ee9656f08a871","trusted":false},"cell_type":"code","source":"df['NameLength'].hist(bins=30) #most of passenger has name length around 20-30 character","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"859e097e338a0d3170d51458ed9a386991af5fe3","trusted":false},"cell_type":"code","source":"# create family size since bigger family may help each other and all survive\ndf['FamilySize'] = df['SibSp'] + df['Parch'] + 1 # plus 1 for passenger itself","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6384aa736ff5d3c005223d27f3d79ce311ab1899","trusted":false},"cell_type":"code","source":"df['FamilySize'].hist(bins=20) #most of passenger travel alone","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"104db5e18a26a3df7b4e99cf5509eaf7ef609a02","trusted":false},"cell_type":"code","source":"# create feature IsAlone to see if the passenger travel alone\ndef IsTravelAlone(col):\n    if col == 1:\n        return 1\n    else:\n        return 0","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"51f08e37fbdb1d337b892338abf46441da8d3ca6","trusted":false},"cell_type":"code","source":"df['IsAlone'] = df['FamilySize'].apply(IsTravelAlone)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"258f8b3df3b66a6ace36b139f9ee4d545c8a9b5b","trusted":false},"cell_type":"code","source":"sns.countplot(data=df,x=df['IsAlone']) # most of passenger travel alone","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3efcfd3dc735487e63d416a955130a44b8f3a3f0"},"cell_type":"markdown","source":"## 3.5 Cut unwanted feature\nBase on my understanding to the data, I want to remove following feature before analysis\n- PassengerID: It's just id of the passenger\n- Name: I think name length can represent this feature in more usable way, name itself not usful.\n- Ticket: I don't think ticket number will relate to surviability at all, however if its telling where the passenger sit, may be we can keep it with new formatting."},{"metadata":{"_uuid":"7e2bf63fc35ac9ebee2a34b637b35fc3554eb68e","trusted":false},"cell_type":"code","source":"cols_drop = ['PassengerId','Name','Ticket']\ndf.drop(cols_drop, axis=1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1b63fc93a5376a2dad2215815883d7fcd3ffddca"},"cell_type":"markdown","source":"# 4. Exploratory Data Analysis\nPersonally, I love this part the most. I want to see what data is telling me."},{"metadata":{"_uuid":"befb079a098a94804d54eacd8142bbbd0fb2b99b","trusted":false},"cell_type":"code","source":"#let's see how each feature interact to each other\nsns.pairplot(data=df,hue='Sex',size=1.2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7eda4709bc96ce7de4e38760a8fe976976f5b424"},"cell_type":"markdown","source":"## Let's answer the questions"},{"metadata":{"_uuid":"7a94238e56f1a6a3945f76ea4efc02c44dce88ff"},"cell_type":"markdown","source":"### 1. Male or Female has higher survive rate ?"},{"metadata":{"_uuid":"2ee4d538ee5854490880e3513816633bc74a9824","trusted":false},"cell_type":"code","source":"print(df.groupby(['Sex'])['Survived'].mean())\nsns.countplot(x=df['Sex'],hue=df['Survived']) # total number of survived female is higher and survived mean is also higher than male","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a439930b4b7ad215276885b0bc355d7fb0b45cca"},"cell_type":"markdown","source":"It's also interesting to see what is age range of those male/female who survived.\nYou may see that most of female who survived has lower age than female as well."},{"metadata":{"trusted":false,"_uuid":"cb6f62f276fd1d10310150ec458556d4536ee007"},"cell_type":"code","source":"fig = plt.figure(figsize=(10,8))\nsns.violinplot(x='Sex',y='Age',hue='Survived',data=df,split=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dbc64d2677b0cc94ab65296092ec8f335b10fdec"},"cell_type":"markdown","source":"### 2. Passenger who in higher class will has higher since staff may help them first.\nplot below show that half or more than half of Pclass=1 are survived, and most of surviver are female that has fare at"},{"metadata":{"_uuid":"6e4e1e507cc4c0f154e2c57b81684a37b40ff680","trusted":false},"cell_type":"code","source":"print(df.groupby(['Pclass'])['Survived'].mean()) # highest class has 62% survaival rate while lowest class has only 24% survival rate\nsns.catplot(x='Sex',y='Fare',hue='Survived',data=df,col='Pclass',kind='swarm')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3f51a886c7e024e05bfffbb7d728b29fcde13ba1"},"cell_type":"markdown","source":"### 3. Age 20-30 should have highest survive rate since they are in the healthiest age range\n20-40 is age range that has highest survived rate, my hypothesis is partially true"},{"metadata":{"trusted":false,"_uuid":"0d59410ea43903e8dd898c82ab18642055f08def"},"cell_type":"code","source":"grid = sns.FacetGrid(data=df,col='Survived',size=8)\ngrid.map(plt.hist,'Age',bins=50)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eda57a8359c3f3cef06f34916cae98d66ccff946"},"cell_type":"markdown","source":"### Additional exploration"},{"metadata":{"_uuid":"74e3f571403833ff8bcca120a1654bf99d79700d","trusted":false},"cell_type":"code","source":"#check family size\nsns.countplot(x=df['FamilySize'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"22e09d0877a0cdc0382a57d45ee8c7edc53d15af","trusted":false},"cell_type":"code","source":"# is there any relationship between age, fare and class\nsns.jointplot(x='Age',y='Fare',data=df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b8b8e0e866b7f5ec51a362c9beca10e2ff7c63c2","trusted":false},"cell_type":"code","source":"#set overall size\nfig = plt.figure(figsize=(15,10))\n#set total number of rows and columns\nrow = 5\ncol = 2\n#set title\nfig.suptitle('Various plot',fontsize=20)\n\n#box 1\nfig.add_subplot()\nax = fig.add_subplot(2,2,1)\nsns.countplot(x='Sex',data=df,hue='IsAlone')\n#box 2\nax = fig.add_subplot(2,2,2)\ndf.groupby('Pclass')['Age'].plot(kind='hist',alpha=0.5,legend=True,title='Pclass vs Age')\n#box 3\nax = fig.add_subplot(2,2,3)\ndf.groupby('Pclass')['Fare'].plot(kind='hist',alpha=0.5,legend=True,title='Pclass vs Fare')\n#box 4\nax = fig.add_subplot(2,2,4)\nsns.violinplot(x='Sex',y='Age',data=df,hue='Survived',split=True)\n\n#some more setting\nplt.tight_layout(pad=4,w_pad=1,h_pad=1.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"897e089a84930d152c6798a9d1820f814b14a1b3"},"cell_type":"markdown","source":"**At the end of EDA, we may talk to friend or some domain expert to see if our insight are align with the history/domain knowledge or not**"},{"metadata":{"_uuid":"dedeefa8f8bd808f737c4d2d895f841adebf4587"},"cell_type":"markdown","source":"# 5. Preprocessing\nSince machine learning accept only numerical value, we have to convert all text to number.\n\n## 5.1 Convert all categorical feature to nummerical"},{"metadata":{"_uuid":"d396359601fb60e89312446e1f5d952197c04d31","trusted":false},"cell_type":"code","source":"df.head() #which feature are still categorical","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"75e52e687443f5f9a0856c7e033fa7cc4c2a66e7","trusted":false},"cell_type":"code","source":"categorical_feature = []\n#loop each column\nfor i in range(df.shape[1]):\n    #if column datatype is object/categorical\n    if df[df.columns[i]].dtype == 'object':\n        categorical_feature.append(df.columns[i])\n        \n#show\ncategorical_feature","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"83e40a5910fe42680790cbe17aecb4603e6300d9","trusted":false},"cell_type":"code","source":"#convert categorical feature to numerical\n#drop_first=True, will help avoid variable dummy trap\ndf = pd.get_dummies(data=df,columns=categorical_feature,drop_first=True) \ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"48c18c96e9116c5c4be252f4262c35359f4dbe0a"},"cell_type":"markdown","source":"## 5.2 See how each feature are correlated"},{"metadata":{"_uuid":"d51d9c1cd180b1628774ca5d1b65c4fb4c432ebe","scrolled":true,"trusted":false},"cell_type":"code","source":"fig = plt.figure(figsize=(15,10))\nsns.heatmap(df.corr(),annot=True,cmap='coolwarm',linewidths=0.2)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5f6beaf06d57c6b29c3dd90caf4ee0b7e127da2f"},"cell_type":"markdown","source":"## 5.3 Split train and test dataset\nSince I have no label for test dataset, So I can not evaluate the model performance. So I will split the train dataset into train and test dataset\n"},{"metadata":{"_uuid":"a4f80b00da30904d5aab116db7b51307bb4be7a7","trusted":false},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e5631e56afadf18940f76df591aa6cdc96cc2f10","trusted":false},"cell_type":"code","source":"dfX = df.drop('Survived',axis=1)\ndfY = df['Survived']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2e557a824e6ee8e974def709a3734d49f0955b69","trusted":false},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(dfX, dfY, test_size=0.20, \n                                                    random_state=0)\n#I saw some kernel split into train, test and validation. Should I do that to improve the model ?","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d1542b7206cf9c73b41ad7a0df4f8074e19e9cfe","trusted":false},"cell_type":"code","source":"#check size of data\nX_train.shape,y_train.shape,X_test.shape,y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e8b6d989dfa6ff3ed4131b4b244332444576b516"},"cell_type":"markdown","source":"## 5.4 Normalization/Standarization\nThere are few popular method to perform this task, it's depend on character of your data.\nFor this study I will use StandardScaler as my tool"},{"metadata":{"_uuid":"290c05ebfd27b64f56ff1fafa845c5948a4aaffc","trusted":false},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"db30b312bebc8deb77a2ec4772d0facf626d6763","trusted":false},"cell_type":"code","source":"sc = StandardScaler()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3137f536999d2068ac54ea3d3eba2459fbcb0605","trusted":false},"cell_type":"code","source":"X_train = sc.fit_transform(X_train) #fit scaler with training data\nX_test = sc.transform(X_test) #apply scaler to test data","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7137a8bac546ea32d34267955c00f65849c993cf","trusted":false},"cell_type":"code","source":"X_train[0,:]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ad867edf4da42a10b2ae6921bd16fb516d704b76","trusted":false},"cell_type":"code","source":"X_test[0,:]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"88bba5b352aa17307ebb2f7ea6d2a5e23edf9ea7"},"cell_type":"markdown","source":"Now all feature are scaled and ready to use as model input."},{"metadata":{"_uuid":"f3b61517cba04bc39e27371ea5aeddaaa058b49f","trusted":false},"cell_type":"code","source":"df.corr().loc['Survived']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"265d6f9ceb11bae6772a526a4b33ada8299b7623"},"cell_type":"markdown","source":"# 6. Model building\nSince I saw that some feature are correlated(has linear relationship) with `Survived`.\nSo I wanted to use `LinearRegression`, but it would not be possible since the output is not limited to 0 and 1. It's can be higher than 1 or lower than 0."},{"metadata":{"_uuid":"c408365b85725f6cb909e628ed4ebff1b2fff33d","trusted":false},"cell_type":"code","source":"# import library to evaluate model\nfrom sklearn.metrics import confusion_matrix,classification_report\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support\nfrom sklearn.model_selection import cross_val_score","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6985f791a116330f7bc8910854cff021fd07d483"},"cell_type":"markdown","source":"**Create function to reduce coding**"},{"metadata":{"_uuid":"b165e78d0c163e560947ff2ca177eff50b4437ab"},"cell_type":"markdown","source":"## 6.1 Create baseline model"},{"metadata":{"_uuid":"872a479c6c2454e4b5b586ddd96097d73edbcfd6","trusted":false},"cell_type":"code","source":"# this part just to show why should not use LinearRegression for binary outcome\nfrom sklearn.linear_model import LinearRegression,LogisticRegression\nmodel_lm = LinearRegression()\nmodel_lm.fit(X_train,y_train)\npred_lm = model_lm.predict(X_test)\n\n# find bad output\nbad_output = []\nfor i in pred_lm:\n    if i < 0 or i > 1:\n        bad_output.append(i)\n\nbad_output # so let's use LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8b9a2bb8e5114f3b45c387b2dfa71ea6cea509bf"},"cell_type":"markdown","source":"**So my baseline model will be LogisticRegression**"},{"metadata":{"_uuid":"a00e3b5b37e5aae499eae83c3ac2f3d3a7ec002a","trusted":false},"cell_type":"code","source":"model_lg = LogisticRegression(solver='lbfgs')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"450251e1378d20f7af36295d54c26330bd7f3938","trusted":false},"cell_type":"code","source":"model_lg.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5f2500d75210a1f5a45dccb4e499fe4fe06d7697","trusted":false},"cell_type":"code","source":"pred_lg = model_lg.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3c7af3eed9816c1998ca27351e6c7780c1052d80","trusted":false},"cell_type":"code","source":"pred_lg","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"964fab985dde60a7a487bc42f1e59695e9202ca5"},"cell_type":"markdown","source":"**Evaluate the model**"},{"metadata":{"_uuid":"cc880fa58e733bffc51311d4753f1fb0295e7e0c","trusted":false},"cell_type":"code","source":"print(classification_report(y_test,pred_lg)) #classification report\n\n#confusion matrix\ncm = confusion_matrix(y_test, pred_lg)\nplt.figure(figsize=(4,4))\nsns.heatmap(cm, annot=True,cmap='RdYlGn')\nplt.title('Model: LogisticRegression \\nAccuracy:{0:.3f}'.format(accuracy_score(y_test, pred_lg)))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9fb6bd5d7f639f1bce92dce21e42321bbc37cd48"},"cell_type":"markdown","source":"# 7. Model improvement\nSince the baseline model may not best fit to this problem, we should try different model, hyper-parameter or data shuffle to improve the model performance.\n\n** To improve the model, I plan to perform following task **\n1. Try various machine learning algorithm. Since each algorithm was developed to solve certain problem, so it other model may fit to this problem better than logistic regression. Other machine learning algorithm that I want to try are below.\n    - K-Nearest-Neigbors\n    - Support Vector Machine\n    - Naive bayes\n    - Decision Tree\n    - Random forrest\n    - GradientBoosting\n    - XGBoost\n    - Artificial Neural Network\n2. Apply cross validation for better generalization\n3. Find better hyper-parameters for each machine learning algorithm\n4. Feature selection\n5. Use differrerent normalization method\n"},{"metadata":{"_uuid":"af3fe803ac4a2b29928fb5d6426578407b3c008e"},"cell_type":"markdown","source":"## 7.1 Try various machine learning algorithm\nSince other machine learning algorithm may perform better on this problem, I will try following model to see if they can give better performance.\n\nTo perform this part I've created a list to store accuracy score of each model."},{"metadata":{"_uuid":"b6a671d68d275f475495efeebda310edefeaf3af","trusted":false},"cell_type":"code","source":"acc_score = [] # create list to store accuracy score","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8e9575707a08251399c4a7a52dfc847fcc888322"},"cell_type":"markdown","source":"**Since the steps of each model would be similar to each other, I will use function to wrap those processes into 1 line**"},{"metadata":{"_uuid":"037f89e4f487c0e42a1f1bd0bbd6f704f00232d4","trusted":false},"cell_type":"code","source":"def build_train_predict(clf,X_train,y_train,X_test,strAlg,acc_score):\n    '''\n    1. Create model\n    2. Train model\n    3. Prediction\n    4. Evaluate\n    5. Keep score\n    '''\n    model = clf\n    model.fit(X_train,y_train)\n    pred = model.predict(X_test)\n    plot_score(y_test,pred,strAlg,acc_score)\n    return clf,pred","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"817776a62e15fa209fd154243af2686c01012bb8","trusted":false},"cell_type":"code","source":"# create function to plot score for later use\ndef plot_score(y_test,y_pred,strAlg,lstScore):\n    '''\n    1. Compare prediction versus real result and plot confusion matrix\n    2. Store model accuracy score to list\n    '''\n    lstScore.append([strAlg,accuracy_score(y_test, y_pred)])\n    #print(classification_report(y_test,y_pred))\n    cm = confusion_matrix(y_test, y_pred)\n    plt.figure(figsize=(4,4))\n    sns.heatmap(cm, annot=True,cmap='RdYlGn')\n    plt.title('Model: {0} \\nAccuracy:{1:.3f}'.format(strAlg,accuracy_score(y_test, y_pred)))\n    plt.ylabel('True')\n    plt.xlabel('Predicted')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a1514a0913aa9296e7213df51e3cca34a6bdeedf"},"cell_type":"markdown","source":"### 7.1.1 Logistic Regression"},{"metadata":{"_uuid":"10bced1d6e797e3998a688787b15073d5bb1c14a","trusted":false},"cell_type":"code","source":"model_lg,pred_lg = build_train_predict(LogisticRegression(),\n                                       X_train,y_train,X_test,\n                                       'LogisticRegression',acc_score)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f3818688e813fe5211ae496ab056ef399e187996"},"cell_type":"markdown","source":"### 7.1.2 K-Nearest-Neigbors\n"},{"metadata":{"_uuid":"3ab026503937563c3d01539e9e44b48192e6a36a","trusted":false},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"273f1a3a33162c6aab3017d6f7e701e08e3bb212","trusted":false},"cell_type":"code","source":"model_knn,pred_knn = build_train_predict(KNeighborsClassifier(),\n                                       X_train,y_train,X_test,\n                                       'KNN',acc_score)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"62c81c8d8af3f8dd9659eb9292092afd3a4ffef6"},"cell_type":"markdown","source":"### 7.1.3 Suport Vector Machine"},{"metadata":{"_uuid":"1c2155a5087ee9bc089de5d7489a98a08592c9ec","trusted":false},"cell_type":"code","source":"from sklearn.svm import SVC","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"efbf15428497abbc07e287451e9482c437105e36","trusted":false},"cell_type":"code","source":"model_svm,pred_svm = build_train_predict(SVC(),\n                                       X_train,y_train,X_test,\n                                       'SVM',acc_score)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"556f3deff3775bd94a18c228439da7a663e866d6"},"cell_type":"markdown","source":"### 7.1.4 Naive Bayes"},{"metadata":{"_uuid":"3b56529767b6e4f9112f192080c3cab92a3df80e","trusted":false},"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nmodel_gnb,pred_gnb = build_train_predict(GaussianNB(),\n                                       X_train,y_train,X_test,\n                                       'GaussianNB',acc_score)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f2889832df6d83e69ecbfa0274bf207c08109827","trusted":false},"cell_type":"code","source":"from sklearn.naive_bayes import BernoulliNB\nmodel_bnb,pred_bnb = build_train_predict(BernoulliNB(),\n                                       X_train,y_train,X_test,\n                                       'BernoulliNB',acc_score)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c6bf0212ae3070be50c43196d42fd65cbca50101"},"cell_type":"markdown","source":"### 7.1.5 Decision Tree"},{"metadata":{"_uuid":"cd80172857eccc5f79658d0b26901676bece47b8","trusted":false},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6ad147a7fed2fd99dee505d8d35b8aa92d93b67c","trusted":false},"cell_type":"code","source":"model_dt,pred_dt = build_train_predict(DecisionTreeClassifier(),\n                                       X_train,y_train,X_test,\n                                       'DecisionTreeClassifier',acc_score)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"80f64eeabb7c1b460595f60b4088bf99e53eb9ce"},"cell_type":"markdown","source":"### 7.1.6 Random forrest"},{"metadata":{"_uuid":"c75197c325740da3d8f731feda3bbc7cfefef24f","trusted":false},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"237dcb2f375ef39140f286db1d7d234225e9d08e","trusted":false},"cell_type":"code","source":"model_rfc,pred_rfc = build_train_predict(RandomForestClassifier(),\n                                       X_train,y_train,X_test,\n                                       'RandomForestClassifier',acc_score)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"543567db5158d2a55defff3e353134eb4ccf669b"},"cell_type":"markdown","source":"### 7.1.7 Gradient Boost"},{"metadata":{"_uuid":"e22e97ff80e943bc6866247da57c60abe4203821","trusted":false},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9d5303585402724bc2c5ba4604c017af151271ec","trusted":false},"cell_type":"code","source":"model_gbc,pred_gbc = build_train_predict(GradientBoostingClassifier(),\n                                       X_train,y_train,X_test,\n                                       'GradientBoostingClassifier',acc_score)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"deff2817a8eaa52d68ea5bd4e070346d2246083c"},"cell_type":"markdown","source":"### 7.1.8 Extra Trees"},{"metadata":{"_uuid":"85849ccde84f67f38ed0f374046cb3c94a292efd","trusted":false},"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1f4079a157e30768798045ef3b321a251d96307c","trusted":false},"cell_type":"code","source":"model_et,pred_et = build_train_predict(ExtraTreesClassifier(),\n                                       X_train,y_train,X_test,\n                                       'ExtraTreesClassifier',acc_score)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"881495c6694cab93163209709d8ee50bd13a42ea"},"cell_type":"markdown","source":"### 7.1.9 Adaboost"},{"metadata":{"_uuid":"95bea02b6332515f875ee647356d57952cd9cefe","trusted":false},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"30fdaaa273c47261801fdec05c4996a3a9787c92","trusted":false},"cell_type":"code","source":"model_adb,pred_adb = build_train_predict(AdaBoostClassifier(),\n                                       X_train,y_train,X_test,\n                                       'AdaBoostClassifier',acc_score)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"171f89afb3128d28ae9bf20a1fb269def67ebcde"},"cell_type":"markdown","source":"### 7.1.10 XGBoost"},{"metadata":{"_uuid":"7fe76bb09b40ce92232cb5e60f54b937e297ef04","trusted":false},"cell_type":"code","source":"from xgboost import XGBClassifier","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"641259be137e0b662afd356619e30675713bb019","trusted":false},"cell_type":"code","source":"model_xgb,pred_xgb = build_train_predict(XGBClassifier(),\n                                       X_train,y_train,X_test,\n                                       'XGBClassifier',acc_score)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f25ae83f54afa27fc1189b0bdb17f3c120991cd3"},"cell_type":"markdown","source":"### 7.1.11 Artificial Neural Network"},{"metadata":{"_uuid":"fd0cd117bf52058e2bb403996c89629ac2106a20","trusted":false},"cell_type":"code","source":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\n#get number of input node and number of neuron in hidden layer\ndims = X_train.shape[1]\nh_dims = int((dims+1)/2)\ndims,h_dims\n\n#create model\nmodel_ann = Sequential() #initialize\n#input\nmodel_ann.add(Dense(units=h_dims,kernel_initializer='uniform',activation='relu',input_dim=dims))\n#hidden\nmodel_ann.add(Dense(units=h_dims,kernel_initializer='uniform',activation='relu'))\n#output\nmodel_ann.add(Dense(units=1,kernel_initializer='uniform',activation='sigmoid'))\n#compile\nmodel_ann.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n#train\nmodel_ann.fit(X_train,y_train,batch_size=32,epochs=100,verbose=0)\n\n#evaluate\npred_ann = model_ann.predict(X_test)\npred_ann = pred_ann > 0.5\nplot_score(y_test,pred_ann,'ANN',acc_score)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7a02ea5ea85e11ed9995e055b596a6272774149e","trusted":false},"cell_type":"code","source":"# See the summary, which model is leading\ndf_acc = pd.DataFrame(acc_score,columns=['Name','TestScore']).sort_values(by=['TestScore','Name'],ascending=False)\ndf_acc","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5f487a16ffe58e85874aa5553ed0710ee57ee8d3"},"cell_type":"markdown","source":"## 7.2 Cross validation\nAccuracy obtain from train dataset can be bias since some extreme observation may not exist in train dataset and lead to bad prediction when facing test dataset.\ncross-validation with Kfold would help on this issue. The score from cross validated model would be more reliable \n\n**This part will not be added to accuracy summmary table**"},{"metadata":{"_uuid":"a83ecd56db02e15639ee5682b1ba1cff5c501c63","trusted":false},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\n#create function to store\ndef cross_val_MinMaxMean(clf,X_train,y_train,fold):\n    scores = cross_val_score(clf,X_train,y_train,cv=fold)\n    print('Min: {} \\nMax: {} \\nMean: {}'.format(scores.min(),scores.max(),scores.mean()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a1514a0913aa9296e7213df51e3cca34a6bdeedf"},"cell_type":"markdown","source":"### 7.2.1 Logistic Regression"},{"metadata":{"_uuid":"10bced1d6e797e3998a688787b15073d5bb1c14a","trusted":false},"cell_type":"code","source":"cross_val_MinMaxMean(LogisticRegression(),X_train,y_train,10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f3818688e813fe5211ae496ab056ef399e187996"},"cell_type":"markdown","source":"### 7.2.2 K-Nearest-Neigbors"},{"metadata":{"_uuid":"273f1a3a33162c6aab3017d6f7e701e08e3bb212","trusted":false},"cell_type":"code","source":"cross_val_MinMaxMean(KNeighborsClassifier(),X_train,y_train,10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"62c81c8d8af3f8dd9659eb9292092afd3a4ffef6"},"cell_type":"markdown","source":"### 7.2.3 Suport Vector Machine"},{"metadata":{"_uuid":"74979393daeecdfa5268dae83abeb29a631db74b","trusted":false},"cell_type":"code","source":"cross_val_MinMaxMean(SVC(),X_train,y_train,10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"556f3deff3775bd94a18c228439da7a663e866d6"},"cell_type":"markdown","source":"### 7.2.4 Naive Bayes"},{"metadata":{"_uuid":"3b56529767b6e4f9112f192080c3cab92a3df80e","trusted":false},"cell_type":"code","source":"cross_val_MinMaxMean(GaussianNB(),X_train,y_train,10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c6bf0212ae3070be50c43196d42fd65cbca50101"},"cell_type":"markdown","source":"### 7.2.5 Decision Tree"},{"metadata":{"_uuid":"e87a3c350b142c84afcc7e588efbd56d7ef7e119","trusted":false},"cell_type":"code","source":"cross_val_MinMaxMean(DecisionTreeClassifier(),X_train,y_train,10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"80f64eeabb7c1b460595f60b4088bf99e53eb9ce"},"cell_type":"markdown","source":"### 7.2.6 Random forrest"},{"metadata":{"_uuid":"c75197c325740da3d8f731feda3bbc7cfefef24f","trusted":false},"cell_type":"code","source":"cross_val_MinMaxMean(RandomForestClassifier(),X_train,y_train,10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"543567db5158d2a55defff3e353134eb4ccf669b"},"cell_type":"markdown","source":"### 7.2.7 Gradient Boost"},{"metadata":{"_uuid":"ae13cd4832d01febea6c6788d1bc1781f3238d78","trusted":false},"cell_type":"code","source":"cross_val_MinMaxMean(GradientBoostingClassifier(),X_train,y_train,10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"deff2817a8eaa52d68ea5bd4e070346d2246083c"},"cell_type":"markdown","source":"### 7.2.8 Extra Trees"},{"metadata":{"_uuid":"d51bcfb31719b6027e82cc45292fd8ce0f515578","trusted":false},"cell_type":"code","source":"cross_val_MinMaxMean(ExtraTreesClassifier(),X_train,y_train,10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"881495c6694cab93163209709d8ee50bd13a42ea"},"cell_type":"markdown","source":"### 7.2.9 Adaboost"},{"metadata":{"_uuid":"f7c29abe6bbf5327a6ddc570422f21c4b1785e7f","trusted":false},"cell_type":"code","source":"cross_val_MinMaxMean(AdaBoostClassifier(),X_train,y_train,10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"171f89afb3128d28ae9bf20a1fb269def67ebcde"},"cell_type":"markdown","source":"### 7.2.10 XGBoost"},{"metadata":{"_uuid":"e0af20b163c046cf8947bd6e294f11a130356d67","trusted":false},"cell_type":"code","source":"cross_val_MinMaxMean(XGBClassifier(),X_train,y_train,10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f25ae83f54afa27fc1189b0bdb17f3c120991cd3"},"cell_type":"markdown","source":"### 7.2.11 Artificial Neural Network"},{"metadata":{"_uuid":"30797d01c308af9a5f897ac39d12f7182b133240","scrolled":true,"trusted":false},"cell_type":"code","source":"from keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import StratifiedKFold\n\ndef create_model():\n    model = Sequential()\n    model.add(Dense(h_dims,input_dim=dims,activation='relu'))\n    model.add(Dense(h_dims,activation='relu'))\n    model.add(Dense(1,activation='sigmoid'))\n    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n    return model\n\nmodel = KerasClassifier(build_fn=create_model,epochs=100,batch_size=10,verbose=0)\nkfold = StratifiedKFold(n_splits=10,shuffle=True)\ncross_val_MinMaxMean(model,X_train,y_train,kfold)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e917c8ae791616ecabe0cda2b8385362a0606104"},"cell_type":"markdown","source":"## 7.3 Find better hyper parameters\n\nThis part will try to find better hyper-parameters for each model using `GridSearchCV`"},{"metadata":{"_uuid":"454b21601f2c0d7113f0d0d21403ae74100b81c6","trusted":false},"cell_type":"code","source":"#import library for model improvement\nfrom sklearn.model_selection import GridSearchCV\n\n# function to reduce coding\ndef wrap_gridsearchCV(clf,X_train,y_train,X_test,param_grid,strAlg,acc_score):\n    '''\n    1. Create GridSearch model\n    2. Train model\n    3. Predict\n    4. Evaluate\n    5. Keep score\n    '''\n    model = GridSearchCV(estimator=clf,param_grid=param_grid,cv=10,\n                         refit=True,verbose=0,n_jobs=-1)\n    model.fit(X_train,y_train)\n    print('\\nBest hyper-parameter: {} \\n'.format(model.best_params_))\n    pred = model.predict(X_test)\n    plot_score(y_test,pred,strAlg,acc_score)\n    return model,pred","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a1514a0913aa9296e7213df51e3cca34a6bdeedf"},"cell_type":"markdown","source":"### 7.3.1 Logistic Regression"},{"metadata":{"_uuid":"10bced1d6e797e3998a688787b15073d5bb1c14a","trusted":false},"cell_type":"code","source":"param_grid = {\n    'C': [0.1,1, 10, 100, 1000],\n    'solver': ['newton-cg','lbfgs','liblinear','sag','saga'],\n}\nmodel_grid_lg,pred_grid_lg = wrap_gridsearchCV(LogisticRegression(),\n                                               X_train,y_train,X_test,\n                                               param_grid,\n                                               'LogisticRegression GCV',acc_score)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f3818688e813fe5211ae496ab056ef399e187996"},"cell_type":"markdown","source":"### 7.3.2 K-Nearest-Neigbors"},{"metadata":{"_uuid":"273f1a3a33162c6aab3017d6f7e701e08e3bb212","trusted":false},"cell_type":"code","source":"param_grid = {\n    'n_neighbors': [i for i in range(1,51)]\n}\nmodel_grid_knn,pred_grid_knn = wrap_gridsearchCV(KNeighborsClassifier(),\n                                               X_train,y_train,X_test,\n                                               param_grid,\n                                               'KNN GCV',acc_score)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"62c81c8d8af3f8dd9659eb9292092afd3a4ffef6"},"cell_type":"markdown","source":"### 7.3.3 Suport Vector Machine"},{"metadata":{"_uuid":"74979393daeecdfa5268dae83abeb29a631db74b","trusted":false},"cell_type":"code","source":"param_grid = {\n    'C': [0.1,1, 10, 100, 1000],\n    'gamma': [1,0.1,0.01,0.001,0.0001]\n}\nmodel_grid_svm,pred_grid_svm = wrap_gridsearchCV(SVC(),\n                                               X_train,y_train,X_test,\n                                               param_grid,\n                                               'SVM GCV',acc_score)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"556f3deff3775bd94a18c228439da7a663e866d6"},"cell_type":"markdown","source":"### 7.3.4 Naive Bayes"},{"metadata":{"_uuid":"3b56529767b6e4f9112f192080c3cab92a3df80e","trusted":false},"cell_type":"code","source":"# there is no hyper-parameter to play with","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c6bf0212ae3070be50c43196d42fd65cbca50101"},"cell_type":"markdown","source":"### 7.3.5 Decision Tree"},{"metadata":{"_uuid":"e87a3c350b142c84afcc7e588efbd56d7ef7e119","trusted":false},"cell_type":"code","source":"param_grid = {\n    'max_depth': [None,1,2,3,4,5,7,8,9,10],\n    'criterion': ['gini', 'entropy']\n}\nmodel_grid_dt,pred_grid_dt = wrap_gridsearchCV(DecisionTreeClassifier(),\n                                               X_train,y_train,X_test,\n                                               param_grid,\n                                               'DecisionTreeClassifier GCV',acc_score)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"80f64eeabb7c1b460595f60b4088bf99e53eb9ce"},"cell_type":"markdown","source":"### 7.3.6 Random forrest"},{"metadata":{"_uuid":"c75197c325740da3d8f731feda3bbc7cfefef24f","trusted":false},"cell_type":"code","source":"param_grid = {\n    'n_estimators': [i for i in range(100,1000,100)],\n    'max_depth': [i for i in range(5,10)],\n    'min_samples_leaf': [2,3,4,5]\n}\nmodel_grid_rfc,pred_grid_rfc = wrap_gridsearchCV(RandomForestClassifier(),\n                                               X_train,y_train,X_test,\n                                               param_grid,\n                                               'RandomForestClassifier GCV',acc_score)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"543567db5158d2a55defff3e353134eb4ccf669b"},"cell_type":"markdown","source":"### 7.3.7 Gradient Boost"},{"metadata":{"_uuid":"ae13cd4832d01febea6c6788d1bc1781f3238d78","trusted":false},"cell_type":"code","source":"param_grid = {\n    'loss': ['deviance', 'exponential'],\n    'n_estimators': [i for i in range(100,1000,100)],\n    'min_samples_leaf': [1,2,3,4,5]\n}\nmodel_grid_gbc,pred_grid_gbc = wrap_gridsearchCV(GradientBoostingClassifier(),\n                                               X_train,y_train,X_test,\n                                               param_grid,\n                                               'GradientBoostingClassifier GCV',acc_score)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"deff2817a8eaa52d68ea5bd4e070346d2246083c"},"cell_type":"markdown","source":"### 7.3.8 Extra Trees"},{"metadata":{"_uuid":"d51bcfb31719b6027e82cc45292fd8ce0f515578","trusted":false},"cell_type":"code","source":"param_grid = {\n    'n_estimators': [i for i in range(100,1000,100)],\n    'max_depth': [i for i in range(5,10)],\n    'min_samples_leaf':[2,3,4,5]\n}\nmodel_grid_et,pred_grid_et = wrap_gridsearchCV(ExtraTreesClassifier(),\n                                               X_train,y_train,X_test,\n                                               param_grid,\n                                               'ExtraTreesClassifier GCV',acc_score)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"881495c6694cab93163209709d8ee50bd13a42ea"},"cell_type":"markdown","source":"### 7.3.9 Adaboost"},{"metadata":{"_uuid":"f7c29abe6bbf5327a6ddc570422f21c4b1785e7f","trusted":false},"cell_type":"code","source":"param_grid = {\n    'n_estimators': [i for i in range(100,1000,100)],\n    'learning_rate' : [0.25, 0.75, 1.00]\n}\nmodel_grid_et,pred_grid_et = wrap_gridsearchCV(AdaBoostClassifier(),\n                                               X_train,y_train,X_test,\n                                               param_grid,\n                                               'AdaBoostClassifier GCV',acc_score)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"171f89afb3128d28ae9bf20a1fb269def67ebcde"},"cell_type":"markdown","source":"### 7.3.10 XGBoost"},{"metadata":{"_uuid":"e0af20b163c046cf8947bd6e294f11a130356d67","trusted":false},"cell_type":"code","source":"param_grid = {\n    'n_estimators': [i for i in range(100,1000,100)],\n    'max_depth': [i for i in range(5,10)]\n}\nmodel_grid_et,pred_grid_et = wrap_gridsearchCV(XGBClassifier(),\n                                               X_train,y_train,X_test,\n                                               param_grid,\n                                               'XGBClassifier GCV',acc_score)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f25ae83f54afa27fc1189b0bdb17f3c120991cd3"},"cell_type":"markdown","source":"### 7.3.11 Artificial Neural Network"},{"metadata":{"_uuid":"30797d01c308af9a5f897ac39d12f7182b133240","trusted":false},"cell_type":"code","source":"## later","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"17147a941015967369345214962e48eec5c10cd0"},"cell_type":"markdown","source":"## 7.4 Feature selection\nAll input features can be `Signal` or `Noise`, we may try to keep only feature that seems to give better signal of our model and remove some noise feature.\nThis would give better model accuracy, we will test this below.\n\nSelecting feature is crucial, I may select a list of feature that well correlated with target feature(`survived`) using correlation matrix but it's may lead to overfitting as well.\nI would be better if model can tell us which feature is more important than other.\n\nTree based classifier has this attribute after the model is trained."},{"metadata":{"_uuid":"94f3e3a85ece0fea2f96b7cdbb39bda21fcec2ca","trusted":false},"cell_type":"code","source":"#Take feature importance to select feature for next training\ndt_fi = model_dt.feature_importances_\nrfc_fi = model_rfc.feature_importances_\ngbc_fi = model_gbc.feature_importances_\net_fi = model_et.feature_importances_\nada_fi = model_adb.feature_importances_\nxgb_fi = model_xgb.feature_importances_\n\nfi = [dt_fi,rfc_fi,gbc_fi,et_fi,ada_fi,xgb_fi]\nmodel_name = ['DecisionTree','RandomForrest','GradientBoost',\n        'ExtraTree','AdaBoost','XGBoost']\nmodel_name = pd.Series(model_name)\ndf_fi = pd.DataFrame(fi,columns=dfX.columns)\ndf_fi.index = model_name\ndf_fi","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2903973d64d3bb5dff71c71773fb25578afbe6a6","trusted":false},"cell_type":"code","source":"#set overall size\nfig = plt.figure(figsize=(20,10))\n#set total number of rows and columns\nrow = 2\ncol = 3\n#set title\nfig.suptitle('Feature importance',fontsize=20)\n\n# boxes\nfor index,i in enumerate(df_fi.index):\n    fig.add_subplot()\n    ax = fig.add_subplot(2,3,index+1)\n    sns.barplot(df_fi.loc[i],df_fi.columns)\n    \n\n#some more setting\nplt.tight_layout(pad=4,w_pad=1,h_pad=1.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"351854bf3ea5219c592bd2944717396efb547022"},"cell_type":"code","source":"# Final score table","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f877b39fb3349a6d7e71d4348e665487b9ff4d96","trusted":false},"cell_type":"code","source":"pd.DataFrame(acc_score,columns=['model','score']).sort_values(by=['score','model'],\n                                                              ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5054f589dfc6d92192c5d75fa0dc0d0d440ec2e2"},"cell_type":"markdown","source":"### Which model should I use ? Please feel free to suggest\n\nThere are some more tasks to do on this kernel, I will continue soon.\nThanks to anyone who's reading this ;)"},{"metadata":{"_uuid":"52030a3a6ce06d7a502cff4219e015066a7c9694"},"cell_type":"markdown","source":"-------------------------------\n\n# Misc"},{"metadata":{"_uuid":"ebe9d9d021031e7d5e38f63b6128206184479bf1","trusted":false},"cell_type":"code","source":"#display tree graph\nimport graphviz\nfrom sklearn import tree\ntree_dot = tree.export_graphviz(model_dt,out_file=None, \n                                feature_names = dfX.columns, class_names = True,\n                                filled = True, rounded = True)\ntree_img = graphviz.Source(tree_dot) \ntree_img","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3ed82429bdfd91410f2bbc9c64cf6689424a7f2b","trusted":false},"cell_type":"code","source":"print(\"--- %s seconds ---\" % (time.time() - start_time))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}