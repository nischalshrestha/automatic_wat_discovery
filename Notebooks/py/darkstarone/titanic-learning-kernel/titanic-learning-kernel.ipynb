{"cells": [{"outputs": [], "execution_count": null, "metadata": {"_cell_guid": "fcbb0d74-6fd1-4176-bfa0-50ebeab2d973", "_kg_hide-output": true, "_uuid": "45308a7747b11d94cb54617986154b39c28c6540", "collapsed": true}, "source": ["# data analysis and wrangling\n", "import pandas as pd\n", "import numpy as np\n", "import random as rnd\n", "\n", "# visualization\n", "import seaborn as sns\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline\n", "\n", "# machine learning\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.svm import SVC, LinearSVC\n", "from sklearn.ensemble import RandomForestClassifier\n", "from sklearn.neighbors import KNeighborsClassifier\n", "from sklearn.naive_bayes import GaussianNB\n", "from sklearn.linear_model import Perceptron\n", "from sklearn.linear_model import SGDClassifier\n", "from sklearn.tree import DecisionTreeClassifier\n", "\n", "#from sklearn.model_selection import train_test_split\n", "from sklearn.metrics import make_scorer, accuracy_score\n", "from sklearn.model_selection import GridSearchCV\n", "from sklearn.cross_validation import KFold, train_test_split , StratifiedKFold\n", "from sklearn.feature_selection import RFECV\n", "\n", "# Data \n", "train_df = pd.read_csv('../input/train.csv')\n", "test_df = pd.read_csv('../input/test.csv')\n", "combine = [train_df, test_df]"], "cell_type": "code"}, {"outputs": [], "execution_count": null, "metadata": {"_cell_guid": "edff76cb-7084-4bef-9fc9-02a818624528", "_uuid": "2250fa10b13797c541e3cb6008111478df4fbc46", "collapsed": true}, "source": ["def plot_histograms( df , variables , n_rows , n_cols ):\n", "    fig = plt.figure( figsize = ( 16 , 12 ) )\n", "    for i, var_name in enumerate( variables ):\n", "        ax=fig.add_subplot( n_rows , n_cols , i+1 )\n", "        df[ var_name ].hist( bins=10 , ax=ax )\n", "        ax.set_title( 'Skew: ' + str( round( float( df[ var_name ].skew() ) , ) ) ) # + ' ' + var_name ) #var_name+\" Distribution\")\n", "        ax.set_xticklabels( [] , visible=False )\n", "        ax.set_yticklabels( [] , visible=False )\n", "    fig.tight_layout()  # Improves appearance a bit.\n", "    plt.show()\n", "\n", "def plot_distribution( df , var , target , **kwargs ):\n", "    row = kwargs.get( 'row' , None )\n", "    col = kwargs.get( 'col' , None )\n", "    facet = sns.FacetGrid( df , hue=target , aspect=4 , row = row , col = col )\n", "    facet.map( sns.kdeplot , var , shade= True )\n", "    facet.set( xlim=( 0 , df[ var ].max() ) )\n", "    facet.add_legend()\n", "\n", "def plot_categories( df , cat , target , **kwargs ):\n", "    row = kwargs.get( 'row' , None )\n", "    col = kwargs.get( 'col' , None )\n", "    facet = sns.FacetGrid( df , row = row , col = col )\n", "    facet.map( sns.barplot , cat , target )\n", "    facet.add_legend()\n", "\n", "def plot_correlation_map( df ):\n", "    corr = df.corr()\n", "    _ , ax = plt.subplots( figsize =( 12 , 10 ) )\n", "    cmap = sns.diverging_palette( 220 , 10 , as_cmap = True )\n", "    _ = sns.heatmap(\n", "        corr, \n", "        cmap = cmap,\n", "        square=True, \n", "        cbar_kws={ 'shrink' : .9 }, \n", "        ax=ax, \n", "        annot = True, \n", "        annot_kws = { 'fontsize' : 12 }\n", "    )\n", "\n", "def describe_more( df ):\n", "    var = [] ; l = [] ; t = []\n", "    for x in df:\n", "        var.append( x )\n", "        l.append( len( pd.value_counts( df[ x ] ) ) )\n", "        t.append( df[ x ].dtypes )\n", "    levels = pd.DataFrame( { 'Variable' : var , 'Levels' : l , 'Datatype' : t } )\n", "    levels.sort_values( by = 'Levels' , inplace = True )\n", "    return levels\n", "\n", "def plot_variable_importance( X , y ):\n", "    tree = DecisionTreeClassifier( random_state = 99 )\n", "    tree.fit( X , y )\n", "    plot_model_var_imp( tree , X , y )\n", "    \n", "def plot_model_var_imp( model , X , y ):\n", "    imp = pd.DataFrame( \n", "        model.feature_importances_  , \n", "        columns = [ 'Importance' ] , \n", "        index = X.columns \n", "    )\n", "    imp = imp.sort_values( [ 'Importance' ] , ascending = True )\n", "    imp[ : 10 ].plot( kind = 'barh' )\n", "    print (model.score( X , y ))"], "cell_type": "code"}, {"outputs": [], "execution_count": null, "metadata": {"_cell_guid": "fd92b5aa-75dc-4664-b736-3583ca23538c", "_kg_hide-output": false, "_uuid": "74316e3a7a51b983399a24c5038b636231be73d6"}, "source": ["# Checks\n", "train_df.head()\n", "train_df.tail()\n", "\n", "train_df.info()\n", "print('_'*40)\n", "test_df.info()\n", "\n", "print(train_df.describe())\n", "print(train_df.describe(include=['O']))\n", "\n", "plot_correlation_map(train_df)"], "cell_type": "code"}, {"outputs": [], "execution_count": null, "metadata": {"_cell_guid": "ee70da1b-883a-4fbe-9617-a443737d31d4", "_uuid": "ac5a095af04a46f951b6d264f2b725751b53ec3a"}, "source": ["# Pivot Tables\n", "train_df[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n", "train_df[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n", "train_df[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n", "train_df[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)"], "cell_type": "code"}, {"outputs": [], "execution_count": null, "metadata": {"_cell_guid": "52496aa5-e911-44b8-97e5-686ccbd5e73b", "_uuid": "ae157f255907731f8cd494f294f8664885fcfce4", "_kg_hide-input": false}, "source": ["# Visualizations\n", "\n", "plot_categories( train_df , cat = 'Embarked' , target = 'Survived' )\n", "plot_categories( train_df , cat = 'Sex' , target = 'Survived' )\n", "plot_categories( train_df , cat = 'Pclass' , target = 'Survived' )\n", "plot_categories( train_df , cat = 'SibSp' , target = 'Survived' )\n", "plot_categories( train_df , cat = 'Parch' , target = 'Survived' )\n", "\n", "# Age vs Survival - Numerical\n", "g = sns.FacetGrid(train_df, col='Survived')\n", "g.map(plt.hist, 'Age', bins=20)\n", "\n", "# Age vs PClass and Survival - Numerical and Ordinal \n", "grid = sns.FacetGrid(train_df, col='Survived', row='Pclass', size=2.2, aspect=1.6)\n", "grid.map(plt.hist, 'Age', alpha=.5, bins=20)\n", "grid.add_legend();\n", "\n", "# Embarked and Gender and PClass - Categorical \n", "grid = sns.FacetGrid(train_df, row='Embarked', size=2.2, aspect=1.6)\n", "grid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep')\n", "grid.add_legend()\n", "\n", "# Fare vs Embarked and Survived - Categorical and Numerical \n", "grid = sns.FacetGrid(train_df, row='Embarked', col='Survived', size=2.2, aspect=1.6)\n", "grid.map(sns.barplot, 'Sex', 'Fare', alpha=.5, ci=None)\n", "grid.add_legend()"], "cell_type": "code"}, {"outputs": [], "execution_count": null, "metadata": {"_cell_guid": "5535fad9-54c7-4b80-84dc-8a0150ca2bf8", "_kg_hide-output": true, "_uuid": "e5f33876362d7ae6eb8eab4fe08afef8c92743e6", "collapsed": true}, "source": ["# Wrangle Data \n", "train_df = train_df.drop(['Ticket'], axis=1)\n", "test_df = test_df.drop(['Ticket'], axis=1)\n", "combine = [train_df, test_df]\n"], "cell_type": "code"}, {"outputs": [], "execution_count": null, "metadata": {"_cell_guid": "d7c9d209-ef7f-489f-8632-14472f35e403", "_kg_hide-output": false, "_uuid": "7b1a64387c55ef278383ad45914646281346959d", "collapsed": true}, "source": ["# Feature Creation\n", "\n", "'''\n", "Numerical mapping can also be done with:\n", "from sklearn import preprocessing\n", "def encode_features(df_train, df_test):\n", "    features = ['Fare', 'Cabin', 'Age', 'Sex', 'Lname', 'NamePrefix']\n", "    df_combined = pd.concat([df_train[features], df_test[features]])\n", "    \n", "    for feature in features:\n", "        le = preprocessing.LabelEncoder()\n", "        le = le.fit(df_combined[feature])\n", "        df_train[feature] = le.transform(df_train[feature])\n", "        df_test[feature] = le.transform(df_test[feature])\n", "    return df_train, df_test\n", "    \n", "data_train, data_test = encode_features(data_train, data_test)\n", "'''\n", "\n", "# Extracts titles from names, and converts them into ordinal groups. \n", "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n", "for dataset in combine:\n", "    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n", "    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n", " \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n", "    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n", "    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n", "    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n", "    dataset['Title'] = dataset['Title'].map(title_mapping)\n", "    dataset['Title'] = dataset['Title'].fillna(0)\n", "    \n", "train_df = train_df.drop(['Name', 'PassengerId'], axis=1)\n", "test_df = test_df.drop(['Name'], axis=1)\n", "combine = [train_df, test_df]\n", "    \n", "# Make Sex a numerical categorical value.\n", "for dataset in combine:\n", "    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n", "    \n", "# Handle Cabin\n", "cabin_mapping = {\"U\": 0, \"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\" : 5,\"F\" : 6,\"G\":7,\"T\":8}\n", "for dataset in combine:\n", "    dataset['Cabin'] = dataset.Cabin.str.extract('([A-Z])', expand=False)\n", "    dataset['Cabin'] = dataset['Cabin'].fillna('U')\n", "    dataset['Cabin'] = dataset['Cabin'].map(cabin_mapping)\n", "    \n", "# Fill in age group blanks.\n", "# This is done by getting the median of sex and pclass combinations and using that as a basis \n", "# For the age, with a 0.5 \n", "guess_ages = np.zeros((2,3))\n", "for dataset in combine:\n", "    for i in range(0, 2):\n", "        for j in range(0, 3):\n", "            guess_df = dataset[(dataset['Sex'] == i) & \\\n", "                                  (dataset['Pclass'] == j+1)]['Age'].dropna()\n", "\n", "            age_guess = guess_df.median()\n", "\n", "            # Convert random age float to nearest .5 age\n", "            guess_ages[i,j] = int( age_guess/0.5 + 0.5 ) * 0.5\n", "            \n", "    for i in range(0, 2):\n", "        for j in range(0, 3):\n", "            dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == i) & (dataset.Pclass == j+1),\\\n", "                    'Age'] = guess_ages[i,j]\n", "\n", "    dataset['Age'] = dataset['Age'].astype(int)\n", "    \n", "# Band ages and convert to categorical \n", "\n", "train_df['AgeBand'] = pd.cut(train_df['Age'], 5)\n", "\n", "for dataset in combine:    \n", "    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n", "    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n", "    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n", "    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n", "    dataset.loc[ dataset['Age'] > 64, 'Age']\n", "    \n", "train_df = train_df.drop(['AgeBand'], axis=1)\n", "combine = [train_df, test_df]\n", "\n", "# Create IsAlone feature \n", "for dataset in combine:\n", "    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n", "    dataset['IsAlone'] = 0\n", "    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n", "\n", "# Combine Age and PClass\n", "for dataset in combine:\n", "    dataset['Age*Class'] = dataset.Age * dataset.Pclass\n", "    \n", "# Fill missing embarkation values and convert to int.\n", "freq_port = train_df.Embarked.dropna().mode()[0]\n", "for dataset in combine:\n", "    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)\n", "    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n", "    \n", "# Convert fare to numeric.\n", "test_df['Fare'].fillna(test_df['Fare'].dropna().median(), inplace=True)\n", "train_df['FareBand'] = pd.qcut(train_df['Fare'], 4)\n", "\n", "for dataset in combine:\n", "    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n", "    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n", "    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n", "    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n", "    dataset['Fare'] = dataset['Fare'].astype(int)\n", "\n", "train_df = train_df.drop(['FareBand'], axis=1)\n", "combine = [train_df, test_df]"], "cell_type": "code"}, {"outputs": [], "execution_count": null, "metadata": {"_cell_guid": "06209b54-5e8c-4895-9636-c5e083412f61", "_uuid": "3575acdd3fab77d081836ecb89b97ac13174e6a7"}, "source": ["# Model, Predict, Solve\n", "X_train = train_df.drop(\"Survived\", axis=1)\n", "Y_train = train_df[\"Survived\"]\n", "X_test  = test_df.drop(\"PassengerId\", axis=1).copy()\n", "\n", "print(X_train, Y_train, X_test)\n", "\n", "# Logistic Regression\n", "# Logistic regression measures the relationship between the categorical dependent variable (feature) \n", "# and one or more independent variables (features) by estimating probabilities using a logistic function, \n", "# which is the cumulative logistic distribution.\n", "logreg = LogisticRegression()\n", "logreg.fit(X_train, Y_train)\n", "Y_pred = logreg.predict(X_test)\n", "acc_log = round(logreg.score(X_train, Y_train) * 100, 2)\n", "\n", "# Correlations.\n", "coeff_df = pd.DataFrame(train_df.columns.delete(0))\n", "coeff_df.columns = ['Feature']\n", "coeff_df[\"Correlation\"] = pd.Series(logreg.coef_[0])\n", "\n", "print(coeff_df.sort_values(by='Correlation', ascending=False))\n", "\n", "# Support Vector Machines.\n", "# Supervised learning models with associated learning algorithms that analyze data used for classification\n", "# and regression analysis. Given a set of training samples, each marked as belonging to \n", "# one or the other of two categories, an SVM training algorithm builds a model that assigns \n", "# new test samples to one category or the other, making it a non-probabilistic binary linear classifier. \n", "svc = SVC()\n", "svc.fit(X_train, Y_train)\n", "Y_pred = svc.predict(X_test)\n", "acc_svc = round(svc.score(X_train, Y_train) * 100, 2)\n", "\n", "# k-Nearest Neighbors\n", "# is a non-parametric method used for classification and regression. \n", "# A sample is classified by a majority vote of its neighbors, with the sample being assigned to the \n", "# class most common among its k nearest neighbors (k is a positive integer, typically small). \n", "# If k = 1, then the object is simply assigned to the class of that single nearest neighbor. \n", "knn = KNeighborsClassifier(n_neighbors = 3)\n", "knn.fit(X_train, Y_train)\n", "Y_pred = knn.predict(X_test)\n", "acc_knn = round(knn.score(X_train, Y_train) * 100, 2)\n", "\n", "# Gaussian Naive Bayes\n", "# naive Bayes classifiers are a family of simple probabilistic classifiers based on applying \n", "# Bayes' theorem with strong (naive) independence assumptions between the features. \n", "# Naive Bayes classifiers are highly scalable, requiring a number of parameters \n", "# linear in the number of variables (features) in a learning problem.\n", "gaussian = GaussianNB()\n", "gaussian.fit(X_train, Y_train)\n", "Y_pred = gaussian.predict(X_test)\n", "acc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\n", "\n", "# Perceptron\n", "# The perceptron is an algorithm for supervised learning of binary classifiers \n", "# (functions that can decide whether an input, represented by a vector of numbers, \n", "# belongs to some specific class or not). It is a type of linear classifier, i.e. a classification \n", "# algorithm that makes its predictions based on a linear predictor function combining a \n", "# set of weights with the feature vector. The algorithm allows for online learning, in that it processes \n", "# elements in the training set one at a time. \n", "perceptron = Perceptron(max_iter=1000, tol=0.003)\n", "perceptron.fit(X_train, Y_train)\n", "Y_pred = perceptron.predict(X_test)\n", "acc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\n", "\n", "# Linear SVC\n", "linear_svc = LinearSVC()\n", "linear_svc.fit(X_train, Y_train)\n", "Y_pred = linear_svc.predict(X_test)\n", "acc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\n", "\n", "# Stochastic Gradient Descent\n", "sgd = SGDClassifier(max_iter=1000, tol=0.003)\n", "sgd.fit(X_train, Y_train)\n", "Y_pred = sgd.predict(X_test)\n", "acc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\n", "\n", "# Decision Tree\n", "# A predictive model which maps features (tree branches) to conclusions about the target value \n", "# (tree leaves). Tree models where the target variable can take a finite set of values are called \n", "# classification trees; in these tree structures, leaves represent class labels and branches represent \n", "# conjunctions of features that lead to those class labels.\n", "decision_tree = DecisionTreeClassifier()\n", "decision_tree.fit(X_train, Y_train)\n", "Y_pred = decision_tree.predict(X_test)\n", "acc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\n", "\n", "# Random Forest\n", "# r. Random forests or random decision forests are an ensemble learning method for classification, \n", "# regression and other tasks, that operate by constructing a multitude of decision trees (n_estimators=100)\n", "# at training time and outputting the class that is the mode of the classes (classification) \n", "# or mean prediction (regression) of the individual trees.\n", "random_forest = RandomForestClassifier(n_estimators=100)\n", "random_forest.fit(X_train, Y_train)\n", "Y_pred = random_forest.predict(X_test)\n", "random_forest.score(X_train, Y_train)\n", "acc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\n", "\n", "# Model eval\n", "models = pd.DataFrame({\n", "    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n", "              'Random Forest', 'Naive Bayes', 'Perceptron', \n", "              'Stochastic Gradient Decent', 'Linear SVC', \n", "              'Decision Tree'],\n", "    'Score': [acc_svc, acc_knn, acc_log, \n", "              acc_random_forest, acc_gaussian, acc_perceptron, \n", "              acc_sgd, acc_linear_svc, acc_decision_tree]})\n", "print(models.sort_values(by='Score', ascending=False))\n", "\n"], "cell_type": "code"}, {"outputs": [], "execution_count": null, "metadata": {"_cell_guid": "6a7f6e48-63c3-4ebf-8e9b-2a33d4a0ceb2", "scrolled": true, "_uuid": "a3729260a95c6621a98f28d80906ab8b45004150"}, "source": ["# Cross Validation\n", "num_test = 0.20\n", "X_train_cross, X_test_cross, Y_train_cross, Y_test_cross = train_test_split(X_train, Y_train, test_size=num_test, random_state=23)\n", "\n", "# Choose the type of classifier. \n", "clf = RandomForestClassifier()\n", "\n", "# Choose some parameter combinations to try\n", "parameters = {'n_estimators': [4, 6, 9], \n", "              'max_features': ['log2', 'sqrt','auto'], \n", "              'criterion': ['entropy', 'gini'],\n", "              'max_depth': [2, 3, 5, 10], \n", "              'min_samples_split': [2, 3, 5],\n", "              'min_samples_leaf': [1,5,8]\n", "             }\n", "\n", "# Type of scoring used to compare parameter combinations\n", "acc_scorer = make_scorer(accuracy_score)\n", "\n", "# Run the grid search\n", "grid_obj = GridSearchCV(clf, parameters, scoring=acc_scorer)\n", "grid_obj = grid_obj.fit(X_train_cross, Y_train_cross)\n", "\n", "# Set the clf to the best combination of parameters\n", "clf = grid_obj.best_estimator_\n", "\n", "# Fit the best algorithm to the data. \n", "clf.fit(X_train_cross, Y_train_cross)\n", "\n", "predictions = clf.predict(X_test_cross)\n", "print(accuracy_score(Y_test_cross, predictions))\n", "\n", "plot_variable_importance(X_train_cross, Y_train_cross)\n", "\n", "print (clf.score( X_train_cross , Y_train_cross ) , clf.score( X_test_cross , Y_test_cross ))\n", "\n", "rfecv = RFECV( estimator = clf , step = 1 , cv = StratifiedKFold( Y_train_cross , 2 ) , scoring = 'accuracy' )\n", "rfecv.fit( X_train_cross , Y_train_cross )\n", "\n", "# KFold\n", "def run_kfold(clf):\n", "    kf = KFold(891, n_folds=10)\n", "    outcomes = []\n", "    fold = 0\n", "    for train_index, test_index in kf:\n", "        fold += 1\n", "        X_train_cross, X_test_cross = X_train.values[train_index], X_train.values[test_index]\n", "        Y_train_cross, Y_test_cross = Y_train.values[train_index], Y_train.values[test_index]\n", "        clf.fit(X_train_cross, Y_train_cross)\n", "        predictions = clf.predict(X_test_cross)\n", "        accuracy = accuracy_score(Y_test_cross, predictions)\n", "        outcomes.append(accuracy)\n", "        print(\"Fold {0} accuracy: {1}\".format(fold, accuracy))     \n", "    mean_outcome = np.mean(outcomes)\n", "    print(\"Mean Accuracy: {0}\".format(mean_outcome)) \n", "\n", "run_kfold(clf)\n", "run_kfold(rfecv)"], "cell_type": "code"}, {"outputs": [], "execution_count": null, "metadata": {"_cell_guid": "a404a1a1-1cb4-4821-9cc8-d5c668450b6b", "_uuid": "05a2e7984d429ec524b29518b8de75ee0fe4baf2"}, "source": ["#Predict and Output\n", "\n", "predictions = rfecv.predict(test_df.drop('PassengerId', axis=1))\n", "\n", "submission = pd.DataFrame({\n", "        \"PassengerId\": test_df[\"PassengerId\"],\n", "        \"Survived\": predictions\n", "    })\n", "submission.to_csv('titanic_submission_cross_val_with_removal.csv', index=False)\n", "\n", "print(submission.to_string())"], "cell_type": "code"}], "nbformat": 4, "nbformat_minor": 1, "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "name": "python", "file_extension": ".py", "version": "3.6.3", "mimetype": "text/x-python"}}}