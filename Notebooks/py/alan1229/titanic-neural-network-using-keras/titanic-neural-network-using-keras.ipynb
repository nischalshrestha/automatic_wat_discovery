{"metadata": {"kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}, "language_info": {"pygments_lexer": "ipython3", "name": "python", "mimetype": "text/x-python", "file_extension": ".py", "nbconvert_exporter": "python", "codemirror_mode": {"version": 3, "name": "ipython"}, "version": "3.6.3"}}, "nbformat_minor": 1, "nbformat": 4, "cells": [{"metadata": {"_cell_guid": "4fdee2ce-5be4-41df-bf16-c917374ea7fc", "_uuid": "813c4df7b052f9ef5cc0f1cb856d68a284cdbb97"}, "source": ["**Prepare data for training**\n", "\n", "My idea was the network would be trained to decide which factor is important, which is not, just feed all available features into the network\n", "\n", "\n", "* Only continuous and 1/0 data\n", "\n", "* Extract TITLE from NAME, idea from another Discussion\n", "\n", "* 1 for alive, -1 for dead\n", "\n", "* Create feature for the same Ticket number"], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "17dc7909-a07f-4f6b-a2e3-0d68fd61c91a", "_uuid": "7c0ef3a3ad39c3120f08f91304f6a1cbb85afc0d"}, "outputs": [], "source": ["import pandas as pd\n", "import numpy as np\n", "\n", "train_df = pd.read_csv('../input/train.csv')\n", "test_df = pd.read_csv('../input/test.csv')\n", "combine = [train_df, test_df]\n", "PassengerId = test_df['PassengerId']\n", "\n", "for dataset in combine:\n", "    #Pclass\n", "    dataset['upperClass'] = np.where(dataset['Pclass']==1,1,0)\n", "    dataset['middleClass'] = np.where(dataset['Pclass']==2,1,0)\n", "    dataset['lowerClass'] = np.where(dataset['Pclass']==3,1,0)\n", "    #Title\n", "    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n", "    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n", " \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n", "    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n", "    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n", "    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n", "    dataset['Mr'] = np.where(dataset['Title']=='Mr',1,0)\n", "    dataset['Miss'] = np.where(dataset['Title']=='Miss',1,0)\n", "    dataset['Mrs'] = np.where(dataset['Title']=='Mrs',1,0)\n", "    dataset['Master'] = np.where(dataset['Title']=='Master',1,0)\n", "    dataset['rareTitle'] = np.where(dataset['Title']=='Rare',1,0)\n", "    #Gender\n", "    dataset['female'] = np.where(dataset['Sex']=='female',1,0)\n", "    dataset['male'] = np.where(dataset['Sex']=='male',1,0)\n", "    #Cabin\n", "    dataset['CabinChar'] = dataset['Cabin'].str[:1]\n", "    dataset['A'] = np.where(dataset['CabinChar']=='A',1,0)\n", "    dataset['B'] = np.where(dataset['CabinChar']=='B',1,0)\n", "    dataset['C'] = np.where(dataset['CabinChar']=='C',1,0)\n", "    dataset['D'] = np.where(dataset['CabinChar']=='D',1,0)\n", "    dataset['E'] = np.where(dataset['CabinChar']=='E',1,0)\n", "    dataset['noCabin'] = np.where(dataset['Cabin'].isnull(),1,0)\n", "    #Embarked\n", "    dataset['Cherbourg'] = np.where(dataset['Embarked']=='C',1,0)\n", "    dataset['Queenstown'] = np.where(dataset['Embarked']=='Q',1,0)\n", "    dataset['Southampton'] = np.where(dataset['Embarked']=='S',1,0)\n", "    #No Family\n", "    dataset['noFamily'] = np.where(dataset['SibSp'] + dataset['Parch']==0,1,0)\n", "    dataset['familySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n", "\n", "train_df['Survived'] = train_df['Survived'].replace(0,-1)\n", "    \n", "# Average age on Title\n", "all_df = pd.concat([train_df, test_df])\n", "ageGroup = all_df.groupby('Title')['Age'].mean()\n", "# Ticket information\n", "ticketSize = all_df.groupby('Ticket')['PassengerId'].count()\n", "\n", "for dataset in combine:\n", "    # Ticket Size, Ticket Survived %\n", "    dataset['ticketSize'] = dataset['Ticket'].map(ticketSize)\n", "    dataset['noTicketPartner'] = np.where(dataset['ticketSize']==1,1,0)\n", "    # Null\n", "    dataset['noAge'] = np.where(dataset['Age'].isnull(),1,0)\n", "    dataset['Age'] = dataset['Age'].fillna(dataset['Title'].map(ageGroup))\n", "    dataset['Fare'] = dataset['Fare'].fillna(dataset['Fare'].mean())\n", "    # Log\n", "    dataset['Age'] = np.where(dataset['Age'] < 1, 1, dataset['Age'])\n", "    dataset['Age'] = np.log(dataset['Age'])\n", "    dataset['Fare'] = np.where(dataset['Fare'] < 1, 1, dataset['Fare'])\n", "    dataset['Fare'] = np.log(dataset['Fare'])\n", "    \n", "\"\"\"\n", "Need to calculate the reference surived rate group by ticket\n", "Calculation need to exclude himself, otherwise the field is already have the survive data\n", "\"\"\"\n", "ticketInTrain = train_df.groupby('Ticket')['PassengerId'].count()\n", "ticketSurvived = train_df.groupby('Ticket')['Survived'].sum()/ train_df.groupby('Ticket')['PassengerId'].count()\n", "\n", "train_df['noTicketRef'] = np.where(train_df['Ticket'].map(ticketInTrain)==1,1,0)\n", "test_df['noTicketRef'] = np.where(test_df['Ticket'].map(ticketInTrain)>0,0,1)\n", "train_df['ticketRef'] = np.where(train_df['noTicketRef']==1,0\n", "        ,(train_df['Ticket'].map(ticketSurvived) * train_df['Ticket'].map(ticketInTrain) - train_df['Survived'])/(train_df['Ticket'].map(ticketInTrain) - 1))\n", "test_df['ticketRef'] = np.where(test_df['noTicketRef']==1,0,test_df['Ticket'].map(ticketSurvived))\n", "\n", "train_result = train_df['Survived']\n", "train_df.to_csv('all.csv', index=False)\n", "#Drop\n", "train_df = train_df.drop(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Title', 'Ticket', 'Embarked', 'CabinChar', 'Cabin', 'noTicketRef'], axis=1)\n", "test_df = test_df.drop(['PassengerId', 'Pclass', 'Name', 'Title', 'Sex', 'Ticket', 'Embarked', 'CabinChar', 'Cabin', 'noTicketRef'], axis=1)\n", "\n", "train_df.head()"], "execution_count": null, "cell_type": "code"}, {"metadata": {"_cell_guid": "124304ac-43b4-4c09-9314-c2c7591aa2c3", "_uuid": "e6448286fe732e3ba2cf53d60fc9b9bca294caae"}, "source": ["**Using Keras build neural network**\n", "\n", "-1st layer nodes = number of input * 2 (dropout 0.5)\n", "\n", "-2nd layer nodes = 1st layer nodes/2\n", "\n", "-Negative output means dead, Positive output means alive"], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "3e98e093-acd0-4707-93db-37fe8f16211c", "_uuid": "67594ee5953332bc9fc370ad0bea3e01d651f845"}, "outputs": [], "source": ["from keras.models import Sequential\n", "from keras.layers import Dense\n", "from keras.layers import Dropout\n", "\n", "# create some data\n", "train_df = train_df.values\n", "train_result = train_result.values\n", "test_df = test_df.values\n", "\n", "x = train_df\n", "y = train_result\n", "z = test_df\n", "\n", "# build a neural network from the 1st layer to the last layer\n", "model = Sequential()\n", "model.add(Dense(units=58, input_dim=29, activation='selu'))\n", "model.add(Dropout(0.5))\n", "model.add(Dense(units=29, activation='selu')) \n", "model.add(Dropout(0.5))\n", "model.add(Dense(units=1, activation='tanh'))\n", "\n", "# choose loss function and optimizing method\n", "model.compile(loss='mse', optimizer='sgd')\n", "\n", "# training\n", "print('Training -----------')\n", "for step in range(100001):\n", "    cost = model.train_on_batch(x, y)\n", "    if step % 10000 == 0:\n", "        print('step', step, 'train cost:', cost)\n", "\n", "# predict\n", "test_predict = model.predict(z)"], "execution_count": null, "cell_type": "code"}, {"metadata": {"collapsed": true, "_cell_guid": "5cb27b58-9c3f-4ae1-bb06-79f62e089bc3", "_uuid": "b0ac9d0fe85bbe5913fb3da41f5f88c3f4c4288a"}, "outputs": [], "source": ["# Generate Submission File\n", "test_predict = np.where(test_predict>0,1,0)\n", "NNSubmission = pd.DataFrame({ 'PassengerId': PassengerId,\n", "                            'Survived': test_predict.ravel() })\n", "NNSubmission.to_csv(\"NNSubmission.csv\", index=False)"], "execution_count": null, "cell_type": "code"}]}