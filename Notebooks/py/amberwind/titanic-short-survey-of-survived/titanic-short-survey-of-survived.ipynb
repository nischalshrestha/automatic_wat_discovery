{"metadata": {"language_info": {"codemirror_mode": {"version": 3, "name": "ipython"}, "pygments_lexer": "ipython3", "name": "python", "mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "3.6.3", "file_extension": ".py"}, "kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}}, "cells": [{"outputs": [], "execution_count": null, "metadata": {"_uuid": "2cd4a69719abb9f2a73c465326a89e0d508dcc45", "collapsed": true, "_cell_guid": "87b997ec-8b03-4693-84ef-21634994eae0"}, "cell_type": "code", "source": ["%matplotlib inline"]}, {"metadata": {"_uuid": "b265a35cd42cf24a3f59e286e706bb7ec293ef9a", "_cell_guid": "0046c66f-6e0a-4882-b776-bc125348dad3"}, "cell_type": "markdown", "source": ["**This is my first Kernel, traditionally based on famous Titanic dataset.**\n", "\n", "Please be patient, but all advices are extremely welcom :) The main goal of this kernel is not to get 100% accuracy in predicting, but to learn in data pre-processing, different models tuning and validation.\n", "\n", "The first part is related to importing numpy, pandas, etc, and loading input data."]}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "06f66aed01f8b6867d0a183a5772f01b6cadfc7a", "collapsed": false, "_cell_guid": "9f5c9889-26e0-4c5d-81ba-d2e425370f55"}, "cell_type": "code", "source": ["import numpy as np # linear algebra\n", "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n", "import seaborn as sns\n", "\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "\n", "from subprocess import check_output\n", "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n", "\n", "test = pd.read_csv('../input/test.csv')\n", "train = pd.read_csv('../input/train.csv')"]}, {"metadata": {"_uuid": "6d67bc69a3f9983f83f2ae25bc111b2e1a6bd324", "_cell_guid": "393983d7-bd9c-4502-88f7-489a2eebeee2"}, "cell_type": "markdown", "source": ["Now let's explore our training data a little bit."]}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "b7e01dede7ea5dba899dbc37a4349af49f9dca39", "_kg_hide-output": true, "collapsed": false, "_cell_guid": "ed9e7ed4-561a-4d84-b543-c52ab44a3b0b", "scrolled": false}, "cell_type": "code", "source": ["train.info()\n", "train.describe()"]}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "9ceae6051ca304487834aefe32cccf8c27173aed", "collapsed": false, "_cell_guid": "1bd1dae7-c159-4031-951e-4a802eadcb08"}, "cell_type": "code", "source": ["train.head(10)"]}, {"metadata": {"_uuid": "02f4d4449e61498b04f9ba1d80857f2131ad8efd", "_cell_guid": "86de7684-ad31-4468-921f-6faa066dd1c6"}, "cell_type": "markdown", "source": ["Ok, obviously **PassengerId** and **Ticket** variables will be useless in our prediction and could be dropped off. Probably the first letter of **Cabin** (the name of deck or part of the ship) can tell us something but there are only *204* not NA values of *891* entries totally. So in my opinion it could be dropped as well."]}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "fd737190a5b5c5b8e4bf9f1eae97da9f4215898e", "_kg_hide-output": true, "collapsed": true, "_cell_guid": "648f2b00-4753-4196-ae20-57776d15dba5"}, "cell_type": "code", "source": ["del train['PassengerId']\n", "del train['Ticket']\n", "del train['Cabin']"]}, {"metadata": {"_uuid": "84113c2777c0c74a3a703f790a2c820432e555ad", "_cell_guid": "8a6974e1-1168-48a9-901a-f2b29e3d95d2"}, "cell_type": "markdown", "source": ["Now let's get useful info from the **Name**, as each name has some title like *Mr*, *Mrs*, *Master* and so on."]}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "4f158b3ff1e95947a6d7e7f9f4e0a32919754150", "collapsed": true, "_cell_guid": "e4be34a0-3aea-488b-afed-b9a239e7ec18"}, "cell_type": "code", "source": ["#Function to get title substring of the name string\n", "def get_title(name):\n", "    return ((name.split(','))[1]).split(' ')[1]\n", "\n", "#And apply this function to get new variable 'Title'\n", "train['Title'] = train['Name'].apply(func=get_title)\n", "\n", "#Now we can remove 'Name' column\n", "del train['Name']"]}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "a14e32857a7f0b44d8b5b9a4da93da3768202085", "collapsed": false, "_cell_guid": "339923fc-d6e7-42f1-8802-52f79c143bee"}, "cell_type": "code", "source": ["train['Title'].value_counts()"]}, {"metadata": {"_uuid": "6ef90201477c3fbee00fde8c67009f59683d8166", "_cell_guid": "57513138-0424-47b0-8942-5aa6c1510001"}, "cell_type": "markdown", "source": ["Ok, now let's decide which titles could be combined. \n", "* \"Mlle\" (Mademoiselle) is unmarried woman - the same as \"Miss\". \n", "* \"Mme\" (Madame) is married as well as \"Mrs\". \n", "* \"Ms.\" marital status is unknown so we'll join it to the majority, which is \"Miss\".\n", "* \"Major.\", \"Col.\", \"Capt.\" - are all officers.\n", "* \"Jonkheer\", \"Dr.\", \"Sir.\", \"Don.\", \"Lady.\" - are related to nobility. Not difficult to find out, that what has been left as \"the\" after get_title function is \"the Countess\", so belongs to nobility as well.\n", "* \"Rev.\" stands for the Reverend - that's the matter of taste, it could be megred with nobility, but I preffer to leave clerics separately.\n", "*  \"Master.\" means boy, and we will leave Mr. and Master separately.\n", "\n", "So time to combine them.\n"]}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "48d913423a8adcaa7d7bbbea95a70cecb5cc4ab3", "collapsed": false, "_cell_guid": "c5bb3bfa-5685-4466-b64c-5c7ea92e17fc"}, "cell_type": "code", "source": ["train.Title = train.Title.replace(\"Mlle.\", \"Miss.\")\n", "train.Title = train.Title.replace(\"Mme.\", \"Mrs.\")\n", "train.Title = train.Title.replace(\"Ms.\", \"Miss.\")\n", "train.Title = train.Title.replace(\"Major.\", \"Officer\")\n", "train.Title = train.Title.replace(\"Col.\", \"Officer\")\n", "train.Title = train.Title.replace(\"Capt.\", \"Officer\")\n", "train.Title = train.Title.replace(\"Jonkheer.\", \"Nobility\")\n", "train.Title = train.Title.replace(\"Dr.\", \"Nobility\")\n", "train.Title = train.Title.replace(\"Sir.\", \"Nobility\")\n", "train.Title = train.Title.replace(\"Don.\", \"Nobility\")\n", "train.Title = train.Title.replace(\"Lady.\", \"Nobility\")\n", "train.Title = train.Title.replace(\"the\", \"Nobility\")\n", "train['Title'].value_counts()"]}, {"metadata": {"_uuid": "51edf1659e440c822d9d830982de78db7a4dcb98", "_cell_guid": "e38e9dc1-7109-4664-bd23-baafae967b70"}, "cell_type": "markdown", "source": ["There are only two missed **Embarked** values, so we will fill them manually with most frequent value which is \"*S*\"."]}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "9b3418ab68615f6d14d9d676e6cdf63e5ef3d0f2", "collapsed": true, "_cell_guid": "4c6f96fd-5f3c-445b-a6b6-27ba29c921d0"}, "cell_type": "code", "source": ["train['Embarked'].value_counts()\n", "train['Embarked'] = train['Embarked'].fillna(value=\"S\")"]}, {"metadata": {"_uuid": "52c849839c66fa7360ce4be46283c67b0c72fa5e", "_cell_guid": "c3d3a3e2-3f55-4b24-90a0-d8142998a48c"}, "cell_type": "markdown", "source": ["Now let's take closer look to the family values such as **SibSp** and **Parch**. "]}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "985682baafd7c89c94aa512d29dc824905a582d7", "collapsed": false, "_cell_guid": "41e1c3ec-55c7-47ec-8fb4-feb577516b60"}, "cell_type": "code", "source": ["sns.countplot(x=\"SibSp\", data=train);\n", "train[\"SibSp\"].value_counts()"]}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "687ac75645d168f15f5c84290fd5faa950eb8954", "collapsed": false, "_cell_guid": "2fbb893a-7b36-4f68-8003-8e14daf687bd"}, "cell_type": "code", "source": ["sns.countplot(x=\"Parch\", data=train);\n", "train[\"Parch\"].value_counts()"]}, {"metadata": {"_uuid": "3ec33c1f32e3b148d530d4edc18aa5b07b1ac8d2", "_cell_guid": "ca8129f2-a5d2-437f-8b98-a00e060cc8be"}, "cell_type": "markdown", "source": ["For both **SibSp** and **Parch** variables we can leave we can leave only 0, 1 and 'More' values. What does it mean? You are alone, or you have just one relative to care of, or more than 1 and it could be a problem to gather all family together in this case.. So now these variables can be treated as categorical."]}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "151b7b8434f099fc0f307fe9df984b63337c3843", "_kg_hide-output": true, "collapsed": false, "_cell_guid": "d1d934cf-dcef-4d29-870c-881c33d045c5"}, "cell_type": "code", "source": ["train.SibSp[train.SibSp>1] = 2\n", "train.Parch[train.Parch>1] = 2"]}, {"metadata": {"_uuid": "7fef2fa67151845ad077a1a8aa77730e452f407e", "_cell_guid": "ed68bbe7-6fa2-4c3f-bb4b-59e601e821a1"}, "cell_type": "markdown", "source": ["For **Age** we have *177* missing values, which is about 20% of the total quantity. I think we will try different ways to deal with this missing values and compare our results later on. So we will try:\n", "* Just to impute missed values with the mean age\n", "* To predict missed age based on other independent variables\n", "\n", "But firstly let's play with visualization. Time to import seaborn. And we will start with **Age** variable exploring (just to decide how we will fill missed values)"]}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "f2ddec205c0e57e42aa53da1279d2cf8f3d64f4f", "_kg_hide-output": false, "collapsed": false, "_cell_guid": "d0bfcb96-3378-4844-be4f-368b29bdbaeb", "scrolled": true}, "cell_type": "code", "source": ["#Boxplot of Age grouped by Survived\n", "sns.boxplot( x=\"Survived\", y=\"Age\", data = train);\n"]}, {"metadata": {"_uuid": "cf9814b9f4cab5a71235a4ed23cbf62d992cb876", "_cell_guid": "2266316d-9d34-49b2-82f3-68d752a7a5c6"}, "cell_type": "markdown", "source": ["Well, seems like no big difference in **Age** between **Survived** groups, but anyway we won't predict independent variable from dependent one. Let's check boxplots for **Age** by other independent categorical values"]}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "2c3e15ed62c4e112fee1f079b9726e36c982791d", "collapsed": false, "_cell_guid": "9ff33a8e-9e38-49e7-98f1-e46ea0492bc9"}, "cell_type": "code", "source": ["sns.boxplot( x=\"Sex\", y=\"Age\", data = train);"]}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "70bb6018345c870b08f129dd97269aa389ca4c3c", "collapsed": false, "_cell_guid": "6881b4d3-0ca1-4e37-8036-e7fec58a9894"}, "cell_type": "code", "source": ["sns.boxplot( x=\"Pclass\", y=\"Age\", data = train);"]}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "686a62fa3e9924f4c411fedc9a7dc84d7e796ab8", "collapsed": false, "_cell_guid": "5f35dda1-7f25-442a-8c68-57aa7aced819"}, "cell_type": "code", "source": ["sns.boxplot( x=\"Embarked\", y=\"Age\", data = train);"]}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "04b62ac09a282d65cd8ee1493f19bf17642d79d4", "collapsed": false, "_cell_guid": "af578adf-ed0a-4e4e-999c-08a6f6124dd6"}, "cell_type": "code", "source": ["sns.boxplot( x=\"Title\", y=\"Age\", data = train);"]}, {"metadata": {"_uuid": "59ba2617222479d7a42a6ea069ad2d05eb7f5efe", "_cell_guid": "b0963cfe-10f5-4981-9dc7-48a8246d83ec"}, "cell_type": "markdown", "source": ["So we see correlation of **Age** and **Title**. And that is logically clear - Master is always young as the boys had this title. Miss is usually younger than Mrs. Officers are usually older than middle age. First class passengers are older. Male and Female groups have about the same age. And the port of embarkation does not actually affect the age.\n", "\n", "\n", "As below, **Age** and **Fare** don't look to be correlated\n"]}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "56ca02a14a63ae4dc9ce49c6671f01660de39122", "collapsed": false, "_cell_guid": "dd0f5c97-fe07-4baa-82a4-dc9416d1ba64"}, "cell_type": "code", "source": ["sns.lmplot(x=\"Fare\", y=\"Age\", data=train);"]}, {"metadata": {"_uuid": "7dc8e289b066fe73eaf70eac44a73425c2af354e", "_cell_guid": "5a28ade6-eaad-42ea-885d-e632b72885d0"}, "cell_type": "markdown", "source": ["And once again all together:"]}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "5bd99b503b7f088b080c86c7390dacad9b3c1ae0", "collapsed": false, "_cell_guid": "4217a3be-d3ea-4969-88de-d13ceef72220"}, "cell_type": "code", "source": ["train.corr()"]}, {"metadata": {"_uuid": "02538a8996fa8c03787f841d07e3ce7aa07e5ea8", "_cell_guid": "06154a48-3255-4f1b-998a-35a0b0e9e1e5"}, "cell_type": "markdown", "source": ["So based on all above let's fill missing **Age** values based on **Pclass** and **Title** values, and **SibSp** / **Parch** values as well. "]}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "a7999d3c149da4481d3f7c6c02ee72e3b3bf2d08", "collapsed": false, "_cell_guid": "7578385c-7793-4ecb-a798-bfa610641820"}, "cell_type": "code", "source": ["from sklearn.ensemble import RandomForestRegressor\n", "from sklearn.model_selection import cross_val_score\n", "from sklearn.preprocessing import LabelEncoder\n", "from sklearn.metrics import mean_squared_error\n", "\n", "def impute_age(data):\n", "    temp_train = pd.DataFrame()\n", "    temp_train['Pclass'] = data['Pclass']\n", "    temp_train['Title'] = data['Title']\n", "    temp_train['SibSp'] = data['SibSp']\n", "    temp_train['Parch'] = data['Parch']\n", "    temp_train['Age'] = data['Age']\n", "    labelencoder_title = LabelEncoder()\n", "    temp_train['Title'] = labelencoder_title.fit_transform(temp_train['Title'])\n", "\n", "    train_with_age = temp_train[temp_train.Age.notnull()]\n", "\n", "    y_age_imputing = train_with_age['Age'].values\n", "    X_age_imputing = train_with_age.iloc[:, train_with_age.columns != 'Age'].values\n", "    age_regressor = RandomForestRegressor()\n", "\n", "    scores_age = cross_val_score(age_regressor, X_age_imputing, y_age_imputing, \n", "                                 scoring='neg_mean_absolute_error')\n", "    print('Mean Absolute Error for Age value imputing prediction %2f' %(-1 * scores_age.mean()))\n", "\n", "    age_regressor.fit(X_age_imputing, y_age_imputing)\n", "    temp_train.Age[temp_train.Age.isnull()] = age_regressor.predict(temp_train.iloc[:, temp_train.columns != 'Age'].values)\n", "    return temp_train.Age\n", "    \n", "train.Age = impute_age(train)\n", "train.info()"]}, {"metadata": {"_uuid": "bfc9272f14db01a9148114a93c4ee0d98ca435e7", "_cell_guid": "9c33dfe0-34d4-46c5-8c7b-7b212a0fd63d"}, "cell_type": "markdown", "source": ["Finally, we've got all values fiiled. Now we will do dummy variables for our categories. and split our set."]}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "0efc6c90700a10fd758fa2970cddf694312d22fa", "collapsed": true, "_cell_guid": "c08ca536-3062-44e4-aa86-a8422ae1f709"}, "cell_type": "code", "source": ["train = pd.get_dummies(train, columns = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Embarked\", \"Title\"], drop_first=True)\n", "X_train = train.iloc[:, train.columns != 'Survived'].values\n", "y_train = train['Survived'].values"]}, {"metadata": {"_uuid": "4ebb45ff13a1721574f8a8eff8ad9ec3baeeb665", "_cell_guid": "321950bf-9dc7-4c40-9d7e-2d155327c7a0"}, "cell_type": "markdown", "source": ["So finally we can build and tune our model."]}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "d5854263fe9d0f891647168ea46756b7da7d70a3", "collapsed": false, "_cell_guid": "d10f1e5b-4bc1-4acb-9a75-dded4368d962"}, "cell_type": "code", "source": ["import xgboost as xgb\n", "#First time on default parameters so we can tune it with Grid Search later\n", "XGB_classifier = xgb.XGBClassifier()\n", "\n", "# Applying k-Fold Cross Validation\n", "from sklearn.model_selection import cross_val_score\n", "accuracies = cross_val_score(estimator = XGB_classifier, X = X_train, y = y_train, cv = 10)\n", "accuracies.mean()"]}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "b4869f8b889f3b099bd9b583a81740da8fe24be2", "collapsed": true, "_cell_guid": "8ac738ff-a586-4548-aefb-78adc6f3d212"}, "cell_type": "code", "source": ["# Applying Grid Search to find the best model and the best parameters\n", "#from sklearn.model_selection import GridSearchCV\n", "#parameters = [{'max_depth': [5, 6, 7, 8], 'learning_rate': [0.018, 0.02, 0.022],\n", "#              'n_estimators': [150, 155, 160, 165, 170], 'objective': ['binary:logistic'], 'booster': ['gbtree'],\n", "#              'gamma':[0.002, 0.003, 0.004, 0.005, 0.006], 'scale_pos_weight':[0.9, 1, 1.1]}]\n", "#grid_search = GridSearchCV(estimator = XGB_classifier,\n", "#                           param_grid = parameters,\n", "#                           scoring = 'accuracy',\n", "#                           cv = 10)\n", "#grid_search = grid_search.fit(X_train, y_train)\n", "#best_accuracy = grid_search.best_score_\n", "#print(best_accuracy)"]}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "4af53d89ab1818d447ba9c42ef53d6de90277e8a", "collapsed": true, "_cell_guid": "f7a94e4a-efa4-488c-813e-ec2a9f0b6726"}, "cell_type": "code", "source": ["#best_parameters = grid_search.best_params_\n", "#print(best_parameters)"]}, {"metadata": {"_uuid": "f6486aa5af0c4a30e91f14eb3318f25d34bd0edc", "_cell_guid": "fe850495-eb9a-4fda-a7e0-87ee76d82d49"}, "cell_type": "markdown", "source": ["I put GridSearch under the comment, as it is not quick process, so after playing with it several times I've got accuracy 0.842873176207.\n", "And the best parameters are the following\n", "{'booster': 'gbtree', 'gamma': 0.003, 'learning_rate': 0.02, 'max_depth': 6, 'n_estimators': 165, 'objective': 'binary:logistic', 'scale_pos_weight': 1}\n", "So let's rebuild our model with the parameters we've got."]}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "bebde659d104cc2e2a82b726606383993a31965e", "collapsed": false, "_cell_guid": "40a68eeb-c6ac-4775-bb9d-a26d3c0d9b1f"}, "cell_type": "code", "source": ["XGB_classifier = xgb.XGBClassifier(booster = 'gbtree', gamma = 0.003, learning_rate = 0.02,\n", "                             max_depth = 6, n_estimators = 165, objective = 'binary:logistic',\n", "                             scale_pos_weight = 1)\n", "\n", "from sklearn.model_selection import cross_val_score\n", "accuracies = cross_val_score(estimator = XGB_classifier, X = X_train, y = y_train, cv = 10)\n", "accuracies.mean()\n", "\n", "XGB_classifier.fit(X_train, y_train)"]}, {"metadata": {"_uuid": "8e4a70d3f0196b1631a170bb78f811700d1c8b29", "_cell_guid": "f821435d-dca1-4e98-8965-620b622a899c"}, "cell_type": "markdown", "source": ["Now let's prepare test set and submit our result. Later on I will try other algoritms like Random Forest and Kernel SVM."]}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "f4ba7adc14a88f2c3f8e7c1aacec01de9442a352", "collapsed": false, "_cell_guid": "7a931350-0414-48ae-99bb-4fd163ac182d"}, "cell_type": "code", "source": ["#So let's repeat exactly the same steps with test set as we did with train one\n", "test.info()\n"]}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "b66c1569186bfec46c9d3b736f91a82442567853", "collapsed": false, "_cell_guid": "f1f9531e-03cb-4c26-97aa-8235ca9ac29d"}, "cell_type": "code", "source": ["PassengerId = test['PassengerId'] #we will need it for submitting result later\n", "del test['PassengerId']\n", "del test['Ticket']\n", "del test['Cabin']\n", "test['Title'] = test['Name'].apply(func=get_title)\n", "del test['Name']\n", "test['Title'].value_counts()"]}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "a0dd9138494c71f227bc682dd3735d1d6f4b4115", "collapsed": false, "_cell_guid": "6275b585-2857-4d04-bc58-3bf86fb8341e"}, "cell_type": "code", "source": ["test.Title = test.Title.replace(\"Ms.\", \"Miss.\")\n", "test.Title = test.Title.replace(\"Col.\", \"Officer\")\n", "test.Title = test.Title.replace(\"Dr.\", \"Nobility\")\n", "test.Title = test.Title.replace(\"Dona.\", \"Nobility\")\n", "test['Title'].value_counts()"]}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "3936678e11af96aed9b1a2800c105e2f02029ede", "_kg_hide-output": true, "collapsed": false, "_cell_guid": "c87c07c4-5680-47d9-8c0b-8beb8ee2fc45"}, "cell_type": "code", "source": ["test.SibSp[test.SibSp>1] = 2\n", "test.Parch[test.Parch>1] = 2"]}, {"metadata": {"_uuid": "4fa4d5225dc52a6c5cf64a2bcda09ce2db46839c", "_cell_guid": "a7f9deb2-438b-4771-a9cb-41bb73c24b89"}, "cell_type": "markdown", "source": ["We have missing value for **Fare**, but just one, so I'll fill it with mean."]}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "0fa58ad4fb795e14c3bbca95a01066b6ebe6f212", "collapsed": false, "_cell_guid": "e5820f5d-0b20-406b-8034-e4d2b764f66f"}, "cell_type": "code", "source": ["test['Fare'] = test['Fare'].fillna(np.mean(test['Fare']))\n", "test.info()"]}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "004419e0749f543cb9d141c9bc250e42115ddb17", "collapsed": false, "_cell_guid": "9c64083d-1e33-4f55-8d35-ad295e227822"}, "cell_type": "code", "source": ["test.Age = impute_age(test)\n", "test.info()"]}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "9ed643247a013b2d5c6290e829baad85a381e099", "collapsed": false, "_cell_guid": "e81a231d-a27e-4d09-9479-19a5aaaf7acf"}, "cell_type": "code", "source": ["test = pd.get_dummies(test, columns = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Embarked\", \"Title\"], drop_first=True)\n", "X_test = test.values"]}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "e21618a27a4d3759e27ee9144dd0ee04ded40c65", "collapsed": true, "_cell_guid": "dffc0b31-d7e8-45cf-ab5d-dd34208270a6"}, "cell_type": "code", "source": ["y_pred_XGB = XGB_classifier.predict(X_test)"]}, {"metadata": {"_uuid": "d31c88a2a873cde5835cf96d67ede44ee1a75cd7", "_cell_guid": "bf4dd1a7-8a48-4a68-8ff4-9ebca1f9b39f"}, "cell_type": "markdown", "source": ["Let's submit it!"]}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "d00b1256fb7a28e70191c5f54f72e75116f95b24", "collapsed": true, "_cell_guid": "00dcba48-99ba-467b-83f9-59b9220672e6"}, "cell_type": "code", "source": ["my_submission_XGB = pd.DataFrame({'PassengerId': PassengerId, 'Survived': y_pred_XGB})\n", "my_submission_XGB.to_csv('submission_XGB.csv', index=False)"]}, {"metadata": {"_uuid": "e86394633b00a97af8e26c5d5436fc981872e76c", "_cell_guid": "bcd70895-8593-405d-a89e-9736a3c9a259"}, "cell_type": "markdown", "source": ["And we've got 0.77990 score. Not bad for the first Kernel. But let's play with model and try other algoritmes. Firstly, Random Forest."]}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "8ce7bdea2248cfd649587f96b52e5539081964d8", "collapsed": false, "_cell_guid": "1fc1653f-89e9-4243-96a4-106fc99d7f0d"}, "cell_type": "code", "source": ["from sklearn.ensemble import RandomForestClassifier\n", "RF_classifier = RandomForestClassifier()\n", "RF_accuracies = cross_val_score(estimator = RF_classifier, X = X_train, y = y_train, cv = 10)\n", "RF_accuracies.mean()"]}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "e45778f5c4eac0c886559a80704ffa953ca66956", "collapsed": true, "_cell_guid": "299b1708-582c-4f3c-a068-56af6eedffc7"}, "cell_type": "code", "source": ["#RF_parameters = [{'n_estimators': [60, 70, 80, 90], \n", "#                  'max_features':['sqrt', 'log2', 'auto'],\n", "#                  'max_depth':[20, 30, 40, 50], 'min_samples_split':[2, 3, 4],\n", "#                  'min_samples_leaf':[2, 3, 4, 5]}]\n", "#RF_grid_search = GridSearchCV(estimator = RF_classifier,\n", "#                           param_grid = RF_parameters,\n", "#                           scoring = 'accuracy',\n", "#                           cv = 10)\n", "#RF_grid_search = RF_grid_search.fit(X_train, y_train)\n", "#RF_best_accuracy = RF_grid_search.best_score_\n", "#print(RF_best_accuracy)"]}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "ca9d60f999f8dad94f634cdc4fe481539ade9f6f", "collapsed": true, "_cell_guid": "5bcb8edb-1db5-4b0c-adc7-8650e291ddc8"}, "cell_type": "code", "source": ["#RF_best_parameters = RF_grid_search.best_params_\n", "#print(RF_best_parameters)"]}, {"metadata": {"_uuid": "e7d87d2364ddd61e7ccca94d57a09f0d8362e5e3", "_cell_guid": "a5668f53-4064-4d27-a1fc-9e7d4fe55a19"}, "cell_type": "markdown", "source": ["For Random Forest maximum accuracy I've got is 0.843995510662. With the following parameters\n", "{'max_depth': 40, 'max_features': 'log2', 'min_samples_leaf': 3, 'min_samples_split': 2, 'n_estimators': 80}\n", "It is worth a try to submit result with Random Forest as well, because even if accuracy on train set is lower than for XGBoost, the difference is very small and we still can have better score on test set."]}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "6b513861b28aabd418dab2796faf8d89aa4059ca", "collapsed": true, "_cell_guid": "e8be70b2-c770-405f-bc31-8ee9413f59de"}, "cell_type": "code", "source": ["RF_classifier = RandomForestClassifier(max_depth=40, max_features='log2',min_samples_leaf=3,\n", "                                      min_samples_split=2, n_estimators=80)\n", "RF_classifier.fit(X_train, y_train)\n", "y_pred_RF = RF_classifier.predict(X_test)\n", "my_submission_RF = pd.DataFrame({'PassengerId': PassengerId, 'Survived': y_pred_RF})\n", "my_submission_RF.to_csv('submission_rf.csv', index=False)"]}, {"metadata": {"_uuid": "55a3d6b0997f756226b6a514a54198958cc055bb", "_cell_guid": "98eccb91-2a78-47db-8843-1f7dd3f8838e"}, "cell_type": "markdown", "source": ["And we've got 0.77033 which is lower than for XGBoost. Finally let's try SVC. But now we need to scale our features."]}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "ed83edbb084d4f010ec712abb0fc740530e87b54", "collapsed": false, "_cell_guid": "4d1b2632-0dc2-4c54-92ea-b65729789e96"}, "cell_type": "code", "source": ["from sklearn.preprocessing import StandardScaler\n", "sc = StandardScaler()\n", "X_train = sc.fit_transform(X_train)\n", "from sklearn.svm import SVC\n", "SVC_classifier = SVC()\n", "SVC_accuracies = cross_val_score(estimator = SVC_classifier, X = X_train, y = y_train, cv = 10)\n", "SVC_accuracies.mean()"]}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "dcfd60b42065ebc6bf8023c2be67378d589b3da2", "collapsed": true, "_cell_guid": "5f57d7f6-32a7-42e6-8494-f343b17ebb31"}, "cell_type": "code", "source": ["#SVC_parameters = [{'C':[5, 20, 50, 80], 'kernel':['linear']},\n", "#                {'C': [40, 50, 60], 'gamma': [0.005, 0.01, 0.03, 0.05], 'kernel': ['rbf']}]\n", "#SVC_grid_search = GridSearchCV(estimator = SVC_classifier,\n", "#                           param_grid = SVC_parameters,\n", "#                           scoring = 'accuracy',\n", "#                           cv = 10)\n", "#SVC_grid_search = SVC_grid_search.fit(X_train, y_train)\n", "#SVC_best_accuracy = SVC_grid_search.best_score_\n", "#print(SVC_best_accuracy)"]}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "ef4e106d6b6363a5add3ecd9d6d861edacebd4f8", "collapsed": true, "_cell_guid": "ccdc7068-c2ba-4f3c-bf67-15876ae4e20c"}, "cell_type": "code", "source": ["#SVC_best_parameters = SVC_grid_search.best_params_\n", "#print(SVC_best_parameters)"]}, {"metadata": {"_uuid": "3142fa3fd206456e3bb8037d3df74f6dab6e043f", "_cell_guid": "38196d5d-1519-4d8e-b8ff-8b033d3c87ae"}, "cell_type": "markdown", "source": ["The best accuracy for Kernel SVM I've got is 0.826038159371 with the parameters {'C': 50, 'gamma': 0.01, 'kernel': 'rbf'}. It's less than for previous models, but let's check the score anyway."]}, {"outputs": [], "execution_count": null, "metadata": {"_uuid": "cc775f8e6f40bb82f96f88697468679cfa3ebcd5", "collapsed": true, "_cell_guid": "645dd0ab-676b-42eb-a031-a58a80cb8683"}, "cell_type": "code", "source": ["X_test = sc.transform(X_test)\n", "SVC_classifier = SVC(C=50, gamma=0.01, kernel='rbf')\n", "SVC_classifier.fit(X_train, y_train)\n", "y_pred_SVC = SVC_classifier.predict(X_test)\n", "my_submission_SVC = pd.DataFrame({'PassengerId': PassengerId, 'Survived': y_pred_SVC})\n", "my_submission_SVC.to_csv('submission_svc.csv', index=False)"]}, {"metadata": {"_uuid": "af170178ece108f16a8cdd5f6cc02c0bf7b65c94", "_cell_guid": "c5e604fb-ae91-4b4f-a43c-58b9a76f5933"}, "cell_type": "markdown", "source": ["Still worse than XGBoost - 0.76555. But anyway all three models are quite close to each other."]}, {"metadata": {"_uuid": "96cb6973fc4522bbe7c56cd5620d9e1fac4651cb", "_cell_guid": "1412d183-685f-4858-88e7-18da77e2a17b"}, "cell_type": "markdown", "source": ["So let's make small final conclusion. The best result we've got was 0.77990 with XGBoost. Of course, there are many ways still to impove it. I see for example the following:\n", "* to not drop Cabin variable, and try to impute it as well\n", "* to impute Age different way (probably based on other variable/model)\n", "* to pre-process and group Name and Title variable with differrent approach\n", "* to leave SibSp and Parch variable as they are, or group them other way (for, example - to create Family binary variable)\n", "* to use other classification algoritms (Naive Bayes or ANN, for example) or continue to play with tuning of used\n", "* and more and more\n", "But I don't want to make this Titanic to be a project of all my life, and is going to move to another challenges."]}], "nbformat": 4, "nbformat_minor": 1}