{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":18,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"trusted":true},"cell_type":"code","source":"# Set seed so that results are reproducible\n\nnp.random.seed(0)","execution_count":19,"outputs":[]},{"metadata":{"_cell_guid":"c07b4b9a-6977-4e2e-9177-50628c203b36","_uuid":"098682394d8fd8e6d07ab67589276dda8eb92fcf","collapsed":true,"trusted":true},"cell_type":"code","source":"# Place train and test data into pandas dataframes\n\ntrain = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")","execution_count":20,"outputs":[]},{"metadata":{"_cell_guid":"0cf7a87a-cc86-48de-b59b-f7efd650acee","_uuid":"871221044813894d8138650200b88d8830119a80","trusted":true},"cell_type":"code","source":"# Observe some train data observations\n\ntrain.head()","execution_count":21,"outputs":[]},{"metadata":{"_cell_guid":"5a35ef5e-1a99-48d8-a2f1-8d89879e435c","_uuid":"6cf49c9633751f481b121cb6917f0fafc1490e65","trusted":true},"cell_type":"code","source":"# Observe some test data observations\n\ntest.head()","execution_count":22,"outputs":[]},{"metadata":{"_cell_guid":"f0d4081c-6034-4642-b958-5be9d1bdcf68","_uuid":"7da92fbffcda9eab5f340597a1a166d0ef4a5229","trusted":true},"cell_type":"code","source":"# Gather brief overview of train data\n\ntrain.describe(include=\"all\")","execution_count":23,"outputs":[]},{"metadata":{"_cell_guid":"2675244b-b39f-4a25-90be-ec19e8ae5c99","_uuid":"6c64c0ed7dcd86053377115ebc376bda4627eb95","trusted":true},"cell_type":"code","source":"# Gather brief overview of test data\n\ntest.describe(include=\"all\")","execution_count":24,"outputs":[]},{"metadata":{"_cell_guid":"6560652b-6a74-4c5e-a08d-93d84fd082e6","_uuid":"2585f09bf3da1f5fefba26658de9944efbdbea1d","collapsed":true,"trusted":true},"cell_type":"code","source":"# Set the PassengerId to be the index for both the train and test sets\n\ntrain.set_index(\"PassengerId\", inplace=True)\ntest.set_index(\"PassengerId\", inplace=True)","execution_count":25,"outputs":[]},{"metadata":{"_cell_guid":"b2996f46-d473-47f4-acdb-0f28ae51c127","_uuid":"02e83341fe3734acf40dd989d73c68add95790e0","collapsed":true,"trusted":true},"cell_type":"code","source":"# Due to the arbitrariness of the values present in the Ticket feature, and \n# the number of missing values in the Cabin feature, both shall be removed \n# from the train and test sets. Where a passenger embarked on intuitively \n# wouldn't affect whether or not they survive, so that feature will also be \n# removed\n\ntrain.drop(\"Ticket\", axis=1, inplace=True)\ntrain.drop(\"Cabin\", axis=1, inplace=True)\ntrain.drop(\"Embarked\", axis=1, inplace=True)\n\ntest.drop(\"Ticket\", axis=1, inplace=True)\ntest.drop(\"Cabin\", axis=1, inplace=True)\ntest.drop(\"Embarked\", axis=1, inplace=True)","execution_count":26,"outputs":[]},{"metadata":{"_cell_guid":"5fc5e465-dcab-4f02-9e15-118f1ed91f3f","_uuid":"cfc2820df0c48e49c46fbfa4542300f095bda498","collapsed":true,"trusted":true},"cell_type":"code","source":"# Due to the number of possible names, the Name feature probably won't yield helpful information \n# with regards to who survives this disaster, and using it as a model feature will probably lead \n# to us overfitting the data. The title in the name may be useful however in giving additional \n# information about a given observation, so we will extract that feature and drop the rest of the \n# information with regards to a person's name\n\ntrain.drop(\"Name\", axis=1, inplace=True)\ntest.drop(\"Name\", axis=1, inplace=True)","execution_count":27,"outputs":[]},{"metadata":{"_cell_guid":"70bea6d1-5a0f-49a9-817a-90c1b10271c7","_uuid":"5af71ff88171a877c834c1207e4d42ccccbaef43","collapsed":true,"trusted":true},"cell_type":"code","source":"# We will set the Age and Fare for any null values to be the Age and Fare median \n# respectively in the train and test sets\n\ntrain[\"Age\"].fillna(train[\"Age\"].median(), inplace=True)\ntrain[\"Fare\"].fillna(train[\"Fare\"].median(), inplace=True)\ntest[\"Age\"].fillna(test[\"Age\"].median(), inplace=True)\ntest[\"Fare\"].fillna(test[\"Fare\"].median(), inplace=True)","execution_count":28,"outputs":[]},{"metadata":{"_cell_guid":"21939468-945a-425d-a17f-75a5ca81eadf","_uuid":"9d1eb56621d2521eb3349a0e5dce4e6c0173af54","collapsed":true,"trusted":true},"cell_type":"code","source":"# Let's transform all categorical features to dummy values\n\ntrain = pd.get_dummies(train)\ntest = pd.get_dummies(test)","execution_count":29,"outputs":[]},{"metadata":{"_cell_guid":"6e5f2fbc-88ba-4091-bf4a-1a096e2c02ca","_uuid":"78d68240a6efb24bef42d39f0ba292b060428746","trusted":true},"cell_type":"code","source":"# Check to see if this is a class imblance problem\n\ntrain[\"Survived\"].value_counts()","execution_count":30,"outputs":[]},{"metadata":{"_cell_guid":"5c21002e-d80c-4528-ab93-3b7f2bb1b7cb","_uuid":"cd99625d6d2fbb1694817431d4ad715b01ae0c22","scrolled":false,"trusted":true},"cell_type":"code","source":"# Parition training set into train and dev set in several different ways and train a \n# linear SVM and gradient boosting classifier on each one to see which \n# machine learning model performs the best\n\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier\n\nbest_model = None\nhighest_score = 0\n\nshuffler = ShuffleSplit(test_size=0.33, random_state=0)\nfor train_index, dev_index in shuffler.split(train):\n    train_instance = train.iloc[train_index]\n    X_train = train_instance.drop(labels=[\"Survived\"], axis=1)\n    Y_train = train_instance[\"Survived\"]\n    dev_instance = train.iloc[dev_index]\n    X_dev = dev_instance.drop(labels=[\"Survived\"], axis=1)\n    Y_dev = dev_instance[\"Survived\"]\n    svc_linear = LinearSVC(random_state=0)\n    rf = RandomForestClassifier(random_state=0)\n    gb = GradientBoostingClassifier(random_state=0)\n    xgb = XGBClassifier(random_state=0)\n    models = [svc_linear, rf, gb, xgb]\n    for model in models:\n        model.fit(X_train, Y_train)\n        score = model.score(X_dev, Y_dev)\n        if score > highest_score:\n            best_model = model\n            highest_score = score\n    \nprint(best_model, highest_score)","execution_count":31,"outputs":[]},{"metadata":{"_cell_guid":"d4203782-f680-4195-aad9-7873b4e77eae","_uuid":"4e8375a812c568d7b1f042bd979633222592f6c7","trusted":true},"cell_type":"code","source":"\"\"\"# Fine-tune XGB classifier hyperparameters to yield the model\n# that best fits the data\n\nbest_n_estimators = None\nbest_lr = None\nhighest_score = 0\n\nshuffler = ShuffleSplit(test_size=0.33, random_state=0)\nfor train_index, dev_index in shuffler.split(train):\n    train_instance = train.iloc[train_index]\n    X_train = train_instance.drop(labels=[\"Survived\"], axis=1)\n    Y_train = train_instance[\"Survived\"]\n    dev_instance = train.iloc[dev_index]\n    X_dev = dev_instance.drop(labels=[\"Survived\"], axis=1)\n    Y_dev = dev_instance[\"Survived\"]\n    for num in [10, 50, 100, 500, 1000]:\n        for lr in [0.001, 0.01, 0.1]:\n            model = XGBClassifier(learning_rate=lr, n_estimators=num, random_state=0)\n            model.fit(X_train, Y_train, early_stopping_rounds=5, eval_set=[(X_dev, Y_dev)], verbose=False)\n            score = model.score(X_dev, Y_dev)\n            if score >= highest_score:\n                best_n_estimators = num\n                best_lr = lr\n                highest_score = score\n    \nprint(best_n_estimators, best_lr, highest_score)\"\"\"","execution_count":32,"outputs":[]},{"metadata":{"_cell_guid":"9d400685-b2a0-432a-aa4b-7bfbc9eb0408","_uuid":"18dbe92675391c82a58ff3d766a0bf9b08a4b58f","trusted":true},"cell_type":"code","source":"# Train classifier using fine-tuned parameters\n\nX_train = train.drop(labels=[\"Survived\"], axis=1)\nY_train = train[\"Survived\"]\nclassifier = XGBClassifier(n_estimators=1000, learning_rate=0.001, random_state=0)\nclassifier.fit(X_train, Y_train)","execution_count":33,"outputs":[]},{"metadata":{"_cell_guid":"3adf8e40-c1f8-4154-a18a-fefadde6d55c","_uuid":"9c727adfe01d5c34515c66918cccb0da17f41142","trusted":true},"cell_type":"code","source":"# Submit chosen model predictions on test set\n\npredictions = classifier.predict(test)\nsubmission = pd.DataFrame(predictions).reset_index()\nsubmission.columns = [\"PassengerId\", \"Survived\"]\nsubmission[\"PassengerId\"] += 892\nsubmission.to_csv(\"submission.csv\", index=False)","execution_count":34,"outputs":[]},{"metadata":{"_cell_guid":"666f77a5-d51a-4fa6-8daa-ee1f3d5424ce","_uuid":"f0a6ef7864fa375be2a85af94ec1c29a404e2444","collapsed":true,"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}