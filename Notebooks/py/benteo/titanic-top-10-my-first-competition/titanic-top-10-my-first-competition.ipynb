{"cells": [{"source": ["# Titanic data. Exploring classifier trees and forest with sklearn and xgboost.\n", "\n", "In this notebook I take a look at the data from the kaggle Titanic training competition and try to predict if one passenger will survive based on various informations. I will use first a simple classifier trees from sklearn, a random forest and try to tune it. Finally I do the same operation with the xgboost library.\n", "\n", "## Import libraries and data\n", "\n", "First let's import all the libraries that we will need in this notebook."], "metadata": {"_cell_guid": "3648846b-c1bf-4f86-80ab-4cb96eed3b61", "_uuid": "75aa85182f1ab82b05acd16cfb165be87245e320"}, "cell_type": "markdown"}, {"source": ["%matplotlib inline\n", "# Import libraries\n", "import numpy as np\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "import random\n", "from sklearn.metrics import mean_squared_error\n", "from sklearn import tree\n", "from sklearn.ensemble import RandomForestClassifier\n", "from sklearn.ensemble.gradient_boosting import GradientBoostingClassifier\n", "from sklearn.model_selection import StratifiedKFold\n", "from sklearn.model_selection import GridSearchCV\n", "import xgboost as xgb"], "metadata": {"collapsed": true, "_cell_guid": "8d338509-17f6-4ddf-96c2-0dc833807139", "_uuid": "b029b6e197e5e8ce4f049b9174d505c9ebb00988"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["Then, let's load the training and test datas in two separate dataframes"], "metadata": {"_cell_guid": "6db656c4-7b68-40e6-a564-702e94f3dbca", "_uuid": "aa019c7cae14b3b32198ffb23c5808b5551eecbf"}, "cell_type": "markdown"}, {"source": ["# Load the training data in a dataframe\n", "train = pd.read_csv(\"../input/train.csv\")\n", "\n", "# Load the test data in a dataframe\n", "test = pd.read_csv(\"../input/test.csv\")"], "metadata": {"collapsed": true, "_cell_guid": "b018744c-5d58-412b-9d27-aad38022ee75", "_uuid": "d53183d0acc728b54c892d3abbb9f27f6650b9b8"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["##\u00a0Description of variables \n", "\n", "The following description of variables comes from the competition input\n", "\n", "| Variable |\u00a0Definition\t|\u00a0Key              |\n", "| -------- | ---------- | ---------------- |\n", "| survival | Survival   | 0 = No, 1 = Yes  |\n", "| pclass   |\u00a0Ticket class\u00a0|\t1 = 1st, 2 = 2nd, 3 = 3rd |\n", "| sex\t   |\u00a0Sex\n", "| Age\t   |\u00a0Age in years | Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n", "|\u00a0sibsp\t   | # of siblings / spouses aboard the Titanic\t\n", "| parch\t   | # of parents / children aboard the Titanic\t\n", "| ticket   |\u00a0Ticket number\t\n", "| fare\t   | Passenger fare\t\n", "| cabin\t   | Cabin number\t\n", "| embarked | Port of Embarkation | C = Cherbourg, Q = Queenstown, S = Southampton\n", "\n", "We can take a look at the data to see what we have to deal with."], "metadata": {"_cell_guid": "12ddeb0b-7686-4b2d-8e97-0b947389188a", "_uuid": "5e5f29286e89ebb1afbe441674ffdb0c05d53fb5"}, "cell_type": "markdown"}, {"source": ["# Display some data\n", "train.head()"], "metadata": {"collapsed": true, "_cell_guid": "cfc5b664-b62f-403c-a0e9-1aada3d573b1", "_uuid": "9eee1009c1f0969f64bd6591a0f5bbceb86b5d7b"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["Now we can take a first look at what we loaded by displaying the column names and their type."], "metadata": {"_cell_guid": "b6dc3c45-90d6-4abf-8c58-5a08c843f46b", "_uuid": "5132d39cedbb40919d956d09b0f52e4091412b67"}, "cell_type": "markdown"}, {"source": ["#\u00a0Show the columns of the dataframe and their original type\n", "for column in train.columns:\n", "    print(column, train[column].dtype)"], "metadata": {"collapsed": true, "_cell_guid": "e1069ab5-24fa-42c7-889c-b0dcb3284641", "_uuid": "a97ea31753be9577d798956986a92180e56d4267"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["Some columns are treated as integer but should be categories. It is easy to convert them."], "metadata": {"_cell_guid": "120fba02-7594-4d37-83c9-169180b68b13", "_uuid": "3af2eac9e37338b2e4762bc7560a1efebc79b840"}, "cell_type": "markdown"}, {"source": ["# Convert some columns into categorical data\n", "train[\"Survived\"] = train[\"Survived\"].astype('category')\n", "train[\"Pclass\"] = train[\"Pclass\"].astype('category')\n", "test[\"Pclass\"] = test[\"Pclass\"].astype('category')\n", "train.describe()"], "metadata": {"collapsed": true, "_cell_guid": "6a03efb6-cd67-4856-aee2-a51d0096a3e4", "_uuid": "866295b70980ed433b03e6b64ff0ca5ef8bdca9a"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["##\u00a0Analyze data\n", "\n", "Before doing any operations and predictions, we can take a look at the distribution of some variables to get an idea on what could depend the survival rate.\n", "\n", "First, we can see that only one third of the passengers survived. This could be a first dummy model."], "metadata": {"_cell_guid": "565e5aa3-f376-4077-be30-186239336945", "_uuid": "82115517fbedda67dd0c1ae0ac8196cf8ada4110"}, "cell_type": "markdown"}, {"source": ["train[\"Survived\"].value_counts().plot.pie(figsize=(4, 4),\n", "                                          autopct='%.2f',\n", "                                          title=\"Percentage of survivors\",\n", "                                          fontsize = 10)"], "metadata": {"collapsed": true, "_cell_guid": "b9982a21-404e-46ca-9eec-ff1f868b5da1", "_uuid": "3867426286b8e932dc28b7a31b458c7571d405fb"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["One third of the passengers are female. Is that correlated with the survival rate ?"], "metadata": {"_cell_guid": "ce8ac857-de49-48a2-a56f-aaaa49a6d3a1", "_uuid": "465856777a56a15bc963c01a7fb35d8fc5d92174"}, "cell_type": "markdown"}, {"source": ["train[\"Sex\"].value_counts().plot.pie(figsize=(4, 4),\n", "                                     autopct='%.2f',\n", "                                     title=\"Percentage of Male and Female passengers\",\n", "                                     fontsize = 10)"], "metadata": {"collapsed": true, "_cell_guid": "93595746-fe96-472b-8691-1cda7a6c0c11", "_uuid": "b1a4760ebd4f330182b218a7ef57e094f1768033"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["Well it appears that it is the case. You have a better chance to survive if you are female."], "metadata": {"_cell_guid": "9f3aff58-9d50-47b8-a94b-52e201209ab0", "_uuid": "19d6cb5b68e2e8bea5612f1315287263c08b3075"}, "cell_type": "markdown"}, {"source": ["sns.countplot(x=\"Survived\", hue=\"Sex\", data=train);"], "metadata": {"collapsed": true, "_cell_guid": "14a4a77c-26e2-4093-929d-6a42e0b9a88b", "_uuid": "de7e5dbc38f021d8adccd9ceece23bd5d2ec2026"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["Also, about half of the passengers are on the third class."], "metadata": {"_cell_guid": "e2599ba0-5412-4598-ba7f-9fa00c8a7e41", "_uuid": "cc0c89a6d7984e3db1eedff59ff81d6c9963e173"}, "cell_type": "markdown"}, {"source": ["train[\"Pclass\"].value_counts().plot.pie(figsize=(4, 4),\n", "                                        autopct='%.2f',\n", "                                        title=\"Percentage of passengers per Class\",\n", "                                        fontsize = 10)"], "metadata": {"collapsed": true, "_cell_guid": "d638716a-eb73-4129-8fb7-fb4d5befb081", "_uuid": "4ed551952a740a850840a033113c7990a878c029"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["And if the class distribution in the survived who survived is approximately equal, most of the third class passengers died."], "metadata": {"_cell_guid": "20007170-fe5a-417f-ad38-007350a09faa", "_uuid": "29ae63739f5589035e04ab40c6c28034f6cc02cf"}, "cell_type": "markdown"}, {"source": ["sns.countplot(x=\"Survived\", hue=\"Pclass\", data=train);"], "metadata": {"collapsed": true, "_cell_guid": "d0f4c2f6-e582-4b57-bb4d-c9ce7274aaf7", "_uuid": "676581227892560682b60ec73ca6aa1fea8c4211"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["From this first look at the data, it is easy to say that you have a better chance of survival if you are female and from the first class, which is not surprising.\n", "\n", "It is possible to further analyse the data with plots and correlations. But the goal is to build a model, not from personal insights, but from machine learning.\n", "\n", "## Data treatment / Feature engineering\n", "\n", "In order to feed data to a machine learning model, some preparations must be made first.\n", "I wrote one subroutine by feature I engineered. Look at the subroutine documentations for informations."], "metadata": {"_cell_guid": "f4c49a9d-2bc4-4086-928c-bd1f8bfd7b08", "_uuid": "225e6a703b960db99c03c4c92e8c2a7cde198939"}, "cell_type": "markdown"}, {"source": ["def process_family(data):\n", "    \"\"\" Aggregate the family size\"\"\"\n", "    print(\"Processing family\")\n", "    data[\"familysize\"] = data[\"SibSp\"] + data[\"Parch\"]\n", "    print(\"    Done\")\n", "    return data\n", "\n", "def process_ticket(data):\n", "    \"\"\" Get further informations from the ticket number.\n", "    Some passengers have the same ticket number. \n", "    We assume that it means they belong to the same group of people.\n", "    \"\"\"\n", "    print(\"Processing ticket\")\n", "    data[\"ticketgroupsize\"] = data.groupby(\"Ticket\")[\"Ticket\"].transform(\"count\") - 1\n", "    print(\"    Done\")\n", "    return data\n", "\n", "def find_nan(data, feature, error=False):\n", "    \"\"\" Look for missing values in a specific feature,\n", "    count them and display the number. Raise an error \n", "    if asked. \"\"\"\n", "    if data[feature].isnull().values.any():\n", "        print(\"    Missing values: \",\n", "              data[feature].isnull().sum(),\n", "              \"over\",\n", "              len(data[feature].index),\n", "              \"(\",\n", "              data[feature].isnull().sum()/len(data[feature].index),\n", "              \"%)\")\n", "        if error:\n", "            raise ValueError(\"NaN\")\n", "    return data[feature].isnull().values.any()\n", "\n", "def process_names(data):\n", "    \"\"\" Get further informations from the name title and put\n", "    it into a new categorical data named type.\"\"\"\n", "    print(\"Processing Names\")\n", "    find_nan(data, \"Name\")\n", "    #\u00a0It is chosen to regroup titles by relevant categories.\n", "    #\u00a0Modifying that should have a big impact on the model results.\n", "    name_dict = {\"Capt\":       \"officer\",\n", "                 \"Col\":        \"officer\",\n", "                 \"Major\":      \"officer\",\n", "                 \"Dr\":         \"officer\",\n", "                 \"Rev\":        \"officer\",\n", "                 \"Jonkheer\":   \"snob\",\n", "                 \"Don\":        \"snob\",\n", "                 \"Sir\" :       \"snob\",\n", "                 \"the Countess\":\"snob\",\n", "                 \"Dona\":       \"snob\",\n", "                 \"Lady\" :      \"snob\",\n", "                 \"Mme\":        \"married\",\n", "                 \"Ms\":         \"married\",\n", "                 \"Mrs\" :       \"married\",\n", "                 \"Miss\" :      \"single\",\n", "                 \"Mlle\":       \"single\",\n", "                 \"Mr\" :        \"man\",\n", "                 \"Master\" :    \"boy\"\n", "                }\n", "    data['prefix'] = data['Name'].map(lambda name:name.split(',')[1].split('.')[0].strip())\n", "    data['type'] = data['prefix'].map(name_dict)\n", "\n", "    # Dummy encoding\n", "    #\u00a0Create one column for each value of the categorical data and assign a\n", "    #\u00a0one or zero value.\n", "    #\u00a0This is needed for sklearn that can only deal with numbers.\n", "    column_dummies = pd.get_dummies(data['type'],prefix='type')\n", "    data = pd.concat([data,column_dummies],axis=1)\n", "    print(\"    Done\")\n", "    \n", "    return data\n", "\n", "def process_sex(data):\n", "    \"\"\" Dummy encoding for the sex. \n", "    Map one to male and zero to female \"\"\"\n", "    print(\"Processing Sex\")\n", "    find_nan(data, \"Sex\", error=True)\n", "    data['Sex'] = data['Sex'].map({'male':1,'female':0})\n", "    print(\"    Done\")\n", "    \n", "    return data\n", "\n", "def process_age(data):\n", "    \"\"\" Deal with missing age values. \n", "    A lot of data is missing in the age column.\n", "    This is filled by a categorized median age.\n", "    \"\"\"\n", "    print(\"Processing Age\")\n", "    find_nan(data, \"Age\")\n", "    \n", "    medianage = data.groupby(['Sex', 'Pclass', 'type'])[\"Age\"].median()\n", "    \n", "    def fillna_age(row, medianage):\n", "        age = medianage.loc[row[\"Sex\"], row['Pclass'], row[\"type\"]]\n", "        return age\n", "\n", "    data[\"Age\"] = data.apply(lambda row : fillna_age(row, medianage) if np.isnan(row['Age']) else row['Age'], axis=1)\n", "    find_nan(data, \"Age\")\n", "    print(\"    Done\")\n", "    \n", "    return data\n", "\n", "def process_fare(data):\n", "    \"\"\" Deal with missing fare values.\"\"\"\n", "    print(\"Processing Fare\")\n", "    find_nan(data, \"Fare\")\n", "    data['Fare'].fillna(data['Fare'].median(), inplace=True)\n", "    print(\"    Done\")\n", "    \n", "    return data\n", "\n", "def process_embarked(data):\n", "    \"\"\" Deal with missing embarked data and create\n", "    the dummy encoding. \"\"\"\n", "    print(\"Processing Embarked\")\n", "    find_nan(data, \"Embarked\")\n", "    #\u00a0Find the most common occurence of the categorical data\n", "    most_common = data['Embarked'].value_counts().index[0]\n", "    #\u00a0Replace NaN values with the most common occurence\n", "    data['Embarked'].fillna(most_common, inplace=True)\n", "    \n", "    # dummy encoding\n", "    #\u00a0Create one column for each value of the categorical data\n", "    column_dummies = pd.get_dummies(data['Embarked'],prefix='Embarked')\n", "    data = pd.concat([data,column_dummies],axis=1)\n", "    #\u00a0Drop the now irrelevant column\n", "    data.drop('Embarked',axis=1,inplace=True)\n", "    print(\"    Done\")\n", "    \n", "    return data\n", "\n", "def process_cabin(data):\n", "    \"\"\" Get the deck information from the cabin feature\n", "    and deal with missing values. \"\"\"\n", "    print(\"Processing Cabin\")\n", "    find_nan(data, \"Cabin\")\n", "    #\u00a0Replace NaN values with U for unknown\n", "    data['Cabin'].fillna(\"U\", inplace=True)\n", "    # Extract the deck information\n", "    data['deck'] = data[\"Cabin\"].map(lambda row: row[0])\n", "    # dummy encoding\n", "    #\u00a0Create one column for each value of the categorical data\n", "    column_dummies = pd.get_dummies(data['deck'],prefix='deck')\n", "    data = pd.concat([data,column_dummies],axis=1)\n", "    print(\"    Done\")\n", "    \n", "    return data\n", "\n", "def process_all(data):\n", "    \"\"\" Process all the dataset features and return the dataset \"\"\"\n", "    data = process_family(data)\n", "    data = process_ticket(data)\n", "    data = process_sex(data)\n", "    data = process_names(data)\n", "    data = process_age(data)\n", "    data = process_fare(data)\n", "    data = process_embarked(data)\n", "    data = process_cabin(data)\n", "    return data\n", "\n", "def write_results(data, model):\n", "    \"\"\" Write results in the csv format for competition submission \"\"\"\n", "    with open(\"titanic.csv\",\"w\") as outfile:\n", "        outfile.write(\"PassengerId,Survived\\n\")\n", "        for passenger in data.index:\n", "            line = str(data.at[passenger, \"PassengerId\"]) + \",\" + str(int(data.at[passenger, model])) + \"\\n\"\n", "            outfile.write(line)"], "metadata": {"collapsed": true, "_cell_guid": "43709b38-c0d7-44db-b53d-3afb97f5e4a1", "_uuid": "e32d369d57f1d0a6bf5da17107e34e328560dc36"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["All this feature preprocessing will have a big impact on the model precision.\n", "Let's apply it to the train and test dataframes.\n", "\n", "Combine the train and test dataset in one dataset for feature engineering.\n", "Then resplit them in two separate datasets."], "metadata": {"_cell_guid": "33e1d08a-ab54-455f-8002-42db288219d7", "_uuid": "2c8db2f3c846be61b093b915d0ae10225125cfcd"}, "cell_type": "markdown"}, {"source": ["#\u00a0Load\n", "train = pd.read_csv(\"../input/train.csv\")\n", "test = pd.read_csv(\"../input/test.csv\")\n", "#\u00a0Merge & process\n", "merged = train.append(test)\n", "merged = process_all(merged)\n", "# Split\n", "train = pd.DataFrame(merged.head(len(train)))\n", "test = pd.DataFrame(merged.iloc[len(train):])"], "metadata": {"collapsed": true, "_cell_guid": "dfcd6f5d-b589-42b4-af05-e7900acf0daa", "_uuid": "b163aad1fc4e6beefcc83f64412371d50ab884fc", "scrolled": true}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["train.columns"], "metadata": {"collapsed": true, "_cell_guid": "b7c9ec0a-a60b-4cd6-aa99-e9bd40619bb1", "_uuid": "e811bc7b47b504efe5d7340290f923dee386ae26"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["## First stupid models and evaluation\n", "\n", "Before doing any advanced machine learning models, we can build stupid model to then know if complex models do better than that. The score is evaluated with the root mean squared error.\n", "\n", "The first stupid model is a weighted random from the observation we made. Only one third of passengers survive."], "metadata": {"_cell_guid": "4ff1b86b-ca20-4d9e-8094-19621874eddd", "_uuid": "4704b6a78dee4f46c20b55b8ef96aa511a2bd329"}, "cell_type": "markdown"}, {"source": ["train[\"eval_random\"] = [np.random.choice([0,1], p=[0.62, 0.38]) for passenger in train.index]\n", "\n", "rmse_random = np.sqrt(mean_squared_error(train[\"Survived\"], train[\"eval_random\"]))\n", "print(rmse_random)"], "metadata": {"collapsed": true, "_cell_guid": "d00852af-f8fb-4016-8cee-1a5c2b2c0d6f", "_uuid": "a1d6de7b4a81135b64f2bdd475ade185d3d5d5ac"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["The second stupid model is rich woman. If you are rich, and a woman, you survive."], "metadata": {"_cell_guid": "c47b335f-dcdc-42b3-9dc2-a04254f7ffad", "_uuid": "21ddd7db0e1c7968544a7744152d14ec9bd9c824"}, "cell_type": "markdown"}, {"source": ["def richwoman(passenger, data):\n", "    \"\"\" If you are a female from the first class, you survive. \"\"\"\n", "    if data.at[passenger, \"Sex\"] == \"female\" and data.at[passenger, \"Pclass\"] == 1:\n", "        return 1\n", "    else:\n", "        return 0\n", "train[\"eval_richwoman\"] = [richwoman(passenger, train) for passenger in train.index]\n", "\n", "rmse_richwoman = np.sqrt(mean_squared_error(train[\"Survived\"], train[\"eval_richwoman\"]))\n", "print(rmse_richwoman)"], "metadata": {"collapsed": true, "_cell_guid": "12d4330f-0e9d-47a7-b90f-c7687aab5602", "_uuid": "99af9c53685b57d4048335182eee62df53e1898e"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["So we see that building a model from feature informations does a better job than an overall probability. Now the goal is to build a more complex model to get a better score.\n", "\n", "#\u00a0Sklearn\n", "##\u00a0Single tree\n", "\n", "Building a single classifier decision tree with sklearn is easy. You have to provide the list of features that you want the decisions to be made on. Here we give the list of all the features that we have. The multiplication of features comes from the dummy encoding preprocessing that is need for sklearn."], "metadata": {"_cell_guid": "9644de6b-1674-47fa-aa79-b98a27f36581", "_uuid": "0a8e09c4b49a0ed01e1ba4c6f46f40600feae1e9"}, "cell_type": "markdown"}, {"source": ["features_names = [\"Fare\",\n", "                  \"SibSp\",\n", "                  \"Parch\",\n", "                  \"familysize\",\n", "                  \"ticketgroupsize\",\n", "                  \"Pclass\",\n", "                  \"Sex\", \n", "                  \"Age\",\n", "                  \"Embarked_C\", \n", "                  \"Embarked_Q\",\n", "                  \"Embarked_S\",\n", "                  \"type_boy\",\n", "                  \"type_officer\",\n", "                  \"type_married\",\n", "                  \"type_single\",\n", "                  \"type_snob\",\n", "                  \"type_man\",\n", "                  \"deck_A\",\n", "                 \"deck_B\",\n", "                 \"deck_C\",\n", "                 \"deck_D\",\n", "                 \"deck_E\",\n", "                 \"deck_F\",\n", "                 \"deck_G\",\n", "                 \"deck_U\"]\n", "\n", "#\u00a0We select here all the features above.\n", "features = train[features_names] \n", "#\u00a0The target is to predict the survived category\n", "target = train[\"Survived\"]\n", "# The tree is a decision tree classifier.\n", "my_tree = tree.DecisionTreeClassifier(max_depth = 10, min_samples_split = 5, random_state = 1)\n", "#\u00a0The tree is fitted to the train data.\n", "my_tree = my_tree.fit(features, target)\n", "\n", "#\u00a0Sklearn has a built in score that we can look at and from that score tune the tree parameters.\n", "#\u00a0Here we look at the score on the train data so beware of overfitting.\n", "print(\"Score of tree on train data: \", my_tree.score(features, target))\n", "\n", "#\u00a0Use the tree to evaluate its answer on the train data.\n", "train[\"eval_tree\"] = my_tree.predict(features)\n", "#\u00a0Look at the RMSE score on the train data.\n", "rmse_tree = np.sqrt(mean_squared_error(train[\"Survived\"], train[\"eval_tree\"]))\n", "print(\"RMSE:\", rmse_tree)"], "metadata": {"collapsed": true, "_cell_guid": "607286c7-bea0-4d66-85e4-3ff59150037d", "_uuid": "1059ed13522ef5ba8f9aaae8dcde34de3e3041d0"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["With this simple tree we have a way better model than the rich woman model (look at the RMSE score). Now we can look at the feature importance. That is to say, what are the most important features when deciding if one passenger will survive ?"], "metadata": {"_cell_guid": "52358bf3-9c97-474e-a536-147bfac918cf", "_uuid": "910b9d2baf3c42b1e393f86e3c0cd5cb525a0b60"}, "cell_type": "markdown"}, {"source": ["features_imp = pd.DataFrame()\n", "features_imp['feature'] = features_names\n", "features_imp['importance'] = my_tree.feature_importances_\n", "features_imp.sort_values(by=['importance'], ascending=True, inplace=True)\n", "features_imp.set_index('feature', inplace=True)\n", "features_imp.plot(kind='barh', figsize=(20, 20))"], "metadata": {"collapsed": true, "_cell_guid": "e5044a4d-0be3-476a-b0da-817ccd18fa1c", "_uuid": "61404fd44da218f769f2045150ee851f1c109d6c"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["So the most important features are if you are a man or not, how much you paid for your ticket and your family size. It is coherent with the first observations that we made.\n", "\n", "We can now use this tree to predict the survival outcome of the passengers in the test data, write the results in a csv and then submit it to kaggle."], "metadata": {"_cell_guid": "d9003702-b1a2-458b-ad04-85c6367b2f6c", "_uuid": "ce4692998bdf2a1308a524487d7a9e69b18ff7e6"}, "cell_type": "markdown"}, {"source": ["test[\"eval_tree\"] = my_tree.predict(test[features_names].values)\n", "\n", "write_results(test, \"eval_tree\")"], "metadata": {"collapsed": true, "_cell_guid": "6515a267-e488-4946-9ed0-b2a1bac6018d", "_uuid": "6aad7e2533689ac957d5ef59f9b927dd567036ef"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["##\u00a0Random forest\n", "\n", "Instead of getting a decision from one tree, it is possible to get the answer from a panel of trees, that is to say a forest. Each tree in the forest is different and is built around a sample of features. It is always better to get an answer from a diverse jury and this way overfitting is limited."], "metadata": {"_cell_guid": "148a414a-5c8f-4518-8884-b19df02bdff0", "_uuid": "a7f830d4f717a10711ce22f49803e03063cb739b"}, "cell_type": "markdown"}, {"source": ["#\u00a0The forest will have 50 trees \n", "#\u00a0and the max number of features by trees is the square root of the total features number\n", "my_forest = RandomForestClassifier(n_estimators=50, max_features='sqrt')\n", "my_forest = my_forest.fit(features, target)"], "metadata": {"collapsed": true, "_cell_guid": "e46ecd6b-2dbc-431a-9431-82e5054cabd3", "_uuid": "ed118ed2c571fe9843488bf1289540eed3dee0dc"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["Let's look if using a forest has an impact on the feature importance compared to a single tree."], "metadata": {"_cell_guid": "0503f9fa-92b8-4ae9-a5a3-ff83d00ebb41", "_uuid": "f35e2489b571472fee68ec07390b3f9a2263bc42"}, "cell_type": "markdown"}, {"source": ["features_imp = pd.DataFrame()\n", "features_imp['feature'] = features_names\n", "features_imp['importance'] = my_forest.feature_importances_\n", "features_imp.sort_values(by=['importance'], ascending=True, inplace=True)\n", "features_imp.set_index('feature', inplace=True)\n", "features_imp.plot(kind='barh', figsize=(20, 20))"], "metadata": {"collapsed": true, "_cell_guid": "276eb459-e285-4c6e-bce1-46122a236c80", "_uuid": "aee25f5cd448fd3944fa53a82b60578cfa0f3cd3"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["The feature importance is not exactly the same but it is close. The main difference is that the Sex is now in fourth position instead of being insignificant with a single tree."], "metadata": {"_cell_guid": "316ce8c7-d5d5-4318-b18e-6bc41de3e9e6", "_uuid": "b4f2f222d6c3e22296e56a0e6694c2c7aaf1a77d"}, "cell_type": "markdown"}, {"source": ["print(\"Score of forest on train data: \", my_forest.score(features, target))\n", "\n", "train[\"eval_forest\"] = my_forest.predict(features)\n", "\n", "rmse_tree = np.sqrt(mean_squared_error(train[\"Survived\"], train[\"eval_forest\"]))\n", "print(\"RMSE:\", rmse_tree)"], "metadata": {"collapsed": true, "_cell_guid": "2e58698b-b540-4fb4-8460-c0de2e6bacb5", "_uuid": "1cfae4467e7358030a60f57f34db92094fc99a5a"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["This random forest model is way better than the single tree model when comparing the rmse score on train data.\n", "We can also use it on the test data and write the csv for kaggle submission."], "metadata": {"_cell_guid": "ff7ec872-d72c-4997-8308-b17691977feb", "_uuid": "8fa385851e14f5c3602e8cd3d52cb28f94ec9abb"}, "cell_type": "markdown"}, {"source": ["test[\"eval_forest\"] = my_forest.predict(test[features_names].values)\n", "\n", "write_results(test, \"eval_forest\")"], "metadata": {"collapsed": true, "_cell_guid": "d1290bcc-79be-4a4b-9c98-1877a989e8a1", "_uuid": "377f318dba03c0428b58723a0c4ff2c9ef62c79d"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["**The kaggle score is 0.73684**"], "metadata": {"_cell_guid": "a9cae441-c38c-4390-bfdf-459d4500c10e", "_uuid": "3557ae5a0661627889e7b7ea77f78bf64bdec446"}, "cell_type": "markdown"}, {"source": ["## Tuned Random Forest\n", "\n", "Single trees and forest have a number of tuning parameters that have a big impact on model performance and overfitting.\n", "(see http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).\n", "\n", "One way to pick the best set of parameters is to brute force test them all with a parameter grid."], "metadata": {"_cell_guid": "0c037db4-0929-4bdb-905a-70a01fc7c30b", "_uuid": "7e3a7a35fadb991b2c9343721aa2c133575a16e5"}, "cell_type": "markdown"}, {"source": ["def tune_forest(features, targets):\n", "    \"\"\" Find the best parameters for the random forest \"\"\"\n", "    #parameter_grid = {\n", "    #             'max_depth' : [5, 6, 7],\n", "    #             'n_estimators': [20],\n", "    #             'max_features': ['sqrt', 'auto', 'log2'],\n", "    #             'min_samples_split': [2, 5, 10, 15],\n", "    #             'min_samples_leaf': [3, 10],\n", "    #             'bootstrap': [True, False],\n", "    #             }\n", "    parameter_grid = None\n", "    parameter_grid = {\n", "                 'max_depth' : [8, 10, 12],\n", "                 'n_estimators': [50, 10],\n", "                 'max_features': ['sqrt'],\n", "                 'min_samples_split': [2, 3, 10],\n", "                 'min_samples_leaf': [1, 3, 10],\n", "                 'bootstrap': [True, False],\n", "                 }\n", "    forest = RandomForestClassifier(n_jobs=2)\n", "\n", "    grid_search = GridSearchCV(forest,\n", "                               scoring='accuracy',\n", "                               param_grid=parameter_grid,\n", "                               cv=3,\n", "                               n_jobs=2,\n", "                               verbose=1)\n", "\n", "    grid_search.fit(features, targets)\n", "    model = grid_search.best_estimator_\n", "    parameters = grid_search.best_params_\n", "\n", "    print('Best score: {}'.format(grid_search.best_score_))\n", "    print('Best estimator: {}'.format(grid_search.best_estimator_))\n", "    \n", "    return model, parameters"], "metadata": {"collapsed": true, "_cell_guid": "a6e9c735-5ce1-44f2-ab4e-0956f4c6b55e", "_uuid": "eddcf73c2ab676efe62431fecebfbea4d360da6d"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["model, parameters = tune_forest(features, target)\n", "train[\"eval_tuned_forest\"] = model.predict(train[features_names].values)\n", "rmse_tuned_tree = np.sqrt(mean_squared_error(train[\"Survived\"], train[\"eval_tuned_forest\"]))\n", "print(\"RMSE:\", rmse_tuned_tree)"], "metadata": {"collapsed": true, "_cell_guid": "5e917f18-52a1-45ba-950a-1d12fb29822b", "_uuid": "e6a80f52f323364e063388d4b9e2a3df425c5ec2"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["test[\"eval_tuned_forest\"] = model.predict(test[features_names].values)\n", "\n", "write_results(test, \"eval_tuned_forest\")"], "metadata": {"collapsed": true, "_cell_guid": "1aefb556-ac9e-45e6-b457-7dc23ca337b9", "_uuid": "c440b0d5ee814330171c54aba30c88a2a15a7767"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["** The kaggle score is 0.80861. ** Even if the rmse score of this tuned random forest is worse than the previous random forest, the kaggle score is better. This is due to overfitting effects."], "metadata": {"_cell_guid": "02b93c7d-fa16-465b-bb48-fb53dd62d5b4", "_uuid": "4f962809156bb9b4371e3a2faace8665065d7583"}, "cell_type": "markdown"}, {"source": ["#\u00a0XGboost\n", "\n", "XGboost is a popular machine learning library when using decision trees.\n", "Work in progress"], "metadata": {"_cell_guid": "919d5cc6-3aab-4254-b810-29b7bf3f95c0", "_uuid": "cbeedea20a172f0000c95491a4b3cbc1cc7d09bd"}, "cell_type": "markdown"}, {"source": [], "metadata": {"collapsed": true, "_cell_guid": "4310dce8-6ebd-4d29-885c-6b944fb22fe1", "_uuid": "04e826518ae9bd709c1fe61b232df6fe86aca9ba"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["model = xgb.XGBClassifier()\n", "model.fit(features, target)\n", "\n", "print(\"Score of tree on train data: \", model.score(features, target))\n", "\n", "train[\"eval_xgb_tree\"] = model.predict(features)\n", "\n", "rmse_xgb_tree = np.sqrt(mean_squared_error(train[\"Survived\"], train[\"eval_xgb_tree\"]))\n", "print(\"RMSE:\", rmse_xgb_tree, \"1-RMSE:\", 1.0-rmse_xgb_tree)"], "metadata": {"collapsed": true, "_cell_guid": "23fb1c8f-6f80-4980-bcc1-7658de258186", "_uuid": "bf929db85dca8b68cec99c35ce2d1731a2ddeb67"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["features_imp = pd.DataFrame()\n", "features_imp['feature'] = features_names\n", "features_imp['importance'] = model.feature_importances_\n", "features_imp.sort_values(by=['importance'], ascending=True, inplace=True)\n", "features_imp.set_index('feature', inplace=True)\n", "features_imp.plot(kind='barh', figsize=(20, 20))"], "metadata": {"collapsed": true, "_cell_guid": "5ecc951f-cf3a-43d0-aeee-9d50db61db82", "_uuid": "ee0a43c145bf270aa5813c2c16632dae30e1f2d1"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["test[\"eval_xgb_tree\"] = model.predict(test[features_names])\n", "\n", "write_results(test, \"eval_xgb_tree\")"], "metadata": {"collapsed": true, "_cell_guid": "08977493-ffa3-4185-8d0a-7de1822c827d", "_uuid": "d271bb5dc579db4f11bd489c5cfef4e99c9a9b4a"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["def tune_xgb_tree(features, targets):\n", "    parameter_grid = {\n", "                 'max_depth' : [7, 8, 9],\n", "                 'max_delta_step': [1],\n", "                 'n_estimators': [20, 40, 60, 80],\n", "                 'colsample_bylevel': [0.8, 0.9, 1.0],\n", "                 'colsample_bytree': [0.6, 0.8, 1.0],\n", "                 'subsample': [0.3, 0.4, 0.5, 0.6],\n", "                 }\n", "    xgb_model = xgb.XGBClassifier()\n", "    print(xgb_model.get_params().keys())\n", "\n", "    grid_search = GridSearchCV(xgb_model,\n", "                               scoring='accuracy',\n", "                               param_grid=parameter_grid,\n", "                               cv=3,\n", "                               n_jobs=2,\n", "                               verbose=1)\n", "\n", "    grid_search.fit(features, targets)\n", "    model = grid_search.best_estimator_\n", "    parameters = grid_search.best_params_\n", "\n", "    print('Best score: {}'.format(grid_search.best_score_))\n", "    print('Best estimator: {}'.format(grid_search.best_estimator_))\n", "    \n", "    return model, parameters"], "metadata": {"collapsed": true, "_cell_guid": "95864c38-14cf-41c1-8e83-d548145f579d", "_uuid": "04e49b58a9ee56c7b1f1d0af7337919e6874e46c"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["model, parameters = tune_xgb_tree(features, target)"], "metadata": {"collapsed": true, "_cell_guid": "93f27b24-0b4e-4aeb-ace2-0e62aaa57e50", "_uuid": "a4a270a8377f0220d9e7bdd3e1bd5607e9f51cf9"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["train[\"eval_tuned_xgb_tree\"] = model.predict(train[features_names])\n", "rmse_tuned_tree = np.sqrt(mean_squared_error(train[\"Survived\"], train[\"eval_tuned_xgb_tree\"]))\n", "print(\"RMSE:\", rmse_tuned_tree, \"1-RMSE:\", 1.0-rmse_tuned_tree)"], "metadata": {"collapsed": true, "_cell_guid": "dce8d551-56d0-4d17-bd36-154d932b5143", "_uuid": "c2596fbe43c9767cda2daffcbdacc1b0c21b57d6"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["parameters"], "metadata": {"collapsed": true, "_cell_guid": "a42d2d05-882b-421c-8f4b-ed39de2f6afb", "_uuid": "d849e44be67a41ba675de394ab7dbb4a5d005481"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["features_imp = pd.DataFrame()\n", "features_imp['feature'] = features_names\n", "features_imp['importance'] = model.feature_importances_\n", "features_imp.sort_values(by=['importance'], ascending=True, inplace=True)\n", "features_imp.set_index('feature', inplace=True)\n", "features_imp.plot(kind='barh', figsize=(20, 20))"], "metadata": {"collapsed": true, "_cell_guid": "d9c4c15b-f1b8-4ea7-8e3e-407eebfca051", "_uuid": "fc1a8c11fb28247385167119428cc73efc689976"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["test[\"eval_tuned_xgb_tree\"] = model.predict(test[features_names])\n", "\n", "write_results(test, \"eval_tuned_xgb_tree\")"], "metadata": {"collapsed": true, "_cell_guid": "13ab09ea-199e-4909-9310-7e8e00acade1", "_uuid": "f3c33005c2f8d34ced89c2d5230a449d311404c4"}, "cell_type": "code", "outputs": [], "execution_count": null}, {"source": ["# Best score\n", "0.79904"], "metadata": {"_cell_guid": "45888746-09d8-4808-ae98-a59327aba83c", "_uuid": "5bb4399311503b393dbd2b863edd9f5e44c94559"}, "cell_type": "markdown"}, {"source": [], "metadata": {"collapsed": true, "_cell_guid": "32b12397-a6ee-44a0-a873-b540669f146f", "_uuid": "23f38d6e3f167e63cbdbeec0a3f1012bc92c3d6f"}, "cell_type": "code", "outputs": [], "execution_count": null}], "nbformat": 4, "metadata": {"language_info": {"nbconvert_exporter": "python", "version": "3.6.3", "pygments_lexer": "ipython3", "codemirror_mode": {"name": "ipython", "version": 3}, "name": "python", "file_extension": ".py", "mimetype": "text/x-python"}, "kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}}, "nbformat_minor": 1}