{"metadata": {"_change_revision": 0, "_is_fork": false, "language_info": {"codemirror_mode": {"version": 3, "name": "ipython"}, "name": "python", "file_extension": ".py", "version": "3.6.3", "mimetype": "text/x-python", "pygments_lexer": "ipython3", "nbconvert_exporter": "python"}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}, "nbformat_minor": 1, "cells": [{"metadata": {"_uuid": "53909837aebaac8a6f37fedead45aea12cac5f3e", "_cell_guid": "1ee6820e-3961-849c-e85b-166a482af606"}, "cell_type": "markdown", "source": ["# Titanic Competition from Kaggle\n", "\n", "The \"Titanic: Machine Learning from Disaster\" is a good data set to get started with hands-on Machine Learning. \n", "\n", "**Note:** This tutorial is just as a simple starting point for beginners. Many additional explorations and optimizations could be done to improve the accuracy of the model. \n", "This kernel is an extension of https://www.kaggle.com/rochellesilva/simple-tutorial-for-beginners\n", "\n", "\n", "Here are the basic steps of a Data Science Pipeline:\n", "\n", "#### 1. Data Exploration and Visualization  \n", "   - Explore the dataset\n", "   - Choose important features and visualize them based on survival labels\n", "   \n", "#### 2. Data Cleaning, Feature Engineering and Feature Selection\n", "   - Handling null/empty values\n", "   - Encode categorical data\n", "   - Transform features\n", "   \n", "#### 3. Test different Classifier Models\n", "   - Logistic Regression (LR)\n", "   - K-NN\n", "   - Support Vector Machines (SVM)\n", "   - Naive Bayes\n", "   - Random Forest (RF)\n"]}, {"metadata": {"_uuid": "42fb0e0114375abc705d9d58facfb6f1a023697d", "_cell_guid": "e077f539-ad91-7277-41d9-1f1246092189"}, "cell_type": "markdown", "source": ["First let's start by importing the essential libraries that we need:\n", "1)\t**pandas** for Data Frames\n", "2)\t**numpy** for Arrays and Matrices\n", "3)\t**matplotlib.pyplot** for Visualization\n"]}, {"metadata": {"collapsed": true, "_uuid": "6a9826d13dd4aa970ae450fe503d2e2f95ab2ef5", "_execution_state": "idle", "_cell_guid": "2dfc55af-0bc1-cae3-14b1-b16ccbaa9a16"}, "execution_count": null, "cell_type": "code", "source": ["# Importing the libraries\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import pandas as pd\n", "\n", "import warnings\n", "warnings.filterwarnings('ignore')"], "outputs": []}, {"metadata": {"_uuid": "a06d3d8e3e9533c1262ce31a3a07e31db77db975", "_cell_guid": "8c4c9f57-5fb2-4037-6b92-ff3ffa6f87c8"}, "cell_type": "markdown", "source": ["Now let's import the csv file with the training dataset. You can download it from [here](https://www.kaggle.com/c/titanic/data).  The explanation of the features (each column from the dataset) is also presented in this link. "]}, {"metadata": {"collapsed": true, "_uuid": "ce504b943fc7571318e50a97aa6f6e472f7aeca9", "_execution_state": "idle", "_cell_guid": "a2bacde5-f0d4-566c-ffdc-512c64ddb9f5"}, "execution_count": null, "cell_type": "code", "source": ["dataset= pd.read_csv(\"../input/train.csv\")"], "outputs": []}, {"metadata": {"_uuid": "d4367cbb31d53b806c32e1441322dec01b8a14a0", "_cell_guid": "798fb3c7-7e83-1410-846e-89d1c673c233"}, "cell_type": "markdown", "source": ["## 1. Data Exploration and Visualization  \n", "\n", "Let\u2019s explore the dataset to identify features that could be useful to predict the survival rate. The features that probably may have an influence are: the **\"P-class\"** (expect to see more survival for higher class), the **\"Sex\"** and **\"Age\"** (\"women and children first\"), and **\"Embarked\"**(port of Embarkation)\n", "\n", "We will now plot some graphs to confirm if these features show some relation with the survival rate. These plots were based in the graphs presented [here](http://ahmedbesbes.com/how-to-score-08134-in-titanic-kaggle-challenge.html).  "]}, {"metadata": {"collapsed": true, "_uuid": "4b6f1c11e2913c2864b1a3f85ebeb14b98c53b32", "_execution_state": "idle", "_cell_guid": "a51d93fc-1e64-f21e-0890-0554ed365f2e"}, "execution_count": null, "cell_type": "code", "source": ["%matplotlib inline\n", "import seaborn\n", "seaborn.set() \n", "\n", "#-------------------Survived/Died by Class -------------------------------------\n", "survived_class = dataset[dataset['Survived']==1]['Pclass'].value_counts()\n", "dead_class = dataset[dataset['Survived']==0]['Pclass'].value_counts()\n", "df_class = pd.DataFrame([survived_class,dead_class])\n", "df_class.index = ['Survived','Died']\n", "df_class.plot(kind='bar',stacked=True, figsize=(5,3), title=\"Survived/Died by Class\")\n", "\n", "Class1_survived= df_class.iloc[0,0]/df_class.iloc[:,0].sum()*100\n", "Class2_survived = df_class.iloc[0,1]/df_class.iloc[:,1].sum()*100\n", "Class3_survived = df_class.iloc[0,2]/df_class.iloc[:,2].sum()*100\n", "print(\"Percentage of Class 1 that survived:\" ,round(Class1_survived),\"%\")\n", "print(\"Percentage of Class 2 that survived:\" ,round(Class2_survived), \"%\")\n", "print(\"Percentage of Class 3 that survived:\" ,round(Class3_survived), \"%\")\n", "\n", "# display table\n", "from IPython.display import display\n", "display(df_class)\n"], "outputs": []}, {"metadata": {"collapsed": true, "_uuid": "d49216dfe8a4bb07978b3a74bf8446cdceddde0b", "_execution_state": "idle", "_cell_guid": "cea9edcb-cedc-2d41-07b0-5dcc86ddf6ef"}, "execution_count": null, "cell_type": "code", "source": ["#-------------------Survived/Died by SEX------------------------------------\n", "   \n", "Survived = dataset[dataset.Survived == 1]['Sex'].value_counts()\n", "Died = dataset[dataset.Survived == 0]['Sex'].value_counts()\n", "df_sex = pd.DataFrame([Survived , Died])\n", "df_sex.index = ['Survived','Died']\n", "df_sex.plot(kind='bar',stacked=True, figsize=(5,3), title=\"Survived/Died by Sex\")\n", "\n", "\n", "female_survived= df_sex.female[0]/df_sex.female.sum()*100\n", "male_survived = df_sex.male[0]/df_sex.male.sum()*100\n", "print(\"Percentage of female that survived:\" ,round(female_survived), \"%\")\n", "print(\"Percentage of male that survived:\" ,round(male_survived), \"%\")\n", "\n", "# display table\n", "from IPython.display import display\n", "display(df_sex) "], "outputs": []}, {"metadata": {"collapsed": true, "_uuid": "6738ed7bb47afb50601321ed0cbcaee615663652", "_execution_state": "idle", "_cell_guid": "d6b077b8-4d61-e44a-84cd-d03cb9b3d9f6"}, "execution_count": null, "cell_type": "code", "source": ["#-------------------- Survived/Died by Embarked ----------------------------\n", "\n", "survived_embark = dataset[dataset['Survived']==1]['Embarked'].value_counts()\n", "dead_embark = dataset[dataset['Survived']==0]['Embarked'].value_counts()\n", "df_embark = pd.DataFrame([survived_embark,dead_embark])\n", "df_embark.index = ['Survived','Died']\n", "df_embark.plot(kind='bar',stacked=True, figsize=(5,3))\n", "\n", "Embark_S= df_embark.iloc[0,0]/df_embark.iloc[:,0].sum()*100\n", "Embark_C = df_embark.iloc[0,1]/df_embark.iloc[:,1].sum()*100\n", "Embark_Q = df_embark.iloc[0,2]/df_embark.iloc[:,2].sum()*100\n", "print(\"Percentage of Embark S that survived:\", round(Embark_S), \"%\")\n", "print(\"Percentage of Embark C that survived:\" ,round(Embark_C), \"%\")\n", "print(\"Percentage of Embark Q that survived:\" ,round(Embark_Q), \"%\")\n", "\n", "from IPython.display import display\n", "display(df_embark)"], "outputs": []}, {"metadata": {"_uuid": "51871309ed568bf789eca8b735c0107c3bd01719", "_cell_guid": "2dc5765e-a510-dd4c-7ba1-0db6b19877bb"}, "cell_type": "markdown", "source": ["## 2. Data Cleaning, Feature Selection and Feature Engineering\n", "The preprocessing of the data is a quite crucial part. If we just give the dataset without cleaning it, most probably the results will not be good! So, in this step we will preprocess the training dataset and this will involve feature selection, data cleaning, and feature engineering.   \n", "\n", "I will start with feature selection. As we saw previously, **\"P-Class\", \"Sex\", \"Age\"** and **\"Embarked\"** showed some relation with Survived rate. Thus, I will drop the remaining features, except **\"Name\"** because it will be useful in a further step of the cleaning process. "]}, {"metadata": {"collapsed": true, "_uuid": "4de4e75241eac9df30de7bd05392caba9764e6f3", "_execution_state": "idle", "_cell_guid": "1c3b2fc8-7692-475d-8068-0942b926967b"}, "execution_count": null, "cell_type": "code", "source": ["def dropfeatures(dataset):\n", "    X = dataset.drop(['PassengerId','Cabin','Ticket','Fare', 'Parch', 'SibSp'], axis=1)\n", "    return X"], "outputs": []}, {"metadata": {"collapsed": true, "_uuid": "e626b4f729540c98a6ad86f9c28be2d3fc92f8a7", "_execution_state": "idle", "_cell_guid": "387d5d13-49f2-1d66-38c4-34e1df20c121"}, "execution_count": null, "cell_type": "code", "source": ["X = dropfeatures(dataset)\n", "y = X.Survived                       # vector of labels (dependent variable)\n", "X=X.drop(['Survived'], axis=1)       # remove the dependent variable from the dataframe X\n", "\n", "X.head(20)"], "outputs": []}, {"metadata": {"_uuid": "7e0deab2aae7507215a519419688f6d442c722e5", "_cell_guid": "bc7fbaba-9357-332d-2621-beb5237931b3"}, "cell_type": "markdown", "source": ["We can see, from this displayed DataFrame, that **\"Sex\"** and **\"Embarked\"** are categorical features and have strings instead of numeric values. We need to encode these strings into numeric data, so the algorithm can perform its calculations. \n", "\n", "For the **\"Sex\"** feature we can use the **LabelEncoder** class from  **sklearn.preprocessing** library. \n", "Another way is by of doing this is by using the **get_dummies** from **pandas**. We will be using this to encode the **\"Embarked\"** feature. \n", "\n", "But first, as **\"Embarked\"** has two NaN values we need to take care of these missing values. In this approach, we can fill the NaN values with 'S' category because it is the most frequent in the data. After fixing the NaN values, we can now use the **get_dummies** and get three new columns (Embarked_C,\tEmbarked_Q, Embarked_S) which are called dummy variables (they assign \u20180\u2019 and \u20181\u2019 to indicate membership in a category). The previous **\"Embarked\"** can be dropped from X as it will not be needed anymore and we can now concatenate the X dataframe with the new **\"Embarked\"** which has the three dummy variables. \n", "\n", "Finally, as the number of dummy variables necessary to represent a single feature is equal to the number of categories in that feature minus one, we can remove one of the dummies created, lets say Embarked_S, for example. This will not remove any information because by having the values from Embarked_C and\tEmbarked_Q the algorithm can easily understand the values from the remaining dummy variable (when Embarked_C and Embarked_Q are '0' Embarked_S will be '1', otherwise it will be '0').  "]}, {"metadata": {"collapsed": true, "_uuid": "bcf70809790192ae239e8b1a2040860c93b52435", "_execution_state": "idle", "_cell_guid": "07df4aa0-b658-32e7-e781-2c26a2dd187b"}, "execution_count": null, "cell_type": "code", "source": ["# ----------------- Encoding categorical data -------------------------\n", "\n", "# encode \"Sex\"\n", "def encodeSex_Embark(X):\n", "    from sklearn.preprocessing import LabelEncoder\n", "    labelEncoder_X = LabelEncoder()\n", "    X.Sex=labelEncoder_X.fit_transform(X.Sex)\n", "\n", "    # encode \"Embarked\"\n", "    # number of null values in embarked:\n", "    print ('Number of null values in Embarked:', sum(X.Embarked.isnull()))\n", "\n", "    # fill the two values with one of the options (S, C or Q)\n", "    row_index = X.Embarked.isnull()\n", "    X.loc[row_index,'Embarked']='S' \n", "\n", "    Embarked  = pd.get_dummies(X.Embarked , prefix='Embarked'  )\n", "    X = X.drop(['Embarked'], axis=1)\n", "    X = pd.concat([X, Embarked], axis=1)  \n", "    # we should drop one of the columns\n", "    X = X.drop(['Embarked_S'], axis=1)\n", "    X.head()\n", "    return X"], "outputs": []}, {"metadata": {"collapsed": true, "_uuid": "48c5113f411a6419e86c36154a302a0885889f5c", "_execution_state": "idle", "_cell_guid": "bfa4c6ec-e937-4ba2-bed0-3f374d414378"}, "execution_count": null, "cell_type": "code", "source": ["X = encodeSex_Embark(X)\n", "print(X)"], "outputs": []}, {"metadata": {"_uuid": "e2beec4c15a6f417882dbb814975ad7b7dbe55d5", "_cell_guid": "6b654220-d347-88f2-3c2e-8159aa9cd887"}, "cell_type": "markdown", "source": ["You may wonder why are we still keeping the **\"Name\"** column. In fact the name does not seem to have influence, it does not matter if a person is named Owen or William, however this column has the title located after the Surname and the comma (\"Mr\", \"Mrs\", \"Miss\", etc.) which can be useful.  \n", "\n", "If we take a look at the table X displayed previously we can see many missing values for the **\"Age\"** column. Removing these rows with missing values would involve removing 177 rows (which is quite a lot!) and we would have less information to create the model. In some cases, it is acceptable to take the average of the column and replace the null values, nonetheless in this case, it is possible to estimate the age of the person by their title, present in the **\"Name\"** column.   \n", "\n", "Therefore, we will first identify the different titles presented and then average the Age for each title. We can provide this averaged Age found for each title to the people with missing Age values, accordingly to their title in **\"Name\"**. \n", "\n", "After using the information in **\"Name\"** we can drop this column. "]}, {"metadata": {"collapsed": true, "_uuid": "8c6c7cbe83daf5ccd0ca2bce12afb070b306dca3", "_execution_state": "idle", "_cell_guid": "fdbf5bc4-1daf-2f04-4480-6889f0dd5994"}, "execution_count": null, "cell_type": "code", "source": ["#-------------- Taking care of missing data  -----------------------------\n", "def imputeAge(X):\n", "\n", "    print ('Number of null values in Age:', sum(X.Age.isnull()))\n", "\n", "\n", "    # -------- Change Name -> Title ----------------------------\n", "    got= X.Name.str.split(',').str[1]\n", "    X.iloc[:,1]=pd.DataFrame(got).Name.str.split('\\s+').str[1]\n", "    # ---------------------------------------------------------- \n", "\n", "\n", "    #------------------ Average Age per title -------------------------------------------------------------\n", "    ax = plt.subplot()\n", "    ax.set_ylabel('Average age')\n", "    X.groupby('Name').mean()['Age'].plot(kind='bar',figsize=(13,8), ax = ax)\n", "\n", "    title_mean_age=[]\n", "    title_mean_age.append(list(set(X.Name)))  #set for unique values of the title, and transform into list\n", "    title_mean_age.append(X.groupby('Name').Age.mean())\n", "    title_mean_age\n", "    #------------------------------------------------------------------------------------------------------\n", "\n", "\n", "    #------------------ Fill the missing Ages ---------------------------\n", "    n_traning= X.shape[0]   #number of rows\n", "    n_titles= len(title_mean_age[1])\n", "    for i in range(0, n_traning):\n", "        if np.isnan(X.Age[i])==True:\n", "            for j in range(0, n_titles):\n", "                if X.Name[i] == title_mean_age[0][j]:\n", "                    X.Age[i] = title_mean_age[1][j]\n", "    #--------------------------------------------------------------------    \n", "\n", "    X=X.drop(['Name'], axis=1)\n", "    return X\n", "\n", "       "], "outputs": []}, {"metadata": {"collapsed": true, "_uuid": "3a52435dc05a5611d59e008693ca6954cb2c23aa", "_execution_state": "idle", "_cell_guid": "daf11cbd-8b26-4026-96b9-c6dcb2710bae"}, "execution_count": null, "cell_type": "code", "source": ["X = imputeAge(X)\n", "print(X)"], "outputs": []}, {"metadata": {"_uuid": "adbff49e855028ec6e38ed1f1e3344b22ab4357a", "_cell_guid": "ba200ead-be7b-a2d8-a952-a86784bb2fb4"}, "cell_type": "markdown", "source": ["We can also make feature transformation. For example, we could transform the **\"Age\"** feature in order to simplify it. We could distinguish the youngsters (age less than 18 years) from the adults.  \n", "\n"]}, {"metadata": {"collapsed": true, "_uuid": "32f5cafd3b4988165ecc8c6dba5e618a02fb3fc1", "_execution_state": "idle", "_cell_guid": "51107085-0fcc-d28f-8de8-fa7e2b2379c6"}, "execution_count": null, "cell_type": "code", "source": ["def classifyAge(X):       \n", "    for i in range(0, X.shape[0]):\n", "        if X.Age[i] > 18:\n", "            X.Age[i]= 0\n", "        else:\n", "            X.Age[i]= 1\n", "\n", "    X.head()\n", "    return X"], "outputs": []}, {"metadata": {"collapsed": true, "_uuid": "6d210811882b0bcf5b57f108e7d474ae9d0058d4", "_execution_state": "idle", "_cell_guid": "9de2113e-bfc7-4da3-8680-1d3b7d866e0d"}, "execution_count": null, "cell_type": "code", "source": ["X = classifyAge(X)\n", "print(X)"], "outputs": []}, {"metadata": {"_uuid": "91edb2a4b8563ec89b1b5a861e296c9915ebec8e", "_cell_guid": "40369ea6-f78a-a8b6-3391-6e0c79de4c07"}, "cell_type": "markdown", "source": ["Now, we can say that we have a clean dataset to provide to our classifier algorithm. \n", "\n", "\n", "## 3. Test different Classifier Models\n"]}, {"metadata": {"_uuid": "b35f2988cc1ff5a70a4050ee4e5835ff1b514073", "_cell_guid": "5209a5b3-cdbe-911e-5c71-9a77594f8859"}, "cell_type": "markdown", "source": ["Now that we have our data preprocessed, we can provide the data to different classifiers and see which one performs better in creating a model of classification for this data. \n", "\n", "We will use cross validation, which is a model validation technique to evaluate how well a model will generalize on unseen data. Python has the **cross_val_score** class from **sklearn.model_selection** library to perform cross validation. "]}, {"metadata": {"collapsed": true, "_uuid": "3f3f7b7d3ab2cf2f0493cc3d44deb99283e25bff", "_execution_state": "idle", "_cell_guid": "e0a7684c-c3a2-f223-7e88-278d1b8fef65"}, "execution_count": null, "cell_type": "code", "source": ["#-----------------------Logistic Regression---------------------------------------------\n", "# Fitting Logistic Regression to the Training set\n", "from sklearn.linear_model import LogisticRegression\n", "lr_classifier = LogisticRegression(penalty='l2',random_state = 0)\n", "\n", "# Applying k-Fold Cross Validation\n", "from sklearn.model_selection import cross_val_score\n", "accuracies = cross_val_score(estimator = lr_classifier, X=X , y=y , cv = 10)\n", "print(\"Logistic Regression:\\n Accuracy:\", accuracies.mean(), \"+/-\", accuracies.std(),\"\\n\")\n", "\n", "\n", "#-----------------------------------K-NN --------------------------------------------------\n", "# Fitting K-NN to the Training set\n", "from sklearn.neighbors import KNeighborsClassifier\n", "classifier = KNeighborsClassifier(n_neighbors = 9, metric = 'minkowski', p = 2)\n", "\n", "\n", "# Applying k-Fold Cross Validation\n", "from sklearn.model_selection import cross_val_score\n", "accuracies = cross_val_score(estimator = classifier, X=X , y=y , cv = 10)\n", "print(\"K-NN:\\n Accuracy:\", accuracies.mean(), \"+/-\", accuracies.std(),\"\\n\")\n", "\n", "\n", "#---------------------------------------SVM -------------------------------------------------\n", "\n", "# Fitting Kernel SVM to the Training set\n", "from sklearn.svm import SVC\n", "svm_classifier = SVC(kernel = 'rbf', random_state = 0)\n", "\n", "# Applying k-Fold Cross Validation\n", "from sklearn.model_selection import cross_val_score\n", "accuracies = cross_val_score(estimator = svm_classifier, X=X , y=y , cv = 10)\n", "print(\"SVM:\\n Accuracy:\", accuracies.mean(), \"+/-\", accuracies.std(),\"\\n\")\n", "\n", "\n", "#---------------------------------Naive Bayes-------------------------------------------\n", "\n", "# Fitting Naive Bayes to the Training set\n", "from sklearn.naive_bayes import GaussianNB\n", "nb_classifier = GaussianNB()\n", "\n", "# Applying k-Fold Cross Validation\n", "from sklearn.model_selection import cross_val_score\n", "accuracies = cross_val_score(estimator = nb_classifier, X=X , y=y , cv = 10)\n", "print(\"Naive Bayes:\\n Accuracy:\", accuracies.mean(), \"+/-\", accuracies.std(),\"\\n\")\n", "\n", "\n", "\n", "#----------------------------Random Forest------------------------------------------\n", "\n", "# Fitting Random Forest Classification to the Training set\n", "from sklearn.ensemble import RandomForestClassifier\n", "rnd_classifier = RandomForestClassifier(n_estimators = 100, criterion = 'entropy', random_state = 0)\n", "rnd_classifier.fit(X,y)\n", "\n", "# Applying k-Fold Cross Validation\n", "from sklearn.model_selection import cross_val_score\n", "accuracies = cross_val_score(estimator = rnd_classifier, X=X , y=y , cv = 10)\n", "print(\"Random Forest:\\n Accuracy:\", accuracies.mean(), \"+/-\", accuracies.std())\n", "\n"], "outputs": []}, {"metadata": {"_uuid": "89165a712f35a00d4745e0a840286f70b1721f36", "_execution_state": "idle", "_cell_guid": "20a69556-9f14-4e94-b87a-4232535d1452"}, "cell_type": "markdown", "source": ["As we can see, from all the 5 classifiers tested in this tutorial, **Random Forest** got better results. \n", "\n", "After changing the test set by performing the same transformations done in the training set we can then use the **Random Forest** model created and do the predictions.  "]}, {"metadata": {"collapsed": true, "_uuid": "cc067a9e321b2de6cf7c52e62e0a916026018e4f", "_execution_state": "idle", "_cell_guid": "8a4c3f2f-a0c8-41ff-a778-b372eaba3bf6"}, "execution_count": null, "cell_type": "code", "source": ["X_test= pd.read_csv(\"../input/test.csv\")\n", "passengerId = X_test[\"PassengerId\"]\n", "X_test = dropfeatures(X_test)\n", "X_test = encodeSex_Embark(X_test)\n", "X_test = imputeAge(X_test)\n", "X_test = classifyAge(X_test)\n", "print('Generating Predictions...')\n", "y_pred = rnd_classifier.predict(X_test)\n"], "outputs": []}, {"metadata": {"collapsed": true, "_uuid": "03ab19046304a6dd4861b82a31d801c6d17b9ba0", "_execution_state": "idle", "_cell_guid": "02b1ef77-73e6-470a-ab3b-29cfd34e169e"}, "execution_count": null, "cell_type": "code", "source": ["print('Creating Submission File...')\n", "y_pred = [str(int(x)) for x in y_pred]\n", "submission = pd.DataFrame()\n", "submission['PassengerId'] = passengerId\n", "submission['Survived'] = y_pred\n", "print(submission['Survived'].value_counts())\n", "submission.set_index('PassengerId', inplace=True)\n", "submission.to_csv('titanic_submission.csv')\n", "print(submission)\n", "print('Done...')\n"], "outputs": []}, {"metadata": {"_uuid": "5d068bae42a73e8996e6278d39d957c2f32ff8a9", "_cell_guid": "d0957229-fb11-4b3c-ac59-bc33cd037bb9"}, "cell_type": "markdown", "source": ["This submission scored an accuracy of 0.77033."]}], "nbformat": 4}