{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":18,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Import the datasets\n\ntest_d = pd.read_csv(\"../input/test.csv\")\ntrain_d= pd.read_csv(\"../input/train.csv\")\n\n# Analyze the Datasets\n\ntrain_d.describe()\ntrain_d.info()\n\n# Therefore, there are missing values in Age, Cabin and Embarked in the train dataset\n\ntest_d.describe()\ntest_d.info()\n\n#Missing values in Age, Fare , Cabin","execution_count":19,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e256128ee1ab889d459447c8fa37c78e309d1483"},"cell_type":"code","source":"# 1) Compare Survived with different numerical data\n\n# Comparing Pclass with Survived \n\ntrain_d[['Pclass','Survived']].groupby(['Pclass'],as_index=False).mean().sort_values(by='Survived',ascending=False)\n","execution_count":20,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0e31330ae2594c95262cfc7adaf9539e243addbe"},"cell_type":"code","source":"# Comparing Sex with Survived \n\ntrain_d[['Sex','Survived']].groupby(['Sex'],as_index=False).mean().sort_values(by='Survived',ascending=False)\n\n","execution_count":21,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7098de640fa53b925772c6e4fe20589f01d7e0af"},"cell_type":"code","source":"# Comparing SibSp with Survived\n\ntrain_d[['SibSp','Survived']].groupby(['SibSp'],as_index=False).mean().sort_values(by='Survived',ascending=False)\n\n","execution_count":22,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"26f8fc36c37af77e4f3721d207c8e6f3f7f41043"},"cell_type":"code","source":"# Comparing Parch with Survived\n\ntrain_d[['Parch','Survived']].groupby(['Parch'],as_index=False).mean().sort_values(by='Survived',ascending=False)\n","execution_count":23,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"45ac75d8846310c9cc05970347e25c62b0595b82"},"cell_type":"code","source":"# Visualizing\n\n# Survived with Age\ng = sns.FacetGrid(train_d, col = 'Survived')\ng.map(plt.hist,'Age',bins=20)\n\n# PClass with Age (Survived = 0 and 1)\n\ng=sns.FacetGrid(train_d,col='Survived',row = 'Pclass')\ng.map(plt.hist,'Age',bins=20)\n\n# Sex and Survived\n\ng = sns.FacetGrid(train_d,col='Survived')\ng.map(plt.hist,'Sex',bins=10)\n\n\n# Pclass, Sex and Embarked with Survived\n\ng=sns.FacetGrid(train_d,row='Embarked')\ng.map(sns.pointplot,'Pclass','Survived','Sex')\ng.add_legend()\n\n# Fare and Embarked with Survived\n\ng=sns.FacetGrid(train_d,col='Survived',row='Embarked')\ng.map(sns.barplot,'Sex','Fare')\n\n\n\"\"\"CONCLUSIONS - \n 1) Pclass has direct correlation with Survived. Class 1 has highest survival rate followed by 2 followed by 1\n 2) Sex has direct correlation with Survival rate. Female > Male.\n 3) SibSp and Parch do not have a direct correlation, but we should make a new feature containing entir family size.\n 4) Age and Survival rate have a correlation. Least number of deaths among children and old people. Should make age bands.\n 5) Either embarked and survival has a correlation or embarked and Pclass. To determine what correlation embarked follows, we compare with Pclass and fare.\n 6) High paying passengers are more likely to survive.Fare Bands must be made. Also, Embarked has a direct correlation with Survival.\n 7) Embarked will have passengers of diff classes and genders. The location of the cabin etc. makes the correlation  \n\"\"\"\n\"\"\"\nWRANGLE DATA\n\n\"\"\"","execution_count":24,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"c7b2a09d0cec2b78f79be116e7dff1665b585d4e"},"cell_type":"code","source":"\"\"\" Drop the Ticket, PassengerID,Cabin columns\"\"\"\n\ntrain_d=train_d.drop(['Ticket','PassengerId','Cabin'],axis=1 )\ntest_d=test_d.drop(['Ticket','Cabin'],axis=1)\n\n\"\"\" Combine SibSp and Parch to form FamilySize \"\"\"\n\ntrain_d['FamilySize']=train_d['Parch']+train_d['SibSp']+1\ntest_d['FamilySize']=test_d['Parch']+test_d['SibSp']+1\n\n\"\"\"Drop Parch and SibSp\"\"\"\n\ntrain_d= train_d.drop(['Parch','SibSp'],axis=1)\ntest_d=test_d.drop(['Parch','SibSp'],axis=1)\n\n\"\"\"Filling Missing Values \"\"\"\ntrain_d.info()  # Age and Embarked Values Missing\ntest_d.info()   # Age and Fare value missing\n\n\"\"\"FILLING MISSING VALUES FOR EMBARKED\"\"\"\nmode_emb = train_d.Embarked.dropna().mode()[0]\ntrain_d['Embarked']=train_d['Embarked'].fillna(mode_emb)\n\n\n\"\"\" Label Encode Sex and Embarked\"\"\"\n\nfrom sklearn.preprocessing import LabelEncoder\nle_tr = LabelEncoder()\nle1_tr= LabelEncoder()\nle_test = LabelEncoder()\nle1_test= LabelEncoder()\ntrain_d['Sex']=le_tr.fit_transform(train_d['Sex'])\ntest_d['Sex']=le_test.fit_transform(test_d['Sex'])\ntrain_d['Embarked']=le1_tr.fit_transform(train_d['Embarked'])\ntest_d['Embarked']=le1_test.fit_transform(test_d['Embarked'])\n\n\"\"\"Extracting title from names\"\"\"\ntrain_d['Title']=np.nan\nfor i in range(0,891):\n    train_d['Title'][i]=(train_d['Name'][i].split(\",\")[1]).split(\".\")[0]\n\ntest_d['Title']=np.nan\nfor i in range(0,len(test_d)):\n    test_d['Title'][i]=(test_d['Name'][i].split(\",\")[1]).split(\".\")[0]\n\n#drop Name\n\ntrain_d=train_d.drop('Name',axis=1)\ntest_d=test_d.drop('Name',axis=1)\n\n\ntrain_d['Title']=train_d['Title'].replace([' Col',' Major',' Sir',' Don',' the Countess',' Jonkheer',' Capt'],' Rare')\ntrain_d['Title']=train_d['Title'].replace([' Mlle',' Ms',' Mme',' Lady'],' Mrs')\ntrain_d['Title'].value_counts()\n\ntest_d['Title']=test_d['Title'].replace([' Col',' Major',' Sir',' Don',' the Countess',' Jonkheer',' Capt',' Dona'],' Rare')\ntest_d['Title']=test_d['Title'].replace([' Mlle',' Ms',' Mme',' Lady'],' Mrs')\n\ntest_d['Title'].value_counts()\n\n# Label Encoding Title\nle_title=LabelEncoder()\ntrain_d[\"Title\"]=le_title.fit_transform(train_d[\"Title\"])\ntest_d['Title']=le_title.transform(test_d['Title'])\n\n\n# Filling fare values in test dataset- \n\ntest_d.info()\ntest_d[test_d['Fare'].isnull()].index.tolist()\n\n#Find avg of Fare of PClass = 3 \n\ntest_d['Fare'][152]=(test_d[test_d['Pclass']==3]['Fare'].mean())\ntest_d.loc[152,:]\n\ntest_d.info()\ntrain_d.info()\n\n#Fill missing age values based on name \ncombine = [train_d,test_d]\ntitles = train_d['Title'].unique()\nfor dataset in combine:\n    for t in titles:\n        dataset['Age']=dataset['Age'].fillna(dataset[dataset['Title']==t]['Age'].mean()).astype(int)\n    \ntrain_d.info()\ntest_d.info()\n\n#All values are filled. Let's create bands for age and Fare\n\ntrain_d.describe()\ntrain_d['AgeBand']=pd.cut(train_d['Age'],5)\nfor dataset in combine:    \n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age']\n    \n#Drop AgeBand\ntrain_d=train_d.drop('AgeBand',axis=1)\ncombine=[train_d,test_d]\n\n#Create fare bands\ntrain_d['FareBands']=pd.qcut(train_d['Fare'],4)\ntrain_d[['FareBands','Survived']].groupby(['FareBands'],as_index=False).mean().sort_values(by='FareBands')  \ntrain_d['FareBands'].value_counts()\nfor dataset in combine:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n    \n","execution_count":29,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ac6e0e47693c7ac35c246a1b29b87d63371e9f4e"},"cell_type":"code","source":"train_d=train_d.drop('FareBands',axis=1)\ntrain_d.describe()\n\n\"\"\"\nThis completes the Cleaning of our data. Now we'll model it and find the best fitting model\"\"\"\n","execution_count":28,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"35712fe0b942b4acd58de6def18736d3db6d40e4"},"cell_type":"code","source":"x_train = train_d.drop('Survived',axis=1)\ny_train= train_d['Survived']\nx_test= test_d.drop(\"PassengerId\",axis=1).copy()\n\n\"\"\" Since it is a Classification and Regression problem (Supervised Learning), we import the relevant libraries\"\"\"\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nscorevector=[]\n\n\"\"\"LOGISTIC REGRESSION\"\"\"\nlr = LogisticRegression()\nlr.fit(x_train,y_train)\ny_pred= lr.predict(x_test)\nacc_log = round(lr.score(x_train, y_train) * 100, 2)\nscorevector.append(acc_log)\n\n\"\"\"SUPPPORT VECTOR CLASSIFICATION\"\"\"\nsv = SVC()\nsv.fit(x_train,y_train)\ny_pred=sv.predict(x_test)\nacc_log = round(sv.score(x_train, y_train) * 100, 2)\nscorevector.append(acc_log)\n\n\"\"\"GAUSSIAN NAIVE BAYES'\"\"\"\ngnb = GaussianNB()\ngnb.fit(x_train,y_train)\ny_pred= gnb.predict(x_test)\nacc_log = round(gnb.score(x_train, y_train) * 100, 2)\nscorevector.append(acc_log)\n\n\"\"\"RANDOM FOREST CLASSIFICATION\"\"\"\nrfc = RandomForestClassifier(n_estimators=200,random_state=0)\nrfc.fit(x_train,y_train)\ny_pred= rfc.predict(x_test)\nacc_log = round(rfc.score(x_train, y_train) * 100, 2)\nscorevector.append(acc_log)\n\n\"\"\"K-NEIGHBORS CLASSIFICATION\"\"\"\nknc = KNeighborsClassifier(n_neighbors=1)\nknc.fit(x_train,y_train)\ny_pred= knc.predict(x_test)\nacc_log = round(knc.score(x_train, y_train) * 100, 2)\nscorevector.append(acc_log)\n\n\"\"\"DECISION TREE CLASSIFIER\"\"\"\ndtc = DecisionTreeClassifier(criterion='entropy')\ndtc.fit(x_train,y_train)\ny_pred= dtc.predict(x_test)\nacc_log = round(dtc.score(x_train, y_train) * 100, 2)\nscorevector.append(acc_log)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"f68e36034d71296fe756d98a445e21cc5ab56e9e"},"cell_type":"code","source":"modelnames = pd.Series(['LogisticRegression','SVC','GaussianNB','RandomForestClassifier','KNeighborsClassifier','DecisionTreeClassifier'],dtype = object)\nmodels = pd.DataFrame({'Model' : modelnames, 'Score':scorevector})\nmodels.sort_values(by='Score',ascending=False)\n\nsubmission = pd.DataFrame({\"PassengerID\" : test_d[\"PassengerId\"], \"Survived\" : y_pred})\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}