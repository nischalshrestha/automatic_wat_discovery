{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "c97648da-ca53-8915-43ac-9c56ea0df123"
      },
      "source": [
        "# Introduction \n",
        "This is my first Kaggle experience, and one of the first machine learning projects that I wrote in Python. \n",
        "\n",
        "When the RMS Titanic sank in 1912, 1502 out of 2224 passengers and crew members died. There were not enough lifeboats for everyone. In the Hollywood blockbuster that was modelled on this tragedy, it seemed to be the case that upper-class people, women and children were more likely to survive than others. But did these properties (socio-economic status, sex and age) really influence one's survival chances? \n",
        "\n",
        "Based on data of a subset of 891 passengers on the Titanic, I will make a model that can be used to predict survival of other Titanic passengers. \n",
        "\n",
        "### Outline\n",
        "In the course of this project I will take the following steps:\n",
        "\n",
        "- Preprocessing/cleaning of the provided data\n",
        "- Exploratory analysis of the data\n",
        "- Preprocessing for machine learning\n",
        "- Fitting machine learning models\n",
        "- Predicting test samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "1d71622d-151a-6d1a-6534-39a31704c78b"
      },
      "source": [
        "### Preprocessing\n",
        "First, let's load the training data to see what we're dealing with. We will import the file to a pandas DataFrame:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7fa1b512-509b-f047-372e-ba6b6dd849c2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "train_data = pd.read_csv('../input/train.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "cb6ecc2e-8f10-d1c6-169f-8d747a66e05d"
      },
      "source": [
        "Now, let's take a look at the first few rows of the DataFrame:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ef39996e-1c05-2e63-61ca-b28144c01e0a"
      },
      "outputs": [],
      "source": [
        "train_data.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "31002ed5-0925-d162-b588-6595eacbf68f"
      },
      "source": [
        "The documentation on Kaggle tells me that the 'Pclass' column contains a number which indicates class of the passenger's ticket:  1 for first class, 2 for second class and 3 for third class. This could function as a proxy for the socio-economic status of the passenger ('upper', 'middle', 'low'). The 'SibSp' column contains the number of siblings + spouses of the passenger also aboard the Titanic; the 'ParCh' column indicates the number of parents + children of the passenger also aboard the Titanic. The 'Ticket' column contains the ticket numbers of passengers (which are not likely to have any predictive power regarding survival); 'Cabin' contains the cabin number of the passenger, if he/she had a cabin, and lastly, 'Embarked' indicates the port of embarkation of the passenger: **C**herbourg, **Q**ueenstown or **S**outhampton. The meaning of the other columns is clear, I think.\n",
        "\n",
        "Let's check some more info on the DataFrame: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a8f8c14a-f0ba-54be-1c2e-bdeb83ab38c9"
      },
      "outputs": [],
      "source": [
        "train_data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "41b720ba-36ea-52f8-4d58-48d5892480c9"
      },
      "source": [
        "The DataFrame contains 891 entries in total, with 12 features. Of those 12 features, 10 have non-null values for every entry, and 2 do not: 'Age', which has 714 non-null entries, and 'Cabin', which has only 204 non-null entries (of course, not everyone had a cabin).\n",
        "\n",
        "First of all, the DataFrame index does not correspond to the PassengerId column. Let's check if the values in PassengerId are unique using numpy.unique():"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e7fbb85d-3f5f-4dd5-132f-65a29d04f9f3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "(np.unique(train_data['PassengerId'].values).size, \n",
        "np.unique(train_data['PassengerId'].values).size == train_data.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9ea84de2-6e15-84fe-dc69-2d8e8e497412"
      },
      "source": [
        "There are 891 unique values in the PassengerId column, which equals to the total 891 id values, so every entry has an unique value in that column. Let's set the index of our DataFrame to that column, since the PassengerId is assigned arbitrarily and thus should not be regarded a real 'feature' of the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "78cae54d-bef2-8404-4b14-b359cba6d9b7"
      },
      "outputs": [],
      "source": [
        "train_data.set_index(['PassengerId'], inplace=True)\n",
        "train_data.head(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "fb56321a-cdb0-c2c8-4013-395834305870"
      },
      "source": [
        "PassengerId has now been set as the DataFrame index. \n",
        "\n",
        "I don't expect the 'Name' feature to be of any value to our classification problem in its raw form, since all or at least most people will have a unique name. We could, however, extract the titles (Mr., Miss. etc.) of the passengers and use the title as a categorical/nominal feature. In the 'Name' column of our DataFrame, the titles are followed by a '.', which will help us extracting them. We'll make use of regex:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "dbbc2f4c-b9c7-ac02-4d1d-2c96ebcf1885"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "patt = re.compile('\\s(\\S+\\.)') # 1 whitespace character followed by several \n",
        "# non-whitespace characters followed by a dot\n",
        "\n",
        "titles = np.array([re.search(patt, i)[1] for i in train_data['Name'].values])\n",
        "\n",
        "print('Unique titles ({}): \\n{}'.format(np.unique(titles).shape[0], np.unique(titles)))\n",
        "print('')\n",
        "print('Number of titles that are NaN/Null: {}'.format(pd.isnull(titles).sum()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "f06a7cfc-7b4a-83c4-5bdd-d47ae3172081"
      },
      "source": [
        "We'll include the titles as a new feature 'Title' in the DataFrame, and drop the 'Name' feature:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "225d0d79-13c2-b767-419a-bd0ce3c35b1e"
      },
      "outputs": [],
      "source": [
        "train_data = train_data.assign(Title=titles)\n",
        "train_data = train_data.drop('Name', axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "2ee6f88a-059f-468e-93f0-d6419bb2da3e"
      },
      "source": [
        "We can count the number of occurrences for each title:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "259f2355-5011-b640-f3ef-340f1d745d1c"
      },
      "outputs": [],
      "source": [
        "train_data['Title'].groupby(train_data['Title']).size()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "61823f93-a66e-d27a-cb3d-51704d1ca4d7"
      },
      "source": [
        "We observe that most of the titles occur very infrequently. Fitting our models to these titles might mean we would be overfitting. Let's group 'Mlle' and 'Mme' with their English counterparts 'Miss' and 'Mrs'. 'Ms' will be grouped with 'Miss'; 'Capt', 'Col' and 'Major' will be put in an 'Army' category, and 'Countess', 'Don', 'Jonkheer', 'Lady' and 'Sir' will be put in a 'Noble' category. 'Dr' will be kept as a category:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "af33115f-6a73-111d-8956-a3a7aa5282f4"
      },
      "outputs": [],
      "source": [
        "train_data['Title'] = train_data['Title'].replace('Mlle.','Miss.')\n",
        "train_data['Title'] = train_data['Title'].replace('Ms.','Miss.')  \n",
        "train_data['Title'] = train_data['Title'].replace('Mme.','Mrs.')\n",
        "train_data['Title'] = train_data['Title'].replace(['Capt.','Col.','Major.'],'Army.')\n",
        "train_data['Title'] = train_data['Title'].replace(['Countess.','Don.','Jonkheer.','Lady.','Sir.'],'Noble.')\n",
        "\n",
        "print('Number of passengers:\\n{}'.format(train_data['Title'].\\\n",
        "                                         groupby(train_data['Title']).size()))\n",
        "print('')\n",
        "print('Average survival:\\n{}'.format(train_data[['Title','Survived']].\\\n",
        "                                     groupby(train_data['Title']).mean()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "be57269d-9b51-7e7d-7908-9ef640c6e746"
      },
      "source": [
        "Even though a few of the remaining groups are small (<1% of the dataset), I think it is worthwile to keep them as separate groups. Even though the groups are small, differences in average survival rate can still be meaningful. The groups do describe some characteristics of their members, after all: based on their professions/status, it could be explained that no reverends in our training data survived, and that army men were less likely to survive than noble people."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "8cc4cd34-53bb-5cdb-c7f1-3becbc998fd6"
      },
      "source": [
        "We will also drop the 'Ticket' column, since I do not see how this feature could explain a person's survival odds. If the ticket numbers encode anything at all, it would be likely that they encode things like cabin number, ticket class, embarkation port, and these features are allready present in the dataset. So unless there is some 'hidden' encoding taking place in the ticket numbers (an example: [Donald Trump's employees had secretly marked the housing applications of minorities with codes, such as \u201cNo. 9\u201d and \u201cC\u201d](https://www.washingtonpost.com/politics/inside-the-governments-racial-bias-case-against-donald-trumps-company-and-how-he-fought-it/2016/01/23/fb90163e-bfbe-11e5-bcda-62a36b394160_story.html?utm_term=.dd272e7475a2)), this feature is not likely to add anything to our analysis. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2afffc0d-9b0d-9857-e0c2-e33096bd5371"
      },
      "outputs": [],
      "source": [
        "train_data = train_data.drop('Ticket', axis=1)\n",
        "train_data.head(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "26845493-deef-1289-e9a4-bf3b99287147"
      },
      "source": [
        "Individual cabin codes are not likely to have much predictive power in our problem. However, the cabin codes can be split in categories based on the letter in the code, e.g. 'C' or 'D'. These letters might encode cabin class and thus social status, which might have predictive power on the survival odds. Since we don't know the ordering of the cabin categories, if there is any at all, this feature will be a nominal feature. Passengers without a class will have entries of 'None'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9d7ae73c-04df-5917-f488-9495e60eccff"
      },
      "outputs": [],
      "source": [
        "def getCabinCat(cabin_code):\n",
        "    if pd.isnull(cabin_code):\n",
        "        cat = 'None' # Use a string so that it is clear that this is \n",
        "                     # a category on its own\n",
        "    else:\n",
        "        cat = cabin_code[0]\n",
        "    return cat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "97b17196-afe4-539e-eba8-267083c054b7"
      },
      "outputs": [],
      "source": [
        "cabin_cats = np.array([getCabinCat(cc) for cc in train_data['Cabin'].values])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d0e019a1-913e-ac89-14cd-7107dd4ba844"
      },
      "source": [
        "Let's add this as a new 'Cabin_cat' feature to the DataFrame, and remove the 'Cabin' feature:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ab3b5219-1697-3d38-3b23-5a2aeb7fadee"
      },
      "outputs": [],
      "source": [
        "train_data = train_data.assign(Cabin_cat=cabin_cats)\n",
        "train_data = train_data.drop('Cabin', axis=1)\n",
        "train_data.head(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "2a11930a-ec09-c9f2-a844-876bd903eb2d"
      },
      "source": [
        "To get an idea about the distribution of passengers amongst the cabin categories:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9edab514-8cbb-efd0-dad8-f1b2bf7506cc"
      },
      "outputs": [],
      "source": [
        "print('Number of passengers:\\n{}'.format(train_data['Cabin_cat'].\\\n",
        "                                groupby(train_data['Cabin_cat']).size()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "4f497201-e12b-2468-dfab-1c3b01c5ebe6"
      },
      "source": [
        "This concludes the data preprocessing that we will be doing. If needed, we can always edit a feature later (e.g. if we need to impute some missing values for a feature). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "aa7bd241-e7fd-ac82-effa-1b296a0c2c8b"
      },
      "source": [
        "### Exploratory Data Analysis\n",
        "Let's create some plots of various features vs. survival, to see the kind of distributions that are present in the data, and to check if there is any obvious correlation. First, let's split the data between passengers who survived and passengers who died."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4723441b-a8b1-e1a2-c7aa-e63a957a50a3"
      },
      "outputs": [],
      "source": [
        "survived_data = train_data.loc[train_data['Survived']==1,:]\n",
        "died_data = train_data.loc[train_data['Survived']==0, :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "ceedc63c-387c-238f-c6ee-bafd7fa69da8"
      },
      "source": [
        "Then, let's plot the effect of sex vs. survival. In order to do that, we are first splitting the data on sex, i.e. between male and female, using Boolean masking:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3cfcaba1-2e4d-d90d-0ab0-213ef302e1be"
      },
      "outputs": [],
      "source": [
        "# Split the survived and died data between male and female\n",
        "survived_male_data = survived_data.loc[survived_data['Sex']=='male',:]\n",
        "died_male_data = died_data.loc[died_data['Sex']=='male',:]\n",
        "survived_female_data = survived_data.loc[survived_data['Sex']=='female',:]\n",
        "died_female_data = died_data.loc[died_data['Sex']=='female',:]\n",
        "\n",
        "# Total number of (fe)males that survived and that died\n",
        "survived_male_n = survived_male_data.shape[0]\n",
        "died_male_n = died_male_data.shape[0]\n",
        "survived_female_n = survived_female_data.shape[0]\n",
        "died_female_n = died_female_data.shape[0]\n",
        "\n",
        "import matplotlib.pyplot as pp\n",
        "\n",
        "# Bar plot drawing\n",
        "fig, axes = pp.subplots(nrows=1, ncols=2)\n",
        "\n",
        "# Sex vs. survival in total numbers\n",
        "pp.axes(axes[0])\n",
        "survived = pp.bar([0.5, 3.5], [survived_male_n, survived_female_n], width=1, \n",
        "       color='#3BB200')\n",
        "died = pp.bar([1.5, 4.5], [died_male_n, died_female_n], width=1, \n",
        "       color='red')\n",
        "pp.xticks([1,4], ('Male', 'Female'))\n",
        "pp.ylabel('Number of passengers')\n",
        "pp.legend((survived, died), ('Survived', 'Died'), loc=0, fontsize = 'medium')\n",
        "\n",
        "# Sex vs. survival in fractions\n",
        "pp.axes(axes[1])\n",
        "survived_pct = pp.bar([0.5, 3.5], [survived_male_n/(survived_male_n+died_male_n), \n",
        "                               survived_female_n/(survived_female_n+died_female_n)], \n",
        "                      width=1, color='#3BB200')\n",
        "died_pct = pp.bar([1.5, 4.5], [died_male_n/(survived_male_n+died_male_n), \n",
        "                           died_female_n/(survived_female_n+died_female_n)], \n",
        "                  width=1, color='red')\n",
        "pp.xticks([1,4], ('Male', 'Female'))\n",
        "pp.ylabel('Fraction')\n",
        "pp.legend((survived, died), ('Survived', 'Died'), fontsize = 'medium')\n",
        "fig.suptitle('Sex vs. survival', fontsize = 'x-large', y=1.03)\n",
        "pp.tight_layout()\n",
        "pp.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "3ce0851c-f774-682e-6f13-7064327dcbcc"
      },
      "source": [
        "We can see that in absolute numbers there are far more male passengers than female passengers in the training data, and the survival odds of males are much lower than those of females. \n",
        "\n",
        "Now, let's take a look at the effects of age on survival. The 'Age' column of our DataFrame contains NaN values, which will have to be filtered out in order to create the histogram. This means that the following plots will be based on only the 714 non-NaN values in the 'Age' column. Because we will have to remove passengers with NaN values more often, we will define a function checkNans that will help us do this: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "494834d4-b8a0-f231-20e0-46184aae2169"
      },
      "outputs": [],
      "source": [
        "def checkNans(arr, arr2=None):\n",
        "    mask_nan = pd.isnull(arr) # using pandas isnull to also operate\n",
        "                              # on string fields\n",
        "    if mask_nan.sum()>0:\n",
        "        any_nan = True\n",
        "    else:\n",
        "        any_nan = False\n",
        "    n_nan = mask_nan.sum()\n",
        "    \n",
        "    masked_arr = arr[~mask_nan]\n",
        "    if arr2 is not None:\n",
        "        masked_arr2 = arr2[~mask_nan]\n",
        "    else: \n",
        "        masked_arr2 = None\n",
        "\n",
        "    return any_nan, masked_arr, masked_arr2, n_nan, mask_nan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "52af636e-877c-1ed9-8345-1f03ad85e604"
      },
      "outputs": [],
      "source": [
        "# Extract age data of the survived and died passengers,\n",
        "# check for nans\n",
        "survived_age = checkNans(survived_data['Age'])[1]\n",
        "died_age = checkNans(died_data['Age'])[1]\n",
        "\n",
        "# Histogram\n",
        "# Determine bin edges of the combined data so that you can use these in \n",
        "# the final histogram, in order to make sure that the histogram bin \n",
        "# widths are equal for both groups\n",
        "stacked = np.hstack((survived_age, died_age))\n",
        "bins = np.histogram(stacked, bins=16, range=(0,stacked.max()))[1] \n",
        "\n",
        "# Creating the histograms\n",
        "survived = pp.hist(survived_age, bins, normed=1, facecolor='green', \n",
        "                   alpha=0.5)\n",
        "died = pp.hist(died_age, bins, normed=1, facecolor='red', alpha=0.5)\n",
        "\n",
        "# Create custom handles for adding a legend to the histogram\n",
        "import matplotlib.patches as mpatches\n",
        "survived_handle = mpatches.Patch(facecolor='green', alpha=0.5, \n",
        "                                label='Survived', edgecolor='black')\n",
        "died_handle = mpatches.Patch(facecolor='red', alpha=0.5, label='Died', \n",
        "                                edgecolor='black')\n",
        "pp.legend((survived_handle, died_handle), ('Survived', 'Died'), loc=0, \n",
        "                                fontsize = 'medium')\n",
        "\n",
        "# Other plot settings\n",
        "pp.title('Age vs. survival', fontsize = 'x-large', y=1.02)\n",
        "pp.xlabel('Age [years]')\n",
        "pp.ylabel('Fraction')\n",
        "pp.xlim([0,stacked.max()])\n",
        "pp.tight_layout()\n",
        "pp.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a29a3722-bcce-7716-e88d-41b6dcb89ab2"
      },
      "source": [
        "So what we can see here is that the age distributions of those who survived and those who died are quite similar. Passengers older than ~65 seem to have worse odds of surviving, and children aged below ~16 seem to have better odds of surviving, but overall the effects are not too great. It would be interesting to replicate this for males and females separately, which is what we'll do next."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c423a672-5149-0ba1-b28a-f6a1a71ef238"
      },
      "outputs": [],
      "source": [
        "# Extract age data of the survived and died passengers for males \n",
        "# and females separately, check for nans\n",
        "survived_male_age = checkNans(survived_male_data['Age'])[1]\n",
        "died_male_age = checkNans(died_male_data['Age'])[1]\n",
        "survived_female_age = checkNans(survived_female_data['Age'])[1]\n",
        "died_female_age = checkNans(died_female_data['Age'])[1]\n",
        "\n",
        "# Histogram\n",
        "# Create subplots with shared y-axis\n",
        "fig, axes = pp.subplots(nrows=1, ncols=2, figsize=(8,4), sharey=True)\n",
        "\n",
        "# Creating the histograms\n",
        "# For the bin edges, we can use the same as bin edges as in the \n",
        "# previous plot\n",
        "# Male histogram\n",
        "pp.axes(axes[0])\n",
        "survived_male = pp.hist(survived_male_age, bins, normed=1, \n",
        "                        facecolor='green', alpha=0.5)\n",
        "died_male = pp.hist(died_male_age, bins, normed=1, facecolor='red', \n",
        "                        alpha=0.5)\n",
        "pp.legend((survived_handle, died_handle), ('Survived', 'Died'),\n",
        "          loc=0, fontsize = 'medium') # Using the same legend handles \n",
        "                                      # as before\n",
        "pp.title('Male')\n",
        "pp.xlabel('Age [years]')\n",
        "pp.ylabel('Fraction')\n",
        "pp.xlim([0,stacked.max()]) # Using the same range as in the previous plot\n",
        "pp.tight_layout()\n",
        "\n",
        "# Female histogram\n",
        "pp.axes(axes[1])\n",
        "survived_female = pp.hist(survived_female_age, bins, normed=1, \n",
        "                          facecolor='green', alpha=0.5)\n",
        "died_female = pp.hist(died_female_age, bins, normed=1, facecolor='red', \n",
        "                      alpha=0.5)\n",
        "pp.legend((survived_handle, died_handle), ('Survived', 'Died'), loc=0, \n",
        "          fontsize = 'medium') # Using the same legend handles as before\n",
        "pp.title('Female')\n",
        "pp.xlabel('Age [years]')\n",
        "pp.xlim([0,stacked.max()]) # Using the same range as in the previous plot\n",
        "pp.tight_layout()\n",
        "fig.suptitle('Age vs. survival', fontsize = 'x-large', y=1.02)\n",
        "pp.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "29a4e0ad-2d4e-2be2-d3e8-65f854de6bba"
      },
      "source": [
        "The added differentation between male and female does not provide any real insights except that for really young (<10 years) male childs the odds of surviving are much higher than for girls of the same age. This could be a statistical glitch if the number of boys aged <10 is much different from the number of girls aged <10, or if both groups are very small in size. However, both groups have 19 members."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c0181d39-88a7-e346-045f-71b6ab212289"
      },
      "outputs": [],
      "source": [
        "print('Number of males aged <10: {}'.format\\\n",
        "      (survived_male_age[survived_male_age<10].count()))\n",
        "print('Number of females aged <10: {}'.format\\\n",
        "      (survived_female_age[survived_female_age<10].count()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "1c11489e-157e-b048-7f8b-2ff3edd1975c"
      },
      "source": [
        "Ticket class and ticket fare most likely will somewhat move together. Let's make some boxplots to verify this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "655094f5-015c-98e0-2c48-140ace9072cc"
      },
      "outputs": [],
      "source": [
        "# Extract the ticket fares for the various ticket classes\n",
        "fare_1 = train_data['Fare'].loc[train_data['Pclass']==1]\n",
        "fare_2 = train_data['Fare'].loc[train_data['Pclass']==2]\n",
        "fare_3 = train_data['Fare'].loc[train_data['Pclass']==3]\n",
        "\n",
        "# Boxplots\n",
        "pp.boxplot([fare_1, fare_2, fare_3])\n",
        "pp.ylim((0,train_data['Fare'].quantile(0.98))) \n",
        "# Only the lowest 98% of fares are shown because \n",
        "# otherwise the boxes would be hard to compare visually\n",
        "\n",
        "# Plot settings\n",
        "pp.ylabel('Fare')\n",
        "pp.xlabel('Ticket class')\n",
        "pp.title('Ticket class vs. fare', fontsize = 'x-large', y=1.02)\n",
        "pp.tight_layout()\n",
        "pp.show()\n",
        "\n",
        "# Histogram\n",
        "# Determine bin edges of the combined data so that you can use \n",
        "# these in the final histogram, in order to make sure that the \n",
        "# histogram bin widths are equal for all three classes\n",
        "bins = np.histogram(train_data['Fare'].loc[train_data['Fare']\n",
        "        <(train_data['Fare'].quantile(0.98))], bins=20, \n",
        "        range=(0,train_data['Fare'].loc[train_data['Fare']<\n",
        "        (train_data['Fare'].quantile(0.98))].max()))[1] \n",
        "\n",
        "# Creating the histograms\n",
        "class_1 = pp.hist(fare_1, bins, normed=1, facecolor='blue', alpha=0.5)\n",
        "class_2 = pp.hist(fare_2, bins, normed=1, facecolor='red', alpha=0.5)\n",
        "class_3 = pp.hist(fare_3, bins, normed=1, facecolor='green', alpha=0.5)\n",
        "\n",
        "# Create custom handles for adding a legend to the histogram\n",
        "class_1_handle = mpatches.Patch(facecolor='blue', alpha=0.5, \n",
        "                                label='Class 1', edgecolor='black')\n",
        "class_2_handle = mpatches.Patch(facecolor='red', alpha=0.5, \n",
        "                                label='Class 2', edgecolor='black')\n",
        "class_3_handle = mpatches.Patch(facecolor='green', alpha=0.5, \n",
        "                                label='Class 3', edgecolor='black')\n",
        "\n",
        "pp.legend((class_1_handle, class_2_handle, class_3_handle), \n",
        "          ('1st Class', '2nd Class', '3rd Class'), \n",
        "          loc=0, fontsize = 'medium')\n",
        "\n",
        "# Plot settings\n",
        "pp.title('Ticket class vs. fare', fontsize = 'x-large', y=1.02)\n",
        "pp.xlabel('Fare')\n",
        "pp.ylabel('Fraction')\n",
        "pp.xlim((0,train_data['Fare'].quantile(0.98)))\n",
        "pp.tight_layout()\n",
        "pp.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d0f6c08f-4d28-298a-f784-52599e983455"
      },
      "source": [
        "So the classes somewhat overlap, although the histogram does not visualize this very clearly. Let's look at the effect of ticket class on survival."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d2b1261d-9e45-40b2-752f-34013cb2325a"
      },
      "outputs": [],
      "source": [
        "# Extract the survival data for the various ticket classes\n",
        "survived_class_1 = survived_data['Survived'].loc[survived_data['Pclass']==1]\n",
        "died_class_1 = died_data['Survived'].loc[died_data['Pclass']==1]\n",
        "survived_class_2 = survived_data['Survived'].loc[survived_data['Pclass']==2]\n",
        "died_class_2 = died_data['Survived'].loc[died_data['Pclass']==2]\n",
        "survived_class_3 = survived_data['Survived'].loc[survived_data['Pclass']==3]\n",
        "died_class_3 = died_data['Survived'].loc[died_data['Pclass']==3]\n",
        "\n",
        "# Total number of passengers from each class that survived and that died\n",
        "survived_class_1_n = survived_class_1.shape[0]\n",
        "died_class_1_n = died_class_1.shape[0]\n",
        "survived_class_2_n = survived_class_2.shape[0]\n",
        "died_class_2_n = died_class_2.shape[0]\n",
        "survived_class_3_n = survived_class_3.shape[0]\n",
        "died_class_3_n = died_class_3.shape[0]\n",
        "\n",
        "# Bar plot drawing\n",
        "fig, axes = pp.subplots(nrows=1, ncols=2)\n",
        "\n",
        "# Ticket class vs. survival in total numbers\n",
        "pp.axes(axes[0])\n",
        "survived = pp.bar([0.5, 3.5, 6.5], [survived_class_1_n, survived_class_2_n,\n",
        "                                    survived_class_3_n], width=1, \n",
        "                                    color='#3BB200')\n",
        "died = pp.bar([1.5, 4.5, 7.5], [died_class_1_n, died_class_2_n, \n",
        "                                died_class_3_n], width=1, color='red')\n",
        "pp.xticks([1.5,4.5,7.5], ('1st Class', '2nd Class', '3rd Class'))\n",
        "\n",
        "pp.ylabel('Number of passengers')\n",
        "pp.legend((survived, died), ('Survived', 'Died'), loc=0, \n",
        "          fontsize = 'medium')\n",
        "\n",
        "# Sex vs. survival in fractions\n",
        "pp.axes(axes[1])\n",
        "survived_pct = pp.bar([0.5, 3.5, 6.5], [survived_class_1_n/(survived_class_1_n + \n",
        "                                                            died_class_1_n), \n",
        "                                        survived_class_2_n/(survived_class_2_n + \n",
        "                                                            died_class_2_n),\n",
        "                                         survived_class_3_n/(survived_class_3_n + \n",
        "                                                            died_class_3_n)], \n",
        "                      width=1, color='#3BB200')\n",
        "died_pct = pp.bar([1.5, 4.5, 7.5], [died_class_1_n/(survived_class_1_n + \n",
        "                                                            died_class_1_n), \n",
        "                                        died_class_2_n/(survived_class_2_n + \n",
        "                                                            died_class_2_n),\n",
        "                                         died_class_3_n/(survived_class_3_n + \n",
        "                                                            died_class_3_n)],  \n",
        "                  width=1, color='red')\n",
        "pp.xticks([1.5,4.5,7.5], ('1st Class', '2nd Class', '3rd Class'))\n",
        "#pp.xlim([0,8])\n",
        "pp.ylabel('Fraction')\n",
        "leg = pp.legend((survived, died), ('Survived', 'Died'), \n",
        "                fontsize = 'medium', loc='upper left')\n",
        "fig.suptitle('Ticket class vs. survival', fontsize = 'x-large', y=1.03)\n",
        "pp.tight_layout()\n",
        "pp.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "eee7d33c-7d7c-6140-6033-a08da3375318"
      },
      "source": [
        "We observe that passengers with a total of 3 or more siblings+spouse aboard the Titanic are far more likely to die than to survive (3rd plot). However, in absolute numbers (1st plot), this group is small. There were no passengers in our training set with 6 or 7 siblings+spouse aboard the Titanic, which explains the absence of bars for these categories, and which could produce some RuntimeWarnings. \n",
        "\n",
        "On to the effect of number of parents+children on survival. Here we observe that those passengers without any parents or children on board, and those with 4 or more parents or children on board, are more likely to die in the incident than to survive (4th plot). There were only a few passengers with 3 or more parents or children on board (2nd plot)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "0e6ac6c1-d941-4135-57c4-b240f03f0d22"
      },
      "source": [
        "So it seems that 1st class passengers had better odds of survival, and 3rd class passengers had far worse odds. 2nd Class passengers were about equally likely to either survive or die. \n",
        "\n",
        "The next thing that we'll look at is the number of siblings+spouse vs. survival and the number of parents+children vs. survival. The feature is discrete and numerical; we will therefore bin the data in bins of width 1 and create a histogram like above. The 'SibSp' and 'Parch' columns of our data do not contain any NaN values so we do not need to preprocess them. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d2ec7871-b56f-f8f2-cf64-b49fbdc74340"
      },
      "outputs": [],
      "source": [
        "fig, axes = pp.subplots(nrows=1, ncols=2, figsize=(8,4), sharey=True)\n",
        "\n",
        "# SIBSP\n",
        "pp.axes(axes[0])\n",
        "# Extract SibSp data of the survived and died passengers\n",
        "survived_sibsp = survived_data['SibSp']\n",
        "died_sibsp = died_data['SibSp']\n",
        "\n",
        "# Histogram\n",
        "# Determine bin edges of the combined data so that you can use \n",
        "# these in the final histogram, in order to make sure that the \n",
        "# histogram bin widths are equal for both groups\n",
        "stacked = np.hstack((survived_sibsp, died_sibsp))\n",
        "bins = np.histogram(stacked, bins=stacked.max()+1, range=\n",
        "                    (0,stacked.max()+1))[1] \n",
        "# The number of bins = stacked.max() so that each bin is of width 1\n",
        "\n",
        "# Creating the histograms\n",
        "survived = pp.hist(survived_sibsp, bins, normed=1, facecolor='green', \n",
        "                   alpha=0.5)\n",
        "died = pp.hist(died_sibsp, bins, normed=1, facecolor='red', alpha=0.5)\n",
        "\n",
        "# Plot settings\n",
        "pp.legend((survived_handle, died_handle), ('Survived', 'Died'), loc=0, \n",
        "          fontsize = 'medium') # Using the same legend handles as before\n",
        "pp.title('No. siblings+spouses vs. survival', fontsize = 'x-large', y=1.02)\n",
        "pp.xlabel('No. siblings+spouses')\n",
        "pp.ylabel('Fraction')\n",
        "pp.xticks(np.arange(train_data['SibSp'].max()+1)+0.5, \n",
        "          np.arange(train_data['SibSp'].max()+1))\n",
        "pp.xlim([0,stacked.max()+1])\n",
        "pp.tight_layout()\n",
        "\n",
        "# PARCH\n",
        "pp.axes(axes[1])\n",
        "# Extract Parch data of the survived and died passengers\n",
        "survived_parch = survived_data['Parch']\n",
        "died_parch = died_data['Parch']\n",
        "\n",
        "# Histogram\n",
        "# Determine bin edges of the combined data so that you can use \n",
        "# these in the final histogram, in order to make sure that the \n",
        "# histogram bin widths are equal for both groups\n",
        "stacked = np.hstack((survived_parch, died_parch))\n",
        "bins = np.histogram(stacked, bins=stacked.max(), \n",
        "                    range=(0,stacked.max()))[1] \n",
        "    # The number of bins = stacked.max() so that each bin is of width 1\n",
        "\n",
        "# Creating the histograms\n",
        "survived = pp.hist(survived_sibsp, bins, normed=1, facecolor='green', \n",
        "                   alpha=0.5)\n",
        "died = pp.hist(died_sibsp, bins, normed=1, facecolor='red', alpha=0.5)\n",
        "\n",
        "# Plot settings\n",
        "pp.legend((survived_handle, died_handle), ('Survived', 'Died'), loc=0, \n",
        "          fontsize = 'medium') # Using the same legend handles as before\n",
        "pp.title('No. parents+children vs. survival', fontsize = 'x-large', y=1.02)\n",
        "pp.xlabel('No. parents+children')\n",
        "pp.xticks(np.arange(train_data['Parch'].max()+1)+0.5, \n",
        "          np.arange(train_data['Parch'].max()+1))\n",
        "pp.ylabel('Fraction')\n",
        "pp.xlim([0,stacked.max()+1])\n",
        "pp.tight_layout()\n",
        "\n",
        "print('Fraction of total survived / died passengers \\n\\\n",
        "accounted for by a certain bin')\n",
        "pp.show()\n",
        "\n",
        "# SIBSP\n",
        "## The same as above, but now with the fractions within each category:\n",
        "# Extract the survival data for the various SibSp classes\n",
        "sibsp_classes = np.arange(train_data['SibSp'].max() + 1)\n",
        "sibsp_data = {'survived_n':np.array([]), 'survived_pct':np.array([]), \n",
        "              'died_n':np.array([]), 'died_pct':np.array([])}\n",
        "for ii in sibsp_classes:\n",
        "    sibsp_data['survived_'+str(ii)] = survived_data['Survived'].loc[\n",
        "        survived_data['SibSp']==ii]\n",
        "    sibsp_data['survived_n'] = np.append(sibsp_data['survived_n'],\n",
        "        np.array((sibsp_data['survived_'+str(ii)].count())))\n",
        "    sibsp_data['died_'+str(ii)] = died_data['Survived'].loc[\n",
        "        died_data['SibSp']==ii]\n",
        "    sibsp_data['died_n'] = np.append(sibsp_data['died_n'],\n",
        "        np.array((sibsp_data['died_'+str(ii)].count())))\n",
        "\n",
        "sibsp_data['survived_pct'] = (sibsp_data['survived_n']/  \n",
        "                                              (sibsp_data['survived_n']+\n",
        "                                               sibsp_data['died_n']))\n",
        "sibsp_data['survived_pct'][np.isnan(sibsp_data['survived_pct'])]=0\n",
        "sibsp_data['died_pct'] = (sibsp_data['died_n']/  \n",
        "                                              (sibsp_data['survived_n']+\n",
        "                                               sibsp_data['died_n']))\n",
        "sibsp_data['died_pct'][np.isnan(sibsp_data['died_pct'])]=0\n",
        "\n",
        "# No. of siblings+spouses vs. survival in fractions\n",
        "\n",
        "survived_pct = pp.bar(np.arange(train_data['SibSp'].max()+1)*3+0.5, \n",
        "                      sibsp_data['survived_pct'], width=1, color='#3BB200')\n",
        "died_pct = pp.bar(np.arange(train_data['SibSp'].max()+1)*3+1.5, \n",
        "                      sibsp_data['died_pct'], width=1, color='red')\n",
        "pp.xticks(np.arange(train_data['SibSp'].max()+1)*3+1.5, \n",
        "          np.arange(train_data['SibSp'].max()+1))\n",
        "pp.xlim([0,(train_data['SibSp'].max()+1)*3])\n",
        "pp.xlabel('No. of siblings+spouse')\n",
        "pp.ylabel('Fraction')\n",
        "leg = pp.legend((survived_pct, died_pct), ('Survived', 'Died'), \n",
        "                fontsize = 'medium', loc='upper left')\n",
        "pp.title('No. of siblings+spouse vs. survival', fontsize = \n",
        "         'x-large', y=1.03)\n",
        "pp.tight_layout()\n",
        "print('Fraction of survived / died passengers within a certain bin')\n",
        "pp.show()\n",
        "\n",
        "# PARCH\n",
        "# Extract ParCh data of the survived and died passengers\n",
        "survived_parch = survived_data['Parch']\n",
        "died_parch = died_data['Parch']\n",
        "\n",
        "# Extract the survival data for the various ParCh classes\n",
        "parch_classes = np.arange(train_data['Parch'].max() + 1)\n",
        "parch_data = {'survived_n':np.array([]), 'survived_pct':np.array([]), \n",
        "              'died_n':np.array([]), 'died_pct':np.array([])}\n",
        "for ii in parch_classes:\n",
        "    parch_data['survived_'+str(ii)] = survived_data['Survived'].loc[\n",
        "        survived_data['Parch']==ii]\n",
        "    parch_data['survived_n'] = np.append(parch_data['survived_n'],\n",
        "        np.array((parch_data['survived_'+str(ii)].count())))\n",
        "    parch_data['died_'+str(ii)] = died_data['Survived'].loc[\n",
        "        died_data['Parch']==ii]\n",
        "    parch_data['died_n'] = np.append(parch_data['died_n'],\n",
        "        np.array((parch_data['died_'+str(ii)].count())))\n",
        "\n",
        "parch_data['survived_pct'] = (parch_data['survived_n']/  \n",
        "                                              (parch_data['survived_n']+\n",
        "                                               parch_data['died_n']))\n",
        "parch_data['survived_pct'][np.isnan(parch_data['survived_pct'])]=0\n",
        "parch_data['died_pct'] = (parch_data['died_n']/  \n",
        "                                              (parch_data['survived_n']+\n",
        "                                               parch_data['died_n']))\n",
        "parch_data['died_pct'][np.isnan(parch_data['died_pct'])]=0\n",
        "\n",
        "# No. of parents+children vs. survival in fractions\n",
        "survived_pct = pp.bar(np.arange(train_data['Parch'].max()+1)*3+0.5, \n",
        "                      parch_data['survived_pct'], width=1, color='#3BB200')\n",
        "died_pct = pp.bar(np.arange(train_data['Parch'].max()+1)*3+1.5, \n",
        "                      parch_data['died_pct'], width=1, color='red')\n",
        "pp.xticks(np.arange(train_data['Parch'].max()+1)*3+1.5, \n",
        "          np.arange(train_data['Parch'].max()+1))\n",
        "pp.xlim([0,(train_data['Parch'].max()+1)*3])\n",
        "pp.xlabel('No. of parents+children')\n",
        "pp.ylabel('Fraction')\n",
        "leg = pp.legend((survived_pct, died_pct), ('Survived', 'Died'), \n",
        "                fontsize = 'medium', loc='upper left')\n",
        "pp.title('No. of parents+children vs. survival', fontsize = 'x-large', \n",
        "         y = 1.03)\n",
        "pp.tight_layout()\n",
        "print('Fraction of survived / died passengers within a certain bin')\n",
        "pp.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "c3ca3571-7b91-90e8-9fe2-670523572ef5"
      },
      "source": [
        "We have already seen that the ticket fares for the three ticket classes do overlap, and that the ticket class has predictive power on survival odds. When we group the ticket fares into bins, we can show the survival odds for each bin. Below I have grouped the data in bins of width=10; above a ticket fare of 250 only a few data points exist, and these have been grouped together. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1fb05940-7dd8-5d1a-ae4f-8e2de8269056"
      },
      "outputs": [],
      "source": [
        "bins = np.append(np.arange(0,251,10), train_data['Fare'].max())\n",
        "train_fare_binned = pd.cut(train_data['Fare'], bins, include_lowest=True)\n",
        "train_data[['Survived']].groupby(train_fare_binned).mean()\n",
        "\n",
        "print('Number of passengers:\\n{}'.format(train_data[\n",
        "    'Fare'].groupby(train_fare_binned).count()))\n",
        "print('\\n')\n",
        "print('Average survival:\\n{}'.format(train_data[\n",
        "    'Survived'].groupby(train_fare_binned).mean()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "97ccc35f-70f1-3c35-cf4c-aa8f397657cd"
      },
      "source": [
        "We observe that passengers in the lowest fare category are very numerous and also have very bad survival odds. We further notice that the next two most-populous groups have better survival odds than for our training set as a whole (38.4%)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "7e5a67f5-82b2-4940-bfef-d0a6974cc93e"
      },
      "source": [
        "Lastly, the average survival rates for the various categories in the 'Embarked', 'Cabin_cat' and 'Title' features will be presented. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "844090ce-8ce6-4ffd-283e-eb1e1aa2c2db"
      },
      "outputs": [],
      "source": [
        "print('Average survival:\\n{}'.format(train_data[['Embarked',\n",
        "    'Survived']].groupby('Embarked').mean()))\n",
        "print('\\n')\n",
        "print('{}'.format(train_data[['Cabin_cat',\n",
        "    'Survived']].groupby('Cabin_cat').mean()))\n",
        "print('\\n')\n",
        "print('{}'.format(train_data[['Title',\n",
        "    'Survived']].groupby('Title').mean()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "da3d0cc0-2d71-58fa-1d74-dbb6a38d3252"
      },
      "source": [
        "There seems to be enough distribution within those features for them to be relevant to our classification problem, so they will be included."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "c7ccab38-fb07-ea9a-1f71-dee0fa11ea50"
      },
      "source": [
        "We have seen some trends in the data. If we assume independence of the examined features, a man aged 20-25, travelling 3rd class, with a total of 5 siblings + spouse, and 4 parents + children, boarded in Southampton, would have far worse odds of surviving than a married woman aged 35-40, travelling 1st class, with 1 sibling or spouse, and 3 parents + children, boarded in Cherbourg. \n",
        "\n",
        "This concludes the Exploratory Data Analysis that we will be doing. We will now focus on training machine learning models on the data. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "18e592f7-c742-693b-cf9e-013be94673b6"
      },
      "source": [
        "### Preprocessing for machine learning\n",
        "Here we will apply some preprocessing that is needed in particular for various machine learning algorithms to efficiently operate on the data. We will start by encoding our categorical features ('Sex', 'Embarked', 'Title' and 'Cabin_cat') in a numerical format. We will use sklearn.preprocessing.OneHotEncoder for this; however, OneHotEncoder only works with *numerical* categorical data. We will use sklearn.preprocessing.LabelEncoder to encode our string labels with numbers.\n",
        "\n",
        "The features that we are going to encode might contain missing values/NaNs/Nones. These will have to be imputed. I am not imputing all features in this stage because for the imputation I want to find the *K* nearest neighbors of a sample that needs imputation (based on its other features), and finding these *K* neigbors will be more accurate once the categorical features are One-Hot labeled. First let's see which of our categories features NaN/None values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f5f62dc2-87a9-f661-087d-e2da885f2a2a"
      },
      "outputs": [],
      "source": [
        "train_data[['Sex','Embarked','Title','Cabin_cat']].info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "fad95476-4b72-aa21-8c29-995af7b720fc"
      },
      "source": [
        "So only the 'Embarked' feature contains null entries, 2 in total. Let's find those entries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "557dd601-947f-2c56-23c4-d7c261f568e1"
      },
      "outputs": [],
      "source": [
        "train_data.loc[pd.isnull(train_data['Embarked'])]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "c74da86f-d61e-2b10-7f16-4f1c71dfb991"
      },
      "source": [
        "The passengers with PassengerId 62 and 830 do not have a known port of embarkation. We will come back to the imputation of those values later; first, lets One-Hot label the other features, so that we can do a more accurate imputation later on. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c371ddf3-4179-8ad0-2839-3d31ce700ea9"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Converting to numerical features\n",
        "# Sex feature\n",
        "le_sex = LabelEncoder()\n",
        "sex_numerical = le_sex.fit_transform(train_data['Sex'])\n",
        "sex_numerical_classes = le_sex.classes_\n",
        "\n",
        "# Title feature\n",
        "le_title = LabelEncoder()\n",
        "title_numerical = le_title.fit_transform(train_data['Title'])\n",
        "title_numerical_classes = le_title.classes_\n",
        "\n",
        "# Cabin_cat feature\n",
        "le_cabin_cat = LabelEncoder()\n",
        "cabin_cat_numerical = le_cabin_cat.fit_transform(train_data['Cabin_cat'])\n",
        "cabin_cat_numerical_classes = le_cabin_cat.classes_\n",
        "\n",
        "print('Classes of Sex feature:\\n{}\\n{}'.format(\n",
        "        np.arange(len(sex_numerical_classes)), sex_numerical_classes))\n",
        "print('')\n",
        "print('Classes of Title feature:\\n{}\\n{}'.format(\n",
        "        np.arange(len(title_numerical_classes)), title_numerical_classes))\n",
        "print('')\n",
        "print('Classes of Cabin_cat feature:\\n{}\\n{}'.format(\n",
        "        np.arange(len(cabin_cat_numerical_classes)), cabin_cat_numerical_classes))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "9c47e0c7-6a80-51ce-42c9-bcbdd06dd5c4"
      },
      "source": [
        "We will create One-Hot labeled features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d69ce7d0-dcdb-263c-4517-48237ab31438"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "# Sex feature\n",
        "enc_sex = OneHotEncoder(sparse=False)\n",
        "sex_onehot = enc_sex.fit_transform(sex_numerical.reshape(-1,1))\n",
        "\n",
        "# Title feature\n",
        "enc_title = OneHotEncoder(sparse=False)\n",
        "title_onehot = enc_title.fit_transform(title_numerical.reshape(-1,1))\n",
        "\n",
        "# Cabin_cat feature\n",
        "enc_cabin_cat = OneHotEncoder(sparse=False)\n",
        "cabin_cat_onehot = enc_cabin_cat.fit_transform(cabin_cat_numerical.reshape(-1,1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "72e22619-a9e8-1eea-8f38-cd4ec5083409"
      },
      "source": [
        "Drop the original categorical features and add the one-hot labeled features. Map names to the new features based on the classes printed above, using a new function pdAssignWithOHLabel():"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5aaf99c8-d2be-950c-0070-a4edd96a12c8"
      },
      "outputs": [],
      "source": [
        "def pdAssignWithOHLabel(df, column, onehot_labeled, class_labels):\n",
        "    to_assign = {}\n",
        "    for c_idx, label in enumerate(class_labels):\n",
        "        to_assign[column+'_'+label] = onehot_labeled[:,c_idx]\n",
        "    df = df.assign(**to_assign)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9761e8f2-f7ad-b41a-7267-1ea0faf7b6fb"
      },
      "outputs": [],
      "source": [
        "# Sex feature\n",
        "train_data = pdAssignWithOHLabel(train_data, 'Sex', \n",
        "                                 sex_onehot, sex_numerical_classes)\n",
        "train_data = train_data.drop('Sex',axis=1)\n",
        "\n",
        "# Title feature\n",
        "train_data = pdAssignWithOHLabel(train_data, 'Title', \n",
        "                                 title_onehot, title_numerical_classes)\n",
        "train_data = train_data.drop('Title',axis=1)\n",
        "\n",
        "# Cabin_cat feature\n",
        "train_data = pdAssignWithOHLabel(train_data, 'Cabin_cat', \n",
        "                            cabin_cat_onehot, cabin_cat_numerical_classes)\n",
        "train_data = train_data.drop('Cabin_cat',axis=1)\n",
        "\n",
        "train_data.head(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "694dcd75-47fe-d200-c77b-08d608d437dc"
      },
      "source": [
        "The 'Embarked' data has not been imputed or Hot-One labeled yet. We will perform imputation based on the 5 nearest neighbors of a passenger; accurate nearest neighbor finding requires our features to be scaled. sklearn.preprocessing.StandardScaler() will be used to scale the features; however, StandardScaler does not provide good results when the data contains outliers. We recall the ticket fare box plot from the Exploratory Data Analysis section; only the smallest 98% of the data was shown in that plot, because there were some far-outlying points. We plot the ticket fare data again:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "57a44f0f-b263-4d41-e023-7ef310a180ee"
      },
      "outputs": [],
      "source": [
        "pp.boxplot([train_data['Fare']]) \n",
        "pp.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "cfa530a6-25c3-e94d-5888-1ad0d1e46c43"
      },
      "source": [
        "Data points that lie outside a range of mean + 3\\* standard deviation or even farther away can certainly bring meaningul information to our models; however, in case of the 'Fare' feature, there are 3 data points that unproportionally influence the results of the StandardScaler because they lie so far apart from the other values. Let's set their values to the mean+5\\* standard deviation of the ticket fares:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "584b6c3f-dba7-aff6-0d62-3a2420153cd1",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "mu = train_data['Fare'].mean()\n",
        "sd = train_data['Fare'].std()\n",
        "\n",
        "row_mask = train_data['Fare']>mu+5*sd\n",
        "train_data.set_value(row_mask, 'Fare', mu+5*sd);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "6aaeb23c-61b4-5735-3477-0abab56c1a1f"
      },
      "source": [
        "Now we can perform standard scaling on all features except the 'Embarked' and 'Age' feature, because both need to be imputed. This scaling will be performed on a temporary copy of the training data because with the sole purpose of being able to more accurately find nearest neighbors for data imputation. Persistent scaling will be performed on the training data in the ML fitting section. Also, the 'Survived' feature does not need scaling since it will be our target label in the ML model training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "456c4626-463e-0517-edde-384e0a290e3a"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc_tmp = StandardScaler()\n",
        "tmp_scaled = train_data.copy().drop(['Embarked','Age','Survived'], axis=1) # create a copy of the data\n",
        "tmp_scaled = pd.DataFrame(sc_tmp.fit_transform(tmp_scaled),columns=tmp_scaled.columns, index=tmp_scaled.index)\n",
        "\n",
        "# Add the non-scaled features to this temporary DataFrame\n",
        "tmp_scaled = tmp_scaled.assign(Survived=train_data['Survived'])\n",
        "tmp_scaled = tmp_scaled.assign(Embarked=train_data['Embarked'])\n",
        "tmp_scaled = tmp_scaled.assign(Age=train_data['Age'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "6b12660d-8ab3-df3a-d900-7f63be88eea3"
      },
      "source": [
        "We found before that passengers 62 and 830 did not have non-null values for the 'Embarked' feature. Here, let's find the 5 nearest neighbors of these passengers based on all features except 'Age', 'Embarked' and 'Survived', and assign a value for 'Embarked' based on the average value of their nearest neighbors on that feature:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "843c1949-028b-1094-c1e0-b9130eb45ec1"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KDTree\n",
        "tmp = tmp_scaled.copy().drop(['Survived','Age','Embarked'], axis=1).values\n",
        "row_idx = pd.isnull(train_data['Embarked'])\n",
        "tree = KDTree(tmp)\n",
        "dist, ind = tree.query(tmp[[62, 830]], k=6) \n",
        "# The k nearest neighbors include the passenger itself, \n",
        "# so we specify k=6 to get the 5 nearest neighbors\n",
        "for i in ind:\n",
        "    print('5 closest neigbors to passenger {} and their values for Embarked:\\n{}\\n'\\\n",
        "          .format(i[0], train_data['Embarked'].loc[i[1:]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "16693a11-363f-11d4-579f-3059fff4a16b"
      },
      "source": [
        "Based on the above, both passengers will be assigned an 'S' in the 'Embarked' feature:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8b1a70ea-7795-bd40-058a-58cbad89fed4"
      },
      "outputs": [],
      "source": [
        "train_data.set_value([62, 830], 'Embarked', 'S');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "cf4c5856-9567-12bb-1bd5-998d3290e4a3"
      },
      "source": [
        "The 'Embarked' feature is now also ready to be One-Hot labeled:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c41b1720-0257-ca17-7431-259c0e02a076"
      },
      "outputs": [],
      "source": [
        "# Encode the values with numerical labels\n",
        "le_embarked = LabelEncoder()\n",
        "embarked_numerical = le_embarked.fit_transform(train_data['Embarked'])\n",
        "embarked_numerical_classes = le_embarked.classes_\n",
        "print('Classes of Embarked feature:\\n{}\\n{}'.format(\n",
        "        np.arange(len(embarked_numerical_classes)), \n",
        "        embarked_numerical_classes))\n",
        "\n",
        "# One-Hot encoding\n",
        "enc_embarked = OneHotEncoder(sparse=False)\n",
        "embarked_onehot = enc_embarked.fit_transform(embarked_numerical.reshape(-1,1))\n",
        "\n",
        "# Add new features\n",
        "train_data = pdAssignWithOHLabel(train_data, 'Embarked', embarked_onehot, \n",
        "                                 embarked_numerical_classes)\n",
        "tmp_scaled = pdAssignWithOHLabel(tmp_scaled, 'Embarked', embarked_onehot, \n",
        "                                 embarked_numerical_classes)\n",
        "# Drop old feature\n",
        "train_data = train_data.drop('Embarked',axis=1)\n",
        "tmp_scaled = tmp_scaled.drop('Embarked',axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "be77506d-cb22-4860-0019-35793bca7a7b"
      },
      "source": [
        "The new columns need to be standard-scaled:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "612f0031-f38d-d13c-54d9-2b94a2bd323d"
      },
      "outputs": [],
      "source": [
        "sc_tmp = StandardScaler()\n",
        "tmp = tmp_scaled[['Embarked_C', 'Embarked_Q', 'Embarked_S']].copy()\n",
        "tmp = pd.DataFrame(sc_tmp.fit_transform(tmp),columns=tmp.columns, index=tmp.index)\n",
        "\n",
        "# Drop the unscaled features from train_data \n",
        "tmp_scaled = tmp_scaled.drop(['Embarked_C', 'Embarked_Q', 'Embarked_S'], \n",
        "                             axis=1)\n",
        "\n",
        "# Assign the scaled features to train_data\n",
        "tmp_scaled = tmp_scaled.assign(Embarked_C=tmp['Embarked_C'])\n",
        "tmp_scaled = tmp_scaled.assign(Embarked_Q=tmp['Embarked_Q'])\n",
        "tmp_scaled = tmp_scaled.assign(Embarked_S=tmp['Embarked_S'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "5a15373f-2d4b-b170-e5bf-3b30e0c24562"
      },
      "source": [
        "We still need to work on feature 'Age' which has null entries and is not scaled yet. As above, we will impute this feature based on nearest neighbors' values. Because the 'Age' column contains quite some null values, we will use 7 nearest neighbors instead of 5. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c43307f8-0692-c341-fd6e-bc0c1afa5771"
      },
      "outputs": [],
      "source": [
        "def knnImpute(ori_arr, tmp_imp_arr, feature, k=6): # improved one\n",
        "    from sklearn.neighbors import KDTree\n",
        "    row_idx = ori_arr[pd.isnull(ori_arr[feature])].index.tolist()\n",
        "    tree = KDTree(tmp_imp_arr) # tmp_arr is the array without \n",
        "                           # the null-containing feature\n",
        "    #row_idx = np.add(row_idx, -1)\n",
        "    for nan_v in row_idx:\n",
        "        # Uncomment print statements below to get some more insight\n",
        "        #print('Passenger: {}'.format(nan_v))\n",
        "        dist, ind = tree.query(tmp_imp_arr[nan_v,:].reshape(1,-1), k)\n",
        "        #print(tmp_arr[nan_v,:])\n",
        "        #print(ind[0])\n",
        "        nn_vals = ori_arr[feature].loc[ind[0][1:]]\n",
        "        imp_val = np.floor(np.nanmean(nn_vals))+0.5 \n",
        "        # Per the documentation on this Kaggle data set, estimated\n",
        "        # 'Age' values are of the form x.5\n",
        "        \n",
        "        #print('{} closest neigbors to passenger {} and their values for ' \\\n",
        "        #'Age:\\n{}\\n'.format(k-1, nan_v, ori_arr[feature].loc[ind[0]]))\n",
        "        #print('Imputed value would be {}\\n'.format(imp_val))\n",
        "        ori_arr.set_value(nan_v, feature, imp_val)\n",
        "    return ori_arr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8705bf94-6791-6583-03e2-fa674101343d"
      },
      "outputs": [],
      "source": [
        "tmp_imp = tmp_scaled.copy().drop('Age', axis = 1).values\n",
        "train_data = knnImpute(train_data, tmp_imp, 'Age', 8)\n",
        "print('New number of null values in \"Age\" column: {}'.format(\n",
        "    pd.isnull(train_data['Age']).sum()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "3e608d8a-07e5-6c87-abd4-139dd87017f4"
      },
      "source": [
        "Now that all the data has been imputed, let's visualize a PCA plot on the first two principal components of the data to see if the data is easily separated:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "14d24ec0-5029-c654-ae10-0ef1afc626f4"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "# First scale the data\n",
        "sc_training = StandardScaler()\n",
        "tmp = train_data.copy().drop(['Survived'], axis=1).values # create a copy of the data\n",
        "sc_training = sc_training.fit(tmp)\n",
        "\n",
        "train_no_surv = train_data.copy().drop('Survived', axis=1)\n",
        "train_no_surv = sc_training.transform(train_no_surv)\n",
        "pca = PCA()\n",
        "pca.fit(train_no_surv)\n",
        "t = pca.transform(train_no_surv)\n",
        "\n",
        "died = t[np.array(train_data['Survived']==0), :]\n",
        "survived = t[np.array(train_data['Survived']==1), :]\n",
        "components = [0, 1] # Which principal components to plot against\n",
        "\n",
        "pp.scatter(died[:,components[0]].reshape(1,-1), died[:,components[1]].reshape(1,-1), \n",
        "           color='red', alpha=.5, label='Died')\n",
        "pp.scatter(survived[:,components[0]].reshape(1,-1), survived[:,components[1]].reshape(1,-1), \n",
        "           color='Green', alpha=.5, label='Survived')\n",
        "\n",
        "pp.legend(loc='best', shadow=False, scatterpoints=1)\n",
        "pp.title('PCA of train_no_surv')\n",
        "pp.xlabel('Principal component {}'.format(components[0]+1))\n",
        "pp.ylabel('Principal component {}'.format(components[1]+1))\n",
        "pp.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "0fd7c754-4eba-2b8a-24f1-6664dbcf8983"
      },
      "source": [
        "We observe that in the PC1, PC2 space some clustering of the data seems to take place; however, the two groups ('Died' and 'Survived') do overlap in this space. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "cbe82b55-2ac5-b951-e8ce-083523b8783d"
      },
      "source": [
        "Now that the all features that required it have been imputed and a StandardScaler has been trained for later usage, we are ready to begin fitting our machine learning models:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "8d51ed2b-47b9-f84c-48df-6d8da4cf8570"
      },
      "source": [
        "### Fitting machine learning models\n",
        "Now that our data has been extensively preprocessed we will (finally!) be training various machine learning models on our data. In the following steps, the training data will be split (using cross-validation) into training and test data sets. ML models and StandardScalers will be trained on the training subsets and subsequently be applied to the test subset.   of Scaling of the final (as of yet unknown) test data will have to be performed using the same StandardScaler that will be used to transform the training data. Since scaling in the previous steps took place inside cross-validations, we'll fit a StandardScaler on the entire training data here, so that it can be used in scaling test data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "37653d2c-df2a-fb4b-e0f9-5c69e4f12d98"
      },
      "outputs": [],
      "source": [
        "# Extract training data (without Survived feature) and class labels\n",
        "columns = ['Age', 'Pclass', 'SibSp', 'Parch', 'Fare', 'Sex_female', 'Sex_male',\n",
        "       'Title_Army.', 'Title_Dr.', 'Title_Master.', 'Title_Miss.', 'Title_Mr.',\n",
        "       'Title_Mrs.', 'Title_Noble.', 'Title_Rev.', 'Cabin_cat_A',\n",
        "       'Cabin_cat_B', 'Cabin_cat_C', 'Cabin_cat_D', 'Cabin_cat_E',\n",
        "       'Cabin_cat_F', 'Cabin_cat_G', 'Cabin_cat_None', 'Cabin_cat_T',\n",
        "       'Embarked_C', 'Embarked_Q', 'Embarked_S']\n",
        "\n",
        "train_data_df = train_data # Keep the full train_data DataFrame for later usage\n",
        "train_labels = train_data['Survived'].values.ravel()\n",
        "train_data = train_data[columns].values\n",
        "\n",
        "# General import statements\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Overall classifier performance DataFrame\n",
        "overall_res = pd.DataFrame(columns=['Classifier', 'Best_clf_retrained', 'Best_test_score'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "722bd633-0529-e228-8300-044d8cb63453"
      },
      "outputs": [],
      "source": [
        "# Naive Bayes classifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "clf_name = 'GaussianNB'\n",
        "\n",
        "sc = StandardScaler()\n",
        "clf = GaussianNB()\n",
        "pipeline = Pipeline([('sc', sc),('clf', clf)])\n",
        "fit_params = {}\n",
        "# Initiate GridSearchCV\n",
        "gs = GridSearchCV(pipeline, fit_params, cv=7, n_jobs=-1)\n",
        "# Fit GridSearchCV\n",
        "gs.fit(train_data, train_labels)\n",
        "\n",
        "cv = pd.DataFrame(gs.cv_results_) # To check all the classifier performances\n",
        "\n",
        "overall_res = overall_res.append({'Classifier': clf_name, 'Best_clf_retrained': \n",
        "                            gs.best_estimator_.fit(train_data, train_labels), \n",
        "                            'Best_test_score': gs.best_score_}, ignore_index=True);\n",
        "print('Best score in CV fitting: {}'.format(gs.best_score_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bc7d95bb-adc3-af44-edfb-120498f270f6"
      },
      "outputs": [],
      "source": [
        "# Decision tree classifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "clf_name = 'DecisionTree'\n",
        "\n",
        "sc = StandardScaler()\n",
        "clf = DecisionTreeClassifier()\n",
        "# Make pipeline\n",
        "pipeline = Pipeline([('sc', sc),('clf', clf)])\n",
        "fit_params = {'clf__criterion':['gini','entropy'],\n",
        "    'clf__min_samples_split':[2,3,4,5,6,7,8,9,10,12,14,16,18,20,22,24,26,28,30,32],\n",
        "    'clf__min_samples_leaf':[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]}\n",
        "# Initiate GridSearchCV\n",
        "gs = GridSearchCV(pipeline, fit_params, cv=7, n_jobs=-1)\n",
        "# Fit GridSearchCV\n",
        "gs.fit(train_data, train_labels)\n",
        "\n",
        "cv = pd.DataFrame(gs.cv_results_) # To check all the classifier performances\n",
        "\n",
        "overall_res = overall_res.append({'Classifier': clf_name, 'Best_clf_retrained': \n",
        "                            gs.best_estimator_.fit(train_data, train_labels), \n",
        "                            'Best_test_score': gs.best_score_}, ignore_index=True);\n",
        "print('Best performing classifier parameters (score {}):\\n{}'.format(gs.best_score_,gs.best_params_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ba757ff2-1c16-640b-ef50-989590613be1"
      },
      "outputs": [],
      "source": [
        "# SVM classifier\n",
        "from sklearn.svm import SVC\n",
        "clf_name = 'SVM'\n",
        "\n",
        "sc = StandardScaler()\n",
        "clf = SVC()\n",
        "# Make pipeline\n",
        "pipeline = Pipeline([('sc', sc),('clf', clf)])\n",
        "fit_params = {'clf__kernel':['rbf','linear', 'poly'],'clf__degree':[2, 3, 4, 5]}\n",
        "# Initiate GridSearchCV\n",
        "gs = GridSearchCV(pipeline, fit_params, cv=7, n_jobs=-1)\n",
        "# Fit GridSearchCV\n",
        "gs.fit(train_data, train_labels)\n",
        "\n",
        "cv = pd.DataFrame(gs.cv_results_) # To check all the classifier performances\n",
        "\n",
        "overall_res = overall_res.append({'Classifier': clf_name, 'Best_clf_retrained': \n",
        "                            gs.best_estimator_.fit(train_data, train_labels), \n",
        "                            'Best_test_score': gs.best_score_}, ignore_index=True);\n",
        "print('Best performing classifier parameters (score {}):\\n{}'.format(gs.best_score_,gs.best_params_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "668e94ec-c735-bcbd-3573-d96b25f43f78"
      },
      "outputs": [],
      "source": [
        "# k-NN classifier\n",
        "from sklearn.neighbors import KNeighborsClassifier as KNC\n",
        "clf_name = 'KNN'\n",
        "\n",
        "sc = StandardScaler()\n",
        "clf = KNC()\n",
        "# Make pipeline\n",
        "pipeline = Pipeline([('sc', sc),('clf', clf)])\n",
        "fit_params = {'clf__n_neighbors':[2,3,4,5,6,7,8,9,10,11,12,13,14,15,17,20,25,30,40],\n",
        "                 'clf__weights':['uniform','distance'],\n",
        "                 'clf__p':[1, 2]}\n",
        "# Initiate GridSearchCV\n",
        "gs = GridSearchCV(pipeline, fit_params, cv=7, n_jobs=-1)\n",
        "# Fit GridSearchCV\n",
        "gs.fit(train_data, train_labels)\n",
        "\n",
        "cv = pd.DataFrame(gs.cv_results_) # To check all the classifier performances\n",
        "\n",
        "overall_res = overall_res.append({'Classifier': clf_name, 'Best_clf_retrained': \n",
        "                            gs.best_estimator_.fit(train_data, train_labels), \n",
        "                            'Best_test_score': gs.best_score_}, ignore_index=True);\n",
        "print('Best performing classifier parameters (score {}):\\n{}'.format(gs.best_score_,gs.best_params_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f129fae8-b911-7ef4-333d-0fd0c0ee8805"
      },
      "outputs": [],
      "source": [
        "# Random Forest classifier\n",
        "from sklearn.ensemble import RandomForestClassifier as RFC\n",
        "clf_name = 'RandomForest'\n",
        "\n",
        "sc = StandardScaler()\n",
        "clf = RFC()\n",
        "# Make pipeline\n",
        "pipeline = Pipeline([('sc', sc),('clf', clf)])\n",
        "fit_params = {'clf__n_estimators':[100],\n",
        "              'clf__min_samples_split':[2,3,4,5,6,7,8,9,10,11,12,13,16],\n",
        "              'clf__min_samples_leaf':[1,2,3,4,5,6,7],\n",
        "              'clf__max_features':[None,'auto']}\n",
        "# Initiate GridSearchCV\n",
        "gs = GridSearchCV(pipeline, fit_params, cv=7, n_jobs=-1)\n",
        "# Fit GridSearchCV\n",
        "gs.fit(train_data, train_labels)\n",
        "\n",
        "cv = pd.DataFrame(gs.cv_results_) # To check all the classifier performances\n",
        "\n",
        "overall_res = overall_res.append({'Classifier': clf_name, 'Best_clf_retrained': \n",
        "                            gs.best_estimator_.fit(train_data, train_labels), \n",
        "                            'Best_test_score': gs.best_score_}, ignore_index=True);\n",
        "print('Best performing classifier parameters (score {}):\\n{}'.format(gs.best_score_,gs.best_params_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "56d65884-3c39-2972-4508-f3d9530de3bf"
      },
      "outputs": [],
      "source": [
        "# AdaBoost classifier\n",
        "from sklearn.ensemble import AdaBoostClassifier as ABC\n",
        "clf_name = 'AdaBoost'\n",
        "\n",
        "sc = StandardScaler()\n",
        "clf = ABC()\n",
        "# Make pipeline\n",
        "pipeline = Pipeline([('sc', sc),('clf', clf)])\n",
        "fit_params = {'clf__n_estimators':[200],\n",
        "              'clf__base_estimator':[\n",
        "                  DecisionTreeClassifier(criterion='gini',\n",
        "                      min_samples_leaf=1, min_samples_split=2),                  \n",
        "                  DecisionTreeClassifier(criterion='gini',\n",
        "                      min_samples_leaf=2, min_samples_split=4), \n",
        "                  DecisionTreeClassifier(criterion='gini',\n",
        "                      min_samples_leaf=2, min_samples_split=5), \n",
        "                  DecisionTreeClassifier(criterion='gini',   \n",
        "                      min_samples_leaf=2, min_samples_split=6), \n",
        "                  DecisionTreeClassifier(criterion='gini',                                          \n",
        "                      min_samples_leaf=2, min_samples_split=10),\n",
        "                  DecisionTreeClassifier(criterion='gini',\n",
        "                      min_samples_leaf=3, min_samples_split=6), \n",
        "                  DecisionTreeClassifier(criterion='gini',\n",
        "                      min_samples_leaf=3, min_samples_split=7), \n",
        "                  DecisionTreeClassifier(criterion='gini',   \n",
        "                      min_samples_leaf=3, min_samples_split=8),                         \n",
        "                  DecisionTreeClassifier(criterion='gini',\n",
        "                      min_samples_leaf=2, min_samples_split=13),\n",
        "                  DecisionTreeClassifier(criterion='gini',\n",
        "                      min_samples_leaf=4, min_samples_split=10),\n",
        "                  DecisionTreeClassifier(criterion='gini',\n",
        "                      min_samples_leaf=4, min_samples_split=13),\n",
        "                  DecisionTreeClassifier(criterion='gini',\n",
        "                      min_samples_leaf=6, min_samples_split=13),\n",
        "                  DecisionTreeClassifier(criterion='gini',\n",
        "                      min_samples_leaf=8, min_samples_split=18),\n",
        "                  DecisionTreeClassifier(criterion='gini',\n",
        "                      min_samples_leaf=10, min_samples_split=22),\n",
        "              ]}\n",
        "                                                                   \n",
        "# Initiate GridSearchCV\n",
        "gs = GridSearchCV(pipeline, fit_params, cv=7, n_jobs=-1)\n",
        "# Fit GridSearchCV\n",
        "gs.fit(train_data, train_labels)\n",
        "\n",
        "cv = pd.DataFrame(gs.cv_results_) # To check all the classifier performances\n",
        "\n",
        "overall_res = overall_res.append({'Classifier': clf_name, 'Best_clf_retrained': \n",
        "                            gs.best_estimator_.fit(train_data, train_labels), \n",
        "                            'Best_test_score': gs.best_score_}, ignore_index=True);\n",
        "print('Best performing classifier parameters (score {}):\\n{}'.format(gs.best_score_,gs.best_params_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8f5bff8c-8654-d004-cb10-d7ee30ddf0c1"
      },
      "outputs": [],
      "source": [
        "# Neural Network classifier\n",
        "from sklearn.neural_network import MLPClassifier as NNC\n",
        "clf_name = 'NeuralNet'\n",
        "\n",
        "sc = StandardScaler()\n",
        "clf = NNC()\n",
        "# Make pipeline\n",
        "pipeline = Pipeline([('sc', sc),('clf', clf)])\n",
        "fit_params = {'clf__solver':['lbfgs'],\n",
        "              'clf__hidden_layer_sizes':[(2,),(3,),(4,),(7,),(10,),\n",
        "                                         (20,),(50,)],\n",
        "              'clf__max_iter':[200]\n",
        "                     }\n",
        "# Initiate GridSearchCV\n",
        "gs = GridSearchCV(pipeline, fit_params, cv=7, n_jobs=-1)\n",
        "# Fit GridSearchCV\n",
        "gs.fit(train_data, train_labels)\n",
        "\n",
        "cv = pd.DataFrame(gs.cv_results_) # To check all the classifier performances\n",
        "\n",
        "overall_res = overall_res.append({'Classifier': clf_name, 'Best_clf_retrained': \n",
        "                            gs.best_estimator_.fit(train_data, train_labels), \n",
        "                            'Best_test_score': gs.best_score_}, ignore_index=True);\n",
        "print('Best performing classifier parameters (score {}):\\n{}'.format(gs.best_score_,gs.best_params_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "96bf2e13-ce02-eb26-b183-72462f38ee88"
      },
      "outputs": [],
      "source": [
        "overall_res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "bced6694-1cb6-8b1a-ec35-17662b350152"
      },
      "source": [
        "We observe that, excluding GaussianNB, most classifiers are comparable in performance. We will classify all test set passengers using the best-performing operators above (SVM, KNN, RandomForest, NeuralNet). We ignore the DecisionTree classifier since it is likely to overfit, and decision trees are also implemented in RandomForest. We will put these classifiers in a list 'best_classifiers'. Their specific parameters have been stored in the results DataFrame and can be extracted to be used with new samples. Scaling of the test data will have to be performed using the same StandardScaler that was used to transform the training data, which we have called sc_training at the end of the previous section. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9a396711-90fb-dcee-83ac-4ea8535caff0"
      },
      "outputs": [],
      "source": [
        "best_classifiers = []\n",
        "for clf in ['SVM','KNN','RandomForest','NeuralNet']:\n",
        "    best_classifiers.append(overall_res[overall_res['Classifier']==clf]['Best_clf_retrained'].values[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "12e2f17a-5271-d3a3-614e-38eabbf43baa"
      },
      "source": [
        "### Predicting test samples\n",
        "We will assume for a moment that test set data only needs imputation on the 'Age' feature. Many operations from above need to be applied to the test set as well; loading the data, creating a DataFrame, One-Hot encoding the data, scaling the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1be29cef-0c5d-0f6d-c35b-8b1358cc3d3a",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Load test data\n",
        "test_data = pd.read_csv('../input/test.csv')\n",
        "\n",
        "# Set DataFrame index\n",
        "test_data.set_index(['PassengerId'], inplace=True)\n",
        "\n",
        "# Extract passenger titles\n",
        "patt = re.compile('\\s(\\S+\\.)') # 1 whitespace character followed by several \n",
        "# non-whitespace characters followed by a dot\n",
        "titles_test = np.array([re.search(patt, i)[1] for i in test_data['Name'].values])\n",
        "# In the test data provided with the project (which will not be the test data on which\n",
        "# my models will be evaluated), the title of 'Dona.' was present, which was not encountered\n",
        "# in the training data. Passengers with the title of 'Dona.' will be merged with 'Noble.'. \n",
        "# New features that are encountered in the real test data but which are not accounted \n",
        "# for individually in this preprocessing stage will be assigned the most occuring value\n",
        "# for this feature (Mr.)\n",
        "test_data = test_data.assign(Title=titles_test)\n",
        "test_data = test_data.drop('Name', axis=1)\n",
        "# Regroup some Title values\n",
        "test_data['Title'] = test_data['Title'].replace('Mlle.','Miss.')\n",
        "test_data['Title'] = test_data['Title'].replace('Ms.','Miss.')  \n",
        "test_data['Title'] = test_data['Title'].replace('Mme.','Mrs.')\n",
        "test_data['Title'] = test_data['Title'].replace(['Capt.','Col.','Major.'],'Army.')\n",
        "test_data['Title'] = test_data['Title'].replace(['Countess.','Don.', 'Dona.', 'Jonkheer.','Lady.','Sir.'],'Noble.')\n",
        "# Set unknown values for Title feature to Mr\n",
        "b_mask = test_data['Title'].isin(['Mr.','Sir.','Master.','Miss.','Mrs.','Lady.','Army.','Rev.', 'Noble.', 'Dr.'])\n",
        "b_mask = ~b_mask\n",
        "if b_mask.sum() > 0:\n",
        "    patt2 = re.compile('.*')\n",
        "    titles_test = test_data['Title'].copy()\n",
        "    titles_test = titles_test.loc[b_mask].replace(patt2,'Mr.')\n",
        "    test_data = test_data.drop('Title', axis=1)\n",
        "    test_data = test_data.assign(Title=titles_test)\n",
        "\n",
        "# Drop Ticket feature\n",
        "test_data = test_data.drop('Ticket', axis=1)\n",
        "\n",
        "# Generate Cabin_cat feature\n",
        "cabin_cats = np.array([getCabinCat(cc) for cc in test_data['Cabin'].values])\n",
        "test_data = test_data.assign(Cabin_cat=cabin_cats)\n",
        "test_data = test_data.drop('Cabin', axis=1)\n",
        "    \n",
        "# Converting textual categorical features to numbers\n",
        "sex_numerical = le_sex.transform(test_data['Sex'])\n",
        "title_numerical = le_title.transform(test_data['Title'])\n",
        "cabin_cat_numerical = le_cabin_cat.transform(test_data['Cabin_cat'])\n",
        "embarked_numerical = le_embarked.transform(test_data['Embarked'])\n",
        "\n",
        "# One-Hot encoding\n",
        "sex_onehot = enc_sex.transform(sex_numerical.reshape(-1,1))\n",
        "title_onehot = enc_title.transform(title_numerical.reshape(-1,1))\n",
        "cabin_cat_onehot = enc_cabin_cat.transform(cabin_cat_numerical.reshape(-1,1))\n",
        "embarked_onehot = enc_embarked.transform(embarked_numerical.reshape(-1,1))\n",
        "\n",
        "# Add One-Hot labels to DataFrame\n",
        "test_data = pdAssignWithOHLabel(test_data, 'Sex', \n",
        "                                 sex_onehot, sex_numerical_classes)\n",
        "test_data = test_data.drop('Sex',axis=1)\n",
        "test_data = pdAssignWithOHLabel(test_data, 'Title', \n",
        "                                 title_onehot, title_numerical_classes)\n",
        "test_data = test_data.drop('Title',axis=1)\n",
        "test_data = pdAssignWithOHLabel(test_data, 'Cabin_cat', \n",
        "                            cabin_cat_onehot, cabin_cat_numerical_classes)\n",
        "test_data = test_data.drop('Cabin_cat',axis=1)\n",
        "test_data = pdAssignWithOHLabel(test_data, 'Embarked', \n",
        "                            embarked_onehot, embarked_numerical_classes)\n",
        "test_data = test_data.drop('Embarked',axis=1)\n",
        "\n",
        "# Impute missing data in all features\n",
        "# Add training and test data together, to more accurately find nearest neighbors\n",
        "all_data = train_data_df.drop('Survived',axis=1).append(test_data)\n",
        "\n",
        "# Define updated knnImpute function:\n",
        "def knnImputeTest(test_arr, all_arr, tmp_imp_arr, feature, k=6): # improved one\n",
        "    from sklearn.neighbors import KDTree\n",
        "    row_idx = test_arr[pd.isnull(test_arr[feature])].index.tolist()\n",
        "    tree = KDTree(tmp_imp_arr.values) # tmp_imp_arr is the scaled array without \n",
        "                                      # the null-containing feature\n",
        "    #row_idx = np.add(row_idx, -1)\n",
        "    for nan_v in row_idx:\n",
        "        dist, ind = tree.query(tmp_imp_arr.loc[nan_v].values.reshape(1,-1), k)\n",
        "        nn_vals = all_arr[feature].loc[ind[0][1:]]\n",
        "        imp_val = np.floor(np.nanmean(nn_vals))+0.5 \n",
        "        # Per the documentation on this Kaggle data set, estimated\n",
        "        # 'Age' values are of the form x.5\n",
        "        test_arr = test_arr.set_value(nan_v, feature, imp_val)\n",
        "        all_arr = all_arr.set_value(nan_v, feature, imp_val)\n",
        "    return test_arr, all_arr\n",
        "\n",
        "feats = []\n",
        "for feat in all_data.columns:\n",
        "    feats.append(feat)\n",
        "need_imp = np.empty([0,2])\n",
        "for feat in feats:\n",
        "    if pd.isnull(all_data[feat]).sum() > 0:\n",
        "        need_imp = np.append(need_imp, np.array([[feat,pd.isnull(all_data[feat]).sum()]]), axis=0)\n",
        "\n",
        "# Sort features by number of imputations that need to be performed\n",
        "sort_idx = need_imp[:,1].argsort()\n",
        "need_imp = need_imp[sort_idx]\n",
        "\n",
        "for ii in range(len(need_imp)):\n",
        "    tmp = all_data = train_data_df.copy().drop('Survived',axis=1).append(test_data)\n",
        "    tmp = tmp.drop(list(need_imp[ii:,0]), axis=1)\n",
        "    sc = StandardScaler()\n",
        "    tmp_scaled =  pd.DataFrame(sc.fit_transform(tmp), columns=tmp.columns, index = tmp.index)\n",
        "    test_data, all_data = knnImputeTest(test_data, all_data, tmp_scaled, need_imp[ii,0], 11)\n",
        "\n",
        "# Check if imputation was done correctly:\n",
        "if pd.isnull(test_data).sum().sum() > 0:\n",
        "    raise ImputeError('{} NaNs in the data, so data was not imputed correctly'.format(\n",
        "        pd.isnull(test_data).sum().sum()))\n",
        "\n",
        "# Make sure column order is the same as in the training data, so that scaling can be performed\n",
        "columns = ['Age', 'Pclass', 'SibSp', 'Parch', 'Fare', 'Sex_female', 'Sex_male',\n",
        "       'Title_Army.', 'Title_Dr.', 'Title_Master.', 'Title_Miss.', 'Title_Mr.',\n",
        "       'Title_Mrs.', 'Title_Noble.', 'Title_Rev.', 'Cabin_cat_A',\n",
        "       'Cabin_cat_B', 'Cabin_cat_C', 'Cabin_cat_D', 'Cabin_cat_E',\n",
        "       'Cabin_cat_F', 'Cabin_cat_G', 'Cabin_cat_None', 'Cabin_cat_T',\n",
        "       'Embarked_C', 'Embarked_Q', 'Embarked_S']\n",
        "test_data = test_data[columns]\n",
        "    \n",
        "# Scale the test data using the StandardScaler that was fit on the original training data,\n",
        "# 'sc_training'\n",
        "tmp = test_data.copy()\n",
        "tmp = pd.DataFrame(sc_training.transform(tmp),columns=test_data.columns, index=test_data.index)\n",
        "test_data_scaled = tmp\n",
        "\n",
        "# Classify the test data with all classifiers in 'best_classifiers'\n",
        "preds_all = np.empty([test_data_scaled.values.shape[0],len(best_classifiers)])\n",
        "for idx, clf in enumerate(best_classifiers):\n",
        "    preds_all[:,idx] = clf.predict(test_data_scaled)\n",
        "preds_all = preds_all.astype('int')\n",
        "\n",
        "# We have n predictions for each sample (where n is the number of classifiers); \n",
        "# from these n predictions we have to generate a final value. We will use\n",
        "# a simple majority voting for this\n",
        "def majorityVoting(preds_all):\n",
        "    preds_final = np.empty([test_data_scaled.values.shape[0], 1])\n",
        "    for i in range(0,len(preds_all)):\n",
        "        bin_count = np.bincount(preds_all[i,:])\n",
        "        if np.bincount(bin_count)[-1]>1:\n",
        "            # There are more than one classes with the highest number of \n",
        "            # predicted values. In this case, select randomly between\n",
        "            # the two classes\n",
        "            preds = np.unique(preds_all[i,:])[bin_count==bin_count.max()]\n",
        "            pred = np.random.choice(preds)\n",
        "        else: \n",
        "            if len(np.unique(preds_all[i,:]))==1:\n",
        "                pred = np.unique(preds_all[i,:])[0]\n",
        "            else:\n",
        "                pred = np.unique(preds_all[i,:])[np.argmax(bin_count)]\n",
        "        preds_final[i,0] = pred\n",
        "    return preds_final\n",
        "\n",
        "# Write predictions to csv file\n",
        "preds_final = majorityVoting(preds_all).astype(int)\n",
        "preds_csv = pd.DataFrame(test_data.index.values, columns=['PassengerId'])\n",
        "preds_csv = preds_csv.assign(Survived=preds_final)\n",
        "preds_csv.to_csv('test_pred.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0edae90d-85ec-f6ce-b89f-e2fd6c22b159"
      },
      "outputs": [],
      "source": [
        "preds_csv"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}