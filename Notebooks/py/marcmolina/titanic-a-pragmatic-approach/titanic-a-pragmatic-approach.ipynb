{"cells":[{"metadata":{"_uuid":"06e1840d9de1679ed85daa45d9591dd4ef12aeca"},"cell_type":"markdown","source":"# Titanic: A Pragmatic Approach\n> Not intended to be read by the absolute beginner.\n\nOverview of the problem: https://www.kaggle.com/c/titanic\n\n![https://pivotsprites.deviantart.com](https://imgur.com/download/vTprxLc)\n\n### Acknowledgments\nThis notebook has been heavily influenced by those *great* contributions:\n* [A Data Science Framework: To Achieve 99% Accuracy](https://www.kaggle.com/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy/code) by LD Freeman\n* [Titanic Top 4% with ensemble modeling](https://www.kaggle.com/yassineghouzam/titanic-top-4-with-ensemble-modeling) by Yassine Ghouzam\n* [Titanic: 2nd degree families and majority voting](https://www.kaggle.com/erikbruin/titanic-2nd-degree-families-and-majority-voting) by Erik Bruin\n* [Pytanic](https://www.kaggle.com/headsortails/pytanic/code) by Heads or Tails\n* [Divide and Conquer [0.82296]](https://www.kaggle.com/pliptor/divide-and-conquer-0-82296) by Oscar Takeshita\n* [Titanic [0.82] - [0.83]](https://www.kaggle.com/konstantinmasich/titanic-0-82-0-83) by Konstantin"},{"metadata":{"_uuid":"f4fb4ba62f5d7046e089dba5d8d43cfe28cd167c"},"cell_type":"markdown","source":"## Our data science workflow\n* [**Step 1:** Defining the problem (description and objective)](#step1)\n* [**Step 2:** Gathering the data (automatic downloading)](#step2)\n* [**Step 3:** Performing exploratory data analysis (visualizing data, getting intuition)](#step3)\n* [**Step 4:** Preparing the data for consumption (data cleaning, feature engineering)](#step4)\n* [**Step 5:** Modeling the data (machine learning algorithms, optimizations)](#step5)\n* [**Step 6:** Drawing conclusions](#step6)"},{"metadata":{"_uuid":"a338ddf9990f919a4da183a4347699d0b82c9e11"},"cell_type":"markdown","source":"## Step 1: Defining the problem <a id=\"step1\"></a>\n\n### Kaggle description (as is)\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\nIn this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\n\n### Objective\nPredict who survived and who did not during the Titanic disaster, based on the features collected for us in the dataset: **BINARY CLASSIFICATION PROBLEM**.\n\n#### Dataset\n\nWe denote our *dataset* by $(X,Y) \\in \\chi^m \\times \\{0,1\\}^m$ where :\n* $\\chi$ is an abstract space of feature vectors\n* $X = (x_1, ..., x_m)$ is our vector of $m$ *feature vectors* where $x_i = (x_1^{(i)},...,x_n^{(i)})$\n* $Y = (y_1, ..., y_m)$ is our vector of labels\n\n#### Goal\n\nWe wish to find a good *classifier* $h$ mapping a vector in the abstract feature space to a binary output:\n$$\\begin{align*}\n  h \\colon \\chi &\\to \\{0,1\\}\\\\\n  x &\\mapsto y\n\\end{align*}$$\n\n*\"good\"* means we want to have a low *classification error (risk)* $\\mathcal{R}(h) = \\mathrm{P}(h(x) \\neq y)$.\n\n#### Hidden goal\n\n$y$ is distributed according to a *Bernoulli distribution* ($y \\in \\{0,1\\}$), so we write $y|x \\sim \\mathrm{Bernoulli}(\\eta(x))$, where $\\eta(x) = \\mathrm{P}(y=1|x) = \\mathrm{E}(y|x)$.\n\nThe problem is we don't have access to the distribution of $y|x$ which makes it hard to find the perfect classifier $\\eta$. Our goal is then not only to find a good classifier, but eventually to transform $x$ such that $y|x$ has a more predictable distribution for a potentially good classifier. In other words, we want our model to be able to have good generalization capabilities, as such we will apply a combination of multiple transformations on our dataset $X$. \n\n$X$ will then be mapped to a dataset $\\widetilde{X}$ in a different feature space $\\widetilde{\\chi} \\simeq [0,1]^n$.\n\nFor a more in-depth look at binary classification, feel free to read those notes: https://ocw.mit.edu/courses/mathematics/18-657-mathematics-of-machine-learning-fall-2015/lecture-notes/MIT18_657F15_L2.pdf."},{"metadata":{"_uuid":"bbf5640a17bf6eb266d6efae31be4421f1a04e14"},"cell_type":"markdown","source":"## Step 2: Gathering the data <a id=\"step2\"></a>\nThe data is available online as 3 CSV files at [https://www.kaggle.com/c/titanic/data](https://www.kaggle.com/c/titanic/data).\n\nLet's download them automatically."},{"metadata":{"_uuid":"f2a0506d25ac44f50c49d2d4ab29708b4c116d03","trusted":true},"cell_type":"code","source":"import os\nfrom pathlib import Path\nimport subprocess\n\n# Create the input directory if it doesn't exist\nif not os.path.exists('../input'):\n    os.makedirs('../input')\n\nfile_on_disk = True\n\n# Check if the files are on disk before download\nfor file in os.listdir('../input'):\n    if not Path('../input/' + file).is_file():\n        # The file is not on disk\n        file_on_disk = False\n        break\n        \nif not file_on_disk:\n    # Download the files with your API token in ~/.kaggle\n    error = subprocess.call('kaggle competitions download -c titanic -p ../input'.split())\n    if not error:\n        print('Files downloaded successfully.')\n    else:\n        print('An error occurred during donwload, check your API token.')\nelse:\n    print('Files are already on disk.')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cd5d26a9b7a74cc359a3e0c60825fe5f85a808fd"},"cell_type":"markdown","source":"## Step 3: Performing exploratory data analysis <a id=\"step3\"></a>\n\nKaggle is providing both **train** and **test** sets, we will perform EDA for each one of them.\n\n### 3.1. Import libraries\n\n**Visualization** is `matplotlib`/`seaborn` based, **data preprocessing** is essentially `pandas` based,  and **modelling** is mostly `scikit-learn` based."},{"metadata":{"_uuid":"f386aeefa195971332c3d05a3fa052bab2f8f330","trusted":true},"cell_type":"code","source":"# Load packages\nprint('Python packages:')\nprint('-'*15)\n\nimport sys\nprint('Python version: {}'. format(sys.version))\n\nimport pandas as pd\nprint('pandas version: {}'. format(pd.__version__))\n\nimport matplotlib\nprint('matplotlib version: {}'. format(matplotlib.__version__))\n\nimport numpy as np\nprint('NumPy version: {}'. format(np.__version__))\n\nimport scipy as sp\nprint('SciPy version: {}'. format(sp.__version__)) \n\nimport IPython\nfrom IPython import display\nprint('IPython version: {}'. format(IPython.__version__)) \n\nimport sklearn\nprint('scikit-learn version: {}'. format(sklearn.__version__))\n\n# Miscsellaneous libraries\nimport random\nimport time\n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\nprint('')\n\n# Check the input directory\nprint('Input directory: ')\nprint('-'*15)\nfrom subprocess import check_output\nprint(check_output(['ls', '../input']).decode('utf8'))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9cbb91961a7901b708efbc734a10769f158bac3e"},"cell_type":"markdown","source":"### 3.2. Load the data modelling libraries"},{"metadata":{"_uuid":"2928f46db44ee14ea435a410a8d3db73b33476e7","trusted":true},"cell_type":"code","source":"# Common model algorithms\nfrom sklearn import neighbors, ensemble\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\nfrom catboost import CatBoostClassifier\n\n# Common model helpers\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn import model_selection\n\n# Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nfrom matplotlib.ticker import PercentFormatter\nimport seaborn as sns\n\n# Configure visualization defaults\n%matplotlib inline\nmpl.style.use('ggplot')\nsns.set_style('white')\npalette = sns.color_palette('Set2', 10)\npylab.rcParams['figure.figsize'] = 18,4","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3c59204388fcad37f22eecaeaa81af565f730839"},"cell_type":"markdown","source":"### 3.3 Meet the data\n\nThe dataset is briefly described here: [https://www.kaggle.com/c/titanic/data](https://www.kaggle.com/c/titanic/data)\n\nIt is composed of **11 independent variables** and **1 dependent variable**.\n\n**Variable description**\n\n|Variable|Definition|Key|Type|\n|--------|----------|---|----|\n|**Survived**|Survival|0 = No, 1 = Yes|**CATEGORICAL**|\n|**Pclass**|Ticket class|1 = 1st, 2 = 2nd, 3 = 3rd|**ORDINAL**|\n|**Name**|Passenger's name|N/A|**MIXED**|\n|**Sex**|Passenger's sex|N/A|**CATEGORICAL**|\n|**Age**|Passenger's age|N/A|**CONTINUOUS**|\n|**SibSp**|# of siblings / spouses aboard the Titanic|N/A|**DISCRETE**|\n|**Parch**|# of parents / children aboard the Titanic|N/A|**DISCRETE**|\n|**Ticket**|Ticket number|N/A|**MIXED**|\n|**Fare**|Passenger fare|N/A|**CONTINUOUS**|\n|**Cabin**|Cabin number|N/A|**MIXED**|\n|**Embarked**|Port of embarkation|C = Cherbourg, Q = Queenstown, S = Southampton|**CATEGORICAL**|"},{"metadata":{"_uuid":"03dc9fdee2fae6f740c80a2a00fdb899c3f8e927","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/train.csv').set_index(keys='PassengerId', drop=True)\ntest_df  = pd.read_csv('../input/test.csv').set_index(keys='PassengerId', drop=True)\n\n# Useful for more accurate feature engineering\ndata_df = train_df.append(test_df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3f1b10056acbfe652012317afaf70ff102aa98c9"},"cell_type":"markdown","source":"#### Samples"},{"metadata":{"_uuid":"c785c99e9df5b0647e74b58550c784c6ea297081","scrolled":true,"trusted":true},"cell_type":"code","source":"train_df.sample(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b6ff1b37938f0e65afa57d9b77800f89740bd996"},"cell_type":"markdown","source":"#### Simple statistics from the train set\n891 samples."},{"metadata":{"_uuid":"f28bd40c08bedfb257a8202d581d8c21e5485eab","scrolled":true,"trusted":true},"cell_type":"code","source":"train_df.describe(include = 'all')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"13e83a30c18dc01d1494443b8ad52c86b0e54a82"},"cell_type":"markdown","source":"#### Simple statistics from the test set\n418 samples."},{"metadata":{"_uuid":"f28bd40c08bedfb257a8202d581d8c21e5485eab","scrolled":true,"trusted":true},"cell_type":"code","source":"test_df.describe(include = 'all')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2c8dce47c900f8b97bff77827d4a432c69fc2cdf"},"cell_type":"markdown","source":"891 samples to predict the outcome of 418 samples is a pretty bad ratio (2.14), there is a high risk of overfitting the train set."},{"metadata":{"_uuid":"3de39f29326f4f1344c8b2dcd4f06757d98d6486"},"cell_type":"markdown","source":"### 3.4 Missing data\nLet's have a quick look at missing data on both sets."},{"metadata":{"_uuid":"d8c87849c1ff5dffb4dcd051201c69417869d83a","trusted":true},"cell_type":"code","source":"def plot_missing_values(dataset):\n    \"\"\"\n        Plots the proportion of missing values per feature of a dataset.\n        \n        :param dataset: pandas DataFrame\n    \"\"\"\n    missing_data_percent = [x / len(dataset) for x in dataset.isnull().sum()]\n    data_percent = [1 - x for x in missing_data_percent]\n\n    fig, axs = plt.subplots(1,1,figsize=(18,4))\n    plt.bar(dataset.columns.values, data_percent, color='#84B044', linewidth=0)\n    plt.bar(dataset.columns.values, missing_data_percent, bottom=data_percent, color='#E76C5D', linewidth=0)\n\n    axs.yaxis.set_major_formatter(PercentFormatter(xmax=1))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"528d515ba59ad3228aa111b176475e13e2b45eb0"},"cell_type":"markdown","source":"#### Train set"},{"metadata":{"_uuid":"b96f7558feec3b12e0de40d3466c42d4694f71e4","trusted":true},"cell_type":"code","source":"train_df.isnull().sum().to_frame('Missing values').transpose()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c0e6f7e34bff8ada09e68c97b884a543fb70d223","trusted":true},"cell_type":"code","source":"plot_missing_values(train_df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"212e67096340dcabab8580776ee141cb71ec153d"},"cell_type":"markdown","source":"#### Test set"},{"metadata":{"_uuid":"d093640af50892a1fd360291cf3129d086fd45ef","trusted":true},"cell_type":"code","source":"test_df.isnull().sum().to_frame('Missing values').transpose()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a8ab7c3125c08d3c0574451672a6dd5b0e11e96a","trusted":true},"cell_type":"code","source":"plot_missing_values(test_df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"26579e2fa649d2159cf11639e570de4ffbf94d17"},"cell_type":"markdown","source":"`Age` and `Cabin` have quite a lot of missing values in both datasets, we will have to deal with those later."},{"metadata":{"_uuid":"539395db4c8af2be705cfa1cff10d56b69065726"},"cell_type":"markdown","source":"### 3.5 Exploring numerical features\nLet's plot the **Pearson's correlation matrix** of the raw numerical features to get a sense of linear correlations between them.\n\nThe coefficients of the matrix for variables $X$ and $Y$ are computed as follows:\n\n$$\\rho _{X,Y}={\\frac {\\operatorname {cov} (X,Y)}{\\sigma _{X}\\sigma _{Y}}}={\\frac {\\operatorname {E} [(X-\\mu _{X})(Y-\\mu _{Y})]}{\\sigma _{X}\\sigma _{Y}}}$$\n\nIt means that variables show a strong linear correlation if the absolute value of the coefficient is close to one."},{"metadata":{"_uuid":"e2cec33377fc007ca1c121b4fde83009cc72b52f","trusted":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18,6))\n\ncorr_train = train_df[['Age', 'Fare', 'Parch', 'SibSp', 'Survived']].corr()\ncorr_test = test_df[['Age', 'Fare', 'Parch', 'SibSp']].corr()\n\n# Generate masks for the upper triangles\nmask_train = np.zeros_like(corr_train, dtype=np.bool)\nmask_train[np.triu_indices_from(mask_train)] = True\n\nmask_test = np.zeros_like(corr_test, dtype=np.bool)\nmask_test[np.triu_indices_from(mask_test)] = True\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the train set heatmap with the mask and correct aspect ratio\nsns.heatmap(corr_train, ax=ax1, mask=mask_train, cmap=cmap, vmax=.5, center=0, square=True, \n            linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True, fmt='.2f')\nax1.set_title('Pearson\\'s correlation matrix of train set')\n\n# Draw the test heatmap with the mask and correct aspect ratio\nsns.heatmap(corr_test, ax=ax2, mask=mask_test, cmap=cmap, vmax=.5, center=0, square=True, \n            linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True, fmt='.2f')\nax2.set_title('Pearson\\'s correlation matrix of test set')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e19c1e88bf609fd573ec90e7d62843debbaf8d9b"},"cell_type":"markdown","source":"Three remarks:\n* it seems that `Fare` has the strongest linear correlation with `Survived`, making it a strong feature ;\n* `Parch` and `SibSp` show a potentially strong linear correlation, it might be a good idea to combine those features ;\n* except with `Fare`, the `Age` feature shows different correlation coefficients between the train set and the test set.\n\nBecause of that last remark, we will try to get more insights by computing the **Jensen-Shannon divergence** between the distributions of the train set and the test set. It is a measure of similarity between two probability distributions based on the **Kullback-Leibler divergence** well-known in information theory.\n\nIt is defined as:\n\n$${{\\rm {JSD}}}(P\\parallel Q)={\\frac  {1}{2}}D_{\\mathrm {KL}}(P\\parallel M)+{\\frac  {1}{2}}D_{\\mathrm {KL}}(Q\\parallel M)$$\n\nwhere $M={\\frac  {1}{2}}(P+Q)$ and $D_{\\mathrm {KL}}$ is the KL divergence."},{"metadata":{"_uuid":"54771f53db660492c58be5ba590817646577c603","trusted":true},"cell_type":"code","source":"from scipy.stats import entropy\nfrom numpy.linalg import norm\n\ndef JSD(P, Q, n_iter=1000):\n    \"\"\"\n        Computes the Jensen-Shannon divergence between two probability distributions of different sizes.\n        \n        :param P: distribution P\n        :param Q: distribution Q\n        :param n_iter: number of iterations\n        :return: Jensen-Shannon divergence\n    \"\"\"\n    size = min(len(P),len(Q))\n    \n    results = []\n    for _ in range(n_iter):\n        P = np.random.choice(P, size=size, replace=False)\n        Q = np.random.choice(Q, size=size, replace=False)\n\n        _P = P / norm(P, ord=1)\n        _Q = Q / norm(Q, ord=1)\n        _M = 0.5 * (_P + _Q)\n\n        results.append(0.5 * (entropy(_P, _M) + entropy(_Q, _M)))\n\n    return results","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9dfb8baa860919c62178f1ec587891575cdda6db"},"cell_type":"markdown","source":"### Univariate analysis\nLet's first analyze features individually.\n\n#### Age"},{"metadata":{"_uuid":"fc170a628fed994e30328f2e4b171b4316a72bce","trusted":true},"cell_type":"code","source":"# Age vs Survived\ng = sns.FacetGrid(train_df, col='Survived', size=4, aspect=2)\ng = g.map(sns.distplot, 'Age', color='#D66A84')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d28d15fcee14b01ad4be88847f0224dbb5ffc64"},"cell_type":"markdown","source":"Even though it just looks like sums of Gaussian distributions, we can clearly observe the impact of `Age` on `Survival` with *very young passengers* and *probably parents passengers* having more chance to survive. (remember that about 20% of the data is missing)\n\nLet's now see how the test set is distributed compared to the train set."},{"metadata":{"_uuid":"ec2fc30ec93a12e9901c04a8c7085ae08fc2cade","trusted":true},"cell_type":"code","source":"# Train set vs Test set\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18,4))\n\nsns.distplot(train_df['Age'].dropna(), ax=ax1, color='#D66A84')\nax1.set_title('Train set')\n\nsns.distplot(test_df['Age'].dropna(), ax=ax2, color='#D66A84')\nax2.set_title('Test set')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5c5019f70b3f0f41d42a807f79964fce69ac24b3"},"cell_type":"markdown","source":"We see that the `Age` feature alone won't be of great help predicting survival on the test set since most of it is composed of 20-30 years passengers which is a range 50/50 chance of survival.\n\nLet's compute the JS divergence for `Age`, we will compare this value later with other features."},{"metadata":{"_uuid":"70c358fc514d179583dbafb13615d2626acc1191","trusted":true},"cell_type":"code","source":"age_jsd = JSD(train_df['Age'].dropna().values, test_df['Age'].dropna().values)\nprint('Jensen-Shannon divergence of Age:', np.mean(age_jsd))\nprint('Standard deviation:', np.std(age_jsd))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7e0a5323757bc106bd11c25d879c9538cccf5474"},"cell_type":"markdown","source":"**Conclusion:** to use `Age`, we will have to impute 20% missing data (not that easy), create bins to avoid overfitting and/or mix it with other features."},{"metadata":{"_uuid":"62d5432574e40cd98a7d90e13a7ab9f0153ed154"},"cell_type":"markdown","source":"#### Fare"},{"metadata":{"_uuid":"6f293a39489f0f0e1c74478df958ad8237483c79","trusted":true},"cell_type":"code","source":"# Fare vs Survived\ng = sns.FacetGrid(train_df, col='Survived', palette=palette, size=4, aspect=2)\ng = g.map(sns.distplot, 'Fare', color='#25627D')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1f73ab30c9ce7697d91a097d3f2091fe4ec06fbd","trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(18,4))\n\ng = sns.distplot(train_df['Fare'], ax=ax, color='#25627D', label='Skewness : %.2f'%(train_df['Fare'].skew()))\ng = g.legend(loc='best')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bf3b941ba9acf0ae3240b245169d3bfe453c5bfb"},"cell_type":"markdown","source":"The `Fare` feature is right-skewed, if we want to make discriminant bins we'll have to address this concern later.\n\nThe skewness of a random variable $X$ is the third standardized moment $\\gamma _{1}$, defined as:\n\n$${\\displaystyle \\gamma _{1}=\\operatorname {E} \\left[\\left({\\frac {X-\\mu }{\\sigma }}\\right)^{3}\\right]={\\frac {\\mu _{3}}{\\sigma ^{3}}}={\\frac {\\operatorname {E} \\left[(X-\\mu )^{3}\\right]}{\\ \\ \\ (\\operatorname {E} \\left[(X-\\mu )^{2}\\right])^{3/2}}}={\\frac {\\kappa _{3}}{\\kappa _{2}^{3/2}}}}$$\n\nLet's now see how the test set is distributed compared to the train set."},{"metadata":{"_uuid":"ac543b6b8a28850524e4784c1e5a206de5c47a1e","trusted":true},"cell_type":"code","source":"# Train set vs Test set\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18,4))\n\nsns.distplot(train_df['Fare'].dropna(), ax=ax1, color='#25627D')\nax1.set_title('Train set')\n\nsns.distplot(test_df['Fare'].dropna(), ax=ax2, color='#25627D')\nax2.set_title('Test set')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"70f4b3933e96e7bd7a10c704656a1805f3993c71"},"cell_type":"markdown","source":"`Fare` looks almost evenly distributed between the train set and the test set.\n\nLet's compute the JS divergence for `Fare`, we will compare this value later with other features."},{"metadata":{"_uuid":"19bd5dd8afe37a48794221a6a86a82676cb1e87b","trusted":true},"cell_type":"code","source":"fare_jsd = JSD(train_df['Fare'].dropna().values, test_df['Fare'].dropna().values)\nprint('Jensen-Shannon divergence of Fare:', np.mean(fare_jsd))\nprint('Standard deviation:', np.std(fare_jsd))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ea38588fb4f4774addc888e05383a0ac2112431f"},"cell_type":"markdown","source":"**Conclusion:** to use `Fare`, we will have to impute 1 missing value, fix the tailed distribution and create bins to avoid overfitting and/or mix it with other features."},{"metadata":{"_uuid":"e50d21a6017b7af3fc8a341d9941f73db45c77a4"},"cell_type":"markdown","source":"#### Parch"},{"metadata":{"_uuid":"e9b7d6f53d8eae40d7d6f49130e66eb5aaf1cd26","trusted":true},"cell_type":"code","source":"palette6 = [\"#F6B5A4\", \"#EB7590\", \"#C8488A\", \"#872E93\", \"#581D7F\", \"#3A1353\"]\n# Parch vs Survived\ng  = sns.catplot(x='Parch', y='Survived', saturation=5, height=4, aspect=4, data=train_df, kind='bar', palette=palette6)\ng.despine(left=True)\ng = g.set_ylabels(\"Survival probability\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1ea65105a9eec44f571711f2ab73e0df8f0031a0"},"cell_type":"markdown","source":"At first glance, we can say that if passengers happened to have a relatively small family on the Titanic, they were more likely to survive. We have to stay careful though because 3 and 5 have high standard deviations.\n\nLet's now see how the test set is distributed compared to the training set."},{"metadata":{"_uuid":"dd022f3644c6b0eb494c8d551238229154d3213c","trusted":true},"cell_type":"code","source":"# Train set vs Test set\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18,4))\n\nsns.distplot(train_df['Parch'], ax=ax1, color='#84B044')\nax1.set_title('Train set')\n\nsns.distplot(test_df['Parch'], ax=ax2, color='#84B044')\nax2.set_title('Test set')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dd5a26b7813107c99e2b5655e9ea4162a5d3e4a5"},"cell_type":"markdown","source":"`Parch` looks like it is evenly distributed between both sets but it is quite not the case.\n\nLet's compute the JS divergence for `Parch`, we will compare this value later with other features."},{"metadata":{"_uuid":"1063c5cb7e119fafd8845329725e177d7b3600eb","trusted":true},"cell_type":"code","source":"parch_jsd = JSD(train_df['Parch'].values, test_df['Parch'].values)\nprint('Jensen-Shannon divergence of Parch:', np.mean(parch_jsd))\nprint('Standard deviation:', np.std(parch_jsd))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c55689308a49b8ff17c3842922cc47de98d319ba"},"cell_type":"markdown","source":"**Conclusion:** we can use `Parch` as is or mix it with other features."},{"metadata":{"_uuid":"f721cfc63fdd76ab56077ace8b3c423b0395ff39"},"cell_type":"markdown","source":"#### SibSp"},{"metadata":{"_uuid":"51f0f6b49c86a8ebe5cf6dd4bfb3fba08aeee472","trusted":true},"cell_type":"code","source":"palette7 = [\"#F7BBA6\", \"#ED8495\", \"#E05286\", \"#A73B8F\", \"#6F2597\", \"#511B75\", \"#37114E\"]\n# SibSp feature vs Survived\ng = sns.catplot(x='SibSp', y='Survived', saturation=5, height=4, aspect=4, data=train_df, kind='bar', palette=palette7)\ng.despine(left=True)\ng = g.set_ylabels(\"Survival probability\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c997b3b3dc66a985642563538a20f13e00fddb6d"},"cell_type":"markdown","source":"It seems that single passengers or with two other persons had more chance to survive.\n\nLet's now see how the test set is distributed compared to the training set."},{"metadata":{"_uuid":"0ad0da19ffc3de981ce5d668e7e2d683cbe92471","trusted":true},"cell_type":"code","source":"# Train set vs Test set\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18,4))\n\nsns.distplot(train_df['SibSp'], ax=ax1, color='#E76C5D')\nax1.set_title('Train set')\n\nsns.distplot(test_df['SibSp'], ax=ax2, color='#E76C5D')\nax2.set_title('Test set')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9c9e6c53cdd5e6cd52d3987a6b1b10c8902d59c9"},"cell_type":"markdown","source":"`SibSp` looks like it is evenly distributed between both sets but it is quite not the case.\n\nLet's compute the JS divergence for `SibSp`, we will compare this value later with other features."},{"metadata":{"_uuid":"599bd6e09338652a947aaeea1d01a897757033f9","trusted":true},"cell_type":"code","source":"sibsp_jsd = JSD(train_df['SibSp'].values, test_df['SibSp'].values)\nprint('Jensen-Shannon divergence of SibSp:', np.mean(sibsp_jsd))\nprint('Standard deviation:', np.std(sibsp_jsd))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"166b1d541c825f6708a9092df8c21746a426a0f0"},"cell_type":"markdown","source":"**Conclusion:** we can use `SibSp` as is or mix it with other features."},{"metadata":{"_uuid":"33ac1826b0642bee9294c7dc437924e6b7d1120a"},"cell_type":"markdown","source":"#### Differences between the distributions of the train set and the test set\nBy looking at the JS divergence, we can tell how the distributions of invidual features differ. Keep in mind that it is ok to observe some divergence."},{"metadata":{"_uuid":"1514fa5fdb930d397d840e108f3ebc40364f54df","trusted":true},"cell_type":"code","source":"palette4 = [\"#F19A9B\", \"#D54D88\", \"#7B2A95\", \"#461765\"]\nfig, ax = plt.subplots(figsize=(18,4))\njsd = pd.DataFrame(np.column_stack([age_jsd, fare_jsd, parch_jsd, sibsp_jsd]), columns=['Age', 'Fare', 'Parch', 'SibSp'])\nsns.boxplot(data=jsd, ax=ax, orient=\"h\", linewidth=1, saturation=5, palette=palette4)\nax.set_title('Jensen-Shannon divergences of numerical features')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"041a2309e4816f24ded518419b2b43a658d85943"},"cell_type":"markdown","source":"### Bivariate analysis\nLet's then see if there is an impact of a feature on another.\n\n#### Age vs Fare"},{"metadata":{"_uuid":"dfbd84d78b3daec6501d09bf7416d2bb3f1ecb5f","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18, 4))\nplt.scatter(train_df['Age'], train_df['Fare'], c=train_df['Survived'].values, cmap='cool')\nplt.xlabel('Age')\nplt.ylabel('Fare')\nplt.title('Age vs Fare')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"65ffca90c86ff42dbdce49438ad674855c4f3bc6","trusted":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18,4))\n\nsns.regplot(x='Age', y='Fare', ax=ax1, data=train_df)\nax1.set_title('Train set')\nsns.regplot(x='Age', y='Fare', ax=ax2, data=test_df)\nax2.set_title('Test set')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"87b755bd2e3747143cdee11231ca167a2181cc01","trusted":true},"cell_type":"code","source":"print('PCC for the train set: ', corr_train['Age']['Fare'])\nprint('PCC for the test set: ', corr_test['Age']['Fare'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"834f886c0a2d216c09d21b74c3c3cdb54f8201e8"},"cell_type":"markdown","source":"**Conclusion:** `Age` and `Fare` tend to be much more linearly correlated on the test set than on the train set. (remember that the `Fare` distribution is skewed though."},{"metadata":{"_uuid":"00b6f039ab8c549e13b751451a0c0b864c92a53a"},"cell_type":"markdown","source":"#### Age vs Parch"},{"metadata":{"_uuid":"f0e603679cbf1914813fa034aea385bc5e565368","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18, 4))\nplt.scatter(train_df['Age'], train_df['Parch'], c=train_df['Survived'].values, cmap='cool')\nplt.xlabel('Age')\nplt.ylabel('Parch')\nplt.title('Age vs Parch')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6a87421fbe3cc10327794ed937cf74983293ea09","trusted":true},"cell_type":"code","source":"palette8 = [\"#F8C1A8\", \"#EF9198\", \"#E8608A\", \"#C0458A\", \"#8F3192\", \"#63218F\", \"#4B186C\", \"#33104A\"]\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18,4))\n\nsns.boxplot(y='Age', x='Parch', ax=ax1, data=train_df, linewidth=1, saturation=5, palette=palette7)\nax1.set_title('Train set')\nsns.boxplot(y='Age', x='Parch', ax=ax2, data=test_df, linewidth=1, saturation=5, palette=palette8)\nax2.set_title('Test set')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"51e6eb9f8d5f3bfff162be6436f31b8fa6e28f67","trusted":true},"cell_type":"code","source":"print('PCC for the train set: ', corr_train['Age']['Parch'])\nprint('PCC for the test set: ', corr_test['Age']['Parch'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"65c26faca44e7a771721a8b1c388c73cdfd97517"},"cell_type":"markdown","source":"**Conclusion:** there are noticeable differences on the distributions of those features between the train set and the test set. It can be stabilized by making age bins though."},{"metadata":{"_uuid":"c0af66b8cd5a8e89c93f789c954e3998748b501f"},"cell_type":"markdown","source":"#### Fare vs Parch"},{"metadata":{"_uuid":"f6f27014e9677f7d5266b59ad353f262859dd175","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18, 4))\nplt.scatter(train_df['Fare'], train_df['Parch'], c=train_df['Survived'].values, cmap='cool')\nplt.xlabel('Fare')\nplt.ylabel('Parch')\nplt.title('Fare vs Parch')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"16045a9f70dce126506fa6b42fb4ead976c9e0b4","trusted":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18,4))\n\nsns.boxplot(y='Fare', x='Parch', ax=ax1, data=train_df, linewidth=1, saturation=5, palette=palette7)\nax1.set_title('Train set')\nsns.boxplot(y='Fare', x='Parch', ax=ax2, data=test_df, linewidth=1, saturation=5, palette=palette8)\nax2.set_title('Test set')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e655d32ae094e2382aa8f77f29b7fbf42cd2e29c","trusted":true},"cell_type":"code","source":"print('PCC for the train set: ', corr_train['Fare']['Parch'])\nprint('PCC for the test set: ', corr_test['Fare']['Parch'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d3adad20985c53d5b3c701343f64f7947de57254"},"cell_type":"markdown","source":"**Conclusion:** although they have similar correlation coefficients, distributions differ between both sets."},{"metadata":{"_uuid":"de653b72700a80f4c8b221094df53acaf8fb4539"},"cell_type":"markdown","source":"#### Fare vs SibSp"},{"metadata":{"_uuid":"00918b116722e366766fa889f03c1fdba049ab1c","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18, 4))\nplt.scatter(train_df['Fare'], train_df['SibSp'], c=train_df['Survived'].values, cmap='cool')\nplt.xlabel('Fare')\nplt.ylabel('SibSp')\nplt.title('Fare vs SibSp')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ad69d385675ad7f22664360eb05bcbc94b0df2a6","trusted":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18,4))\n\nsns.boxplot(y='Fare', x='SibSp', ax=ax1, data=train_df, linewidth=1, saturation=5, palette=palette7)\nax1.set_title('Train set')\nsns.boxplot(y='Fare', x='SibSp', ax=ax2, data=test_df, linewidth=1, saturation=5, palette=palette8)\nax2.set_title('Test set')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"43edbde2914322e177b13041a06a1c35b42d12ee","trusted":true},"cell_type":"code","source":"print('PCC for the train set: ', corr_train['Fare']['SibSp'])\nprint('PCC for the test set: ', corr_test['Fare']['SibSp'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6c1e78a48e429649d3d39167f58a7b2802b812fc"},"cell_type":"markdown","source":"**Conclusion:** although they have similar correlation coefficients, distributions differ between both sets."},{"metadata":{"_uuid":"5a50680cbf551da2b972c86c84b38d74f863aeb9"},"cell_type":"markdown","source":"#### Parch vs SibSp"},{"metadata":{"_uuid":"60f1a9831c4840dade62970e36acae1d50be3100","trusted":true},"cell_type":"code","source":"plt.figure(figsize=(18, 4))\nplt.scatter(train_df['Parch'], train_df['SibSp'], c=train_df['Survived'].values, cmap='cool')\nplt.xlabel('Parch')\nplt.ylabel('SibSp')\nplt.title('Parch vs SibSp')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a3f77f6b4d2e9ff2cb087dd74b132e47e7404862","trusted":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18,4))\n\nsns.boxplot(y='Parch', x='SibSp', ax=ax1, data=train_df, linewidth=1, saturation=5, palette=palette7)\nax1.set_title('Train set')\nsns.boxplot(y='Parch', x='SibSp', ax=ax2, data=test_df, linewidth=1, saturation=5, palette=palette8)\nax2.set_title('Test set')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f5dcbd70c462d2a3ef41a2e86e76518ed42d3585","trusted":true},"cell_type":"code","source":"print('PCC for the train set: ', corr_train['Parch']['SibSp'])\nprint('PCC for the test set: ', corr_test['Parch']['SibSp'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1e0fee945197ecc8226f3a2361320e8e36d3e1dc"},"cell_type":"markdown","source":"**Conclusion:** distributions look quite the same with strong correlation coefficients, we will combine them later."},{"metadata":{"_uuid":"9de1a147314866950b534a0038a616c358325cac"},"cell_type":"markdown","source":"### 3.6 Exploring categorical features\n\n### Univariate analysis\n\nLet's first analyze features individually.\n\n#### Embarked"},{"metadata":{"_uuid":"6b3103da8a8fa0919be47652f84652d7899ac1bb","trusted":true},"cell_type":"code","source":"palette3 = [\"#EE8695\", \"#A73B8F\", \"#501B73\"]\n# Embarked feature vs Survived\ng  = sns.catplot(x='Embarked', y='Survived', saturation=5, height=4, aspect=4, data=train_df, \n                    kind='bar', palette=palette3)\ng.despine(left=True)\ng = g.set_ylabels('Survival probability')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b5395f963797f11afad5f9bc8351896fc782e568"},"cell_type":"markdown","source":"It's curious how an embarkment has an influence on `Survival`, this must be related to another feature and we'll dive in with bivariate analysis."},{"metadata":{"_uuid":"2a406d6b83231b2c1b2763b58ecd3996053a2f46","trusted":true},"cell_type":"code","source":"# Train set vs Test set\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18,4))\n\ntrain_df['Embarked'].value_counts().plot(kind='barh', ax=ax1)\nax1.set_title('Train set')\n\ntest_df['Embarked'].value_counts().plot(kind='barh', ax=ax2)\nax2.set_title('Test set')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"59731f103b28c9f075b79d35b503eb3c8b8389e9"},"cell_type":"markdown","source":"Quite similar distributions between the train set and the test set as we can see.\n\n**Conclusion:** we can use `Embarked` as is or mix it with other features."},{"metadata":{"_uuid":"b84a6be72e975219d0d1b9421f25b553fb18c12c"},"cell_type":"markdown","source":"#### Sex\nEveryone watched *Titanic*, we all know that women were more likely to survive this disaster."},{"metadata":{"_uuid":"fc2232772705ba0a3cb10dfe676b70f026581aec","trusted":true},"cell_type":"code","source":"palette2 = [\"#EE8695\", \"#A73B8F\"]\n# Sex feature vs Survived\ng  = sns.catplot(x='Sex', y='Survived', saturation=5, height=4, aspect=4, data=train_df, \n                    kind='bar', palette=palette2)\ng.despine(left=True)\ng = g.set_ylabels('Survival probability')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8dd312811293743b9179dab9bbf8b18658dacad1","trusted":true},"cell_type":"code","source":"# Train set vs Test set\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18,4))\n\ntrain_df['Sex'].value_counts().plot(kind='barh', ax=ax1)\nax1.set_title('Train set')\n\ntest_df['Sex'].value_counts().plot(kind='barh', ax=ax2)\nax2.set_title('Test set')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e884c7eb673ddf7a7697806a8627a274ef81777b"},"cell_type":"markdown","source":"Quite similar distributions between the train set and the test set as we can see.\n\n**Conclusion:** we can use `Sex` as is or mix it with other features."},{"metadata":{"_uuid":"a1fb4fbe8e1c514b76e56c4d8698bbcfe10e0161"},"cell_type":"markdown","source":"#### Pclass"},{"metadata":{"_uuid":"8b46b2be1014364e002cbc14d85d4202cdf58860","trusted":true},"cell_type":"code","source":"# Pclass feature vs Survived\ng  = sns.catplot(x='Pclass', y='Survived', saturation=5, height=4, aspect=4, data=train_df, \n                    kind='bar', palette=palette3)\ng.despine(left=True)\ng = g.set_ylabels('Survival probability')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7b5541b1c1436b389659c56799f63399f538c2a5"},"cell_type":"markdown","source":"Wealthier passengers had more influence on the Titanic, it appears that they were more likely to find a place on a lifeboat."},{"metadata":{"_uuid":"f1375bc8398acce730addc041fc320599cca3068","trusted":true},"cell_type":"code","source":"# Train set vs Test set\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18,4))\n\ntrain_df['Pclass'].value_counts().plot(kind='barh', ax=ax1)\nax1.set_title('Train set')\n\ntest_df['Pclass'].value_counts().plot(kind='barh', ax=ax2)\nax2.set_title('Test set')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1e98d6fea1e35bed00fdb0de9652e567cd1cb509"},"cell_type":"markdown","source":"Quite similar distributions between the train set and the test set as we can see.\n\n**Conclusion:** we can use `Pclass` as is or mix it with other features."},{"metadata":{"_uuid":"9c26a846802c35f59d0fd2a7a30192d4956c0adf"},"cell_type":"markdown","source":"### Bivariate analysis\nLet's then see if there is an impact of a feature on another.\n\n#### Continuous and categorical variables\n\nWhen dealing with continuous and categorical variables, we can look at statistical significance through variance analysis (**ANOVA**).\n\nIf we denote by $k_i$ the ith value for the continuous variable in the group, $n$ the number of passengers in each group, $T$ the sum of the continuous variable's values for all passengers and $N$ the number of passengers ; we can define $SS_{between}$ the *Sum of Squares Between*:\n\n$$SS_{between} = \\frac{\\sum(\\sum k_i)²}{n} - \\frac{T²}{N}$$\n\nIf we denote by $Y$ a value of the continuous variable ; we can define $SS_{total}$ the *Sum of Squares Total*:\n\n$$SS_{total} = \\sum Y² - \\frac{T²}{N}$$\n\nWe then have access to the *effect size* $\\eta²$ which tells us how much the group has influenced the variable:\n\n$$\\eta² = \\frac{SS_{between}}{SS_{total}}$$\n\nFor the value of $\\eta²$, we will refer to *Cohen's d* guidelines which are as follows:\n* Small effect: 0.01\n* Medium effect: 0.059\n* Large effect: 0.138"},{"metadata":{"_uuid":"d0caf65ea85c87de5e358702189e80715f580718","trusted":true},"cell_type":"code","source":"import statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n\ndef compute_anova(dataset, group, weight):\n    \"\"\"\n        Computes the effect size through ANOVA.\n        \n        :param dataset: pandas DataFrame\n        :param group: categorical feature\n        :param weight: continuous feature\n        :return: effect size\n    \"\"\"\n    mod = ols(weight + ' ~ ' + group, data=dataset).fit()\n    aov_table = sm.stats.anova_lm(mod, typ=2)\n    esq_sm = aov_table['sum_sq'][0]/(aov_table['sum_sq'][0]+aov_table['sum_sq'][1])\n    \n    return esq_sm","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"145f799fe0e1531878a1884708b21f14645d18dc"},"cell_type":"markdown","source":"#### Continuous and continuous variables\nWhen dealing with two continuous variables, we can look at statistical independence through the $\\chi^2$ test. \n\nIn its general statement, if there are $r$ rows and $c$ columns in the dataset, the *theoretical frequency* for a value, given the hypothesis of independence, is:\n\n$$E_{{i,j}}=Np_{{i\\cdot }}p_{{\\cdot j}}$$\n\nwhere $N$ is the total sample size, and:\n\n$$p_{{i\\cdot }}={\\frac  {O_{{i\\cdot }}}{N}}=\\sum _{{j=1}}^{c}{\\frac  {O_{{i,j}}}{N}}$$\n\nis the fraction of observations of type $i$ ignoring the column attribute, and:\n\n$${\\displaystyle p_{\\cdot j}={\\frac {O_{\\cdot j}}{N}}=\\sum _{i=1}^{r}{\\frac {O_{i,j}}{N}}}$$\n\nis the fraction of observations of type $j$ ignoring the row attribute. The term *frequencies* refers to absolute numbers rather than already normalised values.\n\nThe value of the test-statistic is:\n\n$$\\chi ^{2}=\\sum _{{i=1}}^{{r}}\\sum _{{j=1}}^{{c}}{(O_{{i,j}}-E_{{i,j}})^{2} \\over E_{{i,j}}} = N\\sum _{{i,j}}p_{{i\\cdot }}p_{{\\cdot j}}\\left({\\frac  {(O_{{i,j}}/N)-p_{{i\\cdot }}p_{{\\cdot j}}}{p_{{i\\cdot }}p_{{\\cdot j}}}}\\right)^{2}$$\n\nThe null hypothesis $H_0$ is that the two variables are independent. We will then also look at the *p-value*. ($H_0$ rejected if $p \\leq 0.05$)"},{"metadata":{"_uuid":"3dfe4e19c174b2d1e9fa4a52715340ac868c71d5","trusted":true},"cell_type":"code","source":"from scipy.stats import chi2_contingency\n\ndef chisq(dataset, c1, c2):\n    \"\"\"\n        Performs the Chi squared independence test.\n        \n        :param dataset: pandas DataFrame\n        :param c1: continuous feature 1\n        :param c2: continuous feature 2\n        :return: array with [Chi^2, p-value]\n    \"\"\"\n    groupsizes = dataset.groupby([c1, c2]).size()\n    ctsum = groupsizes.unstack(c1)\n\n    result = chi2_contingency(ctsum.fillna(0))\n    \n    print('Chi^2:', result[0])\n    print('p-value:', result[1])\n    print('Degrees of freedom:', result[2])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c03edc0947c718dbe6acd10b01e1a9f94498e4d4"},"cell_type":"markdown","source":"#### Embarked vs Age"},{"metadata":{"_uuid":"de7cd2af11cbd17b37f293b54b579989778e5d00","trusted":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18,4))\n\nsns.boxplot(y='Age', x='Embarked', ax=ax1, data=train_df, linewidth=1, saturation=5, order=['S', 'C', 'Q'], palette=palette3)\nax1.set_title('Train set')\nsns.boxplot(y='Age', x='Embarked', ax=ax2, data=test_df, linewidth=1, saturation=5, order=['S', 'C', 'Q'], palette=palette3)\nax2.set_title('Test set')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cdebb4c0b67cd4bf852d34a5a98d6392cfe920be","trusted":true},"cell_type":"code","source":"train_esq_sm = compute_anova(train_df, 'Embarked', 'Age')\ntest_esq_sm = compute_anova(test_df, 'Embarked', 'Age')\n\nprint('ANOVA 1-way for the train set: ', train_esq_sm)\nprint('ANOVA 1-way for the test set: ', test_esq_sm)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fbd2d04a080e095fc523cb8a989d1aff488fa2fd"},"cell_type":"markdown","source":"For the **train set**, the effect of `Embarked` on `Age` is **low** (0.0019).\n\nFor the **test set**, the effect of `Embarked` on `Age` is **low/medium** (0.0327).\n\n**Conclusion:** the effect of `Embarked` on `Age` differs for about **3%** between the two sets."},{"metadata":{"_uuid":"dde53a062def0da5bd99d075a4ec8849827bbfdf"},"cell_type":"markdown","source":"#### Embarked vs Fare"},{"metadata":{"_uuid":"345e03000f9e8f27acd05a8d3cb0a1a4716556f7","trusted":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18,4))\n\nsns.boxplot(y='Fare', x='Embarked', ax=ax1, data=train_df, linewidth=1, saturation=5, order=['S', 'C', 'Q'], palette=palette3)\nax1.set_title('Train set')\nsns.boxplot(y='Fare', x='Embarked', ax=ax2, data=test_df, linewidth=1, saturation=5, order=['S', 'C', 'Q'], palette=palette3)\nax2.set_title('Test set')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f1c4f27c972a048ca6aeb9ba3fe458488eace155","trusted":true},"cell_type":"code","source":"train_esq_sm = compute_anova(train_df, 'Embarked', 'Fare')\ntest_esq_sm = compute_anova(test_df, 'Embarked', 'Fare')\n\nprint('ANOVA 1-way for the train set: ', train_esq_sm)\nprint('ANOVA 1-way for the test set: ', test_esq_sm)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a463a55146493e598f4cfadf65419092783519e9"},"cell_type":"markdown","source":"For the **train set**, the effect of `Embarked` on `Fare` is **medium/high** (0.0823).\n\nFor the **test set**, the effect of `Embarked` on `Fare` is **medium/high** (0.1064).\n\n**Conclusion:** the effect of `Embarked` on `Fare` differs for about **2.4%** between the two sets."},{"metadata":{"_uuid":"db1040e34d2cec5428813f05717d6e03c2a3dff6"},"cell_type":"markdown","source":"#### Embarked vs Parch\nLet's first write a quick function to plot the proportion of `Embarked` by another discrete variable."},{"metadata":{"_uuid":"438cf969f8624abac4c40020e8544cd56a26d23f","trusted":true},"cell_type":"code","source":"def plot_embarked_variable(dataset, variable):\n    \"\"\"\n        Plots the proportion of variable values per Embarked value of a dataset.\n        \n        :param dataset: pandas DataFrame\n        :param variable: variable to plot\n    \"\"\"\n    s_variable_index = dataset.groupby(['Embarked', variable]).size()['S'].index.values\n    c_variable_index = dataset.groupby(['Embarked', variable]).size()['C'].index.values\n    q_variable_index = dataset.groupby(['Embarked', variable]).size()['Q'].index.values\n\n    index = list(set().union(s_variable_index,c_variable_index,q_variable_index))\n\n    raw_s_variable = dataset.groupby(['Embarked', variable]).size()['S']\n    raw_c_variable = dataset.groupby(['Embarked', variable]).size()['C']\n    raw_q_variable = dataset.groupby(['Embarked', variable]).size()['Q']\n\n    s_variable = []\n    c_variable = []\n    q_variable = []\n\n    for i in range(max(index) + 1):\n        s_variable.append(raw_s_variable[i] if i in s_variable_index else 0)\n        c_variable.append(raw_c_variable[i] if i in c_variable_index else 0)\n        q_variable.append(raw_q_variable[i] if i in q_variable_index else 0)\n\n    percent_s_variable = [s_variable[i]/(s_variable[i] + c_variable[i] + q_variable[i]) if i in index else 0 for i in range(max(index) + 1)]\n    percent_c_variable = [c_variable[i]/(s_variable[i] + c_variable[i] + q_variable[i]) if i in index else 0 for i in range(max(index) + 1)]\n    percent_q_variable = [q_variable[i]/(s_variable[i] + c_variable[i] + q_variable[i]) if i in index else 0 for i in range(max(index) + 1)]\n\n    r = list(range(max(index) + 1))\n    bars = [sum(x) for x in zip(percent_s_variable, percent_c_variable)]\n\n    fig, axs = plt.subplots(1,1,figsize=(18,4))\n    plt.bar(r, percent_s_variable, color='#08c299')\n    plt.bar(r, percent_c_variable, bottom=percent_s_variable, linewidth=0, color='#97de95')\n    plt.bar(r, percent_q_variable, bottom=bars, linewidth=0, color='#fce8aa')\n    plt.xticks(r, r)\n    plt.title('Proportion of Embarked values by ' + variable)\n    axs.legend(labels=['S', 'C', 'Q'])\n    axs.yaxis.set_major_formatter(PercentFormatter(xmax=1))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"515f7d3aab120d00e3025bcb5e73e982135a8bf2"},"cell_type":"markdown","source":"Train set:"},{"metadata":{"_uuid":"8ebf6312609ab342e2f28fae98ae848a65d98531","trusted":true},"cell_type":"code","source":"plot_embarked_variable(train_df, 'Parch')\nchisq(train_df, 'Embarked', 'Parch')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3147e0c70126a609fb9b6b99672d4f2e493a14f8"},"cell_type":"markdown","source":"Test set:"},{"metadata":{"_uuid":"c2969501f66395c3bfddedd94e09204706b18f13","trusted":true},"cell_type":"code","source":"plot_embarked_variable(test_df, 'Parch')\nchisq(test_df, 'Embarked', 'Parch')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bcf2359ca14f26beb7562e0b91fdad7dfbcc6363"},"cell_type":"markdown","source":"**Conclusion:** It is worth noticing that `Embarked` and `Parch` **are not** considered independent on the test set but they **are** on the train set."},{"metadata":{"_uuid":"08fe857bf0b39a73eebf74b065816e3a482b8316"},"cell_type":"markdown","source":"#### Embarked vs SibSp\nTrain set:"},{"metadata":{"_uuid":"33c31c0765c01ba6666978471b33bdc4e0993f85","trusted":true},"cell_type":"code","source":"plot_embarked_variable(train_df, 'SibSp')\nchisq(train_df, 'Embarked', 'SibSp')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ea1e428852b014c04f004a07fe18cfed90f2c80d"},"cell_type":"markdown","source":"Test set:"},{"metadata":{"_uuid":"b1ef1f6f7ea6c7a0a3e61e82e1115564a81d9e0d","trusted":true},"cell_type":"code","source":"plot_embarked_variable(test_df, 'SibSp')\nchisq(test_df, 'Embarked', 'SibSp')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ba08eb7927fc7b40329c217c2ff3022033ce91cb"},"cell_type":"markdown","source":"**Conclusion:** `Embarked` and `SibSp` are not considered independent on the train set but they are on the test set."},{"metadata":{"_uuid":"f722164206fa67d2fdb03fbdd3c6a8dd514413b5"},"cell_type":"markdown","source":"#### Embarked vs Sex"},{"metadata":{"_uuid":"552d2ef3ac21e9a9229de60b87a331d58566f141","trusted":true},"cell_type":"code","source":"tmp_train_df = train_df.copy(deep=True)\ntmp_train_df['Sex'].replace(['male', 'female'], [0,1], inplace=True)\nplot_embarked_variable(tmp_train_df, 'Sex')\nchisq(tmp_train_df, 'Embarked', 'Sex')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a1a51d1417ed53748fa62c8e5d3d2c3d5630e8dc","trusted":true},"cell_type":"code","source":"tmp_test_df = test_df.copy(deep=True)\ntmp_test_df['Sex'].replace(['male', 'female'], [0,1], inplace=True)\nplot_embarked_variable(tmp_test_df, 'Sex')\nchisq(tmp_test_df, 'Embarked', 'Sex')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"acf9bee0988fd3187cbf596b23ce766734a4b8dc"},"cell_type":"markdown","source":"It appears that on both sets, the proportion of male is higher from Southampton (S), thus influencing `Survival`.\n\n**Conclusion:** `Embarked` and `Sex` **are not** considered independent both on the train set and test set."},{"metadata":{"_uuid":"d825c4edb368abe0f30b1e8d8693150169c24f5d"},"cell_type":"markdown","source":"#### Embarked vs Pclass\nTrain set:"},{"metadata":{"_uuid":"cdca36f5b52159b081624d5a990b946a9ae9c02b","trusted":true},"cell_type":"code","source":"plot_embarked_variable(train_df, 'Pclass')\nchisq(train_df, 'Embarked', 'Pclass')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5b56051f7917416df72add773b7bdc8a5b790499"},"cell_type":"markdown","source":"Test set:"},{"metadata":{"_uuid":"77135f02fb1b5afab3c72281fc6ca621bf349228","trusted":true},"cell_type":"code","source":"plot_embarked_variable(test_df, 'Pclass')\nchisq(test_df, 'Embarked', 'Pclass')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0e074767f8bc453e7fa8cd4a520390a09fb74970"},"cell_type":"markdown","source":"It appears that the proportion of whealthy people is higher from Cherbourg (C), thus influencing `Survival`.\n\n**Conclusion:** `Embarked` and `Pclass` are considered **strongly dependent** both on the train set and test set."},{"metadata":{"_uuid":"fdab400f8a756a5c59ead69d6dbdb59bc6c18239"},"cell_type":"markdown","source":"#### Sex vs Age"},{"metadata":{"_uuid":"27e88115937406d80ad42cb76bbf8220dfe80ae1","trusted":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18,4))\n\nsns.boxplot(y='Age', x='Sex', ax=ax1, data=train_df, linewidth=1, saturation=5, palette=palette2)\nax1.set_title('Train set')\nsns.boxplot(y='Age', x='Sex', ax=ax2, data=test_df, linewidth=1, saturation=5, palette=palette2)\nax2.set_title('Test set')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a365bc582239241af3ed6556926b4b4b7ab6f3a8","trusted":true},"cell_type":"code","source":"train_esq_sm = compute_anova(train_df, 'Sex', 'Age')\ntest_esq_sm = compute_anova(test_df, 'Sex', 'Age')\n\nprint('ANOVA 1-way for the train set: ', train_esq_sm)\nprint('ANOVA 1-way for the test set: ', test_esq_sm)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"78e631a5c07c23794d7238cfc400d6ef0b377a8a"},"cell_type":"markdown","source":"For the **train set**, the effect of `Sex` on `Age` is **low** (0.0086).\n\nFor the **test set**, the effect of `Sex` on `Age` is **low** (1.6084e-10).\n\n**Conclusion:** the effect of `Sex` on `Age` differs for less than **1%** between the two sets."},{"metadata":{"_uuid":"2fef73c865241dbd75b4aa3dfc7665da4ade5e00"},"cell_type":"markdown","source":"#### Sex vs Fare"},{"metadata":{"_uuid":"3674cba10144e4045123fe688374c6cbdc44864c","trusted":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18,4))\n\nsns.boxplot(y='Fare', x='Sex', ax=ax1, data=train_df, linewidth=1, saturation=5, palette=palette2)\nax1.set_title('Train set')\nsns.boxplot(y='Fare', x='Sex', ax=ax2, data=test_df, linewidth=1, saturation=5, palette=palette2)\nax2.set_title('Test set')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d3b037f741e23f2b858bb61227705b4051718ef2","trusted":true},"cell_type":"code","source":"train_esq_sm = compute_anova(train_df, 'Sex', 'Fare')\ntest_esq_sm = compute_anova(test_df, 'Sex', 'Fare')\n\nprint('ANOVA 1-way for the train set: ', train_esq_sm)\nprint('ANOVA 1-way for the test set: ', test_esq_sm)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ddb1424038171ed7252b2215fe6e7ab4801d5e77"},"cell_type":"markdown","source":"For the **train set**, the effect of `Sex` on `Fare` is **low/medium** (0.0332).\n\nFor the **test set**, the effect of `Sex` on `Fare` is **low/medium** (0.0367).\n\n**Conclusion:** the effect of `Sex` on `Fare` differs for less than **1%** between the two sets."},{"metadata":{"_uuid":"c5758b920cf7f2031834eb2b36bd7fb0c30b9c89"},"cell_type":"markdown","source":"#### Sex vs Parch\nLet's first write a quick function to plot the proportion of `Sex` by another discrete variable."},{"metadata":{"_uuid":"2dbfd0bc122bcf8ea0caae2f5d3162077b3bb8b0","trusted":true},"cell_type":"code","source":"def plot_sex_variable(dataset, variable):\n    \"\"\"\n        Plots the proportion of variable values per Sex value of a dataset.\n        \n        :param dataset: pandas DataFrame\n        :param variable: variable to plot\n    \"\"\"\n    male_variable_index = dataset.groupby(['Sex', variable]).size()['male'].index.values\n    female_variable_index = dataset.groupby(['Sex', variable]).size()['female'].index.values\n\n    index = list(set().union(male_variable_index, female_variable_index))\n\n    raw_male_variable = dataset.groupby(['Sex', variable]).size()['male']\n    raw_female_variable = dataset.groupby(['Sex', variable]).size()['female']\n\n    male_variable = []\n    female_variable = []\n\n    for i in range(max(index) + 1):\n        male_variable.append(raw_male_variable[i] if i in male_variable_index else 0)\n        female_variable.append(raw_female_variable[i] if i in female_variable_index else 0)\n\n    percent_male_variable = [male_variable[i]/(male_variable[i] + female_variable[i]) if i in index else 0 for i in range(max(index) + 1)]\n    percent_female_variable = [female_variable[i]/(male_variable[i] + female_variable[i]) if i in index else 0 for i in range(max(index) + 1)]\n\n    r = list(range(max(index) + 1))\n\n    fig, axs = plt.subplots(1,1,figsize=(18,4))\n    plt.bar(r, percent_male_variable, color='#ce2525')\n    plt.bar(r, percent_female_variable, bottom=percent_male_variable, linewidth=0, color='#ff6600')\n    plt.xticks(r, r)\n    plt.title('Proportion of Sex values by ' + variable)\n    axs.legend(labels=['male', 'female'])\n    axs.yaxis.set_major_formatter(PercentFormatter(xmax=1))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3ae2fa361bba3ef06701cd29e8f117b6e1c555ba"},"cell_type":"markdown","source":"Train set:"},{"metadata":{"_uuid":"4fe3210a2d74e8c374107b933738039c75cc988b","trusted":true},"cell_type":"code","source":"plot_sex_variable(train_df, 'Parch')\nchisq(train_df, 'Sex', 'Parch')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6201b8c406a048ca3cc4e958e7aa5e6ec58f585f"},"cell_type":"markdown","source":"Test set:"},{"metadata":{"_uuid":"b3e1d40336e225490f13479a2932d88ffb04a74b","trusted":true},"cell_type":"code","source":"plot_sex_variable(test_df, 'Parch')\nchisq(test_df, 'Sex', 'Parch')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b7bca3b8b3b70be698049ccb2fa58b10d11db326"},"cell_type":"markdown","source":"**Conclusion:** `Sex` and `Parch` are considered **strongly dependent** both on the train set and test set."},{"metadata":{"_uuid":"833561e8cc32b8d82dc7c62a5e32af8402e0a1b4"},"cell_type":"markdown","source":"#### Sex vs SibSp\nTrain set:"},{"metadata":{"_uuid":"4eb4e33ba9e7678c93e960cd4caf2c8da814fb48","trusted":true},"cell_type":"code","source":"plot_sex_variable(train_df, 'SibSp')\nchisq(train_df, 'Sex', 'SibSp')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3f213ad0c64280c4e36802233af4c327b73a74e8"},"cell_type":"markdown","source":"Test set:"},{"metadata":{"_uuid":"9371043c64b60512ad2257f0454549701d83b603","trusted":true},"cell_type":"code","source":"plot_sex_variable(test_df, 'SibSp')\nchisq(test_df, 'Sex', 'SibSp')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"78a6683447fd44e7900f42e1741540198809e875"},"cell_type":"markdown","source":"**Conclusion:** `Sex` and `SibSp` are considered **strongly dependent** both on the train set and test set."},{"metadata":{"_uuid":"47b8c5760d83b4c661e8dd6328fe15d925dbadb8"},"cell_type":"markdown","source":"#### Sex vs Pclass\nTrain set:"},{"metadata":{"_uuid":"37241498f23097526181ce2aac83f5f4d81853fe","trusted":true},"cell_type":"code","source":"plot_sex_variable(train_df, 'Pclass')\nchisq(train_df, 'Sex', 'Pclass')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d29dad67f7c233bf7f71f2cfb04b847f443dfa41"},"cell_type":"markdown","source":"Test set:"},{"metadata":{"_uuid":"8adb0a106fe5ba0f694e917e5a3912ee2038d4fb","trusted":true},"cell_type":"code","source":"plot_sex_variable(test_df, 'Pclass')\nchisq(test_df, 'Sex', 'Pclass')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a0df8a940b3420bfbdf641006d9e361cf8ec973a"},"cell_type":"markdown","source":"**Conclusion:** `Sex` and `Pclass` are considered **strongly dependent** both on the train set and test set."},{"metadata":{"_uuid":"3b3d02b6434f76800c60d40a01e212416ec09df6"},"cell_type":"markdown","source":"#### Pclass vs Age"},{"metadata":{"_uuid":"fb38424a08cfbfe26f0f1a6566d0bba8e4d75d53","trusted":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18,4))\n\nsns.boxplot(y='Age', x='Pclass', ax=ax1, data=train_df, linewidth=1, saturation=5, palette=palette3)\nax1.set_title('Train set')\nsns.boxplot(y='Age', x='Pclass', ax=ax2, data=test_df, linewidth=1, saturation=5, palette=palette3)\nax2.set_title('Test set')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1577bc89dd0c0128be782810fa03c87f874ae8db","trusted":true},"cell_type":"code","source":"train_esq_sm = compute_anova(train_df, 'Age', 'Pclass')\ntest_esq_sm = compute_anova(test_df, 'Age', 'Pclass')\n\nprint('ANOVA 1-way for the train set: ', train_esq_sm)\nprint('ANOVA 1-way for the test set: ', test_esq_sm)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2bdb9b6cf80bb698e9d6a6b1548957f55098f22a"},"cell_type":"markdown","source":"For the **train set**, the effect of `Pclass` on `Age` is **medium/high** (0.1363).\n\nFor the **test set**, the effect of `Pclass` on `Age` is **high** (0.2422).\n\n**Conclusion:** the effect of `Pclass` on `Age` differs for about **11%** between the two sets."},{"metadata":{"_uuid":"63cb9283da63ed456e7a56b8dc2246583aa9cefb"},"cell_type":"markdown","source":"#### Pclass vs Fare"},{"metadata":{"_uuid":"a7dc459b3491fea84ff7be2d56f0e98366e43d66","trusted":true},"cell_type":"code","source":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18,4))\n\nsns.boxplot(y='Fare', x='Pclass', ax=ax1, data=train_df, linewidth=1, saturation=5, palette=palette3)\nax1.set_title('Train set')\nsns.boxplot(y='Fare', x='Pclass', ax=ax2, data=test_df, linewidth=1, saturation=5, palette=palette3)\nax2.set_title('Test set')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"39be4e506c63255cc739c1ae72cddf813bc63879","trusted":true},"cell_type":"code","source":"train_esq_sm = compute_anova(train_df, 'Fare', 'Pclass')\ntest_esq_sm = compute_anova(test_df, 'Fare', 'Pclass')\n\nprint('ANOVA 1-way for the train set: ', train_esq_sm)\nprint('ANOVA 1-way for the test set: ', test_esq_sm)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"08d77c5cbfb466b1cb19ec9d665ba52f008b879a"},"cell_type":"markdown","source":"For the **train set**, the effect of `Pclass` on `Fare` is **high** (0.3019).\n\nFor the **test set**, the effect of `Pclass` on `Fare` is **high** (0.3331).\n\n**Conclusion:** the effect of `Pclass` on `Age` differs for about **3%** between the two sets."},{"metadata":{"_uuid":"3a8a81ca450690435e21e399ac041d6c224fc256"},"cell_type":"markdown","source":"#### Pclass vs Parch\nLet's first write a quick function to plot the proportion of `Pclass` by another discrete variable."},{"metadata":{"_uuid":"1f45d3b13386451384706543987f3883e6458048","trusted":true},"cell_type":"code","source":"def plot_pclass_variable(dataset, variable):\n    \"\"\"\n        Plots the proportion of variable values per Pclass value of a dataset.\n        \n        :param dataset: pandas DataFrame\n        :param variable: variable to plot\n    \"\"\"\n    first_variable_index = dataset.groupby(['Pclass', variable]).size()[1].index.values\n    second_variable_index = dataset.groupby(['Pclass', variable]).size()[2].index.values\n    third_variable_index = dataset.groupby(['Pclass', variable]).size()[3].index.values\n\n    index = list(set().union(first_variable_index, second_variable_index, third_variable_index))\n\n    raw_first_variable = dataset.groupby(['Pclass', variable]).size()[1]\n    raw_second_variable = dataset.groupby(['Pclass', variable]).size()[2]\n    raw_third_variable = dataset.groupby(['Pclass', variable]).size()[3]\n\n    first_variable = []\n    second_variable = []\n    third_variable = []\n\n    for i in range(max(index) + 1):\n        first_variable.append(raw_first_variable[i] if i in first_variable_index else 0)\n        second_variable.append(raw_second_variable[i] if i in second_variable_index else 0)\n        third_variable.append(raw_third_variable[i] if i in third_variable_index else 0)\n\n    percent_first_variable = [first_variable[i]/(first_variable[i] + second_variable[i] + third_variable[i]) if i in index else 0 for i in range(max(index) + 1)]\n    percent_second_variable = [second_variable[i]/(first_variable[i] + second_variable[i] + third_variable[i]) if i in index else 0 for i in range(max(index) + 1)]\n    percent_third_variable = [third_variable[i]/(first_variable[i] + second_variable[i] + third_variable[i]) if i in index else 0 for i in range(max(index) + 1)]\n\n    r = list(range(max(index) + 1))\n\n    fig, axs = plt.subplots(1,1,figsize=(18,4))\n    plt.bar(r, percent_first_variable, color='#264e86')\n    plt.bar(r, percent_second_variable, bottom=percent_first_variable, linewidth=0, color='#0074e4')\n    plt.bar(r, percent_third_variable, bottom=percent_second_variable, linewidth=0, color='#74dbef')\n    plt.xticks(r, r)\n    plt.title('Proportion of Pclass values by ' + variable)\n    axs.legend(labels=['1', '2', '3'])\n    axs.yaxis.set_major_formatter(PercentFormatter(xmax=1))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ce97d816c5a7166a6c58dac6bb80617fbf0a2c3e"},"cell_type":"markdown","source":"Train set:"},{"metadata":{"_uuid":"25a89e754156d669c68ea7e35136a62fad43de05","trusted":true},"cell_type":"code","source":"plot_pclass_variable(train_df, 'Parch')\nchisq(train_df, 'Pclass', 'Parch')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"423121e2fdb40622a5bde99191e267513661f423"},"cell_type":"markdown","source":"Test set:"},{"metadata":{"_uuid":"0d66921c064074ff389466596fb0f51d0489e22f","trusted":true},"cell_type":"code","source":"plot_pclass_variable(test_df, 'Parch')\nchisq(test_df, 'Pclass', 'Parch')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3ca69e18a6e52c44ba1ba82bedf35056c161772b"},"cell_type":"markdown","source":"We can witness very different distributions when `Parch` is equal to 3 between both sets.\n\n**Conclusion**: `Pclass` and `Parch` are considered **strongly dependent** both on the train set and test set."},{"metadata":{"_uuid":"681c8b40c68d92ee85f508ecf44c8d72780fa8ca"},"cell_type":"markdown","source":"#### Pclass vs SibSp\nTrain set:"},{"metadata":{"_uuid":"c0bc21f2e7edd4b51e520df5bd88be79c7153280","trusted":true},"cell_type":"code","source":"plot_pclass_variable(train_df, 'SibSp')\nchisq(train_df, 'Pclass', 'SibSp')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7cda90728282cccf3ef0cb0039002c77e270d0f2"},"cell_type":"markdown","source":"Test set:"},{"metadata":{"_uuid":"3a6c2abc874440523b4fb054eb2e51ccdbc8a909","trusted":true},"cell_type":"code","source":"plot_pclass_variable(test_df, 'SibSp')\nchisq(test_df, 'Pclass', 'SibSp')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c2e44ecc35d76d809326ea05ea35ed119a0c7dd8"},"cell_type":"markdown","source":"**Conclusion:** contrary to `Parch` which is strongly linearly correlated to `SibSp`, `Pclass` and `SibSp` **are not** considered dependent both on the train set and test set."},{"metadata":{"_uuid":"32ba64a77cee02a82ef750d528ab7c06b6bf4bc4"},"cell_type":"markdown","source":"## Step 4: Preparing the data for consumption <a id=\"step4\"></a>\nWe will proceed with **outliers elimination** and **feature engineering**.\n### 4.1 Outliers elimination\nOutliers are usually bad for model generalization, let's drop a 1% ratio with the **Isolation Forest** algorithm on `Age`, `Fare`, `Parch`, `SibSp`. For more details on the algorithm, feel free to read the original paper: https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf."},{"metadata":{"_uuid":"e5ad5e599e7239d87f4282024d3d0b50e708a625","trusted":true},"cell_type":"code","source":"X_train = train_df[['Age', 'Fare', 'Parch', 'SibSp']].copy(deep=True).dropna()\n\nstd_scaler = StandardScaler()\nX_scaled = std_scaler.fit_transform(X_train)\n\nclf = ensemble.IsolationForest(contamination=0.01)\nclf.fit(X_scaled)\ny_pred = clf.predict(X_scaled)\n\nX_train['isOutlier'] = y_pred\n\noutliers_list = X_train.index[X_train['isOutlier'] == -1].tolist()\n\ndata_df.drop(outliers_list, inplace=True)\ntrain_df.drop(outliers_list, inplace=True)\n\nTRAINING_LENGTH = len(train_df)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4db6efe8726587478f6e733e7eb214c3b0e55c60"},"cell_type":"markdown","source":"#### Deleted outliers (1% ratio)"},{"metadata":{"_uuid":"f66428a1b1a387081b34aab1537f02ed9e960c4f","trusted":true},"cell_type":"code","source":"X_train[X_train['isOutlier'] == -1]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f43f76c9bb6cb56fa2626fb29f3b234c8f13ba69"},"cell_type":"markdown","source":"### 4.2. Feature engineering\nLet's now create and transform existing features to have stable distributions between both sets.\n### Sex\n#### Mapping Sex\nThe `Sex` feature can't be used as is, it has to be mapped to a boolean feature. Let's quickly map the values."},{"metadata":{"_uuid":"c053cf85cbd6de4e774a5da77e940e73382069fb","trusted":true},"cell_type":"code","source":"data_df['Sex'].replace(['male', 'female'], [0,1], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"83e942d8faf5f3024ba07024598b612be856f6da"},"cell_type":"markdown","source":"### Fare\n#### Guessing Fare\nWe will see this below in greater detail but there is a missing value for `Fare`.\nLet's quickly fill this value with the median."},{"metadata":{"_uuid":"57e7121bc9116bbe6b57ec97bd95319a0c91db04","trusted":true},"cell_type":"code","source":"data_df['Fare'].fillna(data_df['Fare'].median(), inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"87ba4c375dbe10bacfb0f037d5cfc703655fd0e1"},"cell_type":"markdown","source":"#### Reducing Fare skewness"},{"metadata":{"_uuid":"d9d70625f60aa6048531db1c34c74af3ad871b7a","trusted":true},"cell_type":"code","source":"# Apply log to Fare to reduce skewness distribution\ndata_df[\"Fare\"] = data_df[\"Fare\"].map(lambda i: np.log(i) if i > 0 else 0)\n\nfig, ax = plt.subplots(figsize=(16,4))\ng = sns.distplot(data_df[\"Fare\"], ax=ax, color='#25627D', label=\"Skewness : %.2f\"%(data_df[\"Fare\"].skew()))\ng = g.legend(loc=\"best\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b4059fe451192f48417be19ec8a3a3a487f4bf06"},"cell_type":"markdown","source":"#### Making Fare bins\nTo help our model better generalize, it often helps to use bins rather than raw values. Let's make `Fare` bins."},{"metadata":{"_uuid":"b7573eeecdc870b1e735551ea03aca90376d4883","trusted":true},"cell_type":"code","source":"data_df['FareBin'] = pd.qcut(data_df['Fare'], 6)\n\nlabel = LabelEncoder()\ndata_df['FareBin_Code'] = label.fit_transform(data_df['FareBin'])\n\ndata_df.drop(['Fare'], 1, inplace=True)\ndata_df.drop(['FareBin'], 1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ed2ad964917e71917dca89f0eb020ae9e9f47900"},"cell_type":"markdown","source":"Let's see if we reduced the divergence between both sets."},{"metadata":{"_uuid":"3359a6b22a393b4f2bfe22acf699ed0e0d60f647","trusted":true},"cell_type":"code","source":"train_df = data_df[:TRAINING_LENGTH]\ntest_df = data_df[TRAINING_LENGTH:]\n\nlogfare_jsd = JSD(train_df['FareBin_Code'].dropna().values, test_df['FareBin_Code'].dropna().values)\nprint('Jensen-Shannon divergence of Fare:', np.mean(logfare_jsd))\nprint('Standard deviation:', np.std(logfare_jsd))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"45ddf311d81099759ee21cc129d6f601716bfd0e","trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(16,4))\njsd = pd.DataFrame(np.column_stack([fare_jsd, logfare_jsd]), columns=['Fare', 'LogFare'])\nsns.boxplot(data=jsd, ax=ax, orient=\"h\", linewidth=1, saturation=5, palette=palette2)\nax.set_title('Jensen-Shannon divergences of Fare and LogFare')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"331b13bb5d6e33584beee895ad1e138ff52b48c7"},"cell_type":"markdown","source":"Great, we **reduced the divergence** between the train set and the test set a bit, making it a more consistent feature."},{"metadata":{"_uuid":"795be29ac552fc7a409892afe07cde777cefffd1"},"cell_type":"markdown","source":"### Ticket\n#### Extracting the prefix\nTicket's numbers may have a prefix that could be an indicator of the booking process (tied to wealth) and/or location on the boat. Let's extract it."},{"metadata":{"_uuid":"9ca5f79a95d8ace618845c312f1e078ba2f69beb","trusted":true},"cell_type":"code","source":"Ticket = []\nfor i in data_df['Ticket'].values:\n    if not i.isdigit() :\n        Ticket.append(i.replace('.', '').replace('', '').strip().split()[0])\n    else:\n        Ticket.append('X')\n        \ndata_df['Ticket'] = Ticket","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"45a47650fbf66e3accea7852a1d259092d2074b9"},"cell_type":"markdown","source":"#### Getting Ticket dummy variables"},{"metadata":{"_uuid":"61adb06f54bfcf44cb3c4e5c7b56bc348f6d98aa","trusted":true},"cell_type":"code","source":"data_df = pd.get_dummies(data_df, columns=['Ticket'], drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cb57e7abc7b143978d5e5f1b2b8b68bea8680d23"},"cell_type":"markdown","source":"### Title\n#### Creating Title\nA feature that often helps categorize in this problem is `Title` derived from `Name`."},{"metadata":{"_uuid":"ba3c8ce125dd593f191c71966e7e6e59d550a926","trusted":true},"cell_type":"code","source":"# Get Title from Name\ntitles = [i.split(',')[1].split('.')[0].strip() for i in data_df['Name']]\ndata_df['Title'] = pd.Series(titles, index=data_df.index)\n\nrare_titles = pd.Series(titles).value_counts()\nrare_titles = rare_titles[rare_titles < 10].index\n\ndata_df['Title'] = data_df['Title'].replace(rare_titles, 'Rare')\ndata_df['Title'] = data_df['Title'].map({'Mr': 0, 'Miss': 1, 'Mrs': 2, 'Master': 3, 'Rare': 4})\ndata_df['Title'] = data_df['Title'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2208f4ac2c1ba52db4615a8ab597531718336cfd"},"cell_type":"markdown","source":"#### Getting Title dummy variables\nGreat, the feature shows some discrimination. It is not ordinal though, lets create dummy variables out of it. (we only need `k-1` columns)"},{"metadata":{"_uuid":"9fbaf65b09a225efe42fd48a26326b0aa964b3b3","trusted":true},"cell_type":"code","source":"data_df = pd.get_dummies(data_df, columns=['Title'], drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5a5c450cb345ff0f152edc339b1a9d42d9ef5a52"},"cell_type":"markdown","source":"### Family_Size\n#### Creating Family_Size\nFinally we are combining `Parch` and `SibSp`: `Family_Size = Parch + SibSp + 1`."},{"metadata":{"_uuid":"16f6ad19caf3d169ff1f582af65e54659626536e","trusted":true},"cell_type":"code","source":"data_df['Family_Size'] = data_df['Parch'] + data_df['SibSp'] + 1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cde0a5e24c07d662cc7b7ac7a0b185f5bb4408d3","trusted":true},"cell_type":"code","source":"tmp_train_df = data_df[:TRAINING_LENGTH].copy(deep=True)\ntmp_test_df = data_df[TRAINING_LENGTH:].copy(deep=True)\n\nfs_jsd = JSD(tmp_train_df['Family_Size'].dropna().values, tmp_test_df['Family_Size'].dropna().values)\nprint('Jensen-Shannon divergence of Family_Size:', np.mean(fs_jsd))\nprint('Standard deviation:', np.std(fs_jsd))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ee86a2dbc68061979a84b98b4134563658c1849f","trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(16,4))\njsd = pd.DataFrame(np.column_stack([parch_jsd, sibsp_jsd, fs_jsd]), columns=['Fare', 'FareBin', 'Family_Size'])\nsns.boxplot(data=jsd, ax=ax, orient=\"h\", linewidth=1, saturation=5, palette=palette3)\nax.set_title('Jensen-Shannon divergences of Parch, SibSp and Family_Size')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6bbc9018ca2f84b7c1498820ec6e567def4614a5"},"cell_type":"markdown","source":"Great, we **reduced the divergence** between the train set and the test set a bit, making it a more consistent feature. We didn't lose much information as we are adding two linearly correlated features."},{"metadata":{"_uuid":"a3c7a0bb662ca164a7d7e32f49fac82f2c98644a"},"cell_type":"markdown","source":"### Missing values\nLet's first have a quick look at missing values in the datasets:"},{"metadata":{"_uuid":"ebd4812e2d6cbbc3a84a4cbadc3fd9167d0cbbf6","trusted":true},"cell_type":"code","source":"print('Train dataset:')\ntrain_df.isnull().sum().to_frame('Missing values').transpose()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f6d7846794c877d84480a7ee8e7650bd587811d6","trusted":true},"cell_type":"code","source":"print('Test/Validation dataset:')\ntest_df.isnull().sum().to_frame('Missing values').transpose()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4f2ce81f5e36433ec1aef3d30a2616fe8df3244a"},"cell_type":"markdown","source":"#### Dropping Name, Parch, SibSp\nLet's drop those features:\n* `Name`: was used to create the `Title` feature\n* `Parch`: was used to create the `Family_Size` feature\n* `SibSp`: was used to create the `Family_Size` feature"},{"metadata":{"_uuid":"26d1421979a7c7b2acba9f231373c4272e8084f1","trusted":true},"cell_type":"code","source":"data_df.drop(['Name', 'Parch', 'SibSp'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"abff21a4239486217958ff1be77f8e3d3baada36"},"cell_type":"markdown","source":"### Embarked\n#### Filling Embarked\nFor 2 missing values, let's fill `Embarked` with the most frequent value."},{"metadata":{"_uuid":"c2cfe7583b33241a9146d4df06e2013708f010d7","trusted":true},"cell_type":"code","source":"data_df['Embarked'].fillna(data_df['Embarked'].mode()[0], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cf20977bbbee6804dbc48deb1f57eda7578bb74a"},"cell_type":"markdown","source":"#### Getting Embarked dummy variables"},{"metadata":{"_uuid":"0c9382e6a9f75df0b0fd619c5b5c2950718fb4fe","trusted":true},"cell_type":"code","source":"data_df = pd.get_dummies(data_df, columns=['Embarked'], drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f48dd7c50bb2936d878592cde4b23fca742d8700"},"cell_type":"markdown","source":"### Deck\n#### Creating Deck\nThere are a lot of missing values for the `Cabin` feature. This can be explained as some passengers didn't even have a cabin. Let's fill `NaN` values with `X` and extract the deck (letter) as it could indicate the location of the passenger's cabin on the boat."},{"metadata":{"_uuid":"bc9ea17df6151662f43e51c58fcfddc76c2bfea2","trusted":true},"cell_type":"code","source":"data_df['Cabin'] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in data_df['Cabin'] ])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"923a17fd3d85c2fd91cec7d93ab12817dbaaf65d","trusted":true},"cell_type":"code","source":"palette9 = [\"#F8C7AA\", \"#F19B9C\", \"#EA708E\", \"#D54D88\", \"#A73B8F\", \"#7A2995\", \"#5B1F84\", \"#451764\", \"#300F45\"]\ng = sns.catplot(x='Cabin', y='Survived',saturation=5, aspect=2.5, data=data_df, kind='bar', order=['A','B','C','D','E','F','G','T','X'], palette=palette9)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d4a97dcd2892e99b1d1c4557d650f622bdb0c90b"},"cell_type":"markdown","source":"#### Getting Deck dummy variables"},{"metadata":{"_uuid":"84eae361bf60560c8f9aa5f46e5e36917c53abb2","trusted":true},"cell_type":"code","source":"data_df = pd.get_dummies(data_df, columns=['Cabin'], prefix='Deck', drop_first=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ecae164ada54fad9c44a0b4570958ea26e458e2"},"cell_type":"markdown","source":"### Age\n#### Guessing the Age\nThe `Age` feature has quite a lot of missing values but it is still manageable. It can easily be completed with the median value (remember, not the mean) but we will rather predict those values with a **MICE imputer** so that it better fits the distributions of the other features.\n\n**Side note:** for some reason it appears that the MICE imputer was removed from `sklearn 0.20`, weird."},{"metadata":{"_uuid":"f61d6b31e3d61d91e72385b00040832a30933a5b","trusted":true},"cell_type":"code","source":"tmp_data_df = data_df.copy(deep = True)[['Age']]\n\nimp = SimpleImputer(missing_values=np.nan, strategy='median')\ntmp_data_df = pd.DataFrame(data=imp.fit_transform(tmp_data_df),index=tmp_data_df.index.values,columns=tmp_data_df.columns.values)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2c43f379984fcdf2b7b0fc45a252684b1242bb01"},"cell_type":"markdown","source":"#### Making Age bins"},{"metadata":{"_uuid":"6014a1261d390977133a313f4db24b75dff631ed","trusted":true},"cell_type":"code","source":"tmp_data_df['AgeBin'] = pd.qcut(tmp_data_df['Age'], 5, duplicates='drop')\ntmp_data_df['AgeBin'].replace(np.NaN, -1, inplace = True)\n\nlabel = LabelEncoder()\ntmp_data_df['AgeBin_Code'] = label.fit_transform(tmp_data_df['AgeBin'])\ntmp_data_df.drop(['Age', 'AgeBin'], axis=1, inplace=True)\n\ndata_df['AgeBin_Code'] = tmp_data_df['AgeBin_Code']\ndata_df.drop(['Age'], 1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d0df578e0a504d8fb375ed79a19f6be34d93189c"},"cell_type":"markdown","source":"Let's then compare the 3 most important features."},{"metadata":{"_uuid":"46b752460dac83c0888a6454ef6ecb58421f6437","scrolled":false,"trusted":true},"cell_type":"code","source":"# Histogram comparison of Sex, Pclass, and Age by Survival\nh = sns.FacetGrid(data_df, row='Sex', col='Pclass', hue='Survived')\nh.map(plt.hist, 'AgeBin_Code', alpha=.75)\nh.add_legend()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1db8484ff8399fc53c7e20a25f738e7bdbbecfdb"},"cell_type":"markdown","source":"### A glance at our dataset"},{"metadata":{"_uuid":"59b18176e84af9e02fcafdd1f14fd78c414a22f0","trusted":true},"cell_type":"code","source":"train_df = data_df[:TRAINING_LENGTH]\ntrain_df.Survived = train_df.Survived.astype(int)\ntest_df = data_df[TRAINING_LENGTH:]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"41eda3f5f35e90de6f15e39cffc7776b113070aa","trusted":true},"cell_type":"code","source":"train_df.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"60172cf578a24ec13f8c4ac4cd3b1e4793419745"},"cell_type":"markdown","source":"### 4.2. Data formatting\n\nLet's create our `X` and `y` and scale them."},{"metadata":{"_uuid":"9b12145f75c643bde1a05763fdc490a778f9654a","trusted":true},"cell_type":"code","source":"X = train_df.drop('Survived', 1)\ny = train_df['Survived']\nX_test = test_df.copy().drop(columns=['Survived'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7a691d731ac6351a437dd0ffece8da7d8efe93c4","trusted":true},"cell_type":"code","source":"std_scaler = StandardScaler()\nX = std_scaler.fit_transform(X)\nX_test = std_scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"12b52115324ceda414a4dcbfea3ced53f5da7145"},"cell_type":"markdown","source":"## Step 5: Modeling the data <a id=\"step5\"></a>\n### 5.1. Model performance with Cross-Validation (CV)\nLet's quickly compare several classification algorithms with default parameters from `scikit-learn`, `xgboost`, `lightgbm` and `catboost` through cross-validation."},{"metadata":{"trusted":true,"_uuid":"261d04769acc2c070610765bf4c22f6c501a4887"},"cell_type":"code","source":"class CatBoostClassifierCorrected(CatBoostClassifier):\n    def fit(self, X, y=None, cat_features=None, sample_weight=None, baseline=None, use_best_model=None, \n            eval_set=None, verbose=None, logging_level=None, plot=False, column_description=None, verbose_eval=None, \n            metric_period=None, silent=None, early_stopping_rounds=None, save_snapshot=None, snapshot_file=None, snapshot_interval=None):\n        # Handle different types of label\n        self.le_ = LabelEncoder().fit(y)\n        transformed_y = self.le_.transform(y)\n\n        self._fit(X=X, y=transformed_y, cat_features=cat_features, pairs=None, sample_weight=sample_weight, group_id=None,\n                  group_weight=None, subgroup_id=None, pairs_weight=None, baseline=baseline, use_best_model=use_best_model, \n                  eval_set=eval_set, verbose=verbose, logging_level=logging_level, plot=plot, column_description=column_description,\n                  verbose_eval=verbose_eval, metric_period=metric_period, silent=silent, early_stopping_rounds=early_stopping_rounds,\n                  save_snapshot=save_snapshot, snapshot_file=snapshot_file, snapshot_interval=None)\n        return self\n        \n    def predict(self, data, prediction_type='Class', ntree_start=0, ntree_end=0, thread_count=1, verbose=None):\n        predictions = self._predict(data, prediction_type, ntree_start, ntree_end, thread_count, verbose)\n\n        # Return same type as input\n        return self.le_.inverse_transform(predictions.astype(np.int64))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b52688ca33688a9a56799bc4421f4e4b41f72c31","trusted":true},"cell_type":"code","source":"# Machine Learning Algorithm (MLA) Selection and Initialization\nMLA = [\n    # Ensemble Methods\n    ensemble.RandomForestClassifier(),\n    \n    # Nearest Neighbors\n    neighbors.KNeighborsClassifier(),\n    \n    # XGBoost\n    XGBClassifier(),\n    \n    # LightGBM\n    lgb.LGBMClassifier(),\n    \n    # CatBoost\n    CatBoostClassifierCorrected(iterations=100, logging_level='Silent')\n    ]\n\n# Split dataset in cross-validation with this splitter class\ncv_split = model_selection.ShuffleSplit(n_splits = 10, test_size = .3, train_size = .6, random_state = 0)\n\n# Create table to compare MLA metrics\nMLA_columns = ['MLA Name', 'MLA Parameters','MLA Train Accuracy Mean', 'MLA Test Accuracy Mean', 'MLA Test Accuracy 3*STD' ,'MLA Time']\nMLA_compare = pd.DataFrame(columns = MLA_columns)\n\n# Create table to compare MLA predictions\nMLA_predict = pd.Series()\n\n# Index through MLA and save performance to table\nrow_index = 0\nfor alg in MLA:\n\n    # Set name and parameters\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n    MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n    \n    # Score model with cross validation\n    cv_results = model_selection.cross_validate(alg, X, y, cv  = cv_split)\n\n    MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n    MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean()\n    MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()   \n    # If this is a non-bias random sample, then +/-3 standard deviations (std) from the mean, should statistically capture 99.7% of the subsets\n    MLA_compare.loc[row_index, 'MLA Test Accuracy 3*STD'] = cv_results['test_score'].std()*3   #let's know the worst that can happen!\n    \n    # Save MLA predictions - see section 6 for usage\n    alg.fit(X, y)\n    MLA_predict[MLA_name] = alg.predict(X)\n    \n    row_index+=1\n    \n# Print and sort table\nMLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\nMLA_compare","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"70033fb732a3df3b75b8f195e3072748c892acc1","scrolled":false,"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(16,6))\n\n# Barplot\nsns.barplot(x='MLA Test Accuracy Mean', y='MLA Name', ax=ax, data=MLA_compare, palette=sns.color_palette(\"coolwarm_r\", 5))\n\n# Prettify\nplt.title('Machine Learning Algorithm Accuracy Score')\nplt.xlabel('Accuracy Score (%)')\nplt.ylabel('Algorithm')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bb39baf64730ae65c0aefde761519157754ea5df"},"cell_type":"markdown","source":"So as we can see, the first models are pretty similar in terms of **accuracy**."},{"metadata":{"_uuid":"e1b8952e83b093b382a2c7d5b3920c69bdd5eeee"},"cell_type":"markdown","source":"### 5.2. Tune the model with ensemble methods\n\nLet's try to leverage ensemble methods to maximize accuracy on the test set.\n\nLet's try two ensemble methods:\n* **hard voting**: classification is the most frequent answer\n* **soft voting**: classification is based on the argmax of the sums of the predicted probabilities"},{"metadata":{"_uuid":"cbb82f3662a907e376e08ef6fabfe83958ba76e6","trusted":true},"cell_type":"code","source":"# Removed models w/o attribute 'predict_proba' required for vote classifier and models with a 1.0 correlation to another model\nvote_est = [\n    # Ensemble Methods: \n    ('rfc', ensemble.RandomForestClassifier()),\n    \n    # Nearest Neighbors:\n    ('knn', neighbors.KNeighborsClassifier()),\n    \n    # XGBoost:\n    ('xgb', XGBClassifier()),\n    \n    # LightGBM:\n    ('lgb', lgb.LGBMClassifier()),\n    \n    # CatBoost:\n    ('cat', CatBoostClassifierCorrected(iterations=100, logging_level='Silent'))\n]\n\n# Hard vote or majority rules\nvote_hard = ensemble.VotingClassifier(estimators = vote_est, voting = 'hard')\nvote_hard_cv = model_selection.cross_validate(vote_hard, X, y, cv = cv_split)\nvote_hard.fit(X, y)\n\nprint(\"Hard Voting Training w/bin score mean: {:.2f}\". format(vote_hard_cv['train_score'].mean()*100)) \nprint(\"Hard Voting Test w/bin score mean: {:.2f}\". format(vote_hard_cv['test_score'].mean()*100))\nprint(\"Hard Voting Test w/bin score 3*std: +/- {:.2f}\". format(vote_hard_cv['test_score'].std()*100*3))\nprint('-'*15)\n\n# Soft vote or weighted probabilities\nvote_soft = ensemble.VotingClassifier(estimators = vote_est , voting = 'soft')\nvote_soft_cv = model_selection.cross_validate(vote_soft, X, y, cv  = cv_split)\nvote_soft.fit(X, y)\n\nprint(\"Soft Voting Training w/bin score mean: {:.2f}\". format(vote_soft_cv['train_score'].mean()*100)) \nprint(\"Soft Voting Test w/bin score mean: {:.2f}\". format(vote_soft_cv['test_score'].mean()*100))\nprint(\"Soft Voting Test w/bin score 3*std: +/- {:.2f}\". format(vote_soft_cv['test_score'].std()*100*3))\nprint('-'*15)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c29d04f4aecfda4a67f53b8d5d74da73e086eafd"},"cell_type":"markdown","source":"Ok so good results, but there is room for improvement. The reason is we didn't touch any of the hyperparameters of the voting models.\n\nLet's perform grid search on the different classifiers. **(careful, this will take A LOT OF time)**\n\n(everything is set though)"},{"metadata":{"_uuid":"013471e02ec2430837133c8312696ca9c6315fa5","trusted":true},"cell_type":"code","source":"# Hyper-parameter tuning with GridSearchCV:\ngrid_param = [\n            [{\n            # RandomForestClassifier\n            'criterion': ['gini'], #['gini', 'entropy'],\n            'max_depth': [8], #[2, 4, 6, 8, 10, None],\n            'n_estimators': [100], #[10, 50, 100, 300],\n            'oob_score': [False] #[True, False]\n             }],\n    \n            [{\n            # KNeighborsClassifier\n            'algorithm': ['auto'], #['auto', 'ball_tree', 'kd_tree', 'brute'],\n            'n_neighbors': [7], #[1,2,3,4,5,6,7],\n            'weights': ['distance'] #['uniform', 'distance']\n            }],\n    \n            [{\n            # XGBClassifier\n            'learning_rate': [0.05], #[0.05, 0.1,0.16],\n            'max_depth': [10], #[10,30,50],\n            'min_child_weight' : [6], #[1,3,6]\n            'n_estimators': [200]\n             }],\n    \n            [{\n            # LightGBMClassifier\n            'learning_rate': [0.01], #[0.01,0.05,0.1],\n            'n_estimators': [200],\n            'num_leaves': [300], #[300,900,1200],\n            'max_depth': [25], #[25,50,75],\n             }],\n    \n            [{\n            # CatBoostClassifier\n            'depth': [4],\n            'learning_rate' : [0.03],\n            'l2_leaf_reg': [4],\n            'iterations': [300],\n            'thread_count': [4]\n            }]\n        ]\n\nstart_total = time.perf_counter()\nfor clf, param in zip (vote_est, grid_param):\n    start = time.perf_counter()        \n    best_search = model_selection.GridSearchCV(estimator = clf[1], param_grid = param, cv = cv_split, scoring = 'roc_auc')\n    best_search.fit(X, y)\n    run = time.perf_counter() - start\n\n    best_param = best_search.best_params_\n    print('The best parameter for {} is {} with a runtime of {:.2f} seconds.'.format(clf[1].__class__.__name__, best_param, run))\n    clf[1].set_params(**best_param) \n\nrun_total = time.perf_counter() - start_total\nprint('Total optimization time was {:.2f} minutes.'.format(run_total/60))\n\nprint('-'*15)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7bef78aadc4d97a4dceaa55f22f173f68271c4dd","trusted":true},"cell_type":"code","source":"# Hard vote or majority rules w/Tuned Hyperparameters\ngrid_hard = ensemble.VotingClassifier(estimators = vote_est , voting = 'hard')\ngrid_hard_cv = model_selection.cross_validate(grid_hard, X, y, cv  = cv_split)\ngrid_hard.fit(X, y)\n\nprint(\"Hard Voting w/Tuned Hyperparameters Training w/bin score mean: {:.2f}\". format(grid_hard_cv['train_score'].mean()*100)) \nprint(\"Hard Voting w/Tuned Hyperparameters Test w/bin score mean: {:.2f}\". format(grid_hard_cv['test_score'].mean()*100))\nprint(\"Hard Voting w/Tuned Hyperparameters Test w/bin score 3*std: +/- {:.2f}\". format(grid_hard_cv['test_score'].std()*100*3))\nprint('-'*15)\n\n# Soft vote or weighted probabilities w/Tuned Hyperparameters\ngrid_soft = ensemble.VotingClassifier(estimators = vote_est , voting = 'soft')\ngrid_soft_cv = model_selection.cross_validate(grid_soft, X, y, cv  = cv_split)\ngrid_soft.fit(X, y)\n\nprint(\"Soft Voting w/Tuned Hyperparameters Training w/bin score mean: {:.2f}\". format(grid_soft_cv['train_score'].mean()*100)) \nprint(\"Soft Voting w/Tuned Hyperparameters Test w/bin score mean: {:.2f}\". format(grid_soft_cv['test_score'].mean()*100))\nprint(\"Soft Voting w/Tuned Hyperparameters Test w/bin score 3*std: +/- {:.2f}\". format(grid_soft_cv['test_score'].std()*100*3))\nprint('-'*15)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2d2507daab2018394dd285021a9cacc9e1468ac5"},"cell_type":"markdown","source":"###  5.2. Submission\nGood scores overall, let's prepare the data for submission."},{"metadata":{"_uuid":"5e16e3f1490131d31c5131eeeadc37a4d731997a","trusted":true},"cell_type":"code","source":"test_df['Survived'] = grid_soft.predict(X_test)\ntest_df['Survived'] = test_df['Survived'].astype(int)\nprint('Validation Data Distribution: \\n', test_df['Survived'].value_counts(normalize = True))\nsubmit = test_df[['Survived']]\n#submit.to_csv(\"../output/submission.csv\", index=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"567b755525172a3a1dbd4beea3ee996d80720c02"},"cell_type":"markdown","source":"Wait a minute, **0.79904** accuracy after submission? (still top 14% though)"},{"metadata":{"_uuid":"7cd87e24fd16f02b0eb166f3104971aff8f9b9fe"},"cell_type":"markdown","source":"## Step 6: Drawing conclusions <a id=\"step6\"></a>\nIt appears that our models capture some distributions from the engineered training dataset that differ slightly in the testing dataset: this is a sign of **overfitting**.\n\nWe did use a lot of features from our datasets and generally, if you want to avoid overfitting, **less is better**.  Remember the mapping of $X$ to a different feature space? We discovered through EDA that **the train set and the test set are not equally distributed**. We need a mapping that simplifies the distributions of features that have an influence on survival. And that's why this problem is hard.\n\n### 6.1. Simplifying our datasets\n\n#### Dropping Deck, Embarked and Ticket\nLet's drop the most ambiguous features."},{"metadata":{"_uuid":"4931ea72fffe3aed5c0b90496f4a00311a0baf63","trusted":true},"cell_type":"code","source":"columns = [c for c in data_df.columns if 'Deck' in c or 'Embarked' in c or 'Ticket' in c]\n\nsimple_data_df = data_df.copy(deep=True)\nsimple_data_df.drop(columns=columns, axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a13e0cef0db72744155f95140157a75ca8221f0a"},"cell_type":"markdown","source":"#### Simplifying Age\nLet's create the `Young`boolean feature telling us if the passenger is young (< 2nd bin)."},{"metadata":{"_uuid":"b4645ed3761b705adcb8233cf7515a6f672bc576","trusted":true},"cell_type":"code","source":"simple_data_df['Young'] = np.where((simple_data_df['AgeBin_Code']<2), 1, 0)\nsimple_data_df.drop(columns=['AgeBin_Code'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2659b48bb95529511fe4af13689acc29db776084"},"cell_type":"markdown","source":"#### Merging Pclass and Sex\nLet's merge `Pclass` and `Sex`."},{"metadata":{"_uuid":"cbc48657427d185cb104c643db8f6675ca4353ce","trusted":true},"cell_type":"code","source":"simple_data_df['P1_Male'] = np.where((simple_data_df['Sex']==0) & (simple_data_df['Pclass']==1), 1, 0)\nsimple_data_df['P2_Male'] = np.where((simple_data_df['Sex']==0) & (simple_data_df['Pclass']==2), 1, 0)\nsimple_data_df['P3_Male'] = np.where((simple_data_df['Sex']==0) & (simple_data_df['Pclass']==3), 1, 0)\nsimple_data_df['P1_Female'] = np.where((simple_data_df['Sex']==1) & (simple_data_df['Pclass']==1), 1, 0)\nsimple_data_df['P2_Female'] = np.where((simple_data_df['Sex']==1) & (simple_data_df['Pclass']==2), 1, 0)\nsimple_data_df['P3_Female'] = np.where((simple_data_df['Sex']==1) & (simple_data_df['Pclass']==3), 1, 0)\n\nsimple_data_df.drop(columns=['Pclass', 'Sex'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2be9b547551393e35618eb62640371a31bccc15d","scrolled":true,"trusted":true},"cell_type":"code","source":"simple_train_df = simple_data_df[:TRAINING_LENGTH]\nsimple_test_df = simple_data_df[TRAINING_LENGTH:]\n\nsimple_data_df.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"36c5e87faebdc070afdb999024443260fd80a970"},"cell_type":"markdown","source":"### 6.2. Data formatting\nLet's create our `X` and `y` and scale them."},{"metadata":{"_uuid":"c84a2d0bbc2da1eb3c97b308068a4e9f8146c8a3","trusted":true},"cell_type":"code","source":"X = simple_train_df.drop('Survived', 1)\ny = simple_train_df['Survived']\nX_test = simple_test_df.copy().drop(columns=['Survived'], axis=1)\n\nstd_scaler = StandardScaler()\nX = std_scaler.fit_transform(X)\nX_test = std_scaler.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5d465ca9cafb562993b9e5f93bf41f2b4190fcf8"},"cell_type":"markdown","source":"### 6.3. Hyper-parameter tuning and ensemble methods\nLet's try to leverage ensemble methods to maximize accuracy on the test set."},{"metadata":{"_uuid":"50c3059aa9e3236462fd9c9992b5fe46558a10dd","trusted":true},"cell_type":"code","source":"# Hyper-parameter tuning with GridSearchCV:\ngrid_param = [\n            [{\n            # RandomForestClassifier\n            'criterion': ['gini'], #['gini', 'entropy'],\n            'max_depth': [8], #[2, 4, 6, 8, 10, None],\n            'n_estimators': [100], #[10, 50, 100, 300],\n            'oob_score': [False] #[True, False]\n             }],\n    \n            [{\n            # KNeighborsClassifier\n            'algorithm': ['auto'], #['auto', 'ball_tree', 'kd_tree', 'brute'],\n            'n_neighbors': [7], #[1,2,3,4,5,6,7],\n            'weights': ['distance'] #['uniform', 'distance']\n            }],\n    \n            [{\n            # XGBClassifier\n            'learning_rate': [0.05], #[0.05, 0.1,0.16],\n            'max_depth': [10], #[10,30,50],\n            'min_child_weight' : [6], #[1,3,6]\n            'n_estimators': [200]\n             }],\n    \n            [{\n            # LightGBMClassifier\n            'learning_rate': [0.01], #[0.01,0.05,0.1],\n            'n_estimators': [200],\n            'num_leaves': [300], #[300,900,1200],\n            'max_depth': [25], #[25,50,75],\n             }],\n    \n            [{\n            # CatBoostClassifier\n            'depth': [4],\n            'learning_rate' : [0.03],\n            'l2_leaf_reg': [4],\n            'iterations': [300],\n            'thread_count': [4]\n            }]\n        ]\n\nstart_total = time.perf_counter()\nfor clf, param in zip (vote_est, grid_param):\n    start = time.perf_counter()        \n    best_search = model_selection.GridSearchCV(estimator = clf[1], param_grid = param, cv = cv_split, scoring = 'roc_auc')\n    best_search.fit(X, y)\n    run = time.perf_counter() - start\n\n    best_param = best_search.best_params_\n    print('The best parameter for {} is {} with a runtime of {:.2f} seconds.'.format(clf[1].__class__.__name__, best_param, run))\n    clf[1].set_params(**best_param) \n\nrun_total = time.perf_counter() - start_total\nprint('Total optimization time was {:.2f} minutes.'.format(run_total/60))\n\nprint('-'*15)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"031f74ea92716863b602bf45887989021bb49085","trusted":true},"cell_type":"code","source":"# Hard vote or majority rules w/Tuned Hyperparameters\ngrid_hard = ensemble.VotingClassifier(estimators = vote_est , voting = 'hard')\ngrid_hard_cv = model_selection.cross_validate(grid_hard, X, y, cv  = cv_split)\ngrid_hard.fit(X, y)\n\nprint(\"Hard Voting w/Tuned Hyperparameters Training w/bin score mean: {:.2f}\". format(grid_hard_cv['train_score'].mean()*100)) \nprint(\"Hard Voting w/Tuned Hyperparameters Test w/bin score mean: {:.2f}\". format(grid_hard_cv['test_score'].mean()*100))\nprint(\"Hard Voting w/Tuned Hyperparameters Test w/bin score 3*std: +/- {:.2f}\". format(grid_hard_cv['test_score'].std()*100*3))\nprint('-'*15)\n\n# Soft vote or weighted probabilities w/Tuned Hyperparameters\ngrid_soft = ensemble.VotingClassifier(estimators = vote_est , voting = 'soft')\ngrid_soft_cv = model_selection.cross_validate(grid_soft, X, y, cv  = cv_split)\ngrid_soft.fit(X, y)\n\nprint(\"Soft Voting w/Tuned Hyperparameters Training w/bin score mean: {:.2f}\". format(grid_soft_cv['train_score'].mean()*100)) \nprint(\"Soft Voting w/Tuned Hyperparameters Test w/bin score mean: {:.2f}\". format(grid_soft_cv['test_score'].mean()*100))\nprint(\"Soft Voting w/Tuned Hyperparameters Test w/bin score 3*std: +/- {:.2f}\". format(grid_soft_cv['test_score'].std()*100*3))\nprint('-'*15)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e8a6bcce146eb5e9f4e909e728f22cbe0482b654"},"cell_type":"markdown","source":"### 6.4. Submission\nOk, let's prepare the data for submission."},{"metadata":{"_uuid":"f523d57b737ceade347b8247b6890933ef433edc","scrolled":true,"trusted":true},"cell_type":"code","source":"simple_test_df['Survived'] = grid_soft.predict(X_test)\nsimple_test_df['Survived'] = test_df['Survived'].astype(int)\nprint('Validation Data Distribution: \\n', simple_test_df['Survived'].value_counts(normalize = True))\nsubmit = simple_test_df[['Survived']]\n#submit.to_csv(\"../output/submission.csv\", index=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1134d072b1af28fdef680117b47902cffc8dab91"},"cell_type":"markdown","source":"## Changelog\nv1: initial submission."},{"metadata":{"_uuid":"b99c00b47ae4d145ff239446e9ad229f0b4e64f0"},"cell_type":"markdown","source":"*This is a work in progress. Comments and critical feedback are always welcome.*"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}