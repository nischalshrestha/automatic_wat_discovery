{"cells": [{"outputs": [], "source": "Here is my first notebook in kaggle. \nI used information and source code from many other kaggle notebooks, listed bellow.\n\nhttps://www.kaggle.com/omarelgabry/a-journey-through-titanic\n\nhttps://www.kaggle.com/startupsci/titanic-data-science-solutions\n\nhttps://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python\n\nhttps://www.kaggle.com/benhamner/random-forest-benchmark-r\n\nhttps://www.kaggle.com/headsortails/pytanic\n\nhttps://www.kaggle.com/poonaml/titanic-survival-prediction-end-to-end-ml-pipeline\n\nhttps://www.kaggle.com/sachinkulkarni/an-interactive-data-science-tutorial\n\nI thank them.\n\nI intend to generate as many features as possible and then use a genetic algorithm to find optimal features and model parameters. Genetic algorithms are known for usefulness in search problems and therefore I believe to be useful in the search for the best set of features and parameters for the prediction model of Titanic survivors.\n\nMuch of the feature engineering presented below is taken from the notebooks listed above, so I will not explain it in depth unless I find it necessary.\n\n\nThis is an experiment of my studies in machine learning and I would appreciate any contribution to make it better.\n------------------------------------------------------------------------\n", "cell_type": "markdown", "execution_count": null, "metadata": {"_uuid": "605e68089822c61ab0b686a4d824489a9d99d2bb", "_cell_guid": "13433947-af96-cb1e-c0f6-c240340a36ef"}}, {"outputs": [], "source": "#Here some imports...\n# data analysis and wrangling\nimport pandas as pd\nimport numpy as np\nimport random as rnd\n\n#Genetic Algorithm - https://github.com/deap/deap\nfrom deap import base\nfrom deap import creator\nfrom deap import tools\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# machine learning models\nfrom sklearn import preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import VotingClassifier\n# machine learning auxiliaries\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression\nfrom sklearn.pipeline import make_pipeline\n\n#loading data\ntrain_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')\ndfs = [train_df, test_df]\nfull_origin = pd.concat([train_df.drop('Survived',1),test_df])\n\nprint('Data loaded...')\n\n# a function that extracts each prefix of the ticket, returns 'XXX' if no prefix (i.e the ticket is a digit)\n#From https://www.kaggle.com/sachinkulkarni/an-interactive-data-science-tutorial\ndef cleanTicket(ticket):\n    ori_ticket = ticket\n    ticket = ticket.replace( '.' , '' )\n    ticket = ticket.replace( '/' , '' )\n    ticket = ticket.split()\n    ticket = map( lambda t : t.strip() , ticket )\n    ticket = list(filter( lambda t : not t.isdigit() , ticket ))\n    if len(ticket) > 0:\n        return ticket[0]\n    else: \n        return 'XXX'\n\n#from https://www.kaggle.com/rajatshah/scikit-learn-ml-from-start-to-finish?scriptVersionId=1260742\ndef simplify_ages(df):\n    df.Age = df.Age.fillna(-0.5)\n    bins = (-1, 0, 5, 12, 18, 25, 35, 60, 120)\n    group_names = ['Unknown', 'Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Senior']\n    categories = pd.cut(df.Age, bins, labels=group_names)\n    df['AgeBins'] = categories\n    return df\n\n#From https://www.kaggle.com/rajatshah/scikit-learn-ml-from-start-to-finish?scriptVersionId=1260742\ndef simplify_fares(df):\n    df.Fare = df.Fare.fillna(-0.5)\n    bins = (-1, 0, 8, 15, 31, 1000)\n    group_names = ['Unknown', '1_quartile', '2_quartile', '3_quartile', '4_quartile']\n    categories = pd.cut(df.Fare, bins, labels=group_names)\n    df['FareBins'] = categories\n    return df\n\n#From https://www.kaggle.com/rajatshah/scikit-learn-ml-from-start-to-finish?scriptVersionId=1260742\n#Map features to Numeric values\ndef encode_features(df_train, df_test, features):\n    df_combined = pd.concat([df_train[features], df_test[features]])    \n    for feature in features:\n        le = preprocessing.LabelEncoder()\n        le = le.fit(df_combined[feature])\n        df_train[feature] = le.transform(df_train[feature])\n        df_test[feature] = le.transform(df_test[feature])\n    return df_train, df_test\n\n#Creating Title feature and mapping some synonyms\nfor dataset in dfs:\n    dataset['Title'] = dataset['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n\n#Converting Sex categorical feature to int value\nfor dataset in dfs:\n    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n\n#Checking Age missing values\nfor dataset in dfs:\n    dataset['Age_known'] = dataset['Age'].isnull() == False\n\n#Filling in missing values: Embarked (missing only in train dataset)\ntrain_df['Embarked'].iloc[61] = \"C\"\ntrain_df['Embarked'].iloc[829] = \"C\"\n\n#Filling in missing values: Fare (missing only in test dataset)\n#using the median value for the 3st class \nall_df = pd.concat([train_df.drop('Survived',1),test_df])\ntest_df['Fare'].iloc[152] = all_df['Fare'][all_df['Pclass'] == 3].dropna().median()\n\n#Filling in missing values: Age\n#Guessing value from similares sex-title and sex-class median values\nall_df = pd.concat([train_df.drop('Survived',1),test_df])\ntitleList = all_df['Title'].unique().tolist()\nguess_ages_sex_title = np.zeros((2,len(titleList)))\nguess_ages_sex_pclass = np.zeros((2,3))\nfor dataset in dfs:\n    for i in range(0, 2):#Sex\n        for j in range(0, 3):#Pclass\n            guess_df = all_df[(all_df['Sex'] == i) & (all_df['Pclass'] == j+1)]['Age'].dropna()\n            age_guess = guess_df.median()\n            guess_ages_sex_pclass[i,j] = int( age_guess/0.5 + 0.5 ) * 0.5\n            for title in titleList:#Title\n                k = titleList.index(title)\n                guess_df = all_df[(all_df['Title'] == title) & (all_df['Sex'] == i)]['Age'].dropna()\n                                \n                # age_mean = guess_df.mean()\n                # age_std = guess_df.std()\n                # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)\n\n                age_guess2 = guess_df.median()                \n                if (age_guess2!=age_guess2):#if Nan\n                    age_guess2 = age_guess\n                \n                guess_ages_sex_title[i,k] = int( age_guess2/0.5 + 0.5 ) * 0.5\n    \n    for i in range(0, 2):\n        for j in range(0, 3):\n            for k in range(0, len(titleList)):\n                dataset.loc[ (dataset.Age.isnull()) & (dataset.Title == titleList[k]) & (dataset.Sex == i) ,\\\n                    'Age'] = guess_ages_sex_title[i,k]\n            dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == i) & (dataset.Pclass == j+1),\\\n                    'Age'] = guess_ages_sex_pclass[i,j]\n\n    dataset['Age'] = dataset['Age'].astype(int)\n    \n#creating Deck and FamilyName features\nfor dataset in dfs:    \n    dataset['Deck'] = dataset['Cabin'].str[0]\n    dataset['FamilyName'] = dataset['Name'].str.split(\", \", expand=True)[0]\n    \nall_df = pd.concat([train_df.drop('Survived',1),test_df])\nprint(\"Initial missing Deck values: \",len(all_df.loc[all_df['Deck'].isnull()]))", "cell_type": "code", "execution_count": null, "metadata": {"_execution_state": "idle", "_uuid": "47a3c99ee76a27e7fd4341f848f15edc24f19d65", "_cell_guid": "390da25e-b2b0-e588-b367-58c725adf4f8", "trusted": false}}, {"outputs": [], "source": "So, there is 1014 missing Deck values. We can try to guess these values using the shared tickets information, assuming that people who share ticket are in the same Deck. ", "cell_type": "markdown", "execution_count": null, "metadata": {"_execution_state": "idle", "_uuid": "453b702685df50f35d1d8f33657950ecbe96dcb9", "_cell_guid": "58d569be-77ee-4189-96f5-34c422794f02", "collapsed": false}}, {"outputs": [], "source": "#Guessing Deck missing values from the Ticket value. \nTicketList = all_df['Ticket'].unique().tolist()\nfor dataset in dfs:    \n    for ticket in TicketList:\n        guess_deck = all_df[(all_df['Ticket'] == ticket)]['Deck'].dropna()\n        if(len(guess_deck.index)>0):\n            guess_deck = guess_deck.iloc[0][0]\n            dataset.loc[(dataset.Deck.isnull()) & (dataset.Ticket == ticket),'Deck'] = guess_deck\nall_df = pd.concat([train_df.drop('Survived',1),test_df])\nprint(\"Missing Deck values after apply shared ticket heuristic: \",len(all_df.loc[all_df['Deck'].isnull()]))", "cell_type": "code", "execution_count": null, "metadata": {"_execution_state": "idle", "_uuid": "5a9d718e63bef48973e7c82c05021d8c88d39be4", "_cell_guid": "1ea9344f-94df-4856-8f11-b815c72362f8", "trusted": false, "collapsed": false}}, {"outputs": [], "source": "So, left 998 missing Deck values after our shared ticket heuristic. We can try to guess these values using the shared tickets, assuming that people who share ticket are in the same Deck. \nWe will tri to predict these values from a prediction model. We are going to use Random Forests as model and Pclass, Fare and Embarked as predication features.", "cell_type": "markdown", "execution_count": null, "metadata": {"_execution_state": "idle", "_uuid": "851b96c436553bb575189520e0d3b85483dea395", "_cell_guid": "b76a77f1-f568-4be2-9d32-f222e54957ce", "collapsed": false}}, {"outputs": [], "source": "#Trying to guess Deck missing values using a prediction Model.\nall_df = pd.concat([train_df,test_df])\ndf = all_df[['Pclass','Fare','Embarked','Deck']]\ndf['Embarked'] = df['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)   \ndf1 = df[df['Deck'].notnull()]\nXDeck_train = df1[['Pclass','Fare','Embarked']]\nYDeck_train = df1.Deck\nrandom_forest = RandomForestClassifier(n_estimators=100)\n\nscores = cross_val_score(random_forest, XDeck_train, YDeck_train, cv=5, n_jobs=-1)\nprint(\"CV score: \",scores.mean())\n\nrandom_forest.fit(XDeck_train, YDeck_train)\nXDeck_test = df[['Pclass','Fare','Embarked']]\nYDeck_pred = random_forest.predict(XDeck_test)\nall_df['DeckPred'] = YDeck_pred\nall_df.loc[(all_df.Deck.isnull()),'Deck'] = all_df.loc[(all_df.Deck.isnull()),'DeckPred']\ntrain_df['Deck'] = all_df[ 0:891 ]['Deck']\ntest_df['Deck'] = all_df[ 891: ]['Deck']\n\n'''\n#Guessing Deck missing values from the Embarked distribution\n#print(pd.crosstab(all_df['Embarked'], all_df['Deck']))\nall_df = pd.concat([train_df.drop('Survived',1),test_df])\nprint(\"Missing Deck values: \",len(all_df.loc[all_df['Deck'].isnull()]))\nfor dataset in dfs:    \n    for index, row in dataset.iterrows():\n        if(row[\"Deck\"]==row[\"Deck\"]): continue#not Null\n        if row['Embarked'] == 'C':\n            Cchoice = np.random.choice(['A', 'B', 'C', 'D','E', 'F', 'G', 'T'], p=[11/129, 36/129, 46/129, 20/129,11/129, 5/129, 0/129, 0/129])\n            dataset.loc[index, 'Deck'] = Cchoice\n        elif row['Embarked'] == 'S':\n            Schoice = np.random.choice(['A', 'B', 'C', 'D','E', 'F', 'G', 'T'], p=[11/177, 32/177, 55/177, 26/177,30/177, 17/177, 5/177, 1/177])\n            dataset.loc[index, 'Deck'] = Schoice\nall_df = pd.concat([train_df.drop('Survived',1),test_df])\nprint(\"Missing Deck values: \",len(all_df.loc[all_df['Deck'].isnull()]))\n\nfor dataset in dfs:    \n    dataset['Deck'] = dataset['Deck'].fillna(value='U')\n'''\n                \nall_df = pd.concat([train_df.drop('Survived',1),test_df])\nprint(\"Missing Deck values: \",len(all_df.loc[all_df['Deck'].isnull()]))\n", "cell_type": "code", "execution_count": null, "metadata": {"_execution_state": "idle", "_uuid": "b8a792ef57718da99dde7678eb646a446613ca2a", "_cell_guid": "72f98870-eeef-41ee-94fb-339581055bd1", "trusted": false, "collapsed": false}}, {"outputs": [], "source": "Ok, now we are going to create some new features we suppose be helpful.\n\nEspecially I've created the NameContainsP feature. this feature has a promissory asymmetry presented in the following graph.", "cell_type": "markdown", "execution_count": null, "metadata": {"_execution_state": "idle", "_uuid": "2c65f5eb2a23d424c280c77b5f4d0cc105e01e67", "_cell_guid": "36da1aff-4edb-42b9-9662-d8c6aedc011e", "collapsed": false}}, {"outputs": [], "source": "#Creating feature NameContainsP\nfor dataset in dfs:    \n    dataset[\"NameContainsP\"] = dataset[\"Name\"].apply(lambda x: \"(\" in x) #If the name contain \"(\"\n\ng = sns.FacetGrid(train_df, col='Survived')\ng.map(plt.hist, 'NameContainsP')\n\npd.crosstab(train_df['NameContainsP'], train_df['Survived'])", "cell_type": "code", "execution_count": null, "metadata": {"_execution_state": "idle", "_uuid": "80db584fb482076fe7591482760c0bd1d98b338e", "_cell_guid": "f0dcd87a-b4bf-407a-a8a7-3a1fce1ea0a6", "trusted": false, "collapsed": false}}, {"outputs": [], "source": "#creating some new features\nall_df = pd.concat([train_df.drop('Survived',1),test_df])\nfor dataset in dfs:    \n    simplify_fares(dataset)#ranges of fares\n    simplify_ages(dataset)#ranges of ages\n    dataset['Child'] = dataset['Age']<=10\n    dataset['MedianAge'] = (dataset['Age']>=18) & (dataset['Age']<=40)\n    dataset['Young_m'] = (dataset['Age']>=18) & (dataset['Age']<=40) & (dataset['Sex']==0)\n    dataset['Young_f'] = (dataset['Age']>=18) & (dataset['Age']<=40) & (dataset['Sex']==1)\n    dataset['Family'] = dataset['SibSp'] + dataset['Parch']\n    dataset['Alone']  = (dataset['SibSp'] + dataset['Parch']) == 0\n    dataset['Cabin_known'] = dataset['Cabin'].isnull() == False\n    dataset[\"Cabin_known\"] = dataset[\"Cabin_known\"].astype(\"int\")    \n    dataset['Ttype'] = dataset['Ticket'].str[0]\n    dataset['Ttype2'] = dataset['Ticket'].map(cleanTicket)    \n    dataset['Bad_ticket'] = dataset['Ttype'].isin(['3','4','5','6','7','8','A','L','W'])\n    dataset[\"NameLength\"] = dataset[\"Name\"].apply(lambda x: len(x))  #Create feture for name length         \n    dataset['Ticket_group'] = dataset.groupby('Ticket')['Name'].transform('count')\n    dataset['Fare_eff'] = dataset['Fare']/dataset['Ticket_group']\n    dataset['Shared_ticket'] = 3\n    for i in range(len(dataset)):\n        if dataset['Shared_ticket'].iloc[i]==3:            \n            if ((len(all_df.groupby('Ticket').get_group(dataset['Ticket'].iloc[i]))) > 1 ):\n                dataset.loc[dataset['Ticket'] == dataset['Ticket'].iloc[i], 'Shared_ticket'] = 1\n            else:\n                dataset.loc[dataset['Ticket'] == dataset['Ticket'].iloc[i], 'Shared_ticket'] = 0\n    \n    dataset['Young'] = (dataset['Age']<=20) | (dataset['Title'].isin(['Master','Miss','Mlle','Mme']))\n     #FareBand\n    dataset['FareBand'] = 0\n    dataset.loc[ dataset['Fare'] <= 7.91, 'FareBand'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'FareBand'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'FareBand']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'FareBand'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n    #AgeBand\n    dataset['AgeBand'] = 0\n    dataset.loc[ dataset['Age'] <= 16, 'AgeBand'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'AgeBand'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'AgeBand'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'AgeBand'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'AgeBand'] = 4    \n    \nTitle_Dictionary = {\n                    \"Capt\":       \"Officer\",\n                    \"Col\":        \"Officer\",\n                    \"Major\":      \"Officer\",\n                    \"Jonkheer\":   \"Royalty\",\n                    \"Don\":        \"Royalty\",\n                    \"Sir\" :       \"Royalty\",\n                    \"Dr\":         \"Officer\",\n                    \"Rev\":        \"Officer\",\n                    \"the Countess\":\"Royalty\",\n                    \"Dona\":       \"Royalty\",\n                    \"Mme\":        \"Mrs\",\n                    \"Mlle\":       \"Miss\",\n                    \"Ms\":         \"Mrs\",\n                    \"Mr\" :        \"Mr\",\n                    \"Mrs\" :       \"Mrs\",\n                    \"Miss\" :      \"Miss\",\n                    \"Master\" :    \"Master\",\n                    \"Lady\" :      \"Royalty\"\n                    }\n\n#Converting categorical features to integer values\nfor dataset in dfs:      \n    #Title\n    # we map each title\n    dataset[ 'Title' ] = dataset.Title.map( Title_Dictionary )\n    title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Officer\": 5, \"Royalty\": 6}    \n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n    dataset['Title'] = dataset['Title'].astype(int)    \n    dataset['Ttype'] = dataset['Ttype'].map( {'1': 1, '2': 2, '3': 3, '4': 4,'5': 5,'6': 6, '7': 7, '8': 8,'9': 9,'A': 10, 'C': 11, 'F': 12,'L': 13, 'P': 14, 'S': 15,'W': 16} ).astype(int)    \n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)    \n    dataset['Deck'] = dataset['Deck'].map( {'U': 0, 'C': 1, 'E': 2,'G': 3, 'D': 4, 'A': 5,'B': 6, 'F': 7, 'T': 8} ).astype(int)    \n    for col in dataset.columns:\n        if(dataset[col].dtype == 'bool'):\n            dataset[col] = dataset[col].astype(int)\n\nprint(\"Ok. Data are now almost preprocessed...\")", "cell_type": "code", "execution_count": null, "metadata": {"_execution_state": "idle", "_uuid": "9575336a41196ba0b38250af1f7601c3b6a95ac2", "_cell_guid": "85134f3f-9333-4e0d-b306-c7c9eecac029", "trusted": false, "collapsed": false}}, {"outputs": [], "source": "# preview the final data\nall_df = pd.concat([train_df.drop('Survived',1),test_df])\nprint(all_df.info())\nall_df.head()\n", "cell_type": "code", "execution_count": null, "metadata": {"_execution_state": "idle", "_uuid": "e0d8b7925a369e6d377af3c97bfa3895c0ac165c", "_cell_guid": "1d81fd0b-c75e-6e75-19a1-29390e11cc0e", "trusted": false}}, {"outputs": [], "source": "Let's go to encoding some categorical features...", "cell_type": "markdown", "execution_count": null, "metadata": {"_uuid": "dbc7d5b2916f5cf6f07c9d1ca87d3be9b3cccceb", "_execution_state": "idle", "collapsed": false}}, {"outputs": [], "source": "#Encoding to Numeric values\ntrain_df, test_df = encode_features(train_df, test_df, ['FamilyName','AgeBins','FareBins','Ttype2'])\n", "cell_type": "code", "execution_count": null, "metadata": {"_uuid": "961937d3a29ec94af216ea58f6616f585932a693", "_execution_state": "idle", "collapsed": false}}, {"outputs": [], "source": "Lets to select only some of the features...Only numerical features will be helpful in our models...\nThis process will eliminate features like Cabin, name, ...", "cell_type": "markdown", "execution_count": null, "metadata": {"_execution_state": "idle", "_uuid": "25738d0db0b4ebddf16ed64dd1f3bf28c542a243", "_cell_guid": "940bfba8-a1ce-43df-b6aa-e4a660201fda", "collapsed": false}}, {"outputs": [], "source": "#preparing dataset\nselCols = []\n#filtering only numeric attributes\nfor col in test_df.columns:\n    if(test_df[col].dtype == 'int64' or test_df[col].dtype == 'float64' or test_df[col].dtype == 'uint8'):\n        selCols.append(col)        \n\n#removing some supposed helpless attributes\n#if 'SibSp' in selCols: selCols.remove('SibSp')\n#if 'Parch' in selCols: selCols.remove('Parch')\n#if 'AgeBand' in selCols: selCols.remove('AgeBand')\n#if 'FareBand' in selCols: selCols.remove('FareBand')\nif 'PassengerId' in selCols: selCols.remove('PassengerId')\nif 'Survived' in selCols: selCols.remove('Survived')\n\ntrain_df = train_df.loc[:,selCols+['Survived']]\ntest_df = test_df.loc[:,selCols+['PassengerId']]\ntrain_df.head()\n\nprint(\"Number of selected cols \",len(selCols),\" :\",selCols)\nprint()\nall_df = pd.concat([train_df,test_df])\nprint(all_df.describe())\n", "cell_type": "code", "execution_count": null, "metadata": {"_execution_state": "idle", "_uuid": "3560cbbdb4c081b71264a7876d5ab33779fd26e2", "_cell_guid": "022d3189-50ab-497d-98b3-8fc0bc90026e", "trusted": false, "collapsed": false}}, {"outputs": [], "source": "**Try a RF model..**", "cell_type": "markdown", "execution_count": null, "metadata": {"_execution_state": "idle", "_uuid": "74a6fbc396690e77e44d21563fc8a44cf3ad3a20", "_cell_guid": "979b63d5-a744-41f7-972f-0d68b9947902", "collapsed": false}}, {"outputs": [], "source": "colsRF =  ['Pclass', 'Sex', 'Embarked', 'Title', 'Age_known', 'Deck', 'FareBins', 'AgeBins', 'Alone', 'Ttype', 'Ttype2', 'NameLength', 'NameContainsP', 'Young']\n#colsRF =  ['Pclass', 'Sex', 'Embarked', 'Deck', 'FareBins', 'AgeBins', 'Alone', 'Ttype', 'Ttype2', 'NameContainsP']\ntcols = np.append(['Survived'],colsRF)\ndf = train_df.loc[:,tcols].dropna()\nX_train = df.loc[:,colsRF]\nY_train = np.ravel(df.loc[:,['Survived']])\n\nmodel = RandomForestClassifier(n_estimators=100)\nscores = cross_val_score(model, X_train, Y_train, cv=5, n_jobs=-1)\ncv_rf_score = scores.mean()\nprint(\"RF CV score: \",scores.mean())\nmodel.fit( X_train , Y_train )\nprint(\"Training score: \",model.score(X_train, Y_train))\n\nX_submit = test_df.loc[:,colsRF]\nY_pred = model.predict(X_submit)\nsubmission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\nsubmission.to_csv( 'titanic_pred_RF.csv' , index = False )\n", "cell_type": "code", "execution_count": null, "metadata": {"_execution_state": "idle", "_uuid": "d5e6d76946a04571c6d64dc7329e6872cbc146a1", "_cell_guid": "4bc0f504-cd21-4a28-ab78-f995fa47a477", "trusted": false, "collapsed": false}}, {"outputs": [], "source": "**Try Knn...(K-Nearest neighbors)**", "cell_type": "markdown", "execution_count": null, "metadata": {"_execution_state": "idle", "_uuid": "788c3c5a2f16949eed7414e7dcb35ad9476e4c21", "_cell_guid": "9f8d4f61-fd35-4d5f-b210-0ee92e6ea494", "collapsed": false}}, {"outputs": [], "source": "#colsRF =  ['Pclass', 'Sex', 'Embarked', 'Title', 'Age_known', 'Deck', 'FareBins', 'AgeBins', 'Alone', 'Ttype', 'Ttype2', 'NameLength', 'NameContainsP', 'Young']\ncolsRF =  ['Pclass', 'Sex', 'Embarked', 'Deck', 'FareBins', 'AgeBins', 'Alone', 'Ttype', 'Ttype2', 'NameContainsP']\ntcols = np.append(['Survived'],colsRF)\ndf = train_df.loc[:,tcols].dropna()\nX_train = df.loc[:,colsRF]\nY_train = np.ravel(df.loc[:,['Survived']])\n\nmodel = KNeighborsClassifier(n_neighbors = 3)\nscores = cross_val_score(model, X_train, Y_train, cv=5, n_jobs=-1)\ncv_knn_score = scores.mean()\nprint(\"KNN CV score: \",scores.mean())\nmodel.fit( X_train , Y_train )\nprint(\"Training score: \",model.score(X_train, Y_train))\n\nX_submit = test_df.loc[:,colsRF]\nY_pred = model.predict(X_submit)\nsubmission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\nsubmission.to_csv( 'titanic_pred_KNN.csv' , index = False )\n", "cell_type": "code", "execution_count": null, "metadata": {"_execution_state": "idle", "_uuid": "2c3bab9182a90873aa3dea78f5162123a90082f6", "_cell_guid": "ef44ad56-f289-4409-8bef-22451af50167", "trusted": false, "collapsed": false}}, {"outputs": [], "source": "**Try SVM...**", "cell_type": "markdown", "execution_count": null, "metadata": {"_execution_state": "idle", "_uuid": "288dd30522fcb8b1363fe127638d6264cbcc3d27", "_cell_guid": "18a858ed-38ff-4b99-a2b6-fdf8c684efd3", "collapsed": false}}, {"outputs": [], "source": "#colsRF =  ['Pclass', 'Sex', 'Embarked', 'Title', 'Age_known', 'Deck', 'FareBins', 'AgeBins', 'Alone', 'Ttype', 'Ttype2', 'NameLength', 'NameContainsP', 'Young']\ncolsRF =  selCols\ntcols = np.append(['Survived'],colsRF)\ndf = train_df.loc[:,tcols].dropna()\nX_train = df.loc[:,colsRF]\nY_train = np.ravel(df.loc[:,['Survived']])\nscaler = preprocessing.StandardScaler().fit(X_train)    \nX_train = scaler.transform(X_train)\n\nmodel = SVC(kernel='rbf')\nscores = cross_val_score(model, X_train, Y_train, cv=5, n_jobs=-1)\ncv_svm_score = scores.mean()\nprint(\"SVM CV score: \",scores.mean())\nmodel.fit( X_train , Y_train )\nprint(\"Training score: \",model.score(X_train, Y_train))\n\nX_submit = test_df.loc[:,colsRF]\nX_submit = scaler.transform(X_submit)\nY_pred = model.predict(X_submit)\nsubmission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\nsubmission.to_csv( 'titanic_pred_SVM.csv' , index = False )\n", "cell_type": "code", "execution_count": null, "metadata": {"_execution_state": "idle", "_uuid": "68ca5b58623337606925625463ee9ae62d2355d0", "_cell_guid": "62759311-915c-419c-bd73-9457354c9a0c", "trusted": false, "collapsed": false}}, {"outputs": [], "source": "**Try SVM with GA**", "cell_type": "markdown", "execution_count": null, "metadata": {"_execution_state": "idle", "_uuid": "99cc09b6e0693d00c9def9bcda5ac3570bb9396b", "_cell_guid": "2cb65aa4-4d2d-4d5a-95b2-6a719ea2d01c", "collapsed": false}}, {"outputs": [], "source": "#GA applied to select optimal features and parameters of a SVM model\n#Extra examples of this GA library could be find here: http://deap.readthedocs.io/en/master/examples/\n\ncols = selCols#initial set of features, the space search...\ntraining = pd.concat([train_df]) #A working copy of the training dataset\n\n#Random generator of SVM parameter C\ndef getC():\n    #[0.01 - 3[\n    r = rnd.random()\n    r2 = rnd.randint(0,2)\n    r3 = r+r2+0.000000001    \n    #print(\"New C: \",r3)\n    return r3\n#Random generator of SVM parameter Gamma\ndef getGamma():\n    #[0.01 - 1[\n    r = rnd.random()#[0-1]\n    r2 = rnd.randint(0,3)\n    r3 = 0.000000001+(r/(10**r2))\n    #r3=0.01\n    #print(\"New Gamma: \",r3)\n    return r3\n#Random generator of SVM Kernel\ndef getKernel():\n    kernels = ['rbf','linear','svcLinear']\n    ind = rnd.randint(0,len(kernels)-1)   \n    r = kernels[ind]\n    #print(\"New Kernel: \", r)\n    return r\n    \ncreator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\ncreator.create(\"Individual\", list, fitness=creator.FitnessMax)\n\ntoolbox = base.Toolbox()\n\ntoolbox.register(\"attr_bool\", rnd.randint, 0, 1)\ntoolbox.register(\"attr_C\", getC)\ntoolbox.register(\"attr_Gamma\", getGamma)\ntoolbox.register(\"attr_kernel\", getKernel)\n\n#features\nfunc_seq = [toolbox.attr_C , toolbox.attr_Gamma,toolbox.attr_kernel]#[C,Gamma,kernel]\nfor c in cols:\n    func_seq.append(toolbox.attr_bool)\n\nprint(\"individuals size: \",len(func_seq))\n\ntoolbox.register(\"individual\", tools.initCycle, creator.Individual, func_seq, 1)\n\n# define the population to be a list of individuals\ntoolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n\ndef getModel(individual):\n    k = individual[2]\n    if k == 'svcLinear':\n        clf = LinearSVC(C=individual[0])\n    elif k == 'rbf':\n        clf = SVC(kernel=k, C=individual[0],gamma=individual[1])\n    else:\n        #linear\n        clf = SVC(kernel=k, C=individual[0])\n    return clf\n\ndef getXy(individual):\n    scols = list(cols)\n    for i in range(len(individual[3:])):\n        if individual[3+i]<1: scols.remove(cols[i])\n    #print(\"Selected cols: \",scols)\n    tcols = np.append(['Survived'],scols)\n    df = training.loc[:,tcols].dropna()\n    X = df.loc[:,scols]\n    scaler = preprocessing.StandardScaler().fit(X)    \n    #scaler= preprocessing.MinMaxScaler().fit(X)\n    X = scaler.transform(X)\n    y = np.ravel(df.loc[:,['Survived']])\n    return [X,y,scols,scaler]\n\n# the goal ('fitness') function to be maximized\ndef evalOneMax(individual):\n    clf = getModel(individual)\n    Xy = getXy(individual)\n    scores = cross_val_score(clf, Xy[0], Xy[1], cv=5, n_jobs=-1)\n    res1 = scores.mean(),\n    return res1\n\ndef myMutate(individual,indpb=0.05):\n    #print(individual)\n    #C\n    if rnd.random() < indpb:\n        individual[0] = toolbox.attr_C()\n    #Gamma\n    if rnd.random() < indpb:\n        individual[1] = toolbox.attr_Gamma()\n    #Kernel\n    if rnd.random() < indpb:\n        individual[2] = toolbox.attr_kernel()\n    #features\n    for i in range(len(individual[3:])):\n        if rnd.random() < indpb:\n            individual[3+i] = toolbox.attr_bool()\n    #print(individual)\n                  \n#----------\n# Operator registration\n#----------\n# register the goal / fitness function\ntoolbox.register(\"evaluate\", evalOneMax)\n\n# register the crossover operator\ntoolbox.register(\"mate\", tools.cxTwoPoint)\n\n# register a mutation operator with a probability to\n# flip each attribute/gene of 0.05\ntoolbox.register(\"mutate\", myMutate, indpb=0.15)\n\n# operator for selecting individuals for breeding the next\n# generation: each individual of the current generation\n# is replaced by the 'fittest' (best) of three individuals\n# drawn randomly from the current generation.\ntoolbox.register(\"select\", tools.selTournament, tournsize=3)\n\nrnd.seed(66)\n                \n# CXPB  is the probability with which two individuals\n#       are crossed\n#\n# MUTPB is the probability for mutating an individual\n#\n# NGEN  is the number of generations for which the\n#       evolution runs\nCXPB, MUTPB, NGEN, POPSIZE = 0.5, 0.2, 40, 100\n\n# create an initial population of 300 individuals (where\n# each individual is a list of integers)\npop = toolbox.population(n=POPSIZE)    \n#print(pop)\n\nprint(\"Start of evolution SVM\")\n\n# Evaluate the entire population\nfitnesses = list(map(toolbox.evaluate, pop))\nfor ind, fit in zip(pop, fitnesses):\n    ind.fitness.values = fit\n\nprint(\"  Evaluated %i individuals\" % len(pop))\n\n# Begin the evolution\nfor g in range(NGEN):\n    print(\"-- Generation %i --\" % g)\n    \n    # Select the next generation individuals\n    offspring = toolbox.select(pop, len(pop))\n    # Clone the selected individuals\n    offspring = list(map(toolbox.clone, offspring))\n\n    # Apply crossover and mutation on the offspring\n    for child1, child2 in zip(offspring[::2], offspring[1::2]):\n\n        # cross two individuals with probability CXPB\n        if rnd.random() < CXPB:\n            #print(\"CX\")\n            #print(child1,child2)\n            c1 = toolbox.clone(child1)\n            c2 = toolbox.clone(child2)\n            toolbox.mate(child1, child2)\n            #print(child1,child2)\n            # fitness values of the children\n            # must be recalculated later\n            if c1!=child1: del child1.fitness.values\n            if c2!=child2: del child2.fitness.values\n\n    for mutant in offspring:\n\n        # mutate an individual with probability MUTPB\n        if rnd.random() < MUTPB:\n            #print(\"mut\")\n            #print(mutant)\n            m1 = toolbox.clone(mutant)\n            toolbox.mutate(mutant)\n            if m1!=mutant: del mutant.fitness.values\n            #print(mutant)\n\n    # Evaluate the individuals with an invalid fitness\n    invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n    #print(invalid_ind)\n    fitnesses = map(toolbox.evaluate, invalid_ind)\n    for ind, fit in zip(invalid_ind, fitnesses):\n        ind.fitness.values = fit\n    \n    print(\"  Evaluated %i individuals\" % len(invalid_ind))\n    #print(invalid_ind)\n    \n    # The population is entirely replaced by the offspring\n    pop[:] = offspring\n    \n    # Gather all the fitnesses in one list and print the stats\n    fits = [ind.fitness.values[0] for ind in pop]\n    \n    length = len(pop)\n    mean = sum(fits) / length\n    sum2 = sum(x*x for x in fits)\n    std = abs(sum2 / length - mean**2)**0.5\n    best_ind = tools.selBest(pop, POPSIZE)[0]\n    print(\"Best individual is %s, %s\" % (best_ind, best_ind.fitness.values))        \n    print(\"  Min %s\" % min(fits))\n    print(\"  Max %s\" % max(fits))\n    print(\"  Avg %s\" % mean)\n    print(\"  Std %s\" % std)\n\nprint(\"-- End of (successful) evolution --\")\n\nbest_ind = tools.selBest(pop, POPSIZE)[0]\nprint(\"Best individual is %s, %s\" % (best_ind, best_ind.fitness.values))\n\n\nmodel = getModel(best_ind)\nXy = getXy(best_ind)\ncolsSVM = Xy[2]\nscaler = Xy[3]\nprint(\"Selected Features: \",colsSVM)\n\nX_train = Xy[0]\nY_train = Xy[1]\n\nscores = cross_val_score(model, X_train, Y_train, cv=5).mean()\ncv_SVMGA_score = scores.mean()\nprint(\"SVMGA CV score: \",scores.mean())\nmodel.fit( X_train , Y_train )\nprint(\"Training score: \",model.score(X_train, Y_train))\n\n#Train using all data from training dataset\ntcols = np.append(['Survived'],colsSVM)\ndf = train_df.loc[:,tcols].dropna()\nX = df.loc[:,colsSVM]\nscaler = preprocessing.StandardScaler().fit(X)    \nX = scaler.transform(X)\ny = np.ravel(df.loc[:,['Survived']])\nmodel.fit(X, y)\n\nX_submit = test_df.loc[:,colsSVM]\nX_submit = scaler.transform(X_submit)\nY_pred = model.predict(X_submit)\nsubmission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\nsubmission.to_csv( 'titanic_pred_SVMGA.csv' , index = False )\n", "cell_type": "code", "execution_count": null, "metadata": {"_execution_state": "idle", "_uuid": "adf48b0bbe9fa0ca69c4b454666d6c4b01f9b6c3", "_cell_guid": "ed855c0b-cedc-4cfa-a9c6-45f4bfde5493", "trusted": false, "collapsed": false}}, {"outputs": [], "source": "**Final results:**", "cell_type": "markdown", "execution_count": null, "metadata": {"_uuid": "efe4ec4aa70120c0c699cc6bf46aa7e542725a5a", "_cell_guid": "966b0bd5-877e-4913-6996-d24ffa27f6c1"}}, {"outputs": [], "source": "d = {'cv_rf_score': cv_rf_score, 'cv_knn_score': cv_knn_score, 'cv_svm_score': cv_svm_score, 'cv_SVMGA_score': cv_SVMGA_score}\ndf = pd.DataFrame(data=d,index=[0])\ndf.head()", "cell_type": "code", "execution_count": null, "metadata": {"_execution_state": "idle", "_uuid": "4bbee50c5b9eb8bbe549006cc8d586d571726e54", "_cell_guid": "6ad0c8ff-aa7f-4dda-adf4-c071c8d0f6e4", "trusted": false, "collapsed": false}}], "nbformat": 4, "nbformat_minor": 0, "metadata": {"_change_revision": 0, "language_info": {"file_extension": ".py", "version": "3.6.1", "codemirror_mode": {"version": 3, "name": "ipython"}, "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "mimetype": "text/x-python", "name": "python"}, "_is_fork": false, "kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}}}