{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "3bd4f62c-decb-5e63-aaac-0ba8fadafce8"
      },
      "source": [
        " Yet Another Titanic Dataset Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f7ddac7e-e94a-5e0a-a815-c2421f3540ab"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "\n",
        "# pandas\n",
        "import pandas as pd\n",
        "from pandas import Series,DataFrame\n",
        "\n",
        "# numpy, matplotlib, seaborn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style('whitegrid')\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a058857e-ea18-2581-c4c9-9d62ec1231ad"
      },
      "outputs": [],
      "source": [
        "# get titanic & test csv files as a DataFrame\n",
        "titanic_df = pd.read_csv(\"../input/train.csv\")\n",
        "test_df    = pd.read_csv(\"../input/test.csv\")\n",
        "\n",
        "# preview the data\n",
        "titanic_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "6a4708f6-6553-81c9-e1d3-e89176f95e37"
      },
      "source": [
        "Passenger ID, name, and ticket will be dropped. Fare could be other candidate to be dropped (correlation with class), and perhaps cabin too. I'll initially assume that cabin doesn't help predict Survive, and let's first see what happens with fare and class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7ea78d21-7b5c-3950-53d2-377a18f6c519"
      },
      "outputs": [],
      "source": [
        "train = titanic_df.drop(['PassengerId','Name','Ticket','Cabin'], axis=1)\n",
        "test    = test_df.drop(['PassengerId','Name','Ticket','Cabin'], axis=1)\n",
        "#We also add a 1/0 variable for Sex\n",
        "df_sex=pd.get_dummies(train['Sex'],drop_first=True)\n",
        "train=train.join(df_sex)\n",
        "\n",
        "df_sex_2=pd.get_dummies(test['Sex'],drop_first=True)\n",
        "test=test.join(df_sex_2)\n",
        "#Also df['Gender'] = df['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n",
        "\n",
        "\n",
        "#Dummies for Pclass too\n",
        "\n",
        "df_pclass=pd.get_dummies(train['Pclass'],prefix='Class').astype(int)\n",
        "train=train.join(df_pclass)\n",
        "\n",
        "df_pclass_2=pd.get_dummies(test['Pclass'],prefix='Class').astype(int)\n",
        "test=test.join(df_pclass_2)\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "146b9b32-f227-8727-b0bc-92280a7c70e6"
      },
      "outputs": [],
      "source": [
        "#Deal with age\n",
        "\n",
        "avg_age_train=train['Age'].mean()\n",
        "std_age_train=train['Age'].std()\n",
        "nans_age_train=train['Age'].isnull().sum()\n",
        "\n",
        "avg_age_test=test['Age'].mean()\n",
        "std_age_test=test['Age'].std()\n",
        "nans_age_test=test['Age'].isnull().sum()\n",
        "\n",
        "#Generate random ages\n",
        "rand_1 = np.random.randint(avg_age_train-std_age_train,avg_age_train+std_age_train,size=nans_age_train)\n",
        "rand_2 = np.random.randint(avg_age_test-std_age_test,avg_age_test+std_age_test,size=nans_age_test)\n",
        "\n",
        "#Fill NaNs\n",
        "#train[\"Age\"][np.isnan(train[\"Age\"])] = rand_1\n",
        "#test[\"Age\"][np.isnan(test[\"Age\"])] = rand_2\n",
        "#Median better than mean to avoid outliers\n",
        "train['Age'].fillna(train['Age'].median(), inplace=True)\n",
        "test['Age'].fillna(test['Age'].median(), inplace=True)\n",
        "#Check\n",
        "np.all(~np.isnan(train[\"Age\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3363f4fb-700c-1ba8-0721-b13f0b95ad5f"
      },
      "outputs": [],
      "source": [
        "corrmat=train[['Survived','Class_1','Class_2','Class_3','SibSp','Parch','Fare','male','Age']].corr()\n",
        "print(corrmat)\n",
        "sns.heatmap(corrmat,vmax=.8,annot=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "90a65f19-9f52-525a-139f-c2d088a1a136"
      },
      "source": [
        "There doesn't seem to be NaNs or weird things, no need for cleaning. But we do need to fill some values for age. Also, I should check whether embarked does something. Let's go with that first. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1179794a-9d50-7770-2b06-c81e3b41e46a"
      },
      "outputs": [],
      "source": [
        "sns.countplot(x=\"Embarked\", data=train);\n",
        "plt.figure();\n",
        "sns.barplot(x='Embarked',y=\"Survived\", data=train);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "0c4309b5-ee4a-288f-667d-188e6f128471"
      },
      "source": [
        "But is this spurious due to higher variability in C and Q because there are less of them? Maybe. Perhaps it'd be worthwhile to keep these around, so let's dummy them, and run a regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "ef569cb5-5010-bbee-57df-760a84e91dc7"
      },
      "source": [
        "It seems to make sense to impute the missing values to S, as it is the most common, and near the average"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0c6f229b-13b1-6542-aab8-0819ad348e73"
      },
      "outputs": [],
      "source": [
        "train['Embarked']=train['Embarked'].fillna('S')\n",
        "test['Embarked']=test['Embarked'].fillna('S')\n",
        "\n",
        "df_em=pd.get_dummies(train['Embarked'],prefix='Embarked')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "6c39b4a1-5f85-8936-61c7-4a3b4e882d7e"
      },
      "source": [
        "Let's study the fare-class relationship"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "95c64c89-7b0c-ee55-ba2d-f2f4b9683bff"
      },
      "outputs": [],
      "source": [
        "# First, I need to extract the fares and  group them by Pclass, then plot.\n",
        "#pclass_fare=train[['Fare','Pclass']]\n",
        "#print(pclass_fare.groupby(['Pclass']).describe())\n",
        "#print(pclass_fare.groupby(['Pclass']).mean())\n",
        "#sns.boxplot(x=\"Pclass\", y=\"Fare\", data=pclass_fare);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "1dad3e5e-b11d-9224-3f35-bb52ed1fedca"
      },
      "source": [
        "As expected, the correlation seems to be there. But how strong is the correlation?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "1d1da293-d5b3-4b8a-e5eb-7c2be2962dda"
      },
      "source": [
        "Hm, perhaps better to keep it. $R^2$ is not that high"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "43a8da16-19da-68d2-b804-65807e41e811"
      },
      "source": [
        "For fun, how are fares by Pclass distributed?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ec3324b1-d4ec-99b0-02a8-4b9515cd9830"
      },
      "outputs": [],
      "source": [
        "sns.distplot(train[train['Class_1']==1]['Fare'])\n",
        "plt.figure()\n",
        "sns.distplot(train[train['Class_2']==1]['Fare'])\n",
        "sns.distplot(train[train['Class_3']==1]['Fare'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "8f966e21-cccf-7a99-4e7b-cc324bda81bd"
      },
      "source": [
        "Back to Embarked and survived:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c660f179-3473-e94d-3e30-23e723b4879d"
      },
      "outputs": [],
      "source": [
        "# result = sm.ols(formula=\"Survived ~Embarked_Q+Embarked_C+Embarked_S \", data=train).fit()\n",
        "# print(result.params)\n",
        "# print (result.summary())\n",
        "#R2 too small, disregard."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "e6c326b0-88ab-29d1-a605-dcb86c26e87c"
      },
      "source": [
        "So finally, the choosen features will be Pclass, Fare, male, and age. Time to do some predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7e7428af-f908-5df5-84ff-23d3a3f9bec0"
      },
      "outputs": [],
      "source": [
        "# define training and testing sets\n",
        "#There happens to be one missing element in Fare. So let's fix that\n",
        "test[\"Fare\"].fillna(test[\"Fare\"].median(), inplace=True)\n",
        "X_train = train[['Pclass','Fare','male','Age']]\n",
        "Y_train = train[[\"Survived\"]]\n",
        "X_test  = test[['Pclass','Fare','male','Age']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bc9013e8-dfd3-1d6c-f958-7122cefbc2c8"
      },
      "outputs": [],
      "source": [
        "# Logistic Regression\n",
        "# machine learning\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "logreg = LogisticRegression()\n",
        "\n",
        "logreg.fit(X_train, Y_train.values.ravel())\n",
        "\n",
        "Y_pred = logreg.predict(X_test)\n",
        "logreg.score(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "e8695182-2fee-947e-b758-2e081e71e174"
      },
      "source": [
        "Let's add some more features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b6c7d6be-8daf-eb66-eedb-9456b096650d"
      },
      "outputs": [],
      "source": [
        "train['FamilySize'] = train['SibSp'] + train['Parch']\n",
        "test['FamilySize'] = test['SibSp'] + test['Parch']\n",
        "\n",
        "# If greater than zero, it'll be one. If not, zero\n",
        "train['HaveFamily'] = 0\n",
        "test['HaveFamily'] = 0\n",
        "test['FamilySize'] = test['SibSp'] + test['Parch']\n",
        "train['FamilySize'] = train['SibSp'] + train['Parch']\n",
        "\n",
        "test.loc[test['FamilySize']>0,'HaveFamily']=1\n",
        "train.loc[train['FamilySize']>0,'HaveFamily']=1\n",
        "\n",
        "#Childs\n",
        "train['child'] = 0\n",
        "test['child'] = 0\n",
        "test.loc[test['Age']<15,'child']=1\n",
        "train.loc[train['Age']<15,'child']=1\n",
        "predictors=['Parch','SibSp','Fare','male','Age','FamilySize','HaveFamily','Class_1','Class_2','Class_3','child']\n",
        "X_train = train[predictors]\n",
        "\n",
        "Y_train = train[[\"Survived\"]]\n",
        "X_test  = test[predictors]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8ef3d597-bf25-22da-d175-6ed885c69b2b"
      },
      "outputs": [],
      "source": [
        "corrmat=train[predictors].corr()\n",
        "print(corrmat)\n",
        "\n",
        "a=sns.heatmap(corrmat,annot=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c171e0df-3ff8-e189-bb33-4f0e2748656d"
      },
      "outputs": [],
      "source": [
        "logreg = LogisticRegression()\n",
        "\n",
        "logreg.fit(X_train, Y_train.values.ravel())\n",
        "\n",
        "Y_pred = logreg.predict(X_test)\n",
        "logreg.score(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "0ce6a682-aa04-73de-f039-89a05862aad6"
      },
      "source": [
        "Including the family bits and Class increases the score. (!) Guess it wasn't that good of an idea to disregard those at the beginning.\n",
        "Let's now try a bunch of other methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6bc3d8d4-07bf-10b3-58ed-051cc11c271c"
      },
      "outputs": [],
      "source": [
        "# Import the random forest package\n",
        "from sklearn.ensemble import RandomForestClassifier \n",
        "from sklearn.model_selection import learning_curve\n",
        "\n",
        "\n",
        "# Create the random forest object which will include all the parameters\n",
        "# for the fit\n",
        "forest=RandomForestClassifier(max_features='sqrt',max_depth=8,n_estimators=240)\n",
        "\n",
        "# Fit the training data to the Survived labels and create the decision trees\n",
        "forest.fit(X_train,Y_train.values.ravel())\n",
        "\n",
        "# Take the same decision trees and run it on the test data\n",
        "Y_pred = forest.predict(X_test)\n",
        "forest.score(X_train, Y_train)\n",
        "# What features are important?\n",
        "features = pd.DataFrame()\n",
        "features['feature'] = X_train.columns\n",
        "features['importance'] = forest.feature_importances_\n",
        "features.sort(['importance'],ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1cc409de-ee2d-b951-ead0-a733cd37b894"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import validation_curve\n",
        "X, y = X_train,Y_train['Survived']\n",
        "#X, y = train_new,Y_train['Survived']\n",
        "def cross_val(model,X,y,pname):\n",
        "    param_range = np.array([100,110,140,150])\n",
        "    train_scores, test_scores = validation_curve(\n",
        "        model, X, y, param_name=pname, param_range=param_range, scoring=\"accuracy\", n_jobs=4)\n",
        "    train_scores_mean = np.mean(train_scores, axis=1)\n",
        "    train_scores_std = np.std(train_scores, axis=1)\n",
        "    test_scores_mean = np.mean(test_scores, axis=1)\n",
        "    test_scores_std = np.std(test_scores, axis=1)\n",
        "\n",
        "    plt.title(\"Validation Curve with Random Forest\")\n",
        "    plt.xlabel(\"Number of estimators\")\n",
        "    plt.ylabel(\"Score\")\n",
        "    lw = 2\n",
        "    plt.plot(param_range, train_scores_mean, label=\"Training score\",\n",
        "                 color=\"darkorange\", lw=lw)\n",
        "    plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
        "                     train_scores_mean + train_scores_std, alpha=0.2,\n",
        "                     color=\"darkorange\", lw=lw)\n",
        "    plt.plot(param_range, test_scores_mean, label=\"Cross-validation score\",\n",
        "                 color=\"navy\", lw=lw)\n",
        "    plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
        "                     test_scores_mean + test_scores_std, alpha=0.2,\n",
        "                     color=\"navy\", lw=lw)\n",
        "    plt.legend(loc=\"best\")\n",
        "    plt.show()\n",
        "cross_val(RandomForestClassifier(),X,y,'n_estimators')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0be51acc-b46a-01aa-d787-6015722aeb13"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "\n",
        "\n",
        "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
        "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
        "    \"\"\"\n",
        "    Generate a simple plot of the test and training learning curve.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
        "        An object of that type which is cloned for each validation.\n",
        "\n",
        "    title : string\n",
        "        Title for the chart.\n",
        "\n",
        "    X : array-like, shape (n_samples, n_features)\n",
        "        Training vector, where n_samples is the number of samples and\n",
        "        n_features is the number of features.\n",
        "\n",
        "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
        "        Target relative to X for classification or regression;\n",
        "        None for unsupervised learning.\n",
        "\n",
        "    ylim : tuple, shape (ymin, ymax), optional\n",
        "        Defines minimum and maximum yvalues plotted.\n",
        "\n",
        "    cv : int, cross-validation generator or an iterable, optional\n",
        "        Determines the cross-validation splitting strategy.\n",
        "        Possible inputs for cv are:\n",
        "          - None, to use the default 3-fold cross-validation,\n",
        "          - integer, to specify the number of folds.\n",
        "          - An object to be used as a cross-validation generator.\n",
        "          - An iterable yielding train/test splits.\n",
        "\n",
        "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
        "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
        "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
        "\n",
        "        Refer :ref:`User Guide <cross_validation>` for the various\n",
        "        cross-validators that can be used here.\n",
        "\n",
        "    n_jobs : integer, optional\n",
        "        Number of jobs to run in parallel (default 1).\n",
        "    \"\"\"\n",
        "    plt.figure()\n",
        "    plt.title(title)\n",
        "    if ylim is not None:\n",
        "        plt.ylim(*ylim)\n",
        "    plt.xlabel(\"Training examples\")\n",
        "    plt.ylabel(\"Score\")\n",
        "    \n",
        "    train_sizes, train_scores, test_scores = learning_curve(\n",
        "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
        "    train_scores_mean = np.mean(train_scores, axis=1)\n",
        "    train_scores_std = np.std(train_scores, axis=1)\n",
        "    test_scores_mean = np.mean(test_scores, axis=1)\n",
        "    test_scores_std = np.std(test_scores, axis=1)\n",
        "    plt.grid()\n",
        "\n",
        "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
        "                     color=\"r\")\n",
        "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
        "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
        "             label=\"Training score\")\n",
        "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
        "             label=\"Cross-validation score\")\n",
        "\n",
        "    plt.legend(loc=\"best\")\n",
        "    return plt\n",
        "\n",
        "\n",
        "\n",
        "title = \"Learning Curves (RandomForest)\"\n",
        "# Cross validation with 100 iterations to get smoother mean test and train\n",
        "# score curves, each time with 20% data randomly selected as a validation set.\n",
        "#cv = ShuffleSplit(n_splits=20, test_size=0.33, random_state=0)\n",
        "\n",
        "#estimator=RandomForestClassifier(n_estimators=100)\n",
        "#plot_learning_curve(estimator, title, X, y, ylim=(0.7, 1.01), cv=cv, n_jobs=4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "fa819562-c729-b963-d07b-4c68a38fdf5a"
      },
      "source": [
        "Hmm, underfitting :/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "6ec89f97-bc47-d0bf-b6ca-3ed0db050e4c"
      },
      "source": [
        "The prediction seems good enough for now, let's submit'...\n",
        "\n",
        "... Or maybe not. First public submission got 0.65. Time to try another method?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "ed9b095d-c2db-bd4c-96b2-84b493be7eee"
      },
      "source": [
        "About 0.775 with RandomForests. One final trial, with XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5751f65f-167b-4b3c-e7ca-dc79683ee1f5"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "# fit model no training data\n",
        "model = XGBClassifier()\n",
        "model_params = {\n",
        "    'learning_rate': [0.05, 0.1],\n",
        "    'n_estimators': [100,150,200],\n",
        "    'max_depth': [2, 3,7, 10],\n",
        "}\n",
        "cv = StratifiedKFold()\n",
        "cv.get_n_splits(X_train,Y_train)\n",
        "grid= GridSearchCV(model,model_params,scoring='roc_auc',cv=cv,verbose=2)\n",
        "best_params={'n_estimators': 100, 'max_depth': 2, 'learning_rate': 0.05}\n",
        "#grid.fit(X_train, Y_train.values.ravel())\n",
        "model = XGBClassifier(n_estimators= 100, max_depth= 2, learning_rate= 0.05)\n",
        "model.fit(X_train,Y_train.values.ravel())\n",
        "\n",
        "# Take the same decision trees and run it on the test data\n",
        "Y_pred = model.predict(X_test)\n",
        "#Y_pred = [round(value) for value in Y_pred]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "2966e84b-b8b2-5356-d3ce-186f7a364d67"
      },
      "outputs": [],
      "source": [
        "#cross_val(XGBClassifier(),X,y,'n_estimators')\n",
        "#About 200 seems okay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "20a26afd-d146-f273-1c66-60cc4b43fb0f"
      },
      "outputs": [],
      "source": [
        "title = \"Learning Curves (RandomForest)\"\n",
        "# Cross validation with 100 iterations to get smoother mean test and train\n",
        "# score curves, each time with 20% data randomly selected as a validation set.\n",
        "cv = ShuffleSplit(n_splits=10, test_size=0.33, random_state=0)\n",
        "\n",
        "\n",
        "plot_learning_curve(model, title, X, y, ylim=(0.7, 1.01), cv=cv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "66a0fc67-4664-45b8-7a51-e66063f7fe8d"
      },
      "outputs": [],
      "source": [
        "test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "de994922-51b6-9225-ffba-ca1b960fdb11"
      },
      "outputs": [],
      "source": [
        "submission = pd.DataFrame({\n",
        "        \"PassengerId\": test_df[\"PassengerId\"],\n",
        "        \"Survived\": Y_pred\n",
        "    })\n",
        "submission.to_csv('titanic.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "8580ea16-95be-1d09-b55b-7cba942d1fbb"
      },
      "source": [
        "# Lessons learned\n",
        "\n",
        "1. Establishing a process for analysing the data is important, even if that process requires iteration\n",
        "2. It really comes down to explore the data, generate features, then feed to the algorithm\n",
        "3. Which algorithm? Apparently XGBoost is the best. Also, try neural nets with keras.\n",
        "4. Correlation plots are interesting to heuristically decide what to keep/drop. In the end\n",
        "5. Some plots are better than others, depending on the case"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}