#!/usr/bin/env python
# coding: utf-8

# I am new to Machine Learning, any comment or advice will be appreciated. As this is a record for my machine learning, so I use my native language Chinese to write kernel. But I think code and visualization is general, wish you could understand.
# 
# I have also referred some best kernel, like [End-to-End process for Titanic problem
# ](https://www.kaggle.com/massquantity/end-to-end-process-for-titanic-problem). It provide a complete best practise about machine learning, a complete way of model selection, hyperparameter tunning and ensemble methods.
# 
# I would also recommend another best article [Titanic top 4% with ensemble modeling ](https://www.kaggle.com/yassineghouzam/titanic-top-4-with-ensemble-modeling/notebook)
# 
# # 1. Titanic Data Set Feature Description
# - survival, å­˜æ´», 0 = No, 1 = Yes
# - Name, ä¹˜å®¢å§“å
# - pclass, ç¥¨ç­‰, 1 = 1st, 2 = 2nd, 3 = 3rdï¼ŒæŠ˜å°„é˜¶çº§ï¼Œ1st = Upper, 2nd = Middle, 3rd = Lower
# - sexï¼Œæ€§åˆ«	
# - Ageï¼Œå¹´é¾„
# - sibspï¼Œ èˆ¹ä¸Šå…„å¼Ÿå§å¦¹æˆ–è€…é…å¶çš„æ•°é‡
# - parchï¼Œèˆ¹ä¸Šçˆ¶æ¯æˆ–å„¿ç«¥çš„æ•°é‡	
# - ticketï¼Œç¥¨å·
# - fareï¼Œæ—…å®¢è´¹ç”¨	
# - cabinï¼Œå®¢èˆ±å·ç ï¼Œ	
# - embarkedï¼Œç™»èˆ¹æ¸¯å£ï¼ŒC = Cherbourg, Q = Queenstown, S = Southampton
# 
# # 2. Question
# é—®é¢˜æ˜¯è¦é¢„æµ‹Survived äººæ•°ã€‚
# > On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. Translated 32% survival rate.
# 
# è¯´æ˜ï¼Œ32%åº”è¯¥æ˜¯ä¸€ä¸ªbaselineäº†ï¼Œä¹Ÿå³æˆ‘ä»¬é¢„æµ‹çš„ç»“æœï¼Œåº”è¯¥æ•´ä½“åˆèµ·æ¥å¾—åœ¨32%çš„å­˜æ´»ç‡é™„è¿‘ï¼Œè¶…è¿‡æˆ–è€…å°‘äº†ï¼Œé¢„æµ‹éƒ½æ²¡è¾¾åˆ°æœ€å¥½ã€‚
# 
# > One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.
# 
# æ²¡æœ‰è¶³å¤Ÿçš„æ•‘ç”Ÿåœˆç»™ä¹˜å®¢å’Œèˆ¹å‘˜ï¼Œé‚£ä¹ˆå“ªäº›äººèƒ½å¤Ÿå®¹æ˜“è·å¾—æ•‘ç”Ÿåœˆå‘¢ï¼Ÿä¸Šå±‚é˜¶çº§ï¼Ÿæˆ–è€…æ˜¯é‚£äº›èˆ¹å‘˜æˆ–ä¹˜å®¢ä¹˜å®¢è·ç¦»æ•‘ç”Ÿåœˆè¶³å¤Ÿè¿‘çš„äººã€‚è¿™äº›äººè·å¾—æ•‘ç”Ÿåœˆåº”è¯¥å¯ä»¥å¤§å¤§æé«˜å­˜æ´»å‡ ç‡ã€‚ç¬¬ä¸€ä¸ªæ˜¯èˆ¹èˆ±æ¯”è¾ƒé è¿‘çš„ï¼Œç¬¬äºŒä¸ªæ˜¯æ›´æœ‰æ¥è¿‘è¿™äº›å°±å‰©è®¾æ–½çš„èˆ¹å‘˜ï¼Œç¬¬ä¸‰ä¸ªæ˜¯å…·æœ‰æ›´é«˜ä¼˜å…ˆçº§çš„ä¹˜å®¢ï¼Œä¸Šå±‚æˆ–è€…é«˜å®˜ã€‚è¿™æš—ç¤ºæˆ‘ä»¬åº”è¯¥å»äº†è§£èˆ¹èˆ±åˆ†å¸ƒä»¥åŠæ•‘ç”Ÿè®¾æ–½åˆ†å¸ƒã€‚æ‰€ä»¥å»äº†è§£featureCabin,å’ŒNameé‡Œçš„Titleã€‚
# 
# > Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.
# 
# **è¿™å¥æ˜æ˜¾åœ¨ç»™æç¤ºï¼Œwomenï¼Œè¡¨æ˜æ€§åˆ«å¾ˆé‡è¦ï¼Œchildrenï¼Œè¡¨æ˜å¹´é¾„å¾ˆé‡è¦ï¼Œupper-classè¯´æ˜ï¼Œèˆ±ç­‰å’ŒFareæ›´é«˜çš„äººå­˜æ´»å‡ ç‡åº”è¯¥æ›´é«˜ã€‚**
# 
# # 3. Loading Data

# In[1]:


# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
import seaborn as sns

# disable copy warning in pandas
pd.options.mode.chained_assignment = None  # default='warn'
# disable sklearn deprecation warning
import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)

# Input data files are available in the "../input/" directory.
# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory

import os
print(os.listdir("../input/"))
# Any results you write to the current directory are saved as output.


# In[2]:


df_train = pd.read_csv('../input/train.csv').set_index('PassengerId')
df_test = pd.read_csv('../input/test.csv').set_index('PassengerId')
dataset = pd.concat([df_train, df_test], axis=0)
id_test = df_test.index
dataset.tail()


# In[3]:


dataset.info()


# æˆ‘ä»¬å¯ä»¥çœ‹åˆ°å…±æœ‰891æ¡æ•°æ®ï¼Œå…¶ä¸­Cabinç¼ºå¤±ä¸¥é‡ï¼Œåªæœ‰295æ¡ï¼ŒAgeä¹Ÿç¼ºå¤±263æ¡ã€‚Ageæˆ–è®¸å¯ä»¥æ ¹æ®sibspä¸Parchæ¨æ–­ã€‚åœ¨çœ‹ä¸€ä¸‹ç»Ÿè®¡ä¿¡æ¯ã€‚

# In[4]:


dataset.describe(include='all').T


# æˆ‘ä»¬çŸ¥é“uniqueå¤ªå¤šçš„è¯ï¼Œä¸æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é¢„æµ‹å˜é‡ï¼Œå› æ­¤Nameæ˜¾ç„¶ä¸æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é¢„æµ‹å˜é‡ã€‚Ticketä¸ºä»€ä¹ˆä¼šæœ‰é‡å¤çš„,è™½ç„¶ç¥¨å·ä¸æ˜¯å¾ˆå¥½çš„é¢„æµ‹å˜é‡ï¼Œä½†æˆ–è®¸å¯¹æˆ‘ä»¬æ¨æ–­Cabinä¼šæœ‰ç”¨å¤„ã€‚å…ˆçœ‹çœ‹æŠŠã€‚

# # 4. Overview Data
# æˆ‘ä»¬å…ˆçœ‹çœ‹æ¯ä¸ªfeatureæˆ–è€…feature ç»„åˆå¯¹Survivedå­˜æ´»çš„å½±å“ã€‚æŒ‰ç…§ISLé‡Œä»‹ç»ï¼Œè¿™ä¸ªåº”è¯¥å»è®¡ç®—Få€¼æˆ–è€…æ˜¯På€¼ï¼Œæ‰çŸ¥é“æœ‰æ— å…³ç³»ï¼Œä¸è¿‡è¿™æ˜¯é’ˆå¯¹çº¿æ€§å›å½’çš„ï¼Œæˆ‘ä¸æ¸…æ¥šè¿™ä¸ªåˆ†ç±»è¦æ€ä¹ˆçœ‹ã€‚ç°åœ¨å…ˆå‡­å€Ÿè‚‰çœ¼çœ‹ç½¢ã€‚

# In[5]:


dataset.columns


# æˆ‘ä»¬å¯ä»¥å…ˆçœ‹çœ‹æ•°å€¼ç±»å‹çš„æ•°æ®å¯¹Survivedç›¸å…³æ€§ï¼Œä¸‹å›¾æ˜¾ç¤ºï¼Œåªæœ‰Pclasså’ŒFareæœ‰ç‚¹å…³ç³»ï¼Œè¿˜æ˜¯å¼±ç›¸å…³ã€‚

# In[6]:


fig = plt.figure(figsize=(9, 6))
sns.heatmap(df_train[['Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare']].corr(), cmap='cool', annot=True, ax=fig.gca())


# ## 4.1 Pclass
# è®©æˆ‘ä»¬å…ˆçœ‹Pclassèˆ±ç­‰å¯¹å­˜æ´»ç‡çš„å½±å“ã€‚

# In[7]:


df_train.groupby('Pclass').Survived.agg(['count', 'sum', 'mean', 'std'])


# In[8]:


# plt.bar([1, 2, 3], (df_train.groupby('Pclass').Survived.mean()), color='rgb', tick_label = ['1', '2', '3'])
# sns.despine()

sns.factorplot(x='Pclass', y='Survived', data=df_train, kind='bar', size=5, palette='cool')


# ä¸€ç­‰èˆ±çš„å¹¸å­˜ç‡è¦æ¯”äºŒã€ä¸‰ç­‰èˆ±çš„éƒ½è¦é«˜ï¼Œå¹¶ä¸”éšç€èˆ±ç­‰çš„ä¸‹é™ï¼Œå¹¸å­˜ç‡ä¹Ÿåœ¨ä¸‹é™ã€‚
# ## 4.2 Name
# Nameæ˜¯ä¸€ä¸ªUniqueç±»å‹çš„æ•°æ®ï¼Œæœ¬æ¥æˆ‘ä»¬æ˜¯ä¸æ‰“ç®—çœ‹è¿™ä¸ªæ•°æ®çš„ã€‚ä½†å…¶å®æµè§ˆä¸€ä¸‹Nameä¹Ÿæ˜¯èƒ½çœ‹å‡ºä¸€äº›ç«¯å€ªçš„ï¼Œæ¯”å¦‚Braund, Mr. Owen Harrisï¼Œé™¤äº†åå­—è¿˜æœ‰ç§°å‘¼ï¼Œå¦‚Mrï¼ŒMissï¼ŒMrsä»¥åŠMasterè¿™æ ·çš„ä¿¡æ¯ã€‚æˆ–é¡»ä¹Ÿèƒ½å¤Ÿè®©æˆ‘ä»¬å¯¹Survivedåšå‡ºä¸€äº›åˆ¤æ–­ã€‚åŒæ ·ä¸Šæ–‡æåˆ°ï¼Œæˆ‘ä»¬å¯¹äºå…¶æ˜¯å¦ä¸ºä¸Šå±‚äººå£«éå¸¸æ„Ÿå…´è¶£ï¼Œæ‰€ä»¥titleæ²¡å‡†ä¹Ÿæ˜¯å¯ä»¥è¯´æ˜é—®é¢˜çš„ã€‚
# 

# In[9]:


dataset.Name.head(10)


# In[10]:


dataset['title'] = dataset.Name.apply(lambda x: x.split(', ')[1].split('. ')[0])
# Major, å°‘æ ¡ï¼›Ladyï¼Œè´µå¦‡ï¼›Sirï¼Œå­çˆµ; Capt, ä¸Šå°‰ï¼›the Countessï¼Œä¼¯çˆµå¤«äººï¼›Colï¼Œä¸Šæ ¡ã€‚Dr,åŒ»ç”Ÿï¼Ÿ
dataset['title'].replace(['Mme', 'Ms', 'Mlle'], ['Mrs', 'Miss', 'Miss'], inplace = True)
dataset['title'].value_counts()


# æˆ‘ä»¬çœ‹åˆ°ï¼Œè´µæ—æˆ–è€…å¤§äººæ•°ï¼Œéƒ½æ˜¯ç›¸å¯¹è¾ƒå°‘çš„ï¼Œæ‰€ä»¥è¿™é‡Œï¼Œæˆ‘ä»¬åªè¦é’ˆå¯¹ç»Ÿè®¡é‡ï¼Œå°±èƒ½å¤ŸåŒºåˆ†å¼€è¿™äº›äººç¾¤ã€‚å¦å¤–Masterè¿™ä¸ªtitle, å¯¹åº”æœªæ»¡18å²çš„å°ç”·å­©ï¼Œè€Œå°å¥³å­©åœ¨Missé‡Œã€‚å…¶ä»–titleéƒ½æ˜¯å¤§äººçš„å°Šç§°ï¼Œè¿™æˆ–è®¸æ˜¯ä¸ªæ¯”è¾ƒå¥½çš„Ageé¢„æµ‹é‡ã€‚

# In[11]:


dataset['title'] = dataset.title.apply(lambda x: 'rare' if x not in ['Mr', 'Miss', 'Mrs', 'Master'] else x)


# æˆ‘ä»¬å¦‚æœå°†å°å¥³å­©ä¹Ÿç‹¬ç«‹ç‹¬ç«‹å‡ºæ¥ï¼Œthis would be a great predictive variable to Ageï¼Œä½†è¿™ä¸ªå°±ç›¸å½“äºæ¼æ‰é‚£äº›æ²¡æœ‰å¹´é¾„çš„å§‘å¨˜äº†ï¼Œæˆ‘ä»¬éœ€è¦çœ‹ä¸€ä¸‹ï¼Œé‚£äº›æ²¡æœ‰å¹´é¾„çš„å§‘å¨˜å æ¯”èƒ½æœ‰å¤šå¤§ã€‚

# In[12]:


age_na_miss_rate = len(dataset[(dataset.title == 'Miss') & (dataset.Age.isnull()) ]) / (dataset.title == 'Miss').sum()
age_nna_not_mister_rate = len(dataset[(dataset.title == 'Miss') & (dataset.Age.notnull()) & (dataset.Age >= 18)]) / (dataset.title == 'Miss').sum()
print(age_na_miss_rate, age_nna_not_mister_rate)

len(dataset[(dataset.title == 'Miss') & (dataset.Age.isnull())]) / len(dataset)


# æ²¡æœ‰å¹´é¾„çš„å°å§å§å æ•´ä¸ªæ•°æ®é›†çš„3.89%,å› æ­¤å°±ç®—å› æ­¤è®¡ç®—é”™äº†ï¼Œå½±å“å€’ä¹Ÿä¸å¤§ï¼Œæ‰€ä»¥æˆ‘ä»¬å°±è¿™ä¹ˆåšäº†ã€‚

# In[13]:


dataset.title[(dataset.title == 'Miss') & (dataset.Age < 18)] = 'Mister'


# In[14]:


dataset.groupby('title').Age.agg([('number', 'count'), 'min', 'median',  'max'])


# æˆ‘ä»¬å†çœ‹çœ‹itleå¯¹survived å½±å“ã€‚

# In[15]:


dataset.groupby('title').Survived.agg(['count', 'sum', 'mean', 'std'])


# In[16]:


g= sns.factorplot(x='title', y = 'Survived', data=dataset, kind='bar', palette='cool', size=5)
g.ax.set_title('title for survived');


# In[17]:


dataset['title_level'] = dataset.title.map({"Miss": 3, "Mrs": 3,  "Master": 2, 'Mister': 2,  "rare":2, "Mr": 1})


# ## 4.3 Sex

# In[18]:


dataset.groupby('Sex').Survived.agg(['count', 'sum', 'mean', 'std'])


# å“‡å“¦ï¼Œå¥³æ€§å‡ ä¹è¾¾åˆ°äº†74%çš„å­˜æ´»ç‡ï¼Œè€Œç”·æ€§å‡ ä¹åªæœ‰18%çš„å­˜æ´»å‡ ç‡ï¼Œå¦‚æœæˆ‘ä»¬å†æ’é™¤ç”·æ€§é’å°‘å¹´ï¼Œä¼°è®¡æˆå¹´ç”·æ€§çš„å­˜æ´»æ›´æ˜¯ä½äº†ã€‚è¿™æ˜¯ä¸€ä¸ªå¾ˆé‡è¦çš„é¢„æµ‹å˜é‡ã€‚
# 

# In[19]:


dataset.Sex = dataset.Sex.map({'female':0, 'male':1})


# ## 4.4 SipSpä¸Parch
# å†æ¥çœ‹ä»¥ä¸‹ï¼ŒSibSpä¸Parchå¯¹Survivedçš„å½±å“ã€‚

# In[20]:


dataset.SibSp.value_counts().plot(kind='bar')


# In[21]:


dataset.Parch.value_counts().plot(kind='bar')


# In[22]:


# dataset.groupby('SibSp').Survived.agg(['count', 'sum', 'mean', 'std'])
sns.factorplot(x='SibSp', y='Survived', size=5, data=dataset, kind='bar', palette='cool')


# In[23]:


# df.groupby('Parch').Survived.agg(['count', 'sum', 'mean', 'std'])
sns.factorplot(x='Parch', y='Survived', size=5, data=dataset, kind='bar', palette='cool')


# ä»¥ä¸Šä¸¤ç»„æ•°æ®ï¼Œå¯ä»¥è®¤ä¸ºä¼¼ä¹æœ‰ä¸€ä¸¤ä¸ªParchæˆ–è€…ä¸€ä¸¤ä¸ªSibSpå­˜æ´»å‡ ç‡æ›´é«˜ä¸€ç‚¹ã€‚é‚£ä¹ˆæˆ‘ä»¬å°†è¿™ä¸ªç»„æˆå®¶åº­å¤§å°ï¼Œæ‰€è°“äººå¤šåŠ›é‡å¤§ï¼Œå¦‚æœæœ‰å…„å¼Ÿå§å¦¹çš„ä¼¼ä¹æ›´é«˜ã€‚å¾ˆå¤škernelé‡Œå°†family size + 1ï¼Œç„¶åå•ç‹¬åˆ—å‡ºä¸€ä¸ªaloneåˆ—ã€‚æˆ‘å†æƒ³familysizeä¸º0æ—¶å€™ï¼Œä¸å°±æ˜¯ä¸€ä¸ªäººå—ï¼Ÿä¸ºä»€ä¹ˆè¦å•ç‹¬åˆ—å‡ºæ¥ï¼Ÿ

# In[24]:


dataset['family_size'] = dataset.SibSp + dataset.Parch
# df.groupby('family_size').Survived.agg(['count', 'sum', 'mean', 'std'])
sns.factorplot(x='family_size', y='Survived', size=5, data=dataset, kind='bar', palette='cool')


# ç»“æœæ˜¾ç¤ºæœ‰1-3ä¸ªäº²å±çš„äººï¼Œå­˜æ´»å‡ ç‡ä¹Ÿæ›´é«˜ã€‚æˆ–è®¸æˆ‘ä»¬åº”è¯¥æ ¹æ®family sizeè¿›è¡Œåˆ†ç»„ï¼Œå¦‚åªèº«ä¸€äººï¼Œä»¥åŠæœ‰1-3äººäº²å±çš„å­˜æ´»ç‡ä¹Ÿè¾ƒé«˜ï¼Œè€Œæ›´å¤§çš„å®¶åº­ï¼Œè™½è¯´æ•°é‡ä¸å¤šï¼Œä½†æ˜¯å­˜æ´»å‡ ç‡å°±ç«‹åˆ»ä¸‹é™ã€‚å®¶åº­æˆå‘˜è¿‡å¤šï¼Œå°†å˜æˆè´Ÿæ‹…ã€‚æ‰€ä»¥å­˜æ´»ç‡å› æ­¤ä¸‹é™ã€‚è®©æˆ‘ä»¬è¯•è¯•çœ‹ï¼Œç»“æœä¸é”™å“¦ï¼Œå¯ä»¥è®¤ä¸ºä¹Ÿæ˜¯éå¸¸ä¸é”™çš„é¢„æµ‹å˜é‡ã€‚æ ‡ç­¾ç”¨æ•°å­—ï¼Œè¿™æ ·åé¢ä¹Ÿèƒ½å¤Ÿè®¡ç®—ç›¸å…³æ€§ã€‚

# In[25]:


dataset['family_size_level'] = pd.cut(dataset.family_size, bins=[-1,0, 3.5, 12], labels=['alone', 'middle', 'large'])
# df.groupby('family_size_level').Survived.agg(['count', 'sum', 'mean', 'std'])
sns.factorplot(x='family_size_level', y='Survived', size=5, data=dataset, kind='bar', palette='cool')
dataset['family_size_level'] = dataset['family_size_level'].map({'alone':1, 'middle':2, 'large':0})


# æˆ–è®¸ç»“åˆä¸€ä¸‹æ€§åˆ«çœ‹çœ‹ï¼Ÿ

# In[26]:


# dataset.groupby(['family_size_level', 'Sex']).Survived.agg(['count', 'sum', 'mean', 'std'])
sns.factorplot(x='family_size_level', y='Survived', hue='Sex', size=5, data=dataset, kind='bar', palette='cool')


# å¥³æ€§ä¸è®ºæ˜¯ç‹¬è‡ªä¸€äººè¿˜æ˜¯æœ‰äº²å±å­˜åœ¨ï¼Œéƒ½è¾¾åˆ°äº†éå¸¸é«˜çš„å­˜æ´»å‡ ç‡ã€‚è€Œç‹¬èº«ä¸€äººçš„é’å£®ä¸­è€å¹´æ­»äº¡ç‡éƒ½å¾ˆé«˜ã€‚è€Œå¯¹äºä¸­å‹å®¶åº­ï¼Œé™¤äº†è€å¹´å­˜æ´»ç‡è¾ƒä½å¤–ï¼Œå…¶ä»–ä¹Ÿéƒ½å¾ˆé«˜ã€‚

# ## 4.5 Fare
# ä¸Šé¢æåˆ°ï¼Œä¸Šå±‚çš„å­˜æ´»å‡ ç‡æ›´é«˜ï¼Œæ‰€ä»¥é™¤äº†Pclassèƒ½å¤Ÿè¡¨æ˜èº«ä»½ä¹‹å¤–ï¼Œå…¶å®æ‰€èŠ±è´¹ç”¨ä¹Ÿæ˜¯èº«ä»½è±¡å¾ã€‚

# In[27]:


dataset[dataset.Fare.isna()]


# In[28]:


dataset.Fare.fillna(dataset[dataset.Pclass == 3].Fare.median(), inplace=True)


# In[29]:


# dataset.Fare.hist(bins=20)
sns.distplot(dataset.Fare)


# ä¸Šå›¾æ˜¾ç¤ºï¼Œç»å¤§éƒ¨åˆ†çš„äººçš„è´¹ç”¨éƒ½é›†ä¸­åœ¨100è‹±é•‘ä»¥å†…ï¼ˆå‡å¦‚è´¹ç”¨æ˜¯ä»¥è‹±é•‘è®°çš„è¯ï¼Œå…¶å®è¿™éƒ½ä¸æ˜¯é—®é¢˜ï¼‰ã€‚æˆ‘ä»¬çŸ¥é“è´¢å¯Œæ˜¯ç¬¦åˆæ­£å¤ªåˆ†å¸ƒçš„ï¼Œä½†ä¹çœ‹ä¹‹ä¸‹è¿™å¹…å›¾å³è¾¹å°¾å·´å¤ªé•¿ï¼Œå·¦è¾¹å¤ªé«˜ï¼Œçœ‹èµ·æ¥åƒæ˜¯å¯¹æ•°æ­£å¤ªåˆ†å¸ƒã€‚æˆ‘ä»¬å†çœ‹çœ‹Pclasså’ŒFareçš„å…³ç³»ã€‚

# In[30]:


dataset.Fare = dataset.Fare.apply(lambda x: np.log(x) if x > 0 else 0)
sns.distplot(dataset.Fare)


# In[31]:


dataset.groupby('Pclass').Fare.agg(['count', 'sum', 'mean'])


# æˆ‘ä»¬åº”è¯¥æ ¹æ®PClasså¯¹è´¹ç”¨è¿›è¡Œä¸€ä¸ªåˆ†ç»„ï¼Œä¸è¿‡è¿™å…¶å®ç›¸å½“äºå¦ä¸€ä¸ªPclassäº†ï¼Œæ‰€ä»¥è¿˜æ˜¯ç®—äº†ã€‚æ„Ÿè§‰è¿™ä¸ªå˜é‡çš„ä½œç”¨ä¸Pclassçš„é‡å¤äº†ã€‚
# é‚£ä¹ˆæˆ‘ä»¬è¦æ€ä¹ˆåšå‘¢ï¼Ÿæœ‰ä¸¤ç§æ€è·¯ï¼Œä¸€ç§æ˜¯å°†FareæŒ‰ç…§2/8åˆ†ã€‚ç¬¬äºŒç§ï¼Œæ˜¯æ„å»ºæ–°çš„Featureï¼Œå³Pclass * Fare, å…¶ä¸­å°†Pclassè½¬æ¢æˆ3ï¼Œ2ï¼Œ1ä½¿å¾—èˆ±ç­‰çº§è¶Šé«˜ï¼Œè´¹ç”¨è¶Šå¤§ï¼Œè®©äºŒè€…æ›´åŠ å‡¸æ˜¾èº«ä»½å’Œåœ°ä½ã€‚å†è¯´è¿™ï¼Œåœ¨è¿™ä¸¤è€…åŸºç¡€ä¸Šï¼Œåœ¨ç¬¬äºŒç§æ„å»ºå®Œæˆåï¼Œå†æŒ‰ç…§2/8åˆ†ã€‚ä¼¼ä¹æœ€åä¸€ç§æœ€é è°±ï¼Œä¹Ÿè®¸å¯Œäººä½è°ƒï¼ŸèŠ±çš„å°‘ï¼Ÿé¦–å…ˆèˆ±ç­‰æ˜¯å¯¹Fareçš„ä¸€ç§å€ä¹˜ã€‚æ˜¯å¦éœ€è¦ä¸¤ä¸ªfeatureå€ä¹˜ï¼Œå¾—çœ‹ä¸¤ä¸ªfeatureæ˜¯å¦æœ‰ç›¸äº’æ•ˆåº”ï¼ŒæŒ‰ç…§ISLé‡Œæ‰€è¯´ã€‚

# In[32]:


# def doInverse(x):
#     if x == 3:
#         return 1
#     elif x == 1:
#         return 3
#     else:
#         return x

# df.Pclass.head(3), df.Pclass.apply(lambda x: doInverse(int(x))).head(3), df.Fare.head(3)

# fares = df.Fare.multiply(df.Pclass.apply(lambda x: doInverse(int(x))))
# plt.figure(figsize=(9, 6))
# fares.hist(bins=40)
# # è¿‡æ»¤å‡º20%çš„äººã€‚
# fares.quantile(0.80)


# In[33]:


# a = list(range(0, 401, 40))
# a.append(2500)


# df_temp = df.copy()
# df_temp['fare_level']= pd.cut(fares, bins=a)
# df_temp.groupby('fare_level').Survived.agg(['count', 'sum', 'mean'])


# In[34]:


# ä¸Šé™å°½é‡è®¾çš„å¤§ç‚¹ï¼Œå› ä¸ºæˆ‘ä»¬è¿™é‡Œæ²¡æŠŠtest dataä¹Ÿä¸€èµ·æ‹¿å‡ºæ¥çœ‹ï¼Œæ‰€ä»¥ä¸çŸ¥é“èŒƒå›´ã€‚
# df['upper_class'] = pd.cut(fares, bins=[0, 40, 160, 2500], labels=['low', 'middle', 'upper'])
# df.groupby('upper_class').Survived.agg(['count', 'sum', 'mean'])


# å½“æˆ‘æŠŠupper classï¼Œä¸è®ºæ˜¯åŠ å…¥åˆ°é¢„æµ‹Ageæˆ–è€…æ˜¯åŠ å…¥åˆ°é¢„æµ‹Survivedä¸­ï¼Œæœ€åç»“æœéƒ½æ˜¯ä¿æŒä¸å˜ï¼Œæ‰€ä»¥æˆ‘ä¾æ—§è§‰å¾—è¿™ä¸ªå˜é‡ä¸Pclassé‡äº†ï¼Œå†—ä½™äº†ã€‚

# In[35]:


# plt.figure(figsize=(9, 6))
# sns.heatmap(df[['Pclass', '']], cmap='cool', annot=True)


# ## 4.6 Ticket

# In[36]:


dataset.Ticket.head(5)


# Ticketæˆ–è®¸èƒ½å¤Ÿè¯´æ˜è·ç¦»é€ƒç”Ÿçª—å£ä¹‹ç±»çš„è¿œè¿‘ï¼Œç„¶åæˆ‘ä»¬æ²¡æœ‰è¿™æ–¹é¢çš„çŸ¥è¯†ï¼Œæš‚ä¸”ä¸è€ƒè™‘è¿™ä¸ªå˜é‡ï¼Œå¦åˆ™å°±èƒ½å¤Ÿæ ¹æ®ä»–ä»¬å’Œçª—å£çš„è·ç¦»åˆ’åˆ†ã€‚
# ## 4.7 Cabin
# å®¢èˆ±å·ç ç±»ä¼¼äºTicketçš„ä¸€ä¸ªå˜é‡ï¼Œä½†æ˜¯ç¼ºå¤±ä¸¥é‡ï¼Œ77%çš„æ•°æ®éƒ½å·²ç»ç¼ºå¤±ã€‚ä¼šæœ‰ç±»ä¼¼å›½å†…æ— åº§è¿™ç§æƒ…å†µå­˜åœ¨å—ï¼Ÿ

# In[37]:


dataset.Cabin.isna().sum() / len(dataset.Cabin)


# In[38]:


dataset.Cabin = dataset.Cabin.apply(lambda x : 0 if pd.isna(x) else 1)
sns.factorplot(x='Cabin', y='Survived', data=dataset, kind='bar')


# æœ‰åº§çš„æ˜æ˜¾è¦æ¯”æ— åº§çš„å¹¸å­˜ç‡é«˜å•Š

# ## 4.8 Embarked
# ç™»é™†æ¸¯å£ä¸å­˜æ´»å‡ ç‡æœ‰å…³å—

# In[39]:


dataset.Embarked.isna().sum(), '--'*12, dataset.Embarked.value_counts()


# In[40]:


dataset.Embarked.fillna('S', inplace=True)


# In[41]:


dataset.groupby('Embarked').Survived.agg(['count', 'sum', 'mean', 'std'])


# ä¼¼ä¹å„ä¸ªæ¸¯å£ç™»é™†çš„å­˜æ´»å‡ ä¹éƒ½å·®ä¸å¤šï¼Œå¦‚æœæˆ‘ä»¬ç»“åˆPClassçœ‹çœ‹æ˜¯å¦è¿˜æ˜¯å¦‚æ­¤å‘¢ã€‚

# In[42]:


dataset.groupby(['Pclass', 'Embarked']).Survived.agg(['count', 'sum', 'mean'])
sns.factorplot(x='Embarked', y = 'Survived', hue='Pclass', data=dataset, size=5, kind='bar', palette='cool')


# æˆ‘ä»¬æ³¨æ„åˆ°ä¸‰ç­‰èˆ±ä»Sæ¸¯å£ç™»èˆ¹çš„å¹¸å­˜ç‡æœ€ä½ï¼Œåªæœ‰0.189ã€‚ä¸Šé¢æåˆ°ï¼Œå¥³æ€§å­˜æ´»ç‡å¾ˆé«˜ï¼Œé‚£ä¹ˆä¸‰ç­‰èˆ±Sæ¸¯å£æ˜¯å¦æ˜¯ç”·æ€§æ¯”ä¾‹è¾ƒé«˜å¯¼è‡´çš„å‘¢ï¼Ÿé‚£ä¹ˆæˆ‘ä»¬é‡ç‚¹è§‚å¯Ÿä»¥ä¸‹Sæ¸¯å£çš„ä¸‰ç­‰èˆ±çš„æ€§åˆ«æ¯”ä¾‹ã€‚

# In[43]:


# a = df.groupby(['Pclass', 'Embarked', 'Sex']).Survived.agg(['count', 'sum', 'mean', 'std'])
dataset[(dataset.Embarked == 'S') & (dataset.Pclass == 3)].groupby('Sex').Survived.agg(['count', 'sum', 'mean', 'std'])


# ä¸‰ç­‰èˆ±çš„å­˜æ´»å‡ ç‡éƒ½å¾ˆä½ï¼Œå¥³æ€§ä¹Ÿè¶…è¿‡ä¸€èˆ¬æ­»äº¡ï¼Œç”·æ€§å°±æ›´ä½äº†ï¼Œåªæœ‰12.8%ã€‚è¦åŠ å…¥ä¸€ç±»åˆ¤æ–­æ˜¯å¦æ˜¯ä¸‰ç­‰èˆ±ï¼ŒSæ¸¯å£çš„ï¼Ÿ
# 

# ## 4.9 Age
# ç”±äºAgeæ•°æ®æœ‰ç¼ºå¤±ï¼Œåªé’ˆå¯¹æœ‰æ•°æ®çš„éƒ¨åˆ†å…ˆåšä¸€ä¸‹ç»Ÿè®¡ã€‚å…ˆçœ‹çœ‹å¹´é¾„åˆ†å¸ƒï¼Œè¿˜æ˜¯å‘ˆç°æ­£å¤ªåˆ†å¸ƒçš„ã€‚å¦‚æœæˆ‘ä»¬åé¢Wrongle Dataçš„è¯ï¼Œä¹Ÿæ˜¯æœ‰å¿…é¡»è¦çœ‹çœ‹å¡«å……åæ˜¯å¦ç»§ç»­ç¬¦åˆè¿™ä¸ªå¹´é¾„åˆ†å¸ƒã€‚

# In[44]:


# df.Age.hist(bins=20)
sns.distplot(dataset.Age[dataset.Age.notna()])


# In[45]:


fig = plt.figure(figsize=(9, 7))
g = sns.kdeplot(dataset.Age[(dataset.Survived == 1) & (dataset.Age.notna())], color='r', ax=fig.gca())
g = sns.kdeplot(dataset.Age[(dataset.Survived == 0) & (dataset.Age.notna())], color='b', ax=fig.gca())
g.set_xlabel('Age')
g.set_ylabel('Survived')
g.legend(['Survived', 'Not'])


# In[46]:


dataset['age_level'] = pd.cut(dataset.Age, bins=[0, 18, 60, 100], labels=[3, 2, 1])
# dataset.groupby('age_level').Survived.agg(['count', 'sum', 'mean', 'std'])
sns.factorplot('age_level', 'Survived', hue='Sex', data=dataset, kind='bar', size=5, palette='cool')


# è¯´æ˜å¹¼å„¿å­˜æ´»ç‡æ›´é«˜ï¼Œè€äººçš„å­˜æ´»å‡ ç‡éå¸¸ä½ï¼Œè€Œé’å£®å¹´ç”·æ€§æ˜¯ç‰ºç‰²çš„ä¸»åŠ›ã€‚æˆ‘ä»¬å°†å…¶ä¸æ€§åˆ«ç»“åˆçœ‹çœ‹ç»“æœã€‚holycrap, é’å£®å¹´ç”·æ€§å­˜æ´»å‡ ç‡ä¸åˆ°20%ï¼Œå³ä½¿æ˜¯è€å¹´ç»„é‡Œï¼Œè€å¹´ç”·æ€§ä¹Ÿå‡ ä¹å…¨è·ªã€‚

# # 5. Cleaning Data, Wrangle
# æˆ‘ä»¬çŸ¥é“ç¼ºå¤±æ•°æ®çš„æœ‰Cabin>Age>Embarkedã€‚Cabinæˆ‘ä»¬å·²ç»å°†å…¶è½¬æ¢ä¸ºæœ‰æˆ–æ— äº†ï¼ŒEmbarkedçš„ä¸¤ä¸ªç¼ºå¤±ä¹Ÿè¢«æˆ‘ä»¬å¡«å……äº†ä¼—æ•°ã€‚è€ŒAgeæ¯•ç«Ÿæ˜¯ä¸€ä¸ªéå¸¸å½±å“é¢„æµ‹ç»“æœçš„å˜é‡ï¼Œæˆ‘ä»¬éœ€è¦å¥½å¥½å¡«å……ä¸€ä¸‹
# ## 5.1 Fill Age
# æˆ‘ä»¬è¿™é‡Œå°±ä¸å»é¢„æµ‹ç¦»æ•£çš„å¹´é¾„äº†ï¼Œè€Œæ˜¯é¢„æµ‹å¹´é¾„çš„åˆ†ç»„ï¼Œè¿™æ ·å°±ä½¿å¾—å›å½’çš„å¹´é¾„é—®é¢˜è½¬å˜ä¸ºåˆ†ç»„é—®é¢˜ã€‚

# In[47]:


dataset.head(5)


# æˆ‘ä»¬éœ€è¦é‚£äº›å˜é‡æ¥é¢„æµ‹æˆ‘ä»¬çš„å¹´é¾„å‘¢ï¼Ÿè¿™é‡Œæˆ‘ä»¬å°±å¯ä»¥çœ‹çœ‹ç›¸å…³æ€§çŸ©é˜µ

# In[48]:


# dataset[dataset.age_level.notna()].age_level = dataset.age_level[dataset.age_level.notna()].astype('int')
fig = plt.figure(figsize=(9, 6))
sns.heatmap(dataset[['Age', 'Survived', 'Pclass', 'Sex', 'family_size_level', 'Parch', 'SibSp', 'title_level', 'age_level', 'Fare']].corr(), cmap='cool', annot=True, ax=fig.gca())


# ä¸Šå›¾ä¸­ï¼Œè·ŸAgeç›¸å…³æ¯”è¾ƒå¤§çš„æœ‰Pclass, family_size, Fareï¼ŒSexå’Œtitle_levelæ— å…³ï¼Œè¿™å¯èƒ½ä¸æˆ‘ä»¬è®¾ç½®çš„å€¼æœ‰å…³ç³»ã€‚

# In[49]:


dataset.groupby('title_level').Age.agg(['mean', 'median', 'std'])


# In[50]:


dataset.groupby(['title']).Age.agg(['mean', 'median', 'std', 'max'])


# æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œtitle_levelè®¾ç½®çš„å¯¹Ageæ³¢åŠ¨éå¸¸å¤§ï¼Œè€Œå…¶æœ¬èº«titleçš„stdæ³¢åŠ¨å°±éå¸¸å°ï¼Œè¯´æ˜åŸºæœ¬ä¸Šéƒ½åœ¨meançš„é™„è¿‘ã€‚æ‰€ä»¥æˆ‘ä»¬è®¤ä¸ºtitleä¹Ÿæ˜¯ä¸ageæ¯”è¾ƒç›¸å…³çš„ä¸€ä¸ªé‡è¦å› ç´ ã€‚å‡å¦‚æˆ‘ä»¬æŠŠtitleæŒ‰ç…§å¹´é¾„é‡æ–°åˆ†ç»„ï¼Œæˆ‘ä»¬å°±æ¯”è¾ƒæœ‰å¯èƒ½å¾—åˆ°ç›¸å…³æ€§çš„è¯æ®äº†ï¼Œæˆ‘ä»¬æŒ‰ç…§Ageçš„å‡å€¼ä¸ä¸­å€¼å¯¹titleè¿›è¡Œé‡æ–°ç¼–ç å€¼ã€‚

# In[51]:


dataset['title_age_level'] = dataset.title.map({"Master": 1, 'Mister': 1, "Miss": 2, "Mrs": 3,  "Mr": 3, "rare": 4})
# dataset['title_age_level'] 


# In[52]:


fig = plt.figure(figsize=(9, 6))
sns.heatmap(dataset[['Age', 'Survived', 'Pclass', 'Sex', 'family_size_level', 'Parch', 'SibSp', 'title_level', 'title_age_level', 'age_level', 'Fare']].corr(), cmap='cool', annot=True, ax=fig.gca())


# In[53]:


['Survived', 'Pclass', 'Sex', 'family_size_level', 'title_level', 'age_level', 'Fare']


# è¿™æ ·ç»“æœå°±æ¯”è¾ƒæ˜æ˜¾äº†, 
# - Ageä¸Pclassï¼Œfamily_size, title_age_level, Fareçš„ç›¸å…³æ€§å°±æ¯”è¾ƒæ˜æ˜¾äº†ã€‚
# - åŒæ—¶ï¼Œæˆ‘ä»¬è¿˜å‘ç°Fareä¸Pclassä¹Ÿå…·æœ‰å¼ºç›¸å…³æ€§ï¼Œ
# - Sexä¸çŸ¥é“æ€ä¹ˆä¸title_levelè¾¾åˆ°å¦‚æ­¤å¼ºçš„è´Ÿç›¸å…³ã€‚ã€‚ã€‚ã€‚
# - å¦‚æœè€ƒè™‘Survivedçš„è¯ï¼Œä¹Ÿèƒ½å¤Ÿçœ‹å‡ºï¼ŒSurvivedä¸Pclassï¼ŒSexï¼Œtitle_level, title_age_level, Fareçš„ç›¸å…³æ€§éƒ½ä¸é”™ï¼Œè¿™é‡Œæˆ‘ä»¬çš„family_sizeå±…ç„¶æ²¡é‚£ä¹ˆæ˜æ˜¾ï¼Œçœ‹æ¥è¿˜æ˜¯å€¼è®¾ç½®çš„æœ‰é—®é¢˜ï¼Ÿ

# In[54]:


df_age_train = dataset[dataset.Age.notnull()]
df_age_test = dataset[dataset.Age.isnull()]

df_age_train.shape, df_age_test.shape


# In[55]:


df_age = df_age_train[['Pclass', 'Sex', 'Parch', 'SibSp', 'title_age_level', 'Fare', 'age_level']]
X_age_dummied = pd.get_dummies(df_age.drop(columns='age_level'), columns=['Pclass', 'Sex', 'Parch', 'SibSp', 'title_age_level'])
X_age_dummied['SibSp_8'] = np.zeros(len(X_age_dummied))
X_age_dummied['Parch_9'] = np.zeros(len(X_age_dummied))


Y_age = df_age['age_level']


# In[56]:


from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier

params = {'n_estimators':range(10, 40, 5), 'max_depth':[3, 4, 5], 'max_features':range(3, 9)}
clf = RandomForestClassifier(random_state=0)

gscv = GridSearchCV(estimator=clf, param_grid=params, scoring='f1_micro', n_jobs=1, cv=5, verbose=1)
gscv.fit(X_age_dummied, Y_age)


# In[57]:


gscv.best_score_, gscv.best_params_, gscv.best_estimator_.feature_importances_


# > åœ¨åŠ å…¥å¯¹å°å§å§çš„titleMisteråï¼Œæˆ‘ä»¬çš„Ageçš„é¢„æµ‹å‡†ç¡®ç‡åˆä¸Šå‡äº†ï¼Œå¹¶ä¸”ç»è¿‡ä¸¤æ¬¡è°ƒå‚ï¼Œè·å¾—äº†0.94537815126050417çš„f1_micro.
# 
# ä¹‹å‰çš„ç‰ˆæœ¬è¾¾åˆ°äº†0.945ï¼Œç°åœ¨æ”¹æˆå…¨éƒ¨æ•°æ®ï¼Œåè€Œè¾¾ä¸åˆ°äº†ã€‚çœ‹æ¥æœ‰ç¦»ç¾¤ç‚¹åŠ å…¥è¿›æ¥äº†ï¼Œå¯¼è‡´æˆ‘ä»¬æ— æ³•è·å¾—æ¯”è¾ƒå¥½çš„ç»“æœã€‚ğŸ˜…

# In[58]:


pd.Series(gscv.best_estimator_.feature_importances_, index=X_age_dummied.columns).sort_values(ascending=True).plot.barh(figsize=(9, 6))
sns.despine(bottom=True)


# In[59]:


X_age_dummied = pd.get_dummies(df_age.drop(columns='age_level'), columns=['Pclass', 'Sex', 'Parch', 'SibSp', 'title_age_level'])

ab = df_age_test[['Pclass', 'Sex', 'Parch', 'SibSp', 'title_age_level', 'Fare']]
X_age_dummied_test = pd.get_dummies(ab, columns=['Pclass', 'Sex', 'Parch', 'SibSp', 'title_age_level'])
X_age_dummied.shape, X_age_dummied_test.shape


# In[60]:


X_age_dummied.columns, X_age_dummied_test.columns


# In[61]:


X_age_dummied_test['Parch_3'] = np.zeros(len(X_age_dummied_test))
X_age_dummied_test['Parch_5'] = np.zeros(len(X_age_dummied_test))
X_age_dummied_test['Parch_6'] = np.zeros(len(X_age_dummied_test))

X_age_dummied_test['SibSp_4'] = np.zeros(len(X_age_dummied_test))
X_age_dummied_test['SibSp_5'] = np.zeros(len(X_age_dummied_test))

df_age_test.age_level = gscv.predict(X_age_dummied_test)


# In[62]:


df_age_test.shape


# In[63]:


df_final = pd.concat([df_age_test, df_age_train]).sort_index()
df_final.info()


# ## 5.3 Clean Done
# Trainningé‡Œçš„æ‰€æœ‰æ•°æ®éƒ½å·²ç»æ¸…ç†å®Œæ¯•ï¼Œæ²¡æœ‰NAå€¼äº†ï¼Œæ¥ä¸‹æ¥å°±è¿›å…¥æ¨¡å‹è¯„ä¼°é˜¶æ®µã€‚
# # 6. Basic Model & Evalution
# ## 6.1 Model & Evaluation
# æˆ‘ä»¬é€‰ç”¨å¦‚ä¸‹çš„Modelsï¼Œéƒ½æ˜¯æˆ‘åœ¨è¯¾ä¸Šå­¦è¿‡çš„é¢ï¼Œç„¶åç”¨cross_validataion_scoreè·å¾—ä¸€ä¸ªåˆå§‹è¯„ä¼°ã€‚
# - Logistic Regression
# - KNN
# - SVM
# - Naive Bayes
# - Nerual Network, MLP
# - DecisionTree
# - RandomForest
# - Gradient Boosting Decision Tree

# In[64]:


from sklearn.model_selection import cross_val_score

from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.neural_network import MLPClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier


# In[65]:


names = ['LR', 'KNN', 'SVM', 'Bayes', 'NW', 'DT', 'RF', 'GBDT']
models = [LogisticRegression(random_state=0), KNeighborsClassifier(n_neighbors=3), SVC(gamma='auto', random_state=0), GaussianNB(), MLPClassifier(solver='lbfgs', random_state=0),
         DecisionTreeClassifier(), RandomForestClassifier(random_state=0), GradientBoostingClassifier(random_state=0)]


# Feature Selectionï¼Œæˆ‘ä»¬é€‰æ‹©åœ¨ä¸Šé¢é‚£å¼ é‡Œå’ŒSurvivedå…·æœ‰æ¯”è¾ƒå¤§çš„ç›¸å…³æ€§çš„Featureã€‚

# In[66]:


selection = ['Survived', 'Pclass', 'Sex', 'family_size_level', 'title_level', 'age_level', 'Fare', 'Embarked', 'Cabin']
train_set = df_final[df_final.Survived.notna()][selection]
test_set = df_final[df_final.Survived.isna()][selection]
X_train = train_set.drop(columns='Survived')
Y_train = train_set['Survived']
X_test = test_set.drop(columns='Survived')

X_train= pd.get_dummies(X_train, columns=['Pclass', 'Sex', 'family_size_level', 'title_level', 'age_level', 'Embarked', 'Cabin'])
X_test = pd.get_dummies(X_test, columns=['Pclass', 'Sex', 'family_size_level', 'title_level', 'age_level', 'Embarked', 'Cabin'])
X_train.shape, X_test.shape


# In[67]:


X_train.columns, X_test.columns


# In[68]:


for name, model in zip(names, models):
    score = cross_val_score(model, X_train, Y_train, cv=5, scoring='roc_auc')
    print('score on {}, mean:{:.4f}, from {}'.format(name, score.mean(), score))


# æœªè°ƒä»“æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬åœ¨LRå’ŒGBDTä¸Šè·å¾—ä¸¤ä¸ªæœ€é«˜åˆ†æ•°0.8668ï¼Œ0.8842ã€‚ç”±äºæŸäº›Classifierå¯¹æ•°æ®çš„é‡çº§æ•æ„Ÿï¼Œå› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦å¯¹æ•°æ®è¿›è¡Œä¸€ä¸ªé¢„å¤„ç†ã€‚ä½†æ˜¯ï¼Œæˆ‘ä»¬å‡ ä¹æ‰€æœ‰çš„æ•°æ®éƒ½è¢«è½¬æ¢æˆäº†dummiesçš„å½¢å¼ï¼ŒFareæ•°æ®ä¹Ÿè¢«æˆ‘ä»¬logåŒ–ï¼Œè½¬æ¢åˆ°0-7ä¹‹é—´äº†ï¼Œæ‰€ä»¥æˆ‘è§‰å¾—å¹¶ä¸éœ€è¦`from sklearn.preprocessing import StandardScaler`å®ƒå»ç¼©æ”¾æ•°æ®ã€‚
# 
# ## 6.2 Feature importance
# æˆ‘ä»¬å¯ä»¥ç›´è§‚ä¸Šçœ‹çœ‹LogisticRegressionè®¤ä¸ºå“ªäº›featureæƒé‡æ¯”è¾ƒé«˜ã€‚

# In[69]:


lr = LogisticRegression(random_state=0).fit(X_train, Y_train)
lr.coef_


# In[70]:


from matplotlib import cm
color = cm.inferno_r(np.linspace(.4,.8, len(X_train.columns)))

pd.DataFrame({'weights': lr.coef_[0]}, index=X_train.columns).sort_values(by='weights',ascending=True).plot.barh(figsize=(7, 7),fontsize=12, legend=False, title='Feature weights from Logistic Regression', color=color);
sns.despine(bottom=True);


# è¿™ä¸ªFeature Weightsè¿˜æŒºç¬¦åˆæˆ‘ä»¬çš„è®¤çŸ¥çš„ï¼Œ**familysize_level_1ï¼ˆ1äººï¼‰ï¼Œfamilysize_level_2ï¼ˆ2-4äººï¼‰ï¼Œtitle_level_3(Mrs, Miss)ï¼Œage_level_3(child)ï¼Œ Sex_0(å¥³æ€§)ï¼Œtitle_level_2ï¼ˆMister, Master, rareï¼‰, Fare(è´¹ç”¨)ï¼ŒCabin_1(æœ‰å®¢èˆ±)ï¼ŒPclass_1ï¼ˆ1ç­‰èˆ±ï¼‰ï¼ŒPclass_2(äºŒç­‰èˆ±)ï¼ŒEmbarked_C(Cæ¸¯å£)**éƒ½å…·æœ‰æé«˜å¹¸å­˜ç‡çš„å‚æ•°ã€‚**è¿™é‡Œä¸ºä»€ä¹ˆfamilysize_levelä¸º1ä¸ªäººæ—¶å€™ï¼Œä¸ºä»€ä¹ˆå¯¹åˆ¤å®šå½±å“è¿™ä¹ˆå¤§**ï¼Ÿï¼Ÿï¼Ÿ
# 
# å¦å¤–ï¼Œ**faimily_size_level_0ï¼ˆå¤§å‹å®¶åº­ï¼‰ï¼Œtitle_level_1ï¼ˆMrï¼‰, Sex_1(male)ï¼Œage_level_1(è€å¹´äºº)ï¼ŒCabin_0(æ— å®¢èˆ±)ï¼ŒPclass_3ï¼ˆä¸‰ç­‰èˆ±ï¼‰ï¼ŒEmbarked_S(Sæ¸¯å£)ã€age_level_2(å¹´è½»-ä¸­å¹´äºº)**éƒ½å°±å…·æœ‰è¾ƒå¤§çš„è´Ÿæƒé‡ï¼Œè¿™äº›éƒ½æ˜¯å¯¹Survivedçš„debuffã€‚ä¸æˆ‘ä»¬ä¸Šé¢æœ€å¼€å§‹featureåˆ†æçš„è¿˜æ˜¯ä¸€è‡´çš„ã€‚
# 
# 
# æˆ‘ä»¬è¿˜å¯ä»¥è¯•ç€çœ‹çœ‹DecisionTreeçš„feature importance

# In[71]:


def plot_decision_tree(clf, feature_names, class_names):
    from sklearn.tree import export_graphviz
    import graphviz
    export_graphviz(clf, out_file="adspy_temp.dot", feature_names=feature_names, class_names=class_names, filled = True, impurity = False)
    with open("adspy_temp.dot") as f:
        dot_graph = f.read()
    return graphviz.Source(dot_graph)

dtc = DecisionTreeClassifier().fit(X_train, Y_train)

pd.DataFrame({'importance': dtc.feature_importances_}, index=X_train.columns).sort_values(by='importance',ascending=True).plot.barh(figsize=(7, 7),fontsize=12, legend=False, title='Feature importance from Decision Tree', color=color);
sns.despine(bottom=True, left=True);

# è¿™ä¸ªDecisionTreeå¤ªå¤§äº†å•Šï¼ŒçœŸå®å¤©æ€§å®¹æ˜“è¿‡æ‹Ÿåˆï¼Œè¿™å±‚æ•°ä¹Ÿå¤ªå¤šäº†ï¼Œæ‰€ä»¥ä¸ç”»äº†ã€‚
# plot_decision_tree(dtc, X_train.columns, 'Survived')


# DTé‡Œçš„feature importanceï¼ŒæŒ‰æˆ‘çš„ç†è§£ï¼Œæ˜¯æŒ‰ç…§**title_level_1ï¼ˆMrï¼‰è¿›è¡Œå¤§èŒƒå›´æ‹†åˆ†ï¼Œç„¶åæ˜¯Fareã€family_size_0ï¼ˆå®¶åº­æˆå‘˜è¿‡å¤šçš„ï¼‰ã€Pclass_3ï¼ˆä¸‰ç­‰èˆ±ï¼‰ã€Embarked_Sï¼ˆSæ¸¯å£ï¼‰ï¼Œage_level_2ï¼ˆå¹´è½»-ä¸­å¹´ï¼‰ã€age_level_3(è€å¹´äºº)ã€age_level_1(å„¿ç«¥)ã€Cabin_0ï¼ˆæ— å®¢èˆ±ï¼‰**ç­‰ç­‰ç­‰ã€‚
# 
# ## 6.3 Misclassification Analysis
# é€šè¿‡è§‚å¯Ÿè¢«ç®—æ³•è¯¯åˆ†ç±»çš„recordsï¼Œè®¾è®¡å‡ºæ–°çš„featureæˆ–å…¶ä»–çš„æ–¹æ³•ï¼Œæ¥å‡å°‘è¯¯åˆ†ç±»ï¼Œæœ€ç»ˆè¿›ä¸€æ­¥æé«˜åˆ†ç±»çš„å‡†ç¡®åº¦ã€‚æ€ä¹ˆåšå‘¢ï¼Ÿè¿˜æ˜¯åœ¨K-Foldä¸Šï¼Œç„¶åè°ƒç”¨ç®—æ³•ï¼Œå°†å…¶åˆ†ç±»é”™è¯¯çš„æ‰¾å‡ºæ¥ã€‚

# In[72]:


from sklearn.model_selection import StratifiedKFold
skf = StratifiedKFold(n_splits=4, random_state=0)

# from sklearn.model_selection import LeaveOneOut
# loo = LeaveOneOut()
# miss classifications
mcs = []

for train_index, test_index in skf.split(X_train, Y_train):
    x_train, x_test = X_train.iloc[train_index], X_train.iloc[test_index]
    y_train, y_test = Y_train.iloc[train_index], Y_train.iloc[test_index]
    y_pred =lr.fit(x_train, y_train).predict(x_test)
    mcs.append(x_test[y_pred != y_test].index.tolist())


# In[73]:


mcs_index = np.concatenate((mcs))
len(mcs_index), len(Y_train), 'miss classification rate:', len(mcs_index)/len(Y_train)


# æˆ‘ä»¬çœ‹åˆ°Miss Classification Rateä¸º18.4%ï¼Œä¸æˆ‘ä»¬ä¹‹å‰æ‰€å¾—ä¹Ÿå·®ä¸å¤šã€‚æ¥ä¸‹æ¥æˆ‘ä»¬å°±çœ‹çœ‹è¿™166ä¸ªï¼ŒæŒ‰ç…§LRçš„feature weightsä¸ºä½•ä¼šè¢«é”™åˆ†ç±»å‘¢ï¼Œæœ‰ä»€ä¹ˆç‰¹ç‚¹å—ï¼Ÿ

# In[74]:


# mcs_df = pd.concat([X_train.iloc[mcs_index], Y_train.iloc[mcs_index]], axis=1)
mcs_df = train_set.iloc[mcs_index]
mcs_df.head()


# In[75]:


mcs_df.describe(include='all').T


# æˆ‘ä»¬è§£è¯»ä¸€ä¸‹ä»¥ä¸Šæ•°æ®ï¼Œå®ƒæè¿°äº†å¤§éƒ¨åˆ†è¢«åˆ†ç±»é”™è¯¯çš„äººçš„åŸºæœ¬ä¿¡æ¯ï¼Œè¿™é‡Œçš„å­˜æ´»ç‡ä¸º0.373ï¼Œéå¸¸ä½ï¼Œå³å¤§éƒ¨åˆ†éƒ½æ˜¯æ­»äº¡ï¼Œ**ä¹Ÿå°±æ˜¯æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ€ä¹ˆè®©è¿™äº›äººè¢«é¢„æµ‹ä¸ºæ­»äº¡**ã€‚å…¶ä¸­å¤§éƒ¨åˆ†ä¸‰ç­‰èˆ±ï¼Œç”·æ€§ï¼ŒSæ¸¯å£ï¼Œç‹¬èº«ï¼Œä¸­å¹´äººè¢«é¢„æµ‹ä¸ºå­˜æ´»ã€‚æ‰€ä»¥æˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ ¹æ®LRçš„feature weightsï¼Œæ·»åŠ æ›´å®¹æ˜“è¢«é¢„æµ‹ä¸ºæ­»äº¡çš„featureèåˆï¼Œå³Feature weightsä¸ºè´Ÿçš„é‚£äº›featureã€‚
# 
# æˆ‘ä»¬æŠŠmcs_dfé‡Œé‚£äº›å æ¯”ä¾‹æ¯”è¾ƒå¤šçš„Featureï¼Œå¹¶ä¸”æ˜¯LR feature weightsé‡Œä¸ºæ­£çš„é‚£äº›ä¸»è¦featureæå–å‡ºæ¥ï¼Œè¿›è¡Œç»„åˆä»¥ä¾¿äºé™ä½åˆ¤å®šä¸º1çš„å‡ ç‡ã€‚
# > faimily_size_level_0ï¼ˆå¤§å‹å®¶åº­ï¼‰ï¼Œtitle_level_1ï¼ˆMrï¼‰, Sex_1(male)ï¼Œage_level_1(è€å¹´äºº)ï¼ŒCabin_0(æ— å®¢èˆ±)ï¼ŒPclass_3ï¼ˆä¸‰ç­‰èˆ±ï¼‰ï¼ŒEmbarked_S(Sæ¸¯å£)ã€age_level_2(å¹´è½»-ä¸­å¹´äºº)
# 
# 
# æ­£å‚æ•°featureã€‚
# 
# > familysize_level_1ï¼ˆ1äººï¼‰ï¼Œfamilysize_level_2ï¼ˆ2-4äººï¼‰ï¼Œtitle_level_3(Mrs, Miss)ï¼Œage_level_3(child)ï¼Œ Sex_0(å¥³æ€§)ï¼Œtitle_level_2ï¼ˆMister, Master, rareï¼‰, Fare(è´¹ç”¨)ï¼ŒCabin_1(æœ‰å®¢èˆ±)ï¼ŒPclass_1ï¼ˆ1ç­‰èˆ±ï¼‰ï¼ŒPclass_2(äºŒç­‰èˆ±)ï¼ŒEmbarked_C(Cæ¸¯å£)

# In[76]:


mcs_df[mcs_df.family_size_level == 1].shape, mcs_df[mcs_df.title_level == 3].shape, mcs_df[mcs_df.age_level == 3].shape


# ç”±äºfamily_size_level=1ï¼ˆä¸€ä¸ªäººï¼‰ç»™äº†è¿‡å¤§çš„æ¯”é‡ï¼Œé€ æˆé”™è¯¯çš„åˆ†ç±»ï¼Œé‚£ä¹ˆæ€ä¹ˆé™ä½è¿™ä¸ªfamily_size_levelä¸ºä¸€ä¸ªäººæ—¶å€™çš„æ¯”é‡å‘¢ï¼Ÿ

# In[77]:


mcs_df.groupby('family_size_level').Survived.mean()


# In[78]:


# df_final['MPSE'] = np.ones(len(df_final))
# df_final.MPSE[(df_final.title_level == 1) & (df_final.Pclass == 3) & (df_final.Sex == 0) & (df_final.Embarked == 'S') & (df_final.family_size_level.isin([1, 2]))] = 4
# df_final.MPSE[(df_final.title_level == 1) & (df_final.Pclass == 2) & (df_final.Sex == 0) & (df_final.Embarked == 'S') & (df_final.family_size_level.isin([1, 2]))] = 3
# df_final.MPSE[(df_final.title_level == 1) & (df_final.Pclass == 1) & (df_final.Sex == 0) & (df_final.Embarked == 'S') & (df_final.family_size_level.isin([1, 2]))] = 2
# df_final.MPSE.value_counts()

df_final['Alone'] = np.zeros(len(df_final))
df_final['Alone'][df_final.family_size_level == 1] = 10
df_final['EmbkS'] = np.zeros(len(df_final))
df_final['EmbkS'][df_final.Embarked == 'S'] = 10
df_final['MrMale'] = np.zeros(len(df_final))
df_final['MrMale'][df_final.title_level == 1] = 10


# éªŒè¯ä¸€ä¸‹ï¼Œæ˜¯å¦æœ‰æ”¹è¿›ï¼Ÿä»0.8668ï¼Œåˆ°0.8674ï¼Œ åªæœ‰0.001ä¸ç‚¹å„¿çš„æ”¹è¿›ï¼ŒğŸ˜°

# In[79]:


selection = ['Survived', 'Pclass', 'Sex', 'family_size_level', 'title_level', 'age_level', 'Fare', 'Embarked', 'EmbkS', 'MrMale', 'Cabin', 'Alone']
train_set = df_final[df_final.Survived.notna()][selection]
test_set = df_final[df_final.Survived.isna()][selection]
X_train = train_set.drop(columns='Survived')
Y_train = train_set['Survived']
X_test = test_set.drop(columns='Survived')

X_train= pd.get_dummies(X_train, columns=['Pclass', 'Sex', 'family_size_level', 'title_level', 'age_level', 'Embarked', 'Cabin'])
X_test = pd.get_dummies(X_test, columns=['Pclass', 'Sex', 'family_size_level', 'title_level', 'age_level', 'Embarked', 'Cabin'])

val_lr = LogisticRegression(random_state=0)
a_score = cross_val_score(val_lr, X_train, Y_train, cv=5, scoring='roc_auc')
a_score.mean()


# # 7 Hyperparameters Tuning
# æˆ‘ä»¬é€‰å–å‡ ä¸ªè¡¨ç°æ¯”è¾ƒå¥½çš„è¿›è¡Œè°ƒå‚ï¼Œçœ‹çœ‹æœ€ç»ˆç»“æœã€‚å¦‚Logistic Regressionï¼ŒKNNï¼ŒSVMï¼Œ NWï¼Œ RFï¼ŒGBDT

# In[80]:


from sklearn.ensemble import GradientBoostingClassifier

def tune_estimator(name, estimator, params):
    gscv_training = GridSearchCV(estimator=estimator, param_grid=params, scoring='roc_auc', n_jobs=1, cv=5, verbose=False)
    gscv_training.fit(X_train, Y_train)
    return name, gscv_training.best_score_, gscv_training.best_params_


# ## 7.1 LR

# In[81]:


from sklearn.linear_model import LogisticRegression
params = {'C':[0.03, 0.1, 0.3, 1, 3, 10, 20, 30, 50]}
clf = LogisticRegression(random_state=0)
tune_estimator('LR', clf, params)


# In[82]:


# Second fine tuning.
params = {'C':[0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]}
clf = LogisticRegression(random_state=0)

tune_estimator('LR', clf, params)


# ## 7.2 KNN

# In[83]:


params = {'n_neighbors': range(3, 15, 3)}
clf = KNeighborsClassifier()
tune_estimator('KNN', clf, params)


# In[84]:


params = {'n_neighbors': [10, 11, 12, 13, 14]}
clf = KNeighborsClassifier()
tune_estimator('KNN', clf, params)


# ## 7.3 SVM

# In[85]:


params = {'C':[0.01, 0.1, 1, 10], 'gamma':[0.001, 0.01, 0.1], 'kernel':['rbf']}
clf = SVC(random_state=0)
tune_estimator('SVM', clf, params)


# In[86]:


params = {'C':range(8, 14), 'gamma':[0.05, 0.08, 0.01, 0.03, 0.05], 'kernel':['rbf']}
clf = SVC(random_state=0)
tune_estimator('SVM', clf, params)


# ## 7.4 Nerual Network

# In[87]:


params = {'hidden_layer_sizes':[x for x in zip(range(20, 100, 10), range(20, 100, 20))],
          'solver':['lbfgs'], 'alpha': [0.0001, 0.001, 0.01]}
clf = MLPClassifier(random_state = 0)
tune_estimator('NW', clf, params)


# ## 7.5 Random Forest

# In[88]:


params = {'n_estimators':range(10, 40, 5), 'max_depth':[3, 5], 'max_features':range(3, 7)}
clf = RandomForestClassifier(random_state=0)
tune_estimator('RF', clf, params)


# In[89]:


# second round
params = {'n_estimators':range(31, 40, 2), 'max_depth':range(4, 7), 'max_features':range(3, 6)}
clf = RandomForestClassifier(random_state=0)
tune_estimator('RF', clf, params)


# ## 7.6 GBDT

# In[90]:


params = {'learning_rate':[0.001, 0.01, 0.1, 1], 'n_estimators':range(100, 250, 20), 'max_depth':range(2, 5), 'max_features':range(3, 6)}
clf = GradientBoostingClassifier(random_state=0)
tune_estimator('GBDT', clf, params)


# In[91]:


# second tune
params = {'learning_rate':[0.3, 0.5, 0.7, 0.9, 1.2, 1.4, 1.7, 2], 'n_estimators':range(130, 200, 20), 'max_depth':[2, 3, 4], 'max_features':[3, 4, 5]}
clf = GradientBoostingClassifier(random_state=0)
tune_estimator('GBDT', clf, params)


# ## 7.7 Conclusion
# **æœ€ç»ˆï¼Œæˆ‘ä»¬è·å¾—æœ€å¤§çš„å¾—åˆ†æ˜¯é‡‡ç”¨GBDTï¼Œå¾—åˆ†0.87741180236289573ã€‚**

# # 8. Ensemble Methods
# çŸ¥ä¹æ–‡ç« [ã€scikit-learnæ–‡æ¡£è§£æã€‘é›†æˆæ–¹æ³• Ensemble Methodsï¼ˆä¸Šï¼‰ï¼šBaggingä¸éšæœºæ£®æ—](https://zhuanlan.zhihu.com/p/26683576)å¯¹é›†æˆæ–¹æ³•è¿›è¡Œäº†ä¸€äº›åŸç†ã€ä½¿ç”¨æ–¹æ³•çš„ä»‹ç»ã€‚
# > åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œé›†æˆæ–¹æ³•ï¼ˆensemble learningï¼‰æŠŠè®¸å¤šä¸ªä½“é¢„æµ‹å™¨ï¼ˆbase estimatorï¼‰ç»„åˆèµ·æ¥ï¼Œä»è€Œæé«˜æ•´ä½“æ¨¡å‹çš„é²æ£’æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚
# > 
# > é›†æˆæ–¹æ³•æœ‰ä¸¤å¤§ç±»ï¼š
# > 
# > Averagingï¼šç‹¬ç«‹å»ºé€ å¤šä¸ªä¸ªä½“æ¨¡å‹ï¼Œå†å–å®ƒä»¬å„è‡ªé¢„æµ‹å€¼çš„å¹³å‡ï¼Œä½œä¸ºé›†æˆæ¨¡å‹çš„æœ€ç»ˆé¢„æµ‹ï¼›èƒ½å¤Ÿé™ä½varianceã€‚ä¾‹å­ï¼šéšæœºæ£®æ—ï¼Œbaggingã€‚
# > Boostingï¼šé¡ºåºå»ºç«‹ä¸€ç³»åˆ—å¼±æ¨¡å‹ï¼Œæ¯ä¸€ä¸ªå¼±æ¨¡å‹éƒ½åŠªåŠ›é™ä½biasï¼Œä»è€Œå¾—åˆ°ä¸€ä¸ªå¼ºæ¨¡å‹ã€‚ä¾‹å­ï¼šAdaBoostï¼ŒGradient Boostingã€‚
# 
# GBDTï¼ˆgradient boosting decision treeï¼‰å¯ä»¥è¯´æ˜¯æœ€å¼ºå¤§çš„æ•°æ®æŒ–æ˜ç®—æ³•ä¹‹ä¸€ï¼šå®ƒèƒ½è§£å†³å„ç§åˆ†ç±»ã€å›å½’å’Œæ’åºé—®é¢˜ï¼Œèƒ½ä¼˜ç§€åœ°å¤„ç†å®šæ€§å’Œå®šé‡ç‰¹å¾ï¼Œé’ˆå¯¹outlierçš„é²æ£’æ€§å¾ˆå¼ºï¼Œæ•°å€¼ä¸éœ€è¦normalizeã€‚è€Œä¸”ï¼ŒLightGBMå’ŒXGBoostç­‰ç®—æ³•åº“å·²ç»è§£å†³äº†GBDTå¹¶è¡Œè®¡ç®—çš„é—®é¢˜ï¼ˆç„¶è€Œscikit-learnæš‚ä¸æ”¯æŒå¹¶è¡Œï¼‰[XGBoost](https://xgboost.readthedocs.io/en/latest/)ã€‚åœ¨æœ¬è´¨ä¸Šï¼Œå†³ç­–æ ‘ç±»å‹çš„ç®—æ³•å‡ ä¹ä¸å¯¹æ•°æ®çš„åˆ†å¸ƒåšä»»ä½•ç»Ÿè®¡å­¦å‡è®¾ï¼Œè¿™ä½¿å®ƒä»¬èƒ½æ‹Ÿåˆå¤æ‚çš„éçº¿æ€§å‡½æ•°ã€‚
# 
# å…¶ä¸­RandomForestå’ŒGBDTæˆ‘ä»¬åœ¨ä¸Šè¿°æ–¹æ³•ä¸­ä¹Ÿå·²ç»ä½¿ç”¨è¿‡äº†ï¼Œè¿™é‡Œæˆ‘ä»¬ä½¿ç”¨å…¶ä»–çš„æ–¹æ³•ï¼Œæ¯”å¦‚Baggingã€AdaBoostingã€Votingä»¥åŠStackingã€‚
# 
# ## 8.1 Bagging
# å› ä¸ºRandomForestå’ŒGBDTéƒ½æ˜¯åŸºäºå†³ç­–æ ‘çš„ï¼Œè¿™é‡Œæˆ‘ä»¬é’ˆå¯¹è¡¨ç°æ¯”è¾ƒå¥½çš„LRåšä¸€æ¬¡Baggingã€‚ç»“æœ0.87594ï¼Œæ˜¯ç¨å¥½äºä¹‹å‰å•ä¸€çš„LR 0.87440ã€‚æµç¨‹å›¾ç‰‡æ¥è‡ªæ–‡ç« [Ensemble of Weak Learners](http://manish-m.com/?p=794)ï¼Œä»¥åŠä»¥ä¸‹å‡ ä¸ªensemble methodsçš„æµç¨‹å›¾ç‰‡ä¿±æ¥è‡ªäºæ­¤ã€‚
# ![](http://manish-m.com/wp-content/uploads/2012/11/BaggingCropped.png)
# 
# é‡‡ç”¨æœ‰æ”¾å›çš„æŠ½æ ·ï¼Œå¦‚æœNè¶‹å‘äºMï¼Œåˆ™æ¯ä¸ªè®­ç»ƒé›†å°†åªæ‹¥æœ‰63%çš„åŸå§‹æ•°æ®é›†ï¼Œå…¶ä»–éƒ½æ˜¯é‡å¤çš„ã€‚

# In[92]:


from sklearn.ensemble import BaggingClassifier
params = {'n_estimators': range(50, 150, 10)}
bagging = BaggingClassifier(LogisticRegression(C=0.3))
tune_estimator('bagging', bagging, params)


# ## 8.2 AdaBoosting
# çŸ¥ä¹æ–‡ç« [ã€scikit-learnæ–‡æ¡£è§£æã€‘é›†æˆæ–¹æ³• Ensemble Methodsï¼ˆä¸‹ï¼‰ï¼šAdaBoostï¼ŒGBDTä¸æŠ•ç¥¨åˆ†ç±»å™¨](https://zhuanlan.zhihu.com/p/26704531)å¯¹Adabootingåšäº†ä»‹ç»ï¼Œå…¨ç¨‹ä¸ºAdaptive Boostingã€‚
# > AdaBoostçš„æ ¸å¿ƒç†å¿µï¼Œæ˜¯æŒ‰é¡ºåºæ‹Ÿåˆä¸€ç³»åˆ—å¼±é¢„æµ‹å™¨ï¼Œåä¸€ä¸ªå¼±é¢„æµ‹å™¨å»ºç«‹åœ¨å‰ä¸€ä¸ªå¼±é¢„æµ‹å™¨è½¬æ¢åçš„æ•°æ®ä¸Šã€‚**æ¯ä¸€ä¸ªå¼±é¢„æµ‹å™¨çš„é¢„æµ‹èƒ½åŠ›ï¼Œä»…ä»…ç•¥å¥½äºéšæœºä¹±çŒœã€‚**æœ€ç»ˆçš„é¢„æµ‹ç»“æœï¼Œæ˜¯æ‰€æœ‰é¢„æµ‹å™¨çš„åŠ æƒå¹³å‡ï¼ˆæˆ–æŠ•ç¥¨ç»“æœï¼‰ã€‚
# > 
# > æœ€åˆï¼ŒåŸå§‹æ•°æ®æœ‰Nä¸ªæ ·æœ¬ï¼ˆw_1, w_2, w_3, ... , w_Nï¼‰ï¼Œæ¯ä¸ªæ ·æœ¬çš„æƒé‡æ˜¯1/Nã€‚**åœ¨æ¯ä¸€ä¸ªboostingè¿­ä»£ä¸­ï¼Œä¸€ä¸ªå¼±åˆ†ç±»å™¨ä¼šæ‹Ÿåˆè®­ç»ƒæ•°æ®ã€‚ä¹‹åï¼Œå¼±åˆ†ç±»å™¨åˆ†ç±»é”™è¯¯çš„æ ·æœ¬å°†ä¼šå¾—åˆ°æ›´å¤šçš„æƒé‡ã€‚ç›¸åº”åœ°ï¼Œæ­£ç¡®åˆ†ç±»çš„æ ·æœ¬çš„æƒé‡ä¼šå‡å°‘ã€‚åœ¨ä¸‹ä¸€ä¸ªçš„è¿­ä»£é‡Œï¼Œä¸€ä¸ªæ–°çš„å¼±åˆ†ç±»å™¨ä¼šæ‹Ÿåˆï¼ˆæƒé‡è¢«è°ƒæ•´åçš„ï¼‰æ–°çš„è®­ç»ƒæ•°æ®ã€‚**å› æ­¤ï¼Œè¶Šéš¾åˆ†ç±»çš„æ ·æœ¬ï¼Œåœ¨ä¸€æ¬¡åˆä¸€æ¬¡çš„è¿­ä»£ä¸­çš„æƒé‡ä¼šè¶Šæ¥è¶Šé«˜ã€‚
# > 
# > å› æ­¤ï¼ŒAdaBoostçš„ä¸€ä¸ªç¼ºç‚¹å°±æ˜¯ä¸å…ç–«æ•°æ®ä¸­çš„outlierï¼ˆå°¤å…¶æ˜¯åˆ†ç±»åˆ†ä¸å¯¹çš„outlierï¼‰ã€‚
# > 
# > AdaBoostå¯ä»¥çœ‹ä½œæ˜¯ä½¿ç”¨äº†exponential lossæŸå¤±å‡½æ•°çš„Gradient Boostingã€‚
# 
# æœ‰ç‚¹ä¸æ˜ç™½çš„æ˜¯ï¼Œæ¯æ¬¡é”™è¯¯æ ·æœ¬å¦‚ä½•è·å¾—æ›´å¤šçš„æƒé‡å‘¢ï¼Ÿ
# 
# å®ƒç»„åˆçš„æ˜¯ä¸€ç§å«åšdecision stumpï¼Œå†³ç­–æ ‘æ¡©çš„åˆ†ç±»å™¨ï¼Œä¹Ÿå°±æ˜¯å•å±‚å†³ç­–æ ‘ï¼Œå•å±‚ä¹Ÿå°±æ„å‘³ç€å°½å¯å¯¹æ¯ä¸€åˆ—å±æ€§è¿›è¡Œä¸€æ¬¡åˆ¤æ–­ã€‚å¤§æ¦‚é•¿è¿™æ ·
# ![](https://img-blog.csdn.net/20160325144422445)
# 
# ä¼˜åŒ–ç›®æ ‡ï¼Œå°±æ˜¯ä½¿å¾—æ¯ä¸€åˆ—çš„é”™è¯¯ç‡åˆ¤æ–­çš„é”™è¯¯ç‡æœ€ä½ã€‚
# \begin{equation}
# \underset{1\leq i\leq d}{\arg\min}\frac1N\sum_{n=1}^N1_{y_n\neq g_i(\mathbf x)}
# \end{equation}
# 
# æµç¨‹
# ![](http://manish-m.com/wp-content/uploads/2012/11/BoostingCropped2.png)

# In[93]:


from sklearn.ensemble import AdaBoostClassifier
params = {'n_estimators':range(80, 200, 20), 'learning_rate':[0.5, 1, 3, 5]}
adc = AdaBoostClassifier(random_state=0)
tune_estimator('AdaBoosting', adc, params)


# In[94]:


# secound round
params = {'n_estimators':range(150, 180, 10), 'learning_rate':[ 0.7, 0.9, 1.3, 2]}
adc = AdaBoostClassifier(random_state=0)
tune_estimator('AdaBoosting', adc, params)


# ## 8.3 Voting
# > æŠ•ç¥¨åˆ†ç±»å™¨æ˜¯ä¸€ç§å…ƒé¢„æµ‹å™¨ï¼ˆmeta-estimatorï¼‰ï¼Œå®ƒæ¥æ”¶ä¸€ç³»åˆ—ï¼ˆåœ¨æ¦‚å¿µä¸Šå¯èƒ½å®Œå…¨ä¸åŒçš„ï¼‰æœºå™¨å­¦ä¹ åˆ†ç±»ç®—æ³•ï¼Œå†å°†å®ƒä»¬å„è‡ªçš„ç»“æœè¿›è¡Œå¹³å‡ï¼Œå¾—åˆ°æœ€ç»ˆçš„é¢„æµ‹ã€‚
# > 
# > æŠ•ç¥¨æ–¹æ³•æœ‰ä¸¤ç§ï¼š
# > 
# > ç¡¬æŠ•ç¥¨ï¼šæ¯ä¸ªæ¨¡å‹è¾“å‡ºå®ƒè‡ªè®¤ä¸ºæœ€å¯èƒ½çš„ç±»åˆ«ï¼ŒæŠ•ç¥¨æ¨¡å‹ä»å…¶ä¸­é€‰å‡ºæŠ•ç¥¨æ¨¡å‹æ•°é‡æœ€å¤šçš„ç±»åˆ«ï¼Œä½œä¸ºæœ€ç»ˆåˆ†ç±»ã€‚
# > è½¯æŠ•ç¥¨ï¼šæ¯ä¸ªæ¨¡å‹è¾“å‡ºä¸€ä¸ªæ‰€æœ‰ç±»åˆ«çš„æ¦‚ç‡çŸ¢é‡(1 * n_classes)ï¼ŒæŠ•ç¥¨æ¨¡å‹å–å…¶åŠ æƒå¹³å‡ï¼Œå¾—åˆ°ä¸€ä¸ªæœ€ç»ˆçš„æ¦‚ç‡çŸ¢é‡ã€‚
# 
# è½¯æŠ•ç¥¨ï¼Œæ˜¯å¯¹æ¯ä¸ªæ¨¡å‹çš„æ‰€æœ‰ç±»åˆ«æ¦‚ç‡è¿›è¡ŒåŠ æƒå¹³å‡å¾—åˆ°çš„ï¼Œè€Œä¸æ˜¯åƒç¡¬æŠ•ç¥¨æ¯ä¸ªç±»è®¤ä¸ºæœ€å¯é çš„æŠ•ç¥¨å†³å®šã€‚
# 
# ![](http://manish-m.com/wp-content/uploads/2012/11/Voting_Cropped.png)
# 
# è¿„ä»Šä¸ºæ­¢ï¼Œæˆ‘ä»¬å·²ç»è¿ç”¨å¹¶è·å¾—äº†å¤§éƒ¨åˆ†å…·æœ‰è‰¯å¥½æ€§èƒ½çš„ç®—æ³•äº†ï¼Œè¿™é‡Œæˆ‘ä»¬å°±å¯¹å…¶æ’ä¸ªåºï¼Œä»¥æœŸä½¿ç”¨ç»“æœæ’åé å‰çš„ç®—æ³•åšä¸€ä¸ªVotingã€‚

# In[95]:


lr = LogisticRegression(C=0.3, random_state=0)
knn = KNeighborsClassifier(n_neighbors=10)
svc = SVC(C=9, gamma=0.1, random_state=0)
mlp = MLPClassifier(alpha=0.001, hidden_layer_sizes=(20, 20), solver='lbfgs', random_state=0)
rf = RandomForestClassifier(max_depth=5, max_features=5, n_estimators=31, random_state=0)
gbdt = GradientBoostingClassifier(learning_rate=0.5, max_depth=2, max_features=3, n_estimators=190, random_state=0)
bagging = BaggingClassifier(lr, n_estimators=140, random_state=0)
abc = AdaBoostClassifier(learning_rate=1.3, n_estimators=150, random_state=0)

names = ['LR', 'KNN', 'SVC', 'MLP', 'RF', 'GBDT', 'Bagging', 'AdaB']
models = [lr, knn, svc, mlp, rf, gbdt, bagging, abc]

result_scores = []
for name, model in zip(names, models):
    scores = cross_val_score(model, X_train, Y_train, cv=5, scoring='roc_auc')
    result_scores.append(scores.mean())
    print('{} has a mean score {:.4f} based on {}'.format(name, scores.mean(), scores))    


# In[96]:


sorted_score = pd.Series(data=result_scores, index = names).sort_values(ascending=False)
ax = plt.subplot(111)
sorted_score.plot(kind='line', ax=ax, title='score order', figsize=(9, 6), colormap='cool')
ax.set_xticks(range(0, 9))
ax.set_xticklabels(sorted_score.index);
sns.despine()


# æˆ‘ä»¬å°±é€‰æ‹©GBDTã€RFã€LRã€AdaBoostingã€Baggingã€SVCä½œä¸ºæˆ‘ä»¬çš„æŠ•ç¥¨base learnersã€‚

# In[97]:


from sklearn.ensemble import VotingClassifier
names = ['LR', 'KNN', 'RF', 'GBDT', 'Bagging', 'AdaB']

lr = LogisticRegression(C=0.3, random_state=0)
knn = KNeighborsClassifier(n_neighbors=10)
rf = RandomForestClassifier(max_depth=5, max_features=5, n_estimators=31, random_state=0)
gbdt = GradientBoostingClassifier(learning_rate=0.5, max_depth=2, max_features=3, n_estimators=190, random_state=0)
bagging = BaggingClassifier(lr, n_estimators=140, random_state=0)
abc = AdaBoostClassifier(learning_rate=1.3, n_estimators=150, random_state=0)

# ç›´æ¥æŠ•ç¥¨ï¼Œç¥¨æ•°å¤šçš„è·èƒœã€‚
vc_hard = VotingClassifier(estimators=[('LR', lr), ('KNN', knn), ('GBDT', gbdt), ('Bagging', bagging), ('AdaB', abc)], voting='hard')
# å‚æ•°é‡Œè¯´ï¼Œsoftæ›´åŠ é€‚ç”¨äºå·²ç»è°ƒåˆ¶å¥½çš„base learnersï¼ŒåŸºäºæ¯ä¸ªlearnerè¾“å‡ºçš„æ¦‚ç‡ã€‚çŸ¥ä¹æ–‡ç« é‡Œè®²ï¼ŒSoftä¸€èˆ¬è¡¨ç°çš„æ›´å¥½ã€‚
vc_soft = VotingClassifier(estimators=[('LR', lr), ('KNN', knn), ('RF', rf), ('GBDT', gbdt), ('Bagging', bagging), ('AdaB', abc)], voting='soft')

# 'vc hard:', cross_val_score(vc_hard, X_dummied, Y, cv=5, scoring='roc_auc').mean(),\
'vc soft:', cross_val_score(vc_soft, X_train, Y_train, cv=5, scoring='roc_auc').mean()


# æˆ‘ä»¬è¿è¡ŒVC Hardä¸€ç›´å‡ºé”™ï¼Œè¡¨ç¤ºæ²¡æœ‰decision_functionå¯ç”¨è¿™ç±»çš„ã€‚
# å¦å¤–æ¯”èµ·GBDTï¼ŒVC softæå‡äº†0.002ã€‚

# ## 8.4 Stacking
# ![](http://manish-m.com/wp-content/uploads/2012/11/StackingCropped.png)
# çœ‹å¾ˆå¤šæ•™ç¨‹ï¼ŒStacking éƒ½æ˜¯è¦æŠŠTest Dataæ”¾è¿›å»ï¼Œç›´æ¥é¢„æµ‹äº†ã€‚

# ## 8.4.1 Stacking
# ### æ„å»ºç¬¬ä¸€å±‚Stacking

# In[98]:


n_train=X_train.shape[0]
n_test=X_test.shape[0]
kf=StratifiedKFold(n_splits=5,random_state=1,shuffle=True)


# å®šä¹‰Stackingç¬¬ä¸€å±‚å‡½æ•°ï¼Œæ¥æ”¶æœªFitçš„estimatorï¼Œç„¶åè¿”å›é¢„æµ‹åçš„train_yå’Œtest_predict

# In[99]:


def get_oof(clf, X, y, test_X):
    oof_train = np.zeros((n_train, ))
    oof_test_mean = np.zeros((n_test, ))
    # 5 is kf.split
    oof_test_single = np.empty((kf.get_n_splits(), n_test))
    for i, (train_index, val_index) in enumerate(kf.split(X,y)):
        kf_X_train = X.iloc[train_index]
        kf_y_train = y.iloc[train_index]
        kf_X_val = X.iloc[val_index]
        
        clf.fit(kf_X_train, kf_y_train)
        
        oof_train[val_index] = clf.predict(kf_X_val)
        oof_test_single[i,:] = clf.predict(test_X)
    # oof_test_single, å°†ç”Ÿæˆä¸€ä¸ª5è¡Œ*n_teståˆ—çš„predict valueã€‚é‚£ä¹ˆmean(axis=0), å°†å¯¹5è¡Œï¼Œæ¯åˆ—çš„å€¼è¿›è¡Œæ±‚meanã€‚ç„¶åreshapeè¿”å›   
    oof_test_mean = oof_test_single.mean(axis=0)
    return oof_train.reshape(-1,1), oof_test_mean.reshape(-1,1)


# In[100]:


lr = LogisticRegression(C=0.3, random_state=0)
knn = KNeighborsClassifier(n_neighbors=10)
rf = RandomForestClassifier(max_depth=5, max_features=5, n_estimators=31, random_state=0)
gbdt = GradientBoostingClassifier(learning_rate=0.5, max_depth=2, max_features=3, n_estimators=190, random_state=0)
bagging = BaggingClassifier(lr, n_estimators=140, random_state=0)
abc = AdaBoostClassifier(learning_rate=1.3, n_estimators=150, random_state=0)

lr_train, lr_test = get_oof(lr, X_train, Y_train, X_test)
knn_train, knn_test = get_oof(knn, X_train, Y_train, X_test)
rf_train, rf_test = get_oof(rf, X_train, Y_train, X_test)
gbdt_train, gbdt_test=get_oof(gbdt, X_train, Y_train, X_test)
bagging_train, bagging_test = get_oof(bagging,X_train, Y_train, X_test)
abc_train, abc_test = get_oof(abc,X_train, Y_train, X_test)


# In[101]:


y_train_pred_stack = np.concatenate([lr_train, knn_train, rf_train, gbdt_train, bagging_train, abc_train], axis=1)
y_train_stack = Y_train.reset_index(drop=True)
y_test_pred_stack = np.concatenate([lr_test, knn_test, rf_test, gbdt_test, bagging_test, abc_test], axis=1)

y_train_pred_stack.shape, y_train_stack.shape, y_test_pred_stack.shape


# - y_train_pred_stackæœ‰891*6ï¼Œè¿™ä¸ª6åˆ—åˆ†åˆ«æ˜¯æˆ‘ä»¬æ¯ä¸ªestimatoråœ¨y_trainä¸Šé¢„æµ‹çš„å€¼yå€¼ã€‚
# - y_train_stackï¼Œæ˜¯trainä¸ŠçœŸå®çš„yå€¼ã€‚
# - y_test_pred_stackï¼Œæ˜¯æ¯ä¸ªestimatoråœ¨df_testä¸Šé¢„æµ‹çš„yå€¼ï¼Œç”±äºæ¯ä¸ªestimatorå°†é¢„æµ‹5æ¬¡ï¼ˆK-Foldï¼‰ï¼Œæˆ‘ä»¬éœ€è¦å°†è¿™5æ¬¡çš„é¢„æµ‹ç»“æœæ±‚å¹³å‡ï¼Œç„¶åå¾—å‡ºä¸€ä¸ªestimatorçš„é¢„æµ‹çš„yå€¼ï¼Œå†æŠŠ6ä¸ªestimatorå†æµ‹è¯•é›†ä¸Šé¢„æµ‹çš„yå€¼ç»„åˆèµ·æ¥ã€‚
# 
# ç„¶åä½¿ç”¨y_train_pred_stack(X_train)å’Œy_train_stack(Y_train)å†å»åšä¸€ä¸ªè®­ç»ƒï¼Œè¿›è€Œå¾—å‡ºæœ€ç»ˆçš„è¯„åˆ†ã€‚ç„¶åå†åœ¨y_test_pred_stack(X_test)é¢„æµ‹æœ€ç»ˆç»“æœã€‚
# 
# ### æ„å»ºç¬¬äºŒå±‚Stacking

# In[102]:


# params = {'learning_rate':[0.001, 0.01, 0.1, 1], 'n_estimators':range(100, 250, 20), 'max_depth':[2, 5], 'max_features':range(1, 3)}
# clf = GradientBoostingClassifier(random_state=0)

# params = {'C':[0.05, 0.08, 0.1, 0.2, 0.3]}
# clf = LogisticRegression(random_state=0)

params = {'n_estimators':range(90, 150, 10)}
clf = RandomForestClassifier(random_state=0)

gscv_test= GridSearchCV(estimator=clf, param_grid=params, scoring='roc_auc', n_jobs=-1, cv=5, verbose=False)
gscv_test.fit( y_train_pred_stack, y_train_stack)
gscv_test.best_score_, gscv_test.best_params_


# ### é¢„æµ‹

# In[103]:


y_pred = RandomForestClassifier(random_state=0, n_estimators=100).fit(y_train_pred_stack, y_train_stack).predict(y_test_pred_stack)


# è¯¥Stackingçš„cross validation è¯„åˆ†æœ€ç»ˆåªæœ‰0.8363ã€‚æˆ‘æŠŠé¢„æµ‹ç»“æœä¸å®é™…ç»“æœè¿›è¡Œä¸€å¯¹æ¯”ï¼Œå‘ç°æˆ‘ä½¿ç”¨Stackingè®­ç»ƒæ‰€é¢„æµ‹çš„ç»“æœï¼Œä»ç„¶æœ‰138è¡Œçš„æ˜¯é¢„æµ‹å¤±è´¥çš„ï¼Œå¹¶ä¸”é¢„æµ‹å¤±è´¥çš„caseå‡ ä¹æ˜¯æ¯ä¸ªestimatoréƒ½ç»™å‡ºåŒæ ·çš„ç»“æœï¼Œè¿™è¡¨æ˜ï¼Œæˆ‘ä»¬éœ€è¦æ„é€ æŸäº›featureæ¥åˆ†ç±»å‡ºè¿™äº›é¢„æµ‹å¤±è´¥çš„caseã€‚è¿™ä¸ªå¾ˆåƒæ˜¯æˆ‘ä»¬åœ¨Misclassification Analysisä¸­æ‰€ä½œçš„äº‹æƒ…ï¼Œæˆ‘åœ¨é‚£ä¸ªç¯èŠ‚å¹¶æ²¡æœ‰åšå¥½ï¼Œæ„é€ çš„featureä¹Ÿæ²¡æœ‰ä»€ä¹ˆåµç”¨ã€‚Andrew Ngè¯´è¦è‚‰çœ¼çœ‹ä¸€ç™¾è¡Œçš„æ•°æ®ï¼Œï¼Œï¼Œæˆ‘è¿™è‚‰çœ¼è¿˜æ²¡ä¿®ç‚¼æˆç«çœ¼é‡‘ç›çœ‹æ¥ã€‚

# In[125]:


c = gscv_test.predict(y_train_pred_stack)

a = pd.DataFrame(y_train_pred_stack)
b = pd.concat([a, pd.Series(c), y_train_stack], axis=1)
b.columns = ['lr', 'knn', 'rf', 'gbdt', 'bagging', 'abc', 'predicted', 'Survived']
b[b.predicted != b.Survived]


# ä»¥ä¸Šç»“æœï¼ŒStackingåªæœ‰0.8363çš„è¯„åˆ†ã€‚æ‰€ä»¥æœ€ç»ˆï¼Œæˆ‘ä»¬è¿˜æ˜¯é€‰æ‹©è€ƒè™‘ä½¿ç”¨è·å¾—æœ€é«˜è¯„åˆ†çš„vote_softæ¥é¢„æµ‹å¹¶è¾“å‡ºç»“æœã€‚
# # 9. Predict

# In[118]:


y_pred = vc_soft.fit(X_train, Y_train).predict(X_test)
result_df = pd.DataFrame({'PassengerId': X_test.index, 'Survived':y_pred}).set_index('PassengerId')
result_df.Survived = result_df.Survived.astype('int')
result_df.to_csv('predicted_survived.csv')


# In[123]:


pd.read_csv('predicted_survived.csv').head()


# In[124]:


pd.read_csv('../input/gender_submission.csv').head()


# # æ ¼å¼ä¸€è‡´ï¼Œæäº¤ï¼
# # Right formatï¼ŒSubmitï¼
