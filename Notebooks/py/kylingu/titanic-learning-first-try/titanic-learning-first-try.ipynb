{"cells":[{"metadata":{"_uuid":"6c0663b9c3c8f31d81b0c72b5af1e4ab4d0da2d6"},"cell_type":"markdown","source":"I am new to Machine Learning, any comment or advice will be appreciated. As this is a record for my machine learning, so I use my native language Chinese to write kernel. But I think code and visualization is general, wish you could understand.\n\nI have also referred some best kernel, like [End-to-End process for Titanic problem\n](https://www.kaggle.com/massquantity/end-to-end-process-for-titanic-problem). It provide a complete best practise about machine learning, a complete way of model selection, hyperparameter tunning and ensemble methods.\n\nI would also recommend another best article [Titanic top 4% with ensemble modeling ](https://www.kaggle.com/yassineghouzam/titanic-top-4-with-ensemble-modeling/notebook)\n\n# 1. Titanic Data Set Feature Description\n- survival, 存活, 0 = No, 1 = Yes\n- Name, 乘客姓名\n- pclass, 票等, 1 = 1st, 2 = 2nd, 3 = 3rd，折射阶级，1st = Upper, 2nd = Middle, 3rd = Lower\n- sex，性别\t\n- Age，年龄\n- sibsp， 船上兄弟姐妹或者配偶的数量\n- parch，船上父母或儿童的数量\t\n- ticket，票号\n- fare，旅客费用\t\n- cabin，客舱号码，\t\n- embarked，登船港口，C = Cherbourg, Q = Queenstown, S = Southampton\n\n# 2. Question\n问题是要预测Survived 人数。\n> On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. Translated 32% survival rate.\n\n说明，32%应该是一个baseline了，也即我们预测的结果，应该整体合起来得在32%的存活率附近，超过或者少了，预测都没达到最好。\n\n> One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.\n\n没有足够的救生圈给乘客和船员，那么哪些人能够容易获得救生圈呢？上层阶级？或者是那些船员或乘客乘客距离救生圈足够近的人。这些人获得救生圈应该可以大大提高存活几率。第一个是船舱比较靠近的，第二个是更有接近这些就剩设施的船员，第三个是具有更高优先级的乘客，上层或者高官。这暗示我们应该去了解船舱分布以及救生设施分布。所以去了解featureCabin,和Name里的Title。\n\n> Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\n**这句明显在给提示，women，表明性别很重要，children，表明年龄很重要，upper-class说明，舱等和Fare更高的人存活几率应该更高。**\n\n# 3. Loading Data"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# disable copy warning in pandas\npd.options.mode.chained_assignment = None  # default='warn'\n# disable sklearn deprecation warning\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input/\"))\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","scrolled":true,"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv('../input/train.csv').set_index('PassengerId')\ndf_test = pd.read_csv('../input/test.csv').set_index('PassengerId')\ndataset = pd.concat([df_train, df_test], axis=0)\nid_test = df_test.index\ndataset.tail()","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"65ee19e551fc82c49f5561089efe150a3ef66341","trusted":true},"cell_type":"code","source":"dataset.info()","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"4281874f72f9cb6224d2689f3afc7c687fbc8a50"},"cell_type":"markdown","source":"我们可以看到共有891条数据，其中Cabin缺失严重，只有295条，Age也缺失263条。Age或许可以根据sibsp与Parch推断。在看一下统计信息。"},{"metadata":{"_uuid":"7a4719e0bd974db343098e9779bf7720379fe80b","trusted":true},"cell_type":"code","source":"dataset.describe(include='all').T","execution_count":4,"outputs":[]},{"metadata":{"_uuid":"c12676df29c804f40253cc291a6d04bded77a282"},"cell_type":"markdown","source":"我们知道unique太多的话，不是一个很好的预测变量，因此Name显然不是一个很好的预测变量。Ticket为什么会有重复的,虽然票号不是很好的预测变量，但或许对我们推断Cabin会有用处。先看看把。"},{"metadata":{"_uuid":"05f7a8c35bb0c4c27e025cfc238bdb1ddd01f47f"},"cell_type":"markdown","source":"# 4. Overview Data\n我们先看看每个feature或者feature 组合对Survived存活的影响。按照ISL里介绍，这个应该去计算F值或者是P值，才知道有无关系，不过这是针对线性回归的，我不清楚这个分类要怎么看。现在先凭借肉眼看罢。"},{"metadata":{"_uuid":"450f39f0c7bb8f4277e17f46177874848a51c103","trusted":true},"cell_type":"code","source":"dataset.columns","execution_count":5,"outputs":[]},{"metadata":{"_uuid":"0f554557f2689b182fee1523560496845455d679"},"cell_type":"markdown","source":"我们可以先看看数值类型的数据对Survived相关性，下图显示，只有Pclass和Fare有点关系，还是弱相关。"},{"metadata":{"_uuid":"d442542d0990b382e2cc9b6f45d9685a92101339","trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(9, 6))\nsns.heatmap(df_train[['Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare']].corr(), cmap='cool', annot=True, ax=fig.gca())","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"16cafd5d01b993b06aa1e9e900a9f63bb2c63a42"},"cell_type":"markdown","source":"## 4.1 Pclass\n让我们先看Pclass舱等对存活率的影响。"},{"metadata":{"_uuid":"355b8c22c77e8a09f4dcfb4cdcc87f7e55565671","trusted":true},"cell_type":"code","source":"df_train.groupby('Pclass').Survived.agg(['count', 'sum', 'mean', 'std'])","execution_count":7,"outputs":[]},{"metadata":{"_uuid":"4483ccf2983d4a9cf72f3b581865e175dcff34ff","trusted":true},"cell_type":"code","source":"# plt.bar([1, 2, 3], (df_train.groupby('Pclass').Survived.mean()), color='rgb', tick_label = ['1', '2', '3'])\n# sns.despine()\n\nsns.factorplot(x='Pclass', y='Survived', data=df_train, kind='bar', size=5, palette='cool')","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"10a53033e57b4e71761cc6cb7547e3c12f1132a0"},"cell_type":"markdown","source":"一等舱的幸存率要比二、三等舱的都要高，并且随着舱等的下降，幸存率也在下降。\n## 4.2 Name\nName是一个Unique类型的数据，本来我们是不打算看这个数据的。但其实浏览一下Name也是能看出一些端倪的，比如Braund, Mr. Owen Harris，除了名字还有称呼，如Mr，Miss，Mrs以及Master这样的信息。或须也能够让我们对Survived做出一些判断。同样上文提到，我们对于其是否为上层人士非常感兴趣，所以title没准也是可以说明问题的。\n"},{"metadata":{"_uuid":"3604786f6e35b83a99ee1475919605475343ef8a","trusted":true},"cell_type":"code","source":"dataset.Name.head(10)","execution_count":9,"outputs":[]},{"metadata":{"_uuid":"2a520589d6e501cafd90ffc278b88092217f65b3","trusted":true},"cell_type":"code","source":"dataset['title'] = dataset.Name.apply(lambda x: x.split(', ')[1].split('. ')[0])\n# Major, 少校；Lady，贵妇；Sir，子爵; Capt, 上尉；the Countess，伯爵夫人；Col，上校。Dr,医生？\ndataset['title'].replace(['Mme', 'Ms', 'Mlle'], ['Mrs', 'Miss', 'Miss'], inplace = True)\ndataset['title'].value_counts()","execution_count":10,"outputs":[]},{"metadata":{"_uuid":"668d00d5fc5f3f347e84fbcb35bd3c69b4c87e84"},"cell_type":"markdown","source":"我们看到，贵族或者大人数，都是相对较少的，所以这里，我们只要针对统计量，就能够区分开这些人群。另外Master这个title, 对应未满18岁的小男孩，而小女孩在Miss里。其他title都是大人的尊称，这或许是个比较好的Age预测量。"},{"metadata":{"_uuid":"d1fa8912b9d45bf3f174ca39f9b8e1f3d28e1a84","trusted":true,"collapsed":true},"cell_type":"code","source":"dataset['title'] = dataset.title.apply(lambda x: 'rare' if x not in ['Mr', 'Miss', 'Mrs', 'Master'] else x)","execution_count":11,"outputs":[]},{"metadata":{"_uuid":"c5cbe3b3fe0b588b0902d3195bff39c097545037"},"cell_type":"markdown","source":"我们如果将小女孩也独立独立出来，this would be a great predictive variable to Age，但这个就相当于漏掉那些没有年龄的姑娘了，我们需要看一下，那些没有年龄的姑娘占比能有多大。"},{"metadata":{"_uuid":"3196ef7b7476b013f533de6916bca8d637736ef8","trusted":true},"cell_type":"code","source":"age_na_miss_rate = len(dataset[(dataset.title == 'Miss') & (dataset.Age.isnull()) ]) / (dataset.title == 'Miss').sum()\nage_nna_not_mister_rate = len(dataset[(dataset.title == 'Miss') & (dataset.Age.notnull()) & (dataset.Age >= 18)]) / (dataset.title == 'Miss').sum()\nprint(age_na_miss_rate, age_nna_not_mister_rate)\n\nlen(dataset[(dataset.title == 'Miss') & (dataset.Age.isnull())]) / len(dataset)","execution_count":12,"outputs":[]},{"metadata":{"_uuid":"60e9f129510a918b874fda9bf6073c2eb88c708a"},"cell_type":"markdown","source":"没有年龄的小姐姐占整个数据集的3.89%,因此就算因此计算错了，影响倒也不大，所以我们就这么做了。"},{"metadata":{"_uuid":"70f923f8679da1a0cbf9b1b770a522f0d015760f","trusted":true,"collapsed":true},"cell_type":"code","source":"dataset.title[(dataset.title == 'Miss') & (dataset.Age < 18)] = 'Mister'","execution_count":13,"outputs":[]},{"metadata":{"_uuid":"3aa189237bf62cf70fc1babc64af91e3837cbfd3","trusted":true},"cell_type":"code","source":"dataset.groupby('title').Age.agg([('number', 'count'), 'min', 'median',  'max'])","execution_count":14,"outputs":[]},{"metadata":{"_uuid":"cf2f96cce5fb7794c87df2734ab26d482dbc7c3e"},"cell_type":"markdown","source":"我们再看看itle对survived 影响。"},{"metadata":{"_uuid":"747f0ae827c4fd254ad76ddd4b27503eda176aff","trusted":true},"cell_type":"code","source":"dataset.groupby('title').Survived.agg(['count', 'sum', 'mean', 'std'])","execution_count":15,"outputs":[]},{"metadata":{"_uuid":"737178f011bb059d5922e97721c45a2785bdb440","trusted":true},"cell_type":"code","source":"g= sns.factorplot(x='title', y = 'Survived', data=dataset, kind='bar', palette='cool', size=5)\ng.ax.set_title('title for survived');","execution_count":16,"outputs":[]},{"metadata":{"_uuid":"73aca79ffb4642b6a4ef37e7d6196a912f6dc223","trusted":true,"collapsed":true},"cell_type":"code","source":"dataset['title_level'] = dataset.title.map({\"Miss\": 3, \"Mrs\": 3,  \"Master\": 2, 'Mister': 2,  \"rare\":2, \"Mr\": 1})","execution_count":17,"outputs":[]},{"metadata":{"_uuid":"0d27315e6e403c63b7faa358ab0b916e357a7169"},"cell_type":"markdown","source":"## 4.3 Sex"},{"metadata":{"_uuid":"b7b9259983dc95320d3e7e1b22cc839d60e82031","trusted":true},"cell_type":"code","source":"dataset.groupby('Sex').Survived.agg(['count', 'sum', 'mean', 'std'])","execution_count":18,"outputs":[]},{"metadata":{"_uuid":"110802e2bf927c0227ccc9dc6bc7db2d41b78752"},"cell_type":"markdown","source":"哇哦，女性几乎达到了74%的存活率，而男性几乎只有18%的存活几率，如果我们再排除男性青少年，估计成年男性的存活更是低了。这是一个很重要的预测变量。\n"},{"metadata":{"_uuid":"21b7f24be13af7377a3f1c40b88f0f3e40cfb66c","trusted":true,"collapsed":true},"cell_type":"code","source":"dataset.Sex = dataset.Sex.map({'female':0, 'male':1})","execution_count":19,"outputs":[]},{"metadata":{"_uuid":"7cce3031b9032730579d30603dfd4019075b2fd7"},"cell_type":"markdown","source":"## 4.4 SipSp与Parch\n再来看以下，SibSp与Parch对Survived的影响。"},{"metadata":{"_uuid":"8798b53a7b9c6a538b36d2a5e11bad1cc7ffc8a0","trusted":true},"cell_type":"code","source":"dataset.SibSp.value_counts().plot(kind='bar')","execution_count":20,"outputs":[]},{"metadata":{"_uuid":"7ab7e13d591ebfa22b25872f63c7209241f0cbc8","trusted":true},"cell_type":"code","source":"dataset.Parch.value_counts().plot(kind='bar')","execution_count":21,"outputs":[]},{"metadata":{"_uuid":"208c46f4434d3ea0434041b1e1373063bf03113f","trusted":true},"cell_type":"code","source":"# dataset.groupby('SibSp').Survived.agg(['count', 'sum', 'mean', 'std'])\nsns.factorplot(x='SibSp', y='Survived', size=5, data=dataset, kind='bar', palette='cool')","execution_count":22,"outputs":[]},{"metadata":{"_uuid":"aa2a5fc78187226925a4f8bcc55316f7d89a34b6","trusted":true},"cell_type":"code","source":"# df.groupby('Parch').Survived.agg(['count', 'sum', 'mean', 'std'])\nsns.factorplot(x='Parch', y='Survived', size=5, data=dataset, kind='bar', palette='cool')","execution_count":23,"outputs":[]},{"metadata":{"_uuid":"d0c552346d2019ce3a0dd1dae23b56330c9fedf4"},"cell_type":"markdown","source":"以上两组数据，可以认为似乎有一两个Parch或者一两个SibSp存活几率更高一点。那么我们将这个组成家庭大小，所谓人多力量大，如果有兄弟姐妹的似乎更高。很多kernel里将family size + 1，然后单独列出一个alone列。我再想familysize为0时候，不就是一个人吗？为什么要单独列出来？"},{"metadata":{"_uuid":"9ddaaa01e71445b45514275470aef5663b3ce3fa","trusted":true},"cell_type":"code","source":"dataset['family_size'] = dataset.SibSp + dataset.Parch\n# df.groupby('family_size').Survived.agg(['count', 'sum', 'mean', 'std'])\nsns.factorplot(x='family_size', y='Survived', size=5, data=dataset, kind='bar', palette='cool')","execution_count":24,"outputs":[]},{"metadata":{"_uuid":"d145fd8aa4ae9772f021737fc14d50c527f8453e"},"cell_type":"markdown","source":"结果显示有1-3个亲属的人，存活几率也更高。或许我们应该根据family size进行分组，如只身一人，以及有1-3人亲属的存活率也较高，而更大的家庭，虽说数量不多，但是存活几率就立刻下降。家庭成员过多，将变成负担。所以存活率因此下降。让我们试试看，结果不错哦，可以认为也是非常不错的预测变量。标签用数字，这样后面也能够计算相关性。"},{"metadata":{"_uuid":"a7fa9ff36812c6aafb619ae4fb88aa03d5eaf7a4","trusted":true},"cell_type":"code","source":"dataset['family_size_level'] = pd.cut(dataset.family_size, bins=[-1,0, 3.5, 12], labels=['alone', 'middle', 'large'])\n# df.groupby('family_size_level').Survived.agg(['count', 'sum', 'mean', 'std'])\nsns.factorplot(x='family_size_level', y='Survived', size=5, data=dataset, kind='bar', palette='cool')\ndataset['family_size_level'] = dataset['family_size_level'].map({'alone':1, 'middle':2, 'large':0})","execution_count":25,"outputs":[]},{"metadata":{"_uuid":"5cca547d1e7259377d3a69318d91d211fd169706"},"cell_type":"markdown","source":"或许结合一下性别看看？"},{"metadata":{"_uuid":"62fa4a04fcf013a2db816bd67cde32c6bef90a1d","trusted":true},"cell_type":"code","source":"# dataset.groupby(['family_size_level', 'Sex']).Survived.agg(['count', 'sum', 'mean', 'std'])\nsns.factorplot(x='family_size_level', y='Survived', hue='Sex', size=5, data=dataset, kind='bar', palette='cool')","execution_count":26,"outputs":[]},{"metadata":{"_uuid":"99b8aad64569ce9ef80b76d4c83e56ca323bff40"},"cell_type":"markdown","source":"女性不论是独自一人还是有亲属存在，都达到了非常高的存活几率。而独身一人的青壮中老年死亡率都很高。而对于中型家庭，除了老年存活率较低外，其他也都很高。"},{"metadata":{"_uuid":"acf659a92da770300d9e8e598aacfbcd3f4fc51b"},"cell_type":"markdown","source":"## 4.5 Fare\n上面提到，上层的存活几率更高，所以除了Pclass能够表明身份之外，其实所花费用也是身份象征。"},{"metadata":{"_uuid":"572139d3b5e4b60b033d327784af3e4e150f13a4","trusted":true},"cell_type":"code","source":"dataset[dataset.Fare.isna()]","execution_count":27,"outputs":[]},{"metadata":{"_uuid":"83bdf916ea8a9f598b69e5e7cec2231be671c8e8","trusted":true,"collapsed":true},"cell_type":"code","source":"dataset.Fare.fillna(dataset[dataset.Pclass == 3].Fare.median(), inplace=True)","execution_count":28,"outputs":[]},{"metadata":{"_uuid":"90e4f560f44316b639b6d896d1805aa6c7abcc89","trusted":true},"cell_type":"code","source":"# dataset.Fare.hist(bins=20)\nsns.distplot(dataset.Fare)","execution_count":29,"outputs":[]},{"metadata":{"_uuid":"a68063bd89d6d22869b293132326c965a311313e"},"cell_type":"markdown","source":"上图显示，绝大部分的人的费用都集中在100英镑以内（假如费用是以英镑记的话，其实这都不是问题）。我们知道财富是符合正太分布的，但乍看之下这幅图右边尾巴太长，左边太高，看起来像是对数正太分布。我们再看看Pclass和Fare的关系。"},{"metadata":{"_uuid":"3d584577b3fcae5eab965f41eb4c4f765ae90b00","trusted":true},"cell_type":"code","source":"dataset.Fare = dataset.Fare.apply(lambda x: np.log(x) if x > 0 else 0)\nsns.distplot(dataset.Fare)","execution_count":30,"outputs":[]},{"metadata":{"_uuid":"1a2a10a9bee4a33d48066ad77e3b814c697f0f49","trusted":true},"cell_type":"code","source":"dataset.groupby('Pclass').Fare.agg(['count', 'sum', 'mean'])","execution_count":31,"outputs":[]},{"metadata":{"_uuid":"18c3fa5f7d5101606dff8c852f9f5f351bb03ca3"},"cell_type":"markdown","source":"我们应该根据PClass对费用进行一个分组，不过这其实相当于另一个Pclass了，所以还是算了。感觉这个变量的作用与Pclass的重复了。\n那么我们要怎么做呢？有两种思路，一种是将Fare按照2/8分。第二种，是构建新的Feature，即Pclass * Fare, 其中将Pclass转换成3，2，1使得舱等级越高，费用越大，让二者更加凸显身份和地位。再说这，在这两者基础上，在第二种构建完成后，再按照2/8分。似乎最后一种最靠谱，也许富人低调？花的少？首先舱等是对Fare的一种倍乘。是否需要两个feature倍乘，得看两个feature是否有相互效应，按照ISL里所说。"},{"metadata":{"_uuid":"7d279dec2639ba541223343a9fb3b5c3e9d592df","trusted":true,"collapsed":true},"cell_type":"code","source":"# def doInverse(x):\n#     if x == 3:\n#         return 1\n#     elif x == 1:\n#         return 3\n#     else:\n#         return x\n\n# df.Pclass.head(3), df.Pclass.apply(lambda x: doInverse(int(x))).head(3), df.Fare.head(3)\n\n# fares = df.Fare.multiply(df.Pclass.apply(lambda x: doInverse(int(x))))\n# plt.figure(figsize=(9, 6))\n# fares.hist(bins=40)\n# # 过滤出20%的人。\n# fares.quantile(0.80)","execution_count":32,"outputs":[]},{"metadata":{"_uuid":"ec5392e13518ba9a74c478180e3698542da8279e","trusted":true,"collapsed":true},"cell_type":"code","source":"# a = list(range(0, 401, 40))\n# a.append(2500)\n\n\n# df_temp = df.copy()\n# df_temp['fare_level']= pd.cut(fares, bins=a)\n# df_temp.groupby('fare_level').Survived.agg(['count', 'sum', 'mean'])","execution_count":33,"outputs":[]},{"metadata":{"_uuid":"67d11d8a3f1fa62440c4307bf08fd52aade58472","trusted":true,"collapsed":true},"cell_type":"code","source":"# 上限尽量设的大点，因为我们这里没把test data也一起拿出来看，所以不知道范围。\n# df['upper_class'] = pd.cut(fares, bins=[0, 40, 160, 2500], labels=['low', 'middle', 'upper'])\n# df.groupby('upper_class').Survived.agg(['count', 'sum', 'mean'])","execution_count":34,"outputs":[]},{"metadata":{"_uuid":"24ca74eb9a7c49b5e4f46609fdc69b6d845379b4"},"cell_type":"markdown","source":"当我把upper class，不论是加入到预测Age或者是加入到预测Survived中，最后结果都是保持不变，所以我依旧觉得这个变量与Pclass重了，冗余了。"},{"metadata":{"_uuid":"c03c945249ef31d3b5847604192cb05fb91f509f","trusted":true,"collapsed":true},"cell_type":"code","source":"# plt.figure(figsize=(9, 6))\n# sns.heatmap(df[['Pclass', '']], cmap='cool', annot=True)","execution_count":35,"outputs":[]},{"metadata":{"_uuid":"ca42b2cdca5d14f910b96b5ec51eab1e8100c390"},"cell_type":"markdown","source":"## 4.6 Ticket"},{"metadata":{"_uuid":"37d2152b0f48620a0622cad7ef7096d8f77ce8b7","trusted":true},"cell_type":"code","source":"dataset.Ticket.head(5)","execution_count":36,"outputs":[]},{"metadata":{"_uuid":"f91ae5445541b2f8891e346552bc0cfbaafeefb1"},"cell_type":"markdown","source":"Ticket或许能够说明距离逃生窗口之类的远近，然后我们没有这方面的知识，暂且不考虑这个变量，否则就能够根据他们和窗口的距离划分。\n## 4.7 Cabin\n客舱号码类似于Ticket的一个变量，但是缺失严重，77%的数据都已经缺失。会有类似国内无座这种情况存在吗？"},{"metadata":{"_uuid":"34444252360a27c9bade3496b70a3e85c59b509f","trusted":true},"cell_type":"code","source":"dataset.Cabin.isna().sum() / len(dataset.Cabin)","execution_count":37,"outputs":[]},{"metadata":{"_uuid":"cd82f2b50f42b29fb72f5f4ba2038be159508df0","trusted":true},"cell_type":"code","source":"dataset.Cabin = dataset.Cabin.apply(lambda x : 0 if pd.isna(x) else 1)\nsns.factorplot(x='Cabin', y='Survived', data=dataset, kind='bar')","execution_count":38,"outputs":[]},{"metadata":{"_uuid":"7fb2adaf0a90688d85dd50a046db41b97609d60a"},"cell_type":"markdown","source":"有座的明显要比无座的幸存率高啊"},{"metadata":{"_uuid":"46a951773d10aef2b177874fe4bfac459147d8ab"},"cell_type":"markdown","source":"## 4.8 Embarked\n登陆港口与存活几率有关吗"},{"metadata":{"_uuid":"765937da2cdb63563079d2190582101bb456b172","trusted":true},"cell_type":"code","source":"dataset.Embarked.isna().sum(), '--'*12, dataset.Embarked.value_counts()","execution_count":39,"outputs":[]},{"metadata":{"_uuid":"554d76d1af354f0e1ba3855771e46bc625188fe6","trusted":true,"collapsed":true},"cell_type":"code","source":"dataset.Embarked.fillna('S', inplace=True)","execution_count":40,"outputs":[]},{"metadata":{"_uuid":"5475d427e9cc5ded11730ef7fa6ec7b7c2aa66e0","trusted":true},"cell_type":"code","source":"dataset.groupby('Embarked').Survived.agg(['count', 'sum', 'mean', 'std'])","execution_count":41,"outputs":[]},{"metadata":{"_uuid":"307782fff9055591ff22333db26cb4b464630353"},"cell_type":"markdown","source":"似乎各个港口登陆的存活几乎都差不多，如果我们结合PClass看看是否还是如此呢。"},{"metadata":{"_uuid":"79f737e73007bb48f1d44b884b58315475c27673","trusted":true},"cell_type":"code","source":"dataset.groupby(['Pclass', 'Embarked']).Survived.agg(['count', 'sum', 'mean'])\nsns.factorplot(x='Embarked', y = 'Survived', hue='Pclass', data=dataset, size=5, kind='bar', palette='cool')","execution_count":42,"outputs":[]},{"metadata":{"_uuid":"3bd58b135a4660fe768f2301191122d5c7a90f90"},"cell_type":"markdown","source":"我们注意到三等舱从S港口登船的幸存率最低，只有0.189。上面提到，女性存活率很高，那么三等舱S港口是否是男性比例较高导致的呢？那么我们重点观察以下S港口的三等舱的性别比例。"},{"metadata":{"_uuid":"5deda59442dfe49fbf872764dab80eb161fb36f4","trusted":true},"cell_type":"code","source":"# a = df.groupby(['Pclass', 'Embarked', 'Sex']).Survived.agg(['count', 'sum', 'mean', 'std'])\ndataset[(dataset.Embarked == 'S') & (dataset.Pclass == 3)].groupby('Sex').Survived.agg(['count', 'sum', 'mean', 'std'])","execution_count":43,"outputs":[]},{"metadata":{"_uuid":"c6aff5f59072c23b19e964eeb818658aa786091e"},"cell_type":"markdown","source":"三等舱的存活几率都很低，女性也超过一般死亡，男性就更低了，只有12.8%。要加入一类判断是否是三等舱，S港口的？\n"},{"metadata":{"_uuid":"b330064cba810f67bb00d668b2acb94701db56c0"},"cell_type":"markdown","source":"## 4.9 Age\n由于Age数据有缺失，只针对有数据的部分先做一下统计。先看看年龄分布，还是呈现正太分布的。如果我们后面Wrongle Data的话，也是有必须要看看填充后是否继续符合这个年龄分布。"},{"metadata":{"_uuid":"58baf47ecdac3a1db8508b0ad5e9394ce273b154","trusted":true},"cell_type":"code","source":"# df.Age.hist(bins=20)\nsns.distplot(dataset.Age[dataset.Age.notna()])","execution_count":44,"outputs":[]},{"metadata":{"_uuid":"ba0be416ebdd9fe2642b6e6b69108f40c4da4d92","trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(9, 7))\ng = sns.kdeplot(dataset.Age[(dataset.Survived == 1) & (dataset.Age.notna())], color='r', ax=fig.gca())\ng = sns.kdeplot(dataset.Age[(dataset.Survived == 0) & (dataset.Age.notna())], color='b', ax=fig.gca())\ng.set_xlabel('Age')\ng.set_ylabel('Survived')\ng.legend(['Survived', 'Not'])","execution_count":45,"outputs":[]},{"metadata":{"_uuid":"e51a0487eccf8ed785b62354a4065663d8e8a978","trusted":true},"cell_type":"code","source":"dataset['age_level'] = pd.cut(dataset.Age, bins=[0, 18, 60, 100], labels=[3, 2, 1])\n# dataset.groupby('age_level').Survived.agg(['count', 'sum', 'mean', 'std'])\nsns.factorplot('age_level', 'Survived', hue='Sex', data=dataset, kind='bar', size=5, palette='cool')","execution_count":46,"outputs":[]},{"metadata":{"_uuid":"12b0042f9892f1c5a02766b53f218372edbce142"},"cell_type":"markdown","source":"说明幼儿存活率更高，老人的存活几率非常低，而青壮年男性是牺牲的主力。我们将其与性别结合看看结果。holycrap, 青壮年男性存活几率不到20%，即使是老年组里，老年男性也几乎全跪。"},{"metadata":{"_uuid":"67f2dcb02d6b8517077c272a11e3f8375a96116e"},"cell_type":"markdown","source":"# 5. Cleaning Data, Wrangle\n我们知道缺失数据的有Cabin>Age>Embarked。Cabin我们已经将其转换为有或无了，Embarked的两个缺失也被我们填充了众数。而Age毕竟是一个非常影响预测结果的变量，我们需要好好填充一下\n## 5.1 Fill Age\n我们这里就不去预测离散的年龄了，而是预测年龄的分组，这样就使得回归的年龄问题转变为分组问题。"},{"metadata":{"_uuid":"895d492fa5a53a4ecee6ac97799a39c46450602d","trusted":true},"cell_type":"code","source":"dataset.head(5)","execution_count":47,"outputs":[]},{"metadata":{"_uuid":"fbc8bbe08ad2a452c131b3a00f057e2d6c815648"},"cell_type":"markdown","source":"我们需要那些变量来预测我们的年龄呢？这里我们就可以看看相关性矩阵"},{"metadata":{"_uuid":"948363f90d64c152767109e0ec265e53ad96fb71","trusted":true},"cell_type":"code","source":"# dataset[dataset.age_level.notna()].age_level = dataset.age_level[dataset.age_level.notna()].astype('int')\nfig = plt.figure(figsize=(9, 6))\nsns.heatmap(dataset[['Age', 'Survived', 'Pclass', 'Sex', 'family_size_level', 'Parch', 'SibSp', 'title_level', 'age_level', 'Fare']].corr(), cmap='cool', annot=True, ax=fig.gca())","execution_count":48,"outputs":[]},{"metadata":{"_uuid":"1ad31b8a0bf794273a3adb2b1b84eb0036ef920f"},"cell_type":"markdown","source":"上图中，跟Age相关比较大的有Pclass, family_size, Fare，Sex和title_level无关，这可能与我们设置的值有关系。"},{"metadata":{"_uuid":"458c889e0a18afc25b2be152ef7430b3442e79aa","trusted":true},"cell_type":"code","source":"dataset.groupby('title_level').Age.agg(['mean', 'median', 'std'])","execution_count":49,"outputs":[]},{"metadata":{"_uuid":"402110cdf54cc06fb134d2e1452d5c87c459a4f9","trusted":true},"cell_type":"code","source":"dataset.groupby(['title']).Age.agg(['mean', 'median', 'std', 'max'])","execution_count":50,"outputs":[]},{"metadata":{"_uuid":"4e3b64aff9e46c276a8ac8df1a31cfbbc8322144"},"cell_type":"markdown","source":"我们可以看到，title_level设置的对Age波动非常大，而其本身title的std波动就非常小，说明基本上都在mean的附近。所以我们认为title也是与age比较相关的一个重要因素。假如我们把title按照年龄重新分组，我们就比较有可能得到相关性的证据了，我们按照Age的均值与中值对title进行重新编码值。"},{"metadata":{"_uuid":"1ad72b13c2654f5685960414744cc4b10cb90b0a","trusted":true,"collapsed":true},"cell_type":"code","source":"dataset['title_age_level'] = dataset.title.map({\"Master\": 1, 'Mister': 1, \"Miss\": 2, \"Mrs\": 3,  \"Mr\": 3, \"rare\": 4})\n# dataset['title_age_level'] ","execution_count":51,"outputs":[]},{"metadata":{"_uuid":"f0aeb85f20ab94b9b0c1f1ad26b3986c5d9126ca","trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(9, 6))\nsns.heatmap(dataset[['Age', 'Survived', 'Pclass', 'Sex', 'family_size_level', 'Parch', 'SibSp', 'title_level', 'title_age_level', 'age_level', 'Fare']].corr(), cmap='cool', annot=True, ax=fig.gca())","execution_count":52,"outputs":[]},{"metadata":{"_uuid":"247d84469fdf85105f32bd10d5160eab46bcb86a","trusted":true},"cell_type":"code","source":"['Survived', 'Pclass', 'Sex', 'family_size_level', 'title_level', 'age_level', 'Fare']","execution_count":53,"outputs":[]},{"metadata":{"_uuid":"a61bc3ac8494bb71c02387f8c07c4b055724be88"},"cell_type":"markdown","source":"这样结果就比较明显了, \n- Age与Pclass，family_size, title_age_level, Fare的相关性就比较明显了。\n- 同时，我们还发现Fare与Pclass也具有强相关性，\n- Sex不知道怎么与title_level达到如此强的负相关。。。。\n- 如果考虑Survived的话，也能够看出，Survived与Pclass，Sex，title_level, title_age_level, Fare的相关性都不错，这里我们的family_size居然没那么明显，看来还是值设置的有问题？"},{"metadata":{"_uuid":"6d0ea9be8b55ed48e40aca490501e3266da25809","trusted":true},"cell_type":"code","source":"df_age_train = dataset[dataset.Age.notnull()]\ndf_age_test = dataset[dataset.Age.isnull()]\n\ndf_age_train.shape, df_age_test.shape","execution_count":54,"outputs":[]},{"metadata":{"_uuid":"13e852b43d20a166dc2776fc2257a06cf54c2cda","trusted":true,"collapsed":true},"cell_type":"code","source":"df_age = df_age_train[['Pclass', 'Sex', 'Parch', 'SibSp', 'title_age_level', 'Fare', 'age_level']]\nX_age_dummied = pd.get_dummies(df_age.drop(columns='age_level'), columns=['Pclass', 'Sex', 'Parch', 'SibSp', 'title_age_level'])\nX_age_dummied['SibSp_8'] = np.zeros(len(X_age_dummied))\nX_age_dummied['Parch_9'] = np.zeros(len(X_age_dummied))\n\n\nY_age = df_age['age_level']","execution_count":55,"outputs":[]},{"metadata":{"_uuid":"625737318e70651498b88cfe61d02b7f7a92da9d","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\n\nparams = {'n_estimators':range(10, 40, 5), 'max_depth':[3, 4, 5], 'max_features':range(3, 9)}\nclf = RandomForestClassifier(random_state=0)\n\ngscv = GridSearchCV(estimator=clf, param_grid=params, scoring='f1_micro', n_jobs=1, cv=5, verbose=1)\ngscv.fit(X_age_dummied, Y_age)","execution_count":56,"outputs":[]},{"metadata":{"_uuid":"8e61d2e871c8ae7d399f5a15cd56883a7b101946","trusted":true},"cell_type":"code","source":"gscv.best_score_, gscv.best_params_, gscv.best_estimator_.feature_importances_","execution_count":57,"outputs":[]},{"metadata":{"_uuid":"fe4ca4960d841bfebb2a228bd00bca5b1d1d06ed"},"cell_type":"markdown","source":"> 在加入对小姐姐的titleMister后，我们的Age的预测准确率又上升了，并且经过两次调参，获得了0.94537815126050417的f1_micro.\n\n之前的版本达到了0.945，现在改成全部数据，反而达不到了。看来有离群点加入进来了，导致我们无法获得比较好的结果。😅"},{"metadata":{"_uuid":"286d23c42c2c67086b81f9a17afad65fb29f886b","trusted":true},"cell_type":"code","source":"pd.Series(gscv.best_estimator_.feature_importances_, index=X_age_dummied.columns).sort_values(ascending=True).plot.barh(figsize=(9, 6))\nsns.despine(bottom=True)","execution_count":58,"outputs":[]},{"metadata":{"_uuid":"2bad3642db8d9b49f752e2c5617216f6af58093b","trusted":true},"cell_type":"code","source":"X_age_dummied = pd.get_dummies(df_age.drop(columns='age_level'), columns=['Pclass', 'Sex', 'Parch', 'SibSp', 'title_age_level'])\n\nab = df_age_test[['Pclass', 'Sex', 'Parch', 'SibSp', 'title_age_level', 'Fare']]\nX_age_dummied_test = pd.get_dummies(ab, columns=['Pclass', 'Sex', 'Parch', 'SibSp', 'title_age_level'])\nX_age_dummied.shape, X_age_dummied_test.shape","execution_count":59,"outputs":[]},{"metadata":{"_uuid":"6ba9b2813ab05d12d0fcc320f15785c2ea06da1d","trusted":true},"cell_type":"code","source":"X_age_dummied.columns, X_age_dummied_test.columns","execution_count":60,"outputs":[]},{"metadata":{"_uuid":"c97dd97b5632b7ecb0d5ee033609acde47c96c3d","trusted":true,"collapsed":true},"cell_type":"code","source":"X_age_dummied_test['Parch_3'] = np.zeros(len(X_age_dummied_test))\nX_age_dummied_test['Parch_5'] = np.zeros(len(X_age_dummied_test))\nX_age_dummied_test['Parch_6'] = np.zeros(len(X_age_dummied_test))\n\nX_age_dummied_test['SibSp_4'] = np.zeros(len(X_age_dummied_test))\nX_age_dummied_test['SibSp_5'] = np.zeros(len(X_age_dummied_test))\n\ndf_age_test.age_level = gscv.predict(X_age_dummied_test)","execution_count":61,"outputs":[]},{"metadata":{"_uuid":"447ba205665347a32e818bfee37529f167f3b174","trusted":true},"cell_type":"code","source":"df_age_test.shape","execution_count":62,"outputs":[]},{"metadata":{"_uuid":"32ecb5e05ae9e7c5a764018af39b187a29cf811c","trusted":true},"cell_type":"code","source":"df_final = pd.concat([df_age_test, df_age_train]).sort_index()\ndf_final.info()","execution_count":63,"outputs":[]},{"metadata":{"_uuid":"1510bf9c0f6122d99fc1f7e714d7a42b5aa1d34b"},"cell_type":"markdown","source":"## 5.3 Clean Done\nTrainning里的所有数据都已经清理完毕，没有NA值了，接下来就进入模型评估阶段。\n# 6. Basic Model & Evalution\n## 6.1 Model & Evaluation\n我们选用如下的Models，都是我在课上学过的额，然后用cross_validataion_score获得一个初始评估。\n- Logistic Regression\n- KNN\n- SVM\n- Naive Bayes\n- Nerual Network, MLP\n- DecisionTree\n- RandomForest\n- Gradient Boosting Decision Tree"},{"metadata":{"_uuid":"9acc2bfebab63b83323930f9b882926f96e05a94","trusted":true,"collapsed":true},"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n","execution_count":64,"outputs":[]},{"metadata":{"_uuid":"f7a8777ec205789dba492c673ff3f2fdd83167cc","trusted":true,"collapsed":true},"cell_type":"code","source":"names = ['LR', 'KNN', 'SVM', 'Bayes', 'NW', 'DT', 'RF', 'GBDT']\nmodels = [LogisticRegression(random_state=0), KNeighborsClassifier(n_neighbors=3), SVC(gamma='auto', random_state=0), GaussianNB(), MLPClassifier(solver='lbfgs', random_state=0),\n         DecisionTreeClassifier(), RandomForestClassifier(random_state=0), GradientBoostingClassifier(random_state=0)]","execution_count":65,"outputs":[]},{"metadata":{"_uuid":"9c1ac230b7a1161b65d340e3a27b8e1b6c8125fd"},"cell_type":"markdown","source":"Feature Selection，我们选择在上面那张里和Survived具有比较大的相关性的Feature。"},{"metadata":{"_uuid":"8627de17f14cf961509d467485c4d26a0c58b703","trusted":true},"cell_type":"code","source":"selection = ['Survived', 'Pclass', 'Sex', 'family_size_level', 'title_level', 'age_level', 'Fare', 'Embarked', 'Cabin']\ntrain_set = df_final[df_final.Survived.notna()][selection]\ntest_set = df_final[df_final.Survived.isna()][selection]\nX_train = train_set.drop(columns='Survived')\nY_train = train_set['Survived']\nX_test = test_set.drop(columns='Survived')\n\nX_train= pd.get_dummies(X_train, columns=['Pclass', 'Sex', 'family_size_level', 'title_level', 'age_level', 'Embarked', 'Cabin'])\nX_test = pd.get_dummies(X_test, columns=['Pclass', 'Sex', 'family_size_level', 'title_level', 'age_level', 'Embarked', 'Cabin'])\nX_train.shape, X_test.shape","execution_count":66,"outputs":[]},{"metadata":{"_uuid":"c8a51fe94a0e9dc58be76b919254c032b1133477","trusted":true},"cell_type":"code","source":"X_train.columns, X_test.columns","execution_count":67,"outputs":[]},{"metadata":{"_uuid":"df308426579cf2ba7b6a2143f7cc69c4178db104","trusted":true},"cell_type":"code","source":"for name, model in zip(names, models):\n    score = cross_val_score(model, X_train, Y_train, cv=5, scoring='roc_auc')\n    print('score on {}, mean:{:.4f}, from {}'.format(name, score.mean(), score))","execution_count":68,"outputs":[]},{"metadata":{"_uuid":"f21486845748e3ecd1bc361f156f53b80527d184"},"cell_type":"markdown","source":"未调仓情况下，我们在LR和GBDT上获得两个最高分数0.8668，0.8842。由于某些Classifier对数据的量级敏感，因此，我们需要对数据进行一个预处理。但是，我们几乎所有的数据都被转换成了dummies的形式，Fare数据也被我们log化，转换到0-7之间了，所以我觉得并不需要`from sklearn.preprocessing import StandardScaler`它去缩放数据。\n\n## 6.2 Feature importance\n我们可以直观上看看LogisticRegression认为哪些feature权重比较高。"},{"metadata":{"_uuid":"733c06f3baee30490d087e983c56ec8f64543078","trusted":true},"cell_type":"code","source":"lr = LogisticRegression(random_state=0).fit(X_train, Y_train)\nlr.coef_","execution_count":69,"outputs":[]},{"metadata":{"_uuid":"db6f269df8030f13f3a5e20d772b8d1b9601d3a5","trusted":true},"cell_type":"code","source":"from matplotlib import cm\ncolor = cm.inferno_r(np.linspace(.4,.8, len(X_train.columns)))\n\npd.DataFrame({'weights': lr.coef_[0]}, index=X_train.columns)\\\n.sort_values(by='weights',ascending=True)\\\n.plot.barh(figsize=(7, 7),fontsize=12, legend=False, title='Feature weights from Logistic Regression', color=color);\nsns.despine(bottom=True);","execution_count":70,"outputs":[]},{"metadata":{"_uuid":"425b2d30c4f766fd4201b29e03cbcad302724e57"},"cell_type":"markdown","source":"这个Feature Weights还挺符合我们的认知的，**familysize_level_1（1人），familysize_level_2（2-4人），title_level_3(Mrs, Miss)，age_level_3(child)， Sex_0(女性)，title_level_2（Mister, Master, rare）, Fare(费用)，Cabin_1(有客舱)，Pclass_1（1等舱），Pclass_2(二等舱)，Embarked_C(C港口)**都具有提高幸存率的参数。**这里为什么familysize_level为1个人时候，为什么对判定影响这么大**？？？\n\n另外，**faimily_size_level_0（大型家庭），title_level_1（Mr）, Sex_1(male)，age_level_1(老年人)，Cabin_0(无客舱)，Pclass_3（三等舱），Embarked_S(S港口)、age_level_2(年轻-中年人)**都就具有较大的负权重，这些都是对Survived的debuff。与我们上面最开始feature分析的还是一致的。\n\n\n我们还可以试着看看DecisionTree的feature importance"},{"metadata":{"_uuid":"e98e0cdd7c4a2e20186bf7ad04fc9500e4c65375","trusted":true},"cell_type":"code","source":"def plot_decision_tree(clf, feature_names, class_names):\n    from sklearn.tree import export_graphviz\n    import graphviz\n    export_graphviz(clf, out_file=\"adspy_temp.dot\", feature_names=feature_names, class_names=class_names, filled = True, impurity = False)\n    with open(\"adspy_temp.dot\") as f:\n        dot_graph = f.read()\n    return graphviz.Source(dot_graph)\n\ndtc = DecisionTreeClassifier().fit(X_train, Y_train)\n\npd.DataFrame({'importance': dtc.feature_importances_}, index=X_train.columns)\\\n.sort_values(by='importance',ascending=True)\\\n.plot.barh(figsize=(7, 7),fontsize=12, legend=False, title='Feature importance from Decision Tree', color=color);\nsns.despine(bottom=True, left=True);\n\n# 这个DecisionTree太大了啊，真实天性容易过拟合，这层数也太多了，所以不画了。\n# plot_decision_tree(dtc, X_train.columns, 'Survived')","execution_count":71,"outputs":[]},{"metadata":{"_uuid":"983078fa04fc962e38005581ccce827086c52e8c"},"cell_type":"markdown","source":"DT里的feature importance，按我的理解，是按照**title_level_1（Mr）进行大范围拆分，然后是Fare、family_size_0（家庭成员过多的）、Pclass_3（三等舱）、Embarked_S（S港口），age_level_2（年轻-中年）、age_level_3(老年人)、age_level_1(儿童)、Cabin_0（无客舱）**等等等。\n\n## 6.3 Misclassification Analysis\n通过观察被算法误分类的records，设计出新的feature或其他的方法，来减少误分类，最终进一步提高分类的准确度。怎么做呢？还是在K-Fold上，然后调用算法，将其分类错误的找出来。"},{"metadata":{"_uuid":"08c2585978db7ae651022adcd0f16da559ce2699","trusted":true,"collapsed":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nskf = StratifiedKFold(n_splits=4, random_state=0)\n\n# from sklearn.model_selection import LeaveOneOut\n# loo = LeaveOneOut()\n# miss classifications\nmcs = []\n\nfor train_index, test_index in skf.split(X_train, Y_train):\n    x_train, x_test = X_train.iloc[train_index], X_train.iloc[test_index]\n    y_train, y_test = Y_train.iloc[train_index], Y_train.iloc[test_index]\n    y_pred =lr.fit(x_train, y_train).predict(x_test)\n    mcs.append(x_test[y_pred != y_test].index.tolist())","execution_count":72,"outputs":[]},{"metadata":{"_uuid":"20696b042bae1994992dcc6b424dae79b9ec1182","trusted":true},"cell_type":"code","source":"mcs_index = np.concatenate((mcs))\nlen(mcs_index), len(Y_train), 'miss classification rate:', len(mcs_index)/len(Y_train)","execution_count":73,"outputs":[]},{"metadata":{"_uuid":"cba0276f12a3ff44f22ae06310361eb6efd67e5e"},"cell_type":"markdown","source":"我们看到Miss Classification Rate为18.4%，与我们之前所得也差不多。接下来我们就看看这166个，按照LR的feature weights为何会被错分类呢，有什么特点吗？"},{"metadata":{"_uuid":"9d95a17ecba37d50e5d7daa978e6f259dd347510","trusted":true},"cell_type":"code","source":"# mcs_df = pd.concat([X_train.iloc[mcs_index], Y_train.iloc[mcs_index]], axis=1)\nmcs_df = train_set.iloc[mcs_index]\nmcs_df.head()","execution_count":74,"outputs":[]},{"metadata":{"_uuid":"a1b7c2db142b10d14471b681832b73ab5072b03e","trusted":true},"cell_type":"code","source":"mcs_df.describe(include='all').T","execution_count":75,"outputs":[]},{"metadata":{"_uuid":"e726c7ffc082319ad9471af8b1bf202aa06ed151"},"cell_type":"markdown","source":"我们解读一下以上数据，它描述了大部分被分类错误的人的基本信息，这里的存活率为0.373，非常低，即大部分都是死亡，**也就是我们的目标是怎么让这些人被预测为死亡**。其中大部分三等舱，男性，S港口，独身，中年人被预测为存活。所以我们的目标是根据LR的feature weights，添加更容易被预测为死亡的feature融合，即Feature weights为负的那些feature。\n\n我们把mcs_df里那些占比例比较多的Feature，并且是LR feature weights里为正的那些主要feature提取出来，进行组合以便于降低判定为1的几率。\n> faimily_size_level_0（大型家庭），title_level_1（Mr）, Sex_1(male)，age_level_1(老年人)，Cabin_0(无客舱)，Pclass_3（三等舱），Embarked_S(S港口)、age_level_2(年轻-中年人)\n\n\n正参数feature。\n\n> familysize_level_1（1人），familysize_level_2（2-4人），title_level_3(Mrs, Miss)，age_level_3(child)， Sex_0(女性)，title_level_2（Mister, Master, rare）, Fare(费用)，Cabin_1(有客舱)，Pclass_1（1等舱），Pclass_2(二等舱)，Embarked_C(C港口)"},{"metadata":{"_uuid":"76dab1d8b77db95630ed54cbafd6d2897590eb9e","trusted":true},"cell_type":"code","source":"mcs_df[mcs_df.family_size_level == 1].shape, mcs_df[mcs_df.title_level == 3].shape, mcs_df[mcs_df.age_level == 3].shape","execution_count":76,"outputs":[]},{"metadata":{"_uuid":"ee43f26d2d370850be91670fc63488eeb04bd8cf"},"cell_type":"markdown","source":"由于family_size_level=1（一个人）给了过大的比重，造成错误的分类，那么怎么降低这个family_size_level为一个人时候的比重呢？"},{"metadata":{"_uuid":"2acbbdb10e7059b01b295f467a39478311cc93b2","trusted":true},"cell_type":"code","source":"mcs_df.groupby('family_size_level').Survived.mean()","execution_count":77,"outputs":[]},{"metadata":{"_uuid":"91583cf67ea96ea4ce09cf71f4d41a37c4b88e39","trusted":true,"collapsed":true},"cell_type":"code","source":"# df_final['MPSE'] = np.ones(len(df_final))\n# df_final.MPSE[(df_final.title_level == 1) & (df_final.Pclass == 3) & (df_final.Sex == 0) & (df_final.Embarked == 'S') & (df_final.family_size_level.isin([1, 2]))] = 4\n# df_final.MPSE[(df_final.title_level == 1) & (df_final.Pclass == 2) & (df_final.Sex == 0) & (df_final.Embarked == 'S') & (df_final.family_size_level.isin([1, 2]))] = 3\n# df_final.MPSE[(df_final.title_level == 1) & (df_final.Pclass == 1) & (df_final.Sex == 0) & (df_final.Embarked == 'S') & (df_final.family_size_level.isin([1, 2]))] = 2\n# df_final.MPSE.value_counts()\n\ndf_final['Alone'] = np.zeros(len(df_final))\ndf_final['Alone'][df_final.family_size_level == 1] = 10\ndf_final['EmbkS'] = np.zeros(len(df_final))\ndf_final['EmbkS'][df_final.Embarked == 'S'] = 10\ndf_final['MrMale'] = np.zeros(len(df_final))\ndf_final['MrMale'][df_final.title_level == 1] = 10","execution_count":78,"outputs":[]},{"metadata":{"_uuid":"059943adddcb499383c0bcc4e29876fa947311f0"},"cell_type":"markdown","source":"验证一下，是否有改进？从0.8668，到0.8674， 只有0.001丁点儿的改进，😰"},{"metadata":{"_uuid":"482aeb653eec21db00d942304e1c87cbea3f8056","trusted":true},"cell_type":"code","source":"selection = ['Survived', 'Pclass', 'Sex', 'family_size_level', 'title_level', 'age_level', 'Fare', 'Embarked', 'EmbkS', 'MrMale', 'Cabin', 'Alone']\ntrain_set = df_final[df_final.Survived.notna()][selection]\ntest_set = df_final[df_final.Survived.isna()][selection]\nX_train = train_set.drop(columns='Survived')\nY_train = train_set['Survived']\nX_test = test_set.drop(columns='Survived')\n\nX_train= pd.get_dummies(X_train, columns=['Pclass', 'Sex', 'family_size_level', 'title_level', 'age_level', 'Embarked', 'Cabin'])\nX_test = pd.get_dummies(X_test, columns=['Pclass', 'Sex', 'family_size_level', 'title_level', 'age_level', 'Embarked', 'Cabin'])\n\nval_lr = LogisticRegression(random_state=0)\na_score = cross_val_score(val_lr, X_train, Y_train, cv=5, scoring='roc_auc')\na_score.mean()","execution_count":79,"outputs":[]},{"metadata":{"_uuid":"51c1f6eb0c7ada2028d0d4ebffa1634107fffe3c"},"cell_type":"markdown","source":"# 7 Hyperparameters Tuning\n我们选取几个表现比较好的进行调参，看看最终结果。如Logistic Regression，KNN，SVM， NW， RF，GBDT"},{"metadata":{"_uuid":"d949d3d8addf5e8aca693c73f13704c3eaf60f03","trusted":true,"collapsed":true},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\n\ndef tune_estimator(name, estimator, params):\n    gscv_training = GridSearchCV(estimator=estimator, param_grid=params, scoring='roc_auc', n_jobs=1, cv=5, verbose=False)\n    gscv_training.fit(X_train, Y_train)\n    return name, gscv_training.best_score_, gscv_training.best_params_","execution_count":80,"outputs":[]},{"metadata":{"_uuid":"156f1ec84ce342dc5766d737a5df153336156fe2"},"cell_type":"markdown","source":"## 7.1 LR"},{"metadata":{"_uuid":"33338980100eb67cc430455effcf89216c8ce3cb","trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nparams = {'C':[0.03, 0.1, 0.3, 1, 3, 10, 20, 30, 50]}\nclf = LogisticRegression(random_state=0)\ntune_estimator('LR', clf, params)","execution_count":81,"outputs":[]},{"metadata":{"_uuid":"720851eff64ed65264e10e4dcaa7426db262b4b5","trusted":true},"cell_type":"code","source":"# Second fine tuning.\nparams = {'C':[0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]}\nclf = LogisticRegression(random_state=0)\n\ntune_estimator('LR', clf, params)","execution_count":82,"outputs":[]},{"metadata":{"_uuid":"ead1f544bb4f74171c47c1e5d5698a216b578cfe"},"cell_type":"markdown","source":"## 7.2 KNN"},{"metadata":{"_uuid":"12b61baeb0d1714fa3963ba9fbab65b16dd24bf3","trusted":true},"cell_type":"code","source":"params = {'n_neighbors': range(3, 15, 3)}\nclf = KNeighborsClassifier()\ntune_estimator('KNN', clf, params)","execution_count":83,"outputs":[]},{"metadata":{"_uuid":"aa5445b933e8026cad5677b6c8e3bcce35c600a6","trusted":true},"cell_type":"code","source":"params = {'n_neighbors': [10, 11, 12, 13, 14]}\nclf = KNeighborsClassifier()\ntune_estimator('KNN', clf, params)","execution_count":84,"outputs":[]},{"metadata":{"_uuid":"c8baa6d028365b920e33057618b0198f7d7808cd"},"cell_type":"markdown","source":"## 7.3 SVM"},{"metadata":{"_uuid":"735ca9db485efe8bf9ca1ea27a32ef711a79d28d","trusted":true},"cell_type":"code","source":"params = {'C':[0.01, 0.1, 1, 10], 'gamma':[0.001, 0.01, 0.1], 'kernel':['rbf']}\nclf = SVC(random_state=0)\ntune_estimator('SVM', clf, params)","execution_count":85,"outputs":[]},{"metadata":{"_uuid":"74968f8c6be6ba7b5f1cd25e61cda5efe7dd5df2","trusted":true},"cell_type":"code","source":"params = {'C':range(8, 14), 'gamma':[0.05, 0.08, 0.01, 0.03, 0.05], 'kernel':['rbf']}\nclf = SVC(random_state=0)\ntune_estimator('SVM', clf, params)","execution_count":86,"outputs":[]},{"metadata":{"_uuid":"9885897948ba3c0c8d07d5a87d68ba3c73c44ac1"},"cell_type":"markdown","source":"## 7.4 Nerual Network"},{"metadata":{"_uuid":"96cf63017a9574b025f9b683f60dfaf9e7be67b4","trusted":true},"cell_type":"code","source":"params = {'hidden_layer_sizes':[x for x in zip(range(20, 100, 10), range(20, 100, 20))],\n          'solver':['lbfgs'], 'alpha': [0.0001, 0.001, 0.01]}\nclf = MLPClassifier(random_state = 0)\ntune_estimator('NW', clf, params)","execution_count":87,"outputs":[]},{"metadata":{"_uuid":"4968b338de63440f75e5e24b8a9ddea4195fb7bc"},"cell_type":"markdown","source":"## 7.5 Random Forest"},{"metadata":{"_uuid":"f297660a736ad21b5a1c470c7384c9c1d006482d","trusted":true},"cell_type":"code","source":"params = {'n_estimators':range(10, 40, 5), 'max_depth':[3, 5], 'max_features':range(3, 7)}\nclf = RandomForestClassifier(random_state=0)\ntune_estimator('RF', clf, params)","execution_count":88,"outputs":[]},{"metadata":{"_uuid":"b37d0c608be1ed2224a0398028434192a538dfa9","trusted":true},"cell_type":"code","source":"# second round\nparams = {'n_estimators':range(31, 40, 2), 'max_depth':range(4, 7), 'max_features':range(3, 6)}\nclf = RandomForestClassifier(random_state=0)\ntune_estimator('RF', clf, params)","execution_count":89,"outputs":[]},{"metadata":{"_uuid":"a6464b9d8fd585096d5524d10d421c26eee87c8c"},"cell_type":"markdown","source":"## 7.6 GBDT"},{"metadata":{"_uuid":"3af1abe9963a1440d07ac51ec9edf3752ba6bb9c","trusted":true},"cell_type":"code","source":"params = {'learning_rate':[0.001, 0.01, 0.1, 1], 'n_estimators':range(100, 250, 20), 'max_depth':range(2, 5), 'max_features':range(3, 6)}\nclf = GradientBoostingClassifier(random_state=0)\ntune_estimator('GBDT', clf, params)","execution_count":90,"outputs":[]},{"metadata":{"_uuid":"e4f3b479ffca396c6222f496d6a2433b8c6b0e80","trusted":true},"cell_type":"code","source":"# second tune\nparams = {'learning_rate':[0.3, 0.5, 0.7, 0.9, 1.2, 1.4, 1.7, 2], 'n_estimators':range(130, 200, 20), 'max_depth':[2, 3, 4], 'max_features':[3, 4, 5]}\nclf = GradientBoostingClassifier(random_state=0)\ntune_estimator('GBDT', clf, params)","execution_count":91,"outputs":[]},{"metadata":{"_uuid":"9a6c7f250fa46966515330890cf5f423465fcd78"},"cell_type":"markdown","source":"## 7.7 Conclusion\n**最终，我们获得最大的得分是采用GBDT，得分0.87741180236289573。**"},{"metadata":{"_uuid":"76f045c95ef85d8bded1d36c3e1d593145973b4e"},"cell_type":"markdown","source":"# 8. Ensemble Methods\n知乎文章[【scikit-learn文档解析】集成方法 Ensemble Methods（上）：Bagging与随机森林](https://zhuanlan.zhihu.com/p/26683576)对集成方法进行了一些原理、使用方法的介绍。\n> 在机器学习中，集成方法（ensemble learning）把许多个体预测器（base estimator）组合起来，从而提高整体模型的鲁棒性和泛化能力。\n> \n> 集成方法有两大类：\n> \n> Averaging：独立建造多个个体模型，再取它们各自预测值的平均，作为集成模型的最终预测；能够降低variance。例子：随机森林，bagging。\n> Boosting：顺序建立一系列弱模型，每一个弱模型都努力降低bias，从而得到一个强模型。例子：AdaBoost，Gradient Boosting。\n\nGBDT（gradient boosting decision tree）可以说是最强大的数据挖掘算法之一：它能解决各种分类、回归和排序问题，能优秀地处理定性和定量特征，针对outlier的鲁棒性很强，数值不需要normalize。而且，LightGBM和XGBoost等算法库已经解决了GBDT并行计算的问题（然而scikit-learn暂不支持并行）[XGBoost](https://xgboost.readthedocs.io/en/latest/)。在本质上，决策树类型的算法几乎不对数据的分布做任何统计学假设，这使它们能拟合复杂的非线性函数。\n\n其中RandomForest和GBDT我们在上述方法中也已经使用过了，这里我们使用其他的方法，比如Bagging、AdaBoosting、Voting以及Stacking。\n\n## 8.1 Bagging\n因为RandomForest和GBDT都是基于决策树的，这里我们针对表现比较好的LR做一次Bagging。结果0.87594，是稍好于之前单一的LR 0.87440。流程图片来自文章[Ensemble of Weak Learners](http://manish-m.com/?p=794)，以及以下几个ensemble methods的流程图片俱来自于此。\n![](http://manish-m.com/wp-content/uploads/2012/11/BaggingCropped.png)\n\n采用有放回的抽样，如果N趋向于M，则每个训练集将只拥有63%的原始数据集，其他都是重复的。"},{"metadata":{"_uuid":"6c9a755ee824c4fdb7e17e493b5f5bb1ef44a137","trusted":true},"cell_type":"code","source":"from sklearn.ensemble import BaggingClassifier\nparams = {'n_estimators': range(50, 150, 10)}\nbagging = BaggingClassifier(LogisticRegression(C=0.3))\ntune_estimator('bagging', bagging, params)","execution_count":92,"outputs":[]},{"metadata":{"_uuid":"9f5edc552698fe67306237deba72b8e78cf65a29"},"cell_type":"markdown","source":"## 8.2 AdaBoosting\n知乎文章[【scikit-learn文档解析】集成方法 Ensemble Methods（下）：AdaBoost，GBDT与投票分类器](https://zhuanlan.zhihu.com/p/26704531)对Adabooting做了介绍，全程为Adaptive Boosting。\n> AdaBoost的核心理念，是按顺序拟合一系列弱预测器，后一个弱预测器建立在前一个弱预测器转换后的数据上。**每一个弱预测器的预测能力，仅仅略好于随机乱猜。**最终的预测结果，是所有预测器的加权平均（或投票结果）。\n> \n> 最初，原始数据有N个样本（w_1, w_2, w_3, ... , w_N），每个样本的权重是1/N。**在每一个boosting迭代中，一个弱分类器会拟合训练数据。之后，弱分类器分类错误的样本将会得到更多的权重。相应地，正确分类的样本的权重会减少。在下一个的迭代里，一个新的弱分类器会拟合（权重被调整后的）新的训练数据。**因此，越难分类的样本，在一次又一次的迭代中的权重会越来越高。\n> \n> 因此，AdaBoost的一个缺点就是不免疫数据中的outlier（尤其是分类分不对的outlier）。\n> \n> AdaBoost可以看作是使用了exponential loss损失函数的Gradient Boosting。\n\n有点不明白的是，每次错误样本如何获得更多的权重呢？\n\n它组合的是一种叫做decision stump，决策树桩的分类器，也就是单层决策树，单层也就意味着尽可对每一列属性进行一次判断。大概长这样\n![](https://img-blog.csdn.net/20160325144422445)\n\n优化目标，就是使得每一列的错误率判断的错误率最低。\n\\begin{equation}\n\\underset{1\\leq i\\leq d}{\\arg\\min}\\frac1N\\sum_{n=1}^N1_{y_n\\neq g_i(\\mathbf x)}\n\\end{equation}\n\n流程\n![](http://manish-m.com/wp-content/uploads/2012/11/BoostingCropped2.png)"},{"metadata":{"_uuid":"57357d7dead82737ae5b4d20f5dab3e92afb1c44","trusted":true},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\nparams = {'n_estimators':range(80, 200, 20), 'learning_rate':[0.5, 1, 3, 5]}\nadc = AdaBoostClassifier(random_state=0)\ntune_estimator('AdaBoosting', adc, params)","execution_count":93,"outputs":[]},{"metadata":{"_uuid":"55142292a967f05c591112b0dcd3ceef6e24f203","trusted":true},"cell_type":"code","source":"# secound round\nparams = {'n_estimators':range(150, 180, 10), 'learning_rate':[ 0.7, 0.9, 1.3, 2]}\nadc = AdaBoostClassifier(random_state=0)\ntune_estimator('AdaBoosting', adc, params)","execution_count":94,"outputs":[]},{"metadata":{"_uuid":"652cc78cc90a55dd546d9b660736bfdfd4b4f292"},"cell_type":"markdown","source":"## 8.3 Voting\n> 投票分类器是一种元预测器（meta-estimator），它接收一系列（在概念上可能完全不同的）机器学习分类算法，再将它们各自的结果进行平均，得到最终的预测。\n> \n> 投票方法有两种：\n> \n> 硬投票：每个模型输出它自认为最可能的类别，投票模型从其中选出投票模型数量最多的类别，作为最终分类。\n> 软投票：每个模型输出一个所有类别的概率矢量(1 * n_classes)，投票模型取其加权平均，得到一个最终的概率矢量。\n\n软投票，是对每个模型的所有类别概率进行加权平均得到的，而不是像硬投票每个类认为最可靠的投票决定。\n\n![](http://manish-m.com/wp-content/uploads/2012/11/Voting_Cropped.png)\n\n迄今为止，我们已经运用并获得了大部分具有良好性能的算法了，这里我们就对其排个序，以期使用结果排名靠前的算法做一个Voting。"},{"metadata":{"_uuid":"077201b18c5aad9e56e0fb83313dbcdba0ae80f7","trusted":true},"cell_type":"code","source":"lr = LogisticRegression(C=0.3, random_state=0)\nknn = KNeighborsClassifier(n_neighbors=10)\nsvc = SVC(C=9, gamma=0.1, random_state=0)\nmlp = MLPClassifier(alpha=0.001, hidden_layer_sizes=(20, 20), solver='lbfgs', random_state=0)\nrf = RandomForestClassifier(max_depth=5, max_features=5, n_estimators=31, random_state=0)\ngbdt = GradientBoostingClassifier(learning_rate=0.5, max_depth=2, max_features=3, n_estimators=190, random_state=0)\nbagging = BaggingClassifier(lr, n_estimators=140, random_state=0)\nabc = AdaBoostClassifier(learning_rate=1.3, n_estimators=150, random_state=0)\n\nnames = ['LR', 'KNN', 'SVC', 'MLP', 'RF', 'GBDT', 'Bagging', 'AdaB']\nmodels = [lr, knn, svc, mlp, rf, gbdt, bagging, abc]\n\nresult_scores = []\nfor name, model in zip(names, models):\n    scores = cross_val_score(model, X_train, Y_train, cv=5, scoring='roc_auc')\n    result_scores.append(scores.mean())\n    print('{} has a mean score {:.4f} based on {}'.format(name, scores.mean(), scores))    ","execution_count":95,"outputs":[]},{"metadata":{"_uuid":"3532a3f1a4c1ef1e4272b0ebd60f5845dfec7603","trusted":true},"cell_type":"code","source":"sorted_score = pd.Series(data=result_scores, index = names).sort_values(ascending=False)\nax = plt.subplot(111)\nsorted_score.plot(kind='line', ax=ax, title='score order', figsize=(9, 6), colormap='cool')\nax.set_xticks(range(0, 9))\nax.set_xticklabels(sorted_score.index);\nsns.despine()","execution_count":96,"outputs":[]},{"metadata":{"_uuid":"f48f47dc2b7aab2e84858d943e915c6fc62e88fe"},"cell_type":"markdown","source":"我们就选择GBDT、RF、LR、AdaBoosting、Bagging、SVC作为我们的投票base learners。"},{"metadata":{"_uuid":"7684af2c7aaaa428e0ef8753a31d6504f78249b9","trusted":true},"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\nnames = ['LR', 'KNN', 'RF', 'GBDT', 'Bagging', 'AdaB']\n\nlr = LogisticRegression(C=0.3, random_state=0)\nknn = KNeighborsClassifier(n_neighbors=10)\nrf = RandomForestClassifier(max_depth=5, max_features=5, n_estimators=31, random_state=0)\ngbdt = GradientBoostingClassifier(learning_rate=0.5, max_depth=2, max_features=3, n_estimators=190, random_state=0)\nbagging = BaggingClassifier(lr, n_estimators=140, random_state=0)\nabc = AdaBoostClassifier(learning_rate=1.3, n_estimators=150, random_state=0)\n\n# 直接投票，票数多的获胜。\nvc_hard = VotingClassifier(estimators=[('LR', lr), ('KNN', knn), ('GBDT', gbdt), ('Bagging', bagging), ('AdaB', abc)], voting='hard')\n# 参数里说，soft更加适用于已经调制好的base learners，基于每个learner输出的概率。知乎文章里讲，Soft一般表现的更好。\nvc_soft = VotingClassifier(estimators=[('LR', lr), ('KNN', knn), ('RF', rf), ('GBDT', gbdt), ('Bagging', bagging), ('AdaB', abc)], voting='soft')\n\n# 'vc hard:', cross_val_score(vc_hard, X_dummied, Y, cv=5, scoring='roc_auc').mean(),\\\n'vc soft:', cross_val_score(vc_soft, X_train, Y_train, cv=5, scoring='roc_auc').mean()","execution_count":97,"outputs":[]},{"metadata":{"_uuid":"d9e1cedabe340903e63620d47f5a7b099d7508be"},"cell_type":"markdown","source":"我们运行VC Hard一直出错，表示没有decision_function可用这类的。\n另外比起GBDT，VC soft提升了0.002。"},{"metadata":{"_uuid":"91980b8be9b6d12f8432f496260429aab1f80392"},"cell_type":"markdown","source":"## 8.4 Stacking\n![](http://manish-m.com/wp-content/uploads/2012/11/StackingCropped.png)\n看很多教程，Stacking 都是要把Test Data放进去，直接预测了。"},{"metadata":{"_uuid":"249b089decf38bc129dd4350540f01cd8701da5e","collapsed":true},"cell_type":"markdown","source":"## 8.4.1 Stacking\n### 构建第一层Stacking"},{"metadata":{"_uuid":"9b109915f0186e08e1159f56bc7569276601b4b2","trusted":true,"collapsed":true},"cell_type":"code","source":"n_train=X_train.shape[0]\nn_test=X_test.shape[0]\nkf=StratifiedKFold(n_splits=5,random_state=1,shuffle=True)","execution_count":98,"outputs":[]},{"metadata":{"_uuid":"6e51160bd082c0070aebdd23a7471ea95168894c"},"cell_type":"markdown","source":"定义Stacking第一层函数，接收未Fit的estimator，然后返回预测后的train_y和test_predict"},{"metadata":{"_uuid":"10680e45db67a8d3698d15cc12b666ee284b438a","trusted":true,"collapsed":true},"cell_type":"code","source":"def get_oof(clf, X, y, test_X):\n    oof_train = np.zeros((n_train, ))\n    oof_test_mean = np.zeros((n_test, ))\n    # 5 is kf.split\n    oof_test_single = np.empty((kf.get_n_splits(), n_test))\n    for i, (train_index, val_index) in enumerate(kf.split(X,y)):\n        kf_X_train = X.iloc[train_index]\n        kf_y_train = y.iloc[train_index]\n        kf_X_val = X.iloc[val_index]\n        \n        clf.fit(kf_X_train, kf_y_train)\n        \n        oof_train[val_index] = clf.predict(kf_X_val)\n        oof_test_single[i,:] = clf.predict(test_X)\n    # oof_test_single, 将生成一个5行*n_test列的predict value。那么mean(axis=0), 将对5行，每列的值进行求mean。然后reshape返回   \n    oof_test_mean = oof_test_single.mean(axis=0)\n    return oof_train.reshape(-1,1), oof_test_mean.reshape(-1,1)","execution_count":99,"outputs":[]},{"metadata":{"_uuid":"1e12cf4ef630d0481aab57e65a6dbf8c306347c2","trusted":true,"collapsed":true},"cell_type":"code","source":"lr = LogisticRegression(C=0.3, random_state=0)\nknn = KNeighborsClassifier(n_neighbors=10)\nrf = RandomForestClassifier(max_depth=5, max_features=5, n_estimators=31, random_state=0)\ngbdt = GradientBoostingClassifier(learning_rate=0.5, max_depth=2, max_features=3, n_estimators=190, random_state=0)\nbagging = BaggingClassifier(lr, n_estimators=140, random_state=0)\nabc = AdaBoostClassifier(learning_rate=1.3, n_estimators=150, random_state=0)\n\nlr_train, lr_test = get_oof(lr, X_train, Y_train, X_test)\nknn_train, knn_test = get_oof(knn, X_train, Y_train, X_test)\nrf_train, rf_test = get_oof(rf, X_train, Y_train, X_test)\ngbdt_train, gbdt_test=get_oof(gbdt, X_train, Y_train, X_test)\nbagging_train, bagging_test = get_oof(bagging,X_train, Y_train, X_test)\nabc_train, abc_test = get_oof(abc,X_train, Y_train, X_test)","execution_count":100,"outputs":[]},{"metadata":{"_uuid":"369b9745281918d03f0250c0869d7d961c2db94f","trusted":true},"cell_type":"code","source":"y_train_pred_stack = np.concatenate([lr_train, knn_train, rf_train, gbdt_train, bagging_train, abc_train], axis=1)\ny_train_stack = Y_train.reset_index(drop=True)\ny_test_pred_stack = np.concatenate([lr_test, knn_test, rf_test, gbdt_test, bagging_test, abc_test], axis=1)\n\ny_train_pred_stack.shape, y_train_stack.shape, y_test_pred_stack.shape","execution_count":101,"outputs":[]},{"metadata":{"_uuid":"d3ab6c6d9664d281f53b442d86dcf23f93714b51"},"cell_type":"markdown","source":"- y_train_pred_stack有891*6，这个6列分别是我们每个estimator在y_train上预测的值y值。\n- y_train_stack，是train上真实的y值。\n- y_test_pred_stack，是每个estimator在df_test上预测的y值，由于每个estimator将预测5次（K-Fold），我们需要将这5次的预测结果求平均，然后得出一个estimator的预测的y值，再把6个estimator再测试集上预测的y值组合起来。\n\n然后使用y_train_pred_stack(X_train)和y_train_stack(Y_train)再去做一个训练，进而得出最终的评分。然后再在y_test_pred_stack(X_test)预测最终结果。\n\n### 构建第二层Stacking"},{"metadata":{"_uuid":"243475b83df342fb3f81a6943a4cc10416314332","trusted":true},"cell_type":"code","source":"# params = {'learning_rate':[0.001, 0.01, 0.1, 1], 'n_estimators':range(100, 250, 20), 'max_depth':[2, 5], 'max_features':range(1, 3)}\n# clf = GradientBoostingClassifier(random_state=0)\n\n# params = {'C':[0.05, 0.08, 0.1, 0.2, 0.3]}\n# clf = LogisticRegression(random_state=0)\n\nparams = {'n_estimators':range(90, 150, 10)}\nclf = RandomForestClassifier(random_state=0)\n\ngscv_test= GridSearchCV(estimator=clf, param_grid=params, scoring='roc_auc', n_jobs=-1, cv=5, verbose=False)\ngscv_test.fit( y_train_pred_stack, y_train_stack)\ngscv_test.best_score_, gscv_test.best_params_","execution_count":102,"outputs":[]},{"metadata":{"_uuid":"ae87ad32e1cb4a99db77ceed4cfc2198a0c33998"},"cell_type":"markdown","source":"### 预测"},{"metadata":{"_uuid":"6c69b85e7ba6610ce5195fa19903594b5e4a7597","trusted":true,"collapsed":true},"cell_type":"code","source":"y_pred = RandomForestClassifier(random_state=0, n_estimators=100).fit(y_train_pred_stack, y_train_stack).predict(y_test_pred_stack)","execution_count":103,"outputs":[]},{"metadata":{"_uuid":"b17c4fded40eeef2a0753d8b0409e730f8e6f285"},"cell_type":"markdown","source":"该Stacking的cross validation 评分最终只有0.8363。我把预测结果与实际结果进行一对比，发现我使用Stacking训练所预测的结果，仍然有138行的是预测失败的，并且预测失败的case几乎是每个estimator都给出同样的结果，这表明，我们需要构造某些feature来分类出这些预测失败的case。这个很像是我们在Misclassification Analysis中所作的事情，我在那个环节并没有做好，构造的feature也没有什么卵用。Andrew Ng说要肉眼看一百行的数据，，，我这肉眼还没修炼成火眼金睛看来。"},{"metadata":{"_uuid":"d40888eb1a0872f4061eeae8aee3f7727b1de165","trusted":true},"cell_type":"code","source":"c = gscv_test.predict(y_train_pred_stack)\n\na = pd.DataFrame(y_train_pred_stack)\nb = pd.concat([a, pd.Series(c), y_train_stack], axis=1)\nb.columns = ['lr', 'knn', 'rf', 'gbdt', 'bagging', 'abc', 'predicted', 'Survived']\nb[b.predicted != b.Survived]","execution_count":125,"outputs":[]},{"metadata":{"_uuid":"974f5cdd0d4db49415670124e61050503a7ebaaa","collapsed":true},"cell_type":"markdown","source":"以上结果，Stacking只有0.8363的评分。所以最终，我们还是选择考虑使用获得最高评分的vote_soft来预测并输出结果。\n# 9. Predict"},{"metadata":{"_uuid":"8fdf48863087d9b77545fe3d8b3a784d90e69ad4","trusted":true,"collapsed":true},"cell_type":"code","source":"y_pred = vc_soft.fit(X_train, Y_train).predict(X_test)\nresult_df = pd.DataFrame({'PassengerId': X_test.index, 'Survived':y_pred}).set_index('PassengerId')\nresult_df.Survived = result_df.Survived.astype('int')\nresult_df.to_csv('predicted_survived.csv')","execution_count":118,"outputs":[]},{"metadata":{"_uuid":"3aa56b7df10876af38ece79ea8a2d945003eedc6","trusted":true},"cell_type":"code","source":"pd.read_csv('predicted_survived.csv').head()","execution_count":123,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ebf882b6b160d2969c6866c7f8445479303e8d00"},"cell_type":"code","source":"pd.read_csv('../input/gender_submission.csv').head()","execution_count":124,"outputs":[]},{"metadata":{"_uuid":"26086802c6f6fa6e7efe3d40307c56a59a271b4a"},"cell_type":"markdown","source":"# 格式一致，提交！\n# Right format，Submit！"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}