{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib \nimport matplotlib.pyplot as plt\nfrom IPython.display import HTML,SVG\nimport seaborn as sns\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\n# used to ignore warnings\nimport warnings  \nwarnings.filterwarnings('ignore')\n\n#Import Data set using the pandas read_csv Api.\ntitanic_train = pd.read_csv('../input/train.csv')\ntitanic_test = pd.read_csv('../input/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"print(titanic_train.head())\ntitanic_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1abd577bc525ee00c8a2e809c9fd94c976ae1a8b"},"cell_type":"code","source":"# Lets Remove unwanted columns that might be a integral part for the prediction.\ntitanic_train = titanic_train.drop(titanic_train.columns[[3,8]], axis=1)# The name column is not really going to help in predicting, so droping it from the dataframe.\ntitanic_train = titanic_train.drop(titanic_train.columns[8], axis = 1) # Removing the cabin column as it contains more nan than data.\n\n# Cleaning Test Data\ntitanic_test = titanic_test.drop(titanic_test.columns[[2,7]], axis=1)# The name column is not really going to help in predicting, so droping it from the dataframe.\ntitanic_test = titanic_test.drop(titanic_test.columns[7], axis = 1) # Removing the cabin column as it contains more nan than data.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0352ba0cdefde50974732d9ac7c86700969756ea"},"cell_type":"code","source":"# lets clean the data and convert strings to numerics, so that we can use it in our machine learning models.\nprint(titanic_test.count())\ntitanic = titanic_train.fillna(0) # droping na as Age has a lot of na's, will improvise in the next version by aproximately padding na's of Age. \ntitanic_test = titanic_test.fillna(0)\nprint(titanic_test.count())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e081fd94a5f4c1108ea288c2f1e70365cd100ea6"},"cell_type":"code","source":"# Converting strings to numeric data,so that our model can understand the data.\ntitanic = titanic.replace('S',1).replace('C',2).replace('Q',3) # Converting Embarked feature to numeric data.\ntitanic = titanic.replace('male',1).replace('female',2) # Converting Sex feature to numeric data.\n\n# Cleaning for test\ntitanic_test = titanic_test.replace('S',1).replace('C',2).replace('Q',3) # Converting Embarked feature to numeric data.\ntitanic_test = titanic_test.replace('male',1).replace('female',2) # Converting Sex feature to numeric data.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6e77fbcb5609aa82af61a4916dde40228695165f"},"cell_type":"code","source":"# Seperating the data into feature and class\ntitanic_train = titanic.drop(titanic_train.columns[1], axis=1)\ntitanic_class = titanic['Survived']\nprint(titanic_train.head())\nprint(titanic_class.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"488dd167aa9eb5502cb5715e55e1f49912738dc5"},"cell_type":"code","source":"# lets use train and split classifier from sklearn\n\nx_titanic_train,x_titanic_test,y_titanic_train,y_titanic_test = train_test_split(titanic_train,titanic_class,test_size = 0.3,random_state = 93)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"edf7a8d8de5a55f1fa55a3935c0b5a1e9532c558"},"cell_type":"code","source":"#Lets test the Acuracy and Prediction with SVM\nsvmclf = SVC(kernel='poly', degree=1)\nsvmclf.fit(x_titanic_train, y_titanic_train) \ny_titanic_pred = svmclf.predict(x_titanic_test)\nprint(accuracy_score(y_titanic_test, y_titanic_pred))\nprint(f1_score(y_titanic_test, y_titanic_pred, average='weighted'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0477403e688a651ddcddb1be61b3453aa98842dd"},"cell_type":"code","source":"# Lets try the Acuracy and Prediction score with Random Forest.\nrf = RandomForestClassifier(n_estimators=100, max_depth=4,random_state=0) # My testing shows depth 4 can give almost the best, there is no point increasing depth just for 2-3%.\nrf.fit(x_titanic_train, y_titanic_train) \ny_titanic_pred = rf.predict(x_titanic_test)\nprint(accuracy_score(y_titanic_test, y_titanic_pred))\nprint(f1_score(y_titanic_test, y_titanic_pred, average='weighted'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"39a9b5a2d8091af2c0fb2626862a8d9fb508992d"},"cell_type":"code","source":"# Lets try the Acuracy and Prediction score with Decision Tree.\ndtclf = DecisionTreeClassifier(random_state=0)\ndtclf.fit(x_titanic_train, y_titanic_train) \ny_titanic_pred = dtclf.predict(x_titanic_test)\nprint(accuracy_score(y_titanic_test, y_titanic_pred))\nprint(f1_score(y_titanic_test, y_titanic_pred, average='weighted'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"44d4d851bc12c78f08b41f8ec663a942b7b5513a"},"cell_type":"code","source":"# Lets try the Acuracy and Prediction score with K-nearest neighbor.\nkn = KNeighborsClassifier(n_neighbors=150)\nkn.fit(x_titanic_train, y_titanic_train) \ny_titanic_pred = kn.predict(x_titanic_test)\nprint(accuracy_score(y_titanic_test, y_titanic_pred))\nprint(f1_score(y_titanic_test, y_titanic_pred, average='weighted'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ceb734afb0a6210ce6c57b7e3fb9b3b2341cb2a5"},"cell_type":"code","source":"# Lets try the Acuracy and Prediction score with Neural Network.\nclf = MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto',beta_1=0.9, beta_2=0.999, early_stopping=False,epsilon=1e-08, hidden_layer_sizes=(5000, 200),learning_rate='constant', learning_rate_init=0.001,max_iter=200, momentum=0.9,nesterovs_momentum=True, power_t=0.5, random_state=1,shuffle=True, solver='lbfgs', tol=0.0001,validation_fraction=0.1, verbose=False, warm_start=False)\nclf.fit(x_titanic_train, y_titanic_train) \ny_titanic_pred = clf.predict(x_titanic_test)\nprint(accuracy_score(y_titanic_test, y_titanic_pred))\nprint(f1_score(y_titanic_test, y_titanic_pred, average='weighted'))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"19800f81fa5560125095535075b57342fbef1d7d"},"cell_type":"code","source":"y_titanic_pred = clf.predict(titanic_test)\nprint(y_titanic_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"244440ba75766c3e40abea53d8ed1835512ea54d"},"cell_type":"code","source":"Submission = pd.DataFrame({'PassengerId': titanic_test.PassengerId, 'Survived': y_titanic_pred})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"07ecb5bb2a2a0190a1ebc2057533785ad0ff1c78"},"cell_type":"code","source":"Submission.to_csv('Submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2476f4fbb956ea9a0bad26325aab820dfc1e0d3b"},"cell_type":"code","source":"Submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"914766799c123701fa02dd1ac8072eefcb8242b9"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}