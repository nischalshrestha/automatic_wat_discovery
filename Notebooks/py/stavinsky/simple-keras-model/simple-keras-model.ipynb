{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "c96cd5ba-d617-508a-b392-1fc1c7ba5a07"
      },
      "source": [
        "Hello. I'm very new to Kaggle. This is my first notebook. Here I'm trying to use MLP based on Keras python lib. \n",
        "Also as you can see all data preparation was done with sklearn Pipelines.  Fill free to comment and criticize.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9baa5955-f70b-9790-aa8c-e3298f9ded67"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.preprocessing import (\n",
        "    FunctionTransformer, OneHotEncoder, LabelEncoder, MinMaxScaler)\n",
        "from sklearn.base import TransformerMixin\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.cluster import MeanShift\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "class TransformWrapper(TransformerMixin):\n",
        "    \"\"\"simple wrapper for instruments like LabelEncoder.\n",
        "    It is usefull if we want to encode many features at one time. \n",
        "    They have a little bit different interface compairing to OneHotEncoder for example\"\"\"\n",
        "    def __init__(self, enc):\n",
        "        self.enc = enc\n",
        "    def fit(self, X, y=None):\n",
        "\n",
        "        self.d = {}\n",
        "        for col in X.columns:\n",
        "            self.d[col] = self.enc()\n",
        "            self.d[col].fit(X[col].values)\n",
        "        return self\n",
        "        \n",
        "    def transform(self, X, y=None):\n",
        "\n",
        "        l = list()\n",
        "        for col in X.columns:\n",
        "            l.append(self.d[col].transform(X[col].values).reshape(-1,1))\n",
        "        result = np.hstack(l)\n",
        "        return result\n",
        "\n",
        "\n",
        "def write_answer(y, output='answers.csv'):\n",
        "    answer = pd.DataFrame({\n",
        "        'PassengerId': test.PassengerId.values,\n",
        "        'Survived': y.reshape(-1)\n",
        "    })\n",
        "    answer.to_csv(output, index=False)\n",
        "    \n",
        "\n",
        "train = pd.read_csv(\"../input/train.csv\", index_col=\"PassengerId\")\n",
        "test = pd.read_csv(\"../input/test.csv\")\n",
        "target_column = \"Survived\"\n",
        "target = train[target_column]\n",
        "train = train.drop([\"Survived\"], axis=1)\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f9b8779a-9029-f40d-6876-4093cc2bc21f"
      },
      "outputs": [],
      "source": [
        "# fare is simple float value. We just need to fill NaN and scale values\n",
        "pipe_fare = Pipeline([\n",
        "    ('select', FunctionTransformer(\n",
        "        lambda X: X[[\"Fare\"]], validate=False)),\n",
        "    ('fillna', FunctionTransformer(\n",
        "        lambda X: X.fillna(0), validate=False)),\n",
        "    ('minmax', MinMaxScaler())\n",
        "])\n",
        "\n",
        "# SibSp and Parch is most like categorical features. So we will encode and binarize values. \n",
        "pipe_sibpa = Pipeline([\n",
        "    ('select', FunctionTransformer(\n",
        "        lambda X: X[[\"SibSp\",\"Parch\"]], validate=False)),\n",
        "    ('fillna', FunctionTransformer(\n",
        "        lambda X: X.fillna(0), validate=False)),\n",
        "    ('enc', TransformWrapper(LabelEncoder)),\n",
        "    (\"bin\", OneHotEncoder(sparse=False)),\n",
        "])\n",
        "\n",
        "\n",
        "# Embarked also categorical\n",
        "pipe_emb = Pipeline([\n",
        "    ('select', FunctionTransformer(\n",
        "        lambda X: X[\"Embarked\"], validate=False)),\n",
        "    ('fillna', FunctionTransformer(\n",
        "        lambda X: X.fillna('C'), validate=False)),\n",
        "    ('str', FunctionTransformer(\n",
        "        lambda X: pd.DataFrame(X.apply(str)), validate=False)),\n",
        "    ('enc', TransformWrapper(LabelEncoder)),\n",
        "    (\"bin\", OneHotEncoder(sparse=False)),\n",
        "])\n",
        "\n",
        "# categorical \n",
        "pipe_pclass = Pipeline([\n",
        "    ('select', FunctionTransformer(\n",
        "        lambda X: X[\"Pclass\"], validate=False)),\n",
        "    ('fillna', FunctionTransformer(\n",
        "        lambda X: X.fillna(3), validate=False)),\n",
        "    ('str', FunctionTransformer(\n",
        "        lambda X: pd.DataFrame(X.apply(str)), validate=False)),\n",
        "    ('enc', TransformWrapper(LabelEncoder)),\n",
        "    (\"bin\", OneHotEncoder(sparse=False)),\n",
        "])\n",
        "\n",
        "# I'm going to make categorical feature from Age by splitting this variable to 4 classes\n",
        "pipe_age = Pipeline([\n",
        "    ('select', FunctionTransformer(\n",
        "        lambda X: X[\"Age\"], validate=False)),\n",
        "    ('fillna', FunctionTransformer(\n",
        "        lambda X: X.fillna(0).values.reshape(-1,1), validate=False)),\n",
        "    ('make_cat', FunctionTransformer(\n",
        "        lambda x: np.where(\n",
        "            x==0, 0, np.where(\n",
        "                x<18, 1, np.where(\n",
        "                    x<55, 2, 3))).reshape(-1,1), validate=False)),\n",
        "    (\"bin\", OneHotEncoder(sparse=False)),\n",
        "])\n",
        "\n",
        "# i'm trying to extract first letter from this \n",
        "# feature but look's like it doesn't change anything\n",
        "pipe_cabin = Pipeline([\n",
        "    ('select', FunctionTransformer(\n",
        "        lambda X: X[\"Cabin\"], validate=False)),\n",
        "    ('first_letter', FunctionTransformer(\n",
        "        lambda x: x.str.extract(\" (?P<letter>[A-Z])\", expand=True), validate=False)),\n",
        "    ('fillna', FunctionTransformer(\n",
        "        lambda X: X.fillna('na'), validate=False)),\n",
        "    ('enc', TransformWrapper(LabelEncoder)),\n",
        "    (\"bin\", OneHotEncoder(sparse=False)),\n",
        "\n",
        "])\n",
        "pipe_title = Pipeline([\n",
        "    ('select', FunctionTransformer(\n",
        "        lambda X: X[\"Name\"], validate=False)),\n",
        "    ('first_letter', FunctionTransformer(\n",
        "        lambda x: x.str.extract(\"(?P<title>\\s\\w+\\.)\", expand=True), validate=False)),\n",
        "    \n",
        "    ('enc', TransformWrapper(LabelEncoder)),\n",
        "    (\"bin\", OneHotEncoder(sparse=False)),\n",
        "    \n",
        "])\n",
        "pipe_name_length = Pipeline([\n",
        "    ('select', FunctionTransformer(\n",
        "        lambda X: X[\"Name\"], validate=False)),\n",
        "    ('len', FunctionTransformer(\n",
        "        lambda X: X.apply(len), validate=False)),\n",
        "    ('make_cat', FunctionTransformer(\n",
        "        lambda x: np.where(\n",
        "            x<18, 0, np.where(\n",
        "                x<40, 1, 2)).reshape(-1,1), validate=False)),\n",
        "    (\"bin\", OneHotEncoder(sparse=False)),\n",
        "\n",
        "])\n",
        "pipe = FeatureUnion([\n",
        "    (\"age\", pipe_age),\n",
        "    (\"fare\", pipe_fare),\n",
        "    (\"emb\", pipe_emb),\n",
        "    (\"pclass\", pipe_pclass),\n",
        "    (\"sibpa\", pipe_sibpa),\n",
        "    (\"cabin\", pipe_cabin),\n",
        "    (\"title\", pipe_title),\n",
        "    (\"name_length\", pipe_name_length)\n",
        "])\n",
        "pipe.fit(pd.concat([train, test]))\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    pipe.transform(train), target, test_size=0.2, random_state=42)\n",
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ff14992a-41d5-f2fa-9045-901cd5b7d1a4"
      },
      "outputs": [],
      "source": [
        "def make_model(features):\n",
        "    \"\"\"We are going to make model with 2 layers with sizes: \n",
        "    1: number of features \n",
        "    2: 30\n",
        "    \n",
        "    Also we added Dropout layer, because it help to prevent fast overfitting\n",
        "    \"\"\"\n",
        "    adam = Adam(lr=0.001)\n",
        "    model = Sequential()\n",
        "    model.add(Dense(features.shape[1], input_shape=(features.shape[1], ), activation='linear' ))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(25, activation='sigmoid'))\n",
        "\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7160f13e-7b04-0148-0964-d85140308523"
      },
      "outputs": [],
      "source": [
        "model = make_model(X_train)\n",
        "\n",
        "early_stop = EarlyStopping(monitor=\"val_loss\", patience=1)\n",
        "history = model.fit(\n",
        "    X_train, y_train, nb_epoch=200, batch_size=10, validation_split=0.2,\n",
        "    callbacks=[early_stop], \n",
        "# I was trying to use EarlyStopping but for this moment it shows not very good result\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "predict = model.predict(X_test)\n",
        "predict = np.where(predict > 0.5, 1, 0)\n",
        "plt.plot(history.history[\"val_acc\"])\n",
        "plt.plot(history.history[\"val_loss\"])\n",
        "# It will be interesting to look at  the dynamics of loss and accuracy \n",
        "print(accuracy_score(y_test, predict))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4e3ddcc3-db53-a621-f1d6-857bcbaea149"
      },
      "outputs": [],
      "source": [
        "test_predict = model.predict(pipe.transform(test))\n",
        "test_predict = np.where(test_predict>0.5, 1, 0)\n",
        "write_answer(test_predict, \"mlp.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "461b4e5d-9e7f-467b-b884-4d65ee489534"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from collections import Counter\n",
        "clf = GradientBoostingClassifier(\n",
        "    learning_rate=0.1, n_estimators=200, \n",
        "    max_depth=3, min_samples_leaf=2, random_state=42\n",
        ")\n",
        "clf.fit(X_train, y_train)\n",
        "predict = clf.predict(X_test)\n",
        "accuracy_score(y_test, predict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7fd62dd9-a09a-067d-ab2b-4c41784e8b38"
      },
      "outputs": [],
      "source": [
        "clf_predict = clf.predict(pipe.transform(test))\n",
        "write_answer(clf_predict, \"grad_boost.csv\")"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}