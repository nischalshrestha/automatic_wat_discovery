{"cells":[{"metadata":{"_uuid":"0c6506cb83ce9703c1a0f970088a2d7873c7c6f2"},"cell_type":"markdown","source":"My 1st personal kernel on practising a Classification problem. My goal is to make it a weekly thing with various techniques and datasets. Your feedback and comments are much appreciated."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"110a2f48a47b9c2a93f272a074cc18b486cf330a"},"cell_type":"markdown","source":"Load train file and test file into Dataframe"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"training = pd.read_csv(\"../input/train.csv\")\ntesting = pd.read_csv(\"../input/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b5f646c1950f52066d88dad99797744c5511a6ba"},"cell_type":"markdown","source":"By using Dataframe.head(), we can glimpse some of the data in the Train and Test dataset"},{"metadata":{"trusted":true,"_uuid":"7c8466708e3a2c333ae67eda86a970f1f5b9a3e1"},"cell_type":"code","source":"training.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be5340df469e3c051e41ed2e43fdc295a9cd7de4"},"cell_type":"code","source":"testing.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d60982baf42e20f50e3fe18a09f04be34a5a1537"},"cell_type":"code","source":"training.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3f4484adf2a34380826efc972c0667f8c274e8ca"},"cell_type":"markdown","source":"After looking at the dataset, we need to refurbish the dataset to remove NA value"},{"metadata":{"trusted":true,"_uuid":"bfdc67ddffa26a2ef1f9011986047ed6d24f0bf9"},"cell_type":"code","source":"training.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d22a59a9016247ce616beb3271397d19b277cc5e"},"cell_type":"code","source":"testing.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"82325fbb158bf142cdf6bccadf4792952acc2b3b"},"cell_type":"markdown","source":"* Droppping columns for:\n    1. [Cabin] because the missing value is high.\n    2. [Ticket] because the data is not helping on making the predictions"},{"metadata":{"trusted":true,"_uuid":"ce26cdafe570e32d93da722cf5ee92ac09d540d7"},"cell_type":"code","source":"training.drop(labels=[\"Cabin\", \"Ticket\"], axis = 1 , inplace = True)\ntesting.drop(labels= [\"Cabin\", \"Ticket\"], axis = 1, inplace  = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bf78d42777ccf514ac38722bfc3e556fffd43cd1"},"cell_type":"markdown","source":"* We can check again with Dataframe.isna().sum() to confirm if [Cabin] and [Ticket] are removed from the Training and Testing datasets"},{"metadata":{"trusted":true,"_uuid":"83a8362b870163c70a035f73dc7daae7dc380eef"},"cell_type":"code","source":"training.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"422452a2a2fc1aea7a804b48b383115a97a84ead"},"cell_type":"code","source":"testing.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9408e9a0b1e4dd7e69eb4ab5d488af2b67fb041b"},"cell_type":"markdown","source":"* Fill the remaining empty values in columns [Age] and [Embarked] with medians for both Training and Testing datasets.\n* Combine SibSp and Parch because they are related as the Family size in which\n    * [SibSP]: related to sibling and spouse\n    * [Parch]: related to Parent, Child and some children travelled with a nanny.\n\n and we can analyze later if they help on our predictions"},{"metadata":{"trusted":true,"_uuid":"87d5db9ce8d7334a7f3a504248c7c93b0df074e3"},"cell_type":"code","source":"copy = training.copy()\ncopy.dropna(inplace = True)\nsns.distplot(copy[\"Age\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"59e90b9d822dc9711c88debb12e577d50432e0e6"},"cell_type":"code","source":"training[\"Age\"].fillna(training[\"Age\"].median(), inplace = True)\ntesting[\"Age\"].fillna(training[\"Age\"].median(), inplace = True)\ntraining[\"Embarked\"].fillna(\"S\", inplace = True)\ntesting[\"Fare\"].fillna(testing[\"Fare\"].median(), inplace = True)\ntraining[\"Family_Size\"] = training[\"SibSp\"] + training[\"Parch\"] + 1\ntesting[\"Family_Size\"] = testing[\"SibSp\"] + testing[\"Parch\"] + 1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d42b592cf95ef1b9c7af4dec9c17b52155e93e4a"},"cell_type":"markdown","source":"After replacing NA with median(), the column [Age] is more normally distributed."},{"metadata":{"trusted":true,"_uuid":"294c30080dbe7cbfa9b28503dc643d41888d4e06"},"cell_type":"code","source":"sns.distplot(training[\"Age\"])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"46fb5bbc76fbca7e814d2f58993f88f94faa0777"},"cell_type":"markdown","source":"Perhaps the individual title may help on the predictions Hence, we can add column [Title] from column [Name] for Training dataset"},{"metadata":{"trusted":true,"_uuid":"82dba602763727a64023a2168ffa8dc0d3cd8a93"},"cell_type":"code","source":"for name in training[\"Name\"]:\n    training[\"Title\"] = training[\"Name\"].str.extract(\"([A-Z a-z]+)\\.\", expand= True)\n\ncount = 0\nfor i in list(training[\"Title\"]):\n    if not i in [\" Mr\", \" Mrs\", \" Miss\", \" Master\"]:\n        training.at[count, \"Title\"] = \" Other\"\n    count += 1\n\ntitle_list = list(training[\"Title\"])\nfrequency_title = {}\nfor i in title_list:\n    frequency_title.update({i :(title_list.count(i))})\nfrequency_title","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cd43c2d1a377f65b5d82283fed6f885f0f0ab07c"},"cell_type":"markdown","source":"We then add the  column [Title] from Column [Name] for Testing dataset"},{"metadata":{"trusted":true,"_uuid":"3e425126267a3930b71bda48a7205e738e2cbbff"},"cell_type":"code","source":"for name in testing[\"Name\"]:\n    testing[\"Title\"] = testing[\"Name\"].str.extract(\"([A-Z a-z]+)\\.\", expand= True)\n    \ncount = 0\nfor i in list(testing[\"Title\"]):\n    if not i in [\" Mr\", \" Mrs\", \" Miss\", \" Master\"]:\n        testing.at[count, \"Title\"] = \" Other\"\n    count += 1\n\ntitle_list = list(testing[\"Title\"])\nfrequency_title = {}\nfor i in title_list:\n    frequency_title.update({i :(title_list.count(i))})\nfrequency_title","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b5bc6cc445546daebdaf3a8096dabe201f2678dc"},"cell_type":"markdown","source":"We can validate again with Dataframe.isna().sum() if there is any missing values appear for both Training and Testing datasets"},{"metadata":{"trusted":true,"_uuid":"ff8005481e7b4c8fac7c22b833fb1099633e42d7"},"cell_type":"code","source":"training.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8f0a9b27321a3cce1f92108b94a42ac50c4e7a08"},"cell_type":"code","source":"testing.isna().sum() ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a1c9d2bb869ec13c78b0948a17ead27946d01f31"},"cell_type":"markdown","source":"* Pairplot is good to display relationship between variables.\n* Since Family_Size is from Parch and SibSp, there is a relationship that can be found from those graphs."},{"metadata":{"trusted":true,"_uuid":"68ff6c480c36d3f8ab4f6e744285b71add24345a"},"cell_type":"code","source":"sns.pairplot(training)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b9469779b13584b1149e46e1fe427dfa0debed42"},"cell_type":"markdown","source":"From the heatmap, there seems to be a  correlation between Survived and Fare"},{"metadata":{"trusted":true,"_uuid":"ab8a1a8fe81db463ea6711bc86cbfae9e042525b"},"cell_type":"code","source":"sns.heatmap(training.corr(), annot= True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"17b753747aafcbf06359c2d1731dca2f913859c8"},"cell_type":"markdown","source":"* Next, we use pd.get_dummies to convert the categorical variables into dummy/indicator variables. The categorical variables that we can identify from the datasets are:\n    * Sex\n    * Embarked\n    * Title\n    * Pclass"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"35693bbef1351271d06df255892fa1c9569a1e4b"},"cell_type":"code","source":"train_enc = pd.get_dummies(training, columns = [\"Sex\",\"Embarked\", \"Title\", \"Pclass\"])\ntest_enc = pd.get_dummies(testing, columns = [\"Sex\", \"Embarked\", \"Title\", \"Pclass\"])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e4608b7ebbed4f3cb94c452c3256904e7a2c5c6b"},"cell_type":"markdown","source":"* We can load MinMaxScaler from sklearn.preprocessing to downscale the weights of numerical variables between 0 and 1 for Training and Testing datasets. The numerical values that are identified as below.\n    * Age\n    * Fare\n    * SibSp\n    * Parch\n    * Family_Size"},{"metadata":{"trusted":true,"_uuid":"1aaa7368bb9cfba58b72523ddc3d814e909ebc4d"},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscale = MinMaxScaler()\ntrain_enc[[\"Age\", \"Fare\", \"SibSp\",\"Parch\", \"Family_Size\"]] = scale.fit_transform(train_enc[[\"Age\", \"Fare\", \"SibSp\",\"Parch\", \"Family_Size\"]].as_matrix())\ntest_enc[[\"Age\", \"Fare\", \"SibSp\",\"Parch\", \"Family_Size\"]] = scale.fit_transform(test_enc[[\"Age\", \"Fare\", \"SibSp\",\"Parch\", \"Family_Size\"]].as_matrix())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ab64428ff9bb24f84535f79e0ac7427e0949a6a1"},"cell_type":"markdown","source":"* Before moving to Validation and Training of the model. We remove column [PassengerId], [Name] and [Sex_ male] from both Training and Testing datasets. \n    * The reason being is that, we are not required to know the passengers and their names. \n    * For the Sex_ male, Sex_ female column would be sufficient enough to distinguish the only 2 categories (Male and Female).\n    \n* Furthermore, we remove column [Survived] from the X_train \n    * because that is the one we are doing for our prediction. Hence, we would create a new dataframe Y_train for the column [Survived]"},{"metadata":{"trusted":true,"_uuid":"94654d26d1ae7751fb243e1e49ce290db06bc735"},"cell_type":"code","source":"X_train = train_enc.drop(labels = [\"PassengerId\", \"Survived\", \"Name\", \"Sex_male\"], axis = 1)\nY_train = train_enc[\"Survived\"]\nX_test = test_enc.drop(labels = [\"PassengerId\", \"Name\", \"Sex_male\"], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e7d60eec7f324b24398c12f37d55fb254ae64d87"},"cell_type":"markdown","source":"* We need to further split the Training dataset into Training and Validation datasets because we need a subset of our Training dataset to validate on our model results. \n    * The Testing set cannot be used for our validation of the model when the predicted column [Survived] is not provided.\n\n* Thus, we would split into Training and Validation datasets with train_test_split from sklearn.model_selection. \n    * Then, we would specify on what is the size of the Validation set in which we have set as 20%\n    * The random_state is set to 0 (or any other numerical value of your choice) so we can reproduce the same results for every run\n    \n* After splitting, we need to balance the Training dataset with enough positive and negative samples. Hence, we would attempt to oversample the minority with SMOTE from imblearn.over_sampling"},{"metadata":{"trusted":true,"_uuid":"fbe34cb6822edfed0c9b1057d0165b36adc5243f"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTE\nX_training, X_valid, y_training, y_valid = train_test_split(X_train, Y_train, test_size = 0.2, random_state = 0)\nsm = SMOTE(random_state = 2)\nX_train, Y_train = sm.fit_sample(X_train, Y_train.ravel())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eb3158b298a9cd75ee3e3beb55761c7f5b581b20"},"cell_type":"markdown","source":"We would import \n* GridSearchCV\n    * It is used for exhaustive search over specified parameter values for an estimator. In other words, it can provide the most optimized parameter from a set of parameters provided.\n* maker_scorer\n    * This factory function wraps scoring functions for use in GridSearchCV and cross_val_score.\n* accuracy_score\n    * Accuracy classification score that we are using for the Titanic dataset\n* XGBClassifier, RandomForestClassifier, DecisionTreeClassifier \n    * Those are the models we would use for our Training and Validation\n* Cross_val_score\n    * To estimate the expected accuracy of the model.\n* confusion_matrix\n    * Provide more presentable results by displaying TP, FP, TN,  FN    "},{"metadata":{"trusted":true,"_uuid":"4c4a2a622c42665dea9d661618c755e5d50145ba"},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer, accuracy_score\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cd55b3042b8efe5001d4a0df7175d74c985b3823"},"cell_type":"code","source":"model_xgb = XGBClassifier() #Identify the model for training\nparam_xgb = {\"objective\": [\"reg:linear\"], \"n_estimators\": [5, 10, 15, 20]} #Listing the parameters to find the best parameters \ngrid_xgb = GridSearchCV(model_xgb, param_xgb, scoring = make_scorer(accuracy_score)) #Setup the function to find the best parameters\ngrid_xgb.fit(X_training, y_training) #To find the best parameters for the model\nmodel_xgb = grid_xgb.best_estimator_ #Select the best parameters for the model\nmodel_xgb.fit(X_training, y_training) #Fitting the best estimator parameters for Training\npred_xgb = model_xgb.predict(X_valid) #Predict the trained model with validation dataset\nacc_xgb = accuracy_score(y_valid, pred_xgb) #Find the accuracy of the model\nscores_xgb = cross_val_score(model_xgb, X_training, y_training, cv = 5) #Check on the cross validation score of the dataset with 5 splits\nprint(\"Best parameters: {} \\nCross Validation Score: {} (+/- {} \\nBest prediction accuracy: {})\".format(model_xgb, scores_xgb.mean(), scores_xgb.std(), acc_xgb))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9d0282831f7ce14cd684bda2f2ad5364c8942916"},"cell_type":"code","source":"cm_xgb = confusion_matrix(y_valid, pred_xgb)\ncm_df = pd.DataFrame(cm_xgb, index = [\"Not Survived\", \"Survived\"], columns = [\"Not Survived\", \"Survived\"])\nsns.heatmap(cm_df, annot= True)\nplt.title(\"XGB Classifier \\nAccuracy: {0:.3f}\".format(acc_xgb))\nplt.ylabel(\"True Label\")\nplt.xlabel(\"Predicted Label\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"569cad1e4d13715d84b4dddf4847f4701aa38b05"},"cell_type":"code","source":"sns.barplot(model_xgb.feature_importances_, X_training.columns)\nplt.title(\"Feature Importance for XGB Classifier\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d6fe2fe8609e2e9c102dd05af7c42e6b9f7da6ab"},"cell_type":"code","source":"model_rfc = RandomForestClassifier() #Identify the model for training\nparam_rfc = {\"n_estimators\" : [4, 5, 6, 7, 8, 9, 10, 15], \"criterion\" : [\"gini\", \"entropy\"], \"max_features\": [\"auto\", \"sqrt\", \"log2\"],\n            \"max_depth\": [2, 3, 5, 10], \"min_samples_split\": [2, 3, 5, 10]} #Listing the parameters to find the best parameters \ngrid_rfc = GridSearchCV(model_rfc, param_rfc, scoring = make_scorer(accuracy_score)) #Setup the function to find the best parameters\ngrid_rfc.fit(X_training, y_training) #To find the best parameters for the model \nmodel_rfc = grid_rfc.best_estimator_ #Select the best parameters for the model\nmodel_rfc.fit(X_training, y_training) #Fitting the best estimator parameters for Training\npred_rfc = model_rfc.predict(X_valid) #Predict the trained model with validation dataset\nacc_rfc = accuracy_score(y_valid, pred_rfc) #Find the accuracy of the model\nscores_rfc = cross_val_score(model_rfc, X_training, y_training, cv = 5) #Check on the cross validation score of the dataset with 5 splits\nprint(\"Best parameters: {} \\nCross Validation Score: {} (+/- {}) \\nBest prediction accuracy: {}\".format(model_rfc, scores_rfc.mean(), scores_rfc.std(), acc_rfc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"569d311ca5d09a921b10d1cc5a5fa16011880bd1"},"cell_type":"code","source":"cm_rfc = confusion_matrix(y_valid, pred_rfc)\ncm_df = pd.DataFrame(cm_rfc, index = [\"Not Survived\", \"Survived\"], columns = [\"Not Survived\", \"Survived\"])\nsns.heatmap(cm_df, annot = True)\nplt.title(\"Random Forest Classifier \\nAccuracy: {0:.3f}\".format(acc_rfc))\nplt.ylabel(\"True Label\")\nplt.xlabel(\"Predicted Label\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"62fdfbd4f9a0c3b0ca698c251cb1189ad89517e0"},"cell_type":"code","source":"sns.barplot(model_rfc.feature_importances_, X_training.columns)\nplt.title(\"Feature Importance for Random Forest Classifier\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f76159a9ef0596b22034ddf0fc440ec3dfa85e59"},"cell_type":"code","source":"model_dtc = DecisionTreeClassifier() #Identify the model for training\nparam_dtc = {\"criterion\": [\"gini\", \"entropy\"], \"splitter\": [\"best\",\"random\"], \"max_features\": [\"auto\", \"sqrt\", \"log2\"]} #Listing the parameters to find the best parameters \ngrid_dtc = GridSearchCV(model_dtc, param_dtc, scoring = make_scorer(accuracy_score)) #Setup the function to find the best parameters\ngrid_dtc.fit(X_training, y_training) #To find the best parameters for the model \nmodel_dtc = grid_dtc.best_estimator_ #Select the best parameters for the model\nmodel_dtc.fit(X_training, y_training) #Fitting the best estimator parameters for Training\npred_dtc = model_dtc.predict(X_valid) #Predict the trained model with validation dataset\nacc_dtc = accuracy_score(y_valid, pred_dtc) #Find the accuracy of the model\nscores_dtc = cross_val_score(model_dtc, X_training, y_training, cv = 5) #Check on the cross validation score of the dataset with 5 splits\nprint(\"Best parameters: {} \\nCross Validation Score: {} (+/- {}) \\nBest prediction accuracy: {} \".format(model_dtc, scores_dtc.mean(), scores_dtc.std(), acc_dtc ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ede9ca29d82fedfae6a8e008e6a4e33f87e1e10d"},"cell_type":"code","source":"cm_dtc = confusion_matrix(y_valid, pred_dtc)\ncm_df = pd.DataFrame(cm_dtc, index = [\"Not Survived\", \"Survived\"], columns = [\"Not Survived\", \"Survived\"])\nsns.heatmap(cm_df, annot = True)\nplt.title(\"Decision Tree Classifier \\nAccuracy: {0:.3f}\".format(acc_dtc))\nplt.ylabel(\"True Label\")\nplt.xlabel(\"Predicted Label\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be6eab5b0216b4acdda2d150556107c3b7f9be30"},"cell_type":"code","source":"sns.barplot(model_dtc.feature_importances_, X_training.columns)\nplt.title(\"Feature Importance for Decision Tree Classifier\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a79eddc7aeefeb4d959370a84fd3cb32ec8c7b85"},"cell_type":"markdown","source":"We are going to submit the best model which is random forest classifier since the prediction is the highest among the 3."},{"metadata":{"trusted":true,"_uuid":"0869d410da511556921e682fad6c47f0134c48e8"},"cell_type":"code","source":"model_rfc.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"74957b01cef63886a8c460da9bf56352441593b2"},"cell_type":"code","source":"submission_predictions = model_rfc.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"23e2d4f6022f6be345c2360d179574c8525b938e"},"cell_type":"code","source":"submission = pd.DataFrame({\n    \"PassengerId\": testing[\"PassengerId\"],\n    \"Survived\": submission_predictions\n})\n\nsubmission.to_csv(\"titanic.csv\", index = False)\nprint(submission.shape)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}