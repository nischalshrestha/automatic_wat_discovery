{"cells":[{"metadata":{"_cell_guid":"5bad9068-d72c-453b-943c-92aa36f71568","_uuid":"ef4ad9430e60f8dcf673076147bed5c08502b2ff"},"cell_type":"markdown","source":"# Super Simple Titanic Quickstart\n\nA super short, fast, and minimalistic notebook that compares a few different models using cross-validation. Finally, chooses the best (logistic regression).\n\nUses only four features: Pclass, Age, Sex, Familysize (based on analysis insights from other notebooks, e.g., [this outstanding notebook](https://www.kaggle.com/yassineghouzam/titanic-top-4-with-ensemble-modeling/notebook)). No feature engineering, no hyper-paramter tuning, no fancy stuff but already surprisingly good.\n\n**If you liked the notebook or have feedback, I'd appreciate upvotes and any comments!**\n\nEdit: Added new CatBoost regressor, which - contrary to their claim - seems to take much longer than XGBoost and still perform worse (at least here, in terms of MAE with default parameters)."},{"metadata":{"_cell_guid":"319463f4-3ae6-4cc4-8a91-20b8b52d1a3d","_uuid":"03109b1c3be6da40e519b48ce9ac535218679784","collapsed":true,"trusted":true},"cell_type":"code","source":"# imports\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.pipeline import make_pipeline\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\n\n\n# read data, split into train, validate, test\ntrain_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')","execution_count":16,"outputs":[]},{"metadata":{"_cell_guid":"c52242fc-000a-48cc-b07b-0ee28d53aacd","_uuid":"0e06762c091340431f9dd7c3d01782d64165f698","trusted":true},"cell_type":"code","source":"# prepare data (outside of pipeline)\n# convert sex into binary male feature\ntrain_df[\"Male\"] = train_df.Sex.map({\"male\": 1, \"female\": 0}).astype(int)\ntest_df[\"Male\"] = test_df.Sex.map({\"male\": 1, \"female\": 0}).astype(int)\n\n# calculate family size based on siblings and parents\ntrain_df[\"FamilySize\"] = train_df.SibSp + train_df.Parch + 1\ntest_df[\"FamilySize\"] = test_df.SibSp + train_df.Parch + 1\n\n# drop \"unnecessary features\" (keep id in test set)\ntrain_df = train_df.drop([\"PassengerId\", \"Ticket\", \"Cabin\", \"Name\", \"Sex\", \"SibSp\", \"Parch\", \"Embarked\", \"Fare\"], axis=1)\ntest_df = test_df.drop([\"Ticket\", \"Cabin\", \"Name\", \"Sex\", \"SibSp\", \"Parch\", \"Embarked\", \"Fare\"], axis=1)\n\n# assign features X and target value y\nX = train_df.drop(\"Survived\", axis=1).fillna(np.nan)\ny = train_df.Survived\nX_test = test_df.drop(\"PassengerId\", axis=1).fillna(np.nan)\n\n\nX.tail()","execution_count":17,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"026a40e04c5c8289270fc24593f1bf125869509b"},"cell_type":"code","source":"# fill missing Age values through imputation\ncombined = pd.concat([X, X_test])\nimp = Imputer()\nimp_df = pd.DataFrame(imp.fit_transform(combined))\nimp_df.columns = combined.columns\nimp_df.index = combined.index\ncombined[\"Age\"] = imp_df[\"Age\"]\nprint(combined.isnull().sum())\n\nX = combined[:len(X)]\nX_test = combined[len(X):]\nX.tail()","execution_count":18,"outputs":[]},{"metadata":{"_cell_guid":"1689fe83-b5f8-4edb-ae88-31c49ebe0531","_uuid":"85c3e10ab4a45d3f857a4633250be103157e6b51","trusted":true},"cell_type":"code","source":"# compare pipelines with different models through cross-validation\n# logistic regression\nlog_pipeline = make_pipeline(LogisticRegression())\nscores = cross_val_score(log_pipeline, X, y, scoring=\"neg_mean_absolute_error\")\nprint('Logistic regression MAE: \\t%2f' %(-1 * scores.mean()))\n\n# random forest\nrf_pipeline = make_pipeline(RandomForestRegressor())\nscores = cross_val_score(rf_pipeline, X, y, scoring=\"neg_mean_absolute_error\")\nprint('Random forest MAE: \\t\\t%2f' %(-1 * scores.mean()))\n\n# XGBoost\nxgb_pipeline = make_pipeline(XGBRegressor())\nscores = cross_val_score(xgb_pipeline, X, y, scoring=\"neg_mean_absolute_error\")\nprint('XGBoost MAE: \\t\\t\\t%2f' %(-1 * scores.mean()))\n\n# CatBoost\ncat_pipeline = make_pipeline(CatBoostRegressor(verbose=False))\nscores = cross_val_score(cat_pipeline, X, y, scoring=\"neg_mean_absolute_error\")\nprint('CatBoost MAE: \\t\\t\\t%2f' %(-1 * scores.mean()))\n\n# naive Bayes\ngnb_pipeline = make_pipeline(GaussianNB())\nscores = cross_val_score(gnb_pipeline, X, y, scoring=\"neg_mean_absolute_error\")\nprint('Naive Bayes MAE: \\t\\t%2f' %(-1 * scores.mean()))","execution_count":19,"outputs":[]},{"metadata":{"_cell_guid":"ecae5628-21f7-43d7-ab95-f93d3c56cb88","_uuid":"0abd879bc9fa88ba94c17aa4ddf895401548ea56","trusted":true},"cell_type":"code","source":"# use log. regression for submission (most successful)\nlog_pipeline.fit(X, y)\nprint(\"Accuracy: {}\".format(cross_val_score(log_pipeline, X, y, scoring=\"accuracy\").mean()))\ny_pred = log_pipeline.predict(X_test)\n\nresult = pd.DataFrame()\nresult[\"PassengerId\"] = test_df[\"PassengerId\"].astype(\"int\")\nresult[\"Survived\"] = y_pred.astype(\"int\")\n\nresult.to_csv(\"predicted_survival.csv\", index=False)\nresult.head()","execution_count":20,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}