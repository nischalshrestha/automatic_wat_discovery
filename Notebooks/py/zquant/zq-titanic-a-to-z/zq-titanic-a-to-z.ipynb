{"cells":[{"metadata":{"_uuid":"8db60ddc4a93478327c5045028494fc678605e03","_cell_guid":"7fcf2bbd-7ae5-42dd-8ca5-87058ac9288b"},"cell_type":"markdown","source":"<h2> Titanic: Machine Learning from Disaster </h2>\n\n<p> In this Kernel, we explore the Titanic dataset and look to divide those that are statistically favourable to survive amongst all travelers. In the following report, we cover;\n        <ol>\n            <li>Import/Split the data </li>\n            <li>Data Visualation & Interpretation </li>\n            <li>Data Cleaning & Filtering </li>\n            <li>Feature Engineering/Selection </li>\n            <li>Model Selection & Performance </li>\n            <li>Out of Sample Tests </li>\n            <li> Conclusion </li>\n        </ol>\n        \n   <i>All comments and suggestions are welcomed and appreciated! Enjoy :-D</i>"},{"metadata":{"_uuid":"039d8d630f4df264944549ad53fe9d7e6e0894f1","_cell_guid":"83ef0eeb-4899-49c2-a325-8260d5515245","trusted":false,"collapsed":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n#Preprocessing\nfrom sklearn.preprocessing import Imputer, StandardScaler, LabelEncoder\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scikitplot as skplt\n#Models & Metrics\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.cross_validation import cross_val_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import KFold\n\nimport keras\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\n\nfrom sklearn.model_selection import GridSearchCV\n%matplotlib inline\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"c22415e863013d8e4d7f0951a0580f1e25bebd7b","_cell_guid":"866c257a-85e8-45ed-a079-f9dd4abd72d1"},"cell_type":"markdown","source":"<h1> 1. Import/Split the data </h1>\n  \n<p>We want to immediately split the dataset into testing and training data. It is good practice to set the indexing as 'PassengerId'.</p>"},{"metadata":{"collapsed":true,"_uuid":"fde1c52146003fecf99038b1b022d5f0fd82b860","_cell_guid":"b5b54978-9b4f-46d6-bb48-c46916b7f335","trusted":false},"cell_type":"code","source":"dataset = pd.read_csv('../input/train.csv').set_index('PassengerId', drop=True)\ntestset = pd.read_csv('../input/test.csv').set_index('PassengerId', drop=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3c723279f80d0ae7acd679c1b34574f604e2c8c3","_cell_guid":"ef4463ce-377c-48fe-9277-42d9a11cfe1f","trusted":false,"collapsed":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"507c21b674c6e0845ddaac5ecc5649e4a4201f51","scrolled":true,"_cell_guid":"9ee99841-5f95-47f7-b1c9-185995980a40","trusted":false,"collapsed":true},"cell_type":"code","source":"testset.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2a03441e3212e9c31e1aa02dea56db38d7a81b9f","_cell_guid":"8cd8909a-0dd2-44d8-a9f0-32a520a6b5c2"},"cell_type":"markdown","source":"<p> From the dataset previews above, we gain an understanding of the features we have in our dataset and the kind of features they are. Let's keep it clean and set the feature types appropriately. </p>"},{"metadata":{"_uuid":"f12cbea47747797d100927473befb8848b097284","_cell_guid":"4d21eda1-7c34-4124-b753-aa91990d30ea","trusted":false,"collapsed":true},"cell_type":"code","source":"dataset.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7dbdb43ebd19a2a9cceee8777579c9fc11659472","_cell_guid":"d471da56-9efa-4ce8-9271-06437723573a","trusted":false,"collapsed":true},"cell_type":"code","source":"dataset.Sex = dataset.Sex.astype('category')\ntestset.Sex = testset.Sex.astype('category')\n\ndataset.Embarked = dataset.Embarked.astype('category')\ntestset.Embarked = testset.Embarked.astype('category')\n\ndataset.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2b1a84ae2ccfbefb59e99ad0823e64731b1f507f","_cell_guid":"fcb652ae-37ba-44ea-a712-f1f80e312669"},"cell_type":"markdown","source":"<p> Immediately, we see that Age and Cabin have less rows than others. We should check these for NaN\n values. </p>\n \n <h2>a. What we dont know about the data </h2>"},{"metadata":{"_uuid":"e90fe0adc548378e62beaf8cd06f207222675ea0","_cell_guid":"21d1ca6b-f3aa-4c0d-a10a-f6d019bb2cbc","trusted":false,"collapsed":true},"cell_type":"code","source":"f, ax = plt.subplots(1, 2, figsize=(14, 8))\nsns.heatmap(dataset.isnull(), cbar=False, cmap='viridis', ax=ax[0])\nsns.heatmap(testset.isnull(), cbar=False, cmap='viridis', ax=ax[1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d7784193e3d77e48b6cca30696135c776005c3dd","_cell_guid":"95deb7d9-c033-4298-97b7-a656d84a18f9","trusted":false,"collapsed":true},"cell_type":"code","source":"print('NaN Value Counts\\n\\nTraining Set:\\n%s\\n\\nTesting Set:\\n%s' % \\\n      (dataset.isnull().sum(), testset.isnull().sum()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f476676b84a7f235fbff811bfa0f46959afa3769","_cell_guid":"f59259e9-6425-44b5-bcd4-ce439e5447db"},"cell_type":"markdown","source":"<p> We'll need to do some hole plugging in our datasets seeing as there are missing values in both the training & testing portions. The heatmap and counts above help us understand the missing pieces of data. Columns like Cabin have too many NaN values to be able to fill the rest with anything accurate. It may be wise to drop this column all together. </p>\n\n<h2> b. What we do know about the data </h>\n\n<p> <br>We'll start by looking at the class imbalance between Didn't Survive(0) and Survived(1).</p>"},{"metadata":{"_uuid":"11470823ad88106c1393d3f24150126cb6933860","_cell_guid":"79dc088f-f875-45c6-bdee-0afdab6ec3b0","trusted":false,"collapsed":true},"cell_type":"code","source":"train_deceased = dataset[dataset.Survived==0]\ntrain_survived = dataset[dataset.Survived==1]\nlabels = 'Deceased', 'Survived'\nsizes = [train_deceased.shape[0], train_survived.shape[0]]\ncolors = ['lightcoral', 'lightskyblue']\nexplode = (0.05, 0)  # explode 1st slice\n \nfig = plt.figure(figsize=(14, 8))\nplt.pie(sizes, explode=explode, labels=labels, colors=colors,\n        autopct='%1.1f%%', shadow=True, startangle=140)\nplt.title('Deceased vs Survived Class Imbalance')\n \nplt.axis('equal')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"edcb981c69ff3fbc63efa45a40f380d1f8c55940","_cell_guid":"86b3ffc5-4d20-409b-8426-099512fa8ccb"},"cell_type":"markdown","source":"<p> We'll have to adjust our models to account for the class imbalance seen in the training set. These proportions can be used as the prior probability of classes. We hope for the testing set to have a similar probability of occurence so that our model generalizes well out of sample. \n<br><br> Next, we look at the individual features and their correspondance to the classes. These plots should give us an understanding of a certain features ability to predict if the person survived. </p>"},{"metadata":{"_uuid":"94d48a9fb3f8193a223df00b4a6817ec10cd9737","_cell_guid":"3f303a15-b2b6-47c2-9575-0b68261d6e66","trusted":false,"collapsed":true},"cell_type":"code","source":"fig = plt.figure(figsize=(24, 15))\n\nplt.subplot(331)\nsns.distplot(train_deceased.Age.dropna(), kde=False, color='Red')\nsns.distplot(train_survived.Age.dropna(), kde=False, color='Blue')\nplt.title('Survival Age Distribution')\nplt.ylabel('Frequency')\n\nplt.subplot(332)\nsns.distplot(train_survived.Fare, kde=False, color='Blue')\nsns.distplot(train_deceased.Fare, kde=False, color='Red')\nplt.title('Survival Fare Distribution')\nplt.ylabel('Frequency')\nplt.xlim(0, 200)\n\nfig, ax = plt.subplots(1, 3, figsize=(14, 8))\n\ng = sns.factorplot(x='Embarked', kind='count', data=dataset, hue='Survived', ax=ax[0], legend=False)\ng.ax.set_axis_off()\nax[0].set_title('Embarkment Location Survival')\n\ng = sns.factorplot(x='Pclass', kind='count', data=dataset, hue='Survived', ax=ax[1], legend=False)\ng.ax.set_axis_off()\nax[1].set_title('Ticket Class Survival')\n\ng = sns.factorplot(x='Sex', kind='count', data=dataset, hue='Survived', ax=ax[2], legend=False)\ng.ax.set_axis_off()\nax[2].set_title('Sex Survival')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cbaa51232c45b9c42d03d9383fc25a4812c91380","_cell_guid":"40f1edfb-2498-48a3-ab84-a7899e74be59","trusted":false,"collapsed":true},"cell_type":"code","source":"sns.factorplot(x='Parch', kind='count', hue='Survived', data=dataset, orient='v')\nplt.title('Parent/Children Survival')\n\nsns.factorplot(x='SibSp', kind='count', hue='Survived', data=dataset, orient='v')\nplt.title('Sibling/Spouse Survival')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dea25973ef927bd1732f31e661c0d98576b9435a","_cell_guid":"4c31863f-960a-42e3-aca7-b3c346d22647"},"cell_type":"markdown","source":"<p> In our analysis of individual features benchmarked against their classes, we will use the class imabalance exhibited as an indication of features that have predictive power. Features that show a more balanced or inversed class imbalance than shown above (61% deceased vs 39% survived), are indicators of predictive power. <i>Note** Blue represents Survived where Red represents deceases.\n<br><br>\n    <ol> \n        <li><b>Age</b></li>\n                    <p><i> A change in imbalance is seen in the young age category. The largest discrepancy is in children younger than 5 years old. The second largest discrepancy is seen in the 10-15 year old category. </p>\n         <li><b>Fare</b></li>\n                     <p><i> The fare distribution is hard to analyze because of how oddly distributed it is. We cannot draw much of a conclusion from its distribution. Later, we will transform the distribution to get something more gaussian in nature.</i></p>\n         <li><b>Embarkment Location</b></li>\n                     <p><i> The Cherbourg location shows a higher survival rate than other locations.</i></p>\n         <li><b>Ticket Class</b></li>\n         <p><i>The most interesting and obvious of the features, higher ticket classes show a smaller chance of survival. Intuitively, those that are closer to the ground floors may have had better access to the safety boats on the Titanic. It is a known problem that the Titanic was not equipped with enough safety boats. Implying this logic, the imablance for 3rd class ticket holders is to no surprise. </i></p>\n         <li><b>Sex</b></li>\n         <p><i>We see here that the females are those with the biggest survival rate. This seems to be true to do \"the mans responsibility of protecting his wife and children\". Following this adage, men were last on the list of priorities.</i> </p>\n         <li><b>Family Members</b></li>\n         <p><i>We can combine the last two metrics in the same analysis. These two metrics give us a look at how many loved ones or family members they had on board. It looks like those who traveled alone or with many family members had a low chance of survival where those with few family members had a higher than normal chance of survival.</i> </p> </p>"},{"metadata":{"_uuid":"2f80a39e8da8822123f9916f42b63a4c2c551df1","_cell_guid":"43742288-d3ca-41a9-b979-46dcc85975cb","trusted":false,"collapsed":true},"cell_type":"code","source":"#Look at the Fare feature's distribution.\nf, ax = plt.subplots(1, 2, figsize=(14, 8))\n\nsns.kdeplot(dataset.Fare[dataset.Pclass==1], ax=ax[0], legend=False)\nsns.kdeplot(dataset.Fare[dataset.Pclass==2], ax=ax[0], legend=False)\nsns.kdeplot(dataset.Fare[dataset.Pclass==3], ax=ax[0], legend=False)\nax[0].set_title('Fare per Ticket Class')\nax[0].legend(['1', '2', '3'])\nax[0].set_xlim(-50, 300)\n\nsns.kdeplot(dataset.Fare[dataset.Embarked=='C'], ax=ax[1], legend=False)\nsns.kdeplot(dataset.Fare[dataset.Embarked=='S'], ax=ax[1], legend=False)\nsns.kdeplot(dataset.Fare[dataset.Embarked=='Q'], ax=ax[1], legend=False)\nax[1].set_title('Fare per Embarkment Location')\nax[0].legend(['C', 'S', 'Q'])\nax[1].set_xlim(-50, 300)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"0f2b7c882d5b86ae85e15470fb29a15cec877602","_cell_guid":"30f4ef7b-91e3-43df-b41c-9e0fe9371577"},"cell_type":"markdown","source":"<p>The fare features seems to follow a lognormal distribution. We can transform the data by taking log(Fare) to get a distribution that is more gaussian in nature. <br><br>Last but not least, we have to take a look at the cabin and ticket columns.</p>"},{"metadata":{"_uuid":"4f63134a2c3b99563576a6d215a3c9108f1f8349","_cell_guid":"5313fca1-d358-4f40-b54d-c3ed1960d2f8","trusted":false,"collapsed":true},"cell_type":"code","source":"print('Cabins')\n\nprint('We only know %.2f%% of the cabin numbers in the training set.' % ((len(dataset[dataset.Cabin.notnull()])/\\\n                                                                        len(dataset))*100))\nprint('We only know %.2f%% of the cabin numbers in the testing set.' % ((len(testset[testset.Cabin.notnull()])/\\\n                                                                        len(testset))*100))\n\nprint('\\nTickets')\nprint('There are %s unique versus %s duplicate tickets.' % (dataset.Ticket.nunique(), len(dataset)-dataset.Ticket.nunique()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9bf324cc603665a9d4633ac60b7ad12d63a4f498","_cell_guid":"0e8d2a5b-c93d-4d9b-a0a3-833f00a241a0","trusted":false,"collapsed":true},"cell_type":"code","source":"fig = plt.figure(figsize=(10, 10))\nfont = {'size' : 22}\nplt.rc('font', **font)\nsns.heatmap(data=dataset.loc[:, dataset.columns != 'Survived'].corr(), \\\ncmap='plasma',annot=True, fmt='.2f', linewidth=0.5)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"145936180f32decb8aa63ce93f46c93ed4980b9a","_cell_guid":"906b7db5-9a7b-48ed-8d87-4a0dae6e705b"},"cell_type":"markdown","source":"<p>Looking at the correlation heatmap, there is nothing of particular concern in terms of abnormal positive correlations. SibSp and Parch, the family features, have the highest positive correlation so we might look to combine the features to reduce model complexity.</p>\n\n<h2> c. Out of Sample Invariance Testing </h2>"},{"metadata":{"_uuid":"4f172c056c6312307c27d4cb77d4347ab4aaa0c4","_cell_guid":"3c02c85e-e108-4ef0-b138-8a67f788c6ac","trusted":false,"collapsed":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, 2, figsize=(14, 8))\n\nsns.kdeplot(dataset.Age[dataset.Pclass==1], ax=ax[0], legend=False)\nsns.kdeplot(dataset.Age[dataset.Pclass==2], ax=ax[0], legend=False)\nsns.kdeplot(dataset.Age[dataset.Pclass==3], ax=ax[0], legend=False)\n\nsns.kdeplot(testset.Age[testset.Pclass==1], ax=ax[1], legend=False)\nsns.kdeplot(testset.Age[testset.Pclass==2], ax=ax[1], legend=False)\nsns.kdeplot(testset.Age[testset.Pclass==3], ax=ax[1], legend=False)\n\n#fig.legend(['1', '2', '3'], loc=5)\nfig.suptitle('Training vs Testing Age per Class Distribution')\n\nfig, ax = plt.subplots(1, 2, figsize=(14, 8))\n\nsns.kdeplot(dataset.Age[dataset.Sex=='male'], ax=ax[0], legend=False)\nsns.kdeplot(dataset.Age[dataset.Sex=='female'], ax=ax[0], legend=False)\n\nsns.kdeplot(testset.Age[testset.Sex=='male'], ax=ax[1], legend=False)\nsns.kdeplot(testset.Age[testset.Sex=='female'], ax=ax[1], legend=False)\n\n#fig.legend(['male', 'female'], loc=5)\nfig.suptitle('Training vs Testing Age per Sex Distribution')\n\nfig, ax = plt.subplots(1, 2, figsize=(14, 8), sharex=True)\nax[0].set_xlim(-50, 300)\n\nsns.kdeplot(dataset.Fare[dataset.Embarked=='C'], ax=ax[0])\nsns.kdeplot(dataset.Fare[dataset.Embarked=='S'], ax=ax[0])\nsns.kdeplot(dataset.Fare[dataset.Embarked=='Q'], ax=ax[0])\n\nsns.kdeplot(testset.Fare[testset.Embarked=='C'], ax=ax[1])\nsns.kdeplot(testset.Fare[testset.Embarked=='S'], ax=ax[1])\nsns.kdeplot(testset.Fare[testset.Embarked=='Q'], ax=ax[1])\n\n\n#fig.legend(['C', 'S', 'Q'], loc=5)\nfig.suptitle('Training vs Testing Fare per Embarkment Location')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"57a0826f907f21edd99c49ae7bffb05223a52b70","_cell_guid":"1e493b74-ae02-47e7-b25b-a5f5e6ce349b"},"cell_type":"markdown","source":"<p>I think its important to see that our training and testing set are representative of each other. Typically we would want to reshuffle the training and testing sets if they are vastly different. Looking at the distribution of certain features over both sets, the distributions are quite similar. <br><br>One notable change is the distribution of <i>Ticket Class 3 Ages</i>. The distribution is almost perfectly Gaussian in the testing sample where it shifts to a multimodal gaussian shaped distribution. The change is not drastic and should not break the models. </p> "},{"metadata":{"_uuid":"bd27cffb3b121857ff296366e45e3409971c583c","_cell_guid":"391573bd-89a5-4032-aecd-d6579f33c79d","trusted":false,"collapsed":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, 2, figsize=(14, 8), sharex=True, sharey=True)\nsns.pointplot(x='Parch', y='SibSp', data=dataset, ax=ax[0])\nsns.pointplot(x='Parch', y='SibSp', data=testset, ax=ax[1])\n\nfig.suptitle('Training v Testing Parch to SibSp Samples')","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"83eae870e0adf03ddc4531e942a4314423fcd823","_cell_guid":"37c3dfb5-ca14-44d0-a63f-cb6c523dac0f"},"cell_type":"markdown","source":"<h1>2. Missing Values & Feature Configuration </h1>"},{"metadata":{"_uuid":"901a58157332a3bbdc483e629e0ca077fe277272","_cell_guid":"3cd01dd4-0b4c-44a4-a469-3ec3e1beb628","trusted":false,"collapsed":true},"cell_type":"code","source":"#Convert Sex column to encoded labels\nle = LabelEncoder()\ndataset.Sex = le.fit_transform(dataset.Sex)\ntestset.Sex = le.fit_transform(testset.Sex)\n\n#Since there are only 2 missing values in the embarked row, we can just drop these for now\ndataset = dataset[dataset.Embarked.notnull()]\n\n#Get dummy variables for Embarked column & drop one of the dummy variables\ndataset = pd.concat([dataset.drop('Embarked', axis=1), pd.get_dummies(dataset.Embarked, drop_first=True)],\\\n         axis=1)\ntestset = pd.concat([testset.drop('Embarked', axis=1), pd.get_dummies(testset.Embarked, drop_first=True)],\\\n         axis=1)\n\n#Due to the overwhelmindly large amount of missing cabin values, we will drop the row all together\ndataset.drop('Cabin', inplace=True, axis=1)\ntestset.drop('Cabin', inplace=True, axis=1)\n\ndataset.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"77d142d6c79d9218a0aa2a8ea7a00ec96f5c84f5","_cell_guid":"3c33736c-edf4-465f-b719-ff9ee8d76d2f"},"cell_type":"markdown","source":"<p> One of the bigger challenges in the dataset is to come up with some method to predict and fill the values of the Age feature. Looking back at the data exploration section, we saw that Age followed 3 different distributions based on the ticket class. This might be a hint towards using different ages per ticket class. It is also confirming  to see the testing set followed a similar gaussian distribution. </p> "},{"metadata":{"collapsed":true,"_uuid":"f32a77db3380a8e8b28d23ea62d805dcaae272a8","_cell_guid":"f2d94d8c-3b8c-4095-a5f1-43ec99b4eded","trusted":false},"cell_type":"code","source":"avg_1 = dataset.Age[dataset.Pclass==1].dropna().mean()\navg_2 = dataset.Age[dataset.Pclass==2].dropna().mean()\navg_3 = dataset.Age[dataset.Pclass==1].dropna().mean()\n\ndef fill_na(cols):\n    Age = cols[0]\n    Pclass = cols[1]\n    \n    if(np.isnan(Age)):\n        if(Pclass==1):\n            return avg_1\n        elif(Pclass==2):\n            return avg_2\n        elif(Pclass==3):\n            return avg_3\n    else:\n        return Age\n        \ndataset['Age'] = dataset[['Age', 'Pclass']].apply(fill_na, axis=1)\ntestset['Age'] = testset[['Age', 'Pclass']].apply(fill_na, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"a927c2f1dc2eca2155bed3065bb31509b4b58839","_cell_guid":"a6490456-cea6-4dc8-ac42-2ff0f0c3c688","trusted":false},"cell_type":"code","source":"imp = Imputer(missing_values ='NaN', \\\n             strategy='mean', \\\n             axis=0)\n#print(dataset.columns)\n#print(testset.columns)\n\nimp.fit(dataset.iloc[:, 8:9])\ntestset.Fare = imp.transform(testset.iloc[:, 7:8])","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"5eb6d073b4085d424ff6903f77485a5e758925f5","_cell_guid":"9c739968-e090-4716-8156-a0b5474deebc","trusted":false},"cell_type":"code","source":"#Scale the features\nss = StandardScaler()\n\ni = np.argwhere('Age'==dataset.columns)[0][0]\n\ndataset.Age = ss.fit_transform(dataset.iloc[:, i:i+1])\ntestset.Age = ss.transform(testset.iloc[:, i-1:i])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b0592330f8c68de80d7566fcff252ff27649595e","_cell_guid":"24993267-c0ff-48f0-85ab-be9fb1b8b77c"},"cell_type":"markdown","source":"<p>Above we filled the Age NaN values with the mean of each Ticket Class, scaled the age values in the training and testing sets and filled the missing Fare value in the testing set with the average. <br><br>Let's take a look at our NaN value heatmap to see if we cleaned the data properly. We should see a blank heatmap with no values highlighted in yellow if and only if everything was properly cleaned and formatted. </p>"},{"metadata":{"_uuid":"40e8616f98d11b4ffaf0fc7e12c3c55742ba7db5","_cell_guid":"afc18e06-5cd0-438d-a446-deba12388453","trusted":false,"collapsed":true},"cell_type":"code","source":"f, ax = plt.subplots(1, 2, figsize=(14, 8))\nsns.heatmap(dataset.isnull(), cbar=False, cmap='viridis', ax=ax[0])\nsns.heatmap(testset.isnull(), cbar=False, cmap='viridis', ax=ax[1])","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"50e26705bf628ed04cec1518fa9b7d734d501beb","_cell_guid":"826f55fb-2934-4ed6-9db9-2183e295bfe6"},"cell_type":"markdown","source":"<p> Mission <i>Success</i>!! We can move to feature engineering. <br><br><br>\n<h1> 3. Feature Engineering </h1>"},{"metadata":{"collapsed":true,"_uuid":"7f5b5f6b43941d2238f008fd3800a60fc9b59332","_cell_guid":"9395ae0d-4059-458f-80e8-dbd383ead5c6","trusted":false},"cell_type":"code","source":"def str_freq(cols):\n    \n    ticket = cols\n    return sum(bytearray(ticket, 'utf8'))\n\n\ndataset.Ticket = dataset.Ticket.apply(str_freq)\ntestset.Ticket = testset.Ticket.apply(str_freq)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"957ebc6067947964089d6f5208950e7c1b334319","_cell_guid":"7a389ee5-a6df-46f7-bdf5-cf610bc34298","trusted":false},"cell_type":"code","source":"'''\ndataset['Family'] = dataset.SibSp + dataset.Parch\ntestset['Family'] = testset.SibSp + testset.Parch\n\ndataset.drop(['SibSp', 'Parch', 'Name', 'Ticket'], inplace=True, axis=1)\ntestset.drop(['SibSp', 'Parch', 'Name', 'Ticket'], inplace=True, axis=1)\n'''\ndataset.drop(['Name', 'Ticket'], inplace=True, axis=1)\ntestset.drop(['Name', 'Ticket'], inplace=True, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"112135d265c3c3ecd6232a0dd46f2cceb2bd4059","_cell_guid":"6df818ca-73c4-4baa-a898-b7d1ac2079ff"},"cell_type":"markdown","source":"<p> As stated earlier, the fare feature seems to follow a lognormal distribution. An interesting characteristic lognormal distributions is the ability to be converted to a gaussian distribution. By taking the log of all the Fare values, we can accomplish just that. <br><br>\nBelow we will transform and scale all its values.</p>"},{"metadata":{"_uuid":"d62a7324977a2f5dda24a91b9eff86c8a0f9192d","_cell_guid":"e4b3e005-2a98-4ad6-a8e7-158c1734ab46","trusted":false,"collapsed":true},"cell_type":"code","source":"def is_neginf(cols):\n    Fare = cols[0]\n    \n    if(np.isneginf(Fare)):\n        return 0\n    else:\n        return Fare\n\ndataset.Fare = np.log(dataset.Fare)\ntestset.Fare = np.log(testset.Fare)\n\ndataset.Fare = dataset[['Fare', 'Age']].apply(is_neginf, axis=1)\ntestset.Fare = testset[['Fare', 'Age']].apply(is_neginf, axis=1)\n\ni = np.argwhere('Age'==dataset.columns)[0][0]\n\ndataset.Fare = ss.fit_transform(dataset.iloc[:, i:i+1])\ntestset.Fare = ss.transform(testset.iloc[:, i-1:i])\n\nfig, ax = plt.subplots(1, 2, figsize=(14, 8), sharex=True)\n\nsns.kdeplot(dataset.Fare, ax=ax[0])\nsns.kdeplot(testset.Fare, ax=ax[1])\n\nfig.suptitle('Training vs Testing Log Fare Distribution')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f109ddbaa66e63fad6814b69a966f7aa75d1d764","_cell_guid":"c1c7dce8-3801-42be-b22a-3a5c28ef8190","trusted":false,"collapsed":true},"cell_type":"code","source":"#Make sure all features are in the same order\ndataset.info()\nprint('\\n\\n')\ntestset.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b897e906ee0bdcefe95c764573a7c97c6e2aa09b","_cell_guid":"6747ff65-cae1-422d-b68f-13511fb286f0"},"cell_type":"markdown","source":"<p>Do note that I do all of my model performance out of the kernel. My method of parameter selection is too computationally complex to run in a kernel. I basically iterate through the set of possible parameters. Each configuration outputted shows the average cross validation score as well as the standard deviation of those scores. The configuration with the highest score and lowest standard deviation is chosen. A low standard deviation shows an invariant model to changes in the data which is what we want when testing out of sample. </p>\n\n<h1>4. Model Selection & Performance </h1>"},{"metadata":{"_uuid":"44c26a5c2fdb81ac7ee9b36549d83cdcea1c4b92","scrolled":false,"_cell_guid":"123ce797-9287-4434-b97b-7ec2091971a7","trusted":false,"collapsed":true},"cell_type":"code","source":"y = dataset.Survived\nX = dataset.loc[:, dataset.columns != 'Survived']\n\n'''\nclf = RandomForestClassifier(n_estimators=25, max_depth=4, \\\n                             min_samples_split = 25, \\\n                             min_samples_leaf = 7, \\\n                             random_state=48, \n                            class_weight={0:0.5, 1:0.5})\n'''\nclf = RandomForestClassifier(n_estimators=25, max_depth=7, \\\n                             min_samples_split = 30, \\\n                             min_samples_leaf = 14, \\\n                             random_state=6, \n                             class_weight={0:0.5, 1:0.5})\nclf.fit(X, y)\n\nscoring = {'acc': 'accuracy',\n                   'prec_macro': 'precision_macro',\n                   'rec_micro': 'recall_macro'}\n        \nscore = cross_validate(clf, X, y,\n                       scoring=scoring, \n                       cv=KFold(n_splits=3, \\\n                               shuffle=True, random_state=0))\n        \nprint('Accuracy: %.2f STD: %.3f' % (score['test_acc'].mean(), score['test_acc'].std()))\nprint('Precision: %.2f STD: %.3f' % (score['test_prec_macro'].mean(), score['test_prec_macro'].std()))\nprint('Recall: %.2f STD: %.3f' % (score['test_rec_micro'].mean(), score['test_rec_micro'].std()))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"90e0ad3fab6784f44df1509779e8591d88328bf4","_cell_guid":"8bc191c3-6581-43a0-af03-f14eb888831f","trusted":false},"cell_type":"code","source":"def build_classifier():\n    clf = keras.Sequential()\n    \n    clf.add(Dense(output_dim = 8, input_dim = 8 , init = 'uniform', activation = 'relu'))\n    \n    clf.add(Dense(output_dim = 8, init = 'uniform', activation = 'tanh'))\n    \n    clf.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))\n    \n    clf.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics=['accuracy'])\n    \n    return clf\n\nclf = KerasClassifier(build_fn = build_classifier, batch_size=10, epochs = 100)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1e08f27ddce650524454e07ee5108884b7265b63","_cell_guid":"ef47cf69-7c32-42fe-b9a6-837b2cf5ea75","trusted":false,"collapsed":true},"cell_type":"code","source":"scoring = {'acc': 'accuracy',\n                   'prec_macro': 'precision_macro',\n                   'rec_micro': 'recall_macro'}\n\nscore = cross_validate(clf, X, y, scoring=scoring, \\\n                      cv=KFold(n_splits=3, \\\n                               shuffle=True, random_state=0))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"87b83fce22646cf85cda408832bc846ae1e49257","_cell_guid":"5e80d4fb-094b-46f8-bc76-09b23b6293cc","trusted":false,"collapsed":true},"cell_type":"code","source":"print('Accuracy: %.2f STD: %.3f' % (score['test_acc'].mean(), score['test_acc'].std()))\nprint('Precision: %.2f STD: %.3f' % (score['test_prec_macro'].mean(), score['test_prec_macro'].std()))\nprint('Recall: %.2f STD: %.3f' % (score['test_rec_micro'].mean(), score['test_rec_micro'].std()))","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"ffa0be9042e8890552e1fae93bb75e8a67f7b91d","_cell_guid":"626942e7-5875-4f7f-80ac-245d4fc92825","trusted":false},"cell_type":"code","source":"knn = KNeighborsClassifier()\n\nparams = {'n_neighbors' : [2**i for i in range(6)], \\\n         'weights' : ['uniform', 'distance']}\nclf = GridSearchCV(knn, param_grid=params, scoring='accuracy', cv = KFold(n_splits=3, \\\n                                                                      shuffle=True, \\\n                                                                      random_state=0))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0608b9464c631962f21b781009473e6a329e868f","_cell_guid":"98e55378-6e79-4498-8afd-b7fa33cff2df","trusted":false,"collapsed":true},"cell_type":"code","source":"clf.fit(X, y)\nclf.best_params_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"da4540f8c6a4a0db6a8eeea3ef55e9d026db445e","_cell_guid":"1b2acd98-aa31-4fbe-9521-f2a1d6a94328","trusted":false,"collapsed":true},"cell_type":"code","source":"clf = KNeighborsClassifier(n_neighbors=8, weights='uniform')\n\nscoring = {'acc': 'accuracy',\n                   'prec_macro': 'precision_macro',\n                   'rec_micro': 'recall_macro'}\n        \nscore = cross_validate(clf, X, y,\n                       scoring=scoring, \n                       cv=KFold(n_splits=3, \\\n                               shuffle=True, random_state=0))\n        \nprint('Accuracy: %.2f STD: %.3f' % (score['test_acc'].mean(), score['test_acc'].std()))\nprint('Precision: %.2f STD: %.3f' % (score['test_prec_macro'].mean(), score['test_prec_macro'].std()))\nprint('Recall: %.2f STD: %.3f' % (score['test_rec_micro'].mean(), score['test_rec_micro'].std()))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a4dfe1728a935bb57546a25ad8196b7e484baba6","_cell_guid":"bd947cd6-d8ba-4be6-a03f-45e0624eeae8","trusted":false,"collapsed":true},"cell_type":"code","source":"'''my_submission = pd.DataFrame({'PassengerId': testset.index.values, 'Survived': \\\n                              clf.predict_classes(testset).reshape(1, -1)[0]})\nmy_submission.to_csv('submission.csv', index=False)'''","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"file_extension":".py","version":"3.6.4","codemirror_mode":{"version":3,"name":"ipython"},"pygments_lexer":"ipython3","nbconvert_exporter":"python","mimetype":"text/x-python","name":"python"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}