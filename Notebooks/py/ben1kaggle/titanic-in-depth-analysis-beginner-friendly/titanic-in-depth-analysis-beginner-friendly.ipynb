{"metadata": {"kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}, "language_info": {"nbconvert_exporter": "python", "file_extension": ".py", "version": "3.6.1", "codemirror_mode": {"version": 3, "name": "ipython"}, "pygments_lexer": "ipython3", "name": "python", "mimetype": "text/x-python"}}, "cells": [{"metadata": {"_cell_guid": "83c1b488-2367-46aa-9697-d6e376a74950", "_uuid": "15ecb696e46ae4050a7bb6294e2447f1be7276b3"}, "source": ["# Titanic survival prediction\n", "\n", "## Import general packages"], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "7b1eaab0-9ab2-4d2e-989b-9aa060736100", "collapsed": true, "_uuid": "4a4bb92df43a22357f55ac2e7d4d68880d021eef"}, "execution_count": null, "outputs": [], "source": ["# Importing general packages and set basic plot styling\n", "%matplotlib inline\n", "import pandas as pd\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "from IPython.display import display\n", "from IPython.display import YouTubeVideo\n", "import warnings\n", "warnings.filterwarnings('ignore')\n", "\n", "sns.set_style(\"white\")\n", "sns.set_context(\"notebook\", font_scale=1.2, rc={\"lines.linewidth\": 2.5})\n", "plt.rcParams['axes.color_cycle'] = [u'#1f77b4', u'#ff7f0e', u'#2ca02c', u'#d62728', \n", "                                    u'#9467bd', u'#8c564b', u'#e377c2']"], "cell_type": "code"}, {"metadata": {"_cell_guid": "db1f8c07-6995-4e2b-b211-995b75f2cce2", "_uuid": "981c9e29a41459918c36b98c35ac526eb072b0a6"}, "source": ["## Introduction\n", "The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships. One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.   \n", "<br>\n", "In this notebook, we will first clean data and fill in missing values with linear regression. Then some feature engineering after which we will explore the data thoroughly. Finally, we will use random forest machine learning to predict the survival of Titanic passengers. "], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "ee5d24a3-9390-4727-a7c3-f63b564ee27c", "_uuid": "1537cd82de26b7624f32a1ac54d405cc7dbc500b"}, "execution_count": null, "outputs": [], "source": ["YouTubeVideo('NdZ6TY1pxL8')\n", "# https://www.youtube.com/watch?v=NdZ6TY1pxL8 "], "cell_type": "code"}, {"metadata": {"_cell_guid": "29871799-b0ab-4fca-acf7-4e3ab612bff1", "_uuid": "5f4da806017b6ed258255599d61e357cfbbeea2a"}, "source": ["## Load data"], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "6ca04543-70a1-45bd-a564-88684c2fae92", "_uuid": "91d403e25b2fc559d2846311df2cae1d941af64e"}, "execution_count": null, "outputs": [], "source": ["# Load and merge the train and test data\n", "df1 = pd.read_csv('../input/train.csv')\n", "df2 = pd.read_csv('../input/test.csv')\n", "df1['Set'] = 'train'\n", "df2['Set'] = 'test'\n", "df=df1.append(df2)\n", "df=df.reset_index()\n", "df.info()\n", "df.head()"], "cell_type": "code"}, {"metadata": {"_cell_guid": "8d997c55-973d-4504-b90e-d9b1aaceaf98", "_uuid": "f67c359aacf15b2dd3fc9924ab74fa5a2bc5b968"}, "source": ["## Missing values (part 1)\n", "There are 4 variables that contain missing values which we will have to deal with first.\n", "* A fair amount of missing values in Age, we can fill those in reasonably with a regression model. \n", "* By far the most values for Cabin are missing, so we will not try to fill these in and simply mark them as unknown.\n", "* Only 2 values are missing for Embarked.\n", "* Just one 1 value is missing in Fare.    \n", "<br>\n", "\n", "#### Embarked\n", "Two ladies on the same 1st class ticket have unknown embarked values. A quick google on the ticket number leads to encyclopedia-titanica.org which says they embarked in Southampton. "], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "570c322f-6787-48de-9a55-2ceedc4a35a9", "_uuid": "3d9a1dba96a2ec9f6d42063a8223e9f835e58f2c"}, "execution_count": null, "outputs": [], "source": ["# Missing values Embarked \n", "display(df[df['Embarked'].isnull()])\n", "\n", "df['Embarked'] = df['Embarked'].fillna('S') "], "cell_type": "code"}, {"metadata": {"_cell_guid": "7e945898-b756-489e-82f1-00237d60c23b", "_uuid": "be3d31449b2c0246994edab79491bab3966016ff"}, "source": ["#### Fare\n", "The Fare for Mr. Storey is missing. On encyclopedia-titanica.org his name does not lead to a known Fare. His ticket number is actually 370160 instead of 3701 which has 5 other people on it. Too bad, Ticket 370160 does not occur in our dataset. It looks like Mr. Storey is part of the staff on the Titanic so he probably did not pay a Fare at all. We will ignore that and simply fill in his Fare with the median for 3rd class passengers embarked in Southampton. "], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "efc8949f-1194-49e0-a954-6823ebf6f90c", "_uuid": "4a34458a5e8d97b69929e1b913fcfec92b4780b6"}, "execution_count": null, "outputs": [], "source": ["# Missing value Fare\n", "display(df[df['Fare'].isnull()])\n", "\n", "a = df['Fare'].loc[(df['Pclass']==3) & (df['Embarked']=='S')]\n", "\n", "plt.figure(figsize=[7,3])\n", "sns.distplot(a.dropna(), color='C0')\n", "plt.plot([a.median(), a.median()], [0, 0.16], '--', color='C1')\n", "\n", "df['Fare'] = df['Fare'].fillna(a.median())\n", "\n", "sns.despine(bottom=0, left=0)\n", "plt.title('Fare for 3rd class embarked in S')\n", "plt.xlabel('Fare')\n", "plt.legend(['median'])\n", "plt.show()"], "cell_type": "code"}, {"metadata": {"_cell_guid": "c9608016-7b95-432c-a8a6-01e704b0abaa", "_uuid": "d86728f00561fe2245ecc98b082d555cd248258f"}, "source": ["#### Age\n", "There are many Age values missing, 263 to be precise. We will perform some linear regression to reasonably predict the ages of the passengers, so that the fake ages blend in nicely with the known Titanic population. \n", "\n", "Because we would like to incorporate as much information as possible in the regression model, we will first move on to the feature engineering and cleaning.\n"], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "271b0a46-6742-4ac6-8b28-17f32535a3a6", "_uuid": "c353f2e5ba98989b65287b074c4463ea066adba9"}, "source": ["## Feature engineering\n", "In this section we first clean up some features. Survived and Sex are simply changed to string values, to make the plotting a bit nicer later on. Fare is in british pounds from 1912, because of inflation that is wildly incomparable to today's value of the GBP. We correct for that and then convert to USD, just for fun.  \n", "\n", "* The Name column contains information about the social status of the person because it includes the person's title. We extract it and merge some rare titles into more common categories.  \n", "\n", "\n", "* Another interesting feature we can make from the existing data is family size. Did larger families perhaps get priority to get on the lifeboats? Or would they be lost while looking for eachother in the chaos? We sum the Parch and SibSp together and add 1 for the individual itself into FamSize. FamSize2 is categorized as single, small and large families.  \n", "\n", "\n", "* Similarly to FamSize, we make a group size variable. This is based on number of people sharing the same Ticket. So this includes families (and potential nannies for example), but also groups of friends or colleagues. GrpSize is the actual number of people in the group and GrpSize2 is categorized as FamSize2.  \n", "\n", "\n", "* The Cabin column is pretty useless on it's own. However, we can easily extract information about on which deck the passenger was staying. Perhaps certain decks are easier to escape from. We take the first letter from the Cabin number and assign it to the Deck column. Most Cabin values are NaN, which we assign as 'X' for unknown. \n", "\n", "\n", "* Cabin contains more information. The cabin number after the Deck letter tells us something about the location from front to back. The deckplans are available at encyclopedia-titanica.org/titanic-deckplans/ . Based on that, we make a rough distinction between front, mid and back of the ship. \n"], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "3e1ffa0c-1294-4de8-b892-b1b31c127c69", "collapsed": true, "_uuid": "9f2bd1670608291ec85c5fff3baa00479d45d3a3"}, "execution_count": null, "outputs": [], "source": ["# Clean up and feature engineering\n", "\n", "# Label Survived for plot\n", "df['Survived'] = df['Survived'].replace([0, 1], ['no', 'yes']) \n", "\n", "# Label Sex for plot\n", "df['Sex'] = df['Sex'].replace([0, 1], ['male', 'female']) \n", "\n", "# Transform Fare to today's US dollar, for fun\n", "df['Fare'] = df['Fare']*108*1.3 #historic gbp to current gbp to current usd\n", "\n", "# Get personal title from Name, merge rare titles\n", "df['Title'] = df['Name'].apply(lambda x: x.split(',')[1].split(' ')[1])\n", "toreplace = ['Jonkheer.', 'Ms.', 'Mlle.', 'Mme.', 'Capt.', 'Don.', 'Major.', \n", "             'Col.', 'Sir.', 'Dona.', 'Lady.', 'the']\n", "replacewith = ['Master.', 'Miss.', 'Miss.', 'Mrs.', 'Sir.', 'Sir.', 'Sir.',\n", "              'Sir.', 'Sir.', 'Lady.', 'Lady.', 'Lady.']\n", "df['Title'] = df['Title'].replace(toreplace, replacewith)\n", "\n", "# Get family names\n", "df['FamName'] = df['Name'].apply(lambda x: x.split(',')[0])\n", "\n", "# Get family sizes based on Parch and SibSp, classify as single/small/large\n", "df['FamSize'] = df['Parch'] + df['SibSp'] + 1\n", "df['FamSize2'] = pd.cut(df['FamSize'], [0, 1, 4, 11], labels=['single', 'small', 'large'])\n", "\n", "# Get group sizes based on Ticket, classify as single/small/large\n", "df['GrpSize'] = df['Ticket'].replace(df['Ticket'].value_counts())\n", "df['GrpSize2'] = pd.cut(df['GrpSize'], [0, 1, 4, 11], labels=['single', 'small', 'large'])\n", "\n", "# Get Deck from Cabin letter\n", "def getdeck(cabin):\n", "    if not pd.isnull(cabin) and cabin[0] in ['A', 'B', 'C', 'D', 'E', 'F', 'G']:\n", "        return cabin[0]\n", "    else:\n", "        return 'X'    \n", "    \n", "df['Deck'] = df['Cabin'].apply(getdeck)\n", "\n", "# Get a rough front/mid/back location on the ship based on Cabin number\n", "'''\n", "A front\n", "B until B49 is front, rest mid\n", "C until C46 is front, rest mid\n", "D until D50 is front, rest back\n", "E until E27 is front, until E76 mid, rest back\n", "F back\n", "G back\n", "Source: encyclopedia-titanica.org/titanic-deckplans/\n", "'''\n", "def getfmb(cabin):\n", "    \n", "    if not pd.isnull(cabin) and len(cabin)>1:\n", "        if (cabin[0]=='A'\n", "            or cabin[0]=='B' and int(cabin[1:4])<=49\n", "            or cabin[0]=='C' and int(cabin[1:4])<=46\n", "            or cabin[0]=='D' and int(cabin[1:4])<=50\n", "            or cabin[0]=='E' and int(cabin[1:4])<=27):\n", "            return 'front'\n", "        \n", "        elif (cabin[0]=='B' and int(cabin[1:4])>49\n", "            or cabin[0]=='C' and int(cabin[1:4])>46\n", "            or cabin[0]=='E' and int(cabin[1:4])>27 and int(cabin[1:4])<=76):\n", "            return 'mid'\n", "\n", "        elif (cabin[0]=='F'\n", "           or cabin[0]=='G'\n", "           or cabin[0]=='D' and int(cabin[1:4])>50):\n", "            return 'back'\n", "        \n", "        else:\n", "            return 'unknown'\n", "    else:\n", "        return 'unknown'        \n", "    \n", "df['CabinLoc'] = df['Cabin'].apply(getfmb)\n", "\n", "dfstrings = df.copy() # save df containing string features to use for plotting later"], "cell_type": "code"}, {"metadata": {"_cell_guid": "072c786d-0621-47f1-b1a7-b67cad124fce", "_uuid": "f5ac6a9620da94fdfbc3bb2499e7dccbf31ed5ba"}, "source": ["## Missing values (part 2)\n", "Now that we have all our features, we can use them to create a regression model for filling in the missing values in Age. First, we have to factorize the string based features."], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "48bdd5f0-8eab-48e2-b755-66150674bf3c", "collapsed": true, "_uuid": "fbc7489583998518217e082e9d4f088ae9154155"}, "execution_count": null, "outputs": [], "source": ["# Factorize the string features\n", "\n", "df['CabinLoc'] = df['CabinLoc'].replace(['unknown', 'front', 'mid', 'back'], range(4))\n", "\n", "df['Deck'] = df['Deck'].replace(['X', 'A', 'B', 'C', 'D', 'E', 'F', 'G'], range(8))\n", "\n", "df['GrpSize2'] = df['GrpSize2'].astype(str) #convert from category dtype\n", "df['GrpSize2'] = df['GrpSize2'].replace(['single', 'small', 'large'], range(3))\n", "\n", "df['FamSize2'] = df['FamSize2'].astype(str) #convert from category dtype\n", "df['FamSize2'] = df['FamSize2'].replace(['single', 'small', 'large'], range(3))\n", "\n", "df['Title'] = df['Title'].replace(df['Title'].unique(), range(8))\n", "\n", "df['Embarked'] = df['Embarked'].replace(['S', 'C', 'Q'], range(3))\n", "\n", "df['Sex'] = df['Sex'].replace(['male', 'female'], range(2)) \n", "\n", "df['Survived'] = df['Survived'].replace(['no', 'yes'], range(2)) \n", "\n", "dfnum = df.copy() # save df containing factorized features to use for subsequent analysis"], "cell_type": "code"}, {"metadata": {"_cell_guid": "040707ee-9f25-4dc6-8f35-204e6b984cef", "_uuid": "597397c17285a9aa0de852151c863392fa7abb06"}, "source": ["#### Age\n", "The first thing to do is split the data for which we have the known Age into a train and test set. We use 3/4 of the 1046 values as the training set. This leaves 262 as test set, coinciding nicely with our unknown data which is n=263. \n", "\n", "After fitting a model to the train set, we look at the root mean squared error (rmse). The rmse is 12.2 years, certainly not perfect. For cross validation (top figure), we use the model for predicting the test set. The rmse in this case is 12.5. Thus, the rmse of train and test are the same, which means the model generalizes well to out of sample data (no over or underfitting). \n", "\n", "Next, we use the model to predict the unknown ages. The resulting ages range from 3.8 to 44.1. Compared to applying the model to the known dataset, it does not look so great (middle figure). The model predicts the values quite narrowly around the mean for the unknown data. We'll take it anyway, because better than throwing all the unknown rows out or simply replacing the NaNs with mean/median.\n", "\n", "Lastly, we merge the predicted missing values with the known data. Then, we compare our original age distribution with the distribution of the final result (bottom figure). The density around the mean is increased somewhat compared to the original distribution. \n", "\n"], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "f85a29b2-ac91-4f49-9e4e-01a5a0bdd32e", "_uuid": "fa20423723937039d92f3ba6878e98845958c364"}, "execution_count": null, "outputs": [], "source": ["# Multiple linear regression modeling of Age\n", "from sklearn.linear_model import LinearRegression\n", "from sklearn.model_selection import train_test_split\n", "from sklearn import metrics\n", "\n", "feats = ['Sex', 'Embarked', 'Pclass', 'Fare', 'Title', 'Parch', 'SibSp', 'FamSize', 'FamSize2', \n", "         'GrpSize', 'GrpSize2', 'Deck', 'CabinLoc' ]\n", "\n", "dffeats = df[feats][df['Age'].notnull()]\n", "dfresp = df['Age'][df['Age'].notnull()]\n", "dfmiss = df[feats][df['Age'].isnull()]\n", "\n", "X_train, X_test, y_train, y_test = train_test_split(dffeats, dfresp, test_size=0.25, random_state=100)\n", "\n", "lm = LinearRegression()\n", "lm.fit(X_train, y_train )\n", "\n", "y_predtrain = lm.predict(X_train)\n", "print('Mean train: ' + str(np.mean(y_predtrain)))\n", "print('Std predtrain: ' + str(np.std(y_predtrain)))\n", "print('RMSE predtrain: ' + str(np.sqrt(metrics.mean_squared_error(y_train, y_predtrain))))\n", "\n", "y_predtest = lm.predict(X_test)\n", "print('Mean test: ' + str(np.mean(y_predtest)))\n", "print('Std predtest: ' + str(np.std(y_predtest)))\n", "print('RMSE predtest: ' + str(np.sqrt(metrics.mean_squared_error(y_test, y_predtest))))\n", "\n", "pred1 = lm.predict(dffeats)\n", "pred2 = lm.predict(dfmiss)\n", "print(pred2.min(), pred2.max())\n"], "cell_type": "code"}, {"metadata": {"_cell_guid": "a4824fb5-d67d-46b8-ad7e-2e681cdbc164", "_uuid": "7bdfed5954aed44a73babfcbc62f9009102bb5ec"}, "execution_count": null, "outputs": [], "source": ["# Plots for Age regression\n", "fig, [ax1, ax2, ax3] = plt.subplots(3,1, figsize=[7,6])\n", "\n", "sns.distplot(y_predtrain, hist=False, label='prediction of train set (n=784)', ax=ax1)\n", "sns.distplot(y_predtest, hist=False, label='prediction of test set (n=262)', ax=ax1)\n", "ax1.set_xlim([0, 80])\n", "\n", "sns.distplot(pred1, hist=False, label='prediction of known (n=1046)', ax=ax2)\n", "sns.distplot(pred2, hist=False, label='prediction of missing (n=263)', ax=ax2)\n", "ax2.set_xlim([0, 80])\n", "ax2.set_ylim([0, 0.15])\n", "\n", "sns.distplot(dfresp, hist=False, label='known (n=1046)')\n", "sns.distplot(dfresp.values.tolist() + pred2.tolist(), hist=False, label='known + predicted missing (n=1309)')\n", "ax3.set_xlim([0, 80])\n", "ax3.set_ylim([0, 0.05])\n", "\n", "fig.tight_layout()\n", "sns.despine(bottom=0, left=0)"], "cell_type": "code"}, {"metadata": {"_cell_guid": "433c3779-ffab-4d4d-891b-9411c51a523b", "_uuid": "0b8de934949c1cbb666cdd0cac877cb8c4cf60b9"}, "source": ["#### Update dataframe with Age\n", "Below we add the the newly predicted Age values to the dataframe. Additionally, AgeGrp, AgeDec, and Persontype are created with the Age variable. AgeGrp groups ages into child/teen/adult. AgeDec simplifies Age by categorizing people's age into decades. PersonType is a combination of Sex and Age to differentiate between male kids and male adults, because of the 'women and children first' rule. "], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "ee2f971b-8ba7-464c-8718-7eb0b8a7d47b", "collapsed": true, "_uuid": "d34064e9a0e091e7f310e512d8ea9c639fba83b4"}, "execution_count": null, "outputs": [], "source": ["# Updating the dataframe (strings version) with Age related features\n", "df = dfstrings\n", "df['Age'].loc[df['Age'].isnull()] = pred2\n", "\n", "# Classify age groups\n", "df['AgeGrp'] = pd.cut(df['Age'], [0, 12, 20, 200], labels = ['child', 'teen', 'adult'])\n", "\n", "# Classify age decade\n", "df['AgeDec'] = pd.cut(df['Age'], range(0,90,10), labels=range(8))\n", "df['AgeDec'] = df['AgeDec'].astype(int)\n", "\n", "# Classify male/female/child\n", "df['PersonType'] = df['Sex']\n", "df.loc[df['Age']<12,'PersonType'] = 'child'\n"], "cell_type": "code"}, {"metadata": {"_cell_guid": "c7b43fb8-fea7-43da-80db-c6cb77201cb0", "collapsed": true, "_uuid": "12e0538ec02301ff316aebb1ee8fa57c60f9640c"}, "execution_count": null, "outputs": [], "source": ["# Updating the dataframe (numbers version) with Age related features\n", "dfnum['Age'] = df['Age']\n", "\n", "dfnum['AgeGrp'] = df['AgeGrp'].astype(str) #convert from category dtype\n", "dfnum['AgeGrp'] = dfnum['AgeGrp'].replace(['child', 'teen', 'adult'], range(3))\n", "\n", "dfnum['AgeDec'] = df['AgeDec']\n", "\n", "dfnum['PersonType'] = df['PersonType'].replace(['male', 'female', 'child'], range(3)) \n"], "cell_type": "code"}, {"metadata": {"_cell_guid": "9a4d4b15-53c7-49d1-b96c-658009a79536", "_uuid": "d8ef2e54ba35f963f0b7c44ec44d85e47522b66f"}, "source": ["## Data exploration\n", "#### Histograms\n", "Now, let's have a look at all features in the dataframe that we can use for predicting survival. Making use of the handy 'hue' parameter in the seaborn countplot function we can easily plot histograms split on survival to get a sense of the proportions. \n", "<br>\n", "1. First, we simply plot how many people **Survived** and how many did not. Most people died, so if we were to simply predict that everybody dies we would be correct approximately 62% (342/549=0.62). Hopefully we can do better than that. \n", "\n", "2. **Sex** has a clear effect. By far most men died whereas most women survived. \n", "\n", "3. **Age** also shows promise. Of the survivors, more people are young and less are in the 20-30 range relative to the victims. Strangely, there is a bit of a dip in the number of people aboard that are about 10-15. Perhaps people with school age kids tend not to go on long trips in April?\n", "\n", "4. The conclusions from Age are also reflected in **AgeGrp** and **AgeDec**. Children died about 50-50, which are good odds compared to teens and certainly adults. People below 10 are slightly more likely to survive, but it is already reversed for people in 10-20, with 20-30 being the worst odds to survive. **PersonType** merely combines Sex and AgeGrp.\n", "\n", "5. Place of **Embarkment** strangely has an effect, since people from Cherbourg were more likely to survive whereas Southamptom and Queenstown were more likely to die. Perhaps this is due to differences in class of the people embarking in each location?\n", "\n", "6. **Pclass** is obviously an important feature. First class passenger clearly got priority boarding the lifeboats as most of them survived. Equal odds for 2nd class passengers. By far most people on titanic were 3rd class and by far most of them did not survive.\n", "\n", "7. The Pclass effect is also reflected in the **Fare**. Most people paying less than about 2500 today's USD worth did not survive. Above that amount of money, you were slightly more likely to survive. The most expensive ticket was worth almost 72000 bucks. Pricey. \n", "\n", "8. **Title** seems to reflect mostly the male/female difference, the higher titles do have increased survivability but there are so few of them it might not be of much relevance. \n", "\n", "9. From **Parch, SibSp, FamSize1 **and** FamSize2**, we can conclude that being part of a small family was a good benefit. However, being a single traveller or part of a large family was detrimental to survival odds. Perhaps small families did indeed get priority for lifeboats, with larger families more likely to have to search for eachother?\n", "\n", "10. The same conclusions can be drawn for **GrpSize** and **GrpSize2**. Most groups are families and the non-family groups might not be too plentiful to affect the distribution much. \n", "\n", "11. Lastly, **Deck** and **CabinLoc**. It seems most people survive overall? These distributions are based on only 295 values of known Cabin number. Probably, if your cabin number was known you were 1st or 2nd class and therefore more likely to survive. \n", "\n"], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "a81d5787-4282-4431-aa78-ca7035dcf6ab", "_uuid": "836f5cffc7e0daca41cb5a2c3f595e1d0764c95c"}, "execution_count": null, "outputs": [], "source": ["# Plot histograms of all the features split on 'Survived'\n", "\n", "fig, axes = plt.subplots(5,4, figsize=[12,13])\n", "axes = axes.ravel()\n", "axnr = 0\n", "for i in ['Survived', 'Sex', 'Age', 'Age', 'AgeGrp', 'AgeDec', 'PersonType', 'Embarked', 'Pclass', \n", "          'Fare', 'Fare', 'Title', 'Parch', 'SibSp', 'FamSize', 'FamSize2', 'GrpSize', 'GrpSize2', \n", "          'Deck', 'CabinLoc' ]: \n", "    sns.countplot(i, hue='Survived', data=df, ax=axes[axnr])\n", "    axes[axnr].set(xlabel=i, ylabel=\"\")\n", "    axes[axnr].legend().set_visible(False)\n", "    axnr += 1\n", "\n", "axes[0].cla() #clear and replace plot\n", "sns.countplot('Survived', data=df, ax=axes[0])\n", "axes[0].set(xlabel='Survived', ylabel=\"\")\n", "    \n", "axes[2].cla()\n", "sns.distplot(df['Age'][df['Survived']=='no'], ax=axes[2], bins=range(0, 100, 2))\n", "sns.distplot(df['Age'][df['Survived']=='yes'], ax=axes[2], bins=range(0, 100, 2))\n", "\n", "axes[3].cla()\n", "sns.distplot(df['Age'][df['Survived']=='no'], ax=axes[3], bins=range(0, 100, 2))\n", "sns.distplot(df['Age'][df['Survived']=='yes'], ax=axes[3], bins=range(0, 100, 2))\n", "axes[3].set(xlim=[0,40], ylim=[0,0.05])\n", "\n", "axes[9].cla()\n", "sns.distplot(df['Fare'][df['Survived']=='no'], ax=axes[9], bins=range(0, 72000, 300))\n", "sns.distplot(df['Fare'][df['Survived']=='yes'], ax=axes[9], bins=range(0, 72000, 300))\n", "axes[9].set(ylim=[0,0.0005])\n", "\n", "axes[10].cla()\n", "sns.distplot(df['Fare'][df['Survived']=='no'], ax=axes[10], bins=range(0, 72000, 300))\n", "sns.distplot(df['Fare'][df['Survived']=='yes'], ax=axes[10], bins=range(0, 72000, 300))\n", "axes[10].set(xlim=[0,10000], ylim=[0,0.0005])\n", "\n", "axes[11].set_xticklabels(axes[11].get_xticklabels(), rotation = 45, size='x-small', ha=\"center\")\n", "\n", "axes[18].cla()\n", "sns.countplot(df['Deck'][df['Deck'] != 'X'], hue='Survived', data=df, \n", "              ax=axes[18], order=['A', 'B', 'C', 'D', 'E', 'F', 'G'])\n", "axes[18].set(xlabel='Deck', ylabel=\"\")\n", "axes[18].legend().set_visible(False)\n", "\n", "axes[19].cla()\n", "sns.countplot(df['CabinLoc'][df['CabinLoc'] != 'unknown'], hue='Survived', data=df, \n", "              ax=axes[19], order=['front', 'mid', 'back'])\n", "axes[19].set(xlabel='CabinLoc', ylabel=\"\")\n", "axes[19].legend().set_visible(False)    \n", "    \n", "plt.title('Histograms')    \n", "fig.tight_layout()\n", "sns.despine(bottom=1, left=1)"], "cell_type": "code"}, {"metadata": {"_cell_guid": "07f4ef22-5940-496c-acb3-824c5f557297", "_uuid": "b9905e683029dd123387c8fe5e3e74511a8f79c2"}, "source": ["#### Correlation heatmap\n", "After we got a basic understanding of the features in our dataset, let's plot a correlation heatmap too see if we can detect some interesting patterns.   \n", "<br>\n", "As expected, a strong correlation exists between Survival and Sex. However, the purely Age related features do not show a strong correlation to Survival, potentially due to a U shape relation?  \n", "PersonType is basically the Sex predictor for Survival, but a bit weaker because of the Age related component. Pclass and Fare are naturally correlated to eachother, and both are strongly correlated to Survival. Pclass and Age are also obviously related because older people tend to be richer. Deck and CabinLoc are decently correlated with Survival, but as we hypothesized before, these are indeed strongly correlated with Pclass. Title's correlation with Survival is mainly influenced by Sex and not so much Pclass.  \n", "FamSize2 and GrpSize2 are interesting for Survival prediction. The family/group size variables are logically correlated with age variables since children tend to be part of families and many adults travel single.  \n", "Embarked has a minor correlation to Survival as well, but not to Pclass as we hypothesized before.   \n"], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "1a00dc36-7d16-4f9d-8b0c-1b075323218c", "_uuid": "fc21509cd42ed9774dc65e5e0290162424c73c2b"}, "execution_count": null, "outputs": [], "source": ["df = dfnum\n", "fig = plt.subplots(figsize=[15, 15])\n", "\n", "sns.heatmap(df[['Survived', 'Sex', 'Age', 'AgeGrp', 'AgeDec', 'PersonType', 'Embarked', 'Pclass', \n", "                'Fare', 'Title', 'Parch', 'SibSp', 'FamSize', 'FamSize2', 'GrpSize', 'GrpSize2', \n", "                'Deck', 'CabinLoc']].corr(), \n", "            annot=True, fmt=\".2f\", square=1, cmap=\"RdBu_r\", vmin=-1, vmax=1)\n", "plt.title('Correlation heatmap')\n", "plt.show()"], "cell_type": "code"}, {"metadata": {"_cell_guid": "f87a9393-d573-4cae-8f2b-d50d94fa1fd9", "_uuid": "31b829799d0513f7e88545aded4c3f33c8fd0eec"}, "source": ["Let's also quickly check Age's relation to Survival and investigate our Embarked/Pclass theory. In the plots below, the size of the blue markers indicates the number of people in that group. Red dots are means of the groups on Y, per group on X. Green lines are the regression lines of the data.    \n", "<br>\n", "As we could already see in the histograms; if you were under 10 years old, you'd have a slightly higher chance on survival (above 50%). It was a bit harder to see what happens at the 60+ year level because there are so few. Turns out, seniors were the most likely to perish. All other decades are around 40% survival, and this includes by far the most people so it is also the overall survival rate.   \n", "<br>\n", "Regarding Embarked/Pclass, we can confirm our hypothesis that people embarking in Cherbourg were primarily 1st class. In Southampton and Queenstown the clear majority was 3rd class. Since 1st class passengers are much more likely to survive, this explains why people from Cherbourg were more likely to survive. The order of the factors causes a U shape relation which leads to the flat regression line. We could switch around the order here to form a better correlation, but it is not important. \n"], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "e1e57fbb-35a4-48a6-b716-c8e4b04c8a4c", "_uuid": "99b8305da48eed55ffa23ecf8e6540ddc3c705df"}, "execution_count": null, "outputs": [], "source": ["fig, [ax1, ax2] = plt.subplots(1,2, figsize=[10,4])\n", "x='AgeDec'\n", "y='Survived'\n", "df['Grpcount'] = df.groupby([x, y]).transform('count')['index'].values\n", "df['Grpmean'] = df.groupby([x]).transform('mean')[y].values\n", "sns.regplot(x, y, data=df, scatter_kws={'s': df['Grpcount']*7}, line_kws={'color':'C2'}, ax=ax1)\n", "ax1.plot(df[x], df['Grpmean'], 'o', color='C3')\n", "ax1.set_ylim([-0.15, 1.1])\n", "\n", "x='Embarked'\n", "y='Pclass'\n", "df['Grpcount'] = df.groupby([x, y]).transform('count')['index'].values\n", "df['Grpmean'] = df.groupby([x]).transform('mean')[y].values\n", "sns.regplot(x, y, data=df, scatter_kws={'s': df['Grpcount']*7}, line_kws={'color':'C2'}, ax=ax2, color='C0')\n", "ax2.plot(df[x], df['Grpmean'], 'o', color='C3')\n", "ax2.set_ylim([0.7, 3.4])\n", "ax2.set_xlim([-0.4, 2.3])\n", "ax2.set_xticks(range(3))\n", "\n", "fig.tight_layout()\n", "sns.despine(bottom=0, left=0)\n", "plt.show()"], "cell_type": "code"}, {"metadata": {"_cell_guid": "6eeda713-aa12-4ab5-9c03-8da6e10d87ad", "_uuid": "cedb7930379e63e8f2b42b57cc4229ab05d8e29d"}, "source": ["## Data analysis\n", "#### Machine learning: random forest\n", "We have cleaned the data, created some new features and filled in missing values. After that, we explored the variables to get a good sense of what is going on. Now we are ready to predict the missing survival values of 418 Titanic passengers.  \n", "<br>\n", "To classify survival, we will use the machine learning technique called Random Forest. The idea of a random forest is quite simple. It is a collection of many single decision trees. For every data sample, each tree predicts an outcome. The votes of all the trees are tallied up and the final prediction is whichever outcome was voted for the most.   \n", "<br>\n", "First, we split our dataframe back into the original train and test set that we downloaded. Then we fit the random forest to our train data.   \n", "<br>\n", "Just out of interest, let's also visualize one of the trees. Each tree is different from another because they are based on random subsets of the train data. How is a tree built? At each node, the goal is to minimize impurity/uncertainty. So, the data is split on some value of a feature and the resulting subsets should contain as much as possible of Survival=yes class and as little of Survival=no (the reverse for the other subset). To determine where to split the data, the Gini index is used. The Gini index essentially gives the probability of guessing the wrong class within the subsets created by splitting the data on that particular value. If the subsets are completely pure, you have Gini of 0. If they are both 50/50 Gini is 1.    \n", "<br>\n", "Open the image in a new tab to view it in readable resolution.\n"], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "66a346bc-f109-40f1-aecb-bcdd708def48", "collapsed": true, "_uuid": "af4218c40ea3e44c3630d4c3509c4ac8c452c1b4"}, "execution_count": null, "outputs": [], "source": ["# Split into train/test\n", "feats = ['Sex', 'Age', 'AgeGrp', 'AgeDec', 'PersonType', 'Embarked', 'Pclass', 'Fare', 'Title',\n", "         'Parch', 'SibSp', 'FamSize', 'FamSize2', 'GrpSize', 'GrpSize2', 'Deck', 'CabinLoc'] \n", "\n", "dff = df[['PassengerId', 'Set', 'Survived'] + feats]\n", "\n", "train = dff[dff['Set'] == 'train'].drop('Set', 1)\n", "test = dff[dff['Set'] == 'test'].drop('Set', 1)\n", "\n", "trainx = train[feats].values\n", "trainy = train['Survived'].values.astype(int)\n", "\n", "testx = test[feats].values"], "cell_type": "code"}, {"metadata": {"_cell_guid": "a094a01c-0cb8-4a02-acdc-d093026777a5", "_uuid": "e41f516bd7f89ebe4af66be090c667170e3592fc"}, "execution_count": null, "outputs": [], "source": ["# Fit random forest to the data\n", "from sklearn.ensemble import RandomForestClassifier\n", "\n", "clf = RandomForestClassifier(n_estimators=500, verbose=0, random_state=1)\n", "clf.fit(trainx, trainy)\n"], "cell_type": "code"}, {"metadata": {"_cell_guid": "c3318e70-605f-4798-a39f-da1ef8ef7025", "collapsed": true, "_uuid": "116afec3456bf40313ba8f98ce7be3db22ac2b2e"}, "execution_count": null, "outputs": [], "source": ["# Visualize a tree\n", "import graphviz \n", "from sklearn import tree\n", "\n", "dotdata = tree.export_graphviz(clf.estimators_[0], out_file=None, feature_names=feats, filled=True, \n", "                               rounded=True, class_names=True, special_characters=False, \n", "                               leaves_parallel=False)  \n", "# graphviz.Source(dotdata)\n", "# I uploaded a .png image instead, for viewing convenience\n"], "cell_type": "code"}, {"metadata": {"_cell_guid": "f7601726-4242-4817-a04f-44fde46fafce", "_uuid": "14d9c895d54c64e6fdc979a324a6787ebb9b1306"}, "source": ["\n", " ![tree]](http://i.imgur.com/LINsqUm.png)"], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "d96eadba-3942-47a1-a467-fc29df93d235", "_uuid": "5ce5f450efd11d86e9c41d7e6e1c21fd5273389e"}, "source": ["#### Confusion Matrix\n", "The first thing to look at for describing the performance of our model would be the confusion matrix. The primary measure of interest is the accuracy of the model. This tells us how often our model predicts the correct class. Quite good with 82%.  \n", "Other measures that are generally of interest are sensitivity and specificity. Sensitivity tells us how often the model is correct when the actual value is yes (is it \"sensitive\" enough to detect the disease, for example). Likewise, specificity tells us how often the model is correct when the actual value is no (a test can be perfectly \"sensitive\" by always predicting that a disease occurs, but that is not \"specific\" enough). "], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "1030591a-ed90-44c8-b77e-540856e22c59", "_uuid": "0ff7fde1717d19474bd8bee2ce4e55e9c0241614"}, "execution_count": null, "outputs": [], "source": ["# Plot confusion matrix\n", "from sklearn.metrics import confusion_matrix\n", "\n", "xtrain, xtest, ytrain, ytest = train_test_split(trainx, trainy, test_size=.2, random_state=0)\n", "clf.fit(xtrain, ytrain)\n", "\n", "confm = confusion_matrix(ytest, clf.predict(xtest))\n", "confm = confm.astype(float)\n", "#confm = confm/len(ytest)\n", "sns.heatmap(confm, annot=True, fmt=\".2f\", square=1, cmap='Blues', vmin=0, vmax=100)\n", "\n", "plt.title('Confusion matrix')\n", "plt.xlabel('predicted survival')\n", "plt.ylabel('actual survival')\n", "plt.xticks([0.5,1.5],['no','yes'])\n", "plt.yticks([0.5,1.5],['no','yes'])\n", "plt.show()\n", "\n", "print('Accuracy: ' + str((confm[0,0] + confm[1,1]) / np.sum(confm))) # (true neg + true pos) / total\n", "print('Sensitivity: ' + str(confm[1,1] / np.sum(confm, 1)[1])) # true pos / actual pos\n", "print('Specificity: ' + str(confm[0,0] / np.sum(confm, 1)[0])) # true neg / actual neg"], "cell_type": "code"}, {"metadata": {"_cell_guid": "ab9892e2-8117-42d8-ac1e-9cf3ae2e1353", "_uuid": "81d05aa89f3986ec30b5eb9e0033f346bb10ac6e"}, "source": ["#### ROC Curve and AUC\n", "More info can be extracted from the confusion matrix and this is best plotted as the ROC curve. Namely, the false positive rate (fpr) and the true positive rate (tpr). The tpr is the same as sensitivity (if actually yes, does it predict yes?), and the fpr is simply 1-specificity (if actually no, does it predict yes?).   \n", "\n", "Imagine two, somewhat overlapping, normal distributions, one for actually positive and negative samples. You have to set a classifying threshold between the distributions: all samples above the threshold are classified as positive by your model, below the threshold are negative. Most naturally you would make the distinction where the distributions meet. Because they overlap, some samples end up at the wrong side of the threshold thus getting classified wrongly. This way, the fpr and tpr are balanced. However, perhaps it is very important to e.g. minimize false positives. You could then shift the threshold beyond the distribution with negative samples. No false positives, at the cost of less true positives. The ROC curve shows the tpr/fpr balance of many thresholds.    \n", "\n", "The diagonal line is where the tpr and fpr are equal, meaning it's just 50/50 guessing. The further towards the top left corner the ROC curve goes, the better the classifier is. This can be summarized in one number; the area under the curve (AUC). AUC of 1 is perfect, and 0.5 means pure guessing.   \n", "\n", "In our case: the ROC looks good and the AUC is 0.87. This means our model is actually pretty good at discriminating between survivors and victims. While AUC and accuracy are numbers with similar meaning, AUC is generally better because accuracy can be misleading in the case of very imbalanced true positives and true negatives). \n"], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "dca6908f-fbd8-4a9c-852f-87ff840c759b", "_uuid": "b36fc9659651d2b9a11f5e802e3d05d2617c7443"}, "execution_count": null, "outputs": [], "source": ["# Plot ROC Curve\n", "from sklearn.metrics import roc_curve, auc\n", "from sklearn.metrics import roc_auc_score\n", "\n", "xtrain, xtest, ytrain, ytest = train_test_split(trainx, trainy, test_size=.2, random_state=0)\n", "clf.fit(xtrain, ytrain)\n", "\n", "fpr, tpr, th = roc_curve(ytest, clf.predict_proba(xtest)[:,1])\n", "\n", "plt.figure(figsize=[4,4])\n", "plt.plot(fpr, tpr)\n", "plt.plot([0, 1], [0, 1], 'k--')\n", "\n", "plt.title('ROC curve')\n", "plt.xlabel('false pos rate')\n", "plt.ylabel('true pos rate')\n", "sns.despine(left=False, bottom=False)\n", "plt.show()\n", "\n", "print('AUC: ' + str(roc_auc_score(ytest, clf.predict_proba(xtest)[:,1])))"], "cell_type": "code"}, {"metadata": {"_cell_guid": "9e216a85-dbcd-4b30-b81e-a5e4b1e2d61a", "_uuid": "1a8a08ae2b0571e935c66c427ad896f70eac369c"}, "source": ["#### Learning Curves\n", "In order to see how the number of samples affect the model, we can create learning curves. Ideally, the curve of the train set starts with a fair amount of error, which decreases as the number of training samples increases until the error plateaus. To test the generalization of the model, it is cross validated with a test set. With low number of samples you would want a high error, which subsequently converges towards the training curve. \n", "\n", "Our train set has a near perfect score already with a small training set. The test set does not have a great score and is not really improving with increasing samples. This is a clear-cut case of overfitting: the training set gets a very high score, but it does not generalize well to the test set."], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "cad98110-0bda-4ba2-8c51-64fa04826dd4", "collapsed": true, "_uuid": "ed08cc26cca2af7670cf3eea412a83d6ca47a6b0"}, "execution_count": null, "outputs": [], "source": ["# Get learning curve data (can take minutes)\n", "from sklearn.model_selection import learning_curve\n", "\n", "train_sizes, train_scores, test_scores = learning_curve(\n", "    clf, trainx, trainy, train_sizes = np.linspace(0.1, 1.0, 10), cv=5, verbose=0) #10 steps, cv=5"], "cell_type": "code"}, {"metadata": {"_cell_guid": "bf9dd736-4614-4f44-8592-1e32078dc0de", "_uuid": "e09d0ad31db5c280b6dc360d3ddd4dda06823bdf"}, "execution_count": null, "outputs": [], "source": ["# Plot the learning curves\n", "plt.figure(figsize=[4,3])\n", "sns.tsplot(test_scores.transpose(), time=train_sizes, ci=95, color='C1', marker='o')\n", "sns.tsplot(train_scores.transpose(), time=train_sizes, ci=95, color='C0', marker='o')\n", "plt.ylim((0.6, 1.01))\n", "plt.gca().invert_yaxis()\n", "plt.legend(['test','train'])\n", "leg = plt.gca().get_legend()\n", "leg.legendHandles[0].set_alpha(1)\n", "leg.legendHandles[1].set_alpha(1)\n", "plt.xlabel('N of samples')\n", "plt.ylabel('Score')\n", "plt.title('Learning curves')\n", "sns.despine(left=False, bottom=False)\n", "plt.show()"], "cell_type": "code"}, {"metadata": {"_cell_guid": "38928de3-202c-468d-b1e0-5f3a0cfb3546", "_uuid": "56b0746977fe7374b7d785031f7833a2e770daa4"}, "source": ["#### Feature importance\n", "Feature importances are a very interesting aspect of random forests. We can check which features contribute the most to our model. The decrease in impurity after splitting on each feature is averaged over all trees in the forest. We then plot them, preferably sorted, to easily compare the features.   \n", "In our model, Age and Fare are clearly very important features. Title, PersonType and Sex are also important, but they have huge standard deviations. This is because all three of them essentially split on male/female. This leads to one of them being the root node in the tree, leaving the other 2 with very low importance. The remaining features are not all that useful anymore. Perhaps GrpSize is meaningful since it is not explained by Age/Fare/Sex. "], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "5192c32f-cf13-48cf-a422-ff8ff571f1e5", "_uuid": "e843ce7bce91ebb64dbecf0ddb49b649dbda8466"}, "execution_count": null, "outputs": [], "source": ["# Plot feature importances\n", "featstats = pd.DataFrame(feats, columns=['featnames'])\n", "featstats['featimp'] = clf.feature_importances_\n", "featstats['featstd'] = np.std([tree.feature_importances_ for tree in clf.estimators_], axis=0)\n", "featstats = featstats.sort_values('featimp', ascending=True)\n", "\n", "plt.figure(figsize=[8,6])\n", "xerr=featstats['featstd']\n", "plt.barh(range(len(featstats)), featstats['featimp'], color=sns.color_palette()[0], xerr=xerr)\n", "plt.yticks(range(len(featstats)), featstats['featnames'])\n", "\n", "plt.xlabel('Decrease in impurity')\n", "plt.title('Feature importances')\n", "sns.despine(left=False, bottom=False)\n", "plt.show()"], "cell_type": "code"}, {"metadata": {"_cell_guid": "21412297-af83-4318-8e74-a81d674cd4de", "_uuid": "a3844c3c68704e8154d8e2406b48940237b13231"}, "source": ["## Exporting survival predictions\n", "All there is left to do now, is exporting our predictions for the unknown Survival data. A dataframe is properly formatted according to Kaggle's standards and then saved as a .csv file. "], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "95fe67a0-6af0-40f3-9d98-1def25c69298", "collapsed": true, "_uuid": "3bcb34175dcb8feed824c5e84fb9b4729d1d88cb"}, "execution_count": null, "outputs": [], "source": ["# Set proper format and export for kaggle submission\n", "result = pd.DataFrame(index=test['PassengerId'])\n", "result['Survived'] = clf.predict(testx)\n", "\n", "#result.to_csv('prediction.csv')"], "cell_type": "code"}, {"metadata": {"_cell_guid": "003308a8-7caa-4f08-b6da-d8a9c1e4d1cc", "_uuid": "f20935d768f1dcf842d2d17521e77a73aa880e1a"}, "source": ["## Results\n", "After submitting to Kaggle, our model predicted 76.5% of the unknown set correct. Not bad! Pure guessing would be 50%. Simply setting all to 'not survived' would have lead to about 62% correct because 62% in the known set died. Our 76.5% is significantly better than that, but also quite a bit less than expected from the AUC (87%). Since so many of the features were related to each other, a simpler model could do just as good, if not better. I quickly ran the analysis with only the features Title, Fare, Age and GrpSize. The AUC was similar (85%) but the Kaggle score was a tiny bit improved to 77%. Feature importances of Title, Fare and Age (in that order) were similar, with GrpSize a bit behind. \n", "\n"], "cell_type": "markdown"}, {"metadata": {"_cell_guid": "f06ddda5-abc1-448b-be93-220e8f2f8841", "_uuid": "05688b191047bde33b8cdd0976d897dd9add4ad7"}, "source": [], "cell_type": "markdown"}], "nbformat_minor": 1, "nbformat": 4}