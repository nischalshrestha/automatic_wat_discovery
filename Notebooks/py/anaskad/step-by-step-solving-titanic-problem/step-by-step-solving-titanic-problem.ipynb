{"metadata": {"language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "nbconvert_exporter": "python", "mimetype": "text/x-python", "pygments_lexer": "ipython3", "name": "python", "file_extension": ".py", "version": "3.6.1"}, "kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}}, "nbformat": 4, "nbformat_minor": 0, "cells": [{"metadata": {"_uuid": "d7b440678586b6126557b47ec25ff256f5fc730e", "collapsed": false, "_cell_guid": "05ce2c27-928a-4443-b055-e300583ef4ca", "_execution_state": "idle"}, "source": "**A small road map on solving this problem**", "outputs": [], "execution_count": null, "cell_type": "markdown"}, {"metadata": {"_uuid": "8bae4b9ff6f2671e38f5e3fa19f8e937af08687a", "collapsed": false, "_cell_guid": "d9062ad7-8743-467f-9ec9-ec0fc1c74f67", "_execution_state": "idle"}, "source": "1st you should browse the data,  check the size, the nature of it, and read the requirements.", "outputs": [], "execution_count": null, "cell_type": "markdown"}, {"metadata": {"_uuid": "c741bfd6ba2c171b3606d30478f3a583445655e5", "collapsed": false, "_cell_guid": "862573a8-4d17-4f55-9c1e-711185278afa", "_execution_state": "idle"}, "source": "2nd you need to to load the data, ", "outputs": [], "execution_count": null, "cell_type": "markdown"}, {"metadata": {"_uuid": "2b80e536aadf68948ad2d98f4936a003178d9154", "collapsed": false, "_cell_guid": "cd5ee564-1989-425b-9f4d-9aeacdf95c43", "_execution_state": "idle"}, "source": "3rd do a fast cleaning for the data to be able to implement a fast learning algorithm,", "outputs": [], "execution_count": null, "cell_type": "markdown"}, {"metadata": {"_uuid": "370f7c2e94023769a91a033ea3808e6a0cba8b25", "collapsed": false, "_cell_guid": "0e0ad441-8308-4f2a-86af-916e2174a2ab", "_execution_state": "idle"}, "source": "4th after getting the first result, even if it is a dirty implementation, it will be a good start to go through the problem again check the data cleaning, feature engineering needed, tweak the parameters , try another algorithm to compare the results.", "outputs": [], "execution_count": null, "cell_type": "markdown"}, {"metadata": {"_uuid": "727f065bd5362dcb3321b2f358cb8a78bc84ca6f", "trusted": false, "_cell_guid": "53544c84-5d59-464c-b9ad-670df755f2a1", "_execution_state": "idle"}, "source": "# Import the needed referances\nimport pandas as pd\nimport numpy as np\nimport csv as csv\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\n\n#Shuffle the datasets\nfrom sklearn.utils import shuffle\n\n#Learning curve\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import ShuffleSplit\n\n#import seaborn as sns\n#Output plots in notebook\n#%matplotlib inline ", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "4cbeff31a28660f5e34a48657d4fe102a1c062ca", "collapsed": false, "trusted": false, "_cell_guid": "29134188-3f6d-490f-9b49-d55de341fded", "_execution_state": "idle"}, "source": "#loading the data sets from the csv files\nprint('--------load train & test file------')\ntrain_dataset = pd.read_csv('../input/train.csv')\ntest_dataset = pd.read_csv('../input/test.csv')\nprint(\"------finish loading --------------------\")", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "68ad42d3818f554d88a1308c547e98206b71c1a1", "collapsed": false, "trusted": false, "_cell_guid": "84133a1d-81a7-4568-a62d-b2392dfabbc2", "_execution_state": "idle"}, "source": "# print data sets information \nprint('----train dataset information-------')\ntrain_dataset.info()\nprint('----test dataset information--------')\ntest_dataset.info()\nprint(\"------------------------------------\")", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "117c47d78c776b36c0749a1810d966a046222aa7", "collapsed": false, "trusted": false, "_cell_guid": "cc04a835-15e0-487d-80b4-a1e5fea44479", "_execution_state": "idle"}, "source": "#Check for missing data & list them \nnas = pd.concat([train_dataset.isnull().sum(), test_dataset.isnull().sum()], axis=1, keys=['Train Dataset', 'Test Dataset'])\nnas[nas.sum(axis=1) > 0]", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "3304f08e385320c48a86363ae5064bb722ade8da", "collapsed": false, "trusted": false, "_cell_guid": "9cfb76cd-b88c-4953-94b9-b744b1423f04", "_execution_state": "idle"}, "source": "# Data sets cleaing, fill nan (null) where needed and delete uneeded columns\nprint('----Strat data cleaning ------------')\n\n#manage Age\ntrain_random_ages = np.random.randint(train_dataset[\"Age\"].mean() - train_dataset[\"Age\"].std(),\n                                          train_dataset[\"Age\"].mean() + train_dataset[\"Age\"].std(),\n                                          size = train_dataset[\"Age\"].isnull().sum())\n\ntest_random_ages = np.random.randint(test_dataset[\"Age\"].mean() - test_dataset[\"Age\"].std(),\n                                          test_dataset[\"Age\"].mean() + test_dataset[\"Age\"].std(),\n                                          size = test_dataset[\"Age\"].isnull().sum())\n\ntrain_dataset[\"Age\"][np.isnan(train_dataset[\"Age\"])] = train_random_ages\ntest_dataset[\"Age\"][np.isnan(test_dataset[\"Age\"])] = test_random_ages\ntrain_dataset['Age'] = train_dataset['Age'].astype(int)\ntest_dataset['Age']    = test_dataset['Age'].astype(int)\n\n# Embarked \ntrain_dataset[\"Embarked\"].fillna('S', inplace=True)\ntest_dataset[\"Embarked\"].fillna('S', inplace=True)\ntrain_dataset['Port'] = train_dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\ntest_dataset['Port'] = test_dataset['Embarked'].map({'S': 0, 'C': 1, 'Q': 2}).astype(int)\ndel train_dataset['Embarked']\ndel test_dataset['Embarked']\n\n# Fare\ntrain_dataset[\"Fare\"].fillna(train_dataset[\"Fare\"].median(), inplace=True)\ntest_dataset[\"Fare\"].fillna(test_dataset[\"Fare\"].median(), inplace=True)\n\ntrain_dataset['Fare']    = train_dataset['Fare'].astype(int)\ntest_dataset['Fare'] = test_dataset['Fare'].astype(int)\n\n# Map Sex to a new column Gender as 'female': 0, 'male': 1\ntrain_dataset['Gender'] = train_dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\ntest_dataset['Gender'] = test_dataset['Sex'].map({'female': 0, 'male': 1}).astype(int)\n# Delete Sex column from datasets\ndel train_dataset['Sex']\ndel test_dataset['Sex']\n\n# Delete Ticket column from datasets  (No need for them in the analysis)\ndel train_dataset['Ticket']\ndel test_dataset['Ticket']\n\n# Cabin has a lot of nan values, so i will remove it\ndel train_dataset['Cabin']\ndel test_dataset['Cabin']", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "fd17b09bdbb4e95aa6a668c1fea3a6819b32b050", "collapsed": false, "_cell_guid": "90849a2d-72af-4d4c-bf36-13d59aaa4a66", "_execution_state": "idle"}, "source": "** Engineer New Features **", "outputs": [], "execution_count": null, "cell_type": "markdown"}, {"metadata": {"_uuid": "1720e3fe62ca0cfdaccbf44070201f3d19381ed1", "collapsed": false, "trusted": false, "_cell_guid": "73e20c44-af4d-47df-bab9-6787495e6b5e", "_execution_state": "idle"}, "source": "# engineer a new Title feature\n# Get titles from the names\ntrain_dataset['Title'] = train_dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\ntest_dataset['Title'] = test_dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\n# group them\nfull_dataset = [train_dataset, test_dataset]\nfor dataset in full_dataset:\n    dataset['Title'] = dataset['Title'].replace(['Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev'], 'Officer')\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Sir', 'Jonkheer', 'Dona'], 'Royal')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n\n# Get the average survival rate of different titles\ntrain_dataset[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()\n\n##engineer the family size feature\nfor dataset in full_dataset:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n    \n## Create new column \"FamilySizeGroup\" and assign \"Alone\", \"Small\" and \"Big\"\nfor dataset in full_dataset:\n    dataset['FamilySizeGroup'] = 'Small'\n    dataset.loc[dataset['FamilySize'] == 1, 'FamilySizeGroup'] = 'Alone'\n    dataset.loc[dataset['FamilySize'] >= 5, 'FamilySizeGroup'] = 'Big'\n\n## Get the average survival rate of different FamilySizes\ntrain_dataset[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean()\n\n## engineer a new ischild feature\nfor dataset in full_dataset:\n    dataset['IsChild'] = 0\n    dataset.loc[dataset['Age'] <= 10, 'IsChild'] = 1\n    \nfor dataset in full_dataset:\n    dataset.loc[(dataset['Age'] <= 10), 'AgeGroup'] = 1\n    dataset.loc[(dataset['Age'] > 50) & (dataset['Age'] <= 60), 'AgeGroup'] = 2\n    dataset.loc[(dataset['Age'] > 30) & (dataset['Age'] <= 40), 'AgeGroup'] = 3\n    dataset.loc[(dataset['Age'] > 40) & (dataset['Age'] <= 50), 'AgeGroup'] = 4\n    dataset.loc[(dataset['Age'] > 10) & (dataset['Age'] <= 20), 'AgeGroup'] = 5\n    dataset.loc[(dataset['Age'] > 20) & (dataset['Age'] <= 30), 'AgeGroup'] = 6\n    dataset.loc[(dataset['Age'] > 70) & (dataset['Age'] <= 80), 'AgeGroup'] = 7\n    dataset.loc[(dataset['Age'] > 60) & (dataset['Age'] <= 70), 'AgeGroup'] = 8\n    \n# Convert to integer type\n#full_dataset['AgeGroup'] = full_dataset['AgeGroup'].astype(int)\n#full_dataset['AgeGroup'] = full_dataset['AgeGroup'].astype(int)\n\n    ", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "b970c7f9fb7c4691af688d645e323f85b4f935f4", "collapsed": false, "trusted": false, "_cell_guid": "c972a955-a0c0-4cf6-bf20-5a2966a43848", "_execution_state": "idle"}, "source": "# map the new features\ntitle_mapping = {\"Mr\": 0, \"Officer\": 1, \"Master\": 2, \"Miss\": 3, \"Royal\": 4, \"Mrs\": 5}\nfamily_mapping = {\"Small\": 0, \"Alone\": 1, \"Big\": 2}\nfor dataset in full_dataset:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['FamilySizeGroup'] = dataset['FamilySizeGroup'].map(family_mapping)\n    \n\n# Delete Name column from datasets (No need for them in the analysis)\ndel train_dataset['Name']\ndel test_dataset['Name']\n\ndel train_dataset['SibSp']\ndel test_dataset['SibSp']\n\ndel train_dataset['Parch']\ndel test_dataset['Parch']\n\ndel train_dataset['FamilySize']\ndel test_dataset['FamilySize']\n\nprint('----Finish data cleaning ------------')", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "118c2b346dce8d397f3cd1df3a46a78b689c877d", "collapsed": false, "trusted": false, "_cell_guid": "bdf01b13-7a42-432c-ac51-da34a6d53728", "_execution_state": "idle"}, "source": "train_dataset.head()", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "46ab8f33947351bf104855cd27578bb6b50ba574", "collapsed": false, "trusted": false, "_cell_guid": "4c7f3797-5fea-4ebb-a16f-34c5a4e81119", "_execution_state": "idle"}, "source": "##Shuffling the datasets\n#train_dataset = shuffle(train_dataset)\n#test_dataset = shuffle(test_dataset)\n#print('Finish Shuffling the datasets')", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "c4c6963558c73f499c884c06c5e8840676ce1078", "collapsed": false, "trusted": false, "_cell_guid": "643f5a05-670b-445f-961b-41951ab3b4c7", "_execution_state": "idle"}, "source": "# create a validation data set arround 20 % \ntrain_dataset, valid_dataset = np.split(train_dataset.sample(frac=1), [int(.8*len(train_dataset))])\nprint(len(train_dataset))\nprint(len(valid_dataset))", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "5ee885be0c2425ccdbc423545bea8f4be042c8c5", "collapsed": false, "trusted": false, "_cell_guid": "7b19a752-4e96-40f2-ae19-1dbe0b320b91", "_execution_state": "idle"}, "source": "del train_dataset['PassengerId']\ndel valid_dataset['PassengerId']\n\nX_train = train_dataset.drop(\"Survived\",axis=1).as_matrix()\nY_train = train_dataset[\"Survived\"].as_matrix()\n\nX_val = valid_dataset.drop(\"Survived\",axis=1).as_matrix()\nY_val = valid_dataset[\"Survived\"].as_matrix()\n\nX_test  = test_dataset.drop(\"PassengerId\",axis=1).copy().as_matrix()\n\nprint(X_train.shape)\nprint(Y_train.shape)\nprint(X_val.shape)\nprint(Y_val.shape)\nprint(X_test.shape)", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "ba804030dde8b2a4bd98fed01ba2c5d052ebd3a2", "collapsed": false, "trusted": false, "_cell_guid": "d770f3e0-82d2-43e7-bedf-cf71f2362922", "_execution_state": "idle"}, "source": "# Learning curve\ncv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=0)\nlogreg_model = LogisticRegression(C=1)\ndef Learning_curve_model(X, Y, model, cv, train_sizes):\n\n    plt.figure()\n    plt.title(\"Learning curve\")\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n\n\n    train_sizes, train_scores, test_scores = learning_curve(model, X, Y, cv=cv, n_jobs=4, train_sizes=train_sizes)\n\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std  = np.std(train_scores, axis=1)\n    test_scores_mean  = np.mean(test_scores, axis=1)\n    test_scores_std   = np.std(test_scores, axis=1)\n    plt.grid()\n    \n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",label=\"Cross-validation score\")\n                     \n    plt.legend(loc=\"best\")\n    return plt\n\nplot_lc      = 1   # 1--display learning curve/ 0 -- don't display\n\n#learn curve\nif plot_lc==1:\n    train_size=np.linspace(.1, 1.0, 15)\n    Learning_curve_model(X_train,Y_train , logreg_model, cv, train_size)", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "98699cb1cabecf245bff9f353d104cc0c76f5cd0", "collapsed": false, "_cell_guid": "a686ec17-b366-4c2e-a1e2-ed9a674b6815", "_execution_state": "idle"}, "source": "##Fixing##: Adding features: Fixes high bias , Adding polynomial features: Fixes high bias,Decreasing \u03bb: Fixes high bias**", "outputs": [], "execution_count": null, "cell_type": "markdown"}, {"metadata": {"_uuid": "cbbc6de12470fae1b595b7b06d5869a06783df9f", "collapsed": false, "trusted": false, "_cell_guid": "b01d1ea1-f312-497e-a42f-ed965fb5d68c", "_execution_state": "idle"}, "source": "# Support Vector Machines\nsvc = SVC()\nsvc.fit(X_train, Y_train)\nY_pred = svc.predict(X_test)\n\nresult_val =svc.score(X_val, Y_val)\nresult_train = svc.score(X_train, Y_train)\nprint('taring score = %s , while validation score = %s' %(result_train , result_val))\n", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "15a01038a8e7df931a73ec83ee4262984ddd6625", "collapsed": false, "trusted": false, "_cell_guid": "fca33ad5-9696-4aca-b490-bc8d12f1ebf2", "_execution_state": "idle"}, "source": "# Logistic Regression\nlogreg = LogisticRegression(C=1) #(C=0.1, penalty='l1', tol=1e-6)\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict(X_test)\n\nresult_val =logreg.score(X_val, Y_val)\nresult_train = logreg.score(X_train, Y_train)\nprint('taring score = %s , while validation score = %s' %(result_train , result_val))", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "b250d05cbf6cf37c6ac8b566aa1085f4e8702b65", "collapsed": false, "_execution_state": "idle"}, "source": "# Random Forests\nrandom_forest = RandomForestClassifier(criterion='gini', \n                             n_estimators=700,\n                             min_samples_split=10,\n                             min_samples_leaf=1,\n                             max_features='auto',\n                             oob_score=True,\n                             random_state=1,\n                             n_jobs=-1)\n\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\n\nresult_val =random_forest.score(X_val, Y_val)\nresult_train = random_forest.score(X_train, Y_train)\nprint('taring score = %s , while validation score = %s' %(result_train , result_val))", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "26eeeb7bb46271c50ea92394b6d1a51bf61655f3", "collapsed": false, "trusted": false, "_cell_guid": "84a737f7-3543-417c-bda8-21ac8831fe27", "_execution_state": "idle"}, "source": "cof_df = pd.DataFrame(train_dataset.columns.delete(0))\ncof_df.columns = ['Features']\ncof_df[\"Coefficient Estimation\"] = pd.Series(logreg.coef_[0])\n\n#PRINT\nprint(cof_df)", "outputs": [], "execution_count": null, "cell_type": "code"}, {"metadata": {"_uuid": "4af68544abca59692cbdd7d90c2a45fb038647cf", "collapsed": false, "trusted": false, "_cell_guid": "bf11e1f7-1aae-400d-851c-bdfafd8a839b", "_execution_state": "idle"}, "source": "submission = pd.DataFrame({\n        \"PassengerId\": test_dataset[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\nsubmission.to_csv('titanic.csv', index=False)\nprint('Exported')", "outputs": [], "execution_count": null, "cell_type": "code"}]}