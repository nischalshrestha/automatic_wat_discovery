{"cells":[{"metadata":{"_uuid":"159d8d11a38875791a6244b3a90cf2f843846bee","_cell_guid":"7b09ea38-4b55-4bde-8cbf-2e1ffcb3924e"},"cell_type":"markdown","source":"Fist off - credit due to:  https://www.kaggle.com/sarvajna for the [simple apporach](https://www.kaggle.com/sarvajna/titanic-problem-a-simple-approach) kernel.  The notebook below is an even more simplified take on using logistic regression to solve the Titanic problem with great care taken to explain each and every line of code.  Code is also broken down into very small, easy to digest units.  Note: it does not perform as well since I am not addressing skewed data.  \n"},{"metadata":{"_kg_hide-input":true,"collapsed":true,"_kg_hide-output":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":false},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# -----------------\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\n\n# ---------------\n# load data into pandas dataframe from .csv input flies provided by Kaggle\ntrain = pd.read_csv(\"../input/train.csv\") # load the train.csv data into the pandas dataframe called \"train\"\ntest = pd.read_csv(\"../input/test.csv\") # load the test.csv data into the pandas dataframe called \"test\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dbd7da796369801ac5f20e193cb29e3d83e7adcb","_cell_guid":"55d27647-e1cd-466e-b0f1-5552e6eb5c4b","trusted":false,"collapsed":true},"cell_type":"code","source":"train_shape = train.shape # get the shape (columns and rows) of the training data \ntrain_rows = train.shape[0] # get the number of rows from index zero\ntrain_cols = train.shape[1] # get the number of columns fro index one\nprint(\"Our training set has \" + str(train_rows) + \" rows\")\nprint(\"Our taining set has \" + str(train_cols) + \" columns\")\ntrain.head() # display the first few rows","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1eb2689edaba30efc326e895a0f3f03969308f03","_cell_guid":"0bbf0728-2377-4c30-8383-3987fa2236d7","trusted":false,"collapsed":true},"cell_type":"code","source":"test_shape = test.shape\ntest_rows = test.shape[0]\ntest_cols = test.shape[1]\nprint(\"Our test set has \" + str(test_rows) + \" rows\")\nprint(\"Our test set has \" + str(test_cols) + \" columns\")\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ee4fe499c8c6565e178243315f02710b924f4ff5","_cell_guid":"0a82d234-1dc3-4cce-b523-7486b89909d9","trusted":false,"collapsed":true},"cell_type":"code","source":"# here we concatenate the two sets \n# we do this so that our data transformations span both sets correctly\ncombined = pd.concat((train,test)) # combine the train and test dataframes together\n\n# just for testing combined.to_csv(\"combined.csv\")\n\ncombined_shape = combined.shape # get the shape of the dataframe\ncombined_rows = combined.shape[0] \ncombined_cols = combined.shape[1]\nprint(\"Our test set has \" + str(combined_rows) + \" rows\")\nprint(\"Our test set has \" + str(combined_cols) + \" columns\")\ncombined.head()\n","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"e1d23ba47489839e53845fb384e8b14f969c9cc8","_cell_guid":"93c2adfb-b7c0-463f-b752-5e9f2c5a02bc","trusted":false},"cell_type":"code","source":"# now that sets are combined we'll remove columns we that won't help us make predictions\nnarrowed = combined.drop(['Cabin', 'Name', 'Survived', 'Ticket', 'Parch', 'PassengerId'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"50924fe902a56619ba993b5455e30e11005766c8","_cell_guid":"32cfe425-980b-4cd2-ab72-fc74de68ff5d","trusted":false,"collapsed":true},"cell_type":"code","source":"# now we use get_dummies to convert categorical data into one-hot columns\nnarrowed = pd.get_dummies(narrowed)\nnarrowed.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e6a2b49f192475551039fed5e56f8ae6f0850283","_cell_guid":"edb3cb96-be36-42dc-992e-0e960ed2eb28","trusted":false,"collapsed":true},"cell_type":"code","source":"# now we fill in the blanks \nno_nulls = narrowed.fillna(narrowed.mean())\nno_nulls.head()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"fc3202bc9ca27dcf71085e385d941ae124141bf8","_cell_guid":"5bcc9c74-c0ed-4746-86a6-7e550de63531"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"d611741f83d48b0d5e442a94603833adaaa054bb","_cell_guid":"47450212-f254-4835-9b86-c0ab30fe7ad4","trusted":false,"collapsed":true},"cell_type":"code","source":"# load up the matixes required for fitting\nX_train = no_nulls[:train_rows]\nprint(\"X train: \" + str(X_train.shape))\nX_test = no_nulls[train_rows:]\nprint(\"X test: \" + str(X_test.shape))\ny = train.Survived","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"a9614c7fbdab15bcdf0c7ed8ab77e96f58571401","_cell_guid":"e3b463b5-3d09-4fd9-87bd-561aaf33d438","trusted":false},"cell_type":"code","source":"# load up classifiers\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.datasets import make_moons, make_circles, make_classification\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\nnames = [\"Logistic Regression\", \"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n         \"Naive Bayes\"]\n\nclassifiers = [\n    LogisticRegression(),\n    KNeighborsClassifier(3),\n    SVC(kernel=\"linear\", C=0.025),\n    SVC(gamma=2, C=1),\n    GaussianProcessClassifier(1.0 * RBF(1.0)),\n    DecisionTreeClassifier(max_depth=5),\n    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n    MLPClassifier(alpha=1),\n    AdaBoostClassifier(),\n    GaussianNB(),]\n\n#for name, clf in zip(names, classifiers):\n  #  clf.fit(X_train, y)\n   # accuracy = round(clf.score(X_train, y) * 100, 2)\n   # print(name, accuracy)\n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6f26738600ba7c2096d4211c7422649cbdb7147e","_cell_guid":"5695acaf-47a3-4d60-9590-23af722860bc","trusted":false,"collapsed":true},"cell_type":"code","source":"\n\nclf = RandomForestClassifier(max_depth=15, n_estimators=15, max_features=5)\nclf.fit(X_train, y)\naccuracy = round(clf.score(X_train, y) * 100, 2)\nprint(accuracy)\ny.describe(include = 'all')  # Pandas describe function, wow!\npredictions = clf.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"a9e36eab174685f9ad901458f9195ae0b4e44c05","_cell_guid":"39c72213-dd4f-4a82-ac86-2528ae00e762","trusted":false},"cell_type":"code","source":"solution = pd.DataFrame({\"PassengerId\":test.PassengerId, \"Survived\":predictions})\nsolution.to_csv(\"best_fit.csv\", index = False)","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.5","file_extension":".py","mimetype":"text/x-python","nbconvert_exporter":"python"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"nbformat":4,"nbformat_minor":1}