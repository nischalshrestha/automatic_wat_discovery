{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "d0b473e994222effcb37e8294721f167e1995be1", "collapsed": true, "_cell_guid": "82f17a7b-ec85-4079-9eb7-87a0f9ce0811"}, "outputs": [], "source": ["%matplotlib inline\n", "import pandas as pd\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import sklearn.manifold\n", "from sklearn.cross_validation import train_test_split\n", "import itertools\n", "import time\n", "import seaborn as sns\n", "sns.set_style(\"darkgrid\")\n", "sns.set_context('poster')"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "694c0b5b74e4e997ac8e7bb90b6d9601f2327959", "collapsed": true, "_cell_guid": "da00148d-9cad-4bc0-afd0-50cff8257812"}, "outputs": [], "source": ["# we dont use all the available features - e.g. number of siblings on board as we assume this will not affect survival\n", "df = pd.read_csv('../input/train.csv', usecols=['Survived','Pclass','Sex','Age','Fare'])\n", "df = df.fillna(df.mean())\n", "df = df.round()\n", "df = df.sample(frac=1).reset_index(drop=True)\n", "df.head()"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "ce8fa2dcebe728cf6631da5f70ad01bbff270b59", "collapsed": true, "_cell_guid": "781c91ba-7b47-4ee5-93fd-8027033e4c41"}, "outputs": [], "source": ["# % of those who survived\n", "sum(df.Survived)/len(df)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "8b1d54a0b0180a1bd56a1c7dac67397b655afcc1", "_cell_guid": "660715d5-0748-4977-8cf8-86071b9958c6"}, "source": ["# Fare / Class correlation?\n", "\n", "Lets plot and calculate the relationship between the fare and the class - we do this so we do not use redundant features"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "724bec7acfe5e057ed897509cdf7c556d84711f7", "collapsed": true, "_cell_guid": "01adae9d-173d-4a37-852e-c3723163a1d3"}, "outputs": [], "source": ["fares = np.array(list(df.Fare))\n", "classes = np.array(list(df.Pclass))\n", "\n", "fig, ax = plt.subplots(figsize=(10,7))\n", "ax.plot(classes,fares,'o',alpha=0.2)\n", "ax.set_xlabel('Class', fontsize=20)\n", "ax.set_ylabel('Fare', fontsize=20)\n", "ax.set_xticks([1,2,3])\n", "ax.tick_params(axis='both', which='major', labelsize=15)\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "c385660ac66f9eb6d0c5832338a638eeec1cf6f1", "collapsed": true, "_cell_guid": "cfd41429-999e-450f-8e3e-7b68ad58440a"}, "outputs": [], "source": ["from scipy.stats import pearsonr\n", "pearsonr(fares,classes)[0]"]}, {"cell_type": "markdown", "metadata": {"_uuid": "046ef260fa9c59799bc89b2cda425c131fe22bb8", "_cell_guid": "1080e341-4e84-4955-8537-0b08b12291b2"}, "source": ["There is a slight negative correlation, as the plot and the pearson correlation above shows, but not enough to justify to remove either the 'PClass' or 'Fare' feature."]}, {"cell_type": "markdown", "metadata": {"_uuid": "e29cdd37eb20d3d493ebb8701223760ba0a292ea", "_cell_guid": "51ee3caf-49be-4efd-84cc-61603f1723b0"}, "source": ["# One plot to rule them all...\n", "\n", "And in the darkness show how a 1st class woman was 7X more likely to surivive than a 3rd class man."]}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "bb46071251f088cfe5189f37142fb6cfc6484f8b", "collapsed": true, "_cell_guid": "5520556d-daa8-4941-a258-da19c1e128e6"}, "outputs": [], "source": ["df.groupby(['Pclass', 'Sex'])['Survived'].mean()"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "f9b2008a3b8466c2de15ed17485b7f925935c0de", "collapsed": true, "_cell_guid": "f6016bde-2852-416e-ae93-ec6e099ee855"}, "outputs": [], "source": ["df.groupby(['Pclass', 'Sex'])['Survived'].mean().unstack().plot(kind='bar',figsize=(13,6), fontsize=20, color=['r','b']);\n", "plt.ylabel('Survival Rate',fontsize=20)\n", "plt.title('Survival Rates aboard Titanic',fontsize=20);\n", "plt.xlabel('Class',fontsize=20);\n", "plt.xticks([0,1,2],rotation=0);"]}, {"cell_type": "markdown", "metadata": {"_uuid": "94b852ce050b4890201f958ec7d30244beae88d2", "_cell_guid": "c987cff6-9af3-4a10-b6e6-44ae237485ed"}, "source": ["... \"_women_ and children first please!\"\n", "\n", "_~Crew members loading lifeboats_\n", "\n", "We can see that there is a deffinite pattern here that our model should pick up on."]}, {"cell_type": "markdown", "metadata": {"_uuid": "7b5e5ae8f8d6294de7a6c4af9491d8f3eecd12ac", "_cell_guid": "e036c1fa-198e-458c-bfe1-8960f5b5bbfd"}, "source": ["# TSNE\n", "\n", "We will use TSNE on our dataset to visually inspect it and see if there are any clear clusters."]}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "e0f77e4364d23925e04a9214c285f582701dfc73", "collapsed": true, "_cell_guid": "db1d800b-331e-4ef2-9442-51f392bcd7df"}, "outputs": [], "source": ["# first OHE the gender feature\n", "df = pd.get_dummies(df, columns=['Sex'])\n", "X = df.as_matrix(columns=['Pclass','Age','Sex_female','Sex_male','Fare'])\n", "Y = df.as_matrix(columns=['Survived'])\n", "# normalize age and fare\n", "X[:,1] = (X[:,1] - X[:,1].min())/(X[:,1].max() - X[:,1].min())\n", "X[:,4] = (X[:,4] - X[:,4].min())/(X[:,4].max() - X[:,4].min())"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "45a469c2a1391f376d7d895cae15a035124f5ca1", "collapsed": true, "_cell_guid": "526b4296-a276-499b-ad46-9cbf555b8dc4"}, "outputs": [], "source": ["tsne = sklearn.manifold.TSNE(n_components=2, random_state=0, perplexity=15, n_iter=2000, n_iter_without_progress=1000)\n", "matrix_2d = tsne.fit_transform(X)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "8332ba092f4bd1ce9d22efd2b57d7ac4ebbf90cb", "collapsed": true, "_cell_guid": "171be566-61f8-444f-9bb0-57c79eaad5d5"}, "outputs": [], "source": ["colors = df.Survived.values\n", "colors = ['G' if i==1 else 'R' for i in colors]"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "f681dfe6ec1498d0283fa547ef5d8b97ce05f599", "collapsed": true, "_cell_guid": "f01ad51a-1a05-424b-ba34-fb522f6f3197"}, "outputs": [], "source": ["df_tsne = pd.DataFrame(matrix_2d)\n", "df_tsne['Survived'] = df['Survived']\n", "df_tsne['color'] = colors\n", "df_tsne.columns = ['x','y', 'Survived', 'color']\n", "# rearrange columns\n", "cols = ['Survived','color','x','y']\n", "df_tsne = df_tsne[cols]\n", "# show the 2D coordinates of the TSNE output\n", "df_tsne.head()"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "28570945284866fe219016d6b1b454831edb861e", "collapsed": true, "_cell_guid": "06923e19-4aaa-4e91-a45f-19074b6f1160"}, "outputs": [], "source": ["fig, ax = plt.subplots(figsize=(15,10))\n", "ax.scatter(df_tsne[df_tsne.Survived==1].x.values, df_tsne[df_tsne.Survived==1].y.values,\n", "           c='green', s=10, alpha=0.5, label='Survived')\n", "ax.scatter(df_tsne[df_tsne.Survived==0].x.values, df_tsne[df_tsne.Survived==0].y.values,\n", "           c='red', s=10, alpha=0.5, label='Died')\n", "ax.tick_params(axis='both', which='major', labelsize=15)\n", "ax.legend()\n", "plt.show();"]}, {"cell_type": "markdown", "metadata": {"_uuid": "4471d3bd8f041942d2caa8056d097af890778932", "_cell_guid": "e8fb5b78-c1dd-4916-8e0f-1f8d39cc1708"}, "source": ["We can see there are some clear patterns, although the distribution of points aren't perfectly seperable by some boundary - we see survivors mixed into groups of people who died, and vice versa, at least when reducing dimensionality to 2d.\n", "\n", "Given this, we know that any model wont perform perfeclty accurately on the training set without overfitting - we want the model to generalise well, so an accuracy of 70-90% on training and testing sets will be our goal."]}, {"cell_type": "markdown", "metadata": {"_uuid": "a112010140c4cdc71a3f7e76628eab2b97efbbdd", "collapsed": true, "_cell_guid": "73591a24-1e74-41db-bf6e-d4180223a18c"}, "source": ["# Neural Net\n", "\n", "We implement a 3 layer (1 input, 1 hidden, 1 output) neural network. First forward propagate input, then backpropagate error to update weights.\n", "\n", "The activation functions we use are Leaky ReLU for our hidden layer, and sigmoid for our output layer as it suits binary classification quite well. We implement [dropout](https://en.wikipedia.org/wiki/Dropout_(neural_networks) to regularize the model and prevent overfitting, and SGD to improve chances of finding a global minimum"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "114ffc55274731a1c8f10b13fe27a920e5b01bf1", "collapsed": true, "_cell_guid": "e6a833f4-c8ce-420b-8714-2e4689dc4b4b"}, "outputs": [], "source": ["data = df.as_matrix()\n", "# normalize age and fare\n", "data[:,2] = (data[:,2] - data[:,2].min())/(data[:,2].max() - data[:,2].min())\n", "data[:,3] = (data[:,3] - data[:,3].min())/(data[:,3].max() - data[:,3].min())\n", "X_train, X_test = train_test_split(data, test_size=0.1)\n", "Y_train = X_train[:,0] # first column is class\n", "Y_train = np.reshape(Y_train, newshape=(len(Y_train),1)) # reshape to a columns vector\n", "X_train = X_train[:,1:] # select all columns but class\n", "Y_test = X_test[:,0]\n", "Y_test = np.reshape(Y_test, newshape=(len(Y_test),1)) # reshape to a columns vector\n", "X_test = X_test[:,1:]"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "915b7492fd8a46910ba9c85611df3c06398fc29e", "collapsed": true, "_cell_guid": "2d08c60c-c505-4c4d-821a-b2f3d7152ced"}, "outputs": [], "source": ["def sigmoid(x, deriv=False):\n", "    \"\"\"\n", "    Sigmoid activation function\n", "    \"\"\"\n", "    if(deriv==True):\n", "        return (x*(1-x))\n", "    return 1/(1+np.exp(-x))"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "0280ec94ef280e5659a117d70919880622dd4676", "collapsed": true, "_cell_guid": "fc1f836f-6c69-43cc-91f5-f3a6bb8aaf6a"}, "outputs": [], "source": ["def relu(x, deriv=False):\n", "    \"\"\"\n", "    Leaky ReLU activation function\n", "    \"\"\"\n", "    if deriv == True:\n", "        x[x<0] = 0.01\n", "        x[x>0] = 1.\n", "        return x\n", "    x[x<0] = 0.01*x[x<0]\n", "    return x"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "a38045cd3ef396ca9aa05720d881a44c29708e19", "collapsed": true, "_cell_guid": "260fe880-1687-445f-bda5-0eebd9c98b04"}, "outputs": [], "source": ["def predict(x, w0, w1, b1, b2):\n", "    \"\"\"\n", "    Function to predict an output given a data x, weight matrices w1 & w1 and biases b1 & b2\n", "    \"\"\"\n", "    A = np.dot(x,w0) + b1 # mXN X NxH +1xH ~ mxH\n", "    layer_1 = relu(A)\n", "    B = np.dot(layer_1,w1) + b2 # mxH X Hx1 ~ mx1 (preds)\n", "    layer_2 = B\n", "    return (sigmoid(layer_2) > 0.5).astype(int)"]}, {"cell_type": "markdown", "metadata": {"_uuid": "7fcfb976c4ecee0eef39bdf2fc756fdaca42a827", "_cell_guid": "50ab0693-badf-4f6e-8723-f4ebafd05bde"}, "source": ["## Stochastic Gradient Descent (SGD)\n", "\n", "We iterate over a minibatch, calculate the error and update the weights at each iteration over a minibatch."]}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "f7d9b5a81abb03d7b8295d22000e673d475f00ee", "collapsed": true, "_cell_guid": "b131af6f-554d-4dbe-a60d-a98e572b2346"}, "outputs": [], "source": ["def get_batch(x,y,i,batchSize=32):\n", "    \"\"\"\n", "    Function that returns a minibatch of a dataset\n", "    \"\"\"\n", "    return x[i:i+batchSize],y[i:i+batchSize]"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "9d0da787f288c3daa62f3c278e232a6a37012c91", "scrolled": true, "collapsed": true, "_cell_guid": "ab602c43-d0e1-44de-bab0-a0fd05c78ee8"}, "outputs": [], "source": ["# learning rate, hidden layer dimension, dropout rate, batch size\n", "alpha, hidden_size, drop_rate, batch_size = (0.04,32,0.5,32)\n", "# randomly initialise synapses\n", "syn0 = 2*np.random.random((X_train.shape[1],hidden_size)) - 1 # NxH\n", "syn1 = 2*np.random.random((hidden_size,1)) - 1 # Hx1\n", "# randomly initialise biases\n", "b1 = np.random.randn(hidden_size) # 1xH\n", "b2 = np.random.randn(1) # 1x1\n", "avg_err = []\n", "\n", "for epoch in range(2000):\n", "    err = []\n", "\n", "    for i in range(int(X_train.shape[0]/batch_size)):\n", "\n", "        x,y = get_batch(X_train,Y_train,i,batch_size)\n", "\n", "        # Forward\n", "        layer_0 = x\n", "        A = np.dot(layer_0,syn0) + b1 # BxN X NxH ~ BxH\n", "        layer_1 = relu(A)\n", "        # drop out to reduce overfitting\n", "        layer_1 *= np.random.binomial([np.ones((len(x),hidden_size))],1-drop_rate)[0] * (1/(1-drop_rate))\n", "\n", "        B = np.dot(layer_1,syn1) + b2 # BxH X Hx1 ~ Bx1\n", "        layer_2 = sigmoid(B)\n", "\n", "        # Backprop\n", "        layer_2_error = layer_2 - y # Bx1\n", "        layer_2_delta = layer_2_error * sigmoid(layer_2,deriv=True) # Bx1 * Bx1 ~ Bx1\n", "\n", "        layer_1_error = np.dot(layer_2_delta,syn1.T) # Bx1 X 1xH ~ BxH\n", "        layer_1_delta = layer_1_error * relu(layer_1,deriv=True) # BxH * BxH ~ BxH\n", "\n", "        # update weights\n", "        syn1 -= alpha*np.dot(layer_1.T,layer_2_delta) # HxB X Bx1 ~ Hx1\n", "        syn0 -= alpha*np.dot(layer_0.T,layer_1_delta) # NxB X BxH ~ NxH\n", "\n", "        # update biases\n", "        m = len(y)\n", "        b2 -= alpha * (1.0 / m) * np.sum(layer_2_delta)\n", "        b1 -= alpha * (1.0 / m) * np.sum(layer_1_delta)\n", "\n", "        err.append(layer_2_error)\n", "\n", "    avg_err.append(np.mean( np.abs(err) ))\n", "    if epoch%500 == 0:\n", "        print(\"Epoch: %d, Error: %.8f\" % (epoch, np.mean( np.abs(err) )))"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "a6ba17f89e2236af9c8f6179495d16890eb320b4", "collapsed": true, "_cell_guid": "55a69fb6-d3ed-48f5-9681-5d3bf20bc1e6"}, "outputs": [], "source": ["# accuracy on training set\n", "100*(1-np.sum(np.abs(predict(X_train, syn0, syn1, b1, b2) - Y_train))/len(X_train))"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "332faeb76a478835b6f16d8e85ade26c6e28c2c5", "scrolled": true, "collapsed": true, "_cell_guid": "ae15afe9-a80b-4285-bbc1-f8f7b1b88d6a"}, "outputs": [], "source": ["# accuracy on test set\n", "100*(1-np.sum(np.abs(predict(X_test, syn0, syn1, b1, b2) - Y_test))/len(Y_test))"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "2f71335b074ccca942a42c1dd41a132d7bb708e2", "collapsed": true, "_cell_guid": "9a328ed2-5b5e-4a40-af5f-3a7f116a9d4b"}, "outputs": [], "source": ["fig,ax = plt.subplots(figsize=(15,8))\n", "ax.plot(np.arange(len(avg_err)), np.array(avg_err))\n", "ax.set_xlabel('Iteration', fontsize=18)\n", "ax.set_ylabel('Mean Error', fontsize=18)\n", "ax.tick_params(axis='both', which='major', labelsize=15)\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {"_uuid": "5c930ded64346f7a330f6be33304ceaeb15ef499", "_cell_guid": "8ec5a582-6430-4f9b-8bc3-b9fe85a3f215"}, "source": ["# Random Forest - Scikit\n", "\n", "We will compare our models, accruacy and speed, to a simple implementation of scikit-learns Random Forest class"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "a28b43b906e31f2c25f9eef502e656c994b41cb0", "collapsed": true, "_cell_guid": "d8600e45-3157-49f6-b00e-f2f2d4b7d3d1"}, "outputs": [], "source": ["from sklearn.ensemble import RandomForestClassifier"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "63074b62f0325c5dd17b8f64b507a277d5094481", "collapsed": true, "_cell_guid": "c6d8bd2a-9933-455a-97c7-fc93dbcb9078"}, "outputs": [], "source": ["clf = RandomForestClassifier(n_estimators=50,n_jobs=1,max_depth=10)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "91a662145aca399a8df9f8d14df4ba98ecca80c4", "collapsed": true, "_cell_guid": "9465f113-d9cc-49c3-8d12-b9484a4a6a41"}, "outputs": [], "source": ["model = clf.fit(X_train,Y_train.T[0])"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "910bc0cd6fd0744ee98546abcfc1a1361e7fcb3a", "collapsed": true, "_cell_guid": "4edf9eed-7911-436b-bd72-37b57129374f"}, "outputs": [], "source": ["# accuracy on training set\n", "100*model.score(X_train,Y_train.T[0])"]}, {"cell_type": "code", "execution_count": null, "metadata": {"_uuid": "7012def4aab91604832bf32cbb4dcb73ecf78d32", "collapsed": true, "_cell_guid": "f9abf32e-6b17-4afd-b8d8-857158cab4db"}, "outputs": [], "source": ["# accuracy on test set\n", "100*(1-np.sum(np.abs(model.predict(X_test) - Y_test.T[0]))/len(Y_test))"]}, {"cell_type": "markdown", "metadata": {"_uuid": "85462e8817f3a1736721664a73c0bb358f80b66a", "_cell_guid": "8ac9db49-8992-47e6-99cc-9ce6b606f3ad"}, "source": ["### Conclusion\n", "\n", "It seems that a simpler model, such as random forest, can out perform a 3 layer neural network, both in accuracy and speed. The NN takes about 10-60 seconds to train depending on the number of epochs, whereas the scikit RF implementation is almost instant.\n", "\n", "We can see that the accruacy on the RF is superior, on both the training and test sets, compared to the NN. This may be becuase there was not enough data for the NN to learn from, so a simpler model does better."]}], "nbformat_minor": 1, "metadata": {"language_info": {"file_extension": ".py", "name": "python", "pygments_lexer": "ipython3", "version": "3.6.3", "mimetype": "text/x-python", "nbconvert_exporter": "python", "codemirror_mode": {"name": "ipython", "version": 3}}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}, "nbformat": 4}