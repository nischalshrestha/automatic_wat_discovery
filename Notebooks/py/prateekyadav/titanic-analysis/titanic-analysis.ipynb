{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\nimport os\nprint(os.listdir(\"../input\"))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"053dde06515d33c7a8d406a375aed1239062297d"},"cell_type":"markdown","source":"**1. Let's explore the data first.**"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"scrolled":true,"_kg_hide-input":true,"_kg_hide-output":false},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/train.csv\")\ntest_df = pd.read_csv(\"../input/test.csv\")\n\nprint(train_df.head())\nprint(\"-\"*70)\nprint(test_df.head())\nprint(train_df.columns)\nprint(\"-\"*70)\nprint(test_df.columns)\nprint(\"-\"*70)\nprint(train_df.info())\nprint(\"-\"*70)\nprint(test_df.info())\nprint(\"-\"*70)\nprint(train_df.describe())\nprint(\"-\"*70)\nprint(test_df.describe())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"112093c16438a128d7722dd2ddb7985b35f58335"},"cell_type":"markdown","source":"**2. Checking about NaN values**"},{"metadata":{"trusted":true,"_uuid":"3d2fe840a6f6638d20dcc4a8ac21cd58319f8905"},"cell_type":"code","source":"null_values_in_train = pd.isnull(train_df).sum()\nnull_values_in_test = pd.isnull(test_df).sum()\n\nplt.subplot(1,2,1)\nplt.title(\"NaN values in training set\")\nnull_values_in_train.plot.bar()\nplt.subplot(1,2,2)\nplt.title(\"NaN values in test set\")\n\nnull_values_in_test.plot.bar()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"74da9bd97a5f4645e97779cecb33dd49ae4c2fb6"},"cell_type":"code","source":"#First check the Age distribution\nplt.subplot(2,1,1)\nsns.distplot(train_df.Age.dropna())\nplt.subplot(2,1,2)\nsns.distplot(test_df.Age.dropna())\nplt.show()\n\n#Check the mode & median of distirbution\nprint(\"Mode of Age in train_df is {}\".format(train_df.Age.dropna().mode()))\nprint(\"Median of Age in train_df is {}\".format(train_df.Age.dropna().median()))\nprint(\"Mode of Age in test_df is {}\".format(test_df.Age.dropna().mode()))\nprint(\"Median of Age in train_df is {}\".format(test_df.Age.dropna().median()))\n\n#fill NaN values with interpolation\ntrain_df[\"Age\"] = train_df.Age.interpolate()\ntest_df[\"Age\"] = test_df.Age.interpolate()\n\n#Now Check the distribution of Age\nplt.subplot(2,1,1)\nsns.distplot(train_df.Age, color = \"#388E3C\")\nplt.subplot(2,1,2)\nsns.distplot(test_df.Age, color = \"#388E3C\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c408ae5c9cadb3fa04514908335adbf60e8bc931"},"cell_type":"markdown","source":"**3. Feature Engineering**"},{"metadata":{"trusted":true,"_uuid":"b2c63c609ce0405c8df445f99081b3c1ec4c04c6"},"cell_type":"code","source":"#Let's see how many inputs features we have\nprint(train_df.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0a03dbce49db1a67ba4256d4f72ffef5dc6a7ca2"},"cell_type":"code","source":"#Pclass -> Feature Engineering\n\n#Training Set\n\n#its a categorical vairble (1, 2, 3)\n#change it into categorical variable\ntrain_df.Pclass = train_df.Pclass.astype(\"category\")\n\n#Testing Set\ntest_df.Pclass = train_df.Pclass.astype(\"category\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"834105575c7c974ff61d0ad936da40a63c8649c4"},"cell_type":"code","source":"#Name -> Feature Analysis\n\n#Training Set\n\nprint(train_df.Name)\n#though its text but we can see there are some common prefixes in each name, now we'll extract them.\ntrain_df[\"Name\"] = train_df.Name.str.extract(\".((Mrs?)|(Miss)|(Master)).\", expand = False)\ntrain_df.info()\n\n#As we can see there are 24 NaN values in 'Name_prefix' column.\n#First we'll map these values to int value then we'll fill the NaN values with interpolation.\ntrain_df[\"Name\"] = train_df[\"Name\"].dropna().map({\"Mr\":0, \"Mrs\":1, \"Miss\":2, \"Master\":3})\nprint(train_df.Name)\nsns.distplot(train_df.Name.dropna())\nplt.title(\"Name prefix distribution\")\nplt.legend()\nplt.show()\n\ntrain_df.Name.fillna(train_df[\"Name\"].mode()[0], inplace=True)\nprint(train_df.Name)\n\n#Testing Set\ntest_df.info()\ntest_df[\"Name\"] = test_df.Name.str.extract(\".((Mrs?)|(Miss)|(Master)).\", expand = False)\nprint(test_df.Name.isnull().sum()) # 7 null values\ntest_df.Name.fillna(train_df[\"Name\"].mode()[0], inplace=True)\ntest_df[\"Name\"] = test_df[\"Name\"].dropna().map({\"Mr\":0, \"Mrs\":1, \"Miss\":2, \"Master\":3})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4b8b54feb3340b2d45cde14b4127e49a55d633f2"},"cell_type":"code","source":"#Sex -> Feature Analysis\n\n#Training Set\n\nprint(train_df.Sex)\n#Chnage this into categorical dtype and map it to int values\ntrain_df[\"Sex\"] = train_df[\"Sex\"].map({\"male\": 0, \"female\": 1})\ntrain_df[\"Sex\"] = train_df[\"Sex\"].astype(\"category\")\nprint(train_df.Sex)\n\n#Testing Set\ntest_df[\"Sex\"] = test_df[\"Sex\"].map({\"male\": 0, \"female\": 1})\ntest_df[\"Sex\"] = test_df[\"Sex\"].astype(\"category\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f816db740432f433401ba8f1f1dc54ced37baa09"},"cell_type":"code","source":"#SibSP & Parch -> Feature Analysis\n\n#Training Set\nplt.subplot(2,1,1)\nplt.hist(train_df.SibSp)\nplt.subplot(2,1,2)\nplt.hist(train_df.Parch)\nplt.show()\n\n#We can create a new column called \"family_member\" by combining the these tow columns named \"SibSp\" & \"Parch\"\n#For that we need to check about NaN values in each column\nprint(train_df.Parch.isnull().sum()) # Zero NaN values\nprint(train_df.SibSp.isnull().sum()) #Zero NaN values\n\ntrain_df[\"family_member_no\"] = train_df[\"SibSp\"] + train_df[\"Parch\"]\n\n\n#Testing Set\ntest_df[\"family_member_no\"] = test_df[\"SibSp\"] + test_df[\"Parch\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f7fa9a87397090b8b5c47c546609e8f9cda41f73"},"cell_type":"code","source":"#Ticket -> Feature Analysis\n\n#Training Set\n\n#We need to first extract numeric value of Ticket from string, for that we'll create a function\ndef numeric_extract(string):\n    l = string.split()\n    return l[-1]\n\n#create new column named \"ticket\" by applying that function to training dataset\ntrain_df[\"ticket\"] = train_df[\"Ticket\"].apply(numeric_extract)\n\n\n#Testing set\ntest_df[\"ticket\"] = test_df[\"Ticket\"].apply(numeric_extract)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2735132cccf8465fd1396db699e520a64b1abb53"},"cell_type":"code","source":"#Fare -> Feature Analysis\n\n\n#Training Set\nprint(train_df.Fare.isnull().sum())\n#Its all good, so we'll keep as it is.\n\n#Testing Set\nprint(train_df.Fare.isnull().sum())\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"0fce3babcb627640454a3af9cda23c91c39661d7"},"cell_type":"code","source":"#Embarked -> Feature Analysis\n\n#Training Set\nprint(train_df.Embarked.isnull().sum()) # 2 NaN values\nprint(train_df.Embarked)\n\ntrain_df.Embarked.fillna(train_df.Embarked.mode()[0], inplace=True)\nprint(train_df.Embarked.isnull().sum()) # zero NaN values\n\n#let's change it into category dtype\ntrain_df[\"Embarked\"] = train_df[\"Embarked\"].astype(\"category\")\n\n\n#Map values to integer\ntrain_df[\"Embarked\"] = train_df[\"Embarked\"].map({\"S\":0, \"C\":1, \"Q\":2})\nprint(train_df.Embarked)\n\n#Testing Set\nprint(test_df.Embarked.isnull().sum()) # Zero NaN values\nprint(test_df.Embarked)\n\ntest_df[\"Embarked\"] = test_df[\"Embarked\"].astype(\"category\")\ntest_df[\"Embarked\"] = test_df[\"Embarked\"].map({\"S\":0, \"C\":1, \"Q\":2})\nprint(test_df.Embarked)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6fe6feaebd8e1adc77621bf8d6f33c525390e9e3"},"cell_type":"code","source":"#Now lets finalize final Feature set\n\n#Training Set\n#As we saw the column named \"Cabin\" have so much NaN values so we can drop it.\ntrain_df = train_df.drop([\"Cabin\", \"Ticket\"], axis=1)\ntrain_df\n\n\n#Testing Set\ntest_df =  test_df.drop([\"Cabin\", \"Ticket\"], axis=1)\ntest_df\n\n#Now Split it into Input/Output \nX_train = train_df[[\"PassengerId\", \"Pclass\", \"Name\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\", \"family_member_no\", \"ticket\"]]\nY_train = train_df[\"Survived\"]\n\nX_test = test_df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aa506ec8c973e48ad3b430aab000433724258851"},"cell_type":"markdown","source":"**3.Visualisations & Relation b/w features**"},{"metadata":{"trusted":true,"_uuid":"588752145af55089f68b3c33279d4c1a51921ad4"},"cell_type":"code","source":"#First we create a copy of training set and testing set so that original copy wont affected.\ncopy_train_df = train_df.copy()\ncopy_test_df = test_df.copy()\ncopy_train_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4570aef56df6f8faafe8e1b47fbdf92032df615d"},"cell_type":"code","source":"#Lets see perterson corelation factor b/w all features\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\ncor = copy_train_df.corr()\nsns.heatmap(cor, xticklabels=copy_train_df.columns, yticklabels=copy_train_df.columns, cmap=cmap)\nplt.show()\n\n#Now lets visualize them","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7b049d39ebd5cc92ab981022347f07362461d645","scrolled":false},"cell_type":"markdown","source":"**Predictive Analysis**"},{"metadata":{"trusted":true,"_uuid":"375652c9480b05adf5daa0dd54e48103e0c5e589","scrolled":true},"cell_type":"code","source":"#Prepare I/p and O/p\n\ncopy_train_df = copy_train_df.apply(pd.to_numeric, errors = \"coerce\")\ncopy_test_df = copy_test_df.apply(pd.to_numeric, errors = \"coerce\")\n\ncopy_train_df.info()\ncopy_test_df.info()\n\ncopy_train_df.ticket.fillna(train_df[\"ticket\"].mode()[0], inplace=True)\ncopy_test_df.Name.fillna(train_df[\"Name\"].mode()[0], inplace=True)\ncopy_test_df.Fare.fillna(test_df[\"Fare\"].mode()[0], inplace=True)\n\n\nX_train = copy_train_df.iloc[:, [0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]]\nY_train = copy_train_df.iloc[:, 1]\nX_test = copy_test_df\n\n\nprint(\"columns in X_train is {} and columns in X_test is {}\".format(X_train.shape[1], X_test.shape[1])) #For confirmation whether we're going write or not.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a92edd99b3a8dd2f740b86cefca5d963ac5ef8a2"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import make_scorer, accuracy_score\nfrom sklearn.linear_model import LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3be5215fcbc38d68e3f2e8ae16b597192bb73a1a","scrolled":true},"cell_type":"code","source":"x_train, x_val, y_train, y_val = train_test_split(X_train, Y_train, test_size = 0.3, random_state = 21, stratify = Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"056176e2e7c126e13064fa59cbd6b8028f113768"},"cell_type":"code","source":"#KNighbourClassifier\n\nknn = KNeighborsClassifier(n_neighbors=6)\nknn.fit(x_train, y_train)\nknn_pred = knn.predict(x_val)\nknn_accuracy = accuracy_score(y_val, knn_pred)\nprint(\"Score of KNeighbourClassifier is {}\".format(knn_accuracy))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"038b635524aa3fbfc9e34d69f8c4de6a4243d094"},"cell_type":"code","source":"#LogisiticRegression\n\nlog_clf = LogisticRegression()\nlog_clf.fit(x_train, y_train)\nlogi_pred = log_clf.predict(x_val)\nlogisitic_accouracy = accuracy_score(y_val, logi_pred)\nprint(\"Score of Logisitic Regression is {}\".format(logisitic_accouracy))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cb64fcaecfd26510b80a09b8d02db5a7366ff247"},"cell_type":"markdown","source":"**Submission**"},{"metadata":{"trusted":true,"_uuid":"32145c4f61419fe054b1d9933db511bf972dcfab"},"cell_type":"code","source":"submission_predictions = log_clf.predict(X_test)\n\nsubmission = pd.DataFrame({\n                \"PassengerId\": X_test[\"PassengerId\"],\n                \"Survived\": submission_predictions\n})\n\nsubmission.to_csv(\"titanic.csv\", index = False)\nprint(submission.shape)\nprint(\"Submission Completed!\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}