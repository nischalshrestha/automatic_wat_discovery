{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "4a6a611c-87a5-89d4-c2ca-991650bdda9d"
      },
      "source": [
        "# RandomForest, Feature engineering, Imputation - Titanic survival\n",
        "## 1. Introduction\n",
        "\n",
        "In this exercise, I tried to use random forest algorithm to predict the survival of pessagnes based on selected features. A light level of data imputation and feature engineering was applied to increase prediction score. A few tricks like age imputation are inspired by Omar El Gabry at Kaggle. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "def5ad20-dc4e-675d-c18f-b8c9b73f8dd9"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "style.use('fivethirtyeight')\n",
        "%matplotlib inline\n",
        "\n",
        "# Load the dataset\n",
        "train_data = pd.read_csv(\"../input/train.csv\")\n",
        "test_data = pd.read_csv(\"../input/test.csv\")\n",
        "\n",
        "# Print the dataset information\n",
        "train_data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "de13de29-8f22-2485-b689-b45788300bfb"
      },
      "source": [
        "3 columns have null values. Cabin doesn't contain much information so I'll remove it. First of all, let's just ignore (delete) entries with NaN values. Later below I try to impute missing values in Age and Embarked, and see the difference in prediction by random forest. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "1915c36f-784e-7358-ac0a-186ca1cb3990"
      },
      "source": [
        "### 2. Data exploration\n",
        "Before starting, let's look at the dataset and see how different features have impact on whether a passenger survives. To reduce unnecessary features (such as Name, Ticket, PassengerID), select only the features that will improve predictability. Columns of SibSp and Parch will be combined into one column of Family (two columns have very similar distribution). Define a function that displays simple relationship between features and survival counts. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "074eda43-7880-4b30-8024-2634a083df5c"
      },
      "outputs": [],
      "source": [
        "### Select features that only make sense for survival prediction, SibSp and Parch are combined into Family\n",
        "train_df = train_data[['Name','Pclass','Sex','Age','SibSp','Parch','Fare','Embarked','Survived']]\n",
        "family = (train_df.SibSp + train_df.Parch).rename('Family')\n",
        "train_df = train_df.drop(['SibSp','Parch'],axis=1)\n",
        "train_df = train_df.join(family)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ae96e1c9-c18a-5c64-f28f-5377e8305e15"
      },
      "outputs": [],
      "source": [
        "### Define a fundtion for visualization of features against survival count\n",
        "def visualize(key, ax):\n",
        "    plt.sca(ax)\n",
        "    if key=='Age': ## In case of Age feature, many levels are grouped into bins\n",
        "        bins = [0,15,30,45,60,90]\n",
        "        grouped = train_df.groupby([pd.cut(train_df.Age, bins),'Survived'])['Survived'].count()\n",
        "    elif key=='Fare': ## in case of Fare feature, many levels are grouped into bins\n",
        "        bins = np.append(np.arange(0,90,10),np.array([100,1000]))\n",
        "        grouped = train_df.groupby([pd.cut(train_df.Fare, bins),'Survived'])['Survived'].count()\n",
        "    else: grouped = train_df.groupby([key,'Survived'])['Survived'].count()\n",
        "    \n",
        "    barwidth, offset = 0.3, 0.5\n",
        "    for ii in range(int(len(grouped)/2)):\n",
        "        not_sur = plt.bar(ii+offset-barwidth, grouped.iloc[ii*2], width=barwidth, color='grey', alpha=1)\n",
        "        sur = plt.bar(ii+offset, grouped.iloc[ii*2+1], width=barwidth, color='lightblue', alpha=1)\n",
        "    \n",
        "    ### Plot parameters\n",
        "    xticks = np.arange(len(grouped)/2)+offset\n",
        "    labels = grouped.index.get_level_values(level=0)[::2]\n",
        "    rotation = 30 if (key=='Age')|(key=='Fare') else 0\n",
        "    plt.xticks(xticks, labels, rotation=rotation)\n",
        "    plt.xlabel(key)\n",
        "    plt.ylabel('Count')\n",
        "    plt.legend((not_sur[0],sur[0]),('Not Survived','Survived'),fontsize='small')\n",
        "    plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c8382deb-2d94-1a7b-d0f6-6dd3c470b2d4"
      },
      "outputs": [],
      "source": [
        "### Display plots\n",
        "fig, axes = plt.subplots(3,2, figsize=(12,12))\n",
        "\n",
        "visualize('Pclass',axes[0,0])\n",
        "visualize('Fare',axes[0,1])\n",
        "visualize('Age',axes[1,0])\n",
        "visualize('Sex',axes[1,1])\n",
        "visualize('Family',axes[2,0])\n",
        "visualize('Embarked',axes[2,1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "aa2c560c-2a91-b2c9-9613-5bb037469f4b"
      },
      "source": [
        "### 3. Train without imputation\n",
        "In the first branch, I'll remove all rows with NaN values (179 out of 891 entries - 20% of the dataset, it is not negligable though). Features that are not in numeric types must be transformed into numeric for scikit random forest. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3670116d-ae43-0b52-dcdd-00a5e8761f5c"
      },
      "outputs": [],
      "source": [
        "### train_df dataset \"without\" NaN values\n",
        "train_short = train_df.copy().dropna(axis=0)\n",
        "features_short = train_short[['Pclass','Fare','Age','Sex','Family','Embarked']]\n",
        "target_short = train_short['Survived']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "dd84d625-0a0e-5e33-f68f-e797e31d2a58"
      },
      "outputs": [],
      "source": [
        "### For RandomForestClassifier, turn the column Sex into numeric values (0,1) and the column Embarked into 3 (C,Q,S) binary columns\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "#features_short.loc[:,'Sex'] = LabelEncoder().fit_transform(features_short['Sex'])\n",
        "features_short['Sex_d'] = (features_short.Sex=='male').astype(int)\n",
        "features_short = pd.concat([features_short, pd.get_dummies(features_short.Embarked, prefix='Emb')], axis=1)\n",
        "features_short = features_short.drop(['Sex','Embarked'], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "fdbae3eb-f7db-eff8-93b9-66505b3d0107"
      },
      "source": [
        "First, define a function that returns prediction result of Random Forest. And get the first predition without imputation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e2d2deb9-27c4-4044-f4db-3bbad88373d4"
      },
      "outputs": [],
      "source": [
        "### Prediction\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "### Prediction function (I'll call it several times)\n",
        "def RFPred(features, target):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.4, random_state=123)\n",
        "    clf = RandomForestClassifier(n_estimators=80) #min_samples_leaf=2,min_samples_split=3, random_state=123)\n",
        "    clf.fit(X_train, y_train)\n",
        "    pred = clf.predict(X_test)\n",
        "    print(\"features : {}\".format(features.columns.tolist()))\n",
        "    print(\"feature_importance : {}\".format(clf.feature_importances_))\n",
        "    print(\"score = {}\".format(accuracy_score(pred, y_test)))\n",
        "    return clf, X_train, X_test, y_train, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "18bbffd9-6582-e066-acc9-10cbdaca366a"
      },
      "outputs": [],
      "source": [
        "clf, X_train, X_test, y_train, y_test = RFPred(features_short, target_short)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "cc328652-e278-cf48-580c-a4641e2c7499"
      },
      "outputs": [],
      "source": [
        "### Prediction by AdaBoost just for simple comparaison\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "abc = AdaBoostClassifier()\n",
        "abc.fit(X_train, y_train)\n",
        "pred = abc.predict(X_test)\n",
        "print(\"features : {}\".format(features_short.columns.tolist()))\n",
        "print(\"feature importance : {}\".format(abc.feature_importances_))\n",
        "print(\"score = {}\".format(accuracy_score(pred, y_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "1e1750c3-73e5-4df5-ccc0-596784a71c7a"
      },
      "source": [
        "### 4. Train with imputation\n",
        "Now, I'll try to use as much information as I can from the original dataset by imputing the missing data in Age and Embarked. Imputation will be done by pseudo randomly but based on their distribution patterns. Basically, the more data (age for example) is in the original dataset, the more the probability increases for a NaN value to be replaced by this age. This is random values so doesn't add much information but at least we can utilize data in other columns that were not used in the previous section due to NaN values in Age. So overall it would be beneficial (hopefully). Here, I'll also try to engineer features a bit to discard unnecessary information that may confuse random forest estimator to do a good job. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "cfe27353-dd7c-f94d-5dc1-e852a0e660ee"
      },
      "outputs": [],
      "source": [
        "### train_df dataset \"with\" NaN values\n",
        "train_long = train_df.copy()\n",
        "features_long = train_long[['Name','Pclass','Fare','Age','Sex','Family','Embarked']]\n",
        "target_long = train_long['Survived']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4e516a9f-a6e3-f2b7-cba3-5c53dfd750aa"
      },
      "outputs": [],
      "source": [
        "### Display Age distribution by Sex and Title\n",
        "fig, axes = plt.subplots(2,2,figsize=(10,5),sharey=True,sharex=True)\n",
        "\n",
        "master = features_long[features_long.Name.str.\n",
        "                          contains('Master\\.')].Age.round(0).dropna()\n",
        "axes[0,0].hist(master, bins=np.arange(0,90,1))\n",
        "mr = features_long[features_long.Name.str.\n",
        "                          contains(r'Mr\\.|Dr\\.|Rev\\.|Major\\.|Col\\.|Capt\\.|Don\\.|Jonkheer',regex=True)].Age.round(0).dropna()\n",
        "axes[0,1].hist(mr, bins=np.arange(0,90,1))\n",
        "miss = features_long[features_long.Name.str.\n",
        "                            contains(r'Miss\\.|Mlle\\.',regex=True)].Age.round(0).dropna()\n",
        "axes[1,0].hist(miss, bins=np.arange(0,90,1), color='r')\n",
        "mrs = features_long[features_long.Name.str.\n",
        "                            contains(r'Mrs\\.|Mme\\.|Ms\\.|Countess\\.',regex=True)].Age.round(0).dropna()\n",
        "axes[1,1].hist(mrs, bins=np.arange(0,90,1), color='r')\n",
        "\n",
        "plt.suptitle('Age count for Master / Mr and similar / Miss / Mrs and similar (blue for men, red for women)')\n",
        "fig.text(0.5, -0.01, 'Age', ha='center')\n",
        "fig.text(-0.01, 0.5, 'Count', va='center', rotation='vertical')\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "a9e906e8-b7e4-0b5b-7f42-013c188b5863"
      },
      "outputs": [],
      "source": [
        "### Impute NaN values in age according to their categories above - random selection from the existing distribution\n",
        "age  = features_long.Age\n",
        "name = features_long.Name\n",
        "np.random.seed(123)\n",
        "\n",
        "idx1 = features_long[(features_long.Age.isnull())&(features_long.Name.str.contains('Master\\.'))].index\n",
        "for ii in range(len(idx1)):\n",
        "    features_long.set_value(idx1[ii], 'Age', master.iloc[np.random.randint(len(master))])\n",
        "\n",
        "idx2 = features_long[(features_long.Age.isnull())&(features_long.Name.str.contains(r'Mr\\.|Dr\\.|Rev\\.|Major\\.|Col\\.|Capt\\.|Don\\.|Jonkheer',regex=True))].index\n",
        "for ii in range(len(idx2)):\n",
        "    features_long.set_value(idx2[ii], 'Age', mr.iloc[np.random.randint(len(mr))])\n",
        "\n",
        "idx3 = features_long[(features_long.Age.isnull())&(features_long.Name.str.contains(r'Miss\\.|Mlle\\.',regex=True))].index\n",
        "for ii in range(len(idx3)):\n",
        "    features_long.set_value(idx3[ii], 'Age', miss.iloc[np.random.randint(len(miss))])\n",
        "\n",
        "idx4 = features_long[(features_long.Age.isnull())&(features_long.Name.str.contains(r'Mrs\\.|Mme\\.|Ms\\.|Dr\\.|Countess\\.',regex=True))].index\n",
        "for ii in range(len(idx4)):\n",
        "    features_long.set_value(idx4[ii], 'Age', mrs.iloc[np.random.randint(len(mrs))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9b5ed671-dcde-d769-ed6e-0539f4b0397a"
      },
      "outputs": [],
      "source": [
        "### Age data original distribution\n",
        "fig, axes = plt.subplots(1,2, figsize=(12,4), sharey=True)\n",
        "age = train_df.Age.round(0)\n",
        "grouped = age.groupby(age).count()\n",
        "plt.sca(axes[0])\n",
        "plt.bar(grouped.index,grouped,color='grey')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Age count without imputation')\n",
        "\n",
        "### Imputed Age dataset (new values added shown in orange)\n",
        "age_imp = features_long.Age.round(0)\n",
        "grouped_imp = age_imp.groupby(age_imp).count()\n",
        "plt.sca(axes[1])\n",
        "plt.bar(grouped_imp.index, grouped_imp, color='orange')\n",
        "plt.bar(grouped.index, grouped, color='grey')\n",
        "plt.xlabel('Age')\n",
        "#plt.ylabel('Count')\n",
        "plt.title('Age count with imputation')\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c01b3ec4-3d74-0d9f-9a08-fc77298cb67e"
      },
      "outputs": [],
      "source": [
        "### In the same way, impute Embarked column\n",
        "emb = features_long.Embarked.copy()\n",
        "num = emb.isnull().sum()\n",
        "idx = np.random.randint(len(emb)-num, size=num)\n",
        "emb[emb.isnull()] = emb.dropna().iloc[idx].tolist()\n",
        "features_long = pd.concat([features_long.drop('Embarked', axis=1), emb], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c1fedda4-7abd-096d-cb30-42c9d48975e9"
      },
      "outputs": [],
      "source": [
        "### Sex column into numeric values\n",
        "#features_long.loc[:,'Sex'] = LabelEncoder().fit_transform(features_long['Sex'])\n",
        "features_long['Sex_d'] = (features_long.Sex=='male').astype(int)\n",
        "features_long.drop('Sex',axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1b1c7382-f9df-a8e4-e262-8f6a26ea5dcf"
      },
      "outputs": [],
      "source": [
        "### We saw earlier that Embarked doesn't help much so delete it with Name\n",
        "features_long.drop(['Name','Embarked'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "8797de0a-efd4-34ea-bda2-f62707fa6ef5"
      },
      "source": [
        "The first round of prediction. In the first case, the result is better than in the previous section without imputation. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1360cdd2-55ab-7bda-7d2a-7fc60b06ce18"
      },
      "outputs": [],
      "source": [
        "_ = RFPred(features_long, target_long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bffba0ca-56f9-5848-bdfc-3b81ac1d3e48"
      },
      "outputs": [],
      "source": [
        "\"\"\"features_long['S_Family'] = ((features_long.Family>0)&(features_long.Family<4)).astype(int)\n",
        "#features_long['Solo'] = (features_long.Family==0).astype(int)\n",
        "#features_long['B_Family'] = (features_long.Family>=4).astype(int)\n",
        "features_long.drop(['Family'],axis=1,inplace=True)\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b3166397-36a8-4942-9ee9-e51ac9ecc990"
      },
      "source": [
        "Then let's continue to break down the next weakest, Pclass. After a few tests, Pclass_3 doesn't contribute much to prediction because the similar information might be captured by Fare (which is stronger). Pclass_1 and Pclass_2 seem to have potential."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "180d89ff-5ad0-cb2e-6df7-11fabe4d7f6f"
      },
      "outputs": [],
      "source": [
        "\"\"\"features_long = pd.concat([features_long, pd.get_dummies(features_long.Pclass, prefix='Pclass')], axis=1)\n",
        "features_long = features_long.drop(['Pclass','Pclass_3'], axis=1)\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "7fd8bc13-a0d6-3881-33ad-561514f4e728"
      },
      "source": [
        "Female has a very high rate of survival and it may be well captured by Sex. Within Male, men except male child have high non-survival level. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "95ec4ebf-9166-30e4-ea42-dc441e060668"
      },
      "outputs": [],
      "source": [
        "\"\"\"features_long['maleadult'] = ((features_long.Age>15)&(features_long.Sex_d==1)).astype(int)\n",
        "#features_long['femalesenior'] = ((features_long.Age>30)&(features_long.Sex_d==0)).astype(int)\n",
        "#features_long['malechild'] = ((features_long.Age<=15)&(features_long.Sex_d==1)).astype(int)\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "e29769dc-a65f-0f92-06aa-fa772ceb70b9"
      },
      "source": [
        "Now, we handled features and only meaningful ones are left. The prediction rate doesn't seem to improve but we need to fine-tune a bit more. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bfdb4fa4-6b7f-b0e1-4711-cf81e93326cf"
      },
      "outputs": [],
      "source": [
        "clf_opt, X_train, X_test, y_train, y_test = RFPred(features_long, target_long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "85024584-f9f2-b4d6-e7cf-9a090518c335"
      },
      "outputs": [],
      "source": [
        "### Prediction optimization using GridSearchCV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "grid_param = {'n_estimators': [10,20,40,80,100]}\n",
        "#             'min_samples_split': [2,4,6],\n",
        "#             'min_samples_leaf': [1,2,3],\n",
        "#             'criterion': ['gini'],\n",
        "#             'random_state': [0]}\n",
        "grid_search = GridSearchCV(clf, grid_param)\n",
        "grid_search.fit(X_train, y_train)\n",
        "pred = grid_search.predict(X_test)\n",
        "clf_opt = grid_search.best_estimator_\n",
        "print(\"best parameters : {}\".format(grid_search.best_estimator_))\n",
        "print(\"score = {}\".format(accuracy_score(pred, y_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b4be7bb0-db94-6bab-2d8b-b949f7e3d5af"
      },
      "source": [
        "There surely is a factor of randomness but it does show that we found some findtuning parameters to get you to the current highest points. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "737c00ad-d343-6fcf-6a23-45a3f3defbea"
      },
      "outputs": [],
      "source": [
        "### K-Fold cross validation to check variance of results\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "\n",
        "kfold = KFold(n_splits=5)\n",
        "cross_val_score(clf_opt, X_train, y_train, cv=kfold, n_jobs=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "fb78e405-7724-c913-1270-ef00b2c72ec5"
      },
      "outputs": [],
      "source": [
        "### Prediction by AdaBoost just for simple comparaison\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "abc = AdaBoostClassifier()\n",
        "abc.fit(X_train, y_train)\n",
        "pred = abc.predict(X_test)\n",
        "print(\"features : {}\".format(features_long.columns.tolist()))\n",
        "print(\"feature importance : {}\".format(abc.feature_importances_))\n",
        "print(\"score = {}\".format(accuracy_score(pred, y_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "16e7a198-3848-02e1-d997-3846ff299e34"
      },
      "outputs": [],
      "source": [
        "### For submission - prediction without Age\n",
        "nage = features_long.drop('Age',axis=1)\n",
        "clf_opt_nage, X_train, X_test, y_train, y_test = RFPred(nage, target_long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5fce0391-e985-162d-3ad4-b00b5c9a50e9"
      },
      "outputs": [],
      "source": [
        "### Prediction optimization using GridSearchCV\n",
        "grid_param = {'n_estimators': [10,20,40,80,100]}\n",
        "#             'min_samples_split': [2,4,6],\n",
        "#             'min_samples_leaf': [1,2,3],\n",
        "#             'criterion': ['gini'],\n",
        "#             'random_state': [0]}\n",
        "grid_search = GridSearchCV(clf, grid_param)\n",
        "grid_search.fit(X_train, y_train)\n",
        "pred = grid_search.predict(X_test)\n",
        "clf_opt_nage = grid_search.best_estimator_\n",
        "print(\"best parameters : {}\".format(grid_search.best_estimator_))\n",
        "print(\"score = {}\".format(accuracy_score(pred, y_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4c258455-9ff4-e9d5-80ce-88180f103819"
      },
      "outputs": [],
      "source": [
        "### For submission - prediction without Fare\n",
        "nfare = features_long.drop('Fare',axis=1)\n",
        "clf_opt_nfare, X_train, X_test, y_train, y_test = RFPred(nfare, target_long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "03a5c7c4-6400-8606-ea66-018f0c2f75c7"
      },
      "outputs": [],
      "source": [
        "### Prediction optimization using GridSearchCV\n",
        "grid_param = {'n_estimators': [10,20,40,80,100]}\n",
        "#             'min_samples_split': [2,4,6],\n",
        "#             'min_samples_leaf': [1,2,3],\n",
        "#             'criterion': ['gini'],\n",
        "#             'random_state': [0]}\n",
        "grid_search = GridSearchCV(clf, grid_param)\n",
        "grid_search.fit(X_train, y_train)\n",
        "pred = grid_search.predict(X_test)\n",
        "clf_opt_nfare = grid_search.best_estimator_\n",
        "print(\"best parameters : {}\".format(grid_search.best_estimator_))\n",
        "print(\"score = {}\".format(accuracy_score(pred, y_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "45cc84c2-175e-b2c5-20e5-b0883b60fc35"
      },
      "source": [
        "### 5. End note\n",
        "In Random Forest like any other ML algorithms, feature engineering is an important element for increasing its performance. Features should be chosen with care and then fine-tuned both to capture sensibility and to avoid over-fitting. Imputation is also a critical method to make use of more data, in particular, other features that would be discarded because one or two features in an entry contain null values. Of course, other machine learning algorithms may show better perfomance than random forest but here I remained to play only with RF to make an example. Any suggestion or discussion is welcome!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d7448e11-5016-0591-76e0-4f6799e355fd"
      },
      "source": [
        "### 6. Test set prediction & Submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0ed9794b-0eaf-57ae-4c3d-d035261dd972"
      },
      "outputs": [],
      "source": [
        "test_df = test_data[['PassengerId','Name','Fare','Sex','Age','SibSp','Parch','Pclass']]\n",
        "family = (test_df.SibSp + test_df.Parch).rename('Family')\n",
        "test_df = test_df.join(family)\n",
        "test_df['S_Family'] = ((test_df.Family>0)&(test_df.Family<4)).astype(int)\n",
        "test_df = pd.concat([test_df, pd.get_dummies(test_df.Pclass, prefix='Pclass')], axis=1)\n",
        "test_df['Sex_d'] = (test_df.Sex=='male').astype(int)\n",
        "test_df['maleadult'] = ((test_df.Age>15)&(test_df.Sex_d==1)).astype(int)\n",
        "test_df = test_df.drop(['Pclass_3','SibSp','Parch','Sex'], axis=1) #'Family','Pclass',"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ce30a1cc-862f-76b8-ae58-77db616d53e8"
      },
      "outputs": [],
      "source": [
        "test_df_age = test_df[(test_df.Age.notnull())&(test_df.Fare.notnull())]\n",
        "test_df_nage = test_df[test_df.Age.isnull()]\n",
        "test_df_nfare = test_df[test_df.Fare.isnull()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d5082d45-0576-251b-0dd2-8577a5df8f82"
      },
      "outputs": [],
      "source": [
        "passengerid_age = test_df_age.PassengerId\n",
        "X_test_age = test_df_age[['Pclass', 'Fare', 'Age', 'Family', 'Sex_d']]\n",
        "pred_age = clf_opt.predict(X_test_age)\n",
        "\n",
        "passengerid_nage = test_df_nage.PassengerId\n",
        "X_test_nage = test_df_nage[['Pclass', 'Fare', 'Family', 'Sex_d']]\n",
        "pred_nage = clf_opt_nage.predict(X_test_nage)\n",
        "\n",
        "passengerid_nfare = test_df_nfare.PassengerId\n",
        "X_test_nfare = test_df_nfare[['Pclass', 'Age', 'Family', 'Sex_d']]\n",
        "pred_nfare = clf_opt_nfare.predict(X_test_nfare)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f2e1ed87-e55f-aa43-64df-3296a60fcf08"
      },
      "outputs": [],
      "source": [
        "y_test_age = pd.concat([passengerid_age.reset_index().PassengerId, pd.Series(pred_age)], axis=1)\n",
        "y_test_nage = pd.concat([passengerid_nage.reset_index().PassengerId, pd.Series(pred_nage)], axis=1)\n",
        "y_test_nfare = pd.concat([passengerid_nfare.reset_index().PassengerId, pd.Series(pred_nfare)], axis=1)\n",
        "y_test = pd.concat([y_test_age, y_test_nage, y_test_nfare]).sort_values('PassengerId').reset_index()\n",
        "y_test.columns = ['index','PassengerId','Survived']\n",
        "submission = y_test[['PassengerId','Survived']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "58903ecb-4ce6-d88d-c264-a52bfb58eb79"
      },
      "outputs": [],
      "source": [
        "submission.to_csv('titanic_jpark_basic.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "909be9c2-a0b6-1828-392f-b17bfb7e0035"
      },
      "outputs": [],
      "source": ""
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}