{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "6ba8373f-05c7-d198-94b4-fb0ed4da9ccd"
      },
      "source": [
        "# Prediction with feature engineering, data imputation - Titanic survival\n",
        "## 1. Introduction\n",
        "\n",
        "In this exercise, I tried to use a couple of machine learning algorithms to predict the survival of pessagnes based on selected features. Data imputation and feature engineering was applied to increase prediction score. Currently my submission has got a score of 80.861, top 10%, but it can be further improved by more feature engineering and find-tuning of parameters. I'll keep the code updated and see how far I can get to. I'll appreciate any comments or advice for improvement.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d5943d11-27ce-f93f-d845-532a2952a582"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display\n",
        "from matplotlib import style\n",
        "style.use('fivethirtyeight')\n",
        "%matplotlib inline\n",
        "\n",
        "# Load the dataset\n",
        "train_data = pd.read_csv(\"../input/train.csv\")\n",
        "test_data = pd.read_csv(\"../input/test.csv\")\n",
        "\n",
        "# Print the dataset information\n",
        "#train_data.info()\n",
        "#train_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "e066535a-4dfe-dfe5-d8a8-2a5700d436a9"
      },
      "source": [
        "### 2. Data exploration\n",
        "At first look several columns have null values. Cabin and Ticket don't contain much information so I'll remove them. Below I try to impute missing values in Age, Fare and Embarked, which allows me to use maximum amount of data points for prediction. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "87161efa-611f-4c09-9d3f-e844682cb3f3"
      },
      "outputs": [],
      "source": [
        "### Select features that only make sense for survival prediction, SibSp and Parch are combined into Family\n",
        "train = train_data[['PassengerId','Name','Pclass','Sex','Age','SibSp','Parch','Fare','Embarked']]\n",
        "test = test_data[['PassengerId','Name','Pclass','Sex','Age','SibSp','Parch','Fare','Embarked']]\n",
        "target = train_data['Survived']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "54183c90-926b-9623-753f-6f8fe28a297e"
      },
      "source": [
        "### 3. Feature engineering\n",
        "I'll start by creating some more features that can be drawn from combining several existing features and extracting useful information from other features. <code>Title</code> and <code>Name</code> are information-rich columns. Particularly, we can draw family relationships from <code>Name</code> like who's travelling with whom, also using information on <code>SibSp</code> and <code>Parch</code>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "87f4e061-6ba7-beae-ca02-0eaad6f36d9c"
      },
      "outputs": [],
      "source": [
        "### Create Title column from Name information, Create also Special_Title column\n",
        "import re\n",
        "whole = pd.concat([train, test])#.reset_index()\n",
        "whole['Title'] = whole.Name.map(lambda x: re.search('\\w+\\.',x).group(0))\n",
        "whole['Offi'] = (whole.Title.str.contains(r'Major\\.|Col\\.|Capt\\.')).astype(int)\n",
        "whole['High'] = (whole.Title.str.contains(r'Don\\.|Jonkheer\\.|Sir\\.|Countess\\.|Dona\\.')).astype(int)\n",
        "whole['Dr'] = (whole.Title.str.contains('Dr.')).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "342fd87e-9e99-cb5d-5087-5e3c8e367ef7"
      },
      "outputs": [],
      "source": [
        "### Extract Surname and Maiden name\n",
        "whole['Surname'] = whole.Name.map(lambda x: re.search('(.+)(?=\\,)',x).group(1))\n",
        "def extract(row):\n",
        "    if row.Sex=='male': return np.nan\n",
        "    st = re.search('(\\w+)(?=\\))',row.Name.replace('\"',''))\n",
        "    if st is None: return np.nan\n",
        "    else: return st.group(0).replace('\"', '')\n",
        "whole['Maiden'] = whole.apply(lambda x: extract(x),axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1d314e05-6f37-fb6c-eb0b-ef116d651d4b"
      },
      "outputs": [],
      "source": [
        "whole.columns=['Id','Name','Pclass','Sex','Age','SibSp','Parch','Fare','Emb','Title','Offi','High','Dr','Surname','Maiden']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "578bddfa-5ecc-987b-0063-890857901cd5"
      },
      "outputs": [],
      "source": [
        "whole = whole.set_index('Id')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "8f1f646e-e43f-a470-0f37-325b1364f4fe"
      },
      "source": [
        "Rules of family gathering are the following: 1) if SibSp and Parch are all zeros, then the person is travelling alone. 2) if someone travels with their family, then first look at passengers with the same surname and group them together. If this group looks complete, then assign a family id. 3) If the family doesn't seem having everyone, then extend search to maiden names (for example mother side family) or those who obtain other surnames (for example sister who got married with a man with different surname). Group all of them and see if the family looks complete. If yes, then assign a family id. 4) With these rules, families don't find all members then report for hard search later. \n",
        "\n",
        "And there is a restriction as several surnames are so common that it will certainly group so many unrelated people together. When searching people with the same surname, people can get together as family only if their <code>Pclass</code> and <code>Emb</code> must be the same. <code>Fare</code> should also be the same in many cases but exceptionally we can accept fares like half-price, one-third price or similar (for example for children)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bf893201-2956-f0e8-8a3c-10ae7812d54b"
      },
      "outputs": [],
      "source": [
        "### Assign family id to those who travel together\n",
        "fid=1\n",
        "whole['Fid']= np.nan\n",
        "for idx in whole.index:\n",
        "    row = whole.loc[idx]\n",
        "    ## Consider only those not assigned and having family on board\n",
        "    if (np.isnan(row.Fid))&(row.SibSp+row.Parch>0):        \n",
        "        ## Those who have the same surname, emb, class, and fare (some variation accepted)\n",
        "        temp = whole[(whole.Surname==row.Surname)#|(whole.Surname==row.Maiden)|(whole.Maiden==row.Maiden))\n",
        "                    &((whole.Fare.round(1)==row.Fare.round(1))|(whole.Fare.round(1)==(row.Fare/2).round(1))\n",
        "                      |(whole.Fare.round(1)==(row.Fare*2).round(1))|(whole.Fare.round(1)==(row.Fare/3).round(1))\n",
        "                      |(whole.Fare.round(1)==(row.Fare*3).round(1)))\n",
        "                    &(whole.Emb==row.Emb)&(whole.Pclass==row.Pclass)&(whole.SibSp+whole.Parch>0)]\n",
        "        ## if sibsp, parch numbers match with family size for all family members, then skip below\n",
        "        ## if sibsp, parch numbers each match themselves, then it is also fine, skip below\n",
        "        if (((temp.SibSp+temp.Parch+1)==len(temp)).mean()!= 1) & ~(((temp.SibSp.sum())%2==0)&((temp.Parch.sum())%2==0)):\n",
        "            fname = row.Surname\n",
        "            maiden_coming = temp[(temp.Maiden.notnull())&(temp.Surname==fname)]\n",
        "            for r in maiden_coming.itertuples():\n",
        "                ext1 = whole[(whole.Surname!=r.Surname)\n",
        "                             &((whole.Surname==r.Maiden)|(whole.Maiden==r.Maiden))#|(whole.Maiden==t.Surname))\n",
        "                             &((whole.Fare.round(1)==r.Fare.round(1))|(whole.Fare.round(1)==(r.Fare/2).round(1))\n",
        "                               |(whole.Fare.round(1)==(r.Fare*2).round(1))|(whole.Fare.round(1)==(r.Fare/3).round(1))\n",
        "                               |(whole.Fare.round(1)==(r.Fare*3).round(1)))\n",
        "                             &(whole.Emb==r.Emb)&(whole.Pclass==r.Pclass)&(whole.SibSp+whole.Parch!=0)]\n",
        "                test = pd.concat([temp, ext1])\n",
        "                ## if the new group of family seems complete, then stop\n",
        "                if (((test.SibSp+test.Parch+1)==len(test)).mean()== 1) | (((test.SibSp.sum())%2==0)&((test.Parch.sum())%2==0)):\n",
        "                    temp = test\n",
        "                    #print('EXT1 worked, Fid, surname =',fid, row.Surname)\n",
        "                ## otherwise, extend search to those have the surname as maiden name\n",
        "                else:\n",
        "                    ext2 = whole[(whole.Maiden==row.Surname) #|(whole.Maiden==r.Maiden))#|(whole.Maiden==t.Surname))\n",
        "                             &((whole.Fare.round(1)==row.Fare.round(1))|(whole.Fare.round(1)==(row.Fare/2).round(1))\n",
        "                               |(whole.Fare.round(1)==(row.Fare*2).round(1))|(whole.Fare.round(1)==(row.Fare/3).round(1))\n",
        "                               |(whole.Fare.round(1)==(row.Fare*3).round(1)))\n",
        "                             &(whole.Emb==row.Emb)&(whole.Pclass==row.Pclass)&(whole.SibSp+whole.Parch!=0)]\n",
        "                    test = pd.concat([temp, ext2])\n",
        "                    if (((test.SibSp+test.Parch+1)==len(test)).mean()== 1) | (((test.SibSp.sum())%2==0)&((test.Parch.sum())%2==0)):\n",
        "                        temp = test\n",
        "                        #print('EXT2 worked, Fid, surname =',fid, row.Surname)\n",
        "                    ## in case it is still incomplete, then we need to scrutinize one by one\n",
        "                    else: print('Need hand work, Fid, surname =',fid,row.Surname)\n",
        "\n",
        "        whole.set_value(temp.index, 'Fid', fid)\n",
        "        fid+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "587edecd-a8af-34aa-b191-813ebb9cb473"
      },
      "outputs": [],
      "source": [
        "### Some hard coding for family gathering (these are usually due to fare difference, or large family links 3-4 different surnames together)\n",
        "whole.set_value(268,'Fid',73) #display(whole[whole.Name.str.contains(r'Strom|Persson|Lindell')])\n",
        "whole.set_value([581,1133],'Fid',76) #display(whole[whole.Name.str.contains(r'Jacobsohn|Christy')])\n",
        "whole.set_value(881,'Fid',88) #display(whole[whole.Name.str.contains(r'Parrish')])\n",
        "whole.set_value(1296,'Fid',107) #display(whole[whole.Name.str.contains(r'Frauenthal|Heinsheimer')])\n",
        "whole.set_value([530,775,893,944],'Fid',120) #display(whole[(whole.Surname=='Richards')|(whole.Surname=='Hocking')|(whole.Maiden=='Needs')])\n",
        "whole.set_value(665,'Fid',137) #display(whole[whole.Name.str.contains(r'Hirvonen|Lindqvist')])\n",
        "whole.set_value(600,'Fid',149) #display(whole[whole.Name.str.contains(r'Duff Gordon')])\n",
        "whole.set_value(1025,'Fid',186) #display(whole[whole.Surname=='Thomas']) #[1008,1025,1224]\n",
        "whole.set_value(705,'Fid',192) #display(whole[whole.Name.str.contains('Hansen')]) #(624,'Fid',172)\n",
        "whole.set_value(176,'Fid',211) #display(whole[whole.Name.str.contains('Klasen')])\n",
        "whole.set_value(1197,'Fid',144) #display(whole[whole.Name.str.contains('Crosby')])\n",
        "whole.set_value([70,1268],'Fid',68) #display(whole[whole.Name.str.contains('Kink')])\n",
        "whole.set_value([672,984],'Fid',188) #display(whole[whole.Name.str.contains(r'Davidson|Hays')])\n",
        "whole.set_value(69,'Fid',215) #display(whole[whole.Name.str.contains(r'Andersson')&(whole.Fid!=8)]) #[147,1212]\n",
        "whole.set_value(913,'Fid',72) #display(whole[whole.Surname=='Olsen'])\n",
        "print('Update done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c0c26fe4-d239-072d-5526-dd51288fd6ec"
      },
      "outputs": [],
      "source": [
        "### Exceptions when family size==1\n",
        "whole.set_value(478,'Fid',1) # Braund, couple with different price\n",
        "whole.set_value(193,'Fid',174) # Andersen-Jensen with Jensen\n",
        "whole.set_value(540,'Fid',155) # Frolicher with Frolicher-Stehli\n",
        "whole.set_value([969,1248],'Fid',152) # Lamson female siblings\n",
        "\n",
        "### Exceptions when family size>=2 \n",
        "whole.set_value([105,393],'Fid',36) # Gustafsson, Backstrom\n",
        "whole.set_value(137,'Fid',83) # Beckwith, Monypeny (Newsom)\n",
        "whole.set_value(418,'Fid',102) # Lahtinen, Silven\n",
        "whole.set_value([923,1211],'Fid',135) # Renouf, Jefferys\n",
        "whole.set_value(386,'Fid',147) # Davies Mr. Charles Henry SibSp, Parch =0 but seems like an error\n",
        "print('Update done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b804ecd2-9e46-d5c9-329f-3881cf2a7f0e"
      },
      "outputs": [],
      "source": [
        "### Add a new feature of family size\n",
        "whole['Family'] = 1\n",
        "for idx in whole.index:\n",
        "    row = whole.loc[idx]\n",
        "    temp = whole[whole.Fid==row.Fid]\n",
        "    size = len(temp)\n",
        "    if size==1: \n",
        "        whole.set_value(idx,'Fid',np.nan)\n",
        "    elif size>=2: whole.set_value(idx,'Family',size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "4fb1f8fb-be0f-4bbb-aa7b-ebc168aee553"
      },
      "source": [
        "### 4. Missing data imputation\n",
        "Now, I'll try to use as much information as I can from the original dataset by imputing the missing data in Age (and a few of Emb and Fare). Imputation will be done mainly based on their family membership. Basically, if a person's age is unknown and this person is in a family, then we look at other members of the family and guess this person's estimated age. If one has siblings, he/she must be in the same age range, and if one has a child then the person must be around 30 years older than the child and so on. If there is no reference (i.e. travelling alone), we would look at the group of passengers who have the same title and pick one of the ages randomly.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3832289d-15e8-f499-0b47-94898db7f6ba"
      },
      "outputs": [],
      "source": [
        "### Impute Age based on their title by using sample age in the same group\n",
        "whole_orig = whole.copy()\n",
        "lookup  = whole[whole.Age.notnull()].groupby('Title')\n",
        "np.random.seed(42)\n",
        "def impute(index):\n",
        "    imp = lookup.get_group(whole.loc[index].Title).sample(1).Age.iloc[0]\n",
        "    whole.set_value(idx, 'Age', imp)\n",
        "    \n",
        "for idx in whole[whole.Age.isnull()].index:\n",
        "    row = whole.loc[idx]\n",
        "    fid = row.Fid\n",
        "    \n",
        "    ## in case he/she has no siblings/spouses\n",
        "    if np.isnan(fid): \n",
        "        impute(idx)\n",
        "\n",
        "    ## in case he/she has siblings/spouses\n",
        "    else:\n",
        "        temp = whole[(whole.Fid==fid)&(whole.index!=idx)] ## People in the same family but not himself/herself\n",
        "        ## in case all four family members have SibSp=1, Parch=2 - special case\n",
        "        if (row.SibSp==1)&(row.Parch==2):\n",
        "            if (row.Title=='Master.')|(row.Title=='Miss.'): \n",
        "                age = temp[(temp.Title=='Master.')|(temp.Title=='Miss.')].Age.mean() + np.random.randint(-2,2)\n",
        "                if age>=0: whole.set_value(idx, 'Age', age)\n",
        "                else: whole.set_value(idx, 'Age', np.random.randint(1,15,1)[0])\n",
        "            else: \n",
        "                age = temp[(temp.Title!='Master.')&(temp.Title!='Miss.')].Age.mean() + np.random.randint(-2,2)\n",
        "                if age>=0: whole.set_value(idx, 'Age', age)\n",
        "                else: whole.set_value(idx, 'Age', np.random.randint(30,45,1)[0])\n",
        "        ## otherwise, find a sibling or spouse to estimate age\n",
        "        else:\n",
        "            sibsp = temp[(temp.SibSp==row.SibSp)&(temp.Parch==row.Parch)]\n",
        "            parch = temp[~temp.isin(sibsp)].dropna(how='all')\n",
        "            if (len(parch)>2)&(len(sibsp)>2): print('Need to check, id =',idx)\n",
        "            ## in case sibsp>0 and found the same sibsp number in family\n",
        "            else: \n",
        "                age = sibsp.Age.mean() + np.random.randint(-2,2)\n",
        "                if age>=0: whole.set_value(idx, 'Age', age)\n",
        "                else: \n",
        "                    if (row.Title=='Master.')|((parch.Age>=40).sum()>0): \n",
        "                        whole.set_value(idx, 'Age', np.random.randint(1,15,1)[0])\n",
        "                    elif (parch.Title.isin(['Master.']).sum()>0)|((parch.Age<20).sum()>0): \n",
        "                        whole.set_value(idx, 'Age', np.random.randint(30,45,1)[0]) \n",
        "                    else: \n",
        "                        whole.set_value(idx, 'Age', np.random.randint(20,45,1)[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c6ebf6b1-f4fb-f559-7d8d-3b7afafa5766"
      },
      "outputs": [],
      "source": [
        "### Age data original distribution\n",
        "fig, axes = plt.subplots(1,2, figsize=(12,4), sharey=True)\n",
        "age = whole_orig.Age.round(0)\n",
        "grouped = age.groupby(age).count()\n",
        "plt.sca(axes[0])\n",
        "plt.bar(grouped.index,grouped,color='grey')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Age count without imputation')\n",
        "\n",
        "### Imputed Age dataset (new values added shown in orange)\n",
        "age_imp = whole.Age.round(0)\n",
        "grouped_imp = age_imp.groupby(age_imp).count()\n",
        "plt.sca(axes[1])\n",
        "plt.bar(grouped_imp.index, grouped_imp, color='orange')\n",
        "plt.bar(grouped.index, grouped, color='grey')\n",
        "plt.xlabel('Age')\n",
        "#plt.ylabel('Count')\n",
        "plt.title('Age count with imputation')\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f3d884f8-95b3-e266-82b1-99664f9cd62d"
      },
      "outputs": [],
      "source": [
        "### Imputation of Emb and Fare with mean value\n",
        "whole[whole.Emb.isnull()]\n",
        "whole[(whole.Pclass==1)&(whole.Emb=='S')].Fare.mean()\n",
        "whole.set_value([62,830],'Emb','S')\n",
        "meanfare = whole[(whole.Pclass==3)&(whole.Emb=='S')].Fare.mean()\n",
        "whole.set_value(1044,'Fare',meanfare)\n",
        "whole.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "24ce80ee-0e74-365e-b41a-842407add719"
      },
      "outputs": [],
      "source": [
        "### Handling of Fare==0 (there are even first class passenger with free ticket so assign mean value of each class)\n",
        "z = whole[whole.Fare==0]\n",
        "for idx in z.index:\n",
        "    row = whole.loc[idx]\n",
        "    if row.Fare==0:\n",
        "        m = whole[(whole.Pclass==row.Pclass)&(whole.Emb==row.Emb)&(whole.Fare!=0)].Fare.mean()\n",
        "        whole.set_value(idx,'Fare',m)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5935f318-3b9a-8045-e18b-14e5fbe131b8"
      },
      "outputs": [],
      "source": [
        "s = pd.concat([whole[:891].reset_index(),target],axis=1).set_index('Id')\n",
        "x = pd.concat([s,whole[891:]])#.set_index('Id')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d2f031e2-a43e-9924-cfd1-25aad80b563e"
      },
      "outputs": [],
      "source": [
        "### Create new features related to family members' survival - whether family members (male, female, child) have survived or died\n",
        "x['MP_Surv']=0\n",
        "x['MP_Died']=0\n",
        "x['FP_Surv']=0\n",
        "x['FP_Died']=0\n",
        "x['CP_Surv']=0\n",
        "x['CP_Died']=0\n",
        "\n",
        "for row in x.itertuples():\n",
        "    if ~np.isnan(row.Fid):\n",
        "        temp = x[(x.Fid==row.Fid)&(x.Name!=row.Name)]\n",
        "        if len(temp)>=1:\n",
        "            m = temp[(temp.Sex=='male')&(temp.Age>=20)]\n",
        "            f = temp[(temp.Sex=='female')&(temp.Age>=20)]\n",
        "            c = temp[(temp.Age<20)|(temp.Title=='Master.')]\n",
        "            x.set_value(row.Index,'MP_Surv',len(m[m.Survived==1]))\n",
        "            x.set_value(row.Index,'MP_Died',len(m[m.Survived==0]))\n",
        "            x.set_value(row.Index,'FP_Surv',len(f[f.Survived==1]))\n",
        "            x.set_value(row.Index,'FP_Died',len(f[f.Survived==0]))\n",
        "            x.set_value(row.Index,'CP_Surv',len(c[c.Survived==1]))\n",
        "            x.set_value(row.Index,'CP_Died',len(c[c.Survived==0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "185156d0-7e35-f8cd-e722-6ff0dd076c8c"
      },
      "outputs": [],
      "source": [
        "whole = x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "50639079-1f6e-4780-30fb-f525c23110b4"
      },
      "outputs": [],
      "source": [
        "### Sex column into numeric values\n",
        "#features_long.loc[:,'Sex'] = LabelEncoder().fit_transform(features_long['Sex'])\n",
        "whole['Sex_d'] = (whole.Sex=='male').astype(int)\n",
        "whole.drop('Sex',axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5eb9f4cf-22b4-10e5-82f4-1fce2624115d"
      },
      "outputs": [],
      "source": [
        "### Pclass column into numeric values\n",
        "whole = pd.concat([whole, pd.get_dummies(whole.Pclass, prefix='Pclass')], axis=1)\n",
        "whole = whole.drop(['Pclass'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ed1f96a2-1cb5-a5ae-6f0a-521596787865"
      },
      "outputs": [],
      "source": [
        "whole = whole.drop(['SibSp','Parch','Emb','Name','Surname','Maiden','Title','Fid'],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f19be9f0-e10c-d7fd-f263-a7715bca1b41"
      },
      "outputs": [],
      "source": [
        "### Final selection of features\n",
        "whole.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8d818e51-577e-a48c-b8e9-de7a31c705df"
      },
      "outputs": [],
      "source": [
        "### Split again train and test data sets\n",
        "train_df = whole.drop(['Survived'],axis=1).iloc[:891]\n",
        "test_df = whole.drop(['Survived'],axis=1).iloc[891:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0c5867ed-807b-5d0b-3caa-000ce5350735"
      },
      "outputs": [],
      "source": [
        "target = whole['Survived'][:891]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0a396dc8-994a-d07f-247f-ac0c745e777c"
      },
      "outputs": [],
      "source": [
        "### Prediction optimization of Random Forest using GridSearchCV (You can increase n_splits value for more rigorous test)\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(train_df, target, test_size=0.3, random_state=123)\n",
        "cv = StratifiedKFold(n_splits=10, random_state=42)\n",
        "grid_param = {'n_estimators': [60,80,100,120],\n",
        "             'min_samples_split': [4,6,8],\n",
        "             'min_samples_leaf': [2,3,4],\n",
        "             'criterion': ['gini','entropy']}\n",
        "clf_g = RandomForestClassifier()\n",
        "\n",
        "grid_search = GridSearchCV(clf_g, grid_param, cv=cv)\n",
        "grid_search.fit(X_train, y_train)\n",
        "pred = grid_search.predict(X_test)\n",
        "clf_rf = grid_search.best_estimator_\n",
        "print(\"best parameters : {}\".format(grid_search.best_estimator_))\n",
        "print(\"score = {}\".format(accuracy_score(pred, y_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5a9d7778-b9f7-15d2-d0d9-3101bec672e7"
      },
      "outputs": [],
      "source": [
        "### K-Fold cross validation to check variance of results\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "kfold = KFold(n_splits=10)\n",
        "cross_val_score(clf_rf, train_df, target, cv=kfold, n_jobs=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "74ed6d7e-f563-af94-16d5-78e6352cced1"
      },
      "outputs": [],
      "source": [
        "### Prediction optimization of Decision Tree using GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(train_df, target, test_size=0.3, random_state=123)\n",
        "cv = StratifiedKFold(n_splits=10, random_state=42)\n",
        "grid_param = {'max_depth': range(2,8,1),\n",
        "             'min_samples_leaf': range(2,6,1),\n",
        "             'min_samples_split': range(3,7,1)}\n",
        "clf_g = DecisionTreeClassifier(random_state=0)\n",
        "\n",
        "grid_search = GridSearchCV(clf_g, grid_param, cv=cv)\n",
        "grid_search.fit(X_train, y_train)\n",
        "pred = grid_search.predict(X_test)\n",
        "clf_dt = grid_search.best_estimator_\n",
        "print(\"best parameters : {}\".format(grid_search.best_estimator_))\n",
        "print(\"score = {}\".format(accuracy_score(pred, y_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "0dc4899a-f489-82d3-211f-e76a8b4891b4"
      },
      "outputs": [],
      "source": [
        "### Prediction optimization of Support Vector Machine using GridSearchCV \n",
        "from sklearn.svm import SVC\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(train_df, target, test_size=0.3, random_state=123)\n",
        "cv = StratifiedKFold(n_splits=10, random_state=42)\n",
        "grid_param = {'C': [100000,120000,140000],\n",
        "             'gamma': [1e-6,1e-5,1e-4]}\n",
        "clf_g = SVC(random_state=0)\n",
        "\n",
        "grid_search = GridSearchCV(clf_g, grid_param, cv=cv)\n",
        "grid_search.fit(X_train, y_train)\n",
        "pred = grid_search.predict(X_test)\n",
        "clf_svc = grid_search.best_estimator_\n",
        "print(\"best parameters : {}\".format(grid_search.best_estimator_))\n",
        "print(\"score = {}\".format(accuracy_score(pred, y_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f79f2f19-79e5-b8c4-7916-38d32b11c9b1"
      },
      "outputs": [],
      "source": [
        "### K-Fold cross validation to check variance of results\n",
        "kfold = KFold(n_splits=10)\n",
        "cross_val_score(clf_svc, train_df, target, cv=kfold, n_jobs=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "02b0fb93-ac91-7c56-41d1-d4035d5a2867"
      },
      "outputs": [],
      "source": [
        "### Plot learning curve\n",
        "from sklearn.model_selection import learning_curve\n",
        "cv = StratifiedKFold(n_splits=10,shuffle=True)\n",
        "training_sizes, training_scores, testing_scores = learning_curve(clf_svc, train_df, target, train_sizes=np.linspace(0.1, 1.0, 10), cv=cv)\n",
        "\n",
        "### Get mean and std\n",
        "training_scores_mean = np.mean(training_scores, axis=1)\n",
        "training_scores_std = np.std(training_scores, axis=1)\n",
        "testing_scores_mean = np.mean(testing_scores, axis=1)\n",
        "testing_scores_std = np.std(testing_scores, axis=1)\n",
        "\n",
        "plt.plot(training_sizes, training_scores_mean, 'o-', color='r', label='training_score')\n",
        "plt.plot(training_sizes, testing_scores_mean, 'o-', color='g', label='testing_score')\n",
        "plt.fill_between(training_sizes, training_scores_mean - training_scores_std, \\\n",
        "                     training_scores_mean + training_scores_std, color='r', alpha=0.2)\n",
        "plt.fill_between(training_sizes, testing_scores_mean - testing_scores_std, \\\n",
        "                     testing_scores_mean + testing_scores_std, color='g', alpha=0.2)\n",
        " \n",
        "### Plot aesthetic\n",
        "plt.grid(True)\n",
        "plt.ylim(-0.1, 1.1)\n",
        "plt.ylabel(\"Curve Score\")\n",
        "plt.xlabel(\"Training Points\")\n",
        "plt.legend(bbox_to_anchor=(1.1, 1.1), loc='best')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "02ded923-cb5f-d6f6-2e76-7058ce568fe4"
      },
      "source": [
        "### 5. End note\n",
        "All three ML algorithms mark similar scores for cross validation, And these are all very much improved from the previous version that I tested before. It is mainly due to more rigorous feature engineering particularly on family and age features. This exercise demonstrates feature engineering is an important element for increasing performance. Features should be chosen with care and then fine-tuned both to capture sensibility and to avoid over-fitting. Oftentimes, new and powerful information can be drawn from existing features. Imputation is also a critical method to make use of more data, in particular, other features that would be discarded because one or two features in an entry contain null values. Of course, this can be further improved and score can get higher. I'll come back later to do more work on other feature handling. Any suggestion or discussion is welcome!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "697f0ee9-8bd3-7707-e6a7-bba1e1e1f81b"
      },
      "source": [
        "### 6. Test set prediction & Submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bf21eb93-a7fc-64d6-e180-4d133ce4aafd"
      },
      "outputs": [],
      "source": [
        "pred = clf_svc.predict(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "cef5cb15-e852-cf57-7c87-f04752582f81"
      },
      "outputs": [],
      "source": [
        "submission = pd.Series(pred, index=test_df.index)\n",
        "submission = submission.reset_index()\n",
        "submission.columns=['PassengerId','Survived']\n",
        "submission.Survived = submission.Survived.astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bc77b9e2-6f38-0d59-6de1-1d6de573018a"
      },
      "outputs": [],
      "source": [
        "submission.to_csv('titanic_jpark_v6_SVC.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "dd4d42fa-1ebe-b5d9-86e9-d5f4b5fbd032"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}