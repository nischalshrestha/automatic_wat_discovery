{"cells":[
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "#Import libraries\n\nimport numpy as np\nfrom numpy.random import random_integers\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport sklearn\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom scipy.stats import pointbiserialr, spearmanr\n%matplotlib inline\n\nprint('Libraries Ready!')"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "#Load training data\n\npath = '../input/'\ndf = pd.read_csv(path+'train.csv')\ndf.head()"
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "####Process Data\n\nPeople with stronger titles tend to have more help on board. Hence, we will categorize passengers based on titles."
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "Title_Dictionary = {\n                    \"Capt\":       \"Officer\",\n                    \"Col\":        \"Officer\",\n                    \"Major\":      \"Officer\",\n                    \"Jonkheer\":   \"Royalty\",\n                    \"Don\":        \"Royalty\",\n                    \"Sir\" :       \"Royalty\",\n                    \"Dr\":         \"Officer\",\n                    \"Rev\":        \"Officer\",\n                    \"the Countess\":\"Royalty\",\n                    \"Dona\":       \"Royalty\",\n                    \"Mme\":        \"Mrs\",\n                    \"Mlle\":       \"Miss\",\n                    \"Ms\":         \"Mrs\",\n                    \"Mr\" :        \"Mr\",\n                    \"Mrs\" :       \"Mrs\",\n                    \"Miss\" :      \"Miss\",\n                    \"Master\" :    \"Master\",\n                    \"Lady\" :      \"Royalty\"\n                    } \n\ndf['Title'] = df['Name'].apply(lambda x: Title_Dictionary[x.split(',')[1].split('.')[0].strip()])\n\ndf.head()"
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "The ticket prefix may determine the status or cabin on board and hence will be included"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "def Ticket_Prefix(s):\n    s=s.split()[0]\n    if s.isdigit():\n        return 'NoClue'\n    else:\n        return s\n\ndf['TicketPrefix'] = df['Ticket'].apply(lambda x: Ticket_Prefix(x))\n\ndf.head()"
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "Now let's check for data types and missing values"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "df.info()"
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "We can see that Age and Embarked has missing data.\n\nSimply dropping the Age NaNs would mean throwing away too much data.\n\nWe add in the median age based on the Title, Pclass and Sex of each passenger."
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "mask_Age = df.Age.notnull()\nAge_Sex_Title_Pclass = df.loc[mask_Age, [\"Age\", \"Title\", \"Sex\", \"Pclass\"]]\nFiller_Ages = Age_Sex_Title_Pclass.groupby(by = [\"Title\", \"Pclass\", \"Sex\"]).median()\nFiller_Ages = Filler_Ages.Age.unstack(level = -1).unstack(level = -1)\n\nmask_Age = df.Age.isnull()\nAge_Sex_Title_Pclass_missing = df.loc[mask_Age, [\"Title\", \"Sex\", \"Pclass\"]]\n\ndef Age_filler(row):\n    if row.Sex == \"female\":\n        age = Filler_Ages.female.loc[row[\"Title\"], row[\"Pclass\"]]\n        return age\n    \n    elif row.Sex == \"male\":\n        age = Filler_Ages.male.loc[row[\"Title\"], row[\"Pclass\"]]\n        return age\n    \nAge_Sex_Title_Pclass_missing[\"Age\"]  = Age_Sex_Title_Pclass_missing.apply(Age_filler, axis = 1)   \n\ndf[\"Age\"] = pd.concat([Age_Sex_Title_Pclass[\"Age\"], Age_Sex_Title_Pclass_missing[\"Age\"]])    \n\ndf.head()"
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "Next we fill in the missing Fare."
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "df['Fare']=df['Fare'].fillna(value=df.Fare.mean())\ndf.head()"
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "We do not need Cabin and Ticket and hence can be dropped from our DataFrame.\n\nWe also can combine SibSp and Parch to FamilySize."
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "df['FamilySize'] = df['SibSp'] + df['Parch']\ndf = df.drop(['Ticket', 'Cabin'], axis=1)\ndf.head()"
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "Now we deal with categorical data using dummy variables."
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "dummies_Sex=pd.get_dummies(df['Sex'],prefix='Sex')\ndummies_Embarked = pd.get_dummies(df['Embarked'], prefix= 'Embarked') \ndummies_Pclass = pd.get_dummies(df['Pclass'], prefix= 'Pclass')\ndummies_Title = pd.get_dummies(df['Title'], prefix= 'Title')\ndummies_TicketPrefix = pd.get_dummies(df['TicketPrefix'], prefix='TicketPrefix')\ndf = pd.concat([df, dummies_Sex, dummies_Embarked, dummies_Pclass, dummies_Title, dummies_TicketPrefix], axis=1)\ndf = df.drop(['Sex','Embarked','Pclass','Title','Name','TicketPrefix'], axis=1)\n\ndf.head()"
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "Finally, we set our PassengerId as our index."
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "df = df.set_index(['PassengerId'])\ndf.head()"
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "####Feature Selection\n\nFor feature selection, we will look at the correlation of each feature against Survived.\nBased on our data types, we will use the following aglorithms:\n\n- Spearman-Rank correlation for nominal vs nominal data\n- Point-Biserial correlation for nominal vs continuous data"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "columns = df.columns.values\n\nparam=[]\ncorrelation=[]\nabs_corr=[]\n\nfor c in columns:\n    #Check if binary or continuous\n    if len(df[c].unique())<=2:\n        corr = spearmanr(df['Survived'],df[c])[0]\n    else:\n        corr = pointbiserialr(df['Survived'],df[c])[0]\n    param.append(c)\n    correlation.append(corr)\n    abs_corr.append(abs(corr))\n\n#Create dataframe for visualization\nparam_df=pd.DataFrame({'correlation':correlation,'parameter':param, 'abs_corr':abs_corr})\n\n#Sort by absolute correlation\nparam_df=param_df.sort_values(by=['abs_corr'], ascending=False)\n\n#Set parameter name as index\nparam_df=param_df.set_index('parameter')\n\nparam_df.head()"
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "Now that we have our correlation, we can use the Decision Tree classifier to see the score agaisnt feature space."
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "scoresCV = []\nscores = []\n\nfor i in range(1,len(param_df)):\n    new_df=df[param_df.index[0:i+1].values]\n    X = new_df.ix[:,1::]\n    y = new_df.ix[:,0]\n    clf = DecisionTreeClassifier()\n    scoreCV = sklearn.cross_validation.cross_val_score(clf, X, y, cv= 10)\n    scores.append(np.mean(scoreCV))\n    \nplt.figure(figsize=(15,5))\nplt.plot(range(1,len(scores)+1),scores, '.-')\nplt.axis(\"tight\")\nplt.title('Feature Selection', fontsize=14)\nplt.xlabel('# Features', fontsize=12)\nplt.ylabel('Score', fontsize=12)\nplt.grid();"
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "Based on the plot, a feature space of 10 dimensions provides the most reliable result while avoiding overfit."
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "best_features=param_df.index[1:10+1].values\nprint('Best features:\\t',best_features)"
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "Looking at out best features."
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "df[best_features].hist(figsize=(20,15));"
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "Creating the train and test datasets."
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "X = df[best_features]\ny = df['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=44)"
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "####Decision Tree\n\nAnalyzing the different parameters of Decision Trees."
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "plt.figure(figsize=(15,7))\n\n#Max Features\nplt.subplot(2,3,1)\nfeature_param = ['auto','sqrt','log2',None]\nscores=[]\nfor feature in feature_param:\n    clf = DecisionTreeClassifier(max_features=feature)\n    clf.fit(X_train,y_train)\n    scoreCV = sklearn.cross_validation.cross_val_score(clf, X, y, cv= 10)\n    scores.append(np.mean(scoreCV))\nplt.plot(scores, '.-')\nplt.axis('tight')\n# plt.xlabel('parameter')\n# plt.ylabel('score')\nplt.title('Max Features')\nplt.xticks(range(len(feature_param)), feature_param)\nplt.grid();\n\n#Max Depth\nplt.subplot(2,3,2)\nfeature_param = range(1,51)\nscores=[]\nfor feature in feature_param:\n    clf = DecisionTreeClassifier(max_depth=feature)\n    clf.fit(X_train,y_train)\n    scoreCV = sklearn.cross_validation.cross_val_score(clf, X, y, cv= 10)\n    scores.append(np.mean(scoreCV))\nplt.plot(feature_param, scores, '.-')\nplt.axis('tight')\n# plt.xlabel('parameter')\n# plt.ylabel('score')\nplt.title('Max Depth')\nplt.grid();\n\n#Min Samples Split\nplt.subplot(2,3,3)\nfeature_param = range(1,51)\nscores=[]\nfor feature in feature_param:\n    clf = DecisionTreeClassifier(min_samples_split =feature)\n    clf.fit(X_train,y_train)\n    scoreCV = sklearn.cross_validation.cross_val_score(clf, X, y, cv= 10)\n    scores.append(np.mean(scoreCV))\nplt.plot(feature_param, scores, '.-')\nplt.axis('tight')\n# plt.xlabel('parameter')\n# plt.ylabel('score')\nplt.title('Min Samples Split')\nplt.grid();\n\n#Min Samples Leaf\nplt.subplot(2,3,4)\nfeature_param = range(1,51)\nscores=[]\nfor feature in feature_param:\n    clf = DecisionTreeClassifier(min_samples_leaf =feature)\n    clf.fit(X_train,y_train)\n    scoreCV = sklearn.cross_validation.cross_val_score(clf, X, y, cv= 10)\n    scores.append(np.mean(scoreCV))\nplt.plot(feature_param, scores, '.-')\nplt.axis('tight')\n# plt.xlabel('parameter')\n# plt.ylabel('score')\nplt.title('Min Samples Leaf')\nplt.grid();\n\n#Min Weight Fraction Leaf\nplt.subplot(2,3,5)\nfeature_param = np.linspace(0,0.5,10)\nscores=[]\nfor feature in feature_param:\n    clf = DecisionTreeClassifier(min_weight_fraction_leaf =feature)\n    clf.fit(X_train,y_train)\n    scoreCV = sklearn.cross_validation.cross_val_score(clf, X, y, cv= 10)\n    scores.append(np.mean(scoreCV))\nplt.plot(feature_param, scores, '.-')\nplt.axis('tight')\n# plt.xlabel('parameter')\n# plt.ylabel('score')\nplt.title('Min Weight Fraction Leaf')\nplt.grid();\n\n#Max Leaf Nodes\nplt.subplot(2,3,6)\nfeature_param = range(2,21)\nscores=[]\nfor feature in feature_param:\n    clf = DecisionTreeClassifier(max_leaf_nodes=feature)\n    clf.fit(X_train,y_train)\n    scoreCV = sklearn.cross_validation.cross_val_score(clf, X, y, cv= 10)\n    scores.append(np.mean(scoreCV))\nplt.plot(feature_param, scores, '.-')\nplt.axis('tight')\n# plt.xlabel('parameter')\n# plt.ylabel('score')\nplt.title('Max Leaf Nodes')\nplt.grid();"
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "- Max Depth show high score variance with change in parameter.\n- All otehr parameters show low score variance."
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "####Random Forest Classifier"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "plt.figure(figsize=(15,10))\n\n#N Estimators\nplt.subplot(3,3,1)\nfeature_param = range(1,21)\nscores=[]\nfor feature in feature_param:\n    clf = RandomForestClassifier(n_estimators=feature)\n    clf.fit(X_train,y_train)\n    scoreCV = clf.score(X_test,y_test)\n    scores.append(scoreCV)\nplt.plot(scores, '.-')\nplt.axis('tight')\n# plt.xlabel('parameter')\n# plt.ylabel('score')\nplt.title('N Estimators')\nplt.grid();\n\n#Criterion\nplt.subplot(3,3,2)\nfeature_param = ['gini','entropy']\nscores=[]\nfor feature in feature_param:\n    clf = RandomForestClassifier(criterion=feature)\n    clf.fit(X_train,y_train)\n    scoreCV = clf.score(X_test,y_test)\n    scores.append(scoreCV)\nplt.plot(scores, '.-')\n# plt.xlabel('parameter')\n# plt.ylabel('score')\nplt.title('Criterion')\nplt.xticks(range(len(feature_param)), feature_param)\nplt.grid();\n\n#Max Features\nplt.subplot(3,3,3)\nfeature_param = ['auto','sqrt','log2',None]\nscores=[]\nfor feature in feature_param:\n    clf = RandomForestClassifier(max_features=feature)\n    clf.fit(X_train,y_train)\n    scoreCV = clf.score(X_test,y_test)\n    scores.append(scoreCV)\nplt.plot(scores, '.-')\nplt.axis('tight')\n# plt.xlabel('parameter')\n# plt.ylabel('score')\nplt.title('Max Features')\nplt.xticks(range(len(feature_param)), feature_param)\nplt.grid();\n\n#Max Depth\nplt.subplot(3,3,4)\nfeature_param = range(1,21)\nscores=[]\nfor feature in feature_param:\n    clf = RandomForestClassifier(max_depth=feature)\n    clf.fit(X_train,y_train)\n    scoreCV = clf.score(X_test,y_test)\n    scores.append(scoreCV)\nplt.plot(feature_param, scores, '.-')\nplt.axis('tight')\n# plt.xlabel('parameter')\n# plt.ylabel('score')\nplt.title('Max Depth')\nplt.grid();\n\n#Min Samples Split\nplt.subplot(3,3,5)\nfeature_param = range(1,21)\nscores=[]\nfor feature in feature_param:\n    clf = RandomForestClassifier(min_samples_split =feature)\n    clf.fit(X_train,y_train)\n    scoreCV = clf.score(X_test,y_test)\n    scores.append(scoreCV)\nplt.plot(feature_param, scores, '.-')\nplt.axis('tight')\n# plt.xlabel('parameter')\n# plt.ylabel('score')\nplt.title('Min Samples Split')\nplt.grid();\n\n#Min Weight Fraction Leaf\nplt.subplot(3,3,6)\nfeature_param = np.linspace(0,0.5,10)\nscores=[]\nfor feature in feature_param:\n    clf = RandomForestClassifier(min_weight_fraction_leaf =feature)\n    clf.fit(X_train,y_train)\n    scoreCV = clf.score(X_test,y_test)\n    scores.append(scoreCV)\nplt.plot(feature_param, scores, '.-')\nplt.axis('tight')\n# plt.xlabel('parameter')\n# plt.ylabel('score')\nplt.title('Min Weight Fraction Leaf')\nplt.grid();\n\n#Max Leaf Nodes\nplt.subplot(3,3,7)\nfeature_param = range(2,21)\nscores=[]\nfor feature in feature_param:\n    clf = RandomForestClassifier(max_leaf_nodes=feature)\n    clf.fit(X_train,y_train)\n    scoreCV = clf.score(X_test,y_test)\n    scores.append(scoreCV)\nplt.plot(feature_param, scores, '.-')\nplt.axis('tight')\n# plt.xlabel('parameter')\n# plt.ylabel('score')\nplt.title('Max Leaf Nodes')\nplt.grid();"
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "- Random Forest seems to show high variance in scores with most parameter changes.\n- Max Features, Criterion and Max Leaf Nodes show low variance in scores.\n- The general high varience in N Estimator plot shows the risk of overfitting."
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "####Gradient Boosting"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "plt.figure(figsize=(15,10))\n\n#N Estimators\nplt.subplot(3,3,1)\nfeature_param = range(1,21)\nscores=[]\nfor feature in feature_param:\n    clf = GradientBoostingClassifier(n_estimators=feature)\n    clf.fit(X_train,y_train)\n    scoreCV = clf.score(X_test,y_test)\n    scores.append(scoreCV)\nplt.plot(scores, '.-')\nplt.axis('tight')\n# plt.xlabel('parameter')\n# plt.ylabel('score')\nplt.title('N Estimators')\nplt.grid();\n\n#Learning Rate\nplt.subplot(3,3,2)\nfeature_param = np.linspace(0.1,1,10)\nscores=[]\nfor feature in feature_param:\n    clf = GradientBoostingClassifier(learning_rate=feature)\n    clf.fit(X_train,y_train)\n    scoreCV = clf.score(X_test,y_test)\n    scores.append(scoreCV)\nplt.plot(scores, '.-')\n# plt.xlabel('parameter')\n# plt.ylabel('score')\nplt.title('Learning Rate')\nplt.grid();\n\n#Max Features\nplt.subplot(3,3,3)\nfeature_param = ['auto','sqrt','log2',None]\nscores=[]\nfor feature in feature_param:\n    clf = GradientBoostingClassifier(max_features=feature)\n    clf.fit(X_train,y_train)\n    scoreCV = clf.score(X_test,y_test)\n    scores.append(scoreCV)\nplt.plot(scores, '.-')\nplt.axis('tight')\n# plt.xlabel('parameter')\n# plt.ylabel('score')\nplt.title('Max Features')\nplt.grid();\n\n#Max Depth\nplt.subplot(3,3,4)\nfeature_param = range(1,21)\nscores=[]\nfor feature in feature_param:\n    clf = GradientBoostingClassifier(max_depth=feature)\n    clf.fit(X_train,y_train)\n    scoreCV = clf.score(X_test,y_test)\n    scores.append(scoreCV)\nplt.plot(feature_param, scores, '.-')\nplt.axis('tight')\n# plt.xlabel('parameter')\n# plt.ylabel('score')\nplt.title('Max Depth')\nplt.grid();\n\n#Min Samples Split\nplt.subplot(3,3,5)\nfeature_param = range(1,21)\nscores=[]\nfor feature in feature_param:\n    clf = GradientBoostingClassifier(min_samples_split =feature)\n    clf.fit(X_train,y_train)\n    scoreCV = clf.score(X_test,y_test)\n    scores.append(scoreCV)\nplt.plot(feature_param, scores, '.-')\nplt.axis('tight')\n# plt.xlabel('parameter')\n# plt.ylabel('score')\nplt.title('Min Samples Split')\nplt.grid();\n\n#Min Weight Fraction Leaf\nplt.subplot(3,3,6)\nfeature_param = np.linspace(0,0.5,10)\nscores=[]\nfor feature in feature_param:\n    clf = GradientBoostingClassifier(min_weight_fraction_leaf =feature)\n    clf.fit(X_train,y_train)\n    scoreCV = clf.score(X_test,y_test)\n    scores.append(scoreCV)\nplt.plot(feature_param, scores, '.-')\nplt.axis('tight')\n# plt.xlabel('parameter')\n# plt.ylabel('score')\nplt.title('Min Weight Fraction Leaf')\nplt.grid();\n\n#Max Leaf Nodes\nplt.subplot(3,3,7)\nfeature_param = range(2,21)\nscores=[]\nfor feature in feature_param:\n    clf = GradientBoostingClassifier(max_leaf_nodes=feature)\n    clf.fit(X_train,y_train)\n    scoreCV = clf.score(X_test,y_test)\n    scores.append(scoreCV)\nplt.plot(feature_param, scores, '.-')\nplt.axis('tight')\n# plt.xlabel('parameter')\n# plt.ylabel('score')\nplt.title('Max Leaf Nodes')\nplt.grid();"
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "- The N Estimator plot seems very stable and hence resistant to overfitting.\n- Min Weight Fraction Leaf drops significantly after a certain point.\n- All other plots show very little variance."
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "####Decision Surface"
 },
 {
  "cell_type": "code",
  "execution_count": null,
  "metadata": {
   "collapsed": false
  },
  "outputs": [],
  "source": "#Plotting dicision boundries with columns Age and Fare. \nfrom itertools import product\n\n#Picking Age and Fare as they are continuous and highly correlated with Survived\nX = df[['Age', 'Fare']].values\ny = df['Survived'].values\n\n# Training classifiers\nclf1 = DecisionTreeClassifier()\nclf2 = RandomForestClassifier()\nclf3 = GradientBoostingClassifier()\n\n#Fit models\nclf1.fit(X, y)\nclf2.fit(X, y)\nclf3.fit(X, y)\n\n# Plotting decision regions\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n                     np.arange(y_min, y_max, 0.1))\n\nf, axarr = plt.subplots(1, 3, sharex='col', sharey='row', figsize=(15,4))\n\nfor idx, clf, tt in zip(range(3),\n                        [clf1, clf2, clf3],\n                        ['Decision Tree', 'Random Forest',\n                         'Gradient Boost']):\n\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    axarr[idx].contourf(xx, yy, Z, alpha=0.4)\n    axarr[idx].scatter(X[:, 0], X[:, 1], c=y, alpha=0.8)\n    axarr[idx].set_title(tt, fontsize=14)\n    axarr[idx].axis('off')"
 },
 {
  "cell_type": "markdown",
  "metadata": {},
  "source": "All 3 plots are similar on the right side in terms of classification by initially splitting at Age. The second split occurs at the bottom for Fare. Discrepensies in classification occur as we move further than 2 splits. As more branches are forming, Decision Tree and Random Forest risk overfitting as they try to form more pure leaves. Relatively, Gradient Boost seem to be forming less branches and hence resistant to overfitting."
 }
],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}}, "nbformat": 4, "nbformat_minor": 0}