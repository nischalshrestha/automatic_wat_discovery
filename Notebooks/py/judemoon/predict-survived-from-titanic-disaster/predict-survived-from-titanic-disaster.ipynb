{"cells": [{"cell_type": "markdown", "source": ["# Predict Survived from Titanic Disaster\n", "\n", "## August-September 2017, by Jude Moon\n", "Python3\n", "\n", "\n", "# Project Overview\n", "\n", "The sinking of the RMS Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class. \n", "\n", "In this project, I will analyze what sorts of people were likely to survive. In particular, I will apply the tools of machine learning to predict which passengers survived the tragedy.\n", "\n", "This document is to keep notes as I work through the project and show my thought processes and approaches to solve this problem. It consists of:\n", "\n", "Part1. Data Exploration\n", "- Missing Value (NaN) Investigation\n", "- Outliers Investigation\n", "- Summary of Data Exploration\n", "\n", "Part2. Feature Engineering\n", "- Creating New Features\n", "- Converting to Numeric Variables\n", "- Feature Exploration\n", "- Scaling Features\n", "- Feature Scores\n", "\n", "Part3. Algorithm Search\n", "- Algorithm Exploration\n", "- Building Pipelines\n", "\n", "\n", "***\n", "\n", "# Part1. Data Exploration\n"], "metadata": {"_uuid": "e4ada99f3d467a03f83b6da2e5e945bad48a1182", "_cell_guid": "c1d1c44c-bd08-4846-8993-dea899303674", "collapsed": true}}, {"cell_type": "code", "source": ["%pylab inline\n", "import pickle\n", "import numpy as np\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "import random\n", "import os\n", "import re\n", "import sys\n", "import pprint\n", "import operator\n", "import scipy.stats\n", "from time import time"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "3545a555be08df641275f9b09e17e50423b7130d", "_cell_guid": "73d01897-0cd2-4027-aa2d-d63249d8e403"}}, {"cell_type": "code", "source": ["# load data set\n", "titanic_df = pd.read_csv(\"../input/train.csv\")"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "d7ca8bf673e29a54f4c74e79e64400c3b5201ec0", "_cell_guid": "bfd83257-2bba-4428-9b7a-9a651899ff07", "collapsed": true}}, {"cell_type": "code", "source": ["# the first 5 rows\n", "titanic_df.head()"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "40060a8a991167d2baf32e967069035c294bcfa0", "_cell_guid": "31b3458d-d5bd-43b8-aa97-88d4ab89a8c1"}}, {"cell_type": "code", "source": ["# data type of each column\n", "titanic_df.dtypes"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "ec622bba4b828d9507b081029bf4b953346ebbe8", "_cell_guid": "0ba35932-545f-445c-a640-218b746a3be1"}}, {"cell_type": "code", "source": ["# check any numpy NaN by column\n", "titanic_df.isnull().sum(axis=0) # sum by column"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "73a7d46cc6b8e57a8b284ba110194fa002951a87", "_cell_guid": "bb9ba9e6-9fe2-4a5b-9977-b0fb07e1d76b"}}, {"cell_type": "code", "source": ["# statistics of central tendency and variability\n", "titanic_df.describe()"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "cc5844298f81201eb74800059948e660c38d486d", "_cell_guid": "8e37d4e9-c12b-4bd8-87e4-e2a5f855150a"}}, {"cell_type": "code", "source": ["# the numbers of survived and dead\n", "titanic_df.groupby(titanic_df['Survived']).count()['PassengerId']"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "109ff7137dbc4dbfec8f91187ec7ce4bc969c2c2", "_cell_guid": "9d7aa614-a48d-4ef6-bc03-86bf91bd325b"}}, {"cell_type": "markdown", "source": ["I learned general idea about the passengers: \n", "- total passenger number in training data set is 891\n", "- survival % is about 38%\n", "- Pclass is treated as integer, but actually it is category\n", "- since median of Pclass is 3rd class, passengers are donimated by 3rd class people\n", "- average age is 29.7 with missing 177 data points\n", "- sibsp and parch variables are little bit tricky with a lot of zeros\n", "- mean fare is 32 units\n", "- cabin has so many missing values\n", "\n", "## Missing Value (NaN) Investigation\n", "\n", "### Would NaN introduce bias to 'Cabin'?\n", "\n", "'Cabin' column has 687 missing values with is 77% of the total. This might introduce bias, so I would like to investigate what is average survival rates of the groups with missing value on 'Cabin' compared to others with 'Cabin' value. "], "metadata": {"_uuid": "382e7846709d96e2552e74603f39be253ed4e919", "_cell_guid": "075ff1a4-c58c-4671-85e9-7a84264b2e80"}}, {"cell_type": "code", "source": ["# survival rate by group with missing value on Cabin; True means missing value\n", "titanic_df.groupby(titanic_df['Cabin'].isnull()).mean()['Survived']"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "2a719da7a4aaf05739eebee21bac8edb69b639f4", "_cell_guid": "07b6b260-1e9e-4e47-bd1f-d9cab5243c05"}}, {"cell_type": "markdown", "source": ["The survival rate of the group with missing value (True) is lower than the average (0.38), while that with value (False) is greater than the average. Missing values of 'Cabin' have a high tendency of introducing bias, meaning that the group of passengers with missing value on 'Cabin' is associated with lower survival rate than those with Cabin value. This would cause that if a supervised classification algorithm was to use 'Cabin' as a feature, it might interpret \"NaN\" for 'Cabin' as a clue that a person is not survived. So, I have to carefully use 'Cabin' as a feature for supervised classifier algorithms. I am not going to worry about dealing with NaN on 'Cabin' for now because it is not a number. And so I can simply convert NaN to string 'NaN' if I need to.\n", "\n", "### What about missing value on column 'Age'? How to deal with missing values?"], "metadata": {"_uuid": "fea0202dca7bc7d903aa41794be3baa05eb59db8", "_cell_guid": "51e92ff1-7c09-4f14-942a-e5dd5a067caa"}}, {"cell_type": "code", "source": ["# survival rate by group with missing value on Age; True means missing value\n", "titanic_df.groupby(titanic_df['Age'].isnull()).mean()['Survived']"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "591aa552e8862a605ddcdfacd42a90d8d244db76", "_cell_guid": "2cb4d014-e58b-4e86-b54b-9413045f7990"}}, {"cell_type": "markdown", "source": ["About 20% data is missing for 'Age'. There is a possibility of NaN-drived bias but not strong as the bias for 'Cabin'. My choice to deal with the missing value is fill NaN with the median of the sample."], "metadata": {"_uuid": "c34df3eedca7505ca50b38177038d1f4a9f91e22", "_cell_guid": "3c04cdcc-913e-4d52-ba10-77a563e96c62"}}, {"cell_type": "code", "source": ["# replace NaN with the median of Age and create new column called age\n", "titanic_df['age'] = titanic_df['Age'].fillna(titanic_df[\"Age\"].median())\n", "\n", "titanic_df['age'].describe()"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "b3a2d11b3ba5e4c9f8ad8891f837f5ea50e7f542", "_cell_guid": "668b486b-30f5-4aee-bee5-efd4e65ae110"}}, {"cell_type": "markdown", "source": ["### What about missing value on column 'Embarked'? How to deal with missing values?\n", "Only two observations are missing for 'Embarked'. I could ignore them or replace them with most frequent port."], "metadata": {"_uuid": "c377ba021976cae82038bbd770eb85f9634cf3e2", "_cell_guid": "4b96a28c-769c-4e3d-a2f5-481562a490fb"}}, {"cell_type": "code", "source": ["# survival rate by group with missing value on Age; True means missing value\n", "titanic_df.groupby(titanic_df['Embarked'].isnull()).mean()['Survived']"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "575c0782e0a7165733038a23df8fd0ddba415826", "_cell_guid": "30e97f33-fa09-49ba-a3f6-e0e34a892ff3"}}, {"cell_type": "code", "source": ["titanic_df.groupby(titanic_df['Embarked']).count()['PassengerId']"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "8a22d86676875590034f5553b81c2941ee559a35", "_cell_guid": "82940266-ae9f-4cce-a5fd-b30c063bb733"}}, {"cell_type": "code", "source": ["# replace NaN with the dominant port and create new column called Port\n", "titanic_df['embarked'] = titanic_df['Embarked'].fillna('C')\n", "\n", "titanic_df['embarked'].isnull().sum()"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "18bb4cb595f0032bdcd7c830ba7cb3f168a2c4c2", "_cell_guid": "1707db41-9543-450b-930e-2bc58e7149ad"}}, {"cell_type": "markdown", "source": ["## Outliers Investigation\n", "\n", "### Is there an observation who has a lot of NaN?"], "metadata": {"_uuid": "0d75503eba2f443067fb5b2d8586ef374303c377", "_cell_guid": "eb035108-932c-4da1-af0e-5aaf07f07f08"}}, {"cell_type": "code", "source": ["# check any numpy NaN by row\n", "#titanic_df.isnull().sum(axis=1) # sum by row\n", "titanic_df.isnull().sum(axis=1).max() # find the max"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "ffca1d439093b6232101559f150382ec80fc5122", "_cell_guid": "6fe62929-e6d4-4544-8b92-b32e592d5eef"}}, {"cell_type": "markdown", "source": ["No, there is no observation who has missing values more than two. So, we can keep all the observations.\n", "\n", "### Are there any outliers in the dataset?"], "metadata": {"_uuid": "427d494010a42e23c3367f3c59c4a129b8654f7f", "_cell_guid": "a7c94abe-a98b-4dd1-882d-9e426d6a191d"}}, {"cell_type": "code", "source": ["# I defined outliers as being above of 99% percentile here\n", "# get lists of people above 99% percentile for each feature\n", "highest = {}\n", "for column in titanic_df.columns:\n", "    if titanic_df[column].dtypes != \"object\": # exclude string data typed columns\n", "        highest[column]=[]\n", "        q = titanic_df[column].quantile(0.99)\n", "        highest[column] = titanic_df[titanic_df[column] > q].index.tolist()\n", "    \n", "pprint.pprint(highest)"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "6b050193a3e31484adf343bdf4ef3c4fceb3ec58", "_cell_guid": "71633906-05a6-45cd-89f7-337b12054b00"}}, {"cell_type": "code", "source": ["# delete 'PassengerId' from dictionary highest\n", "highest.pop('PassengerId', 0)"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "e0c104f393ae1c81de31319602bb5924bc1ca7fe", "_cell_guid": "264a3952-fc04-4d0b-ad3c-139890316699"}}, {"cell_type": "markdown", "source": ["### What are the outliers repeatedly shown among the features?"], "metadata": {"_uuid": "4ced217d38ff487fb79beed18f8d1e508b0b5812", "_cell_guid": "0e935ae6-57fa-4670-a08f-0de76e3ab134"}}, {"cell_type": "code", "source": ["# summarize the previous dictionary, highest\n", "# create a dictionary of outliers and the frequency of being outlier\n", "highest_count = {}\n", "for feature in highest:\n", "    for person in highest[feature]:\n", "        if person not in highest_count:\n", "            highest_count[person] = 1\n", "        else:\n", "            highest_count[person] += 1\n", "             \n", "highest_count"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "60c82ac9d6cca14b1c44495752ae467fb9be6550", "_cell_guid": "c556f41d-ad5f-46db-a492-7b2514bd699e"}}, {"cell_type": "code", "source": ["# This time, I defined outliers as being below of 1% percentile here\n", "# get lists of people below 1% percentile for each feature\n", "lowest = {}\n", "for column in titanic_df.columns:\n", "    if titanic_df[column].dtypes != \"object\": # exclude string data typed columns\n", "        lowest[column]=[]\n", "        q = titanic_df[column].quantile(0.01)\n", "        lowest[column] = titanic_df[titanic_df[column] < q].index.tolist()\n", "\n", "# delete 'PassengerId' from dictionary highest\n", "lowest.pop('PassengerId', 0)\n", "\n", "pprint.pprint(lowest)"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "091d4ec2a1b5800de94ebce66c31e58a6b39bc33", "_cell_guid": "81610d7c-c808-494d-a38f-5ad620c59b9c"}}, {"cell_type": "code", "source": ["for person in lowest['age']:\n", "    if person not in highest_count:\n", "        highest_count[person] = 1\n", "    else:\n", "        highest_count[person] += 1\n", " \n", "highest_count"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "85621f71e0ba41f0cb20a39bd0acf5e480b173a7", "_cell_guid": "5d387fee-d1fb-4f47-967d-81e192b9b4de"}}, {"cell_type": "markdown", "source": ["Overall, there is no outlier that are repeatedly shown among the features.\n", "\n", "We can focus on age and Fare for continous values and Parch and SibSp for integer values to further investiage outliers. \n", "\n", "### Take a look at outliers"], "metadata": {"_uuid": "64af8004f39afe42f30f06b183addc956ceda8f2", "_cell_guid": "099e1248-7c10-41e5-a6b3-beecc2fb388d"}}, {"cell_type": "code", "source": ["# fare above 99% percentile\n", "titanic_df.loc[highest['Fare'],['Fare', 'Survived']]"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "ab1b155a47a4c8d8af294606e0b5ce9d923af0c4", "_cell_guid": "39706793-3025-4a32-8e29-a65bae41faa8"}}, {"cell_type": "markdown", "source": ["The mean fare is 32 units but there are outliers who paid 262, 263, or 512 units, which are 8 to 16 times higher than the mean fare. I am going to keep these outliers because this might help to classify survival as extreme cases in such decision tree algorithm."], "metadata": {"_uuid": "5f661fdc340849587f7c21b06ad8d49d32f6c033", "_cell_guid": "ea5e0c61-a557-4199-9e9c-1cb9a91ae04e"}}, {"cell_type": "code", "source": ["# age above 99% percentile\n", "titanic_df.loc[highest['age'],['age', 'Survived']]"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "3d50ed8a8d56cce7da6a8b9d2c10bde260d3086a", "_cell_guid": "0a5a605e-6236-4eda-a489-863391f87d01"}}, {"cell_type": "code", "source": ["# age below 1% percentile\n", "titanic_df.loc[lowest['age'],['age','Survived']]"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "9551a1477e78aaf25d0984dedb4b7c758f2699c8", "_cell_guid": "a31da8e7-c76d-4a1c-a0c8-df1edbe39449"}}, {"cell_type": "markdown", "source": ["The average age is 29, but there are outliers who are 66 to 80 years old, which are 2.3 to 2.8 times higher than the average age, and who are younger than one year old. I am going to keep these outliers for now because the extreme cases of age might be usefull.\n", "\n", "## Summary of Data Exploration\n", "\n", "- Total number of data points: 891\n", "- Target: \u2018Survived\u2019\n", "- Total number of data points labeled as survived: 342 (38%)\n", "- Total number of data points labeled as dead: 549 (62%)\n", "- Slightly imbalanced classes\n", "- Number of initial features: 10\n", "- List of features with missing values or NaN: \n", "\n", "| Feature  | # of NaN | Survival rate of NaN | Survival rate of non-Nan | Difference in survival rate |\n", "|----------|----------|----------------------|--------------------------|-----------------------------|\n", "| Cabin    | 687      | 0.30                 | 0.67                     | 0.37                        |\n", "| Age      | 177      | 0.29                 | 0.41                     | 0.12                        |\n", "| Embarked | 2        | 1.00                 | 0.38                     | -0.62                       |\n", "\n", "- Top 3 people repeatedly shown as outliers:\n", "- The mean fare is 32 units but there are outliers who paid 262, 263, or 512 units, which are 8 to 16 times higher than the mean fare.\n", "- The average age is 29, but there are outliers who are 66 to 80 years old, which are 2.3 to 2.8 times higher than the average age, and who are younger than one year old.\n", "- Overall, there is no outlier that are repeatedly shown among the features. \n", "\n", "***\n", "\n", "\n", "# Part2. Feature Engineering\n", "\n", "### Brainstorming\n", "\n", "The target is 'Survived', and the rest columns are the candidate features: \n", "- 'Fare', 'age', 'Sex', and 'embarked' are ready to go without engineering\n", "- 'Name', 'Ticket', and 'Cabin' are text variables and might require variation reduction. For example, use only last name instead of using full name with title. \n", "- 'SibSp' and 'Parch' have a lot of zeros, where the zero value means truely zero or absence. Squre transformation can be considered. Also, creating a new feature like 'is_family' by combining 'SibSp', 'Parch' and 'family_name' from 'Name'.\n", "\n", "### Challenges\n", "\n", "The features are mixed with continuous and categorial variables. Most ML algorithms work well with numerical variables and some work with mixed data types. I can think of several approaches:\n", "- use algorithms that can handle variables with both data types (DT, NB, KNN)\n", "- use demensionality reduction method to get numerical vectors\n", "- convert non-ordinal categoical variables to numerical or dummy ([padas.get_dummies](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html), [OneHotEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder)) variables and use algorithms that works for numerical variables; the limitation would be that not all features can be utilized due to a large number of unique values\n", "- use ensemble method to combine algorithms for numerical variables and algorithms for categorical variables\n", "http://fastml.com/converting-categorical-data-into-numbers-with-pandas-and-scikit-learn/\n", "\n", "## Creating New Features\n", "\n", "### family_name"], "metadata": {"_uuid": "f6fee30e8feb6979b57a731f87e65740c5a46bf7", "_cell_guid": "6c28eb89-bf54-458c-94b9-80e25d1508b5"}}, {"cell_type": "code", "source": ["# a procedure to create a column with family name only\n", "def get_familyname(name):\n", "    full_name = name.split(',')\n", "    return full_name[0]\n", "\n", "# apply get_familyname procedure to the column of 'Name'\n", "familyname = titanic_df['Name'].apply(get_familyname)\n", "\n", "# add familyname to the DataFrame as new column\n", "titanic_df['family_name'] = familyname.values"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "5d23c04139e963b128fdb6f6f49e02e993474e31", "_cell_guid": "bc3b51d4-a943-4158-b4fc-e36e775222b6", "collapsed": true}}, {"cell_type": "code", "source": ["# how many unique family name?\n", "len(titanic_df['family_name'].unique())"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "b6fed89e92c85e3a65d8fa025630c2c7a2be1e4c", "_cell_guid": "ab84db54-1fcd-4b26-ae69-d4fe80952841"}}, {"cell_type": "markdown", "source": ["### ticket_prefix"], "metadata": {"_uuid": "2085912b01c86da13364391de3c5fb92c24b3536", "_cell_guid": "feddb547-ddb0-4de5-8629-63d377595e6c"}}, {"cell_type": "code", "source": ["# understand 'Ticket' values\n", "titanic_df['Ticket'].head(10)"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "ce91f4b2839bf10aa3dcee52b7294d424297c091", "_cell_guid": "fac5f9d8-11d7-44ca-8ea2-7be5bd3cf7c8"}}, {"cell_type": "code", "source": ["# how many unique family name?\n", "len(titanic_df['Ticket'].unique())"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "933a6c1cd5fd7b7141af620bd5aa20652bbc43cd", "_cell_guid": "06584eff-35c0-46d8-bf2d-17972e862bc8"}}, {"cell_type": "markdown", "source": ["'Ticket' variable is not consistant in terms of the format; some are mixed with letters and numbers, some has symbols (/ or .), and the others consist of only numbers. And not all Tickets are different. "], "metadata": {"_uuid": "71bb23e10d44f90cb5993835e08e3d799402b0e4", "_cell_guid": "39352068-20a8-4aa3-a63c-a7591e9d53ee"}}, {"cell_type": "code", "source": ["# a procedure to create a column with ticket prefix only\n", "def get_prefix(ticket):\n", "    if ' ' in ticket:\n", "        prefix = ticket.split(' ')\n", "        return prefix[0]\n", "    else:\n", "        return 'None'\n", "\n", "# apply get_prefix procedure to the column of 'Ticket'\n", "ticketprefix = titanic_df['Ticket'].apply(get_prefix)\n", "\n", "# add ticket_prefix to the DataFrame as new column\n", "titanic_df['ticket_prefix'] = ticketprefix.values"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "b04c0063763bd8b323131ab3d880f0979e182c20", "_cell_guid": "75dd6ed2-dd6c-4d04-a167-0702ee5ec00b", "collapsed": true}}, {"cell_type": "code", "source": ["# count of ticket prefix; False means ticket_prefix == 'None'\n", "titanic_df.groupby(titanic_df['ticket_prefix'] != 'None').count()['PassengerId']"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "e475111539e6274c73518b25edc5eeda31565cd6", "_cell_guid": "a4f9219b-0f66-4648-8717-5ec44eb7f0ef"}}, {"cell_type": "code", "source": ["# survival rate by group with ticket prefix; False means ticket_prefix == 'None'\n", "titanic_df.groupby(titanic_df['ticket_prefix'] != 'None').mean()['Survived']"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "a7f23413b7c88d6746db6ac2834d60968881bcf9", "_cell_guid": "88c99219-61a6-4ec7-ab61-bbfdb03fe23b"}}, {"cell_type": "markdown", "source": ["I found no difference in survival rate in the group with vs. without ticket prefix, and both rates are similar to the average of the total."], "metadata": {"_uuid": "c0094130189cf33d37bcde9738c259abc32e6436", "_cell_guid": "e903fbcd-4e0e-46aa-9f02-8f7a81cb9d6e"}}, {"cell_type": "code", "source": ["# frequency of ticket_prefix\n", "titanic_df.groupby(titanic_df['ticket_prefix']).count()['PassengerId']"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "38e923b1619c0445a2d64b1276f0c3e36ab5e592", "_cell_guid": "13c59752-b93d-4702-b61b-049fb7096fbd"}}, {"cell_type": "code", "source": ["# how many unique ticket_prefix?\n", "len(titanic_df['ticket_prefix'].unique())"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "b3b6d874540181548562a72601214e4aa8e950d3", "_cell_guid": "a5c18466-64a0-40f3-984c-39ee57caeb5c"}}, {"cell_type": "markdown", "source": ["I found inconsistency in formatting of prefix. For example, A./5., A.5., A/5, and A/5. could be the same prefix and A/S could be the typo for A/5. I am not sure making the formatting consistent would help to better classify the survived, or the differences in the formatting actually would help to classify them. "], "metadata": {"_uuid": "10fb80987b318fbe53fe0608ff7bb1fa1bd70bd8", "_cell_guid": "95c5a18b-9db7-4c2a-b79c-ea3516400a55"}}, {"cell_type": "code", "source": ["# procedure to remove all special characters and change to upper case\n", "def remove_special(initial):\n", "    return (''.join(e for e in initial if e.isalnum())).upper()\n", "\n", "titanic_df['ticket_prefix_v2'] = titanic_df['ticket_prefix'].apply(remove_special)\n", "\n", "# frequency of ticket_prefix_v2\n", "titanic_df.groupby(titanic_df['ticket_prefix_v2']).count()['PassengerId']"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "6feba52360373578264d69e592ddcecaf8a71708", "_cell_guid": "a211bd7c-eb7c-429c-94ff-870c495a45fe"}}, {"cell_type": "code", "source": ["# survival rate by ticket_prefix_v2\n", "titanic_df.groupby(titanic_df['ticket_prefix_v2']).mean()['Survived']"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "e642cda9f28d4e917a7a5125055b098592d64a2f", "_cell_guid": "9c57c8a4-d70e-40b7-b9c4-11840d886c30"}}, {"cell_type": "code", "source": ["# how many unique ticket_prefix_v2?\n", "len(titanic_df['ticket_prefix_v2'].unique())"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "4f41e0ffebaf93402dd0db0cbd9eca2dde1ffccc", "_cell_guid": "0bba4233-5d8b-452d-9c34-2d13837fc244"}}, {"cell_type": "markdown", "source": ["Now the number of unique ticket prefix was 43 and now it is 29 after cleaning the special characters.\n", "\n", "I think putting the cleaned prefix and the number back togther might help."], "metadata": {"_uuid": "34578664153f6c03e8833fe6702206ce66abc388", "_cell_guid": "b9314fbf-0b09-4b81-bad4-462ecbb7907c"}}, {"cell_type": "code", "source": ["# a procedure to create a column with ticket number only\n", "def get_number(ticket):\n", "    if ' ' in ticket:\n", "        number = ticket.split(' ')\n", "        return number[1]\n", "    else:\n", "        return ticket\n", "\n", "# apply get_number procedure to the column of 'Ticket'\n", "ticketnumber = titanic_df['Ticket'].apply(get_number)\n", "\n", "# add ticket_number to the DataFrame as new column\n", "titanic_df['ticket_number'] = ticketnumber.values\n", "\n", "titanic_df['ticket_number'].head(10)"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "faa141068e9276637c566e1b1afa9dd7298eeceb", "_cell_guid": "fe314bd5-1c9b-4e61-91f1-820064bcf931"}}, {"cell_type": "code", "source": ["# add ticket to the DataFrame as new column by concatenating cleaned initial and number\n", "titanic_df['ticket'] = titanic_df['ticket_prefix_v2'] + titanic_df['ticket_number'] \n", "\n", "titanic_df['ticket'].head()"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "7ded346a5d9e9c896524c478e305b15f56a85703", "_cell_guid": "316f37b9-4005-4aa8-89a7-a8082cb28b41"}}, {"cell_type": "code", "source": ["# how many unique ticket?\n", "len(titanic_df['ticket'].unique())"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "c11dc6c9d9d13b5271295f1613bbce0b546c0534", "_cell_guid": "e5c5acf5-7337-4f8c-ad14-5b1dca7d4dd9"}}, {"cell_type": "markdown", "source": ["### cabin_initial"], "metadata": {"_uuid": "e7420f2eafd10cafdc896af8f4040f30fb28d42e", "_cell_guid": "ef1aa1f1-6e8a-46ec-b08c-088b3f2dc033"}}, {"cell_type": "code", "source": ["# understand 'Cabin' values\n", "titanic_df['Cabin'].head(10)"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "a75be0d9edb888f868f78689180e911ac2d9fb3d", "_cell_guid": "e902f67d-0d11-4b5e-8998-055468a99a08"}}, {"cell_type": "markdown", "source": ["The 'Cabin' value consists of a capital letter following by numbers. I am not sure what the letter and numbers mean for but my intuition is that the letter could represent a room location or a room price, so the letter only can be used as a feature. "], "metadata": {"_uuid": "f3981433d3feb5ffc0716c9deb499ecf96f5dd52", "_cell_guid": "17652347-5a59-4102-ac9e-0ed87c745d02"}}, {"cell_type": "code", "source": ["# how many unique Cabin?\n", "len(titanic_df['Cabin'].unique())"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "81a42a1ccb5bd0fa22105b40cbac1542b5ae53e6", "_cell_guid": "65a61d62-1c81-408e-9650-b3315241f248"}}, {"cell_type": "markdown", "source": ["Out of 204 known 'Cabin', 148 are the unique 'Cabin', and some people share the same 'Cabin' value."], "metadata": {"_uuid": "2fb876d6b29ca8e94a38b09e4fea37e171dcaf04", "_cell_guid": "9526b859-d171-4711-a65d-2a1eac7fedb5"}}, {"cell_type": "code", "source": ["def get_initial_letter(cabin):\n", "    cabin = str(cabin) # change data type to string becuase nan is float\n", "    return cabin[0]\n", "\n", "titanic_df['cabin_initial'] = titanic_df['Cabin'].apply(get_initial_letter)\n", "titanic_df['cabin_initial'].head(10)"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "56d6504ba7eedecdcd340fa0ee888f1761e8e589", "_cell_guid": "71ae6499-4bfd-4250-adf5-e4026f553f70"}}, {"cell_type": "code", "source": ["# survival rate by group with missing value on Cabin; True means missing value\n", "titanic_df.groupby(titanic_df['cabin_initial']).mean()['Survived']"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "762888c83892c6ed214bec1bcee3da3bfe28130e", "_cell_guid": "4329c257-635c-4ab4-952b-d972032912d8"}}, {"cell_type": "code", "source": ["# how many unique 'cabin_initial'?\n", "len(titanic_df['cabin_initial'].unique())"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "9dbbd0f3cbe12a71f9d4eaa29f46a150237bae81", "_cell_guid": "9f71d13c-1066-456a-9303-b71ce6fd17e8"}}, {"cell_type": "markdown", "source": ["### w_family"], "metadata": {"_uuid": "5cb8e3498a449b6feb666a5b54b13343b9e54e59", "_cell_guid": "17c83368-d253-4294-9097-f37642f4d8c0"}}, {"cell_type": "code", "source": ["# SibSp and Parch are combined as family by vectoried addition\n", "sibsp = titanic_df['SibSp']\n", "parch = titanic_df['Parch']\n", "\n", "family = sibsp + parch\n", "\n", "#change datatype to categories with 2 groups\n", "def w_family(family):\n", "    if family != 0:\n", "        return 1\n", "    return 0\n", "\n", "# apply w_family procedure to the array\n", "w_family = family.apply(w_family)\n", "\n", "# add w_family to the DataFrame as new column\n", "titanic_df['w_family'] = w_family.values"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "732165f6829ec20918c8ff3437ca5b18ef083696", "_cell_guid": "9d94e430-8b20-408d-ac27-3b25de57c952", "collapsed": true}}, {"cell_type": "markdown", "source": ["## Converting to Numeric Variables\n", "\n", "### sex"], "metadata": {"_uuid": "d2bcf4a2fe360e3642842ff62e2c3f32d7155507", "_cell_guid": "cd9e2d65-0424-4caf-8fc9-bacca27d86d6"}}, {"cell_type": "code", "source": ["# values of Sex\n", "titanic_df[\"Sex\"].unique()"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "eb25d4eb2ea31dc7c48991d80dfac1c5595f6446", "_cell_guid": "e3a23731-7884-439f-9253-30e7527034b7"}}, {"cell_type": "code", "source": ["# if male, return True or 1 and create new column 'sex'\n", "titanic_df['sex'] = (titanic_df['Sex'] == 'male').astype(int)\n", "\n", "# values of sex\n", "titanic_df[\"sex\"].unique()"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "14919d54cd3f366ddc8796219fd4ee4de844d13f", "_cell_guid": "dcfe8b78-f871-45bd-9f55-1499948e155b"}}, {"cell_type": "markdown", "source": ["### embarked to C, Q, and S"], "metadata": {"_uuid": "a6f7445b538b8566e12842fc87aeeed429ad7338", "_cell_guid": "00173cac-aafe-4216-8a44-0a803e450354"}}, {"cell_type": "code", "source": ["# values of embarked\n", "titanic_df[\"embarked\"].unique()"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "8d9eda00b099b72cec7a248d438963626c8203e1", "_cell_guid": "2e59d07e-970e-4580-bb79-19a6b1aaca87"}}, {"cell_type": "code", "source": ["# create dummy variables for embarked\n", "embarked_df = pd.get_dummies(titanic_df[\"embarked\"])"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "c4dad42f71ccc2524144c695c5655b15a2217e46", "_cell_guid": "ea3f5c3e-9943-4254-beb2-70fe2570f3f9", "collapsed": true}}, {"cell_type": "code", "source": ["# combine two dataframes\n", "titanic_df = [titanic_df, embarked_df]\n", "titanic_df = pd.concat(titanic_df, axis=1, join='inner')\n", "\n", "titanic_df.describe()"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "46623caba95720ba22aaea933700cff9f0e8a77b", "_cell_guid": "377cec8a-2b06-4136-ac3f-28bea5caa695"}}, {"cell_type": "code", "source": ["feature_total = np.array(titanic_df.columns)\n", "\n", "feature_total"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "083f9bdb9c04640f12a237f0bdd3febccb64969a", "_cell_guid": "27ca8a1b-21e1-4b3d-af61-6772b9ccb54f"}}, {"cell_type": "code", "source": ["feature_numeric = []\n", "for column in titanic_df.columns:\n", "    if titanic_df[column].dtypes != \"object\" and titanic_df[column].isnull().sum() == 0:\n", "        feature_numeric.append(column)\n", "\n", "feature_numeric"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "9104ef0b1bf5a608709b03f781e7520774dca55c", "_cell_guid": "bfb9931e-665f-4c29-baed-221636fa90bc"}}, {"cell_type": "code", "source": ["# remove id and target\n", "feature_numeric = [e for e in feature_numeric if e not in ('PassengerId', 'Survived', 'survived')]\n", "\n", "feature_numeric"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "83b9fedf26a3b91ad4ed05c47b86e743bd2313ef", "_cell_guid": "e8c33dbd-f2f4-44a4-a951-d114b6e05012"}}, {"cell_type": "code", "source": ["len(feature_numeric)"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "a7bb4dc5cd74dbbc8b0870c06f5ebaf88214b7a9", "_cell_guid": "7d8d5d1d-5112-43dc-a6e1-7ee29e5a01b1"}}, {"cell_type": "code", "source": ["feature_numeric[:5]"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "c73fd8ae7bb6e2d625b85d9c18bb6ebbc908769a", "_cell_guid": "4721a964-2e0d-4926-94fc-6732da7ec43b"}}, {"cell_type": "markdown", "source": ["## Scaling Features\n", "\n", "I will use **MinMaxScaler** to adjust the different units of features to be equally weighted and ranged between 0-1."], "metadata": {"_uuid": "e38c73531035a3185d439cb72fd2bae98cf84cc4", "_cell_guid": "f9a1d600-9290-45e3-9cc4-151ce2676adb"}}, {"cell_type": "code", "source": ["df_numeric = titanic_df[feature_numeric[:5]]"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "75b30491fd1e94e8233bca371d8c988de9d3a399", "_cell_guid": "c9eb3deb-4e3b-4129-96a3-7f27f9e7cc36", "collapsed": true}}, {"cell_type": "code", "source": ["from sklearn.preprocessing import MinMaxScaler\n", "scaler = MinMaxScaler()\n", "df_scaled = pd.DataFrame(scaler.fit_transform(df_numeric), \\\n", "                         index=df_numeric.index, columns=df_numeric.columns)\n", "\n", "df_scaled = df_scaled.rename(columns={\"Pclass\": \"pclass_scl\", \"SibSp\": \"sibsp_scl\", \\\n", "                                      \"Parch\": \"parch_scl\", \"Fare\": \"fare_scl\", \\\n", "                                      \"age\": \"age_scl\"})\n", "\n", "df_scaled.describe()"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "fb7d50060179cd610f24aaf08759eac7fde5b910", "_cell_guid": "b739672a-b36d-4e13-a27a-8d3bebd39f36"}}, {"cell_type": "code", "source": ["df = [titanic_df, df_scaled]\n", "df = pd.concat(df, axis=1, join='inner')\n", "# how to merge two dataframes: https://pandas.pydata.org/pandas-docs/stable/merging.html\n", "\n", "df.describe()"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "048cd4d70d7751bf1ea4b9d594c711318b36f466", "_cell_guid": "321327cf-a001-4a6b-b7b9-bab84e308966"}}, {"cell_type": "code", "source": ["df.dtypes"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "358c6d92bd8465f2caa7bad9dd885827d6188254", "_cell_guid": "a2e87f7c-554a-4ab1-b7d7-9a91b3c48c23"}}, {"cell_type": "code", "source": ["# define features lists\n", "original_numeric = ['Pclass', 'SibSp', 'Parch', 'Fare', 'age', 'sex']\n", "original_categorical = ['Name', 'Ticket', 'Cabin', 'embarked']\n", "original_total = original_numeric + original_categorical\n", "                    \n", "scaled_numeric = ['pclass_scl', 'sibsp_scl', 'parch_scl', 'fare_scl', 'age_scl', 'w_family', 'sex', 'C', 'Q', 'S']\n", "updated_categorical = ['family_name', 'ticket', 'ticket_prefix_v2', 'cabin_initial']\n", "updated_total = scaled_numeric + updated_categorical                    "], "outputs": [], "execution_count": null, "metadata": {"_uuid": "6f5aee454c1c7c70bb629b6f1e722a2c4a43b302", "_cell_guid": "2d1daadf-1cc7-4a0c-825a-705fb9ca42e8", "collapsed": true}}, {"cell_type": "markdown", "source": ["## Feature Exploration\n", "\n", "| List Name            | Features                                                                                          | # of Features |\n", "|----------------------|---------------------------------------------------------------------------------------------------|---------------|\n", "| original_numeric     | ['Pclass', 'SibSp', 'Parch', 'Fare', 'age', sex']                                                 | 6             |\n", "| scaled_numeric       | ['pclass_scl', 'sibsp_scl', 'parch_scl', 'fare_scl', 'age_scl', 'w_family', 'sex', 'C', 'Q', 'S'] | 10            |\n", "| original_categorical | ['Name', 'Ticket', 'Cabin', 'embarked']                                                           | 4             |\n", "| updated_categorical  | ['family_name', 'ticket', 'ticket_prefix_v2', 'cabin_initial']                                    | 4             |\n", "\n", "\n", "\n", "## Selecting Features \n", "\n", "### What selection process to use?\n", "\n", "- Univariate Selection such as SelectKBest: statistical tests can be used to select the features that have the strongest relationship with the output variable. \n", "\n", "    For the first trial, I will choose 9 or less features. The number 9 threshold came from the curve of dimensionality, where you may need exponentially more data points as you add more features, that is, \n", "\n", ">2^(n_featuers) = # of data points \n", "\n", "    I have 891 data points. 2^9 = 512 and 2^10 = 1024, so 9 is the max feature number. Thus, I will keep in mind to use no more than 9 features if I decide to use SelectKBest.\n", "\n", "- Dimensionality Reduction such as PCA: PCA (or Principal Component Analysis) uses linear algebra to transform the dataset into a compressed form. I think chosing 2-3 dimensions after PCA transformation could be good start.\n", "\n", "### Which feature scores to compare?\n", "\n", "I choose **f_classif** scoring function for continous variables and **chi2** for categoerical variables. \n", "\n", "- [Variance](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html#sklearn.feature_selection.VarianceThreshold) can be useful for unsupervised classification. Since I have already labels, utilizing labels for scoring could be better than soley reling on x-variables. \n", "\n", "- [The mutual information (MI)](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html#sklearn.feature_selection.mutual_info_classif) between two random variables is a non-negative value, which measures the dependency between the variables. It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency. MI can be used for unsupervised clustering.\n", "\n", "- The chi-square distribution arises in tests of hypotheses concerning the independence of two random variables and concerning whether a discrete random variable follows a specified distribution. The F-distribution arises in tests of hypotheses concerning whether or not two population variances are equal and concerning whether or not three or more population means are equal. In other words, chi-square is most appropriate for categorical data, whereas f-value can be used for continuous data [(read more)](https://discussions.udacity.com/t/f-classif-versus-chi2/245226).\n", "\n"], "metadata": {"_uuid": "5dfbee2954a16464915cd3ff29fe15fc8e1467c9", "_cell_guid": "94f4dedb-3477-4239-ba18-bfde48f2cf81"}}, {"cell_type": "markdown", "source": ["\n", "***\n", "\n", "# Part3. Algorithm Search\n", "\n", "## Algorithm Exploration\n", "\n", "When dealing with small amounts of data, it\u2019s reasonable to try as many algorithms as possible and to pick the best one since the cost of experimentation is low according to [blog post by Cheng-Tao Chu](http://ml.posthaven.com/machine-learning-done-wrong).\n", "\n", "- SVC\n", "- KNeighbors \n", "- Gaussian Naive Bayes\n", "- Decision Trees\n", "- Ensemble Methods\n", "\n", "## Validation Methods \n", "I think a proper validation method for the dataset with imbalanced classes is using cross validation iterators with stratification based on class labels, such as **StratifiedKFold** and **StratifiedShuffleSplit**. This would ensure that relative class frequencies is approximately preserved in each train and test set."], "metadata": {"_uuid": "0e809e8068f6dfa95948231b5c63b585ad5482f8", "_cell_guid": "7cb28a6c-a4ce-4836-91ba-9926acbdb914"}}, {"cell_type": "markdown", "source": ["## Building Pipelines\n"], "metadata": {"_uuid": "674e8df631b9f6608248f6af6d38a56a23d2472a", "_cell_guid": "837a37e6-7a63-4702-a073-484f124d8d6d"}}, {"cell_type": "code", "source": ["from sklearn.feature_selection import SelectKBest\n", "from sklearn.model_selection import GridSearchCV\n", "from sklearn.model_selection import StratifiedShuffleSplit\n", "from sklearn.model_selection import cross_val_score\n", "#from sklearn.model_selection import cross_validate\n", "from sklearn.pipeline import Pipeline\n", "from sklearn.decomposition import PCA\n", "from sklearn.svm import SVC\n", "from sklearn.naive_bayes import GaussianNB\n", "from sklearn.neighbors import KNeighborsClassifier\n", "from sklearn import tree\n", "from sklearn.ensemble import AdaBoostClassifier\n", "from sklearn.ensemble import RandomForestClassifier"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "cc5d970e83d4a71520408d20a2cdab9804f0b816", "_cell_guid": "f7e89cce-027d-423f-a395-0f99e87b05f7", "collapsed": true}}, {"cell_type": "code", "source": ["#svc = SVC()\n", "# update svc\n", "svc = SVC(class_weight='balanced')\n", "gnb = GaussianNB()\n", "neigh = KNeighborsClassifier()\n", "dt = tree.DecisionTreeClassifier()\n", "rdf = RandomForestClassifier()\n", "adb = AdaBoostClassifier()\n", "\n", "# generate a 1000 train-test pairs iterator with default test set size = 0.1\n", "sss = StratifiedShuffleSplit(n_splits=1000, random_state=44)"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "b563b2e144851ec72514353582b6502139239023", "_cell_guid": "ab63e467-9036-4f32-b49c-5ddeb0857108", "collapsed": true}}, {"cell_type": "code", "source": ["# a procedure to print out mean scores from cv\n", "def print_scores(clf, data, label):\n", "    scores = [\"accuracy\", \"precision\", \"recall\", \"average_precision\", \"f1\", \"roc_auc\"]\n", "    start = time()\n", "    for score in scores:\n", "        mean_score = cross_val_score(clf, data, label, cv=sss, scoring=score).mean()\n", "        print(score, ':', mean_score)\n", "    \n", "    print(\"\\nThis took %.2f seconds\\n\" %(time() - start))"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "52cf6098e9a18dc69f006517e44792405cbca679", "_cell_guid": "0dae1ea5-707f-4918-8a18-cba4f339d697", "collapsed": true}}, {"cell_type": "code", "source": ["# svc classifier performance\n", "print_scores(svc, df[original_numeric], df['Survived'])"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "3085ade22d9361aafab96c00e77ce7d41b7ebaa0", "_cell_guid": "3db6079c-ae19-44a3-afdf-6ba12d1aa6f3"}}, {"cell_type": "code", "source": ["pipeline1 = Pipeline([('selector', SelectKBest()), \\\n", "                      ('clf', svc)])\n", "\n", "parameters = {'selector__k':[6,5,4,3], \\\n", "              'clf__C': [0.1, 1, 10, 100, 1000], \\\n", "              'clf__gamma': [1, 0.1, 0.01, 0.001, 0.0001]}\n", "\n", "grid_search = GridSearchCV(pipeline1, parameters, scoring='f1')\n", "start = time()\n", "gird_result = grid_search.fit(df[original_numeric], df['Survived']).best_estimator_\n", "print(\"\\nThis took %.2f seconds\\n\" %(time() - start))\n", "\n", "selector = gird_result.named_steps['selector']\n", "k_features = gird_result.named_steps['selector'].get_params(deep=True)['k']\n", "print(\"Number of features selected: %i\" %(k_features))\n", "\n", "selected = selector.fit_transform(df[original_numeric], df['Survived'])\n", "scores = zip(original_numeric, selector.scores_, selector.pvalues_)\n", "sorted_scores = sorted(scores, key = lambda x: x[1], reverse=True)\n", "new_list = list(map(lambda x: x[0], sorted_scores))[0:k_features]\n", "\n", "new_clf = gird_result.named_steps['clf']\n", "\n", "new_dataset = df[new_list]"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "9aa5a1e86f0fec69dfc0e6d62d7dbc23ad2160e6", "_cell_guid": "d3518666-feed-4227-b6b6-1302407ba28a"}}, {"cell_type": "code", "source": ["print(new_list)\n", "print(new_clf)"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "2ae192e91354bc9c2666f7ae953f723455866dac", "_cell_guid": "5d708a1d-74db-4a77-9534-ba307ac411ba"}}, {"cell_type": "code", "source": ["# svc classifier performance after tunning parameters using pipeline\n", "print_scores(new_clf, new_dataset, df['Survived'])"], "outputs": [], "execution_count": null, "metadata": {"_uuid": "a970ddf330f1d6c97ea9198c0f90e1a7911eb71e", "_cell_guid": "c45b68ab-c4f3-4719-a92d-7063d4d62bdc"}}], "nbformat_minor": 1, "metadata": {"anaconda-cloud": {}, "kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}, "language_info": {"version": "3.6.1", "file_extension": ".py", "name": "python", "codemirror_mode": {"name": "ipython", "version": 3}, "mimetype": "text/x-python", "pygments_lexer": "ipython3", "nbconvert_exporter": "python"}}, "nbformat": 4}