{"cells":[{"metadata":{"_uuid":"43caed3f9845225b8cf17f4f00dd7dcdffb308db"},"cell_type":"markdown","source":"**Titanic Project** \n\n\nAnÄ±l Berk Altuner\n\n\n\nToday, we will learn to:\n\n1. Question definition\n\n2. Exploring the data\n\n    2.1. Use some pandas functions on the data\n   \n    2.2. Make comparisons between values\n   \n    2.3. Determining valuable information on purpose\n    \n3. Wrangling with data\n\n    3.1. These valuable informations can be more effective. We use some functions for about that.\n    \n    3.2. We will make the data cleaner\n    \n4. Model and predict\n\n    4.1 We try some Machine Learning models and find a best solution.\n\n5. Visualize problem solution\n\n\n\n**Question**\n\nThe Titanic was one of the most tragic wrecks in history. Today we will work on t if a passenger survived the sinking of the Titanic or not. \nFor each PassengerId in the test set, we predict a 0 or 1 value for the Survived variable."},{"metadata":{"_uuid":"e944d29a089339585808c609e0d03a2065f395c3"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":false,"collapsed":true},"cell_type":"code","source":"import pandas as pd #Data processing\nfrom sklearn.linear_model import LogisticRegression #For the machine learning modelling\nfrom sklearn.metrics import accuracy_score #Calculating accuracy for end of the evulation\nfrom sklearn.neighbors import KNeighborsClassifier #KNeighborsClassifier Algorithm in Sklearn Library\nfrom sklearn.tree import DecisionTreeClassifier #Decision Tree Algorithm in Sklearn Library\nfrom sklearn.ensemble import RandomForestClassifier #RandomForest Algortihm in Sklearn Library\nfrom sklearn.naive_bayes import GaussianNB #Naive Bayes Algorithm\nfrom sklearn.svm import SVC #Support Vector Machine\nfrom sklearn.model_selection import cross_val_score,train_test_split #Data proccessing for the modelling\nfrom sklearn.model_selection import GridSearchCV\nimport matplotlib.pyplot as plt #Visual modelling\nimport seaborn as sns","execution_count":108,"outputs":[]},{"metadata":{"_uuid":"58990accaa24c356c7899d30a4ec78a1f6611912"},"cell_type":"markdown","source":"**Input Data to program**\n\nWe take tests and training data separately to ensure that the program is healthy. We perform our operations on the training data and compare it with the test data at the end of the program. This is how we achieve a healthy end result. The data we will guess is \"Survival\" data. We use pandas library. Lets use [pd.read_csv()](http://https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html). We can import the .csv data with this function and we can look shape of these data with [Dataframe.shape](http://http://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.DataFrame.shape.html)"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"\ntest_df = pd.read_csv(\"../input/test.csv\", sep=\",\")\ntrain_df = pd.read_csv(\"../input/train.csv\", sep=\",\")\n# Input data files in program.\n\n","execution_count":109,"outputs":[]},{"metadata":{"_uuid":"527da98eaa29c8432dcb52d06932bd387027b8be"},"cell_type":"markdown","source":"**Explore the Data**\n\nOur data has 7 columns. We analyze these columns.   \n\nPassangerId : This column is the unique number that identifies the passengers\n\nSurvived : This column has 0 and 1. These numbers are value of the passangers survived or died. \"1\" mean passanger survive, \"0\" mean passanger died.\n\nPclass : Ticket class \n\nAge :  The passenger's age in years\n\nSibSp : The number of siblings or spouses the passenger had aboard the Titanic\n\nParch : The number of parents or children the passenger had aboard the Titanic\n\nTicket : The passenger's ticket number\n\nFare :The fare the passenger paid\n\nCabin : The passenger's cabin number\n\nEmbarked : The port where the passenger embarked (C=Cherbourg, Q=Queenstown, S=Southampton)"},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"ae74eb2711aa62d0eede524284d9491cd89519d4","collapsed":true},"cell_type":"code","source":"train_df.info()\nprint(\"_\"*50)\ntest_df.info()\n#For the comparisons train and test.","execution_count":110,"outputs":[]},{"metadata":{"_uuid":"7ade07a3fd7ae324baf753ac4040bcc45a10d830"},"cell_type":"markdown","source":"* Train data has 891 passanger's information, test data has 418\n* PassangerId, Pclass, Age,SibSp,Parch has a numerical values.\n* We can see our train and test data informations but something missed. Age and Cabin columns are less than the others. Probably Age and Cabin columns have some NaN values. "},{"metadata":{"trusted":true,"_uuid":"f89aa2adf00d651d74fb5328438dee155e4582eb","collapsed":true},"cell_type":"code","source":"train_df.describe(include='all') #Statical values from train_df","execution_count":111,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true,"_uuid":"380ed1c5bf5c03d82e91e7a416f446782b3926f7","collapsed":true},"cell_type":"code","source":"train_df.head() #First 5 value from to train_df","execution_count":112,"outputs":[]},{"metadata":{"_uuid":"4dbc08761751ac0c3c0fddb76d76bbcd7ed12b0e"},"cell_type":"markdown","source":"We see first 5 information on data. We need to find effective columns for train our data. We can use Age and Sex for about that. Because they have chance to first preference for a lifeboats. We also use Pclass. If your ticket at first class, you have preference.We can use Embarked too. City's economic value can say something about passangers rich or poor. Fare can give it to us too. We can determine the richness or poverty of the traveler according to the value of the Fare. In this case, we can give us a rate according to the probability of live or dead . We use Age,Sex and Pclass,Embarked,Fare."},{"metadata":{"trusted":true,"_uuid":"d5e138d610a7efc43df04e00039dff926314facc","collapsed":true},"cell_type":"code","source":"#Creat the chosen column and survive value comparision\ndef chart(feature):\n  survived = train_df[train_df[\"Survived\"] == 1] #Transfer survival values of 1 to a new series\n  died = train_df[train_df[\"Survived\"] == 0]#Transfer survival values of 0 to a new series\n  survived[feature].plot.hist(alpha=0.5,color='red',bins=25)#Survived[column we chose].plot.hist(alpha=Visibilty value, color=color of graphic,bins=width of boxes)\n  died[feature].plot.hist(alpha=0.5,color='blue',bins=25)\n  plt.legend(['Survived','Died'])\n  plt.show()\n#We write \"Age\" for age-survive value comprasion    \nchart(\"Age\")","execution_count":113,"outputs":[]},{"metadata":{"_uuid":"a1668c5cc462211a077739af9171c676e233a0b0"},"cell_type":"markdown","source":"Lets look a graph of the relationship between Age and Survived. It seems a little complicated but we can see that most of the babies survive, and the majority of the population in the 20-40 age range is dead.\n\nNow we can look a graph of the relationship between Sex and Survived"},{"metadata":{"trusted":true,"_uuid":"2f6ab298cea1c4f18376c3182c7c86ecebb14585","collapsed":true},"cell_type":"code","source":"train_df[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived') #Comprasion female and male about alive or dead.","execution_count":114,"outputs":[]},{"metadata":{"_uuid":"566a2042b0e5b8f1907ab76ddf0dd9199887c741"},"cell_type":"markdown","source":"As you see when we comparision beetween Sex and Survived, we can see more female alive. Becuase we said female has preference for a lifeboats.\n\n**Wrangling Data**\n\nWe need to more cleaner data for analyze. First we need to focus what we need. We use Age,Pclass and Sex for modelling. After that we can drop the other columns and cutting age column will be more clean data for us. If you remember we said \"Age\" column had some NaN values. These values are missing. We create new variable, its name \"Missing\". These NaN values replaced to -0.5. After that, missing values between -1 , 0. For this process we use [df[columnname].fillna()](http://http://pandas.pydata.org/pandas-docs/stable//generated/pandas.DataFrame.fillna.html) function. For cutting process, we use [pd.cut](http://https://stackoverflow.com/questions/45751390/pandas-how-to-use-pd-cut) function."},{"metadata":{"trusted":true,"_uuid":"fc6b3e99b4732c4e9148da6ba96a114cce080f8c","collapsed":true},"cell_type":"code","source":"#Cut the age and create new age categories. These catagories are more cleaner our data\ndef cutting_age(df,cut_points,label_names):\n    df[\"Age\"] = df[\"Age\"].fillna(-0.5) #Some age values are missing. We fill -0.5 these NaN values.\n    df[\"Age_categories\"] = pd.cut(df[\"Age\"],cut_points,labels=label_names) # pd.cut function cut from the points we want\n    return df\n\ncut_points = [-1,0,5,12,18,35,60,100] #cut points we chose\nlabel_names = [\"Missing\",\"Baby\",\"Child\",\"Teenager\",\"Young Adult\",\"Adult\",\"Old\"] #Every 2 range will describe one label in order.\n\ntrain_df = cutting_age(train_df,cut_points,label_names) #Cut Age column in train data\ntest_df = cutting_age(test_df,cut_points,label_names) #Cut Age column in test data\n\nsns.barplot(x=\"Age_categories\", y=\"Survived\", data=train_df)","execution_count":115,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dd6cfe5ab168be6fb2948c19a0a7bf2b57dc8c0c","collapsed":true},"cell_type":"code","source":"#Cutting fare values for clean data.\ndef cutting_fare(df,label_names): \n    df[\"Fare_categories\"] = pd.qcut(df[\"Fare\"],4,labels=label_names) # pd.qcut() function cut from the equal points. We choose 4 for that value.\n    return df\n\nlabel_names = [\"Least\",\"Less-Middle\",\"Middle\",\"High\"]\n\ntrain_df = cutting_fare(train_df,label_names)\ntest_df = cutting_fare(test_df,label_names)\n\nsns.barplot(x=\"Fare_categories\", y=\"Survived\", data=train_df)\n","execution_count":116,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5af9dc24b656758202282e8e9ef761c323fdcacc","collapsed":true},"cell_type":"code","source":"train_df[[\"Fare_categories\", \"Survived\"]].groupby(['Fare_categories'], as_index=False).mean().sort_values(by='Survived') #Every fare catagories survive values.","execution_count":117,"outputs":[]},{"metadata":{"_uuid":"65b61f709e8c0a9a3d7469340526eb45eeb43846"},"cell_type":"markdown","source":"Now we can more clean if we create dummies. If we extend the divided age categories to each dummy column, we can increase the prediction ratio. We can create dummies with [pd.get_dummies](http://https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html) function. Pandas can easily do seperated every diffrent values to diffrent columns. After extend process we must use [pd.concat](http://https://pandas.pydata.org/pandas-docs/stable/generated/pandas.concat.html) for comibe to dataframes."},{"metadata":{"trusted":true,"_uuid":"293ffe232ab41f20f7f3502cfd6e1c81cac1ded7","collapsed":true},"cell_type":"code","source":"#We create dummies for every created values. After this process we have a lot of columns for every value.\ndef create_dummies(df,column_name):\n    dummies = pd.get_dummies(df[column_name],prefix=column_name) #Seperate to columns for every diffrent value\n    df = pd.concat([df,dummies],axis=1) # created columns adding to  actual data \n    return df\n\nfor column in [\"Pclass\",\"Sex\",\"Age_categories\",\"Embarked\",\"Fare_categories\"]: #Columns who diveded\n    train_df = create_dummies(train_df,column)\n    test_df = create_dummies(test_df,column)","execution_count":118,"outputs":[]},{"metadata":{"_uuid":"f2d7738556840e541e7beebc50a958ccac012d8b"},"cell_type":"markdown","source":"Now we drop unnecessary columns in the train data. We use for that [Dataframe.drop()](http://https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html)"},{"metadata":{"trusted":true,"_uuid":"49f8f213d42d35d0fc0e90039e4e3229aa83709b","collapsed":true},"cell_type":"code","source":"train_df = train_df.drop(['Name','Pclass','Age','Ticket','Sex','SibSp','Parch','Fare','Cabin','Embarked','Age_categories','Fare_categories'], axis=1) \n#We dont need the other columns. We drop columns for cleaner data","execution_count":120,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f4e4f47d7166c3e30e8a7e904930649bdb467271","collapsed":true},"cell_type":"code","source":"train_df.head() # First 5 value of new train_df ","execution_count":121,"outputs":[]},{"metadata":{"_uuid":"54a40e9528eab10b0c034394d6394f85bd8ecdb4"},"cell_type":"markdown","source":"**Model and Predict**\n\nLet's use logistic regression as model of machine learning.  [LogisticRegression](http://http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) function in the Sklearn library makes it very easy to use. We will use the [LogisticRegression.fit(](http://http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)) command to fit the columns we have created. After creating the model we must separate the test data to make predicts. Again, [train.test.split()](http://http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) command in the Sklearn library performs this operation."},{"metadata":{"trusted":true,"_uuid":"eec6efc73eac37399b0f91d467f1467f6baf0f8f","collapsed":true},"cell_type":"code","source":"columns = ['Pclass_1', 'Pclass_2', 'Pclass_3', 'Sex_female', 'Sex_male','Age_categories_Missing','Age_categories_Child', 'Age_categories_Teenager',\n           'Age_categories_Young Adult', 'Age_categories_Adult','Age_categories_Old','Embarked_C','Embarked_S','Embarked_Q','Fare_categories_Least','Fare_categories_Less-Middle',\n           'Fare_categories_Middle','Fare_categories_High'] #We use these columns\n\n\n#Splitting data\nX = train_df[columns] #Train data\ny = train_df['Survived'] #Targer data\ntrain_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.20,random_state=15) #Splitting data to train and test datas. We chose the test size %20.\n#Random state is change rate to data when program started.\n","execution_count":122,"outputs":[]},{"metadata":{"_uuid":"45c52eb343286f1d6356d5355221614fe9102301"},"cell_type":"markdown","source":"**K-Fold Cross Validation**\n\nWe split to data %80 train, %20 test and this datas randomized but we may not get the real performance. We can not get maximum performance because the randomly selected data is not selected very well. In the K-Fold Cross Validation process, the meaning of \"K\" determines how many times we extend the test data to the all dataframe. For example, if we take K as 5, %20 of the data is distributed 5 times, and the average performance of each distribution is determined as the overall performance. During this process we use [cross_val_score()](http://http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) from sklearn library"},{"metadata":{"trusted":true,"_uuid":"0f4937e22585873ce29ec67882b547115e77ce5c","collapsed":true},"cell_type":"code","source":"#Cross Validation function. We call this function when try every model.\ndef cr_val(model,tr_data,test_data):\n    accuracy = (cross_val_score(model, tr_data, test_data, cv=10)).mean() #cross_val_score(ModelWeUse, TrainData, TestData, cv=Number of pieces data will we divide).\n    # .mean() is for take the average after getting the results\n    return accuracy","execution_count":123,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"514367c588c3b521520e64cfc275a08c54d3182b","collapsed":true},"cell_type":"code","source":"#Logistic Regression process\nlr=LogisticRegression()\nlr.fit(train_X, train_y)\npredictions = lr.predict(test_X)\naccuracy = accuracy_score(test_y, predictions)\n\n#Now call the Cross Validation Function\naccuracy = cr_val(lr,X,y)\n\nprint(accuracy)","execution_count":124,"outputs":[]},{"metadata":{"_uuid":"663f363791b803150f1418bacf05f4b350670195"},"cell_type":"markdown","source":"**K-Nearest Neighbor**\n\nK-Nearest Neighbor  Classifier take K nearest item and return value of main item. Value of item is a majority of other items values. We use[ KNeighborsClassifier()](http://http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) in Sklearn library for it."},{"metadata":{"_uuid":"b666d9fd87f5e61f553ac8c980913dcf2f39960e","trusted":true,"collapsed":true},"cell_type":"code","source":"\nhyperparameters = {\n    \"n_neighbors\": range(1,20),\n}\nknn=KNeighborsClassifier()\ngrid=GridSearchCV(knn,param_grid=hyperparameters,cv=10)\ngrid.fit(X,y)\nbest_score=grid.best_score_\n\nprint(best_score)","execution_count":143,"outputs":[]},{"metadata":{"_uuid":"24f10929100ab83c0155a94f679172f992b30c71"},"cell_type":"markdown","source":"**Random Forest Classifier**\n\nRandom Forest Classifier is small tree groups. Every groups have questions and end of the process, our item has value of majority to the answers. We use [RandomForestClassifier()](http://http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) in Sklearn library for it."},{"metadata":{"trusted":true,"_uuid":"610a275a6291ab431951d65fab23e2a74116bfb2","collapsed":true},"cell_type":"code","source":"\nhyperparameters={ \"criterion\":[\"entropy\",\"gini\"], \n                \"max_depth\":[5,10],\n                \"max_features\":[\"log2\",\"sqrt\"],\n                \"min_samples_leaf\":[1,5],\n                \"min_samples_split\":[3,5],\n                \"n_estimators\":[6,9],\n               }\n\nclf=RandomForestClassifier(random_state=1)\ngrid=GridSearchCV(clf,param_grid=hyperparameters,cv=10)\ngrid.fit(X,y)\nbest_params=grid.best_params_\nbest_score=grid.best_score_\n\nprint(best_score)","execution_count":142,"outputs":[]},{"metadata":{"_uuid":"528bee43aa27b4d47aea8c85cd662a1321ec1743"},"cell_type":"markdown","source":"**Decision Tree**\n\nDecision Tree Classifier is bigger than Random Forest. In Decision Tree, the value of end of the chained question is equal to our item's value. We use [DecisionTreeClassifier()](http://http://scikit-learn.org/stable/modules/tree.html) in Sklearn library for it"},{"metadata":{"trusted":true,"_uuid":"7ec1cb64b44b261d888f1d5b137283eb6cb013bb","collapsed":true},"cell_type":"code","source":"clf=DecisionTreeClassifier()\n\n#Cross Validation Function\naccuracy=cr_val(clf,X,y)\nprint(accuracy)","execution_count":128,"outputs":[]},{"metadata":{"_uuid":"5718b83124ded05801af4c50464c742abd43b0b1"},"cell_type":"markdown","source":"**Naive Bayes Algorithm**\n\n\n"},{"metadata":{"trusted":true,"_uuid":"39d56214d7d486cb4f41c6602c63fa32ca912f04","collapsed":true},"cell_type":"code","source":"clf=GaussianNB()\n\n#Cross Validation Function\naccuracy=cr_val(clf,X,y)\nprint(accuracy)","execution_count":129,"outputs":[]},{"metadata":{"_uuid":"f3f70900ec12e2eb190f4fc2ffb3213673503453"},"cell_type":"markdown","source":"**Support Vector Machine**\n\nThe support vector machine is grouped together with an equal line from the middle of the graphical two sides of the graph and this graph becomes clear with a clear line. We use [SVC()](http://http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) from Sklearn library."},{"metadata":{"trusted":true,"_uuid":"1a3e2a4f16e5c922ca9209e4a8d15b43269ef8a5","collapsed":true},"cell_type":"code","source":"clf=SVC()\n\n#Cross Validation Function\naccuracy=cr_val(clf,X,y)\nprint(accuracy)","execution_count":130,"outputs":[]},{"metadata":{"_uuid":"d5b25a0720cd7f998812ad7499c6d923300ff2aa"},"cell_type":"markdown","source":"**Submission File**\n\nWe got the best accuracy from K-Neighbor Classifier. Now we should predict with Kaggle's test data and after that we create submission file."},{"metadata":{"trusted":true,"_uuid":"51ce7a2e9bde6d2f24a6a221f436b707009bfbe7","collapsed":true},"cell_type":"code","source":"\nbest_rf=grid.best_estimator_\ntest_predictions=best_rf.predict(test_df[columns])\nsubmission_df = {\"PassengerId\":test_df[\"PassengerId\"],\n                 \"Survived\": test_predictions}\nsubmission = pd.DataFrame(submission_df)\nprint(submission)","execution_count":145,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"3eab5639e13fe4e2eafd600b3acf1cd785017355"},"cell_type":"code","source":"submission.to_csv(\"submission.csv\",index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"87fa8ba266d7c67fa053c87eefbbc44a6b8eef76"},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}