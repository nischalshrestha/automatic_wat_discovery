{"nbformat": 4, "cells": [{"cell_type": "markdown", "metadata": {"_uuid": "f9e1ec5626c39dfde12d926320f298104a5785f5", "_cell_guid": "87485389-035d-4375-9011-cbcb470d050a"}, "source": ["This notebook follows the steps in **\"EDA To Prediction (DieTanic)\"** by **I,Coder** very closely (thanks for the really awesome notebook!) You can see it here: <br>\n", "https://www.kaggle.com/ash316/eda-to-prediction-dietanic?scriptVersionId=1833006 <br>\n", "<br>\n", "\n", "**Overview: **<br>\n", "* Preprocessing and Feature Engineering\n", "* Modelling\n", "* Cross Validation\n", "* Hyper-Parameter Tuning\n", "<br>\n", "\n", "\n", "A few things I did differently: <br>\n", "1. Instead of filling up observations with missing \"Age\" in the training data, I removed them. <br>\n", "&nbsp;** Rationale:** If we fill up the Ages incorrectly (there were over 100 observations with missing Age), we would be training the model with incorrect data.\n", "2. Instead of using 5 bins for \"Age\", I used 8 (bin size=10). <br>\n", "&nbsp; **Rationale: **When I explored the survival rates based on Age distribution in my previous notebook (https://www.kaggle.com/limjingyu/who-survived-the-titanic), I realized that the survival rates of passengers between consecutive age groups can vary alot."]}, {"cell_type": "code", "outputs": [], "metadata": {"_uuid": "449179e174734e330bd37f731039a4e6a49a7da0", "_cell_guid": "383edbbc-b45e-4a04-85cb-bc5657ae8445", "collapsed": true}, "source": ["import pandas as pd\n", "import numpy as np\n", "from sklearn.linear_model import LogisticRegression #logistic regression\n", "from sklearn.svm import SVC #support vector Machine\n", "from sklearn.ensemble import RandomForestClassifier #Random Forest\n", "from sklearn.neighbors import KNeighborsClassifier #KNN\n", "from sklearn.naive_bayes import GaussianNB #Naive bayes\n", "from sklearn.tree import DecisionTreeClassifier #Decision Tree\n", "from sklearn.model_selection import train_test_split #training and testing data split\n", "from sklearn import metrics #accuracy measure\n", "from sklearn.metrics import confusion_matrix #for confusion matrix\n", "from sklearn.preprocessing import StandardScaler\n", "from sklearn import preprocessing\n", "\n", "# for plotting\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "plt.style.use('fivethirtyeight')\n", "import warnings\n", "warnings.filterwarnings('ignore')\n", "%matplotlib inline"], "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "10f93aea15e77adf9eec71413b59949859d7de8e", "_cell_guid": "bc781bdd-3726-4676-8e41-40d84986e09d"}, "source": ["# Pre-processing and Feature Engineering"]}, {"cell_type": "code", "outputs": [], "metadata": {"_uuid": "b612f2fab702000953ca0ec2b2ee63db16bd4610", "_cell_guid": "6b5e5d88-cc29-4af2-af04-3c857d122e11", "collapsed": false}, "source": ["train = pd.read_csv(\"../input/train.csv\")\n", "test = pd.read_csv(\"../input/test.csv\")\n", "\n", "# remove rows with missing values from training set\n", "train = train[np.isfinite(train['Age'])]\n", "\n", "# pre-processing\n", "def changeFeatures(df, test_data=False):\n", "    # add nFamily column\n", "    df['nFamily'] = df['SibSp'] + df['Parch']\n", "    \n", "    # add initial\n", "    df['Initial']=0\n", "    for i in df:\n", "        df['Initial']=df.Name.str.extract('([A-Za-z]+)\\.') #lets extract the Salutations\n", "        df['Initial'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col',\n", "                               'Rev','Capt','Sir','Don', 'Dona'],['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs',\n", "                                            'Other','Other','Other','Mr','Mr','Mr','Other'],inplace=True)\n", "    \n", "    # only fill in Age values if it is testing data\n", "    if test_data==True:\n", "        df.loc[(df.Age.isnull())&(df.Initial=='Mr'),'Age']=33\n", "        df.loc[(df.Age.isnull())&(df.Initial=='Mrs'),'Age']=36\n", "        df.loc[(df.Age.isnull())&(df.Initial=='Master'),'Age']=5\n", "        df.loc[(df.Age.isnull())&(df.Initial=='Miss'),'Age']=22\n", "        df.loc[(df.Age.isnull())&(df.Initial=='Other'),'Age']=46\n", "    \n", "    # rationale: Continuous features are a problem in ML\n", "    # add age_band column (7 bins)\n", "    df['Age_band']=0\n", "    df.loc[df['Age']<=10,'Age_band']=0\n", "    df.loc[(df['Age']>11)&(df['Age']<=20),'Age_band']=1\n", "    df.loc[(df['Age']>21)&(df['Age']<=30),'Age_band']=2\n", "    df.loc[(df['Age']>31)&(df['Age']<=40),'Age_band']=3\n", "    df.loc[(df['Age']>41)&(df['Age']<=50),'Age_band']=4\n", "    df.loc[(df['Age']>51)&(df['Age']<=60),'Age_band']=5\n", "    df.loc[(df['Age']>61)&(df['Age']<=70),'Age_band']=6\n", "    df.loc[df['Age']>70,'Age_band']=7\n", "    \n", "    # add Fare_cat columns \n", "    df['Fare_cat']=0\n", "    df.loc[(df['Fare']<=7.91),'Fare_cat']=1\n", "    df.loc[(df['Fare']>7.91)&(df['Fare']<=14.454),'Fare_cat']=2\n", "    df.loc[(df['Fare']>14.454)&(df['Fare']<=31),'Fare_cat']=3\n", "    df.loc[(df['Fare']>31)&(df['Fare']<=513),'Fare_cat']=4\n", "    \n", "    # change nominal to categorical \n", "    df['Sex'] = df['Sex'].replace(['male','female'], [0,1])\n", "    df['Embarked'] = df[\"Embarked\"].replace(['S','C','Q'], [0,1,2])\n", "    df['Initial'].replace(['Mr','Mrs','Miss','Master','Other'],[0,1,2,3,4],inplace=True)\n", "    \n", "    # drop columns that are not required\n", "    df.drop(['Name','Age','Ticket','Fare','Cabin'],axis=1,inplace=True)\n", "    \n", "changeFeatures(train)\n", "changeFeatures(test, test_data=True)\n", "\n", "# replace missing value of \"Embarked\" in training set\n", "train[\"Embarked\"].fillna(0, inplace=True)\n", "\n", "print(\"Train data: \\n\", pd.isnull(train).sum())\n", "print(\"Test data: \\n\", pd.isnull(test).sum())"], "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "7516db790c7ad76779fd761db5f16769fc2638f4", "_cell_guid": "6d606999-09f2-4a67-aa10-4833846dbe37"}, "source": ["# Modelling"]}, {"cell_type": "code", "outputs": [], "metadata": {"_uuid": "139f34e8a87fa9cbad3a823165bf6ea317a62b1d", "_cell_guid": "eea5ccb0-89e6-48aa-84f3-aa95eba6e6ac", "collapsed": false}, "source": ["train_data,test_data = train_test_split(train,test_size=0.3,random_state=0,stratify=train['Survived'])\n", "predictor_cols = [\"Pclass\",\"Sex\", \"Embarked\", \"nFamily\", \"Initial\", \"Age_band\", \"Fare_cat\"]\n", "target_col = \"Survived\"\n", "train_X = train_data[predictor_cols]\n", "train_Y = train_data[target_col]\n", "test_X = test_data[predictor_cols]\n", "test_Y = test_data[target_col]\n", "\n", "scaler = StandardScaler()\n", "train_X = scaler.fit_transform(train_X)\n", "test_X = scaler.transform(test_X)"], "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "2869b31bfaf29340ab2d3b7629d40d06a91d6caf", "_cell_guid": "a1671ebd-94a7-46ea-935c-6b5fb502d790"}, "source": ["## SVM (Radial)"]}, {"cell_type": "code", "outputs": [], "metadata": {"_uuid": "46e66c2b637b29fa8f523b1db846c3fc145948f9", "_cell_guid": "b1ec79d1-fa52-4ef2-b0a1-2c04da8ee418", "collapsed": false}, "source": ["svm = SVC(kernel=\"rbf\", C=0.1, gamma=0.1)\n", "# I used ravel() as I was getting a data conversion warning\n", "svm.fit(train_X, train_Y.values.ravel()) \n", "svm_prediction = svm.predict(test_X)\n", "print(\"Accuracy for SVM = \", metrics.accuracy_score(svm_prediction, test_Y))"], "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "b0cffb223dccd1768060343fe6962f7fc75494eb", "_cell_guid": "748e547b-a065-45fd-b544-6c108e65645d", "collapsed": true}, "source": ["## Logistic Regression"]}, {"cell_type": "code", "outputs": [], "metadata": {"_uuid": "fc4e40cfa40199f01c901c39bb08dbc957b3b142", "_cell_guid": "99e74ccb-a005-4d51-a316-e86c2f7ba71d", "collapsed": false}, "source": ["lr = LogisticRegression()\n", "lr.fit(train_X, train_Y.values.ravel())\n", "lr_prediction = lr.predict(test_X)\n", "print(\"Accuracy for Logistic Regression = \", metrics.accuracy_score(lr_prediction, test_Y))"], "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "821e52ca8bd9f3a79cb7d7170df2446e527ea8b1", "_cell_guid": "674c516c-1a1a-4e7c-9c9a-0c5c2c03ab4e"}, "source": ["## Decision Tree"]}, {"cell_type": "code", "outputs": [], "metadata": {"_uuid": "06177dd02613a3f7cd55367d462feafb925ca3b5", "_cell_guid": "93ddcb7a-24b8-40c5-9490-73a7f31853b3", "collapsed": false}, "source": ["tree = DecisionTreeClassifier()\n", "tree.fit(train_X, train_Y.values.ravel())\n", "tree_prediction = tree.predict(test_X)\n", "print(\"Accuracy for Decision Tree = \", metrics.accuracy_score(tree_prediction, test_Y))"], "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "cf90a2429d7c4253fef8d5a8de70590a1f287b97", "_cell_guid": "7a84ced1-af6b-473a-9aee-e5f96b003a7e"}, "source": ["## k-Nearest Neighbors"]}, {"cell_type": "code", "outputs": [], "metadata": {"_uuid": "78f3630f544bfcad7fe834205577612be1777e70", "_cell_guid": "1cd858e9-8ec9-47d7-a734-19119d60cb78", "collapsed": false}, "source": ["knn = KNeighborsClassifier()\n", "knn.fit(train_X, train_Y.values.ravel())\n", "knn_prediction = knn.predict(test_X)\n", "print(\"Accuracy for kNN = \", metrics.accuracy_score(knn_prediction, test_Y))"], "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "dd925a82c64d486e0a718123b14427ff38924392", "_cell_guid": "b317a51f-ae9b-4127-bfa5-d69e27fe0677"}, "source": ["## Gaussian Naive Bayes"]}, {"cell_type": "code", "outputs": [], "metadata": {"_uuid": "ab494ae4a0adfedbb576061e9416c4a3104c9219", "_cell_guid": "4c711614-2acb-435a-978e-eed5460e2384", "collapsed": false}, "source": ["nb = GaussianNB()\n", "nb.fit(train_X, train_Y.values.ravel())\n", "nb_prediction = nb.predict(test_X)\n", "print(\"Accuracy for Gaussian NB = \", metrics.accuracy_score(nb_prediction, test_Y))"], "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "ee1c922523bfc1faafd6ecafa90b9bfe86aac7be", "_cell_guid": "7d6ee5b2-bd41-4f55-9adf-a08104567126"}, "source": ["## Random Forest"]}, {"cell_type": "code", "outputs": [], "metadata": {"_uuid": "d7ff15de448c4c01a4fe04998a8782f8348a369c", "_cell_guid": "cd0eb097-305b-4e2e-81a8-526e176eb243", "collapsed": false}, "source": ["rf = RandomForestClassifier(n_estimators=15)\n", "rf.fit(train_X, train_Y.values.ravel())\n", "rf_prediction = rf.predict(test_X)\n", "print(\"Accuracy for Random Forest = \", metrics.accuracy_score(rf_prediction, test_Y))"], "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "2182d1faa199d9eacde2ab9dac9ad6edcd57567c", "_cell_guid": "7a751ee5-8466-4f44-9bf4-bde999ccae73"}, "source": ["# Cross Validation"]}, {"cell_type": "code", "outputs": [], "metadata": {"_uuid": "bbbaba6a6dc4da1fd6e4a0f87c32789338ae2f3f", "_cell_guid": "a90da7cb-3eb0-4100-8370-3a2508dcc14f", "collapsed": false}, "source": ["from sklearn.model_selection import KFold\n", "from sklearn.model_selection import cross_val_score\n", "from sklearn.model_selection import cross_val_predict\n", "\n", "# datasets for cross validation\n", "X = train[predictor_cols]\n", "Y = train[target_col]\n", "\n", "kfold = KFold(n_splits=10, random_state=22)\n", "mean = []\n", "accuracy =[]\n", "std = []\n", "\n", "classifiers = [\"Radial SVM\", \"Logistic Regression\",\n", "              \"KNN\", \"Decision Tree\", \"Naive Bayes\",\n", "              \"Random Forest\"]\n", "models = [SVC(kernel=\"rbf\"), LogisticRegression(),\n", "         KNeighborsClassifier(n_neighbors=9), \n", "         DecisionTreeClassifier(), GaussianNB(),\n", "         RandomForestClassifier(n_estimators=15)]\n", "\n", "for model in models:\n", "    cv_result = cross_val_score(model, X, Y, cv=kfold, scoring=\"accuracy\")\n", "    cv_result = cv_result\n", "    mean.append(cv_result.mean())\n", "    std.append(cv_result.std())\n", "    accuracy.append(cv_result)\n", "    \n", "new_models_df = pd.DataFrame({\"Mean\": mean, \"Standard Deviation\": std}, index=classifiers)\n", "new_models_df"], "execution_count": null}, {"cell_type": "code", "outputs": [], "metadata": {"_uuid": "eb6e8d2523c44c73325d7a6b1900d5111f3c351a", "_cell_guid": "7cf6a100-1e49-48be-8c02-3da636fd388d", "collapsed": false}, "source": ["plt.subplots(figsize=(12,6))\n", "box = pd.DataFrame(accuracy, index=classifiers)\n", "box.T.boxplot()"], "execution_count": null}, {"cell_type": "code", "outputs": [], "metadata": {"_uuid": "70d6ec94a873a92f60086d3daff2f39eff43066a", "_cell_guid": "ff5a6fe5-04f4-46db-bb64-a73d5a23763c", "collapsed": false}, "source": ["f, ax = plt.subplots(2,3,figsize=(12,10))\n", "\n", "# svc\n", "y_pred = cross_val_predict(SVC(kernel=\"rbf\"), X, Y, cv=10)\n", "sns.heatmap(confusion_matrix(Y, y_pred), ax=ax[0,0], annot=True, fmt=\"2.0f\")\n", "ax[0,0].set_title(\"Matrix for SVM\")\n", "\n", "# logistic regression\n", "y_pred = cross_val_predict(LogisticRegression(), X, Y, cv=10)\n", "sns.heatmap(confusion_matrix(Y, y_pred), ax=ax[0,1], annot=True, fmt=\"2.0f\")\n", "ax[0,1].set_title(\"Matrix for Logistic Regression\")\n", "\n", "# knn\n", "y_pred = cross_val_predict(KNeighborsClassifier(n_neighbors=9), X, Y, cv=10)\n", "sns.heatmap(confusion_matrix(Y, y_pred), ax=ax[0,2], annot=True, fmt=\"2.0f\")\n", "ax[0,2].set_title(\"Matrix for kNN\")\n", "\n", "# decision tress\n", "y_pred = cross_val_predict(DecisionTreeClassifier(), X, Y, cv=10)\n", "sns.heatmap(confusion_matrix(Y, y_pred), ax=ax[1,0], annot=True, fmt=\"2.0f\")\n", "ax[1,0].set_title(\"Matrix for Decision Tree\")\n", "\n", "# naive bayes\n", "y_pred = cross_val_predict(GaussianNB(), X, Y, cv=10)\n", "sns.heatmap(confusion_matrix(Y, y_pred), ax=ax[1,1], annot=True, fmt=\"2.0f\")\n", "ax[1,1].set_title(\"Matrix for Naive Bayes\")\n", "\n", "# random forest\n", "y_pred = cross_val_predict(RandomForestClassifier(), X, Y, cv=10)\n", "sns.heatmap(confusion_matrix(Y, y_pred), ax=ax[1,2], annot=True, fmt=\"2.0f\")\n", "ax[1,2].set_title(\"Matrix for Random Forest\")\n", "\n", "plt.subplots_adjust(hspace=0.5, wspace=0.1)\n", "plt.show()"], "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "998917e59841e8228c3714cd44abddbea99a61f2", "_cell_guid": "1af8febe-af9e-4844-99e3-04dad8f37fde"}, "source": ["Naive Bayes was the best at correctly predicting the survival of passengers. However, it seems to have a significantly higher number of false positives than the other models.\n", "\n", "If we compare SVM and Random Forest, SVM has a lower number of both false positives and false negatives, and higher number of true positives and true negatives. "]}, {"cell_type": "markdown", "metadata": {"_uuid": "b034ef13e0ab8469177446c35dfd688184837103", "_cell_guid": "4b36e9d0-37b6-4525-96ae-84d3dc274e6c"}, "source": ["# Hyper-Parameter Tuning\n", "\n", "Now that I've decided that Radial SVM performs the best, I'll tune the hyper-parameters."]}, {"cell_type": "code", "outputs": [], "metadata": {"_uuid": "0c7e9821afbcbac971a86fae3dd69ed029c1b274", "_cell_guid": "7ef5e6b1-ca5f-4ae1-b926-325d45eca763", "collapsed": false}, "source": ["from sklearn.model_selection import GridSearchCV\n", "C = [0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1]\n", "gamma = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\n", "kernel = [\"rbf\"]\n", "hyper = {\"kernel\":kernel, \"C\":C, \"gamma\":gamma}\n", "\n", "gd = GridSearchCV(estimator=SVC(), param_grid=hyper, verbose=True)\n", "gd.fit(X,Y)\n", "print(gd.best_score_)\n", "print(gd.best_estimator_)"], "execution_count": null}, {"cell_type": "code", "outputs": [], "metadata": {"_uuid": "b5b61f8bca9df2e1b434a66acd5863efa900f218", "_cell_guid": "86c5dbf5-b682-4378-88dd-338b8282a5b0", "collapsed": false}, "source": ["final_model = SVC(C=0.25, cache_size=200, class_weight=None, coef0=0.0, \n", "           decision_function_shape=\"ovr\", degree=3, gamma=0.1,\n", "           kernel=\"rbf\", max_iter=-1, probability=False, random_state=None,\n", "           shrinking=True, tol=0.001, verbose=False)\n", "y_pred = cross_val_predict(final_model, X, Y, cv=10)\n", "sns.heatmap(confusion_matrix(Y, y_pred), annot=True, fmt=\"0.2f\")\n", "plt.show()"], "execution_count": null}, {"cell_type": "markdown", "metadata": {"_uuid": "de1ec271aea62f970ca61e6dab783553b3f2219a", "_cell_guid": "568dbdbe-12b3-4f1d-a8a2-8749d51ff229"}, "source": ["# Final Submission "]}, {"cell_type": "code", "outputs": [], "metadata": {"_uuid": "be3721c6b4d542ad55dd4dbb7cb722b0c3c97aa3", "_cell_guid": "e9979a1c-9583-4348-9a91-69fccba24d46", "collapsed": false}, "source": ["final_model.fit(train_X, train_Y)\n", "\n", "test2_X = test[predictor_cols]\n", "test2_X = scaler.transform(test2_X)\n", "predictions = final_model.predict(test2_X)\n", "my_submission = pd.DataFrame({'PassengerId': test.PassengerId, 'Survived': predictions})\n", "my_submission.to_csv('submission.csv', index=False)"], "execution_count": null}], "metadata": {"language_info": {"pygments_lexer": "ipython3", "codemirror_mode": {"name": "ipython", "version": 3}, "nbconvert_exporter": "python", "file_extension": ".py", "mimetype": "text/x-python", "version": "3.6.3", "name": "python"}, "kernelspec": {"language": "python", "name": "python3", "display_name": "Python 3"}}, "nbformat_minor": 1}