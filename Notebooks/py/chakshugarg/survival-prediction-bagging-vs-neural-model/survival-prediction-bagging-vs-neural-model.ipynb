{"cells":[{"metadata":{"_uuid":"b8080adb147768aecb91764af290f2598cc55d0b"},"cell_type":"markdown","source":" **Titanic Survival Rate Prediction - Bagging(voting) vs Neural Model**\n\n**Key points I tried to cover - **\nMy primary motive to build this kernal was to practice different methods of prediction analysis in Binary Classification.\nTitanic Dataset being very good dataset for beginner was perfect to practice data cleaning as well as modeling.\nI got exposure to pre process data in python, feature selection and use different methods for binary classification.\nI have used voting as well as Neural Model to evaluate which one fits better for this dataset.\nOne thing I skipped but will definately update in next commit is ridge regularization to automatically reduce number of features that are not that relevant.\nSecondly in next commit will try to categorize ticket prices also to check its impact on prediction.\nWill definitely use  Parameter hypertuning with model selection matrix next time.\n\n**If you found this kernal useful please do upvote and comment in the comment section below -**\n**Improvements and suggestions are also welcome-**\n"},{"metadata":{"_uuid":"47e0dd15aeb6d85887bba072c47594ee10373422"},"cell_type":"markdown","source":"**Importing required librares and creating functions used in this kernal** -"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":true,"_kg_hide-input":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport math\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_selection import RFE, f_regression\nfrom sklearn.linear_model import (LinearRegression, Ridge, Lasso, RandomizedLasso)\nfrom sklearn import cross_validation, tree, linear_model\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import explained_variance_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.cross_validation import train_test_split\nimport warnings\nimport pandas\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import  GradientBoostingClassifier\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom sklearn.preprocessing import StandardScaler\nfrom keras.layers import Dense, Dropout\n\nwarnings.filterwarnings(\"ignore\")\n\nprint('########Methods#########')\ndef readData(FileLocation,FileType):\n    print('   ')\n    print('loading file - ' + FileLocation)\n    \n    if FileType=='csv':\n        data = pd.read_csv(FileLocation)\n    \n    print(\"Number of Columns - \" + str(len(data.columns)))\n    print(\"DataTypes - \")\n    print(data.dtypes.unique())\n    print(data.dtypes)\n    print('checking null - ')\n    print(data.isnull().any().sum(), ' / ', len(data.columns))\n    print(data.isnull().any(axis=1).sum(), ' / ', len(data))\n    print ('columns having null values - ')\n    print(data.columns[data.isnull().any()])\n    print(data.head())\n    return data\n    \n\ndef DataCleaning(data, columnsToBeDropped, fillNAValues):\n    if (columnsToBeDropped):\n        data = data.drop(columnsToBeDropped,axis=1)\n    data.dropna(thresh=0.8*len(data), axis=1)\n    data.dropna(thresh=0.8*len(data))\n    for value in fillNAValues:\n        data[value].fillna(data[value].median(), inplace = True)\n    return data\n    \n   \ndef scatterPlot(data,ColumnsToPlot,hueColumn):\n    with sns.plotting_context(\"notebook\",font_scale=2.5):\n        g = sns.pairplot(data[ColumnsToPlot], hue=hueColumn, size=1.2, diag_kind = 'kde', diag_kws=dict(shade=True), plot_kws=dict(s=10) )\n        g.set(xticklabels=[]);\n\ndef PlotDataCorrelation(data,vs):\n    print(data.corr().abs().unstack().sort_values()[vs])\n    \n\ndef featureRankingMatrix(data,x,y):\n    ranks = {}\n    \n    colnames = data.columns\n    def ranking(ranks, names, order=1):\n        minmax = MinMaxScaler()\n        ranks = minmax.fit_transform(order*np.array([ranks]).T).T[0]\n        ranks = map(lambda x: round(x,2), ranks)\n        return dict(zip(names, ranks))\n\n    rlasso = RandomizedLasso(alpha=0.04)\n    rlasso.fit(x, y)\n    ranks[\"rlasso/Stability\"] = ranking(np.abs(rlasso.scores_), colnames)\n    lr = LinearRegression(normalize=True)\n    lr.fit(x,y)\n    rfe = RFE(lr, n_features_to_select=1, verbose =3 )\n    rfe.fit(x,y)\n    ranks[\"RFE\"] = ranking(list(map(float, rfe.ranking_)), colnames, order=-1)\n\n    \n    lr = LinearRegression(normalize=True)\n    lr.fit(x,y)\n    ranks[\"LinReg\"] = ranking(np.abs(lr.coef_), colnames)\n\n\n    ridge = Ridge(alpha = 7)\n    ridge.fit(x,y)\n    ranks['Ridge'] = ranking(np.abs(ridge.coef_), colnames)\n\n\n    lasso = Lasso(alpha=.05)\n    lasso.fit(x,y)\n    ranks[\"Lasso\"] = ranking(np.abs(lasso.coef_), colnames)\n\n    rf = RandomForestRegressor(n_jobs=-1, n_estimators=50, verbose=3)\n    rf.fit(x,y)\n    ranks[\"RF\"] = ranking(rf.feature_importances_, colnames);\n\n    r = {}\n    for name in colnames:\n        r[name] = round(np.mean([ranks[method][name] for method in ranks.keys()]), 2)\n    methods = sorted(ranks.keys())\n    ranks[\"Mean\"] = r\n    meanplot = pd.DataFrame(list(r.items()), columns= ['Feature','Mean Ranking'])\n    meanplot = meanplot.sort_values('Mean Ranking', ascending=False)\n    sns.factorplot(x=\"Mean Ranking\", y=\"Feature\", data = meanplot, kind=\"bar\", size=14, aspect=1.9, palette='coolwarm')\n\n\ndef cleanTitle(data):\n    TitlesCount=data['Title'].value_counts()\n    \n    Title=[]\n    for i, v in TitlesCount.iteritems():\n        if(v < 10):\n            Title.append(i)\n            \n    for index, row in data.iterrows():\n        if row['Title'] in Title:\n            data['Title'][index]='misc'\n            \n    \n    return data\n\ndef ModelSelection(test_data,features,label):\n    MLA = [\n    \n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n        \n    gaussian_process.GaussianProcessClassifier(),\n       \n    linear_model.LogisticRegressionCV(),\n    linear_model.PassiveAggressiveClassifier(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    linear_model.Perceptron(),\n        \n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n        \n    neighbors.KNeighborsClassifier(),\n        \n    svm.SVC(probability=True),\n    svm.NuSVC(probability=True),\n    svm.LinearSVC(),\n        \n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(),\n        \n    discriminant_analysis.LinearDiscriminantAnalysis(),\n    discriminant_analysis.QuadraticDiscriminantAnalysis(),\n        \n    ]\n    \n    MLA_columns = ['MLA Name', 'MLA Parameters','MLA Score']\n    MLA_compare = pd.DataFrame(columns = MLA_columns)\n    x_train,x_test,y_train,y_test = train_test_split (train_data[features],train_data[label],test_size=0.2)\n    row_index = 0\n    MLA_predict = train_data[label]\n    for alg in MLA:\n\n        MLA_name = alg.__class__.__name__\n        MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n        MLA_compare.loc[row_index, 'MLA Parameters'] = str(alg.get_params())\n        alg.fit(x_train, y_train)\n        MLA_predict[MLA_name] = alg.predict(x_test)\n        MLA_compare.loc[row_index, 'MLA Score']=alg.score(x_test,y_test)\n        row_index+=1\n\n    \n    MLA_compare.sort_values(by = ['MLA Score'], ascending = False, inplace = True)\n    return MLA_compare,x_train,x_test,y_train,y_test\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fd9c87772bfe9fb939bb56ab44b5ca9a52e025b3"},"cell_type":"markdown","source":"**Loading Data from CSV using pandas**"},{"metadata":{"trusted":true,"_uuid":"d8059108bd0619084af69c2a948661d3ee0df6a2"},"cell_type":"code","source":"print(\"========Loading Data========\")\nFileLocation=\"../input/train.csv\"\nFileType=\"csv\"\ntrain_data=readData(FileLocation,FileType)\nFileLocation=\"../input/test.csv\"\ntest_data=readData(FileLocation,FileType)\npd.set_option('display.expand_frame_repr', False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"20484bbd364854a0b30d1903e07f67d5eeee76b4"},"cell_type":"markdown","source":"**Cleaning Data**\n\n* During this step have filled 'Age' and 'Fare' features with median values of this column\n* Combined 'SibSp' and 'Parch' to single feature 'FamilySize'\n* Extracted Titles from Name column\n* Dropped unnecessary column 'SibSp','Parch','Name'\n* used get_dummies for one_hot_encoding (converting categorical data to binary data)\n* Changed Data Type of few columns to float which were earlier object type\n"},{"metadata":{"trusted":true,"_uuid":"20a59ca364c89d7b54dd0137fe727b161e0d646e"},"cell_type":"code","source":"print(\"========Cleaning Data========\")\nfillNAValues=['Age','Fare']\ntest_data=DataCleaning(test_data, ['Cabin', 'Ticket'], fillNAValues)\ntrain_data=DataCleaning(train_data, ['PassengerId','Cabin', 'Ticket'], fillNAValues)\ntest_data['FamilySize'] = test_data['SibSp'] + test_data['Parch'] + 1\ntest_data = test_data.drop(['SibSp','Parch'],axis=1)\ntrain_data['FamilySize'] = train_data['SibSp'] + train_data['Parch'] + 1\ntrain_data = train_data.drop(['SibSp','Parch'],axis=1)\n\ntest_data['Title'] = test_data['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\ntrain_data['Title'] = train_data['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\ntrain_data = train_data.drop(['Name'],axis=1)\ntest_data = test_data.drop(['Name'],axis=1)\n    \ntest_data.loc[test_data['Embarked'].isnull()] = 'S'\ntrain_data.loc[train_data['Embarked'].isnull()] = 'S'\n    \ntest_data=cleanTitle(test_data)\ntrain_data=cleanTitle(train_data)\n\nlabel = LabelEncoder()\n\nx=pd.get_dummies(test_data[['Sex','Embarked','Title']])\ny=test_data[['FamilySize','Age','Fare','Pclass','PassengerId']]\ntest_data = pd.concat([x, y], axis=1, sort=False)\n\nx=pd.get_dummies(train_data[['Sex','Embarked','Title']])\ny=train_data[['Survived','FamilySize','Age','Fare','Pclass']]\ntrain_data = pd.concat([x, y], axis=1, sort=False)\n\ntrain_data = train_data.drop(['Sex_S'],axis=1)\ntrain_data=train_data[train_data.Survived != 'S']\ntrain_data[['Survived']] = train_data[['Survived']].astype(int)\ntest_data = test_data.astype('float')\n\nfeatures=['Title_Mr','Title_Miss','Title_Mrs','Sex_male','Sex_female','Embarked_S','Embarked_C','FamilySize','Age','Fare','Embarked_Q','Title_misc','Title_Master','Pclass']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5cb5214b2e8902eb1fdac0a0169eb01ea6f1be6b"},"cell_type":"markdown","source":"**Checking Data Correlation**"},{"metadata":{"trusted":true,"_uuid":"bc7ecd289c42869f0e18c8f3079c5b3d2b85bcb2"},"cell_type":"code","source":"print(\"========Data Correlation========\")\nPlotDataCorrelation(train_data,'Survived')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2fd80052dfe2fb9479e8b0a19932b0f9b39ba745"},"cell_type":"markdown","source":"**Model Selection**\n\nWe have so many Machine Learning algorithms and without using few of them we are not able to tell best model giving good accuracy\nfor this need have used multiple machine learning algorithm and created a loop to score accuracy of each model in list and select top 5 models.\nThese top 5 models now can be used in voting algorithm which is done in next step."},{"metadata":{"trusted":true,"_uuid":"ae897ae5130699c12033a6d7cf0d2568562cd116"},"cell_type":"code","source":"MLA_compare,x_train,x_test,y_train,y_test=ModelSelection(test_data,features,'Survived')\nprint(MLA_compare[['MLA Name','MLA Score']].head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"812f41f0d6a73c98f22ac1a3636894a025b6434e"},"cell_type":"code","source":"import os\ncls=ensemble.GradientBoostingClassifier()\npredictedOutput=test_data[['PassengerId']].astype('int')\ncls.fit(train_data[features], train_data[['Survived']])\npredictedOutput['Survived'] = cls.predict(test_data[features])\nprint(predictedOutput.head())\npredictedOutput.to_csv('gender_submission.csv', sep=',', index=False)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c40770e235bb123144953fe53edb2a5cef5c1907"},"cell_type":"markdown","source":"**Voting Classifier**\n\nIn this multiple classifiers are used and majority output is used as the final predicted value of voting classifier.\nWe picked top 3 classifiers we identified in earlier step for creating our customized voting classifier\n"},{"metadata":{"trusted":true,"_uuid":"d04513db8370b1fff9a5e2113de63aa533ca9ce1"},"cell_type":"code","source":"seed = 7\nkfold = model_selection.KFold(n_splits=10, random_state=seed)\n\nestimators = []\n\nmodel1 = linear_model.LogisticRegressionCV()\nestimators.append(('LRCV', model1))\n\nmodel2 = discriminant_analysis.LinearDiscriminantAnalysis()\nestimators.append(('LDA', model2))\n\nmodel3 = linear_model.RidgeClassifierCV()\nestimators.append(('RCCV', model3))\n\nmodel4 = GradientBoostingClassifier()\nestimators.append(('GBC', model4))\n\nensemble = VotingClassifier(estimators)\nresults = model_selection.cross_val_score(ensemble, x_train, y_train, cv=kfold)\nprint(results.mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8e562d81214682c7be6eb565cbebcb4c5f069ed6"},"cell_type":"code","source":"import os\ncls=ensemble\npredictedOutput=test_data[['PassengerId']].astype('int')\ncls.fit(train_data[features], train_data[['Survived']])\npredictedOutput['Survived'] = cls.predict(test_data[features])\nprint(predictedOutput.head())\npredictedOutput.to_csv('gender_submission.csv', sep=',', index=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"09fe94cdbc6e506f1bb3fb4c7b3369f2a5f40be7"},"cell_type":"markdown","source":"Neural Network in Keras for Binary Classification\n\nTo compare it with Neural Networks have used keras library as we can build models quickly for testing.\n* It consists of 32 neurons in hidden layer\n* 14 input neurons and 1 binary output\n* sigmoid is used as activation function\n\nNote:Also tried deep neural network but did not found it useful thus used network with only 1 hidden layer"},{"metadata":{"trusted":true,"_uuid":"02ec34a0c167be85603c7d9c05f3f171e81b9d73","scrolled":false},"cell_type":"code","source":"x = x_train.values\ny = y_train.values\nx_val = x_test.values\ny_val = y_test.values\n\nmodel = Sequential()\nmodel.add(Dense(32, activation='relu', input_dim=14))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])\nmodel.fit(x, y,epochs=50,batch_size=32)\nscore = model.evaluate(x_val, y_val, batch_size=32)\nprint(score[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"091278837f7b7528273395b7150ac22caff603eb"},"cell_type":"markdown","source":"**Voting classifier seems to produce best result in our case with - 84.5% accuracy**\n\n\n**If you found this kernal useful please do upvote and comment in the comment section below -**\n**Improvements and suggestions are also welcome-**"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}