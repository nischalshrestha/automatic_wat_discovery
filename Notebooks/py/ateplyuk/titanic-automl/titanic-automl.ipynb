{"cells":[{"metadata":{"_cell_guid":"ee4f3377-06dd-4096-a51c-a1b9df31144d","_uuid":"753c352db8307865735b0910c7e378fb354ce092","trusted":true},"cell_type":"code","source":"import h2o\nfrom h2o.automl import H2OAutoML\nimport pandas as pd","execution_count":1,"outputs":[{"name":"stdout","output_type":"stream","text":"[WARNING] H2O requires colorama module of version 0.3.8 or newer. You have version 0.3.7.\nYou can upgrade to the newest version of the module running from the command line\n    $ pip3 install --upgrade colorama\n"}]},{"metadata":{"_cell_guid":"2974ef6f-36a8-4403-9039-fdf9877e77d8","_uuid":"d4dbecf62054a140d56b183e6cce4036d6066e65","trusted":true},"cell_type":"code","source":"h2o.init()","execution_count":2,"outputs":[{"name":"stdout","output_type":"stream","text":"Checking whether there is an H2O instance running at http://localhost:54321..... not found.\nAttempting to start a local H2O server...\n  Java Version: java version \"1.8.0_161\"; Java(TM) SE Runtime Environment (build 1.8.0_161-b12); Java HotSpot(TM) 64-Bit Server VM (build 25.161-b12, mixed mode)\n  Starting server from /opt/conda/lib/python3.6/site-packages/h2o/backend/bin/h2o.jar\n  Ice root: /tmp/tmpslst4djw\n  JVM stdout: /tmp/tmpslst4djw/h2o_unknownUser_started_from_python.out\n  JVM stderr: /tmp/tmpslst4djw/h2o_unknownUser_started_from_python.err\n  Server is running at http://127.0.0.1:54321\nConnecting to H2O server at http://127.0.0.1:54321... successful.\n"},{"data":{"text/html":"<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n<td>22 secs</td></tr>\n<tr><td>H2O cluster version:</td>\n<td>3.16.0.4</td></tr>\n<tr><td>H2O cluster version age:</td>\n<td>22 days </td></tr>\n<tr><td>H2O cluster name:</td>\n<td>H2O_from_python_unknownUser_ts6oxx</td></tr>\n<tr><td>H2O cluster total nodes:</td>\n<td>1</td></tr>\n<tr><td>H2O cluster free memory:</td>\n<td>25.14 Gb</td></tr>\n<tr><td>H2O cluster total cores:</td>\n<td>32</td></tr>\n<tr><td>H2O cluster allowed cores:</td>\n<td>32</td></tr>\n<tr><td>H2O cluster status:</td>\n<td>accepting new members, healthy</td></tr>\n<tr><td>H2O connection url:</td>\n<td>http://127.0.0.1:54321</td></tr>\n<tr><td>H2O connection proxy:</td>\n<td>None</td></tr>\n<tr><td>H2O internal security:</td>\n<td>False</td></tr>\n<tr><td>H2O API Extensions:</td>\n<td>XGBoost, Algos, AutoML, Core V3, Core V4</td></tr>\n<tr><td>Python version:</td>\n<td>3.6.4 final</td></tr></table></div>","text/plain":"--------------------------  ----------------------------------------\nH2O cluster uptime:         22 secs\nH2O cluster version:        3.16.0.4\nH2O cluster version age:    22 days\nH2O cluster name:           H2O_from_python_unknownUser_ts6oxx\nH2O cluster total nodes:    1\nH2O cluster free memory:    25.14 Gb\nH2O cluster total cores:    32\nH2O cluster allowed cores:  32\nH2O cluster status:         accepting new members, healthy\nH2O connection url:         http://127.0.0.1:54321\nH2O connection proxy:\nH2O internal security:      False\nH2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4\nPython version:             3.6.4 final\n--------------------------  ----------------------------------------"},"metadata":{},"output_type":"display_data"}]},{"metadata":{"_cell_guid":"119f7f9e-23db-41a7-bc87-e73bb2c5c283","_uuid":"e31f4345a98346bdc0026c3035d5f760249071f2","trusted":true},"cell_type":"code","source":"df = h2o.import_file('../input/train.csv')","execution_count":3,"outputs":[{"name":"stdout","output_type":"stream","text":"Parse progress: |█████████████████████████████████████████████████████████| 100%\n"}]},{"metadata":{"_cell_guid":"5be6f048-66b7-45bc-8c3e-4447b6453c35","_uuid":"5c3d65c126f8e42c270adff6da509a05c7698814","trusted":true},"cell_type":"code","source":"df_test = h2o.import_file('../input/test.csv')","execution_count":4,"outputs":[{"name":"stdout","output_type":"stream","text":"Parse progress: |█████████████████████████████████████████████████████████| 100%\n"}]},{"metadata":{"_cell_guid":"b95268f4-ec6a-4930-a38b-7dab23abd98e","_uuid":"255e4d339ecd4289a8d9b235a7a64b71bc1c544a","trusted":true},"cell_type":"code","source":"df_test.head()","execution_count":17,"outputs":[{"data":{"text/html":"<table>\n<thead>\n<tr><th style=\"text-align: right;\">  PassengerId</th><th style=\"text-align: right;\">  Pclass</th><th>Name                                        </th><th>Sex   </th><th style=\"text-align: right;\">  Age</th><th style=\"text-align: right;\">  SibSp</th><th style=\"text-align: right;\">  Parch</th><th style=\"text-align: right;\">         Ticket</th><th style=\"text-align: right;\">   Fare</th><th>Cabin  </th><th>Embarked  </th></tr>\n</thead>\n<tbody>\n<tr><td style=\"text-align: right;\">          892</td><td style=\"text-align: right;\">       3</td><td>Kelly, Mr. James                            </td><td>male  </td><td style=\"text-align: right;\"> 34.5</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">330911         </td><td style=\"text-align: right;\"> 7.8292</td><td>       </td><td>Q         </td></tr>\n<tr><td style=\"text-align: right;\">          893</td><td style=\"text-align: right;\">       3</td><td>Wilkes, Mrs. James (Ellen Needs)            </td><td>female</td><td style=\"text-align: right;\"> 47  </td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">363272         </td><td style=\"text-align: right;\"> 7     </td><td>       </td><td>S         </td></tr>\n<tr><td style=\"text-align: right;\">          894</td><td style=\"text-align: right;\">       2</td><td>Myles, Mr. Thomas Francis                   </td><td>male  </td><td style=\"text-align: right;\"> 62  </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">240276         </td><td style=\"text-align: right;\"> 9.6875</td><td>       </td><td>Q         </td></tr>\n<tr><td style=\"text-align: right;\">          895</td><td style=\"text-align: right;\">       3</td><td>Wirz, Mr. Albert                            </td><td>male  </td><td style=\"text-align: right;\"> 27  </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">315154         </td><td style=\"text-align: right;\"> 8.6625</td><td>       </td><td>S         </td></tr>\n<tr><td style=\"text-align: right;\">          896</td><td style=\"text-align: right;\">       3</td><td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td><td>female</td><td style=\"text-align: right;\"> 22  </td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">     3.1013e+06</td><td style=\"text-align: right;\">12.2875</td><td>       </td><td>S         </td></tr>\n<tr><td style=\"text-align: right;\">          897</td><td style=\"text-align: right;\">       3</td><td>Svensson, Mr. Johan Cervin                  </td><td>male  </td><td style=\"text-align: right;\"> 14  </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">  7538         </td><td style=\"text-align: right;\"> 9.225 </td><td>       </td><td>S         </td></tr>\n<tr><td style=\"text-align: right;\">          898</td><td style=\"text-align: right;\">       3</td><td>Connolly, Miss. Kate                        </td><td>female</td><td style=\"text-align: right;\"> 30  </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">330972         </td><td style=\"text-align: right;\"> 7.6292</td><td>       </td><td>Q         </td></tr>\n<tr><td style=\"text-align: right;\">          899</td><td style=\"text-align: right;\">       2</td><td>Caldwell, Mr. Albert Francis                </td><td>male  </td><td style=\"text-align: right;\"> 26  </td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">248738         </td><td style=\"text-align: right;\">29     </td><td>       </td><td>S         </td></tr>\n<tr><td style=\"text-align: right;\">          900</td><td style=\"text-align: right;\">       3</td><td>Abrahim, Mrs. Joseph (Sophie Halaut Easu)   </td><td>female</td><td style=\"text-align: right;\"> 18  </td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">  2657         </td><td style=\"text-align: right;\"> 7.2292</td><td>       </td><td>C         </td></tr>\n<tr><td style=\"text-align: right;\">          901</td><td style=\"text-align: right;\">       3</td><td>Davies, Mr. John Samuel                     </td><td>male  </td><td style=\"text-align: right;\"> 21  </td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">      0</td><td style=\"text-align: right;\">   nan         </td><td style=\"text-align: right;\">24.15  </td><td>       </td><td>S         </td></tr>\n</tbody>\n</table>"},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":""},"execution_count":17,"metadata":{},"output_type":"execute_result"}]},{"metadata":{"_cell_guid":"65897e77-5fdd-42cf-9a74-5617eb88c2bc","_uuid":"577b9affd4e764c6c71b47a7ce40b7ef8df2d4c3","collapsed":true,"trusted":true},"cell_type":"code","source":"train, test = df.split_frame(ratios=[.7])","execution_count":6,"outputs":[]},{"metadata":{"_cell_guid":"4ee52ed1-5a9a-4bc2-be70-09d70172e74e","_uuid":"ec3c3b25058896670adee4300f3f2417335f9490","collapsed":true,"trusted":true},"cell_type":"code","source":"# Identify predictors and response\nx = train.columns\ny = \"Survived\"\nx.remove(y)","execution_count":7,"outputs":[]},{"metadata":{"_cell_guid":"5bc04211-523c-4dce-b8d8-bfeddaa91a75","_uuid":"40124a432069efcc49dfa236d588483d45d9ecce","collapsed":true,"trusted":true},"cell_type":"code","source":"# For binary classification, response should be a factor\ntrain[y] = train[y].asfactor()\ntest[y] = test[y].asfactor()","execution_count":8,"outputs":[]},{"metadata":{"_cell_guid":"c829bcc7-85c0-449d-999b-8a0d68160505","_uuid":"5a7ea9305b6af805e0cc8df248b88989f3e5dc83","trusted":true},"cell_type":"code","source":"# Run AutoML for 900 seconds\naml = H2OAutoML(max_runtime_secs = 900)\naml.train(x = x, y = y, training_frame = train, leaderboard_frame = test)","execution_count":9,"outputs":[{"name":"stdout","output_type":"stream","text":"AutoML progress: |████████████████████████████████████████████████████████| 100%\nParse progress: |█████████████████████████████████████████████████████████| 100%\n"}]},{"metadata":{"_cell_guid":"196d84ff-63f4-4733-aa37-45bbee7a1de6","_uuid":"a23e90840aee77a3495a569a4052fff9397e50ee","trusted":true},"cell_type":"code","source":"# View the AutoML Leaderboard\naml.leaderboard\naml.leader","execution_count":10,"outputs":[{"name":"stdout","output_type":"stream","text":"Model Details\n=============\nH2ODeepLearningEstimator :  Deep Learning\nModel Key:  DeepLearning_grid_0_AutoML_20180207_055702_model_3\n\n\nModelMetricsBinomial: deeplearning\n** Reported on train data. **\n\nMSE: 0.08196010543749273\nRMSE: 0.28628675386313757\nLogLoss: 0.2767090571019453\nMean Per-Class Error: 0.10699974422371894\nAUC: 0.9464319208798704\nGini: 0.8928638417597408\nConfusion Matrix (Act/Pred) for max f1 @ threshold = 0.20552518538232098: \n"},{"data":{"text/html":"<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n<td><b>0</b></td>\n<td><b>1</b></td>\n<td><b>Error</b></td>\n<td><b>Rate</b></td></tr>\n<tr><td>0</td>\n<td>292.0</td>\n<td>25.0</td>\n<td>0.0789</td>\n<td> (25.0/317.0)</td></tr>\n<tr><td>1</td>\n<td>25.0</td>\n<td>160.0</td>\n<td>0.1351</td>\n<td> (25.0/185.0)</td></tr>\n<tr><td>Total</td>\n<td>317.0</td>\n<td>185.0</td>\n<td>0.0996</td>\n<td> (50.0/502.0)</td></tr></table></div>","text/plain":"       0    1    Error    Rate\n-----  ---  ---  -------  ------------\n0      292  25   0.0789   (25.0/317.0)\n1      25   160  0.1351   (25.0/185.0)\nTotal  317  185  0.0996   (50.0/502.0)"},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":"Maximum Metrics: Maximum metrics at their respective thresholds\n\n"},{"data":{"text/html":"<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n<td><b>threshold</b></td>\n<td><b>value</b></td>\n<td><b>idx</b></td></tr>\n<tr><td>max f1</td>\n<td>0.2055252</td>\n<td>0.8648649</td>\n<td>158.0</td></tr>\n<tr><td>max f2</td>\n<td>0.1569280</td>\n<td>0.8835979</td>\n<td>177.0</td></tr>\n<tr><td>max f0point5</td>\n<td>0.5474904</td>\n<td>0.9122085</td>\n<td>111.0</td></tr>\n<tr><td>max accuracy</td>\n<td>0.3674859</td>\n<td>0.9043825</td>\n<td>130.0</td></tr>\n<tr><td>max precision</td>\n<td>0.9999468</td>\n<td>1.0</td>\n<td>0.0</td></tr>\n<tr><td>max recall</td>\n<td>0.0164681</td>\n<td>1.0</td>\n<td>365.0</td></tr>\n<tr><td>max specificity</td>\n<td>0.9999468</td>\n<td>1.0</td>\n<td>0.0</td></tr>\n<tr><td>max absolute_mcc</td>\n<td>0.3674859</td>\n<td>0.7944275</td>\n<td>130.0</td></tr>\n<tr><td>max min_per_class_accuracy</td>\n<td>0.1779459</td>\n<td>0.8864865</td>\n<td>169.0</td></tr>\n<tr><td>max mean_per_class_accuracy</td>\n<td>0.2055252</td>\n<td>0.8930003</td>\n<td>158.0</td></tr></table></div>","text/plain":"metric                       threshold    value     idx\n---------------------------  -----------  --------  -----\nmax f1                       0.205525     0.864865  158\nmax f2                       0.156928     0.883598  177\nmax f0point5                 0.54749      0.912209  111\nmax accuracy                 0.367486     0.904382  130\nmax precision                0.999947     1         0\nmax recall                   0.0164681    1         365\nmax specificity              0.999947     1         0\nmax absolute_mcc             0.367486     0.794428  130\nmax min_per_class_accuracy   0.177946     0.886486  169\nmax mean_per_class_accuracy  0.205525     0.893     158"},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":"Gains/Lift Table: Avg response rate: 36.85 %\n\n"},{"data":{"text/html":"<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n<td><b>group</b></td>\n<td><b>cumulative_data_fraction</b></td>\n<td><b>lower_threshold</b></td>\n<td><b>lift</b></td>\n<td><b>cumulative_lift</b></td>\n<td><b>response_rate</b></td>\n<td><b>cumulative_response_rate</b></td>\n<td><b>capture_rate</b></td>\n<td><b>cumulative_capture_rate</b></td>\n<td><b>gain</b></td>\n<td><b>cumulative_gain</b></td></tr>\n<tr><td></td>\n<td>1</td>\n<td>0.0119522</td>\n<td>0.9998356</td>\n<td>2.7135135</td>\n<td>2.7135135</td>\n<td>1.0</td>\n<td>1.0</td>\n<td>0.0324324</td>\n<td>0.0324324</td>\n<td>171.3513514</td>\n<td>171.3513514</td></tr>\n<tr><td></td>\n<td>2</td>\n<td>0.0219124</td>\n<td>0.9997184</td>\n<td>2.7135135</td>\n<td>2.7135135</td>\n<td>1.0</td>\n<td>1.0</td>\n<td>0.0270270</td>\n<td>0.0594595</td>\n<td>171.3513514</td>\n<td>171.3513514</td></tr>\n<tr><td></td>\n<td>3</td>\n<td>0.0318725</td>\n<td>0.9993597</td>\n<td>2.7135135</td>\n<td>2.7135135</td>\n<td>1.0</td>\n<td>1.0</td>\n<td>0.0270270</td>\n<td>0.0864865</td>\n<td>171.3513514</td>\n<td>171.3513514</td></tr>\n<tr><td></td>\n<td>4</td>\n<td>0.0418327</td>\n<td>0.9991735</td>\n<td>2.7135135</td>\n<td>2.7135135</td>\n<td>1.0</td>\n<td>1.0</td>\n<td>0.0270270</td>\n<td>0.1135135</td>\n<td>171.3513514</td>\n<td>171.3513514</td></tr>\n<tr><td></td>\n<td>5</td>\n<td>0.0517928</td>\n<td>0.9986990</td>\n<td>2.7135135</td>\n<td>2.7135135</td>\n<td>1.0</td>\n<td>1.0</td>\n<td>0.0270270</td>\n<td>0.1405405</td>\n<td>171.3513514</td>\n<td>171.3513514</td></tr>\n<tr><td></td>\n<td>6</td>\n<td>0.1015936</td>\n<td>0.9870320</td>\n<td>2.7135135</td>\n<td>2.7135135</td>\n<td>1.0</td>\n<td>1.0</td>\n<td>0.1351351</td>\n<td>0.2756757</td>\n<td>171.3513514</td>\n<td>171.3513514</td></tr>\n<tr><td></td>\n<td>7</td>\n<td>0.1513944</td>\n<td>0.9336565</td>\n<td>2.7135135</td>\n<td>2.7135135</td>\n<td>1.0</td>\n<td>1.0</td>\n<td>0.1351351</td>\n<td>0.4108108</td>\n<td>171.3513514</td>\n<td>171.3513514</td></tr>\n<tr><td></td>\n<td>8</td>\n<td>0.2011952</td>\n<td>0.8539925</td>\n<td>2.7135135</td>\n<td>2.7135135</td>\n<td>1.0</td>\n<td>1.0</td>\n<td>0.1351351</td>\n<td>0.5459459</td>\n<td>171.3513514</td>\n<td>171.3513514</td></tr>\n<tr><td></td>\n<td>9</td>\n<td>0.3007968</td>\n<td>0.4046448</td>\n<td>2.2250811</td>\n<td>2.5517809</td>\n<td>0.82</td>\n<td>0.9403974</td>\n<td>0.2216216</td>\n<td>0.7675676</td>\n<td>122.5081081</td>\n<td>155.1780920</td></tr>\n<tr><td></td>\n<td>10</td>\n<td>0.4003984</td>\n<td>0.1617308</td>\n<td>1.2482162</td>\n<td>2.2275111</td>\n<td>0.46</td>\n<td>0.8208955</td>\n<td>0.1243243</td>\n<td>0.8918919</td>\n<td>24.8216216</td>\n<td>122.7511093</td></tr>\n<tr><td></td>\n<td>11</td>\n<td>0.5</td>\n<td>0.1088436</td>\n<td>0.3256216</td>\n<td>1.8486486</td>\n<td>0.12</td>\n<td>0.6812749</td>\n<td>0.0324324</td>\n<td>0.9243243</td>\n<td>-67.4378378</td>\n<td>84.8648649</td></tr>\n<tr><td></td>\n<td>12</td>\n<td>0.5996016</td>\n<td>0.0741406</td>\n<td>0.3798919</td>\n<td>1.6046691</td>\n<td>0.14</td>\n<td>0.5913621</td>\n<td>0.0378378</td>\n<td>0.9621622</td>\n<td>-62.0108108</td>\n<td>60.4669121</td></tr>\n<tr><td></td>\n<td>13</td>\n<td>0.6992032</td>\n<td>0.0498025</td>\n<td>0.0542703</td>\n<td>1.3838146</td>\n<td>0.02</td>\n<td>0.5099715</td>\n<td>0.0054054</td>\n<td>0.9675676</td>\n<td>-94.5729730</td>\n<td>38.3814584</td></tr>\n<tr><td></td>\n<td>14</td>\n<td>0.7988048</td>\n<td>0.0309658</td>\n<td>0.1085405</td>\n<td>1.2248029</td>\n<td>0.04</td>\n<td>0.4513716</td>\n<td>0.0108108</td>\n<td>0.9783784</td>\n<td>-89.1459459</td>\n<td>22.4802858</td></tr>\n<tr><td></td>\n<td>15</td>\n<td>0.8984064</td>\n<td>0.0179724</td>\n<td>0.1628108</td>\n<td>1.1070654</td>\n<td>0.06</td>\n<td>0.4079823</td>\n<td>0.0162162</td>\n<td>0.9945946</td>\n<td>-83.7189189</td>\n<td>10.7065380</td></tr>\n<tr><td></td>\n<td>16</td>\n<td>1.0</td>\n<td>0.0023757</td>\n<td>0.0532061</td>\n<td>1.0</td>\n<td>0.0196078</td>\n<td>0.3685259</td>\n<td>0.0054054</td>\n<td>1.0</td>\n<td>-94.6793853</td>\n<td>0.0</td></tr></table></div>","text/plain":"    group    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n--  -------  --------------------------  -----------------  ---------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n    1        0.0119522                   0.999836           2.71351    2.71351            1                1                           0.0324324       0.0324324                  171.351   171.351\n    2        0.0219124                   0.999718           2.71351    2.71351            1                1                           0.027027        0.0594595                  171.351   171.351\n    3        0.0318725                   0.99936            2.71351    2.71351            1                1                           0.027027        0.0864865                  171.351   171.351\n    4        0.0418327                   0.999173           2.71351    2.71351            1                1                           0.027027        0.113514                   171.351   171.351\n    5        0.0517928                   0.998699           2.71351    2.71351            1                1                           0.027027        0.140541                   171.351   171.351\n    6        0.101594                    0.987032           2.71351    2.71351            1                1                           0.135135        0.275676                   171.351   171.351\n    7        0.151394                    0.933657           2.71351    2.71351            1                1                           0.135135        0.410811                   171.351   171.351\n    8        0.201195                    0.853993           2.71351    2.71351            1                1                           0.135135        0.545946                   171.351   171.351\n    9        0.300797                    0.404645           2.22508    2.55178            0.82             0.940397                    0.221622        0.767568                   122.508   155.178\n    10       0.400398                    0.161731           1.24822    2.22751            0.46             0.820896                    0.124324        0.891892                   24.8216   122.751\n    11       0.5                         0.108844           0.325622   1.84865            0.12             0.681275                    0.0324324       0.924324                   -67.4378  84.8649\n    12       0.599602                    0.0741406          0.379892   1.60467            0.14             0.591362                    0.0378378       0.962162                   -62.0108  60.4669\n    13       0.699203                    0.0498025          0.0542703  1.38381            0.02             0.509972                    0.00540541      0.967568                   -94.573   38.3815\n    14       0.798805                    0.0309658          0.108541   1.2248             0.04             0.451372                    0.0108108       0.978378                   -89.1459  22.4803\n    15       0.898406                    0.0179724          0.162811   1.10707            0.06             0.407982                    0.0162162       0.994595                   -83.7189  10.7065\n    16       1                           0.00237569         0.0532061  1                  0.0196078        0.368526                    0.00540541      1                          -94.6794  0"},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":"\n\nModelMetricsBinomial: deeplearning\n** Reported on validation data. **\n\nMSE: 0.12095516697992076\nRMSE: 0.3477860937126738\nLogLoss: 0.41783651386357834\nMean Per-Class Error: 0.14333920187793425\nAUC: 0.8884976525821596\nGini: 0.7769953051643192\nConfusion Matrix (Act/Pred) for max f1 @ threshold = 0.14475907252687872: \n"},{"data":{"text/html":"<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n<td><b>0</b></td>\n<td><b>1</b></td>\n<td><b>Error</b></td>\n<td><b>Rate</b></td></tr>\n<tr><td>0</td>\n<td>61.0</td>\n<td>10.0</td>\n<td>0.1408</td>\n<td> (10.0/71.0)</td></tr>\n<tr><td>1</td>\n<td>7.0</td>\n<td>41.0</td>\n<td>0.1458</td>\n<td> (7.0/48.0)</td></tr>\n<tr><td>Total</td>\n<td>68.0</td>\n<td>51.0</td>\n<td>0.1429</td>\n<td> (17.0/119.0)</td></tr></table></div>","text/plain":"       0    1    Error    Rate\n-----  ---  ---  -------  ------------\n0      61   10   0.1408   (10.0/71.0)\n1      7    41   0.1458   (7.0/48.0)\nTotal  68   51   0.1429   (17.0/119.0)"},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":"Maximum Metrics: Maximum metrics at their respective thresholds\n\n"},{"data":{"text/html":"<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n<td><b>threshold</b></td>\n<td><b>value</b></td>\n<td><b>idx</b></td></tr>\n<tr><td>max f1</td>\n<td>0.1447591</td>\n<td>0.8282828</td>\n<td>50.0</td></tr>\n<tr><td>max f2</td>\n<td>0.1404784</td>\n<td>0.8536585</td>\n<td>53.0</td></tr>\n<tr><td>max f0point5</td>\n<td>0.5242091</td>\n<td>0.9011628</td>\n<td>30.0</td></tr>\n<tr><td>max accuracy</td>\n<td>0.5242091</td>\n<td>0.8571429</td>\n<td>30.0</td></tr>\n<tr><td>max precision</td>\n<td>0.9999856</td>\n<td>1.0</td>\n<td>0.0</td></tr>\n<tr><td>max recall</td>\n<td>0.0122638</td>\n<td>1.0</td>\n<td>115.0</td></tr>\n<tr><td>max specificity</td>\n<td>0.9999856</td>\n<td>1.0</td>\n<td>0.0</td></tr>\n<tr><td>max absolute_mcc</td>\n<td>0.5242091</td>\n<td>0.7218518</td>\n<td>30.0</td></tr>\n<tr><td>max min_per_class_accuracy</td>\n<td>0.1447591</td>\n<td>0.8541667</td>\n<td>50.0</td></tr>\n<tr><td>max mean_per_class_accuracy</td>\n<td>0.1447591</td>\n<td>0.8566608</td>\n<td>50.0</td></tr></table></div>","text/plain":"metric                       threshold    value     idx\n---------------------------  -----------  --------  -----\nmax f1                       0.144759     0.828283  50\nmax f2                       0.140478     0.853659  53\nmax f0point5                 0.524209     0.901163  30\nmax accuracy                 0.524209     0.857143  30\nmax precision                0.999986     1         0\nmax recall                   0.0122638    1         115\nmax specificity              0.999986     1         0\nmax absolute_mcc             0.524209     0.721852  30\nmax min_per_class_accuracy   0.144759     0.854167  50\nmax mean_per_class_accuracy  0.144759     0.856661  50"},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":"Gains/Lift Table: Avg response rate: 40.34 %\n\n"},{"data":{"text/html":"<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n<td><b>group</b></td>\n<td><b>cumulative_data_fraction</b></td>\n<td><b>lower_threshold</b></td>\n<td><b>lift</b></td>\n<td><b>cumulative_lift</b></td>\n<td><b>response_rate</b></td>\n<td><b>cumulative_response_rate</b></td>\n<td><b>capture_rate</b></td>\n<td><b>cumulative_capture_rate</b></td>\n<td><b>gain</b></td>\n<td><b>cumulative_gain</b></td></tr>\n<tr><td></td>\n<td>1</td>\n<td>0.0168067</td>\n<td>0.9998766</td>\n<td>2.4791667</td>\n<td>2.4791667</td>\n<td>1.0</td>\n<td>1.0</td>\n<td>0.0416667</td>\n<td>0.0416667</td>\n<td>147.9166667</td>\n<td>147.9166667</td></tr>\n<tr><td></td>\n<td>2</td>\n<td>0.0252101</td>\n<td>0.9997721</td>\n<td>2.4791667</td>\n<td>2.4791667</td>\n<td>1.0</td>\n<td>1.0</td>\n<td>0.0208333</td>\n<td>0.0625</td>\n<td>147.9166667</td>\n<td>147.9166667</td></tr>\n<tr><td></td>\n<td>3</td>\n<td>0.0336134</td>\n<td>0.9995710</td>\n<td>2.4791667</td>\n<td>2.4791667</td>\n<td>1.0</td>\n<td>1.0</td>\n<td>0.0208333</td>\n<td>0.0833333</td>\n<td>147.9166667</td>\n<td>147.9166667</td></tr>\n<tr><td></td>\n<td>4</td>\n<td>0.0420168</td>\n<td>0.9993121</td>\n<td>2.4791667</td>\n<td>2.4791667</td>\n<td>1.0</td>\n<td>1.0</td>\n<td>0.0208333</td>\n<td>0.1041667</td>\n<td>147.9166667</td>\n<td>147.9166667</td></tr>\n<tr><td></td>\n<td>5</td>\n<td>0.0504202</td>\n<td>0.9991517</td>\n<td>2.4791667</td>\n<td>2.4791667</td>\n<td>1.0</td>\n<td>1.0</td>\n<td>0.0208333</td>\n<td>0.125</td>\n<td>147.9166667</td>\n<td>147.9166667</td></tr>\n<tr><td></td>\n<td>6</td>\n<td>0.1008403</td>\n<td>0.9909731</td>\n<td>2.4791667</td>\n<td>2.4791667</td>\n<td>1.0</td>\n<td>1.0</td>\n<td>0.125</td>\n<td>0.25</td>\n<td>147.9166667</td>\n<td>147.9166667</td></tr>\n<tr><td></td>\n<td>7</td>\n<td>0.1512605</td>\n<td>0.8614616</td>\n<td>2.4791667</td>\n<td>2.4791667</td>\n<td>1.0</td>\n<td>1.0</td>\n<td>0.125</td>\n<td>0.375</td>\n<td>147.9166667</td>\n<td>147.9166667</td></tr>\n<tr><td></td>\n<td>8</td>\n<td>0.2016807</td>\n<td>0.7394365</td>\n<td>2.4791667</td>\n<td>2.4791667</td>\n<td>1.0</td>\n<td>1.0</td>\n<td>0.125</td>\n<td>0.5</td>\n<td>147.9166667</td>\n<td>147.9166667</td></tr>\n<tr><td></td>\n<td>9</td>\n<td>0.3025210</td>\n<td>0.3463768</td>\n<td>1.859375</td>\n<td>2.2725694</td>\n<td>0.75</td>\n<td>0.9166667</td>\n<td>0.1875</td>\n<td>0.6875</td>\n<td>85.9375</td>\n<td>127.2569444</td></tr>\n<tr><td></td>\n<td>10</td>\n<td>0.4033613</td>\n<td>0.1666694</td>\n<td>1.2395833</td>\n<td>2.0143229</td>\n<td>0.5</td>\n<td>0.8125</td>\n<td>0.125</td>\n<td>0.8125</td>\n<td>23.9583333</td>\n<td>101.4322917</td></tr>\n<tr><td></td>\n<td>11</td>\n<td>0.5042017</td>\n<td>0.1286523</td>\n<td>0.6197917</td>\n<td>1.7354167</td>\n<td>0.25</td>\n<td>0.7</td>\n<td>0.0625</td>\n<td>0.875</td>\n<td>-38.0208333</td>\n<td>73.5416667</td></tr>\n<tr><td></td>\n<td>12</td>\n<td>0.5966387</td>\n<td>0.0833826</td>\n<td>0.2253788</td>\n<td>1.5014671</td>\n<td>0.0909091</td>\n<td>0.6056338</td>\n<td>0.0208333</td>\n<td>0.8958333</td>\n<td>-77.4621212</td>\n<td>50.1467136</td></tr>\n<tr><td></td>\n<td>13</td>\n<td>0.6974790</td>\n<td>0.0595259</td>\n<td>0.2065972</td>\n<td>1.3142570</td>\n<td>0.0833333</td>\n<td>0.5301205</td>\n<td>0.0208333</td>\n<td>0.9166667</td>\n<td>-79.3402778</td>\n<td>31.4257028</td></tr>\n<tr><td></td>\n<td>14</td>\n<td>0.7983193</td>\n<td>0.0412425</td>\n<td>0.0</td>\n<td>1.1482456</td>\n<td>0.0</td>\n<td>0.4631579</td>\n<td>0.0</td>\n<td>0.9166667</td>\n<td>-100.0</td>\n<td>14.8245614</td></tr>\n<tr><td></td>\n<td>15</td>\n<td>0.8991597</td>\n<td>0.0251091</td>\n<td>0.2065972</td>\n<td>1.0426402</td>\n<td>0.0833333</td>\n<td>0.4205607</td>\n<td>0.0208333</td>\n<td>0.9375</td>\n<td>-79.3402778</td>\n<td>4.2640187</td></tr>\n<tr><td></td>\n<td>16</td>\n<td>1.0</td>\n<td>0.0085159</td>\n<td>0.6197917</td>\n<td>1.0</td>\n<td>0.25</td>\n<td>0.4033613</td>\n<td>0.0625</td>\n<td>1.0</td>\n<td>-38.0208333</td>\n<td>0.0</td></tr></table></div>","text/plain":"    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n--  -------  --------------------------  -----------------  --------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n    1        0.0168067                   0.999877           2.47917   2.47917            1                1                           0.0416667       0.0416667                  147.917   147.917\n    2        0.0252101                   0.999772           2.47917   2.47917            1                1                           0.0208333       0.0625                     147.917   147.917\n    3        0.0336134                   0.999571           2.47917   2.47917            1                1                           0.0208333       0.0833333                  147.917   147.917\n    4        0.0420168                   0.999312           2.47917   2.47917            1                1                           0.0208333       0.104167                   147.917   147.917\n    5        0.0504202                   0.999152           2.47917   2.47917            1                1                           0.0208333       0.125                      147.917   147.917\n    6        0.10084                     0.990973           2.47917   2.47917            1                1                           0.125           0.25                       147.917   147.917\n    7        0.151261                    0.861462           2.47917   2.47917            1                1                           0.125           0.375                      147.917   147.917\n    8        0.201681                    0.739436           2.47917   2.47917            1                1                           0.125           0.5                        147.917   147.917\n    9        0.302521                    0.346377           1.85938   2.27257            0.75             0.916667                    0.1875          0.6875                     85.9375   127.257\n    10       0.403361                    0.166669           1.23958   2.01432            0.5              0.8125                      0.125           0.8125                     23.9583   101.432\n    11       0.504202                    0.128652           0.619792  1.73542            0.25             0.7                         0.0625          0.875                      -38.0208  73.5417\n    12       0.596639                    0.0833826          0.225379  1.50147            0.0909091        0.605634                    0.0208333       0.895833                   -77.4621  50.1467\n    13       0.697479                    0.0595259          0.206597  1.31426            0.0833333        0.53012                     0.0208333       0.916667                   -79.3403  31.4257\n    14       0.798319                    0.0412425          0         1.14825            0                0.463158                    0               0.916667                   -100      14.8246\n    15       0.89916                     0.0251091          0.206597  1.04264            0.0833333        0.420561                    0.0208333       0.9375                     -79.3403  4.26402\n    16       1                           0.00851589         0.619792  1                  0.25             0.403361                    0.0625          1                          -38.0208  0"},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":"\n\nModelMetricsBinomial: deeplearning\n** Reported on cross-validation data. **\n\nMSE: 0.14960231643877056\nRMSE: 0.38678458660961473\nLogLoss: 0.5144651045673291\nMean Per-Class Error: 0.22839969306846275\nAUC: 0.8281098132833149\nGini: 0.6562196265666298\nConfusion Matrix (Act/Pred) for max f1 @ threshold = 0.174612456004922: \n"},{"data":{"text/html":"<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n<td><b>0</b></td>\n<td><b>1</b></td>\n<td><b>Error</b></td>\n<td><b>Rate</b></td></tr>\n<tr><td>0</td>\n<td>240.0</td>\n<td>77.0</td>\n<td>0.2429</td>\n<td> (77.0/317.0)</td></tr>\n<tr><td>1</td>\n<td>40.0</td>\n<td>145.0</td>\n<td>0.2162</td>\n<td> (40.0/185.0)</td></tr>\n<tr><td>Total</td>\n<td>280.0</td>\n<td>222.0</td>\n<td>0.2331</td>\n<td> (117.0/502.0)</td></tr></table></div>","text/plain":"       0    1    Error    Rate\n-----  ---  ---  -------  -------------\n0      240  77   0.2429   (77.0/317.0)\n1      40   145  0.2162   (40.0/185.0)\nTotal  280  222  0.2331   (117.0/502.0)"},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":"Maximum Metrics: Maximum metrics at their respective thresholds\n\n"},{"data":{"text/html":"<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n<td><b>threshold</b></td>\n<td><b>value</b></td>\n<td><b>idx</b></td></tr>\n<tr><td>max f1</td>\n<td>0.1746125</td>\n<td>0.7125307</td>\n<td>200.0</td></tr>\n<tr><td>max f2</td>\n<td>0.0512977</td>\n<td>0.7661290</td>\n<td>317.0</td></tr>\n<tr><td>max f0point5</td>\n<td>0.7227417</td>\n<td>0.7927786</td>\n<td>94.0</td></tr>\n<tr><td>max accuracy</td>\n<td>0.6830400</td>\n<td>0.8107570</td>\n<td>99.0</td></tr>\n<tr><td>max precision</td>\n<td>0.9999562</td>\n<td>1.0</td>\n<td>0.0</td></tr>\n<tr><td>max recall</td>\n<td>0.0011722</td>\n<td>1.0</td>\n<td>397.0</td></tr>\n<tr><td>max specificity</td>\n<td>0.9999562</td>\n<td>1.0</td>\n<td>0.0</td></tr>\n<tr><td>max absolute_mcc</td>\n<td>0.6830400</td>\n<td>0.5893008</td>\n<td>99.0</td></tr>\n<tr><td>max min_per_class_accuracy</td>\n<td>0.1927540</td>\n<td>0.7621622</td>\n<td>193.0</td></tr>\n<tr><td>max mean_per_class_accuracy</td>\n<td>0.4644910</td>\n<td>0.7716003</td>\n<td>129.0</td></tr></table></div>","text/plain":"metric                       threshold    value     idx\n---------------------------  -----------  --------  -----\nmax f1                       0.174612     0.712531  200\nmax f2                       0.0512977    0.766129  317\nmax f0point5                 0.722742     0.792779  94\nmax accuracy                 0.68304      0.810757  99\nmax precision                0.999956     1         0\nmax recall                   0.00117222   1         397\nmax specificity              0.999956     1         0\nmax absolute_mcc             0.68304      0.589301  99\nmax min_per_class_accuracy   0.192754     0.762162  193\nmax mean_per_class_accuracy  0.464491     0.7716    129"},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":"Gains/Lift Table: Avg response rate: 36.85 %\n\n"},{"data":{"text/html":"<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n<td><b>group</b></td>\n<td><b>cumulative_data_fraction</b></td>\n<td><b>lower_threshold</b></td>\n<td><b>lift</b></td>\n<td><b>cumulative_lift</b></td>\n<td><b>response_rate</b></td>\n<td><b>cumulative_response_rate</b></td>\n<td><b>capture_rate</b></td>\n<td><b>cumulative_capture_rate</b></td>\n<td><b>gain</b></td>\n<td><b>cumulative_gain</b></td></tr>\n<tr><td></td>\n<td>1</td>\n<td>0.0119522</td>\n<td>0.9998135</td>\n<td>2.7135135</td>\n<td>2.7135135</td>\n<td>1.0</td>\n<td>1.0</td>\n<td>0.0324324</td>\n<td>0.0324324</td>\n<td>171.3513514</td>\n<td>171.3513514</td></tr>\n<tr><td></td>\n<td>2</td>\n<td>0.0219124</td>\n<td>0.9989764</td>\n<td>2.7135135</td>\n<td>2.7135135</td>\n<td>1.0</td>\n<td>1.0</td>\n<td>0.0270270</td>\n<td>0.0594595</td>\n<td>171.3513514</td>\n<td>171.3513514</td></tr>\n<tr><td></td>\n<td>3</td>\n<td>0.0318725</td>\n<td>0.9975101</td>\n<td>2.7135135</td>\n<td>2.7135135</td>\n<td>1.0</td>\n<td>1.0</td>\n<td>0.0270270</td>\n<td>0.0864865</td>\n<td>171.3513514</td>\n<td>171.3513514</td></tr>\n<tr><td></td>\n<td>4</td>\n<td>0.0418327</td>\n<td>0.9962256</td>\n<td>2.1708108</td>\n<td>2.5842986</td>\n<td>0.8</td>\n<td>0.9523810</td>\n<td>0.0216216</td>\n<td>0.1081081</td>\n<td>117.0810811</td>\n<td>158.4298584</td></tr>\n<tr><td></td>\n<td>5</td>\n<td>0.0517928</td>\n<td>0.9941699</td>\n<td>2.7135135</td>\n<td>2.6091476</td>\n<td>1.0</td>\n<td>0.9615385</td>\n<td>0.0270270</td>\n<td>0.1351351</td>\n<td>171.3513514</td>\n<td>160.9147609</td></tr>\n<tr><td></td>\n<td>6</td>\n<td>0.1015936</td>\n<td>0.9624861</td>\n<td>2.3878919</td>\n<td>2.5006889</td>\n<td>0.88</td>\n<td>0.9215686</td>\n<td>0.1189189</td>\n<td>0.2540541</td>\n<td>138.7891892</td>\n<td>150.0688924</td></tr>\n<tr><td></td>\n<td>7</td>\n<td>0.1513944</td>\n<td>0.8846268</td>\n<td>2.4964324</td>\n<td>2.4992888</td>\n<td>0.92</td>\n<td>0.9210526</td>\n<td>0.1243243</td>\n<td>0.3783784</td>\n<td>149.6432432</td>\n<td>149.9288762</td></tr>\n<tr><td></td>\n<td>8</td>\n<td>0.2011952</td>\n<td>0.7716563</td>\n<td>2.3878919</td>\n<td>2.4717153</td>\n<td>0.88</td>\n<td>0.9108911</td>\n<td>0.1189189</td>\n<td>0.4972973</td>\n<td>138.7891892</td>\n<td>147.1715280</td></tr>\n<tr><td></td>\n<td>9</td>\n<td>0.3007968</td>\n<td>0.4448591</td>\n<td>1.4110270</td>\n<td>2.1204940</td>\n<td>0.52</td>\n<td>0.7814570</td>\n<td>0.1405405</td>\n<td>0.6378378</td>\n<td>41.1027027</td>\n<td>112.0494004</td></tr>\n<tr><td></td>\n<td>10</td>\n<td>0.4003984</td>\n<td>0.2426898</td>\n<td>0.9225946</td>\n<td>1.8225091</td>\n<td>0.34</td>\n<td>0.6716418</td>\n<td>0.0918919</td>\n<td>0.7297297</td>\n<td>-7.7405405</td>\n<td>82.2509076</td></tr>\n<tr><td></td>\n<td>11</td>\n<td>0.5</td>\n<td>0.1319939</td>\n<td>0.8140541</td>\n<td>1.6216216</td>\n<td>0.3</td>\n<td>0.5976096</td>\n<td>0.0810811</td>\n<td>0.8108108</td>\n<td>-18.5945946</td>\n<td>62.1621622</td></tr>\n<tr><td></td>\n<td>12</td>\n<td>0.5996016</td>\n<td>0.0900632</td>\n<td>0.3256216</td>\n<td>1.4063392</td>\n<td>0.12</td>\n<td>0.5182724</td>\n<td>0.0324324</td>\n<td>0.8432432</td>\n<td>-67.4378378</td>\n<td>40.6339230</td></tr>\n<tr><td></td>\n<td>13</td>\n<td>0.6992032</td>\n<td>0.0643331</td>\n<td>0.4341622</td>\n<td>1.2678525</td>\n<td>0.16</td>\n<td>0.4672365</td>\n<td>0.0432432</td>\n<td>0.8864865</td>\n<td>-56.5837838</td>\n<td>26.7852468</td></tr>\n<tr><td></td>\n<td>14</td>\n<td>0.7988048</td>\n<td>0.0416287</td>\n<td>0.5427027</td>\n<td>1.1774348</td>\n<td>0.2</td>\n<td>0.4339152</td>\n<td>0.0540541</td>\n<td>0.9405405</td>\n<td>-45.7297297</td>\n<td>17.7434791</td></tr>\n<tr><td></td>\n<td>15</td>\n<td>0.8984064</td>\n<td>0.0238613</td>\n<td>0.2170811</td>\n<td>1.0709654</td>\n<td>0.08</td>\n<td>0.3946785</td>\n<td>0.0216216</td>\n<td>0.9621622</td>\n<td>-78.2918919</td>\n<td>7.0965422</td></tr>\n<tr><td></td>\n<td>16</td>\n<td>1.0</td>\n<td>0.0001759</td>\n<td>0.3724430</td>\n<td>1.0</td>\n<td>0.1372549</td>\n<td>0.3685259</td>\n<td>0.0378378</td>\n<td>1.0</td>\n<td>-62.7556969</td>\n<td>0.0</td></tr></table></div>","text/plain":"    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    cumulative_response_rate    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n--  -------  --------------------------  -----------------  --------  -----------------  ---------------  --------------------------  --------------  -------------------------  --------  -----------------\n    1        0.0119522                   0.999814           2.71351   2.71351            1                1                           0.0324324       0.0324324                  171.351   171.351\n    2        0.0219124                   0.998976           2.71351   2.71351            1                1                           0.027027        0.0594595                  171.351   171.351\n    3        0.0318725                   0.99751            2.71351   2.71351            1                1                           0.027027        0.0864865                  171.351   171.351\n    4        0.0418327                   0.996226           2.17081   2.5843             0.8              0.952381                    0.0216216       0.108108                   117.081   158.43\n    5        0.0517928                   0.99417            2.71351   2.60915            1                0.961538                    0.027027        0.135135                   171.351   160.915\n    6        0.101594                    0.962486           2.38789   2.50069            0.88             0.921569                    0.118919        0.254054                   138.789   150.069\n    7        0.151394                    0.884627           2.49643   2.49929            0.92             0.921053                    0.124324        0.378378                   149.643   149.929\n    8        0.201195                    0.771656           2.38789   2.47172            0.88             0.910891                    0.118919        0.497297                   138.789   147.172\n    9        0.300797                    0.444859           1.41103   2.12049            0.52             0.781457                    0.140541        0.637838                   41.1027   112.049\n    10       0.400398                    0.24269            0.922595  1.82251            0.34             0.671642                    0.0918919       0.72973                    -7.74054  82.2509\n    11       0.5                         0.131994           0.814054  1.62162            0.3              0.59761                     0.0810811       0.810811                   -18.5946  62.1622\n    12       0.599602                    0.0900632          0.325622  1.40634            0.12             0.518272                    0.0324324       0.843243                   -67.4378  40.6339\n    13       0.699203                    0.0643331          0.434162  1.26785            0.16             0.467236                    0.0432432       0.886486                   -56.5838  26.7852\n    14       0.798805                    0.0416287          0.542703  1.17743            0.2              0.433915                    0.0540541       0.940541                   -45.7297  17.7435\n    15       0.898406                    0.0238613          0.217081  1.07097            0.08             0.394678                    0.0216216       0.962162                   -78.2919  7.09654\n    16       1                           0.000175889        0.372443  1                  0.137255         0.368526                    0.0378378       1                          -62.7557  0"},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":"\nCross-Validation Metrics Summary: \n"},{"data":{"text/html":"<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n<td><b>mean</b></td>\n<td><b>sd</b></td>\n<td><b>cv_1_valid</b></td>\n<td><b>cv_2_valid</b></td>\n<td><b>cv_3_valid</b></td>\n<td><b>cv_4_valid</b></td>\n<td><b>cv_5_valid</b></td></tr>\n<tr><td>accuracy</td>\n<td>0.8128317</td>\n<td>0.0202150</td>\n<td>0.7722772</td>\n<td>0.8118812</td>\n<td>0.86</td>\n<td>0.82</td>\n<td>0.8</td></tr>\n<tr><td>auc</td>\n<td>0.8248008</td>\n<td>0.0278867</td>\n<td>0.8599677</td>\n<td>0.8651778</td>\n<td>0.8443956</td>\n<td>0.7773488</td>\n<td>0.777114</td></tr>\n<tr><td>err</td>\n<td>0.1871683</td>\n<td>0.0202150</td>\n<td>0.2277228</td>\n<td>0.1881188</td>\n<td>0.14</td>\n<td>0.18</td>\n<td>0.2</td></tr>\n<tr><td>err_count</td>\n<td>18.8</td>\n<td>2.0688162</td>\n<td>23.0</td>\n<td>19.0</td>\n<td>14.0</td>\n<td>18.0</td>\n<td>20.0</td></tr>\n<tr><td>f0point5</td>\n<td>0.7532958</td>\n<td>0.0330798</td>\n<td>0.7061068</td>\n<td>0.7399103</td>\n<td>0.8278146</td>\n<td>0.7843137</td>\n<td>0.7083333</td></tr>\n<tr><td>f1</td>\n<td>0.7355019</td>\n<td>0.0397466</td>\n<td>0.7628866</td>\n<td>0.7764706</td>\n<td>0.78125</td>\n<td>0.7272728</td>\n<td>0.6296296</td></tr>\n<tr><td>f2</td>\n<td>0.7261412</td>\n<td>0.0684566</td>\n<td>0.8295964</td>\n<td>0.8168317</td>\n<td>0.7396449</td>\n<td>0.6779661</td>\n<td>0.5666667</td></tr>\n<tr><td>lift_top_group</td>\n<td>2.7358701</td>\n<td>0.1726890</td>\n<td>2.4047618</td>\n<td>2.5897436</td>\n<td>2.857143</td>\n<td>2.7027028</td>\n<td>3.125</td></tr>\n<tr><td>logloss</td>\n<td>0.5029009</td>\n<td>0.0560471</td>\n<td>0.4682926</td>\n<td>0.4121300</td>\n<td>0.4554099</td>\n<td>0.6382148</td>\n<td>0.5404573</td></tr>\n<tr><td>max_per_class_error</td>\n<td>0.3241156</td>\n<td>0.0604928</td>\n<td>0.3050847</td>\n<td>0.2096774</td>\n<td>0.2857143</td>\n<td>0.3513514</td>\n<td>0.46875</td></tr>\n<tr><td>mcc</td>\n<td>0.5998862</td>\n<td>0.0400026</td>\n<td>0.5699189</td>\n<td>0.6222289</td>\n<td>0.6861318</td>\n<td>0.6057196</td>\n<td>0.5154318</td></tr>\n<tr><td>mean_per_class_accuracy</td>\n<td>0.7892095</td>\n<td>0.0242659</td>\n<td>0.7879338</td>\n<td>0.8182382</td>\n<td>0.8263736</td>\n<td>0.7846418</td>\n<td>0.7288603</td></tr>\n<tr><td>mean_per_class_error</td>\n<td>0.2107905</td>\n<td>0.0242659</td>\n<td>0.2120662</td>\n<td>0.1817618</td>\n<td>0.1736264</td>\n<td>0.2153582</td>\n<td>0.2711397</td></tr>\n<tr><td>mse</td>\n<td>0.1500475</td>\n<td>0.0120461</td>\n<td>0.1481355</td>\n<td>0.1309</td>\n<td>0.1321336</td>\n<td>0.1700031</td>\n<td>0.1690654</td></tr>\n<tr><td>precision</td>\n<td>0.7705002</td>\n<td>0.0490191</td>\n<td>0.6727273</td>\n<td>0.7173913</td>\n<td>0.8620689</td>\n<td>0.8275862</td>\n<td>0.7727272</td></tr>\n<tr><td>r2</td>\n<td>0.3501735</td>\n<td>0.0619417</td>\n<td>0.3901814</td>\n<td>0.4477622</td>\n<td>0.4191927</td>\n<td>0.2706861</td>\n<td>0.2230449</td></tr>\n<tr><td>recall</td>\n<td>0.7242581</td>\n<td>0.0908185</td>\n<td>0.8809524</td>\n<td>0.8461539</td>\n<td>0.7142857</td>\n<td>0.6486486</td>\n<td>0.53125</td></tr>\n<tr><td>rmse</td>\n<td>0.3867353</td>\n<td>0.0155453</td>\n<td>0.3848838</td>\n<td>0.3618011</td>\n<td>0.3635019</td>\n<td>0.4123143</td>\n<td>0.4111757</td></tr>\n<tr><td>specificity</td>\n<td>0.8541610</td>\n<td>0.0679622</td>\n<td>0.6949152</td>\n<td>0.7903226</td>\n<td>0.9384615</td>\n<td>0.9206349</td>\n<td>0.9264706</td></tr></table></div>","text/plain":"                         mean      sd         cv_1_valid    cv_2_valid    cv_3_valid    cv_4_valid    cv_5_valid\n-----------------------  --------  ---------  ------------  ------------  ------------  ------------  ------------\naccuracy                 0.812832  0.020215   0.772277      0.811881      0.86          0.82          0.8\nauc                      0.824801  0.0278867  0.859968      0.865178      0.844396      0.777349      0.777114\nerr                      0.187168  0.020215   0.227723      0.188119      0.14          0.18          0.2\nerr_count                18.8      2.06882    23            19            14            18            20\nf0point5                 0.753296  0.0330798  0.706107      0.73991       0.827815      0.784314      0.708333\nf1                       0.735502  0.0397466  0.762887      0.776471      0.78125       0.727273      0.62963\nf2                       0.726141  0.0684566  0.829596      0.816832      0.739645      0.677966      0.566667\nlift_top_group           2.73587   0.172689   2.40476       2.58974       2.85714       2.7027        3.125\nlogloss                  0.502901  0.0560471  0.468293      0.41213       0.45541       0.638215      0.540457\nmax_per_class_error      0.324116  0.0604928  0.305085      0.209677      0.285714      0.351351      0.46875\nmcc                      0.599886  0.0400026  0.569919      0.622229      0.686132      0.60572       0.515432\nmean_per_class_accuracy  0.78921   0.0242659  0.787934      0.818238      0.826374      0.784642      0.72886\nmean_per_class_error     0.21079   0.0242659  0.212066      0.181762      0.173626      0.215358      0.27114\nmse                      0.150048  0.0120461  0.148136      0.1309        0.132134      0.170003      0.169065\nprecision                0.7705    0.0490191  0.672727      0.717391      0.862069      0.827586      0.772727\nr2                       0.350173  0.0619417  0.390181      0.447762      0.419193      0.270686      0.223045\nrecall                   0.724258  0.0908185  0.880952      0.846154      0.714286      0.648649      0.53125\nrmse                     0.386735  0.0155453  0.384884      0.361801      0.363502      0.412314      0.411176\nspecificity              0.854161  0.0679622  0.694915      0.790323      0.938462      0.920635      0.926471"},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":"Scoring History: \n"},{"data":{"text/html":"<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n<td><b>timestamp</b></td>\n<td><b>duration</b></td>\n<td><b>training_speed</b></td>\n<td><b>epochs</b></td>\n<td><b>iterations</b></td>\n<td><b>samples</b></td>\n<td><b>training_rmse</b></td>\n<td><b>training_logloss</b></td>\n<td><b>training_auc</b></td>\n<td><b>training_lift</b></td>\n<td><b>training_classification_error</b></td>\n<td><b>validation_rmse</b></td>\n<td><b>validation_logloss</b></td>\n<td><b>validation_auc</b></td>\n<td><b>validation_lift</b></td>\n<td><b>validation_classification_error</b></td></tr>\n<tr><td></td>\n<td>2018-02-07 06:06:58</td>\n<td> 0.000 sec</td>\n<td>None</td>\n<td>0.0</td>\n<td>0</td>\n<td>0.0</td>\n<td>nan</td>\n<td>nan</td>\n<td>nan</td>\n<td>nan</td>\n<td>nan</td>\n<td>nan</td>\n<td>nan</td>\n<td>nan</td>\n<td>nan</td>\n<td>nan</td></tr>\n<tr><td></td>\n<td>2018-02-07 06:06:58</td>\n<td> 9 min 18.991 sec</td>\n<td>50200 obs/sec</td>\n<td>10.0</td>\n<td>1</td>\n<td>5020.0</td>\n<td>0.3985145</td>\n<td>0.5222211</td>\n<td>0.8496803</td>\n<td>2.7135135</td>\n<td>0.1932271</td>\n<td>0.3982738</td>\n<td>0.5274121</td>\n<td>0.8482981</td>\n<td>2.4791667</td>\n<td>0.1848739</td></tr>\n<tr><td></td>\n<td>2018-02-07 06:07:03</td>\n<td> 9 min 23.474 sec</td>\n<td>39096 obs/sec</td>\n<td>350.0</td>\n<td>35</td>\n<td>175700.0</td>\n<td>0.2862868</td>\n<td>0.2767091</td>\n<td>0.9464319</td>\n<td>2.7135135</td>\n<td>0.0996016</td>\n<td>0.3477861</td>\n<td>0.4178365</td>\n<td>0.8884977</td>\n<td>2.4791667</td>\n<td>0.1428571</td></tr></table></div>","text/plain":"    timestamp            duration          training_speed    epochs    iterations    samples    training_rmse    training_logloss    training_auc    training_lift    training_classification_error    validation_rmse    validation_logloss    validation_auc    validation_lift    validation_classification_error\n--  -------------------  ----------------  ----------------  --------  ------------  ---------  ---------------  ------------------  --------------  ---------------  -------------------------------  -----------------  --------------------  ----------------  -----------------  ---------------------------------\n    2018-02-07 06:06:58  0.000 sec                           0         0             0          nan              nan                 nan             nan              nan                              nan                nan                   nan               nan                nan\n    2018-02-07 06:06:58  9 min 18.991 sec  50200 obs/sec     10        1             5020       0.398515         0.522221            0.84968         2.71351          0.193227                         0.398274           0.527412              0.848298          2.47917            0.184874\n    2018-02-07 06:07:03  9 min 23.474 sec  39096 obs/sec     350       35            175700     0.286287         0.276709            0.946432        2.71351          0.0996016                        0.347786           0.417837              0.888498          2.47917            0.142857"},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":"Variable Importances: \n"},{"data":{"text/html":"<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>variable</b></td>\n<td><b>relative_importance</b></td>\n<td><b>scaled_importance</b></td>\n<td><b>percentage</b></td></tr>\n<tr><td>Fare</td>\n<td>1.0</td>\n<td>1.0</td>\n<td>0.0091068</td></tr>\n<tr><td>Cabin.E12</td>\n<td>0.9069815</td>\n<td>0.9069815</td>\n<td>0.0082597</td></tr>\n<tr><td>Ticket</td>\n<td>0.8972361</td>\n<td>0.8972361</td>\n<td>0.0081710</td></tr>\n<tr><td>Cabin.B50</td>\n<td>0.8968905</td>\n<td>0.8968905</td>\n<td>0.0081678</td></tr>\n<tr><td>Cabin.E24</td>\n<td>0.8885797</td>\n<td>0.8885797</td>\n<td>0.0080921</td></tr>\n<tr><td>---</td>\n<td>---</td>\n<td>---</td>\n<td>---</td></tr>\n<tr><td>Cabin.D45</td>\n<td>0.5315903</td>\n<td>0.5315903</td>\n<td>0.0048411</td></tr>\n<tr><td>Cabin.B57 B59 B63 B66</td>\n<td>0.5239494</td>\n<td>0.5239494</td>\n<td>0.0047715</td></tr>\n<tr><td>Cabin.C2</td>\n<td>0.4719828</td>\n<td>0.4719828</td>\n<td>0.0042983</td></tr>\n<tr><td>Embarked.missing(NA)</td>\n<td>0.0</td>\n<td>0.0</td>\n<td>0.0</td></tr>\n<tr><td>Sex.missing(NA)</td>\n<td>0.0</td>\n<td>0.0</td>\n<td>0.0</td></tr></table></div>","text/plain":"variable               relative_importance    scaled_importance    percentage\n---------------------  ---------------------  -------------------  --------------------\nFare                   1.0                    1.0                  0.009106823949969454\nCabin.E12              0.9069814682006836     0.9069814682006836   0.008259720556788445\nTicket                 0.8972361087799072     0.8972361087799072   0.008170971284214257\nCabin.B50              0.8968904614448547     0.8968904614448547   0.008167823534785158\nCabin.E24              0.8885797262191772     0.8885797262191772   0.008092139132190104\n---                    ---                    ---                  ---\nCabin.D45              0.5315903425216675     0.5315903425216675   0.004841099662848787\nCabin.B57 B59 B63 B66  0.523949384689331      0.523949384689331    0.004771514805060559\nCabin.C2               0.47198277711868286    0.47198277711868286  0.004298264058637516\nEmbarked.missing(NA)   0.0                    0.0                  0.0\nSex.missing(NA)        0.0                    0.0                  0.0"},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":"\nSee the whole table with table.as_data_frame()\n"},{"data":{"text/plain":""},"execution_count":10,"metadata":{},"output_type":"execute_result"}]},{"metadata":{"_cell_guid":"fe3e979c-579e-4881-bc34-9ca5a1de9c77","_uuid":"2d2eaaf168db480f734b2d78c6fc1090e9cc3072","trusted":true},"cell_type":"code","source":"preds = aml.leader.predict(df_test)","execution_count":11,"outputs":[{"name":"stdout","output_type":"stream","text":"deeplearning prediction progress: |███████████████████████████████████████| 100%\n"},{"name":"stderr","output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/h2o/job.py:69: UserWarning: Test/Validation dataset column 'Cabin' has levels not trained on: [A11, A18, A21, A29, A9, B10, B11, B24, B26, B36, B45, B52 B54 B56, B61, C105, C116, C130, C132, C28, C31, C39, C51, C53, C55 C57, C6, C80, C89, C97, D22, D34, D38, D40, D43, E39 E41, E45, E52, E60, F, F E46, F E57]\n  warnings.warn(w)\n"}]},{"metadata":{"_cell_guid":"a7e4e8d8-bef6-445f-896b-b67a0b578a3e","_uuid":"4e1758a6ddaea3ae70b8eed01c7b5941637a8f05","collapsed":true,"trusted":true},"cell_type":"code","source":"pr = preds['predict']\ndf_pr = pr.as_data_frame()","execution_count":12,"outputs":[]},{"metadata":{"_cell_guid":"8a22d0f2-68fa-4f60-aab2-1cff32f3bafa","_uuid":"1aa7b406d70a4bebe89003780a30ca3632b38531","collapsed":true,"trusted":true},"cell_type":"code","source":"# Запись результата\nresult = df_test['PassengerId'].as_data_frame()\nresult.insert(1,'Survived', df_pr)\nresult.to_csv('Result.csv', index=False)","execution_count":16,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}