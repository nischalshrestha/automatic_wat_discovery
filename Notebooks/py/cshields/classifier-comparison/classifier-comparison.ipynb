{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":107,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"#The train data has features (independent variables) and targets (dependent variable).\n#Feature examples are Name, Age, or Fare. The target is if the passenger survived.\ntrain=pd.read_csv('../input/train.csv')\n#We print the first 5 rows of the train dataset to show what this looks like:\nntrain = train.shape[0] #this gets the number rows in the traning dataset\nprint(\"Training data (\",ntrain,\"rows)\")\n\n#Display the data in Pandas: .head(n_rows) shows the first n rows of the DataFrame\ndisplay(train.head(10))\n\n#The test dataset is used to test how well the classifier performs\n#Test data only has features, the targets are empty and must be predicted\ntest=pd.read_csv('../input/test.csv')\n\n#Lets looks at the test data...\nntest = test.shape[0]\nprint(\"Test data (\",ntest,\"rows), notice that the survived column (target) is missing!\")\ndisplay(test.head(10))\n\ndf_all=pd.concat([train,test],axis=0)\np_id=df_all['PassengerId']\nprint(df_all.info())","execution_count":108,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bd883eb2ef2a994f437ee8334f2e3e4527891e63"},"cell_type":"code","source":"#First we get value to impute on the missing entries\nage_med=df_all['Age'].median()\nfare_med=df_all['Fare'].median()\nemb_mode=df_all['Embarked'].mode()\n\n#Impute values\ndf_all['Age']=df_all['Age'].fillna(age_med)\ndf_all['Fare']=df_all['Fare'].fillna(fare_med)\ndf_all['Embarked']=df_all['Embarked'].fillna(emb_mode)\n\n#Create new features that might be helpful\n#Family size is the number of parents/children plus siblings/spouses\ndf_all['Family Size']=df_all['Parch']+df_all['SibSp'] \n\n#Convert cabin to a dummy variable, 0 if null and 1 if it has value\ndf_all['Cabin']=df_all['Cabin'].notnull().astype('int')\n\n#Drop other columns\ndf_all=df_all.drop(['Name','PassengerId','Ticket'],axis=1)\n\n#Convert all categorical varaibles to dummy variables\n#This is called one-hot encoding\ndf_all=pd.get_dummies(df_all,drop_first=True)\n\ndisplay(df_all.head(10))","execution_count":109,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e83e06d49c9ff13c7a154e7d2e7a8352a0ce118f"},"cell_type":"code","source":"#Split the data\ntrain=df_all[:ntrain]\n\n#Look at the correlations in the dataset\ncorr=train.corr()\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n\n#Based on correlations drop SibSp, Family Size, Embarked_S, Embarked_Q, Age\n# df_all=df_all.drop(['SibSp','Family Size','Embarked_S','Embarked_Q',],axis=1)\n# df_all=df_all.drop(['SibSp','Family Size','Embarked_S','Embarked_Q','Fare','Parch','Cabin'],axis=1)\nprint (df_all.columns)\ntrain=df_all[:ntrain]","execution_count":110,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"62acdc945bdb7f2c056127f751335ab17a810daa"},"cell_type":"code","source":"#Time to test the classifiers\n#load them:\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression \n","execution_count":111,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"2c07305325b8f4c49f21653afdafc83998e06501"},"cell_type":"code","source":"# #Visualize the classifier results\n\n# names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \n#          \"Decision Tree\", \"Naive Bayes\", 'Logistic Regression']\n\n# classifiers = [\n#     KNeighborsClassifier(3),\n#     SVC(kernel=\"linear\", C=0.025),\n#     SVC(gamma=2, C=1),\n#     DecisionTreeClassifier(max_depth=5),\n#     GaussianNB(),\n#     LogisticRegression()]\n\n# cm = plt.cm.RdBu\n# cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n# i=1\n# figure = plt.figure(figsize=(21, 6))\n# for sex in train['Sex_male'].unique():\n#     h=0.2\n#     x_min, x_max = train['Fare'].loc[train['Sex_male']==sex].min() - .5, train['Fare'].loc[train['Sex_male']==sex].max() + .5\n#     y_min, y_max = train['Age'].loc[train['Sex_male']==sex].min() - .5, train['Age'].loc[train['Sex_male']==sex].max() + .5\n# #     x_min, x_max = train['Fare'].min() - .5, train['Fare'].max() + .5\n# #     y_min, y_max = train['Age'].min() - .5, train['Age'].max() + .5\n#     xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n#                          np.arange(y_min, y_max, h))    \n\n#     ax = plt.subplot(2, len(classifiers) + 1, i)\n\n#     ax.scatter(train['Fare'].loc[(train['Sex_male']==sex) & (train['Survived']==1.0)],\n#                train['Age'].loc[(train['Sex_male']==sex) & (train['Survived']==1.0)],\n#                c='#0000FF',edgecolors='k')\n    \n#     ax.scatter(train['Fare'].loc[(train['Sex_male']==sex) & (train['Survived']==0.0)],\n#                train['Age'].loc[(train['Sex_male']==sex) & (train['Survived']==0.0)],\n#                c='#FF0000',edgecolors='k')\n    \n#     # ax = plt.subplot(2, len(classifiers) + 1, 7)\n#     # ax.scatter(train['Fare'].loc[train['Sex_male']==0],train['Age'].loc[train['Sex_male']==0],c=train['Survived'].loc[train['Sex_male']==0],cmap=cm_bright)\n#     if i == 1:\n#         ax.set_title(\"Input Data\")\n#         ax.set_ylabel('Males, Age')\n#         plt.legend(['Survived','Died'])\n# #         ax = plt.gca()\n# #         legend = ax.get_legend()\n# #         legend.legendHandles[0].set_color(cm_bright(0.0))\n# #         legend.legendHandles[1].set_color(cm_bright(1.0))\n        \n#     if i==7:\n#         ax.set_ylabel('Females, Age')\n#         ax.set_xlabel('Fare')\n#     i+=1\n    \n#     for name,clf in zip(names,classifiers):\n#         ax = plt.subplot(2, len(classifiers) + 1, i)\n#         clf.fit(train[['Fare','Age',]].loc[train['Sex_male']==sex], train['Survived'].loc[train['Sex_male']==sex])\n#         score = clf.score(train[['Fare','Age',]].loc[train['Sex_male']==sex], train['Survived'].loc[train['Sex_male']==sex])\n        \n\n#         # Plot the decision boundary. For that, we will assign a color to each\n#         # point in the mesh [x_min, x_max]x[y_min, y_max].\n#         if hasattr(clf, \"decision_function\"):\n#             Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n#         else:\n#             Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n\n#         # Put the result into a color plot\n#         Z = Z.reshape(xx.shape)\n#         ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n\n#         # Plot also the training points\n#         ax.scatter(train['Fare'].loc[train['Sex_male']==sex],train['Age'].loc[train['Sex_male']==sex], c=train['Survived'].loc[train['Sex_male']==sex],cmap=cm_bright,\n#                    edgecolors='k')\n#         # and testing points\n# #         ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,\n# #                    edgecolors='k', alpha=0.6)\n        \n#         if sex == 1:\n#             ax.set_title(name)\n#         ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),\n#                 size=15, horizontalalignment='right')\n#         if sex==0:\n#             ax.set_xlabel('Fare')\n        \n#         i+=1\n\n# plt.tight_layout()\n","execution_count":112,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"95a85f625e3c3bb186f8fb8278f52b959360bf4e"},"cell_type":"code","source":"#Now test the models in with cross-validation\nfeature_columns=list(train.columns)\nfeature_columns.remove('Survived')\ny_train=train['Survived'].values\nx_train=train[feature_columns].values\n\nlog_cols = [\"Classifier\", \"Accuracy\"]\nlog = pd.DataFrame(columns=log_cols)\nnames = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \n         \"Decision Tree\", \"Naive Bayes\", 'Logistic Regression']\n\nclassifiers = [\n    KNeighborsClassifier(3),\n    SVC(kernel=\"linear\", C=0.025),\n    SVC(gamma=2, C=1),\n    DecisionTreeClassifier(max_depth=5),\n    GaussianNB(),\n    LogisticRegression()]\n\n#Split the data into test and training sets\n#Use these split to train the classifier and test it mutliple times\n#Helps us estimate the performance of the classifiers without \"cheating\" by scoring the classifier on data we've already seen\nsss = StratifiedShuffleSplit(n_splits=10, test_size=0.1, random_state=0)\n\nX = x_train\ny = y_train\n\nacc_dict = {}\nfor train_index, test_index in sss.split(X, y):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    for name,clf in zip(names,classifiers):\n        clf.fit(X_train, y_train)\n        train_predictions = clf.predict(X_test)\n        acc = accuracy_score(y_test, train_predictions)\n        if name in acc_dict:\n            acc_dict[name] += acc\n        else:\n            acc_dict[name] = acc\n\nfor clf in acc_dict:\n    acc_dict[clf] = acc_dict[clf] / 10.0\n    log_entry = pd.DataFrame([[clf, acc_dict[clf]]], columns=log_cols)\n    log = log.append(log_entry)\n\nplt.xlabel('Accuracy')\nplt.title('Classifier Accuracy')\n\nsns.set_color_codes(\"muted\")\nsns.barplot(x='Accuracy', y='Classifier', data=log, color=\"b\")","execution_count":113,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1042ef10755ec4b6f0262228dd2d2e12ddf797d5"},"cell_type":"code","source":"#Decision Tree looks the best\n#Train it on the full dataset and predict the test dataste\n# clf=DecisionTreeClassifier(max_depth=5)\nclf=SVC(kernel=\"linear\", C=0.025)\n# clf=GaussianNB()\n# clf=LogisticRegression()\n# clf=KNeighborsClassifier(3)\n\ny_train=train['Survived'].values\nx_train=train[feature_columns].values\n\nclf.fit(X=x_train,y=y_train)\n\ntest_df=df_all[ntrain:]\nx_test=test_df[feature_columns].values\n\n#Now predict our results\nresults=clf.predict(x_test)\n\n#Convert the results to int datatypes (real numbers)\nresults=[int(i) for i in results]\n\n#Get passenger id's from test set with the .iloc command\nresults_id=p_id.iloc[ntrain:].values\n\n#Create a dataframe for submission\nsubmission=pd.DataFrame({'PassengerId':results_id,'Survived':results})\n\n#Check what the submission looks like\ndisplay(submission.head(10))\n\n#Save the dataFrame as a .csv (save to Kaggle)\nsubmission.to_csv('submisison.csv',index=False)","execution_count":114,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"cff08f94d629e7beb9db4f2a866068b96023b3dd"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}