{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "807f80a2-4f3a-9c80-05a0-354bdab7bcdf"
      },
      "source": [
        "## 1. Import Libraries\n",
        "First of all, import the necessary libraries and set the working directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7f57d41f-f49a-6f05-65b7-44bc21fc8d42"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re as re\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.pylab as pylab\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import preprocessing\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# Define working directory\n",
        "#working_directory = 'D:/Kaggle/1. Titanic - Machine Learning from Disaster/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d3855384-1234-5014-4f2b-07684e324965"
      },
      "source": [
        "## 2. Load Data\n",
        "Then, load the training and test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "06e83484-cdb6-7c5f-22e5-6ee155994dc4"
      },
      "outputs": [],
      "source": [
        "########### Load Data ##########  \n",
        "\n",
        "# Method to load data from csv file\n",
        "def load_data():\n",
        "    #train = pd.read_csv(\"\".join((working_directory, 'data/train.csv')))\n",
        "    #test = pd.read_csv(\"\".join((working_directory, 'data/test.csv')))\n",
        "    train = pd.read_csv('../input/train.csv')\n",
        "    test = pd.read_csv('../input/test.csv')\n",
        "    return train, test\n",
        "                                       \n",
        "train, test = load_data()\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d4c074a3-95f8-ea63-8022-8f05d5283e1e"
      },
      "source": [
        "## 3. Pre-processing Data\n",
        "\n",
        "In this section, I will go through the features in the dataset and do some pre-processing on them (like dealing with missing value, create new features based on existing features, etc.) before proceed to building the classification model.\n",
        "\n",
        "###   3.1 Process \"Name\"\n",
        "\n",
        "First of all, the \"Name\" feature. At the first glance, this seems to be quite an insignificant feature as the names of the passengers have nothing much to do with their survival chances. But if we try to look at the hidden contents in the \"Name\" feature, we can actually find out some useful information from the \"Title\" and \"Surname\".\n",
        "\n",
        "The \"Title\" contains some information about the passengers' sex, age, occupation and even social status. All of these will have some influence on the survival chances of the passengers. On the other hand, \"Surname\" can be used to group the passengers in different families, and intuitively, if a passenger's family members have a very high survival rate, this passenger is more likely to be also survived.\n",
        "\n",
        "So first of all, let's extract the \"Title\" feature from the \"Name\", and try to replace some rare titles with a common title based on occupation and social status. Doing this will help reduce the sparsity of this feature. (In this notebook, I am not going to process the \"Surname\" as I don't have a very convincing idea yet how to process it.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c10c4ae8-ba8c-7ec8-1af3-297654dd68e3"
      },
      "outputs": [],
      "source": [
        "### Title ###\n",
        "\n",
        "# Get the titles from the names\n",
        "train['Title'] = train.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
        "test['Title'] = test.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
        "\n",
        "# Replace the rare titles with a more common title or assign a new title \"Officer\" or \"Royal\"\n",
        "full = [train, test]\n",
        "for dataset in full:\n",
        "    dataset['Title'] = dataset['Title'].replace(['Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev'], 'Officer')\n",
        "    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Sir', 'Jonkheer', 'Dona'], 'Royal')\n",
        "    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n",
        "    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n",
        "    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n",
        "\n",
        "# Get the average survival rate of different titles\n",
        "train[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "2e2c5662-19cf-0a2b-f860-f5da3f1513ad"
      },
      "source": [
        "## 3.2 Process \"Age\"\n",
        "Secondly, I will process the \"Age\" feature. As people tend to lend a helping hand to young children in this kind of disastrous situation, and the age also to a certain extent reflects the passengers' physical condition, this feature should contain some of the useful information related to survival chance.\n",
        "In this section, I will first try to fill up the missing ages in the data by a random age within one sigma of the average ages of different titles. After that, I will discretise the ages into 8 groups."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "6d940f0e-dfb2-5465-2828-0fc69ddd5b00"
      },
      "outputs": [],
      "source": [
        "### Age ###\n",
        "\n",
        "# Get the mean and standard deviation of the ages group by title\n",
        "title_age = pd.DataFrame(np.concatenate((train[['Title', 'Age']], test[['Title', 'Age']]))) # Concatenate train and test data\n",
        "title_age.columns = ['Title', 'Age']\n",
        "title_age = title_age.dropna(axis = 0)\n",
        "title_age['Age'] = title_age['Age'].astype(int)\n",
        "# Calculate the mean and standard deviation\n",
        "avg_age = title_age[['Title', 'Age']].groupby('Title', as_index=False).mean()\n",
        "std_age = title_age[['Title', 'Age']].groupby('Title', as_index=False)['Age'].apply(lambda x : x.std())\n",
        "avg_std_age = pd.concat([avg_age, std_age], axis=1)\n",
        "avg_std_age.columns = ['Title', 'Age', 'Std']\n",
        "# Calculate the one sigma boundary ((mean - 1 std) and (mean + 1 std))\n",
        "avg_std_age['Low'] = avg_std_age['Age'] - avg_std_age['Std']\n",
        "avg_std_age['High'] = avg_std_age['Age'] + avg_std_age['Std']\n",
        "\n",
        "# Fill missing ages using random ages within 1 standard deviation boundary of different titles\n",
        "for index, row in avg_std_age.iterrows():\n",
        "    count_nan_train = train[\"Age\"][train['Title'] == row['Title']].isnull().sum()\n",
        "    count_nan_test = test[\"Age\"][test['Title'] == row['Title']].isnull().sum()\n",
        "    train.loc[(np.isnan(train['Age'])) & (train['Title'] == row['Title']), 'Age'] = np.random.randint(row['Low'], row['High'], size = count_nan_train)\n",
        "    test.loc[(np.isnan(test['Age'])) & (test['Title'] == row['Title']), 'Age'] = np.random.randint(row['Low'], row['High'], size = count_nan_test)\n",
        "\n",
        "# Convert the data type to integer\n",
        "train['Age'] = train['Age'].astype(int)\n",
        "test['Age'] = test['Age'].astype(int)\n",
        "\n",
        "# Summarise and visualise the total number of passengers by age\n",
        "total_count = train[['Age', 'Survived']].groupby(['Age'],as_index=False).count()\n",
        "fig, axis1 = plt.subplots(1,1,figsize=(16,5))\n",
        "sns.barplot(x='Age', y='Survived', data=total_count, ax=axis1)\n",
        "axis1.set(xlabel='Age', ylabel='Number of Passenger', title='Number of passenegers by age')\n",
        "\n",
        "# Summarise and visualise the average survived passengers by age\n",
        "average_survival = train[['Age', 'Survived']].groupby(['Age'],as_index=False).mean()\n",
        "fig, axis2 = plt.subplots(1,1,figsize=(16,5))\n",
        "sns.barplot(x='Age', y='Survived', data=average_survival, ax=axis2)\n",
        "axis2.set(xlabel='Age', ylabel='Survival Rate', title='Average survival probability by age\"')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "05bb4b1d-a561-5b1b-2c43-f85581ba1b40"
      },
      "source": [
        "The age of the passengers range from 0 to 80 which is very sparse given the total data size. From the survival probability of different ages, it also seems that young children have relatively higher survival rate and on the contrary, elderly have much lower survival rate. It would be a good idea to discretise the 'Age' into a few categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7667f969-109c-7210-d99f-effa0ff00ec0"
      },
      "outputs": [],
      "source": [
        "# Investigate the survival probability after grouping the age into N categories (Here I use N=8)\n",
        "num_bin = 8\n",
        "train['AgeBin'] = pd.cut(train['Age'], num_bin)\n",
        "train[['AgeBin', 'Survived']].groupby(['AgeBin'], as_index=False).mean().sort_values(by='Survived', ascending=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8cbe2e77-a02f-a7f6-2063-ae5f0281c2a0"
      },
      "outputs": [],
      "source": [
        "# Group the age into N categories and assign integer values to each age category\n",
        "\n",
        "#max_age = max([train['Age'].max(), test['Age'].max()])\n",
        "#for i in range(0, num_bin):\n",
        "#    train.loc[(train['Age'] > max_age / num_bin * i) & (train['Age'] <= max_age / num_bin * (i+1)), 'AgeGroup'] = i\n",
        "#    test.loc[(test['Age'] > max_age / num_bin * i) & (test['Age'] <= max_age / num_bin * (i+1)), 'AgeGroup'] = i\n",
        "    \n",
        "full = [train, test]\n",
        "for dataset in full:\n",
        "    dataset.loc[(train['Age'] <= 10), 'AgeGroup'] = 1\n",
        "    dataset.loc[(train['Age'] > 50) & (train['Age'] <= 60), 'AgeGroup'] = 2\n",
        "    dataset.loc[(train['Age'] > 30) & (train['Age'] <= 40), 'AgeGroup'] = 3\n",
        "    dataset.loc[(train['Age'] > 40) & (train['Age'] <= 50), 'AgeGroup'] = 4\n",
        "    dataset.loc[(train['Age'] > 10) & (train['Age'] <= 20), 'AgeGroup'] = 5\n",
        "    dataset.loc[(train['Age'] > 20) & (train['Age'] <= 30), 'AgeGroup'] = 6\n",
        "    dataset.loc[(train['Age'] > 70) & (train['Age'] <= 80), 'AgeGroup'] = 7\n",
        "    dataset.loc[(train['Age'] > 60) & (train['Age'] <= 70), 'AgeGroup'] = 8\n",
        "    \n",
        "# Convert to integer type\n",
        "train['AgeGroup'] = train['AgeGroup'].astype(int)\n",
        "test['AgeGroup'] = test['AgeGroup'].astype(int)\n",
        "\n",
        "# Remove the AgeBin column\n",
        "train = train.drop(['AgeBin'], axis=1)# Investigate the survival probability after grouping the age into N categories (Here I use N=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "d2a4993b-fe10-a0ec-aa1d-22f246b72278"
      },
      "source": [
        "## 3.3 Process \"Sex\"\n",
        "Next, we investigate the \"Sex\" column.\n",
        "In this dataset, female passengers have much higher survival rate than male. However, male passengers whose age group fall in category 1 (younger than 10 years old) have higher survival rate compared to other age groups, and it is close to the survival rate of female passengers in category 1. In this case, I add a new feature \"IsChild\" to indicate whether the passenger is a child. (In fact this \"IsChild\" feature may not be necessary because this piece of information has more or less been reflected in the \"AgeGroup\" feature.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c0e90a42-04c9-f3ce-b76a-810aeaccf177"
      },
      "outputs": [],
      "source": [
        "### Sex ###\n",
        "\n",
        "# Invastigate how sex is correlated to the survival probability\n",
        "train[['Sex', 'Survived']].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "89f3dbe1-1afc-f42b-ca20-fbf4b2bfbbac"
      },
      "outputs": [],
      "source": [
        "# Invastigate how sex and age category is correlated to the survival probability\n",
        "train[['Sex', 'AgeGroup', 'Survived']].groupby(['Sex','AgeGroup'], as_index=False).mean().sort_values(by='Survived', ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "da0b7236-d720-339e-99bd-94a014b020ba"
      },
      "outputs": [],
      "source": [
        "# Create new feature \"IsChild\"\n",
        "train['IsChild'] = 0\n",
        "test['IsChild'] = 0\n",
        "train.loc[train['Age'] <= 10, 'IsChild'] = 1\n",
        "test.loc[test['Age'] <= 10, 'IsChild'] = 1\n",
        "\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "6ee0a45d-31f3-08a0-45ca-0846d878e50b"
      },
      "source": [
        "## 3.4 Process \"Embarked\"\n",
        "It seems that the different \"Embarked\" ports are also having some small effects on the survival chance of the passengers. Honestly I cannot understand why it is so intuitively, but since the different \"Embarked\"s make different average survival rates, I will keep this feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "470a4839-febe-6929-2141-1c5327903164"
      },
      "outputs": [],
      "source": [
        "### Embarked ###\n",
        "\n",
        "# Count the number of missing value in \"Embarked\" column\n",
        "count_nan_embarked_train = train[\"Embarked\"].isnull().sum() # count_nan_embarked_train = 2\n",
        "count_nan_embarked_test = test[\"Embarked\"].isnull().sum()   # count_nan_embarked_test = 0\n",
        "\n",
        "# Get the most common port from the data\n",
        "freq_port_train = train.Embarked.dropna().mode()[0]\n",
        " \n",
        "# Fill the missing values of the \"Embarked\" column with the most common port in the datasets\n",
        "train['Embarked'] = train['Embarked'].fillna(freq_port_train)\n",
        "\n",
        "train[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).agg(['mean', 'std'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "747328ab-1661-b860-3dd3-ca09c218a627"
      },
      "source": [
        "## 3.5 Process \"SibSp\" and \"Parch\"\n",
        "The \"SibSp\" and \"Parch\" features reflect the passengers' family size. Here I create a new feature \"FamilySize\" from these 2 features.\n",
        "And then from the average survival rates of different family sizes, we can see that small families (family size between 2 and 4) have relatively higher survival chance. And big families with 5 or more family members have lower survival chance. Therefore, I create another new feature \"FamilySizeGroup\" to group the family size into \"Alone\", \"Small\" and \"Big\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "423aef40-fa68-6b97-5868-ab248c5ded0e"
      },
      "outputs": [],
      "source": [
        "### SibSp and Parch ###\n",
        "\n",
        "# Create a new column \"FamilySize\" by adding the passenger with the number of his/her relatives aboard\n",
        "full = [train, test]\n",
        "for dataset in full:\n",
        "    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n",
        "\n",
        "train[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "77e54053-f585-342e-dcff-539105362447"
      },
      "outputs": [],
      "source": [
        "# Create new column \"FamilySizeGroup\" and assign \"Alone\", \"Small\" and \"Big\"\n",
        "for dataset in full:\n",
        "    dataset['FamilySizeGroup'] = 'Small'\n",
        "    dataset.loc[dataset['FamilySize'] == 1, 'FamilySizeGroup'] = 'Alone'\n",
        "    dataset.loc[dataset['FamilySize'] >= 5, 'FamilySizeGroup'] = 'Big'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "5b7860ee-a9a3-f966-4866-501749bae551"
      },
      "source": [
        "## 3.6 Process \"Cabin\" and \"Ticket\"\n",
        "The \"Cabin\" feature has too many missing values. It will generate noise to the data if i were to fill the missing values with the median \"Cabin\" value. So I decided to drop this feature.\n",
        "Most of the values in the \"Ticket\" feature are just random ticket numbers. It seems to contain very little information. So I will drop this feature too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1b7f0b5a-1866-ee3a-cc90-c0cd3b53024a"
      },
      "outputs": [],
      "source": [
        "### Cabin and Ticket ###\n",
        "\n",
        "# Count the number of missing value in \"Cabin\" column\n",
        "count_nan_cabin_train = train[\"Cabin\"].isnull().sum() # count_nan_cabin_train = 687\n",
        "count_nan_cabin_test = test[\"Cabin\"].isnull().sum()   # count_nan_cabin_test = 327\n",
        "\n",
        "# Drop the \"Cabin\" column as there are more than half of data has missing values \n",
        "# Drop the \"ticket\" column as this feature seems totally random\n",
        "for dataset in full:\n",
        "    dataset = dataset.drop(['Cabin','Ticket'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "38958930-168f-f896-ad2a-e1eac10aae7b"
      },
      "source": [
        "## 3.7 Process \"Fare\" and \"Pclass\"\n",
        "Intuitively, the fare should be highly correlated to the ticket class. And these 2 features should more or less reflect the passengers' wealth and social status of the passengers and hence will likely to be correlated to their survival rates. So here I try to analyse and process these 2 columns together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ba284a15-b112-d9a2-5933-11159ba3d48e"
      },
      "outputs": [],
      "source": [
        "### Fare and Pclass ###\n",
        "\n",
        "# Summarise the fare of different ticket classes\n",
        "train[['Pclass', 'Fare']].groupby(['Pclass'], as_index=False).agg(['mean', 'median', 'std', 'min', 'max'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "166106f5-aa9f-dfa0-d509-834247037ded"
      },
      "outputs": [],
      "source": [
        "# Further investigate the survival chance for passengers with zero ticket fare\n",
        "train.loc[(train.Fare == 0), ['Survived', 'Fare']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "e5e0f329-23a7-6c85-d13e-d1899896db40"
      },
      "source": [
        "From the statistics above, there are nil fare values in all the 3 ticket classes and the standard deviation of the class 1 ticket fare is very high. However, most of the zero fare records are having low survival chance, it seems to be a good indicator (may be just by chance) for the survival rate. So I decide to keep the nil fare records."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "1566c42f-80fe-a9f2-86d6-30e8f0ecf60c"
      },
      "outputs": [],
      "source": [
        "# Get average, std, and number of NaN ages in training data\n",
        "average_fare_train   = train[\"Fare\"].mean()\n",
        "std_fare_train       = train[\"Fare\"].std()\n",
        "count_nan_fare_train = train[\"Fare\"].isnull().sum() # count_nan_fare_train = 0\n",
        "count_nil_fare_train = (train[\"Fare\"] == 0).sum()   # count_nil_fare_train = 15\n",
        "\n",
        "# Get average, std, and number of NaN ages in test data\n",
        "average_fare_test   = test[\"Fare\"].mean()\n",
        "std_fare_test       = test[\"Fare\"].std()\n",
        "count_nan_fare_test = test[\"Fare\"].isnull().sum()  # count_nan_fare_test = 1\n",
        "count_nil_fare_test = (test[\"Fare\"] == 0).sum()    # count_nil_fare_test = 2\n",
        "\n",
        "# Fill the missing fare values with the median of the respective ticket class\n",
        "for dataset in full:\n",
        "    dataset.loc[dataset.Fare.isnull(), 'Fare'] = dataset.groupby('Pclass').Fare.transform('median')\n",
        "    #dataset.loc[(dataset.Fare == 0), 'Fare'] = dataset.groupby('Pclass').Fare.transform('median')    \n",
        "\n",
        "train[['Pclass', 'Fare']].groupby(['Pclass'], as_index=False).agg(['mean', 'median', 'std', 'min', 'max'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "bba7f0cc-fa92-4ecf-626c-85eea6404557"
      },
      "outputs": [],
      "source": [
        "# Create new feature \"FareGroup\" based on the median fare of different \"Pclass\"\n",
        "for dataset in full:\n",
        "    dataset.loc[ dataset['Fare'] <= 8.05, 'FareGroup'] = 0\n",
        "    dataset.loc[(dataset['Fare'] > 8.05) & (dataset['Fare'] <= 14.25), 'FareGroup'] = 1\n",
        "    dataset.loc[(dataset['Fare'] > 14.25) & (dataset['Fare'] <= 60.2875), 'FareGroup']   = 2\n",
        "    dataset.loc[ dataset['Fare'] > 60.2875, 'FareGroup'] = 3\n",
        "    \n",
        "    dataset['Fare'] = dataset['Fare'].astype(int)\n",
        "    dataset['FareGroup'] = dataset['FareGroup'].astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "c3c9bbb5-d4ed-2afd-1b1c-aa8311f3f9c4"
      },
      "source": [
        "## 3.8 Map Features\n",
        "Up until this step, all the features in the dataset have been processed. Next, I will map the Categorical features to nominal data type so that they can be processed by the classifiers that can only takes numerical data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "eaef637f-0827-fabd-371d-2a10d8d93cc0"
      },
      "outputs": [],
      "source": [
        "title_mapping = {\"Mr\": 0, \"Officer\": 1, \"Master\": 2, \"Miss\": 3, \"Royal\": 4, \"Mrs\": 5}\n",
        "sex_mapping = {\"female\": 0, \"male\": 1}\n",
        "embarked_mapping = {\"S\": 0, \"Q\": 1, \"C\": 2}\n",
        "family_mapping = {\"Small\": 0, \"Alone\": 1, \"Big\": 2}\n",
        "for dataset in full:\n",
        "    dataset['Title'] = dataset['Title'].map(title_mapping)\n",
        "    dataset['Sex'] = dataset['Sex'].map(sex_mapping)\n",
        "    dataset['Embarked'] = dataset['Embarked'].map(embarked_mapping)\n",
        "    dataset['FamilySizeGroup'] = dataset['FamilySizeGroup'].map(family_mapping)\n",
        "\n",
        "    \n",
        "train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "da140440-03c5-63ac-0f6a-95678aa0e544"
      },
      "source": [
        "## 3.9 Drop features\n",
        "Finally, evaluate the correlation between the survival chance and each of the feature. And drop the features that have low correlation with the survival chance. By doing so may help reduce the data noise and increase the generalisation of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "995f7d52-b31e-617a-87fb-70b38d8c14d4"
      },
      "outputs": [],
      "source": [
        "# Investigate the correlation of all the features with the \"Survived\"\n",
        "train.corr()['Survived']\n",
        "\n",
        "# Drop features in both train and test dataset\n",
        "for dataset in full:\n",
        "    dataset.drop('Name', axis=1, inplace=True)\n",
        "    dataset.drop('FamilySize', axis=1, inplace=True)\n",
        "    dataset.drop(['SibSp','Parch'], axis=1, inplace=True)     \n",
        "    dataset.drop('Age', axis=1, inplace=True) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b54d23da-2198-7df1-58f6-da8d63dcc63a"
      },
      "source": [
        "## 4. Building Models\n",
        "Now, prepare the training and testing data and do the normalisation.\n",
        "After that, fit the training data to different classification models and validate the accuracy using 5-fold cross validation.\n",
        "After the models are trained, compare their cross validation accuracies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "4769e23e-6f80-375f-9b0f-04b36ba21754"
      },
      "outputs": [],
      "source": [
        "# Prepare training and testing data\n",
        "X_train = train.drop(['Survived', 'PassengerId'], axis=1)\n",
        "Y_train = train['Survived']\n",
        "X_test  = test.drop('PassengerId', axis=1).copy()\n",
        "X_train.shape, Y_train.shape, X_test.shape\n",
        "\n",
        "# Standardise/Normalise the data\n",
        "scaler = preprocessing.StandardScaler().fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define variable for k-fold cross validation\n",
        "k = 5\n",
        "\n",
        "# Method to calculate training and k-fold accuracies\n",
        "def calculate_accuracy(clf, X, Y, k):\n",
        "    # Calculate training accuracy\n",
        "    train_score = clf.score(X, Y)\n",
        "    print(\"Training Accuracy: %0.2f\" % train_score)\n",
        "\n",
        "    # Calculate 5-fold cross validation accuracy\n",
        "    scores = cross_val_score(clf, X, Y, cv=k)\n",
        "    cv_score = scores.mean()\n",
        "    print(\"5-Fold CV Accuracy: %0.2f (+/- %0.2f)\" % (cv_score, scores.std() * 2))\n",
        "    \n",
        "    # Return the accuracies\n",
        "    return train_score, cv_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c22a0338-a853-d4d4-3578-476846469838"
      },
      "outputs": [],
      "source": [
        "### Logistic Regression ###\n",
        "\n",
        "# Build model and predict the test data\n",
        "lr = LogisticRegression()\n",
        "lr.fit(X_train, Y_train)\n",
        "Y_pred_lr = lr.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "lr_train_score, lr_cv_score = calculate_accuracy(lr, X_train, Y_train, k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "90fc527d-d3b7-7b11-68b6-e771864e6d4e"
      },
      "outputs": [],
      "source": [
        "### SVM (RBF Kernel)###\n",
        "\n",
        "# Build model and predict the test data\n",
        "svc = SVC(kernel='rbf')\n",
        "svc.fit(X_train, Y_train)\n",
        "Y_pred_svc = svc.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "svc_train_score, svc_cv_score = calculate_accuracy(svc, X_train, Y_train, k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "982f0f01-ec7b-e816-c175-81bb6c46de96"
      },
      "outputs": [],
      "source": [
        "### KNN ###\n",
        "\n",
        "# Build model and predict the test data\n",
        "knn = KNeighborsClassifier(n_neighbors = 11)\n",
        "knn.fit(X_train, Y_train)\n",
        "Y_pred_knn = knn.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "knn_train_score, knn_cv_score = calculate_accuracy(knn, X_train, Y_train, k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e3f336d7-b968-3cb5-8508-04b7814766df"
      },
      "outputs": [],
      "source": [
        "### Gaussian Naive Bayes ###\n",
        "\n",
        "# Build model and predict the test data\n",
        "nb = GaussianNB()\n",
        "nb.fit(X_train, Y_train)\n",
        "Y_pred_nb = nb.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "nb_train_score, nb_cv_score = calculate_accuracy(nb, X_train, Y_train, k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "3c4aef44-fe54-cf42-9359-fb3ea06695e9"
      },
      "outputs": [],
      "source": [
        "### Multi Layer Perceptron ###\n",
        "\n",
        "# Build model and predict the test data\n",
        "mlp = MLPClassifier(solver='lbfgs', activation='logistic', alpha=1e-5, max_iter=1000,\n",
        "                    hidden_layer_sizes=(5,3), learning_rate='adaptive',random_state=1)\n",
        "mlp.fit(X_train, Y_train)\n",
        "Y_pred_mlp = mlp.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "mlp_train_score, mlp_cv_score = calculate_accuracy(mlp, X_train, Y_train, k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "973f4573-228b-ef6b-1aee-694ce50575c6"
      },
      "outputs": [],
      "source": [
        "### Stochastic Gradient Descent ###\n",
        "\n",
        "# Build model and predict the test data\n",
        "sgd = SGDClassifier()\n",
        "sgd.fit(X_train, Y_train)\n",
        "Y_pred_sgd = sgd.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "sgd_train_score, sgd_cv_score = calculate_accuracy(sgd, X_train, Y_train, k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "825266d1-505c-4d7f-520b-49bf40ef9618"
      },
      "outputs": [],
      "source": [
        "### Decision Tree ###\n",
        "\n",
        "# Build model and predict the test data\n",
        "dt = DecisionTreeClassifier()\n",
        "dt.fit(X_train, Y_train)\n",
        "Y_pred_dt = dt.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "dt_train_score, dt_cv_score = calculate_accuracy(dt, X_train, Y_train, k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8cd43d8e-ca1c-5d4f-4b80-7cbbd6588f48"
      },
      "outputs": [],
      "source": [
        "### Random Forest ###\n",
        "\n",
        "# Build model and predict the test data\n",
        "rf = RandomForestClassifier(n_estimators=100)\n",
        "rf.fit(X_train, Y_train)\n",
        "Y_pred_rf = rf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "rf_train_score, rf_cv_score = calculate_accuracy(rf, X_train, Y_train, k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "7b490294-31e7-e87e-37f3-af0da54c9678"
      },
      "outputs": [],
      "source": [
        "summary = pd.DataFrame({\n",
        "    \"Model\": [\"Support Vector Machines\", \"KNN\", \"Logistic Regression\", \n",
        "              \"Random Forest\", \"Naive Bayes\", \"Multi Layer Perceptron\", \n",
        "              \"Stochastic Gradient Decent\", \"Decision Tree\"],\n",
        "    \"Training Score\": [svc_train_score, knn_train_score, lr_train_score, \n",
        "              rf_train_score, nb_train_score, mlp_train_score, \n",
        "              sgd_train_score, dt_train_score],\n",
        "    \"CV Score\": [svc_cv_score, knn_cv_score, lr_cv_score, \n",
        "              rf_cv_score, nb_cv_score, mlp_cv_score, \n",
        "              sgd_cv_score, dt_cv_score]})\n",
        "summary.sort_values(by=\"CV Score\", ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "49743ba3-d9be-e70f-3f0e-e997c7965b59"
      },
      "source": [
        "## 5. Prepare Test Result Submission \n",
        "\n",
        "Finally, prepare the test result using the best model for submission. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "c6f89a1f-1350-0e51-5446-df6d47845663"
      },
      "outputs": [],
      "source": [
        "# Prepare prediction result using svc result\n",
        "prediction_result = pd.DataFrame({\n",
        "        \"PassengerId\": test[\"PassengerId\"],\n",
        "        \"Survived\": Y_pred_svc\n",
        "    })\n",
        "\n",
        "# Save to csv file\n",
        "#prediction_result.to_csv(\"\".join((working_directory, 'output/submission_svc.csv')), index=False)"
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}