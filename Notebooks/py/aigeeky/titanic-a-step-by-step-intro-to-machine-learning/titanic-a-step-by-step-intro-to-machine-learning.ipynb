{"cells":[{"metadata":{"_uuid":"e919d1161f20999e599ba1fd66a5a45b9c82f229","_cell_guid":"bc64948f-5d6a-078d-085d-1beb58687bd3"},"cell_type":"markdown","source":"# Table of content\n\n1. Introduction - Loading libraries and dataset\n2. Exploration, engineering and cleaning features (or variables)\n3. Correlaration analysis - Tri-variate analysis\n4. Prediction models with cross-validation\n5. Stacking predictions\n\n**Note:** Ever feel burnt out? Missing a deeper meaning? Sometimes life gets off-balance, but with the right steps, we can find the personal path to authentic happiness and balance.\n[Check out how Machine Learning and statistical analysis](https://www.amazon.com/dp/B07BNRRP7J?ref_=cm_sw_r_kb_dp_TZzTAbQND85EE&tag=kpembed-20&linkCode=kpe) sift through 10,000 responses to help us define our unique path to better living.\n\n# 1. Introduction - Loading libraries and dataset\nI created this Python notebook as the learning notes of my first Machine Learning project.\nSo many new terms, new functions, new approaches, but the subject really interested me; so I dived into it, studied one line of code at a time, and captured the references and explanations in this notebook.\n\nThe algorithm itself is a fork from **Anisotropic's Introduction to Ensembling/Stacking in Python**, a great notebook in itself.\nHis notebook was itself based on **Faron's \"Stacking Starter\"**, as well as **Sina's Best Working Classfier**. \nI also used multiple functions from **Yassine Ghouzam**.\nI added many variations and additional features to improve the code (as much as I could) as well as additional visualization.\n\nBut most importantly, you can use this kernel as a first project to practice your ML/Python skills.\nI welcome comments , questions and additional feedback.\n\nYvon\n\n## 1.1. Importing Library"},{"metadata":{"_execution_state":"idle","_uuid":"2e37a274400cfeb472b6405d524325245588dd66","_cell_guid":"14630296-b1aa-759e-bafa-b6a73f3896ed","collapsed":true,"_kg_hide-input":true,"_kg_hide-output":true,"trusted":false},"cell_type":"code","source":"# Load libraries for analysis and visualization\nimport pandas as pd # collection of functions for data processing and analysis modeled after R dataframes with SQL like features\nimport numpy as np  # foundational package for scientific computing\nimport re           # Regular expression operations\nimport matplotlib.pyplot as plt # Collection of functions for scientific and publication-ready visualization\n%matplotlib inline\nimport plotly.offline as py     # Open source library for composing, editing, and sharing interactive data visualization \nfrom matplotlib import pyplot\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nfrom collections import Counter\n\n# Machine learning libraries\nimport xgboost as xgb  # Implementation of gradient boosted decision trees designed for speed and performance that is dominative competitive machine learning\nimport seaborn as sns  # Visualization library based on matplotlib, provides interface for drawing attractive statistical graphics\n\nimport sklearn         # Collection of machine learning algorithms\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n                              GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier)\nfrom sklearn.cross_validation import KFold\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8b590aafe06a2ac55daae9d2456155e457914f5f","_cell_guid":"d647b74c-099b-851a-dcd2-3a58c9e8f10c"},"cell_type":"markdown","source":"## 1.2. Loading dataset"},{"metadata":{"_execution_state":"idle","_uuid":"b2ad78041b69ce13d1f41bd9bc8c93cafaf7b8ac","_cell_guid":"5937fd72-d1ad-f678-cc82-f08a96e4cad0","collapsed":true,"trusted":false},"cell_type":"code","source":"# Load in the train and test datasets from the CSV files\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\n# Store our passenger ID for easy access\nPassengerId = test['PassengerId']\n\n# Display the first 5 rows of the dataset, a first look at our data\n# 5 first row, 5 sample rows and basic statistics\ntrain.head(5)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"5b8c4fe8903fbc4c364cc13ef112990e7c8fb0fe","_cell_guid":"b478c2ac-01ee-4f6f-ab5f-9f38e15574f5","trusted":false},"cell_type":"code","source":"train.sample(5)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"d1cc8b9ae2df4b559f2e3ac1b5f082632d634ece","_cell_guid":"69b8d571-e89c-492b-9314-31b329f63524","trusted":false},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"83baed9972334c9971d82212def66fe8f9671d73","_cell_guid":"8f7edf22-e10f-4504-9642-6c43cbdb7f7e"},"cell_type":"markdown","source":"**What are the data types for each feature?**\n* Survived: int\n* Pclass: int\n* Name: string\n* Sex: string\n* Age: float\n* SibSp: int\n* Parch: int\n* Ticket: string\n* Fare: float\n* Cabin: string\n* Embarked: string"},{"metadata":{"_uuid":"f5243d4a9bed6a1d5424bd60375a0318331f3060","_cell_guid":"2ca2f211-5ee3-4d16-a370-a7de6ececa9b"},"cell_type":"markdown","source":"## 1.3. Analysis goal\n**The Survived variable** is the outcome or dependent variable. It is a binary nominal datatype of 1 for \"survived\" and 0 for \"did not survive\".\n**All other variables** are potential predictor or independent variables. The goal is to predict this dependent variable only using the available independent variables. A test dataset has been created to test our algorithm."},{"metadata":{"_uuid":"81378834770e55c76751347588322fe32acf5737","_cell_guid":"dff1c1dd-1e59-3907-88fa-1a1d699122be"},"cell_type":"markdown","source":"# 2. Exploratory analysis: cleaning and engineering features\n\nWe will start with a standard approach of any kernel: correct, complete, engineer the right features for analysis.\n\n## 2.1. Correcting and completing features\n### Detecting and correcting outliers\nReviewing the data, there does not appear to be any aberrant or non-acceptable data inputs.\n\nThere are potential outliers that we will identify and remove with the following method (steps from Yassine Ghouzam):\n* It creates firset a function called detect_outliers, implementing the Tukey method\n* For each column of the dataframe, this function calculates the 25th percentile (Q1) and 75th percentile (Q3) values.\n* The  interquartile range (IQR) is a measure of statistical dispersion, being equal to the difference between the 75th and 25th percentiles, or between upper and lower quartiles.\n* Any data points outside 1.5 time the IQR (1.5 time IQR below Q1, or 1.5 time IQR above Q3), is considered an outlier.\n* The outlier_list_col column captures the indices of these outliers. All outlier data get then pulled into the outlier_indices dataframe.\n* Finally, the detect_outliers function will select only the outliers happening multiple times. This is the datadframe that will be returned.\n"},{"metadata":{"collapsed":true,"_uuid":"5abddbbf118113d33ed95f10cf73edbd4525df82","_cell_guid":"98ca04be-4693-4eb7-b25b-cc32e65b45b9","trusted":false},"cell_type":"code","source":"# Outlier detection \ndef detect_outliers(df,n,features):\n    outlier_indices = []\n    # iterate over features(columns)\n    for col in features:\n        # 1st quartile (25%)\n        Q1 = np.percentile(df[col],25)\n        # 3rd quartile (75%)\n        Q3 = np.percentile(df[col],75)\n        # Interquartile range (IQR)\n        IQR = Q3 - Q1\n        # outlier step\n        outlier_step = 1.5 * IQR\n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index       \n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n        \n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    return multiple_outliers   \n# detect outliers from Age, SibSp , Parch and Fare\nOutliers_to_drop = detect_outliers(train,2,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"])\n\ntrain.loc[Outliers_to_drop] # Show the outliers rows","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6cf77f7bdf1d38ca620d102e75ce54ed759e7af0","_cell_guid":"4d70580d-8bcb-434c-8630-8bcb7aada36b"},"cell_type":"markdown","source":"** Observations**\n* The Detect_Outliers function found 10 outliers.\n* PassengerID 28, 89 and 342 passenger have an high Ticket Fare\n* The seven others have very high values of SibSP.\n* We will drop those."},{"metadata":{"collapsed":true,"_uuid":"1cb4a4a0d63f644850356d29521119025f95a257","_cell_guid":"f0acac5f-79ba-4c1a-aab1-f6416f963cc7","trusted":false},"cell_type":"code","source":"# Drop outliers\ntrain = train.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6c82c0056c3492ae9a4f91fac98fb40f7048b252","_cell_guid":"3fa8b862-a11f-48f5-b87a-c9bd7114382a"},"cell_type":"markdown","source":"### Completing features\nThe .info function below shows how complete or incomplete the datasets are.\nThere are null values or missing data in the age, cabin, and embarked field. Missing values can be bad, because some algorithms don't know how-to handle null values and will fail. While others, like decision trees, can handle null values.\n\nThe approach to to complete missing data is to impute using mean, median, or mean + randomized standard deviation. \nWe will do this in section 2.2 with the  **fillna** function:  dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())"},{"metadata":{"collapsed":true,"_uuid":"3b87698ec3ae2a12fc189a570f8871fd82053245","_cell_guid":"3cc01b03-52b8-4c55-9dae-ed4c396d8601","trusted":false},"cell_type":"code","source":"train.info()\nprint('_'*40)\ntest.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c1a3ee40ac0c78fb443276741e8f71229d14850f","_cell_guid":"6188601d-4426-49c1-8048-dfa05e143889"},"cell_type":"markdown","source":"## 2.2. Descriptive analysis (univariate) "},{"metadata":{"collapsed":true,"_uuid":"01ad05b95eb87e1d0a179f476339f5dc232501d7","_cell_guid":"6948be07-4fa1-456e-876f-e171390303e7","trusted":false},"cell_type":"code","source":"full_data = [train, test]\nSurvival = train['Survived']\nSurvival.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"69581914493afab72108ad3e7fcf3185cbb435e1","_cell_guid":"1409f12a-e327-47ee-a6d2-980b65ccf2c1"},"cell_type":"markdown","source":"## 2.3 Feature Engineering - Bi-variate statistical analysis\n\nOne of the first tasks in Data Analytics is to **convert the variables into numerical/ordinal values**.\nThere are multiple types of data\n\n**a) Qualitative data: discrete**\n* Nominal: no natural order between categories. In this case: Name\n* Categorical: Sex\n\n**b) Numeric or quantitative data**\n* Discrete: could be ordinal like Pclass or not like Survived.\n* Continuous. e.g.: age\nMany feature engineering steps were taken from Anisotropic's excellent kernel.\n\n### Pclass"},{"metadata":{"collapsed":true,"_uuid":"4451a474e5ce6cd18261e29277eff3fcd0a9754d","_cell_guid":"3c566817-c659-4ec7-a05b-c868d02d13c9","trusted":false},"cell_type":"code","source":"sns.barplot(x=\"Embarked\", y=\"Survived\", hue=\"Sex\", data=train);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4defc9757a1c64545d410bdbfd59f43f926e772a","_cell_guid":"af55be9f-804f-443d-b089-dfcbc3c63736"},"cell_type":"markdown","source":"### Name_length"},{"metadata":{"collapsed":true,"_uuid":"7561e22789e3f2e4e2a3ae3b529d496be0999dcb","_cell_guid":"2f7e788b-5101-4297-9375-dcb8f83375ae","trusted":false},"cell_type":"code","source":"for dataset in full_data:\n    dataset['Name_length'] = train['Name'].apply(len)\n    # Qcut is a quantile based discretization function to autimatically create categories\n    # dataset['Name_length'] = pd.qcut(dataset['Name_length'], 6, labels=False)\n    # train['Name_length'].value_counts()\n\nsum_Name = train[[\"Name_length\", \"Survived\"]].groupby(['Name_length'],as_index=False).sum()\naverage_Name = train[[\"Name_length\", \"Survived\"]].groupby(['Name_length'],as_index=False).mean()\nfig, (axis1,axis2,axis3) = plt.subplots(3,1,figsize=(18,6))\nsns.barplot(x='Name_length', y='Survived', data=sum_Name, ax = axis1)\nsns.barplot(x='Name_length', y='Survived', data=average_Name, ax = axis2)\nsns.pointplot(x = 'Name_length', y = 'Survived', data=train, ax = axis3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d3b31cd1cb45d6daccca97e90f6216354b0752cf","_cell_guid":"a084b418-5ece-45be-a71f-75dca192764d"},"cell_type":"markdown","source":"The first graph shows the amount of people by Name_length.\n\nThe second one, their average survival rates.\n\nThe proposed categories are: less than 23 (mostly men), 24 to 28, 29 to 40, 41 and more (mostly women)"},{"metadata":{"collapsed":true,"_uuid":"2c10630f38727745f89cdc3f9ef1e089eee490a6","_cell_guid":"31274424-f19d-4fab-a041-9ad2129da7d3","trusted":false},"cell_type":"code","source":"for dataset in full_data:\n    dataset.loc[ dataset['Name_length'] <= 23, 'Name_length'] \t\t\t\t\t\t            = 0\n    dataset.loc[(dataset['Name_length'] > 23) & (dataset['Name_length'] <= 28), 'Name_length']  = 1\n    dataset.loc[(dataset['Name_length'] > 28) & (dataset['Name_length'] <= 40), 'Name_length']  = 2\n    dataset.loc[ dataset['Name_length'] > 40, 'Name_length'] \t\t\t\t\t\t\t        = 3\ntrain['Name_length'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ef01d1f465ac8d9f15e20807a71756ef5e606cdf","_cell_guid":"ba18ae99-1947-4429-aeb1-035e8c5d5695"},"cell_type":"markdown","source":"### Gender (Sex)"},{"metadata":{"collapsed":true,"_uuid":"53345f625d3cdc5d515fa619327bf24fdaade750","_cell_guid":"faba1caa-0ab6-45a6-aa0f-a6b270e3964c","trusted":false},"cell_type":"code","source":"for dataset in full_data:# Mapping Gender\n    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9eed1dd267dd07d20edc4c730a1502471891d5be","_cell_guid":"2decad09-0ef2-405e-a7a6-0674cdb72f52"},"cell_type":"markdown","source":"### Age"},{"metadata":{"collapsed":true,"_uuid":"4906ec9f7249077e67e873d6ec6eeb7d1f073b50","_cell_guid":"6b817f31-bbfe-4cd1-ad18-cdbb403c2c09","trusted":false},"cell_type":"code","source":"#plot distributions of age of passengers who survived or did not survive\na = sns.FacetGrid( train, hue = 'Survived', aspect=6 )\na.map(sns.kdeplot, 'Age', shade= True )\na.set(xlim=(0 , train['Age'].max()))\na.add_legend()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c47f9c6044a024d2f1cff9ae2cd3281e95d32add","_cell_guid":"c6d7004d-a67c-48ca-be93-f6c320710c83"},"cell_type":"markdown","source":"The best categories for age are:\n* 0:  Less than 14\n* 1:  14 to 30\n* 2:  30 to 40\n* 3:  40 to 50\n* 4:  50 to 60\n* 5:  60 and more"},{"metadata":{"collapsed":true,"_uuid":"ba2a81859462878076a50e05cfd4cae890175eb0","_cell_guid":"f305dc89-836e-4968-820c-9b898d14b934","trusted":false},"cell_type":"code","source":"for dataset in full_data:\n    age_avg = dataset['Age'].mean()\n    age_std = dataset['Age'].std()\n    age_null_count = dataset['Age'].isnull().sum()\n    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n    dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\n    dataset['Age'] = dataset['Age'].astype(int)\n# Qcut is a quantile based discretization function to autimatically create categories (not used here)\n# dataset['Age'] = pd.qcut(dataset['Age'], 6, labels=False)\n# Using categories as defined above\n    dataset.loc[ dataset['Age'] <= 14, 'Age'] \t\t\t\t\t\t          = 0\n    dataset.loc[(dataset['Age'] > 14) & (dataset['Age'] <= 30), 'Age']        = 1\n    dataset.loc[(dataset['Age'] > 30) & (dataset['Age'] <= 40), 'Age']        = 2\n    dataset.loc[(dataset['Age'] > 40) & (dataset['Age'] <= 50), 'Age']        = 3\n    dataset.loc[(dataset['Age'] > 50) & (dataset['Age'] <= 60), 'Age']        = 4\n    dataset.loc[ dataset['Age'] > 60, 'Age'] \t\t\t\t\t\t\t      = 5\ntrain['Age'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d861ce6213c63c6f241d3441845e07c3cfdbc86d","_cell_guid":"3315b27c-eba6-45ce-82ab-958d137a5df6"},"cell_type":"markdown","source":"### Family: SibSp and Parch"},{"metadata":{"collapsed":true,"_uuid":"2afc6cb8fc4ac8aa64e7a48444fc22206c4f2906","_cell_guid":"8be96fda-45ea-440c-a3ac-77f28a414fb8","trusted":false},"cell_type":"code","source":"for dataset in full_data:\n# Remove all NULLS in the Fare column and create a new feature Categorical Fare\n    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\n# Create new feature FamilySize as a combination of SibSp and Parch\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n# Create new feature IsAlone from FamilySize\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n    \n# Create new feature IsAlone from FamilySize\n    dataset['Boys'] = 0\n    dataset.loc[(dataset['Age'] == 0) & (dataset['Sex']==1), 'Boys'] = 1\n    \nfig, (axis1,axis2) = plt.subplots(1,2,figsize=(18,6))\nsns.barplot(x=\"FamilySize\", y=\"Survived\", hue=\"Sex\", data=train, ax = axis1);\nsns.barplot(x=\"IsAlone\", y=\"Survived\", hue=\"Sex\", data=train, ax = axis2);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9177a25e4aafaa52f0d7cdb7244d2ecab6a3a300","_cell_guid":"32f0292d-91ab-4455-a7db-16c95c8d8e26"},"cell_type":"markdown","source":"### Fare"},{"metadata":{"collapsed":true,"_uuid":"83226906c0e1598b54b49de588407007087e4f89","_cell_guid":"c84ce842-21f8-4d32-97f5-03e7fbb659a2","trusted":false},"cell_type":"code","source":"# Explore Fare distribution \ng = sns.distplot(dataset[\"Fare\"], color=\"m\", label=\"Skewness : %.2f\"%(dataset[\"Fare\"].skew()))\ng = g.legend(loc=\"best\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1aa0af52793efa3a5cd86f8a6e63a9c6e17636e6","_cell_guid":"54bb83d0-ce33-473b-a3ee-9ef7e9b802e3"},"cell_type":"markdown","source":"**Observations**\n* The Fare distribution is very skewed to the left. This can lead to overweigthing the model with very high values.\n* In this case, it is better to transform it with the log function to reduce the skewness and redistribute the data."},{"metadata":{"collapsed":true,"_uuid":"2e249c1203e6616e86ea41b0120c5e60e1a4ee4c","_cell_guid":"57516840-ecae-48b8-88f0-b4ecd44ccf6a","trusted":false},"cell_type":"code","source":"# Apply log to Fare to reduce skewness distribution\nfor dataset in full_data:\n    dataset[\"Fare\"] = dataset[\"Fare\"].map(lambda i: np.log(i) if i > 0 else 0)\na4_dims = (20, 6)\nfig, ax = pyplot.subplots(figsize=a4_dims)\ng = sns.distplot(train[\"Fare\"][train[\"Survived\"] == 0], color=\"r\", label=\"Skewness : %.2f\"%(train[\"Fare\"].skew()), ax=ax)\ng = sns.distplot(train[\"Fare\"][train[\"Survived\"] == 1], color=\"b\", label=\"Skewness : %.2f\"%(train[\"Fare\"].skew()))\n#g = g.legend(loc=\"best\")\ng = g.legend([\"Not Survived\",\"Survived\"])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9f85e56b9bc0719071d4954ecfa21f576ba387d9","_cell_guid":"4e42f446-91c9-4789-a7d3-3eaa58ca0eb2"},"cell_type":"markdown","source":"**Observations**\nLog Fare categories are:\n* 0 to 2.7: less survivors\n* More than 2.7 more survivors"},{"metadata":{"collapsed":true,"_uuid":"f932674fd50af20a7962cc48173ca084d9592731","_cell_guid":"4bb510c8-ae73-44c2-b3dd-b8582c99cd2d","trusted":false},"cell_type":"code","source":"for dataset in full_data:\n    dataset.loc[ dataset['Fare'] <= 2.7, 'Fare'] \t\t\t\t\t\t      = 0\n#    dataset.loc[(dataset['Fare'] > 2.7) & (dataset['Fare'] <= 3.2), 'Fare']   = 1\n#    dataset.loc[(dataset['Fare'] > 3.2) & (dataset['Fare'] <= 3.6), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 2.7, 'Fare'] \t\t\t\t\t\t\t  = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\ntrain['Fare'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7b340889434f276651ebc8eabef131a83bf33164","_cell_guid":"73b94ea9-1425-482a-bc80-fd3913d351ce"},"cell_type":"markdown","source":"### Cabin"},{"metadata":{"collapsed":true,"_uuid":"69bdfde97b9fb7392f257640d3713831afc02470","_cell_guid":"c9853a1b-3760-4336-a7fc-c1d79a788dd1","trusted":false},"cell_type":"code","source":"# Feature that tells whether a passenger had a cabin on the Titanic (O if no cabin number, 1 otherwise)\nfor dataset in full_data:\n    dataset['Has_Cabin'] = dataset[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n\ntrain[[\"Has_Cabin\", \"Survived\"]].groupby(['Has_Cabin'], as_index=False).sum().sort_values(by='Survived', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"a80942192868d63a901ada08b1b6b29d2f540862","_cell_guid":"2c91ef14-b77b-40b7-9ae7-ec1f1a1af791","trusted":false},"cell_type":"code","source":"train[[\"Has_Cabin\", \"Survived\"]].groupby(['Has_Cabin'], as_index=False).mean().sort_values(by='Survived', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"06bcc944b62d3b7a9374bf8abd2bc75593c19075","_cell_guid":"d05f03cc-2cf1-4c97-bc5f-950d5e0daf4b"},"cell_type":"markdown","source":"### Embarked"},{"metadata":{"collapsed":true,"_uuid":"10ae2b5348f8da830360198a7a00429cfe683721","_cell_guid":"8e320857-ed76-4c25-a87e-ffba6701e17a","trusted":false},"cell_type":"code","source":"for dataset in full_data:\n# Remove all NULLS in the Embarked column\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n# Mapping Embarked\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n    \ntrain_pivot = pd.pivot_table(train, values= 'Survived',index=['Embarked'],columns='Pclass',aggfunc=np.mean, margins=True)\ndef color_negative_red(val):\n    # Takes a scalar and returns a string with the css property 'color: red' if below 0.4, black otherwise.\n    color = 'red' if val < 0.4 else 'black'\n    return 'color: %s' % color\ntrain_pivot = train_pivot.style.applymap(color_negative_red)\ntrain_pivot","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df1b2bc0562960d6b5de46223e77a4a2461b568f","_cell_guid":"2a110592-a7c2-4cec-83cc-c992927611f3"},"cell_type":"markdown","source":"Irrespective of the class, passengers embarked in 0 (S) and 2 (Q) have lower chance of survival. I will combine those into the first category."},{"metadata":{"collapsed":true,"_uuid":"d5a381a5a14fd7cfa7697d783868f4e3b3b05f4d","_cell_guid":"06aa2b9f-4291-43ba-9acf-638be8f7ef5c","trusted":false},"cell_type":"code","source":"dataset['Embarked'] = dataset['Embarked'].replace(['0', '2'], '0')\ntrain['Fare'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f8082a689fd95a5c8643ce1746b13a89318f50e9","_cell_guid":"d0e80ec4-b741-4c84-91b9-b9d3272977f5"},"cell_type":"markdown","source":"### Titles"},{"metadata":{"collapsed":true,"_uuid":"545cdddcf6561fcd180e07d8c51b10993bd7b13a","_cell_guid":"55bdf2af-3432-45c2-89d2-38c5852f5f3a","trusted":false},"cell_type":"code","source":"# Define function to extract titles from passenger names\ndef get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n # If the title exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    return \"\"\nfor dataset in full_data:\n# Create a new feature Title, containing the titles of passenger names\n    dataset['Title'] = dataset['Name'].apply(get_title)\n\nfig, (axis1) = plt.subplots(1,figsize=(18,6))\nsns.barplot(x=\"Title\", y=\"Survived\", data=train, ax=axis1);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ce583b77f1d8af8f9784c1d332e696ff94bcde4e","_cell_guid":"d004379b-af09-4c09-85be-1523ea984e74"},"cell_type":"markdown","source":"There are 4 types of titles:\n0. Mme, Ms, Lady, Sir, Mlle, Countess: 100%. \n1. Mrs, Miss: around 70% survival\n2. Master: around 60%\n3. Don, Rev, Capt, Jonkheer: no data\n4. Dr, Major, Col: around 40%\n5. Mr: below 20%"},{"metadata":{"collapsed":true,"_uuid":"c030d8811ff9c10554b64451f13920d0afa5fbfc","_cell_guid":"a042e575-102a-4a5d-b0dd-822d3ea390c5","trusted":false},"cell_type":"code","source":"for dataset in full_data:\n    dataset['Title'] = dataset['Title'].replace(['Mrs', 'Miss'], 'MM')\n    dataset['Title'] = dataset['Title'].replace(['Dr', 'Major', 'Col'], 'DMC')\n    dataset['Title'] = dataset['Title'].replace(['Don', 'Rev', 'Capt', 'Jonkheer'],'DRCJ')\n    dataset['Title'] = dataset['Title'].replace(['Mme', 'Ms', 'Lady', 'Sir', 'Mlle', 'Countess'],'MMLSMC' )\n# Mapping titles\n    title_mapping = {\"MM\": 1, \"Master\":2, \"Mr\": 5, \"DMC\": 4, \"DRCJ\": 3, \"MMLSMC\": 0}\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(3)\n    \n# Explore Age vs Survived\ng = sns.FacetGrid(train, col='Survived')\ng = g.map(sns.distplot, \"Age\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f0962278ecad96763bd57279ab27e4b2908be45e","_cell_guid":"1414cb27-994d-4147-bd4b-037d7c93b6e4"},"cell_type":"markdown","source":"### Extracting deck from cabin\nA cabin number looks like ‘C123’ and the letter refers to the deck: a big thanks to Nikas Donge.\nTherefore we’re going to extract these and create a new feature, that contains a persons deck. Afterwords we will convert the feature into a numeric variable. The missing values will be converted to zero."},{"metadata":{"collapsed":true,"_uuid":"501ccfdb8e80e7f17cad906c1df68a5fbcaad1e0","_cell_guid":"cc6c3f84-55d9-46d4-b5d4-6155017b5758","trusted":false},"cell_type":"code","source":"deck = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7, \"U\": 8}\nfor dataset in full_data:\n    dataset['Cabin'] = dataset['Cabin'].fillna(\"U0\")\n    dataset['Deck'] = dataset['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\n    dataset['Deck'] = dataset['Deck'].map(deck)\n    dataset['Deck'] = dataset['Deck'].fillna(0)\n    dataset['Deck'] = dataset['Deck'].astype(int) \ntrain['Deck'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"fd4d281901a8d460bbdb6d0fdb7fae65906287ed","_cell_guid":"aba7fa4f-867a-4caf-9185-27a079f49b7d","trusted":false},"cell_type":"code","source":"sns.barplot(x = 'Deck', y = 'Survived', order=[1,2,3,4,5,6,7,8], data=train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"df0bf2763c869182380da8611b2a3aca2a83fa83","_cell_guid":"0105edb5-86c2-4f4c-af82-f3c55908e211"},"cell_type":"markdown","source":"3 types of deck: 1 with 15 passengers, 2 to 6, and 7 to 8 (most passengers)"},{"metadata":{"collapsed":true,"_uuid":"9e3df054a01706ac8980a1f673281e8996f74c46","_cell_guid":"c36e2a86-29cc-4c2a-aeb5-1d9de7efa4d2","trusted":false},"cell_type":"code","source":"for dataset in full_data:\n    dataset.loc[ dataset['Deck'] <= 1, 'Deck'] = 1\n    dataset.loc[(dataset['Deck'] > 1) & (dataset['Deck'] <= 6), 'Deck']  = 2\n    dataset.loc[ dataset['Deck'] > 6, 'Deck'] = 3\ntrain['Deck'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5d1c24e4a573361655895c80baca45481a7e3994","_cell_guid":"c51cad77-7519-48ee-8ce4-3bedce6efd80"},"cell_type":"markdown","source":"### 2.3.2. Removing unnecessary features"},{"metadata":{"collapsed":true,"_uuid":"d792004f61d78fb86478868a49da85882f8dcec0","_cell_guid":"e70f54bd-ca12-4b7b-8ae2-ad3a8b49f11f","trusted":false},"cell_type":"code","source":"# for dataset in full_data:\n#    dataset['Age_Class']= dataset['Age']* dataset['Pclass']\n# train['Age_Class'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"72b80572bc1da9966600df60f2a3c0406729ab34","_cell_guid":"a0b606af-a464-41de-ad34-9ba2277ffc27","trusted":false},"cell_type":"code","source":"# for dataset in full_data:\n#    dataset['Fare_Per_Person'] = dataset['Fare']/(dataset['FamilySize'])\n#    dataset['Fare_Per_Person'] = dataset['Fare_Per_Person'].astype(int)\n# train['Fare_Per_Person'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dd288776321804d99e4e4a7e88594c1d631e4409","_cell_guid":"e9814168-b7cd-d4e4-1b1d-e21c6637a663"},"cell_type":"markdown","source":"We now have clean features by replacing relevant categorical information with ordinal features.\n\n## 2.4 Visualising updated dataset"},{"metadata":{"collapsed":true,"_uuid":"5f8a57d9785370134f2488610dd81a84e9e45096","scrolled":false,"_cell_guid":"6bd36cde-7b08-42ed-b191-87e27b320fc5","trusted":false},"cell_type":"code","source":"test.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_execution_state":"idle","_uuid":"1f280a1c11dc35a93b57af494938998e6d0b4544","_cell_guid":"fc426b8f-873d-6f23-4299-99f174956cca","collapsed":true,"trusted":false},"cell_type":"code","source":"train.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"069695c07d229c23cb2edf99d6f5ad901b934e1d","_cell_guid":"009ce45c-f4db-4718-ba0c-9c3e34d0e8ee"},"cell_type":"markdown","source":"## 2.5. Descriptive statistics"},{"metadata":{"collapsed":true,"_uuid":"9c27c628974b2556f03b8bd54fb81771bce4cf25","scrolled":true,"_cell_guid":"522a767e-e466-4f8e-9255-039c391cded8","trusted":false},"cell_type":"code","source":"train.describe()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"bb72bebe9916f26581bd773823dbe4d410787ac9","_cell_guid":"25f0e07e-4d62-4f12-abb8-cfcf1ba0a95f","trusted":false},"cell_type":"code","source":"train[['Pclass', 'Sex', 'Age', 'Parch', 'Fare', 'Embarked', 'Has_Cabin', 'FamilySize', 'Title', 'Survived']].groupby(['Survived'], as_index=False).mean().sort_values(by='Pclass', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"991534cba51f0387c1cd8a8d5457c7ccd7bafd4b","_cell_guid":"6a0bde77-40d6-4b59-8e3c-7a08b219bd30"},"cell_type":"markdown","source":"**Initial observations from the descriptive statistics:**\n* Only 38% survived, a real tragedy :-(\n* Passengers in more expensive classes 1 and 2 had much higher chance of surviving than classes 3 or 4.\n* Also, the higher the fare, the higher the chance. Similarly, having a cabin increases the chance of survival.\n* Women (0) higher chance than men (1)\n* Younger people slightly more chance than older\n* Being alone decreased your chance to survive.\n\nWe will drop unncessary features just before Section 3.1. Pearson Correlation heatmap."},{"metadata":{"_uuid":"d3d1a1236728a132b0bc4e02f85da6e1a7ce2942","_cell_guid":"d51a4163-10c2-443e-bec2-0ec3b2a93b40"},"cell_type":"markdown","source":"# 3. Correlation analysis - Tri-variate analysis\nThis section summarizes  bivariate analysis asthe simplest forms of quantitative (statistical) analysis.\nIt involves the analysis of one or two features, and their relative impact of \"Survived\". \nThis is a useful frist step of our anblaysis in order to determine the empirical relationship between all features."},{"metadata":{"_uuid":"58a067cb3eb5edbccb6f9f20fab06c26c1d02da0","_cell_guid":"2ae4e483-0ae8-422d-abe5-71330366e0a3"},"cell_type":"markdown","source":"## 3.2. Correlation analysis with histograms and pivot-tables"},{"metadata":{"collapsed":true,"_uuid":"4a7d5a8d1e764a7f5a7cea45ca373488717382da","_cell_guid":"3e32fa2e-496f-47ab-a8a0-0a7277d0473f","trusted":false},"cell_type":"code","source":"fig, (axis1,axis2) = plt.subplots(1,2,figsize=(18,6))\n\nsns.barplot(x=\"Embarked\", y=\"Survived\", hue=\"Sex\", data=train, ax = axis1);\n#axis1.set_title('Pclass vs Fare Survival Comparison')\n\nsns.barplot(x=\"Age\", y=\"Survived\", hue=\"Sex\", data=train, ax = axis2);\n#axis2.set_title('Pclass vs Fare Survival Comparison')\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"de57c4821ae253395ad26821f6eaacfcde79801c","_cell_guid":"25e95e2f-5be3-4afe-b23f-47bc8fe80620"},"cell_type":"markdown","source":"**Observations for Age graph:**\n* 0 or blue represent women; 1 or orange represent men. Gender and age seem to have a stronger influece of the survival rate.\n* We start to find where most survivors are: older women (48 to 64 year old), and younger passengers.\n* What is statistically interesting is that only young boys (Age Category = 0) have  high survival rates, unlike other age groups for men.\n* We will create a new feature called young boys"},{"metadata":{"collapsed":true,"_uuid":"d513de669190fa5f4f77e1df27112e03299d4235","_cell_guid":"63f1405e-1616-432f-a573-8f838da729d4","trusted":false},"cell_type":"code","source":"# for dataset in full_data:\n#    dataset['Boys'] = 0\n#    dataset.loc[(dataset['Age'] == 0) & (dataset['Sex']==1), 'Boys'] = 1\n# dataset['Boys'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"cad712b4c8479d44b80790ea3d30f0e481e4b3a1","_cell_guid":"96ad7209-d261-4004-a696-e0747d831ea3","trusted":false},"cell_type":"code","source":"train[[\"FamilySize\", \"Survived\"]].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"97d135ceed4cbb2a5df1ae118dff34feb90d3930","_cell_guid":"1ad428d8-c429-40e1-bf63-f1322ab0378f","trusted":false},"cell_type":"code","source":"train_pivot = pd.pivot_table(train, values= 'Survived',index=['Title', 'Pclass'],columns='Sex',aggfunc=np.mean, margins=True)\ndef color_negative_red(val):\n    # Takes a scalar and returns a string with the css property 'color: red' if below 0.4, black otherwise.\n    color = 'red' if val < 0.4 else 'black'\n    return 'color: %s' % color\ntrain_pivot = train_pivot.style.applymap(color_negative_red)\ntrain_pivot","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"224a3f391434aa77da825c3984324be2c0b17335","_cell_guid":"f6f28a5b-fee9-4f70-bfb1-7f0f026de045","trusted":false},"cell_type":"code","source":"# grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')\ngrid = sns.FacetGrid(train, col='Survived', row='Pclass', size=2, aspect=3)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=8)\ngrid.add_legend();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0ae82877c2b4154eb3fe24c94028ee3ad6cef603","_cell_guid":"9585af36-9516-4067-8ba5-a290fea149df"},"cell_type":"markdown","source":"**Observations: here are the survivors!**\n1. Family-size of 3 or 4 from first pivot\n2. Women and men alone on first class (second pivot, red showing survival rate below 0.4)\n3. Top-right in the graph above: first class and age categories 1 and 2\n\n** The not-so lucky are mostly in men, Pclass 3 and age category 1 (younger folks)**"},{"metadata":{"collapsed":true,"_uuid":"a3b586bbc09c57c2ff16aed5ebf16c4188400b86","_cell_guid":"1388dfc6-3fed-4012-ae0e-d5663f8ca0dc","trusted":false},"cell_type":"code","source":"#graph distribution of qualitative data: Pclass\nfig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(18,8))\n\nsns.boxplot(x = 'Pclass', y = 'Fare', hue = 'Survived', data = train, ax = axis1)\naxis1.set_title('Pclass vs Fare Survival Comparison')\n\nsns.violinplot(x = 'Pclass', y = 'Age', hue = 'Survived', data = train, split = True, ax = axis2)\naxis2.set_title('Pclass vs Age Survival Comparison')\n\nsns.boxplot(x = 'Pclass', y ='FamilySize', hue = 'Survived', data = train, ax = axis3)\naxis3.set_title('Pclass vs Family Size Survival Comparison')","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"29ae992e23cde108c90ee8b0130352590bd507c4","_cell_guid":"c1b66928-c57e-430b-b6f3-6156eaeb82cc","trusted":false},"cell_type":"code","source":"fig, saxis = plt.subplots(2, 3,figsize=(18,8))\n\nsns.barplot(x = 'Embarked', y = 'Survived', data=train, ax = saxis[0,0])\nsns.barplot(x = 'Pclass', y = 'Survived', order=[1,2,3], data=train, ax = saxis[0,1])\nsns.barplot(x = 'Deck', y = 'Survived', order=[1,0], data=train, ax = saxis[0,2])\n\nsns.pointplot(x = 'Fare', y = 'Survived',  data=train, ax = saxis[1,0])\nsns.pointplot(x = 'Age', y = 'Survived',  data=train, ax = saxis[1,1])\nsns.pointplot(x = 'FamilySize', y = 'Survived', data=train, ax = saxis[1,2])","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"479bc09cced41f18fa04843b6cad657de698cb53","_cell_guid":"5723fb3e-1bfc-435a-9e5f-0586378712d6","trusted":false},"cell_type":"code","source":"# grid = sns.FacetGrid(train_df, col='Embarked')\ngrid = sns.FacetGrid(train, row='Has_Cabin', size=2.2, aspect=1.2)\ngrid.map(sns.pointplot, 'Parch', 'Survived', 'Sex', palette='deep')\ngrid.add_legend()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"46df6a79557425480e5121b7a3ecae97778ab2bb","_cell_guid":"f921f497-9142-44a8-9c75-0cda20c18c35"},"cell_type":"markdown","source":"**Observations:**\n* The colors represent: blue=0 is for women, green=1 for men\n* Clearly, women had more chance of surviving, with or without cabin\n* Interesting is that accompanied women without a cabin had less survival chance than women alone without cabin.\n    But this is not true for men. Men alone have less chance than accompanied.\n    \n    **Bottom-line: it would have been better for women without cabin to pretend that they were alone.\n    And lone men should join a family to improve their survival rates.**\n    \n    Bottom-line of the bi-variate and tri-variate analysis as well as the feature importance analysis (from running the classifiers multiple times), I decided to drop the following features:"},{"metadata":{"collapsed":true,"_uuid":"bf9305cbadc75baad3cefec40ca0da934cfe94a7","_cell_guid":"a6f00634-34d5-408b-8e5e-e45102483a62","trusted":false},"cell_type":"code","source":"# Feature selection\ndrop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp', 'Boys', 'IsAlone', 'Embarked', 'Has_Cabin']\ntrain = train.drop(drop_elements, axis = 1)\ntest  = test.drop(drop_elements, axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f41988e138dff50dcb8d619ea01f39d5d1da6072","_cell_guid":"f4a88f82-63d4-4e40-82f2-b508c9337438"},"cell_type":"markdown","source":"## 3.1. Pearson Correlation Heatmap\n\nThe Seaborn plotting package allows us to plot heatmaps showing the Pearson product-moment correlation coefficient (PPMCC) correlation between features.\nPearson is bivariate correlation, measuring the linear correlation between two features. "},{"metadata":{"collapsed":true,"_uuid":"7c73bf7277d92eff2c85a30184b4547fc2bbf3b5","_cell_guid":"53b4f1cb-489f-4b86-9764-eaec11d3525c","trusted":false},"cell_type":"code","source":"colormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b6c7a1ea3866d177016a1e13c5d9e7341c49c147","_cell_guid":"ccc92745-0680-df4d-d709-10003475d8e3"},"cell_type":"markdown","source":"**Observations from the Pearson analysis:** \n* Correlation coefficients with magnitude between 0.5 and 0.7 indicate variables which can be considered **moderately correlated**.\n* We can see from the red cells that many features are \"moderately\" correlated: specifically, IsAlone, Pclass, Name_length, Fare, Sex.\n* This is influenced by the following two factors: 1) Women versus men (and the compounding effect of Name_length) and 2) Passengers paying a high price (Fare) have a higher chance of survival: there are also in first class, have a title. \n\n\n## 3.3. Pairplots\n\nFinally let us generate some pairplots to observe the distribution of data from one feature to the other.\nThe Seaborn pairplot class will help us visualize the distribution of a feature in relationship to each others."},{"metadata":{"_execution_state":"idle","_uuid":"624446543aafd518025fd3f5346d32ee1aab6f9a","_cell_guid":"ea6b0a8f-5a33-666f-8057-c0d689f370f5","collapsed":true,"trusted":false},"cell_type":"code","source":"g = sns.pairplot(train[[u'Survived', u'Pclass', u'Sex', u'Age', u'Parch', u'Fare',\n       u'FamilySize', u'Title', u'Name_length', u'Deck']], hue='Survived', palette = 'seismic',size=1.2,diag_kind = 'kde',diag_kws=dict(shade=True),plot_kws=dict(s=10) )\ng.set(xticklabels=[])","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"4ccf0eb696b2ae00be51777dee3f75f32cd19ad2","_cell_guid":"18bd73d2-b436-4500-af02-59274fa97358","trusted":false},"cell_type":"code","source":"# X_train (all features for training purpose but excluding Survived)\n# Y_train (survival result of X-Train)\n# and test are our 3 main datasets for the next sections\nX_train = train.drop(\"Survived\", axis=1)\nY_train = train[\"Survived\"]\nX_train.shape, Y_train.shape, test.shape","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5336a714f678d54e161172ef22827489d1a86a83","_cell_guid":"3eb2dad3-31d7-4367-a0ab-881729c6e8e8"},"cell_type":"markdown","source":"# 4. Predictive models\nWe will first apply the following nine algorithms to the entire three datasets listed above.\n* Logistic Regression\n* KNN or k-Nearest Neighbors\n* Support Vector Machines\n* Naive Bayes classifier\n* Decision Tree\n* Random Forrest\n* Perceptron\n* Artificial neural network\n* RVM or Relevance Vector Machine\n\n## 4.1. Logistic Regression\nLogistic regression measures the relationship between the categorical dependent feature (in our case Survived) and the other independent features.\nIt estimates probabilities using a cumulative logistic distribution:\n* The first value shows the accuracy of this model\n* The table after this shows the importance of each feature according this classifier."},{"metadata":{"collapsed":true,"_uuid":"1e8259699d46e14e005b9cb104b4d68848a6d8de","_cell_guid":"d64bab2c-7654-4d0e-a2d3-77103e80e50f","trusted":false},"cell_type":"code","source":"logreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict(test)\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nacc_log","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"25f194716b1252b1f8c38e4bafc125b508ff400f","_cell_guid":"5768d5f9-4d74-4396-a493-64f99e1fba0e","trusted":false},"cell_type":"code","source":"coeff_df = pd.DataFrame(X_train.columns.delete(0))\ncoeff_df.columns = ['Feature']\ncoeff_df[\"Correlation\"] = pd.Series(logreg.coef_[0])\ncoeff_df.sort_values(by='Correlation', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2e3d6369eb83be933e874845ee87610be012a1cb","_cell_guid":"537434f6-8d86-4bab-9828-a92909c14fe9"},"cell_type":"markdown","source":"**Observation:**\n* This classfier confirms the importance of Name_length\n* FamilySize did not show a strong Pearson correlation with Survived but comes here to the top. This can be due to its strong relationship with other features such as Is_Alone or Parch (Parent-Children).\n\n\n## 4.2. Support Vector Machines (supervised)\nGiven a set of training samples, each sample is marked as belonging to one or the other of two categories.\n\nThe SVM training algorithm builds a model that assigns new test samples to one category or the other, making it a non-probabilistic binary linear classifier."},{"metadata":{"collapsed":true,"_uuid":"79bdd20f5eb3ef2685384aeea7fe6c9140442163","_cell_guid":"0f01682c-4a1d-414c-8905-76897a32ce13","trusted":false},"cell_type":"code","source":"svc=SVC()\nsvc.fit(X_train, Y_train)\nY_pred = svc.predict(test)\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\nacc_svc","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"be87e04341b24917d87761256e67304c19412cc6","_cell_guid":"a9c6eaa4-bad7-4c48-9d6b-3e02e655a183"},"cell_type":"markdown","source":"## 4.3. k-Nearest Neighbors algorithm (k-NN)\nThis is a non-parametric method used for classification and regression.\nA sample is classified by a majority vote of its neighbors, with the sample being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor. "},{"metadata":{"collapsed":true,"_uuid":"0baf00ea77739487b08035892793c9983c6141d8","_cell_guid":"3e21e1c1-abb6-4c00-a6aa-ce293e0d9ca4","trusted":false},"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, Y_train)\nY_pred = knn.predict(test)\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\nacc_knn","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"691639deb7d9cc3a5200a38ccf45c45204b694a1","_cell_guid":"08739336-522e-4b96-a51d-b32ca035937b"},"cell_type":"markdown","source":"## 4.4. Naive Bayes classifier\nThis is a family of simple probabilistic classifiers based on applying Bayes' theorem with strong (naive) independence assumptions between the features. Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of features in a learning problem."},{"metadata":{"collapsed":true,"_uuid":"f8338165241ba93eb873903e0bf02fe603c5bfb3","_cell_guid":"4d363cdb-9f1d-4e43-9750-38b82971f223","trusted":false},"cell_type":"code","source":"gaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\nY_pred = gaussian.predict(test)\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\nacc_gaussian","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e32fab307ef58b8e00bcf2f956ed50094a4bb5c9","_cell_guid":"940ba7a6-109d-447c-8f0f-35d95d751ee7"},"cell_type":"markdown","source":"## 4.5. Perceptron\nThis is an algorithm for supervised learning of binary classifiers: like the other classifiers before, it decides whether an input, represented by a vector of numbers, belongs to some specific class or not. It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector. The algorithm allows for online learning, in that it processes elements in the training set one at a time."},{"metadata":{"collapsed":true,"_uuid":"74a727ebe8bd1a1ee7ea25840754e94af455ee2c","_cell_guid":"e1548c66-6c08-4f0a-a8a4-79962ede6cb1","trusted":false},"cell_type":"code","source":"perceptron = Perceptron()\nperceptron.fit(X_train, Y_train)\nY_pred = perceptron.predict(test)\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\nacc_perceptron","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2a9436f61d14962f8516df1fc1a18b1a3a72c135","_cell_guid":"b9a23f92-0bbf-48c4-9ab6-15dbfe5116a7"},"cell_type":"markdown","source":"## 4.6. Linear SVC\nThis is another implementation of Support Vector Classification (similar to 4.2.) for the case of a linear kernel."},{"metadata":{"collapsed":true,"_uuid":"17e366e5f4eb069cbfdbad90453b6e76f05acdef","_cell_guid":"60611a3f-fd51-4229-acaf-c133ca2e9847","trusted":false},"cell_type":"code","source":"linear_svc = LinearSVC()\nlinear_svc.fit(X_train, Y_train)\nY_pred = linear_svc.predict(test)\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\nacc_linear_svc","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5fce77fff72d983e6e079a9a4a878f200c8249c5","_cell_guid":"244579fa-56f2-4bb6-8b1f-22f4ff061da9"},"cell_type":"markdown","source":"## 4.7. Stochastic Gradient Descent (sgd)\nThis is a stochastic approximation of the gradient descent optimization and iterative method for minimizing an objective function that is written as a sum of differentiable functions. In other words, SGD tries to find minima or maxima by iteration."},{"metadata":{"collapsed":true,"_uuid":"a6ec7ea6c3b3a749ac38e9d54aa3571b14952322","_cell_guid":"2b46b161-4da8-4652-a250-6173618b8f10","trusted":false},"cell_type":"code","source":"sgd = SGDClassifier()\nsgd.fit(X_train, Y_train)\nY_pred = sgd.predict(test)\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\nacc_sgd","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e5ff90c38f7a875bdc8cc78e3a56204759d6d088","_cell_guid":"56d4b334-9d9b-461a-bcc8-e249647222a2"},"cell_type":"markdown","source":"## 4.8. Decision tree\nThis predictive model  maps features (tree branches) to conclusions about the target value (tree leaves).\n\nThe target features  take a finite set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees."},{"metadata":{"collapsed":true,"_uuid":"94fa5a802bfd7692d3f9fb3f10a77840bedc2807","_cell_guid":"ff611104-8d18-40ea-a4d9-1cc6ae190312","trusted":false},"cell_type":"code","source":"decision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_pred = decision_tree.predict(test)\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nacc_decision_tree","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d2fb348dae0ae80f8d92a7cb352dc476c0df834e","_cell_guid":"d2250de2-014f-44fe-8d1e-6802dc4256c9"},"cell_type":"markdown","source":"## 4.9. Random Forests\nThis is one of the most popular classfier.\nRandom forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees (n_estimators=100) at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees"},{"metadata":{"collapsed":true,"_uuid":"c97d4e9bb57a147d8871ac55ede16c92f94f8937","_cell_guid":"a23b35cd-e90b-4414-b19a-aab033fc5197","trusted":false},"cell_type":"code","source":"random_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(test)\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nacc_random_forest","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a6c47798f6078f3af1e46714dd9ea2f3afd89662","_cell_guid":"af2231ef-5396-4dcc-9622-b134f75adcf3"},"cell_type":"markdown","source":"## 4.10. Model summary\nI found that the picture illustrates the various model better than words.\nThis should be taken with a grain of salt, as the intuition conveyed by these two-dimensional examples does not necessarily carry over to real datasets.\nThe reality os that the algorithms work with many dimensions (11 in our case).\n\nBut it shows how each classifier algorithm partitions the same data in different ways.\nThe three rows represent the three different data set on the right.\nThe plots show training points in solid colors and testing points semi-transparent. The lower right shows the classification accuracy on the test set.\n\nFor instance, the visualization helps understand how RandomForest uses multiple Decision Trees, the linear SVC, or Nearest Neighbors grouping sample by their relative distance to each others.\n\n![image](http://scikit-learn.org/0.15/_images/plot_classifier_comparison_0011.png)\n"},{"metadata":{"collapsed":true,"_uuid":"03ca3ec943ac68754945b897831809945cc49ba1","_cell_guid":"9235ce64-cec0-496f-a906-9d9fb11ac6eb","trusted":false},"cell_type":"code","source":"objects = ('Logistic Regression', 'SVC', 'KNN', 'Gaussian', 'Perceptron', 'linear SVC', 'SGD', 'Decision Tree', 'Random Forest')\nx_pos = np.arange(len(objects))\naccuracies1 = [acc_log, acc_svc, acc_knn, acc_gaussian, acc_perceptron, acc_linear_svc, acc_sgd, acc_decision_tree, acc_random_forest]\n    \nplt.bar(x_pos, accuracies1, align='center', alpha=0.5, color='r')\nplt.xticks(x_pos, objects, rotation='vertical')\nplt.ylabel('Accuracy')\nplt.title('Classifier Outcome')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9fa35c1dda5fe275477b1aac2a23b18ca29a0bbb","_cell_guid":"04d20d05-98ed-45f6-84d9-aa3e70e9d2af"},"cell_type":"markdown","source":"**Observations**\n* The above models (classifiers) were applied to the entire training dataset.\n* This results in some classifiers (Decision_tree and Random_Forest) over-fitting the model to the training data. \n* This happens when the classifiers use many input features (to include noise in each feature) on the complete dataset, and ends up “memorizing the noise” instead of finding the signal.\n* This overfit model will then make predictions based on that noise. It performs unusually well on its training data (97 above), but will not necessarilyimprove the prediction quality with new data from the test dataset.\n* We therefore need to cross-validate the models using sample data. We will therefore use StratifiedKFold to train and test the models on sample data from the overall dataset.\nStratified K-Folds is a cross validation iterator. It provides train/test indices to split data in train test sets. This cross-validation object is a variation of KFold, which returns stratified folds. The folds are made by preserving the percentage of samples for each class."},{"metadata":{"_uuid":"230e8659fb39cf8fdc6e0d531bf9a98c9674d84d","_cell_guid":"6f4eb75d-3718-4bf9-9bda-ca53fe8f3826"},"cell_type":"markdown","source":"## 4.11. Model cross-validation with K-Fold\n\nThe fitting process applied above optimizes the model parameters to make the model fit the training data as well as possible.\nCross-validation is a way to predict the fit of a model to a hypothetical validation set when an explicit validation set is not available.\nIn simple words, it alows to test how well the model performs on new data.\nIn our case, cross-validation wikll also be applied to compare the performances of different predictive modeling procedures. \n\n### 4.11.1. Cross-validation scores"},{"metadata":{"collapsed":true,"_uuid":"cd1a76ef5d8a7093e3d05ef0f244591e9ddafe8c","_cell_guid":"51a3d12e-6709-4c99-8499-e9065de25d21","trusted":false},"cell_type":"code","source":"# Cross validate model with Kfold stratified cross validation\nfrom sklearn.model_selection import StratifiedKFold\nkfold = StratifiedKFold(n_splits=10)\n# Modeling step Test differents algorithms \nrandom_state = 2\nclassifiers = []\nclassifiers.append(LogisticRegression(random_state = random_state))\nclassifiers.append(SVC(random_state=random_state))\nclassifiers.append(KNeighborsClassifier())\nclassifiers.append(GaussianNB())\nclassifiers.append(Perceptron(random_state=random_state))\nclassifiers.append(LinearSVC(random_state=random_state))\nclassifiers.append(SGDClassifier(random_state=random_state))\nclassifiers.append(DecisionTreeClassifier(random_state = random_state))\nclassifiers.append(RandomForestClassifier(random_state = random_state))\n\ncv_results = []\nfor classifier in classifiers :\n    cv_results.append(cross_val_score(classifier, X_train, y = Y_train, scoring = \"accuracy\", cv = kfold, n_jobs=4))\n\ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n\ncv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\"Algorithm\":['Logistic Regression',  'KNN', 'Gaussian',\n    'Perceptron', 'linear SVC', 'SGD', 'Decision Tree','SVMC', 'Random Forest']})\n\ng = sns.barplot(\"CrossValMeans\",\"Algorithm\",data = cv_res, palette=\"Set3\",orient = \"h\",**{'xerr':cv_std})\ng.set_xlabel(\"Mean Accuracy\")\ng = g.set_title(\"Cross validation scores\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4d268142781fde8a736992af97a30e606094c7a5","_cell_guid":"056a8fdb-8d59-452a-82b1-86e37e62ea5b"},"cell_type":"markdown","source":"## 4.12 Hyperparameter tuning & Learning curves for selected classifiers\n1. Adaboost\n2. ExtraTrees \n3. RandomForest\n4. GradientBoost\n5. SVC"},{"metadata":{"collapsed":true,"_uuid":"1de7046e7704ee29b4827b3bde077af98bedb4df","_cell_guid":"b0263595-bfd7-498c-8161-e76fbc2bdd28","trusted":false},"cell_type":"code","source":"# Adaboost\nDTC = DecisionTreeClassifier()\nadaDTC = AdaBoostClassifier(DTC, random_state=7)\nada_param_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n              \"algorithm\" : [\"SAMME\",\"SAMME.R\"],\n              \"n_estimators\" :[1,2],\n              \"learning_rate\":  [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3,1.5]}\ngsadaDTC = GridSearchCV(adaDTC,param_grid = ada_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\ngsadaDTC.fit(X_train,Y_train)\nadaDTC_best = gsadaDTC.best_estimator_\ngsadaDTC.best_score_","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"1f6162dde2e422d192470675a6e7fa5f8bc3fba4","_cell_guid":"b45f6aa5-d003-460e-b051-c412409e946f","trusted":false},"cell_type":"code","source":"#ExtraTrees \nExtC = ExtraTreesClassifier()\nex_param_grid = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 9],\n              \"min_samples_split\": [2, 3, 9],\n              \"min_samples_leaf\": [1, 3, 9],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\ngsExtC = GridSearchCV(ExtC,param_grid = ex_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\ngsExtC.fit(X_train,Y_train)\nExtC_best = gsExtC.best_estimator_\ngsExtC.best_score_","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"d0e880fe0a20338bd6ae7bfd756c192b2424a0a9","_cell_guid":"29ca52ff-55d8-4948-aaac-baf1b4be4bf2","trusted":false},"cell_type":"code","source":"# Gradient boosting tunning\nGBC = GradientBoostingClassifier()\ngb_param_grid = {'loss' : [\"deviance\"],\n              'n_estimators' : [100,200,300],\n              'learning_rate': [0.1, 0.05, 0.01],\n              'max_depth': [4, 8],\n              'min_samples_leaf': [100,150],\n              'max_features': [0.3, 0.1] }\ngsGBC = GridSearchCV(GBC,param_grid = gb_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\ngsGBC.fit(X_train,Y_train)\nGBC_best = gsGBC.best_estimator_\ngsGBC.best_score_","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"cd3db5d04fbcb9b2b672ddc672f008e61ef10af3","_cell_guid":"2f5ace1f-c5fd-4793-933a-4e0d1b78d68e","trusted":false},"cell_type":"code","source":"### SVC classifier\nSVMC = SVC(probability=True)\nsvc_param_grid = {'kernel': ['rbf'], \n                  'gamma': [ 0.001, 0.01, 0.1, 1],\n                  'C': [1, 10, 50, 100,200,300, 1000]}\ngsSVMC = GridSearchCV(SVMC,param_grid = svc_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\ngsSVMC.fit(X_train,Y_train)\nSVMC_best = gsSVMC.best_estimator_\n# Best score\ngsSVMC.best_score_","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"29f6e95093b5d3d5f54a6042ffe9a1ef2245af2a","scrolled":true,"_cell_guid":"f841ae86-1e02-4626-b1e3-f3087cfee304","trusted":false},"cell_type":"code","source":"# Random Forest\nrf_param_grid = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 9],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[300,600],\n              \"criterion\": [\"gini\"]}\n\ngsrandom_forest = GridSearchCV(random_forest,param_grid = rf_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\ngsrandom_forest.fit(X_train,Y_train)\n# Best score\nrandom_forest_best = gsrandom_forest.best_estimator_\ngsrandom_forest.best_score_","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"81070e0011002d63d313a322bd38636d23f48580","_cell_guid":"eb168d27-5841-4b06-a320-9309c980468e","trusted":false},"cell_type":"code","source":"def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n    plt.legend(loc=\"best\")\n    return plt\n\ng = plot_learning_curve(gsadaDTC.best_estimator_,\"AdaBoost learning curves\",X_train,Y_train,cv=kfold)\ng = plot_learning_curve(gsExtC.best_estimator_,\"ExtC ExtraTrees learning curves\",X_train,Y_train,cv=kfold)\ng = plot_learning_curve(gsGBC.best_estimator_,\"GBC Gradient Boost learning curves\",X_train,Y_train,cv=kfold)\ng = plot_learning_curve(gsrandom_forest.best_estimator_,\"RF learning curves\",X_train,Y_train,cv=kfold)\ng = plot_learning_curve(gsSVMC.best_estimator_,\"SVMC learning curves\",X_train,Y_train,cv=kfold)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ce75e068abae919bc0c8e8e0d88353d85290cd84","_cell_guid":"fc320444-1097-4bd7-995b-908065b188e9"},"cell_type":"markdown","source":"**Observations to fine-tune our models**\nFirst, let's compare their best score after fine-tuning their parameters:\n1. Adaboost: 80\n2. ExtraTrees: 83\n3. RandomForest: 82\n4. GradientBoost: 82\n5. SVC: 83\n\nIt appears that ExtraTree and SVC are doing the best job. This is good because we want to keep the model as close to the training data as possible. But not too close!\nThe two major sources of error are bias and variance; as we reduce these two, then we could build more accurate models:\n\n* **Bias**: The less biased a method, the greater its ability to fit data well.\n* **Variance**: with a lower bias comes typically a higher the variance. And therefore the risk that the model will not adapt accurately to new test data.\nThis is the case here with Gradient Boost: high score but cross-validation is very distant.\n\nThe reverse also holds: the greater the bias, the lower the variance. A high-bias method builds simplistic models that generally don't fit well training data. \nWe can see the red and green curves from ExtraTrees, RandomForest and SVC are pretty close.\n**This points to a lower variance, i.e. a stronger ability to apply the model to new data.**\n\n## 4.13 Selecting and combining the best classifiers\nSo, how do we achieve the best trade-off beween bias and variance?\n1. We will first compare in the next section the classifiers; results between themselves and applied to the same test data.\n2. Then \"ensemble\" them together with an automatic function callled *voting*."},{"metadata":{"collapsed":true,"_uuid":"26ccf158bbddbf74058fd2bddb6f54e5afc657c6","_cell_guid":"c9f5ad35-050a-4bb1-a3b6-bb88042b2e82","trusted":false},"cell_type":"code","source":"test_Survived_AdaDTC = pd.Series(adaDTC_best.predict(test), name=\"AdaDTC\")\ntest_Survived_ExtC = pd.Series(ExtC_best.predict(test), name=\"ExtC\")\ntest_Survived_GBC = pd.Series(GBC_best.predict(test), name=\"GBC\")\ntest_Survived_SVMC = pd.Series(SVMC_best.predict(test), name=\"SVMC\")\ntest_Survived_random_forest = pd.Series(random_forest_best.predict(test), name=\"random_forest\")\n\n\n# Concatenate all classifier results\nensemble_results = pd.concat([test_Survived_AdaDTC, test_Survived_ExtC, test_Survived_GBC,test_Survived_SVMC,test_Survived_random_forest],axis=1)\ng= sns.heatmap(ensemble_results.corr(),annot=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7f758f0ee9d19608100c393c5b7d0800346ddf05","_cell_guid":"bed2c50a-063f-45e7-b4c7-523f4a5af86d"},"cell_type":"markdown","source":"**Observations:**\n* As observed before, Adaboost has the lowest correlation to other predictors. This indicates that it does not predict differently than the others when it comes to the test data.\n* We will therefore 'ensemble' the remaining four predictors.\n\n## 4.14 Ensembling\nThis is the final step, pulling it together with an amazing 'Voting' function from sklearn.\nThe last line applied the \"stacked predictor\" to the test data for submission in section 6."},{"metadata":{"collapsed":true,"_uuid":"6b36c683ce6f17cbf9803ac508cb3dabafedf059","_cell_guid":"d2cc701c-b37f-4745-939a-8062a41f66d7","trusted":false},"cell_type":"code","source":"VotingPredictor = VotingClassifier(estimators=[ ('ExtC', ExtC_best), ('GBC',GBC_best), ('SVMC', SVMC_best), ('random_forest', random_forest_best)],\nvoting='hard', n_jobs=4)\nVotingPredictor = VotingPredictor.fit(X_train, Y_train)\nVotingPredictor_predictions = VotingPredictor.predict(test)\ntest_Survived = pd.Series(VotingPredictor_predictions, name=\"Survived\")\nStackingSubmission4 = pd.concat([PassengerId,test_Survived],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"860488f8a6cdbc5fe388d5124d4900bd4c8df94a","_cell_guid":"65a21655-0fbf-4806-a783-58498de603d4"},"cell_type":"markdown","source":"## 4.14. Summary of most important features"},{"metadata":{"collapsed":true,"_uuid":"660931bb1dad9956f7c8d25397c03be1935f2cf7","_cell_guid":"9513241c-db0d-4771-80d2-40796b884ebf","trusted":false},"cell_type":"code","source":"nrows = ncols = 2\nfig, axes = plt.subplots(nrows = nrows, ncols = ncols, sharex=\"all\", figsize=(15,7))\nnames_classifiers = [(\"AdaBoosting\", adaDTC_best),(\"ExtraTrees\",ExtC_best),\n(\"GradientBoosting\",GBC_best), (\"RandomForest\",random_forest_best)]\nnclassifier = 0\nfor row in range(nrows):\n    for col in range(ncols):\n        name = names_classifiers[nclassifier][0]\n        classifier = names_classifiers[nclassifier][1]\n        indices = np.argsort(classifier.feature_importances_)[::-1][:40]\n        g = sns.barplot(y=X_train.columns[indices][:40],x = classifier.feature_importances_[indices][:40] , orient='h',ax=axes[row][col])\n        g.set_xlabel(\"Relative importance\",fontsize=12)\n        g.set_ylabel(\"Features\",fontsize=12)\n        g.tick_params(labelsize=9)\n        g.set_title(name + \" feature importance\")\n        nclassifier += 1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b74f445f5d9fb9622c4609c00679254abb3c91b1","_cell_guid":"32ac3ce1-42bc-9a7d-44f3-4b7c025a921c"},"cell_type":"markdown","source":"# 5. Stacking predictions\n## 5.1. Preparing common class and out-of-fold data \n\nThis section describes an alternative method to creating a Stacking ensemble. \nThe overall result is not as good and I am not using for my submission, but I kept it there because of its nice use of a Python class to optimize the code."},{"metadata":{"_uuid":"4e5fda8c6c92fc3cdf8e906089494f80e7c37245","_cell_guid":"3e922821-5a10-040b-305a-c2d47d633c49"},"cell_type":"markdown","source":"### 5.1.1. Defining helpers with a Python class\n\nHere we invoke the use of Python's classes to help make it more convenient for us. For any newcomers to programming, one normally hears Classes being used in conjunction with Object-Oriented Programming (OOP). In short, a class helps to extend some code/program for creating objects (variables for old-school peeps) as well as to implement functions and methods specific to that class.\n\nIn the section of code below, we essentially write a class *SklearnHelper* that allows one to extend the inbuilt methods (such as train, predict and fit) common to all the Sklearn classifiers. Therefore this cuts out redundancy as  won't need to write the same methods five times if we wanted to invoke five different classifiers."},{"metadata":{"collapsed":true,"_uuid":"04d921ea89a0560cf010e956e4065bb2eaf21619","_cell_guid":"c017c078-172d-16e9-65f2-4a01c6e0626f","trusted":false},"cell_type":"code","source":"# Some useful parameters which will come in handy later on\nntrain = train.shape[0] # A numpy array is a grid of values, all of the same type, and is indexed by a tuple.\nntest = test.shape[0]\n# The number of dimensions is the rank of the array; the shape is a tuple of integers giving the size of the array along each dimension\n\nSEED = 0 # for reproducibility\nNFOLDS = 5 # set folds for out-of-fold prediction\nkf = KFold(ntrain, n_folds= NFOLDS, random_state=SEED) \n# KFold provides train/test indices to split data in train and test sets.\n# It splits the dataset into k consecutive folds (without shuffling by default).\n# Each fold is then used as validation set once while the k - 1 remaining folds form the training set \n\n# Class to extend the Sklearn classifier (train, predict and fit)\nclass SklearnHelper(object):\n    def __init__(self, clf, seed=0, params=None):\n        params['random_state'] = seed\n        self.clf = clf(**params)\n\n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        return self.clf.predict(x)\n    \n    def fit(self,x,y):\n        return self.clf.fit(x,y)\n    \n    def feature_importances(self,x,y):\n        return(self.clf.fit(x,y).feature_importances_)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"634199a5b36cba6b4d85db2c0401426a58b95279","_cell_guid":"75f59fa7-65a4-e53e-95df-4c747a608408"},"cell_type":"markdown","source":"Bear with me for those who already know this but for people who have not created classes or objects in Python before, let me explain what the code given above does. In creating my base classifiers, I will only use the models already present in the Sklearn library and therefore only extend the class for that.\n\n**def init** : Python standard for invoking the default constructor for the class. This means that when you want to create an object (classifier), you have to give it the parameters of clf (what sklearn classifier you want), seed (random seed) and params (parameters for the classifiers).\n\nThe rest of the code are simply methods of the class which simply call the corresponding methods already existing within the sklearn classifiers. Essentially, we have created a wrapper class to extend the various Sklearn classifiers so that this should help us reduce having to write the same code over and over when we implement multiple learners to our stacker."},{"metadata":{"_uuid":"4d193c581df258e823aff2796bf015cf906aac99","_cell_guid":"6f67620d-b531-a2fa-c297-e951970c3c28"},"cell_type":"markdown","source":"### 5.1.2. Out-of-Fold Predictions\n\nNow as alluded above, stacking uses predictions of base classifiers as input for training to a second-level model. However one cannot simply train the base models on the full training data, generate predictions on the full test set and then output these for the second-level training. We therefore use Kfold again to sample the data and prevent over-fitting."},{"metadata":{"collapsed":true,"_uuid":"46a93dc062e973832cecd50246d0d7581aafb02b","_cell_guid":"406d0494-1d0c-3126-19d9-bc53127c4249","trusted":false},"cell_type":"code","source":"def get_oof(clf, x_train, y_train, x_test):\n    oof_train = np.zeros((ntrain,)) # ntrain represents the size of the 'train' array along each dimension\n    oof_test = np.zeros((ntest,))\n    oof_test_skf = np.empty((NFOLDS, ntest))\n\n# The enumerate function adds a counter, and starts counting from 0, if no argument\n    for i, (train_index, test_index) in enumerate(kf): # kf was created with the KFold function, it creates indices to split the train/test data\n        x_tr = x_train[train_index]\n        y_tr = y_train[train_index]\n        x_te = x_train[test_index]\n\n        clf.train(x_tr, y_tr) # trains classifier on selected train data (indexed with train_index)\n        oof_train[test_index] = clf.predict(x_te) # applies classifier on selected test data (indexed with test_index)\n\n #      oof_test_skf[i, :] = clf.predict(x_test) # applies classifier on all test data (indexed with x_test)\n\n #   oof_test[:] = oof_test_skf.mean(axis=0) # mean of the first axis (indexed as 0 in tupple arrays)\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1) # reshape gives a new shape to an array without changing its data","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7b1a7767ae61b6b217a3311e89190b05ab0a4891","_cell_guid":"3cd92196-f7ba-4f14-0fc4-36520fbcb2ca"},"cell_type":"markdown","source":"## 5.2. Applying classifyer (first pass)\nSo now let us prepare five learning models as our first level classification.\nThese models can all be conveniently invoked via the Sklearn library and are listed as follows:\n\n 1. Random Forest classifier\n 2. Extra Trees classifier\n 3. AdaBoost classifer\n 4. Gradient Boosting classifer\n 5. Support Vector Machine"},{"metadata":{"_uuid":"12e3a5f76fb118ff6906431fc60e7010e33106ad","_cell_guid":"0ef6862a-b5cc-6829-f040-d2b2b2c817f3"},"cell_type":"markdown","source":"### 5.2.1. Parameters\n\nJust a quick summary of the parameters that we will be listing here for completeness,\n\n**n_jobs** : Number of cores used for the training process. If set to -1, all cores are used.\n\n**n_estimators** : Number of classification trees in your learning model ( set to 10 per default)\n\n**max_depth** : Maximum depth of tree, or how much a node should be expanded. Beware if set to too high  a number would run the risk of overfitting as one would be growing the tree too deep\n\n**verbose** : Controls whether you want to output any text during the learning process. A value of 0 suppresses all text while a value of 3 outputs the tree learning process at every iteration.\n\n Please check out the full description via the official Sklearn website. There you will find that there are a whole host of other useful parameters that you can play around with. "},{"metadata":{"collapsed":true,"_uuid":"d77772886c0125e022d1fbb39cd484c95121d74d","_cell_guid":"6e634aba-90b6-0620-eceb-3e1a39fbfedc","trusted":false},"cell_type":"code","source":"# Put in our parameters for said classifiers\n# Random Forest parameters\nrf_params = {\n    'n_jobs': -1,\n    'n_estimators': 500,\n     'warm_start': True, \n     #'max_features': 0.2,\n    'max_depth': 6,\n    'min_samples_leaf': 2,\n    'max_features' : 'sqrt',\n    'verbose': 0\n}\n\n# Extra Trees Parameters\net_params = {\n    'n_jobs': -1,\n    'n_estimators':500,\n    #'max_features': 0.5,\n    'max_depth': 8,\n    'min_samples_leaf': 2,\n    'verbose': 0\n}\n\n# AdaBoost parameters\nada_params = {\n    'n_estimators': 500,\n    'learning_rate' : 0.75\n}\n\n# Gradient Boosting parameters\ngb_params = {\n    'n_estimators': 500,\n     #'max_features': 0.2,\n    'max_depth': 5,\n    'min_samples_leaf': 2,\n    'verbose': 0\n}\n\n# Support Vector Classifier parameters \nsvc_params = {\n    'kernel' : 'linear',\n    'C' : 0.025\n    }","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4a49a6c0cbe7028035b6efb7f9374084f3fa152a","_cell_guid":"ee6325d7-a7c1-c767-fcba-4c59eaa83787"},"cell_type":"markdown","source":"Furthermore, since having mentioned about Objects and classes within the OOP framework, let us now create 5 objects that represent our 5 learning models via our Helper Sklearn Class we defined earlier."},{"metadata":{"collapsed":true,"_uuid":"3bd54b4dfbbedc86d6c415cf83936bb8cd6c0973","_cell_guid":"6798243f-5ff9-527b-01b1-09cfe62284bb","trusted":false},"cell_type":"code","source":"# Create five objects that represent our five models\nrf = SklearnHelper(clf=RandomForestClassifier, seed=SEED, params=rf_params)\net = SklearnHelper(clf=ExtraTreesClassifier, seed=SEED, params=et_params)\nada = SklearnHelper(clf=AdaBoostClassifier, seed=SEED, params=ada_params)\ngb = SklearnHelper(clf=GradientBoostingClassifier, seed=SEED, params=gb_params)\nsvc = SklearnHelper(clf=SVC, seed=SEED, params=svc_params)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cd3f527543e61b3841f242b744c3c5d9af608941","_cell_guid":"46e1e6fc-92e2-e7e2-1ab9-470e6c7039ce"},"cell_type":"markdown","source":"### 5.2.2. Creating NumPy arrays out of our train and test datasets\n\nHaving prepared our first layer base models as such, we can now ready the training and test test data for input into our classifiers by generating NumPy arrays out of their original dataframes as follows.\nNote: NumPy's main object is an \"homogeneous multidimensional array:. It is like a table of elements (usually numbers), all of the same type, indexed by a tuple (a sequence, like a list) of positive integers.\nIn NumPy dimensions are called axes, "},{"metadata":{"collapsed":true,"_uuid":"95fdd1e8cd9f23cfcef3bed92511da084a323c55","scrolled":false,"_cell_guid":"968cbd7f-80b2-7f8d-2ad6-b68b3aeae671","trusted":false},"cell_type":"code","source":"# Create Numpy arrays of train, test and target ( Survived) dataframes to feed into our models\ny_train = train['Survived'].ravel()\ntrain = train.drop(['Survived'], axis=1)\nx_train = train.values # Creates an Numpy array of the train data\nx_test = test.values # Creats an Numpy array of the test data\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d30ac9528b69cdaec565b647fee98a92391112c9","_cell_guid":"606b619c-2301-8aaa-a5de-781d981c4a6f"},"cell_type":"markdown","source":"### 5.2.3. Output of the First level Predictions\n\nWe now feed the training and test data into our 5 base classifiers and use the Out-of-Fold prediction function we defined earlier to generate our first level predictions. Allow a handful of minutes for the chunk of code below to run."},{"metadata":{"collapsed":true,"_uuid":"114750e2d5e4fdd234ccd8647fc349463a56fa09","_cell_guid":"79bd2a86-82e2-648a-e816-9660e89794ad","trusted":false},"cell_type":"code","source":"# Create our OOF train and test predictions. These base results will be used as new features\net_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test) # Extra Trees\nrf_oof_train, rf_oof_test = get_oof(rf,x_train, y_train, x_test) # Random Forest\nada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) # AdaBoost \ngb_oof_train, gb_oof_test = get_oof(gb,x_train, y_train, x_test) # Gradient Boost\nsvc_oof_train, svc_oof_test = get_oof(svc,x_train, y_train, x_test) # Support Vector Classifier\n\nfrom sklearn.metrics import accuracy_score\nacc_et  = round(accuracy_score(et_oof_train, y_train) * 100, 2)\nacc_rf  = round(accuracy_score(rf_oof_train, y_train) * 100, 2)\nacc_ada = round(accuracy_score(ada_oof_train, y_train) * 100, 2)\nacc_gb  = round(accuracy_score(gb_oof_train, y_train) * 100, 2)\nacc_svc = round(accuracy_score(svc_oof_train, y_train) * 100, 2)\n#print(\" Extra Tree accuracy: \",  acc_et, '\\n',\"Random Forest accuracy: \",acc_rf, '\\n',\"AdaBoost accuracy: \",acc_ada, '\\n',\"Gradient Boost accuracy: \",acc_gb, '\\n',\"Support Vector accuracy: \",acc_svc)\naccuracy_table = { 'Extra Tree': acc_et, 'Random Forest': acc_rf, 'AdaBoost': acc_ada, 'Gradient Boost': acc_gb,'Support Vector': acc_svc}\naccuracy_table","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"28ea39f701e184aa8da3af4a82a5eb728191baeb","_cell_guid":"c77f2ede-3ac9-4505-a470-35cd742bf739","trusted":false},"cell_type":"code","source":"objects = ('Extra Tree', 'Random Forest', 'AdaBoost', 'Gradient Boost', 'Support Vector')\ny_pos = np.arange(len(objects))\naccuracies = [acc_et,acc_rf,acc_ada,acc_gb,acc_svc]\n \nplt.barh(y_pos, accuracies, align='center', alpha=0.5)\nplt.yticks(y_pos, objects)\nplt.xlabel('Accuracy')\nplt.title('Classifier Outcome')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4cf41b3d9a541c9d39b645a66c8f1116eaf76861","_cell_guid":"3f292e65-fe8a-d662-6ace-41a19866d671"},"cell_type":"markdown","source":"**Observation:**\n* With the random OOF data, Random Forest gets the best result\n* All other algorithms 'hover' around 80%\n\n### 5.2.4. Feature importances generated from the different classifiers\n\nNow having learned our the first-level classifiers, we can utilise a very nifty feature of the Sklearn models and that is to output the importances of the various features in the training and test sets with one very simple line of code.\n\nAs per the Sklearn documentation, most of the classifiers are built in with an attribute which returns feature importances by simply typing in **.feature_importances_**. Therefore we will invoke this very useful attribute via our function earliand plot the feature importances as such"},{"metadata":{"collapsed":true,"_uuid":"b3b0356c8bef0dceb5fcfa7fb7a11359010b2098","_cell_guid":"ed9cf8b5-95a4-d974-fb11-592214949d1f","trusted":false},"cell_type":"code","source":"rf_feature = rf.feature_importances(x_train,y_train)\net_feature = et.feature_importances(x_train, y_train)\nada_feature = ada.feature_importances(x_train, y_train)\ngb_feature = gb.feature_importances(x_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d59169f89c5541177f537200a90805420b449001","_cell_guid":"2a03ad8f-0ea1-5afa-a6e8-56284482c646"},"cell_type":"markdown","source":"Create a dataframe from the lists containing the feature importance data for easy plotting via the Plotly package."},{"metadata":{"collapsed":true,"_uuid":"6f68b3033a8f185f61d83e80323c2486024f5d4d","_cell_guid":"635a063f-281d-66d4-6572-587ebecd6b4b","trusted":false},"cell_type":"code","source":"cols = train.columns.values\n# Create a dataframe with features\nfeature_dataframe = pd.DataFrame( {'features': cols,\n     'Random Forest feature importances': rf_feature,\n     'Extra Trees  feature importances': et_feature,\n      'AdaBoost feature importances': ada_feature,\n    'Gradient Boost feature importances': gb_feature\n    })","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7e25675f239b0ab008e0264917abff497795681a","_cell_guid":"06b9f410-f93e-0206-b029-24df035eea2b"},"cell_type":"markdown","source":"### 5.2.5. Interactive feature importances via Plotly scatterplots\nI'll use the interactive Plotly package at this juncture to visualise the feature importances values of the different classifiers  via a plotly scatter plot by calling \"Scatter\" as follows:"},{"metadata":{"collapsed":true,"_uuid":"d8ee9114cd391433835f1272ef81d0a729c78b71","_cell_guid":"1ac351c6-83c6-c35b-9d66-64f16b5d073f","trusted":false},"cell_type":"code","source":"# Scatter plot \ntrace = go.Scatter(\n    y = feature_dataframe['Random Forest feature importances'].values,\n    x = feature_dataframe['features'].values,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 25,\n        color = feature_dataframe['Random Forest feature importances'].values,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = feature_dataframe['features'].values\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Random Forest Feature Importance',\n    hovermode= 'closest',\n#     xaxis= dict(\n#         title= 'Pop',\n#         ticklen= 5,\n#         zeroline= False,\n#         gridwidth= 2,\n#     ),\n    yaxis=dict(\n        title= 'Feature Importance',\n        ticklen= 5,\n        gridwidth= 2\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig,filename='scatter2010')\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9d7b8fdd0c3102d7e3ddcffaf26ce19b02e5ad74","_cell_guid":"553828f0-c994-5ee1-695f-9373f11a1a7b"},"cell_type":"markdown","source":"Now let us calculate the mean of all the feature importances and store it as a new column in the feature importance dataframe."},{"metadata":{"collapsed":true,"_uuid":"f611812e2c9de3773df2264dfb2b13c0995807ac","_cell_guid":"06847850-a829-0858-b12c-7b66e53e030a","trusted":false},"cell_type":"code","source":"# Create the new column containing the average of values\n\nfeature_dataframe['mean'] = feature_dataframe.mean(axis= 1) # axis = 1 computes the mean row-wise\nfeature_dataframe.head(3)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1f0aff0896fef90b326fff7816393fef0e0cb992","_cell_guid":"5645e647-c517-7822-f881-b8d7e38ef5da"},"cell_type":"markdown","source":"### 5.2.6. Plotly Barplot of Average Feature Importances\nHaving obtained the mean feature importance across all our classifiers, we can plot them into a Plotly bar plot as follows:"},{"metadata":{"collapsed":true,"_uuid":"0bd069388b419fe45306c01825aa3e6f5466ba2b","_cell_guid":"63d86121-8c29-4b7f-b2ad-12b0a593f1d6","trusted":false},"cell_type":"code","source":"y = feature_dataframe['mean'].values\nx = feature_dataframe['features'].values\ndata = [go.Bar(\n            x= x,\n             y= y,\n            width = 0.5,\n            marker=dict(\n               color = feature_dataframe['mean'].values,\n            colorscale='Portland',\n            showscale=True,\n            reversescale = False\n            ),\n            opacity=0.6\n        )]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Barplots of Mean Feature Importance',\n    hovermode= 'closest',\n#     xaxis= dict(\n#         title= 'Pop',\n#         ticklen= 5,\n#         zeroline= False,\n#         gridwidth= 2,\n#     ),\n    yaxis=dict(\n        title= 'Feature Importance',\n        ticklen= 5,\n        gridwidth= 2\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='bar-direct-labels')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bbb76d189e8d03921caaacfa9545cef894348c7d","_cell_guid":"c1201ecc-b07d-f8a1-0870-b8d78c89ebc0"},"cell_type":"markdown","source":"## 5.3. Second-Level Predictions from the First-level Output"},{"metadata":{"_uuid":"fed132782b73dda8d265065867e7f57c0aed7f50","_cell_guid":"6b901750-ccdd-38ca-d8ea-1c361121ec4f"},"cell_type":"markdown","source":"## 5.3.1. Adding first-level output as new features\n\nHaving now obtained our first-level predictions, one can think of it as essentially building a new set of features to be used as training data for the next classifier. As per the code below, we are therefore having as our new columns the first-level predictions from our earlier classifiers and we train the next classifier on this.\nIt is as if we are weighting each of the features (or variables in the data set) with additional parameters to highlight their respective roles. And run the prediction algorithm again."},{"metadata":{"collapsed":true,"_uuid":"a5945e93337b87a1a8ee5580856768bbb14c07cd","_cell_guid":"7330a71c-0b71-87c2-1f4d-dd0f6d6fa586","trusted":false},"cell_type":"code","source":"base_predictions_train = pd.DataFrame( {'RandomForest': rf_oof_train.ravel(),\n     'ExtraTrees': et_oof_train.ravel(),\n     'AdaBoost': ada_oof_train.ravel(),\n     'GradientBoost': gb_oof_train.ravel()\n    })\nbase_predictions_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2a6f987bd9b8ffc32a72e21cb8c43a6bc43ba200","_cell_guid":"f69c11db-d84e-8536-4c7e-382fbe67483e"},"cell_type":"markdown","source":"## 5.3.2. Correlation heatmap of the second level training set"},{"metadata":{"collapsed":true,"_uuid":"9714ecaedf7385c5b8ad346ab909215eb9f2abc6","_cell_guid":"4cf590ee-133f-6487-cf5a-53f346893d1c","trusted":false},"cell_type":"code","source":"data = [\n    go.Heatmap(\n        z= base_predictions_train.astype(float).corr().values ,\n        x= base_predictions_train.columns.values,\n        y= base_predictions_train.columns.values,\n          colorscale='Viridis',\n            showscale=True,\n            reversescale = True\n    )\n]\npy.iplot(data, filename='labelled-heatmap')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4d9ef0298b568e43da6925f385403e0d77bd6e33","_cell_guid":"4d6b61c0-5d72-b02a-3b37-cbf6518d71b6"},"cell_type":"markdown","source":"Nice graphics, but the obsevation is unclear in my opinion:\n* On one side, we hope as analyst that the models come out with similar patterns. An easy direction to follow.\n* At the same time, \"there have been quite a few articles and Kaggle competition winner stories about the merits of having trained models that are more uncorrelated with one another producing better scores\". As we say in business, diversity brings better results, this seems to be true with algorithms as well!"},{"metadata":{"collapsed":true,"_uuid":"fef365199854ca3fff754399b4699d941b7e43b8","_cell_guid":"6685fa11-497f-3fc2-ab1f-97f92d6eca61","trusted":false},"cell_type":"code","source":"x_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\nx_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)\n#len(x_train)\n#x_test_table = x_test[:,:]\nprint(\"x_test numpy array: {}\".format(x_test.shape))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"65727ae393d3f7118215fde76e4fd5a9d0e9dd6c","_cell_guid":"a02a94ab-3c9c-a824-7168-e964c5a0f5d5"},"cell_type":"markdown","source":"Having now concatenated and joined both the first-level train and test predictions as x_train and x_test, we can now fit a second-level learning model."},{"metadata":{"_uuid":"dc4a32e9a8e7c9e611124cba676e5d28240b38be","_cell_guid":"628a03ea-933c-7075-a589-0ff7af237dfd"},"cell_type":"markdown","source":"# 5.4. Second level learning model via XGBoost\n\nHere we choose the eXtremely famous library for boosted tree learning model, XGBoost. It was built to optimize large-scale boosted tree algorithms. For further information about the algorithm, check out the [official documentation][1].\n\n  [1]: https://xgboost.readthedocs.io/en/latest/\n\nAnyways, we call an XGBClassifier and fit it to the first-level train and target data and use the learned model to predict the test data as follows:"},{"metadata":{"collapsed":true,"_uuid":"5155d370069fe6de0fe5105309342ce55130dae8","_cell_guid":"3a7c7517-b9a3-3a21-3a7b-299ca37c6843","trusted":false},"cell_type":"code","source":"gbm = xgb.XGBClassifier(\n    #learning_rate = 0.02,\n n_estimators= 2000,\n max_depth= 4,\n min_child_weight= 2,\n #gamma=1,\n gamma=0.9,                        \n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread= -1,\n scale_pos_weight=1).fit(x_train, y_train)\ngbm_predictions = gbm.predict(x_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0101e6b843f6378838874ccfb844ed464b81d627","_cell_guid":"0a8152d8-6842-ed00-6bc5-47a511adce1c"},"cell_type":"markdown","source":"Just a quick run down of the XGBoost parameters used in the model:\n\n**max_depth** : How deep you want to grow your tree. Beware if set to too high a number might run the risk of overfitting.\n\n**gamma** : minimum loss reduction required to make a further partition on a leaf node of the tree. The larger, the more conservative the algorithm will be.\n\n**eta** : step size shrinkage used in each boosting step to prevent overfitting"},{"metadata":{"_uuid":"52ac0cd99cee0099d86a180127da42ff7fff960a","_cell_guid":"6b4a5c81-e968-d41e-27e4-871481019867"},"cell_type":"markdown","source":"# 6. Producing the submission file for Kaggle\n\nFinally having trained and fit all our first-level and second-level models, we can now output the predictions into the proper format for submission to the Titanic competition.\nWhich model to choose? These are the results of my many submissions:\n\n1.  The prediction with random_forest (Section 4.9) generates a public score of 0.75119.\n2.  The prediction with gsrandom_forest (Section 4.11, after stratification and model cross validation) generates a public score of 0.77990. \n3. The stacked prediction (Section 5) with gbm generates a public score of 0.62679.\n\nDecision: submit #2 as best predictor"},{"metadata":{"collapsed":true,"_uuid":"9d607d829dbadd6c72ee01c9735a642435eb53e6","_cell_guid":"f5a31787-5fe1-a559-bee9-ad6b6d83ae14","trusted":false},"cell_type":"code","source":"# Generate Submission File \nStackingSubmission5 = pd.DataFrame({ 'PassengerId': PassengerId,\n                            'Survived': gbm_predictions })\n#print(\"Predictions: \", len(predictions),'\\n', \"y-train: \", len(y_train), '\\n','PassengerId', len(PassengerId))\nStackingSubmission4.to_csv(\"StackingSubmission.csv\", index=False)\n\nprint(\"Completed...\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9db5fd7cbb0d406ab0ef9aa08cf56532c51ec8b5","_cell_guid":"1e56c738-b8f3-95e4-d642-c483f9757ed8"},"cell_type":"markdown","source":"**Steps for Further Improvement**\n\nAs a closing remark it must be noted that the steps taken above just show a very simple way of producing an ensemble stacker. You hear of ensembles created at the highest level of Kaggle competitions which involves monstrous combinations of stacked classifiers as well as levels of stacking which go to more than 2 levels. \n\nSome additional steps that may be taken to improve one's score could be:\n\n 1. Implementing a good cross-validation strategy in training the models to find optimal parameter values\n 2. Introduce a greater variety of base models for learning. The more uncorrelated the results, the better the final score."},{"metadata":{"_uuid":"c32d1d64e1a5f8fbe5f51a0a7afd952ccfdec57e","_cell_guid":"9a8f83fd-d0e8-035a-cf7f-25c9012e9373"},"cell_type":"markdown","source":"# 7. Credits\n**Huge credits to Anisotropic,  Yassine Ghouzam, Faron and Sina** for pulling together most of the code in this kernel."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"_change_revision":0,"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"_is_fork":false},"nbformat":4,"nbformat_minor":1}