{"nbformat": 4, "cells": [{"execution_count": null, "cell_type": "markdown", "outputs": [], "source": "In this module I'll tackle the Titanic problem with the Random Forest Classifier. It is an ensemble method itself, and is particularly suitable for the situation-the data size is small, and there are not many features. Hopefully it would yield robust results.", "metadata": {"_cell_guid": "ced183ef-1533-4e38-8e88-f83f3ba34ecd", "_execution_state": "idle", "_uuid": "926e22cbbbb0d9cac376d6b5536584aa07402129", "collapsed": false}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "#Basic libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport string", "metadata": {"trusted": false, "_execution_state": "idle", "_uuid": "deccceeb14dab461a02ebf585a2d4dca51699363", "_cell_guid": "4ec85fde-78fd-417d-bbfd-d10a2f4be686"}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "train = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')", "metadata": {"_cell_guid": "e33ff6fb-9e6f-4bb5-982a-b28fe8fcf951", "trusted": false, "_execution_state": "idle", "_uuid": "9e441854657f0a6a676bdc55bd068dc6acce045d", "collapsed": false}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "train.head()", "metadata": {"_cell_guid": "cc6a83a8-6a79-408a-9a5f-3995ead2d86c", "trusted": false, "_execution_state": "idle", "_uuid": "34d54196fc0db96c39a267aa921918a7f9341b02", "collapsed": false}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "#Make a copy, leave the target column (Survived) aside, and drop the useless features (PassengerId\n#and Ticket do not provide any predictive power, otherwise it's counterintuitive)\ntrain_c = train.copy()\ntest_c = test.copy()\nlabel = train_c.Survived\ntrain_c.drop(['PassengerId', 'Ticket', 'Survived'], inplace=True, axis=1)\ntest_c.drop(['PassengerId', 'Ticket'], inplace=True, axis=1)\ncombine = [train_c, test_c]", "metadata": {"_cell_guid": "2cada347-e8c9-4e33-a957-294393b0593e", "trusted": false, "_execution_state": "idle", "_uuid": "26636447ddae823e8e424be40b1fd9cd6ec0acdf", "collapsed": false}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "#Take a look at the summary of the remaining features\ntrain_c.info()", "metadata": {"_cell_guid": "363105df-892d-4fb1-940d-967dfba48024", "trusted": false, "_execution_state": "idle", "_uuid": "dbc83379bc950886b9637e9f8c2b77b404d6bb60", "collapsed": false}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "#We need to fill in the null values for both train and test set. Let's deal with Age first. It's\n#easy to simply fill the median or mean, but on closer look I bet the passengers' name provide some\n#information on Age.\nfor data in combine:\n    data['Title'] = data.Name.apply(lambda x: x.split()[1])\n    data.drop('Name', inplace=True, axis=1)", "metadata": {"_cell_guid": "2cf372fe-f7cc-4762-895f-210c0eecac38", "trusted": false, "_execution_state": "idle", "_uuid": "d0b5f3c696ae8e048302bdc6b97e0cf88fbfeddd", "collapsed": false}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "test_c.Title.unique()", "metadata": {"_cell_guid": "0d6acd53-fb61-41a9-bab7-57c482c55838", "trusted": false, "_execution_state": "idle", "_uuid": "a6198b61a204f8ccac356c58102b5cf1c144640b", "collapsed": false}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "#Sort all rare titles into another category, then look at the Age distribution for each title\ntrain_c.Title = train_c.Title.replace(['Planke,', 'Don.', 'Rev.',\n       'Billiard,', 'der', 'Walle,', 'Dr.', 'Pelsmaeker,', 'Mulder,', 'y',\n       'Steen,', 'Carlo,', 'Mme.', 'Impe,', 'Ms.', 'Major.', 'Gordon,',\n       'Messemaeker,', 'Mlle.', 'Col.', 'Capt.', 'Velde,', 'the',\n       'Shawah,', 'Jonkheer.', 'Melkebeke,', 'Cruyssen,'], 'Other')\ntest_c.Title = test_c.Title.replace(['Carlo,', 'Khalil,', 'y', 'Ms.',\n       'Palmquist,', 'Col.', 'Planke,', 'Rev.', 'Billiard,',\n       'Messemaeker,', 'Dr.', 'Brito,'], 'Other')", "metadata": {"_cell_guid": "4b0e30d5-e9bb-4ac9-8c3a-af55f6d02f61", "trusted": false, "_execution_state": "idle", "_uuid": "c0a89ba192ddd2a9241e4bc7d915961f224bf0db", "collapsed": false}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "#It seems the title does provide some information on age. Let's fill in the Age null values with\n#the median of their respective title\nfor data in combine:\n    for title in ['Master.', 'Miss.', 'Mr.', 'Mrs.', 'Other']:\n        part = data[data.Title == title]\n        median = part['Age'].median()\n        data.loc[(data.Title == title) & data['Age'].isnull(), 'Age'] = median", "metadata": {"_cell_guid": "736e5cdd-6d5e-43f5-948e-9d890ca24fc3", "trusted": false, "_execution_state": "idle", "_uuid": "931faca93b4c33cc1cdb9640fabd47caddffa759", "collapsed": false}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "#Now let's deal with Cabin. I bet the first letter provides the most predictive power, so let's\n#discard the following numbers\nfor data in combine:\n    data.Cabin = data.Cabin.astype(str).str[0]", "metadata": {"_cell_guid": "f4b987f7-af4b-4a6a-b86a-9d55300739f8", "trusted": false, "_execution_state": "idle", "_uuid": "4fe00a63a8f526fc272cb2b6daf8a82aec51063e", "collapsed": false}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "#Seems Cabin does have some predictive power on survival rate. The only problem is there are too many\n#missing values. The good thing is we haven't lost any information by converting missing values to \n#another category(n)\nlabel.groupby(train_c.Cabin).mean()", "metadata": {"_cell_guid": "612f52f2-9fdb-4185-9432-85e5f5865bf5", "trusted": false, "_execution_state": "idle", "_uuid": "704b055e106ac87e479a2d744d6e1155d50d832b", "collapsed": false}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "#For the two missing values in Embarked, simply fill in the mode\ntrain_c.Embarked.value_counts()\ntrain_c.Embarked = train_c.Embarked.fillna('S')", "metadata": {"_cell_guid": "fdf486ba-5adc-464c-9f00-280491f3c2bc", "trusted": false, "_execution_state": "idle", "_uuid": "8e95fefa6343fb150e2d9f74b7c9a796b2c4663b", "collapsed": false}}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"collapsed": false, "trusted": false, "_execution_state": "idle", "_uuid": "de086e900b46e7c43f5c3255b7df545d73432813", "_cell_guid": "44e44ce0-9983-46e7-b02e-f9032965e5ed"}, "source": "#There is only one missing Fare value in test set. Simply fill in the median\nmedian = test_c.Fare.median()\ntest_c.Fare = test_c.Fare.fillna(median)"}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "#Now everything is in place. Next we need to convert all non-numeric values into numeric\ntrain_c.info()", "metadata": {"_cell_guid": "e53b087e-149e-4e29-bcbe-d64f3dc1aec9", "trusted": false, "_execution_state": "idle", "_uuid": "1ce0c56b69d2851280418a1c7aa25fedfb2cbfd3", "collapsed": false}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nfor column in ['Sex', 'Cabin', 'Embarked', 'Title']:\n    le.fit(list(train_c[column].values)+list(test_c[column].values))\n    train_c[column] = le.transform(train_c[column])\n    test_c[column] = le.transform(test_c[column])", "metadata": {"_cell_guid": "81ea2261-f9e4-436d-b237-1d19cd8ca119", "trusted": false, "_execution_state": "idle", "_uuid": "048bb09d51def0f072ff000444b98e050a548957", "collapsed": false}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "#Now everything is numeric. The good thing about Random Forest is that we don't need to make strong\n#assumptions like the values have equal intervals\ntrain_c", "metadata": {"_cell_guid": "120f0b0d-d9b3-42bd-b97a-0b36a72a9474", "trusted": false, "_execution_state": "idle", "_uuid": "dd9065af3c58d4ac93112faf02bce27109325c10", "collapsed": false}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\nrf = RandomForestClassifier()\ncross_val_score(rf, train_c, label, cv=3)", "metadata": {"_cell_guid": "88bc905a-06e3-433e-8ef4-774d85403fda", "trusted": false, "_execution_state": "idle", "_uuid": "5b49e408de1cc8a382e3ba8e10372ff3866543cb", "collapsed": false}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "#To select the best parameters, we need to grid search\nfrom sklearn.model_selection import GridSearchCV\nparameters = {'max_features': ['auto', 'log2', None], 'max_depth': np.arange(5, 13),\n              'min_samples_leaf': np.arange(2, 10)}\nclf = GridSearchCV(RandomForestClassifier(), parameters, cv=5)\nclf.fit(train_c, label)", "metadata": {"_cell_guid": "29c2fa64-de3b-4551-8d57-43955f2f2163", "trusted": false, "_execution_state": "idle", "_uuid": "8052fe36dc7e4c676f7995cb87981157f13ce5d2", "collapsed": false}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "#We will use the grid-searched parameters, but grow more trees, i.e. take a larger n_estimator\nclf.best_params_", "metadata": {"_cell_guid": "87cd1cc7-2800-4990-ba41-8f050dc94846", "trusted": false, "_execution_state": "idle", "_uuid": "2e8fead56acd690bbfa16816e39912aad066fdb1", "collapsed": false}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "rf_tuned = RandomForestClassifier(n_estimators=1000, max_depth=10, \n                                  max_features=None, min_samples_leaf=3)\ncross_val_score(rf_tuned, train_c, label, cv=3)", "metadata": {"_cell_guid": "18a6c26e-c2fe-466d-8104-ef2bcba0a779", "trusted": false, "_execution_state": "idle", "_uuid": "05117a43573f314c55d93c71b3931834f1008a2d", "collapsed": false}}, {"execution_count": null, "cell_type": "code", "outputs": [], "metadata": {"collapsed": false, "trusted": false, "_execution_state": "idle", "_uuid": "2942a3698fb3798409003adcfe4dfcb6de6cf000", "_cell_guid": "f63139e9-043d-4450-8cc7-b05455c4d691"}, "source": "rf_tuned.fit(train_c, label)\nprediction = rf_tuned.predict(test_c)\nsubmission = pd.DataFrame()\nsubmission['PassengerId'] = test.PassengerId\nsubmission['Survived'] = prediction\nsubmission.to_csv('rf.csv', index=False)"}], "metadata": {"language_info": {"version": "3.6.1", "pygments_lexer": "ipython3", "mimetype": "text/x-python", "nbconvert_exporter": "python", "file_extension": ".py", "name": "python", "codemirror_mode": {"version": 3, "name": "ipython"}}, "kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}}, "nbformat_minor": 0}