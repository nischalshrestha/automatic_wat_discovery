{"cells":[{"metadata":{"_uuid":"5c37b63f765ac79c17dd22486b888e6903a3e44a"},"cell_type":"markdown","source":"# Titanic Revisited (w/ XGBoost)\n\n### Goal\nI originally worked with this dataset about 2.5 years ago when working through Udacity's Nanodegree in Data Analytics. I has, more or less, no idea what I was doing then. I thought it would be nice to revisit this dataset and see if I could get a better accuracy than my first time through (which was around 74% if I recall). Particularly, I've been wanting to work with XGBoost for awhile and this seems an appropriate classification problem to give it a go on!"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport math\nimport xgboost\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\nsub = pd.read_csv('../input/gender_submission.csv')\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a7d322844010247155dd2ad7d8412532f8bd98b6"},"cell_type":"markdown","source":"## Feature Generation and Removal\n\nTime to generate a few features which may be of use in classification."},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"f605be608550de514ea281e6143b4bd4edecd39c"},"cell_type":"code","source":"#Does the passanger have a cabin?\ntrain['cabin_binary'] = train[\"Cabin\"].apply(lambda i: 0 if str(i) == \"nan\" else 1)\n\n#Family Size\ntrain['family_size'] = 1 + train['SibSp'] + train['Parch']\ntrain['solo'] = train[\"family_size\"].apply(lambda i: 1 if i == 1 else 0)\n\n#Fix Nulls\ntrain['Embarked'] = train['Embarked'].fillna('S')\ntrain['Age'] = train['Age'].fillna(int(np.mean(train['Age'])))\n\n#A few age specific Binaries\ntrain['Child'] = train[\"Age\"].apply(lambda i: 1 if i <= 17 and i > 6 else 0)\ntrain['toddler'] = train[\"Age\"].apply(lambda i: 1 if i <= 6 else 0)\ntrain['Elderly'] = train[\"Age\"].apply(lambda i: 1 if i >= 60 else 0)\n\n# Fancy fancy\ntrain['fancy'] = train['Fare'].apply(lambda i: 1 if i >= 100 else 0)\n\n# standard\ntrain['standard_fare'] = train['Fare'].apply(lambda i: 1 if i <= 10.0 else 0)\n\n#No requirement to standardize in DT models, but might as well\nfare_scaler = StandardScaler()\nfare_scaler.fit(train['Fare'].values.reshape(-1, 1))\ntrain['fare_std'] = fare_scaler.transform(train['Fare'].values.reshape(-1, 1))\n\n#get status of passanger\ntrain['title'] = 'default'\n\nfor i in train.values:\n    name = i[3] #First checks for rare titles (Thanks Anisotropic's wonderful Kernel for inspiration//help here!)\n    for e in ['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona']:\n        if e in name:\n            train.loc[train['Name'] == name, 'title'] = 'rare'\n    if 'Miss' in name or  'Mlle' in name or 'Ms' in name or 'Mme' in name or 'Mrs' in name:\n        train.loc[train['Name'] == name, 'title'] = 'Ms'\n    if 'Mr.' in name or 'Master' in name:\n        train.loc[train['Name'] == name, 'title'] = 'Mr'\n\n\ntrain.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3bf3803007b843b51108210cc3b339d580ca8176"},"cell_type":"markdown","source":"Lets send the test data through the same pipeline!"},{"metadata":{"trusted":true,"_uuid":"e08c03615014007b86d9756a8848de6b10132099","scrolled":true},"cell_type":"code","source":"#Does the passanger have a cabin?\ntest['cabin_binary'] = test[\"Cabin\"].apply(lambda i: 0 if str(i) == \"nan\" else 1)\n\n#Family Size\ntest['family_size'] = 1 + test['SibSp'] + test['Parch']\ntest['solo'] = test[\"family_size\"].apply(lambda i: 1 if i == 1 else 0)\n\n#Fix Nulls\ntest['Embarked'] = test['Embarked'].fillna('S')\ntest['Age'] = test['Age'].fillna(int(np.mean(test['Age'])))\n\n#A few age specific Binaries\ntest['Child'] = test[\"Age\"].apply(lambda i: 1 if i <= 17 and i > 6 else 0)\ntest['toddler'] = test[\"Age\"].apply(lambda i: 1 if i <= 6 else 0)\ntest['Elderly'] = test[\"Age\"].apply(lambda i: 1 if i >= 60 else 0)\n\n# Fancy fancy\ntest['fancy'] = test['Fare'].apply(lambda i: 1 if i >= 100 else 0)\ntest['standard_fare'] = test['Fare'].apply(lambda i: 1 if i <= 10.0 else 0)\n\n#standardize\ntest['fare_std'] = fare_scaler.transform(test['Fare'].values.reshape(-1, 1))\n\n#get status of passanger\ntest['title'] = 'default'\n\nfor i in test.values:\n    name = i[2] #First checks for rare titles (Thanks Anisotropic's wonderful Kernel for inspiration//help here!)\n    for e in ['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona']:\n        if e in name:\n            test.loc[test['Name'] == name, 'title'] = 'rare'\n    if 'Miss' in name or  'Mlle' in name or 'Ms' in name or 'Mme' in name or 'Mrs' in name:\n        test.loc[test['Name'] == name, 'title'] = 'Ms'\n    if 'Mr.' in name or 'Master' in name:\n        test.loc[test['Name'] == name, 'title'] = 'Mr'\n\n\ntest.head(10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"95146228ee6c1459f778e86fa516d512c0488ea7"},"cell_type":"markdown","source":" Remove Unneccesary Features and Encode Categorical"},{"metadata":{"trusted":true,"_uuid":"ae4c4e04b7c805e90a15463c62d59d919410382e"},"cell_type":"code","source":"train = pd.get_dummies(train, columns=[\"Sex\", \"Embarked\", \"title\"])\ntest = pd.get_dummies(test, columns=[\"Sex\", \"Embarked\", \"title\"])\n\ntrain = train.drop(['Name','PassengerId', 'Ticket', 'Cabin', 'Fare', 'SibSp'], axis = 1)\ntest = test.drop(['Name','PassengerId', 'Ticket', 'Cabin', 'Fare', 'SibSp'], axis = 1)\n\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"## Quick EDA Visuals\nLets do just a little bit of EDA with Seaborn:"},{"metadata":{"trusted":true,"_uuid":"1ee62752530353b4bf998a35d52713723719371d"},"cell_type":"code","source":"#COR MATRIX OF numerical vars\nplt.figure(figsize=(14,12))\nplt.title('Corelation Matrix', size=12)\nsns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=plt.cm.RdBu, linecolor='white', annot=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"877322bde341652821a9faee78a51ffdc453ce25"},"cell_type":"code","source":"#Family size histo\nsns.distplot(train['family_size'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dad9e54a24e6d0efc9a0556d1fecd0775e442d52"},"cell_type":"code","source":"#boxplot of family size and survival\nsns.boxplot(\"Survived\", y=\"family_size\", data = train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3a84ac1855f7ab67b09877ba3ddcd44ec6b014ad"},"cell_type":"code","source":"#Fare to Age relationship?\nsns.lmplot('fare_std', 'Age', data = train, \n           fit_reg=False,scatter_kws={\"marker\": \"D\", \"s\": 20}) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8c4c1fb5c78027bf4d0e6f2f02a08f98b64dc0ac"},"cell_type":"code","source":"#boxplot of age and survival\nsns.boxplot(\"Survived\", \"Age\", data = train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"844338d035ed71df3c7d818746d88898c2be3c07"},"cell_type":"code","source":"#bar chart of age and survival\nsns.lmplot(\"Survived\", \"fare_std\", data = train, fit_reg=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"01d76701bd2db0aafaddb198980f088459432ef4"},"cell_type":"markdown","source":"## Classifing with XGBoost\n\nI'll be comparing XG to AdaBoost, GradientBoost, RandomForest, andddd maybe a SVC or something else.\n\nFirst off to split the train data into a train/test for cross validation testing"},{"metadata":{"trusted":true,"_uuid":"f030b10a237f80b7c98aeda97bc72f36c892552e"},"cell_type":"code","source":"X = train.drop(['Survived'], axis = 1)\ny = train['Survived']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1b88ff069acd42afff746905113b7f54f593e4d0"},"cell_type":"markdown","source":"Alright, first letsfirst try some more traditional models.. AKA: Random Forest and Standard Tree\n"},{"metadata":{"trusted":true,"_uuid":"c5e4f1822d2ab414aa3d5b18c11bdec504761da7"},"cell_type":"code","source":"#Random Forest Setup\nranfor = RandomForestClassifier()\nparameters = {'n_estimators':[10,50,100], 'random_state': [42, 138], \\\n              'max_features': ['auto', 'log2', 'sqrt']}\nranfor_clf = GridSearchCV(ranfor, parameters)\nranfor_clf.fit(X_train, y_train)\n\n\n'''CROSS VALIDATE'''\ncv_results = cross_validate(ranfor_clf, X_train, y_train)\ncv_results['test_score']  \n\ny_pred = ranfor_clf.predict(X_test)\nprint(accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5233cb663775c6be254deb3ea752f814b6538644","trusted":true},"cell_type":"code","source":"##Decision Tree Go\ndt = DecisionTreeClassifier()\nparameters = {'random_state': [42, 138],'max_features': ['auto', 'log2', 'sqrt']}\ndt_clf = GridSearchCV(dt, parameters)\ndt_clf.fit(X_train, y_train)\n\ny_pred = dt_clf.predict(X_test)\nprint(accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"551d4f1d55539b129f01f0c394ea95e60a74ad4f"},"cell_type":"markdown","source":"At 81% with a random forest.. I'm stoked as this is already better than my last attempt.  Lets keep pushing on with it and give some boosting models  a go."},{"metadata":{"trusted":true,"_uuid":"c7acfab0901ef6b4e3f7a80cda0ce74db900c1d3"},"cell_type":"code","source":"ada = AdaBoostClassifier(base_estimator = DecisionTreeClassifier())\nparameters = {'n_estimators':[10,50,100], 'random_state': [42, 138], 'learning_rate': [0.1, 0.5, 0.8, 1.0]}\nada_clf = GridSearchCV(ada, parameters)\nada_clf.fit(X_train, y_train)\n\ncv_results = cross_validate(ada_clf, X_train, y_train)\ncv_results['test_score']  \n\ny_pred = ada_clf.predict(X_test)\nprint(accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a60599e647649dd945b5076f7e0374e9816a7d9e"},"cell_type":"code","source":"gradBoost = GradientBoostingClassifier()\nparameters = {'n_estimators':[10,50,100], 'random_state': [42, 138], 'learning_rate': [0.1, 0.5, 0.8, 1.0], \\\n             'loss' : ['deviance', 'exponential']}\ngb_clf = GridSearchCV(gradBoost, parameters)\ngb_clf.fit(X_train, y_train)\n\ncv_results = cross_validate(gb_clf, X_train, y_train)\ncv_results['test_score']  \n\ny_pred = gb_clf.predict(X_test)\nprint(accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a88cc3c96c8f2e2da39d25a117bf8e835b9d7362"},"cell_type":"markdown","source":"![](http://)So GradientBoosting has given me the best score at 82% so far..  lets finally make our way to XG:"},{"metadata":{"trusted":true,"_uuid":"0214ab60c649ef5c066c51583b69c3cdc7d7c845"},"cell_type":"code","source":"xg = xgboost.XGBClassifier(max_depth = 3, n_estimators = 600, learning_rate = 0.05)\nxg.fit(X_train, y_train)\n\ncv_results = cross_validate(xg, X_train, y_train)\ncv_results['test_score']  \n\ny_pred = xg.predict(X_test)\nprint(accuracy_score(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d2a992d3aa35f61e1f458a4b4d8103450e14e39e"},"cell_type":"code","source":"'''Confusion Matrix'''\ny_pred = xg.predict(X_test)\n# TN, FP, FN, TP\nconfusion_matrix(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"080f0a46ca43551ce1881bf184709fbbed8f8ac4"},"cell_type":"markdown","source":"So.. A liiiittttleee better than the Gradient boost. I'm happy with it for a quick project like this.  Lets write it out and submit."},{"metadata":{"trusted":true,"_uuid":"2c17fa340b60a5a7cffaaad6e8c397339bfcb860"},"cell_type":"code","source":"predictions = xg.predict(test)\n\nsub['Survived'] = predictions\nsub.to_csv(\"first_submission_xgb.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}