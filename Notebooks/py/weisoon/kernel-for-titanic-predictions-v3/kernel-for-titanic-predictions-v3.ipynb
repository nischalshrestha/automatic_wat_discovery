{"cells":[{"metadata":{"_uuid":"d37584923eb6503a1d378b528853a891feb20b96"},"cell_type":"markdown","source":"<font size=5>**Welcome to my notebook**</font>. \n\nTitantic prediction is  my first attempt in Kaggle to train my python skill and enhance my data science knowledge. In this notebook, I am going to break down my code according to the methodology that I adopted for this Titantic prediction.\n\nThis is a live document as I see that there is still room for improvement in my tuning and coding which I will update accordingly here.\n\n    Code version: v3\n    Prediction score: 0.78468 (about 78.5% accuracy)"},{"metadata":{"_uuid":"31ada1b2ab602333fc13f793e4d587d6d9e5e6ae"},"cell_type":"markdown","source":"<font size=5>**Data Science Methodology**</font>. \n  \n\n**1. Define the problem**\n\n**2. Collect the data **\n\n**3. Perform exploratory data analysis and feature engineering**\n\n**4. Data model selection**\n\n**5. Validate and implement data model**\n\n**6. Submitting results**"},{"metadata":{"_uuid":"2c3624ddd1a67105dfe3340891cb1e8914398638"},"cell_type":"markdown","source":"**1. Define the problem**\n\n* To predict if a passenger survived the sinking of the Titanic or not.\nRequired to indicate 1 (= Survived), or 0 = (Not survived) for each passenger in test data."},{"metadata":{"_uuid":"ac55906d6469b8b36ea2d4aff69b21410791d1ba"},"cell_type":"markdown","source":"**2. Collect the data**\n\n* Importing libraries (some libraries were not required for this version but imported for future tuning)\n* Loading train and test data given by Kaggle\n"},{"metadata":{"trusted":true,"_uuid":"7b0f6960dfdb649acf576877509d64f273e90a1e","collapsed":true},"cell_type":"code","source":"# Load data wrangling libraries\nimport pandas as pd # library for data manipulation and analysis\nimport numpy as np  # library for scientific computing\nimport re           # regular expression operations\n\n# Load visualization libraries\nimport matplotlib.pyplot as plt # library for plotting scientific and publication-ready visualization\n# enable inline backend usage with IPython\n%matplotlib inline\nimport plotly.offline as py     # library for offline composing, editing, and sharing interactive data visualization\npy.init_notebook_mode(connected=True) # initiate the Plotly Notebook mode for offline plot\nimport plotly.tools as tls # module that communicates with plotly \nimport plotly.graph_objs as go # module contains all of the class definitions for the object graph objects\nfrom collections import Counter # import dict subclass for counting hashable objects from module implements specialized container datatypes\nimport seaborn as sns  # Visualization library based on matplotlib, provides interface for drawing attractive statistical graphics\n\n# Load machine learning libraries\nimport xgboost as xgb  # Implementation of gradient boosted decision trees designed for speed and performance that is dominative competitive machine learning\nimport sklearn         # Collection of machine learning algorithms\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n                              GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier)\nfrom sklearn.cross_validation import KFold\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score,classification_report, precision_recall_curve, confusion_matrix\n\n# Load python runtime services\nimport warnings # module for issuing warning\nwarnings.filterwarnings('ignore') # warnings filter controls for never print matching warnings\n\n# Load train and test datasets from CSV files\ntrain = pd.read_csv('../input/train.csv')\ntest = pd.read_csv('../input/test.csv')\n\n# Store our passenger ID for easy access\nPassengerId = test['PassengerId']\n\n# Create 3 row by 4 col tuple and unpack into the figure (f) and axes objects (ax)\nf,ax = plt.subplots(3,4,figsize=(20,15))\nsns.countplot('Pclass',data=train,ax=ax[0,0])\nsns.countplot('Sex',data=train,ax=ax[0,1])\nsns.countplot('Embarked',data=train,ax=ax[0,2])\nsns.boxplot(x='Pclass',y='Age',data=train,ax=ax[0,3])\n\nsns.countplot('Pclass',hue='Survived',data=train,ax=ax[1,0],palette='husl')\nsns.countplot('Sex',hue='Survived',data=train,ax=ax[1,1],palette='husl')\nsns.countplot('Embarked',hue='Survived',data=train,ax=ax[1,2],palette='husl')\nsns.distplot(train[train['Survived']==0]['Age'].dropna(),ax=ax[1,3],kde=False,color='r',bins=5)\nsns.distplot(train[train['Survived']==1]['Age'].dropna(),ax=ax[1,3],kde=False,color='g',bins=5)\n\nsns.swarmplot(x='Pclass',y='Fare',hue='Survived',data=train,palette='husl',ax=ax[2,0])\nsns.distplot(train['Fare'].dropna(),ax=ax[2,1],kde=False,color='b')\nsns.countplot('SibSp',hue='Survived',data=train,ax=ax[2,2],palette='husl')\nsns.countplot('Parch',hue='Survived',data=train,ax=ax[2,3],palette='husl')\n\nax[0,0].set_title('Total Passengers by Pclass')\nax[0,1].set_title('Total Passengers by Gender')\nax[0,2].set_title('Total Passengers by Embarked')\nax[0,3].set_title('Age Box Plot By Class')\n\nax[1,0].set_title('Survival Rate by Pclass')\nax[1,1].set_title('Survival Rate by Gender')\nax[1,2].set_title('Survival Rate by Embarked')\nax[1,3].set_title('Survival Rate by Age')\n\nax[2,0].set_title('Survival Rate by Fare and Pclass')\nax[2,1].set_title('Fare Distribution')\nax[2,2].set_title('Survival Rate by SibSp')\nax[2,3].set_title('Survival Rate by Parch')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4b06525e5b65c1723b10bb009a6a60d1fc7a74e8"},"cell_type":"markdown","source":"**3. Perform exploratory data analysis and feature engineering**\n*  Outlier detection (dropped as it didn't improve the overall score)\n*  Filling missing values\n*  Plot graph to look for pattern and correlations in the dataset\n*  Determine features (predictor variables)\n        Title\n        Sex\n        Age\n        Embarked\n        Fare\n        Deck\n        Cabin\n        Family Size"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":false,"_kg_hide-output":true,"scrolled":true},"cell_type":"code","source":"# Outlier detection\ndef detect_outliers(data,n,features):\n    outlier_indices = []\n    # iterate over features (columns)\n    for x in features:\n        Q1 = np.percentile(data[x],25)\n        Q3 = np.percentile(data[x],75)\n        IQR = Q3 - Q1\n        Q1_outlier = Q1 - (1.5 * IQR)\n        Q3_outlier = Q3 + (1.5 * IQR)\n        #determine a list of indices of outliers (data less than Q1_outlier OR more than Q3_outlier) for feature col\n        outlier_list_col = data[(data[x] < Q1_outlier) | (data[x] > Q3_outlier)].index\n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n    \n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    return multiple_outliers  \n\n# detect outliers from Age, SibSp, Parch and Fare\nOutliers_to_drop = detect_outliers(train,2,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"])\ntrain.loc[Outliers_to_drop] # Show the outliers rows\n\n# Drop outliers\n# train = train.drop(Outliers_to_drop, axis = 'index').reset_index(drop=True)\n\n# Descriptive analysis (univariate)\nfull_data = [train, test]\nSurvival = train['Survived']\nSurvival.describe()\n\n# Titles feature\n# Define function to extract titles from passenger names\ndef get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n # If the title exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    return \"\"\nfor dataset in full_data:\n    \n# Create a new feature Title, containing the titles of passenger names\n    dataset['Title'] = dataset['Name'].apply(get_title)\n\nfig, (axis1) = plt.subplots(1,figsize=(18,6))\nsns.barplot(x=\"Title\", y=\"Survived\", data=train, ax=axis1);\n\n# Descriptive analysis (univariate)\nfull_data = [train, test]\nSurvival = train['Survived']\nSurvival.describe()\n\nfor dataset in full_data:\n    dataset['Title'] = dataset['Title'].replace(['Mrs', 'Miss'], 'MM')\n    dataset['Title'] = dataset['Title'].replace(['Dr', 'Major', 'Col'], 'DMC')\n    dataset['Title'] = dataset['Title'].replace(['Don', 'Dona', 'Rev', 'Capt', 'Jonkheer'],'DRCJ')\n    dataset['Title'] = dataset['Title'].replace(['Mme', 'Ms', 'Lady', 'Sir', 'Mlle', 'Countess'],'MMLSMC' )\n    \n    # Mapping titles\n    title_mapping = {\"Mr\": 0, \"MM\": 1, \"Master\":2, \"DMC\": 3, \"DRCJ\": 4, \"MMLSMC\": 5}\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n\ntrain[[\"Title\", \"Survived\"]].groupby(['Title'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n\nfig, (axis1) = plt.subplots(1,figsize=(18,6))\nsns.barplot(x=\"Title\", y=\"Survived\", data=train, ax=axis1);\n\n# delete unnecessary feature from dataset\ntrain.drop('Name', axis=1, inplace=True)\ntest.drop('Name', axis=1, inplace=True)\n\n# Sex Feature\nfor dataset in full_data:# Mapping Gender\n    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int) \n\nfig, (axis1) = plt.subplots(1,figsize=(18,6))\nsns.barplot(x=\"Sex\", y=\"Survived\", data=train, ax=axis1);\n\n# Age Feature\n# fill missing age with median age for each title\nfor dataset in full_data:\n    dataset[\"Age\"].fillna(train.groupby(\"Title\")[\"Age\"].transform(\"median\"), inplace=True)\n    \n# plot distributions of age of passengers who survived or did not survive\na = sns.FacetGrid( train, hue = 'Survived', aspect=4 )\na.map(sns.kdeplot, 'Age', shade= True )\na.set(xlim=(0 , train['Age'].max()))\na.add_legend()        \n\n#facet = sns.FacetGrid(train, hue=\"Survived\",aspect=4)\n#facet.map(sns.kdeplot,'Age',shade= True)\n#facet.set(xlim=(0, train['Age'].max()))\n#facet.add_legend()\n#plt.xlim(43,60)\n\n# Binning\nfor  dataset  in  full_data:\n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0,\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 34), 'Age'] = 1,\n    dataset.loc[(dataset['Age'] > 34) & (dataset['Age'] <= 42), 'Age'] = 2,\n    dataset.loc[(dataset['Age'] > 42) & (dataset['Age'] <= 60), 'Age'] = 3,\n    dataset.loc[ dataset['Age'] > 60, 'Age'] = 4\n    dataset['Age'] = dataset['Age'].astype(int)\n\nfig, (axis1) = plt.subplots(1,figsize=(18,6))\nsns.barplot(x=\"Age\", y=\"Survived\", data=train, ax=axis1);\n\n# Embarked feature\n# filling missing values\nfor dataset in full_data:\n# Replace all NULLS in the Embarked column with majority embark = S \n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n# Mapping Embarked\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n\nfig, (axis1) = plt.subplots(1,figsize=(18,6))\nsns.barplot(x=\"Embarked\", y=\"Survived\", data=train, ax=axis1);\n\n# Fare feature\n# Remove all NULLS in the Fare column and create a new feature Categorical Fare\nfor dataset in full_data:\n    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\n\n# Explore Fare distribution \nfig, (axis1) = plt.subplots(1,figsize=(18,6))\ng = sns.distplot(dataset[\"Fare\"], color=\"m\", label=\"Skewness : %.2f\"%(dataset[\"Fare\"].skew()))\ng = g.legend(loc=\"best\")\n\n# Apply log to Fare to reduce skewness distribution\nfor dataset in full_data:\n    dataset[\"Fare\"] = dataset[\"Fare\"].map(lambda i: np.log(i) if i > 0 else 0)\na4_dims = (20, 6)\nfig, ax = plt.subplots(figsize=a4_dims)\ng = sns.distplot(train[\"Fare\"][train[\"Survived\"] == 0], color=\"r\", label=\"Skewness : %.2f\"%(train[\"Fare\"].skew()), ax=ax)\ng = sns.distplot(train[\"Fare\"][train[\"Survived\"] == 1], color=\"b\", label=\"Skewness : %.2f\"%(train[\"Fare\"].skew()))\ng = g.legend([\"Not Survived\",\"Survived\"])\n\n# Binning\nfor dataset in full_data:\n    dataset.loc[ dataset['Fare'] <= 2.7, 'Fare']      = 0\n    dataset.loc[ dataset['Fare'] > 2.7, 'Fare']       = 1\n    dataset['Fare'] = dataset['Fare'].astype(int)\n\nfig, (axis1) = plt.subplots(1,figsize=(18,6))\nsns.barplot(x=\"Fare\", y=\"Survived\", data=train, ax=axis1);\n\n# Deck feature\nfor dataset in full_data:\n    dataset['Deck'] = dataset['Cabin'].str[:1]\n\nPclass1 = train[train['Pclass']==1]['Deck'].value_counts()\nPclass2 = train[train['Pclass']==2]['Deck'].value_counts()\nPclass3 = train[train['Pclass']==3]['Deck'].value_counts()\ndf = pd.DataFrame([Pclass1, Pclass2, Pclass3])\ndf.index = ['1st class','2nd class', '3rd class']\ndf.plot(kind='bar',stacked=True, figsize=(10,5))\n\ncabin_mapping = {\"A\": 0, \"B\": 1, \"C\": 2, \"D\": 1, \"E\": 1, \"F\": 3, \"G\": 4, \"T\": 1}\nfor dataset in full_data:\n    dataset['Deck'] = dataset['Deck'].map(cabin_mapping)\n    \n# fill missing Fare with median fare for each Pclass\nfor dataset in full_data:\n    dataset[\"Deck\"].fillna(train.groupby(\"Pclass\")[\"Deck\"].transform(\"median\"), inplace=True)\n    dataset['Deck'] = dataset['Deck'].astype(int)\n    \nfig, (axis1) = plt.subplots(1,figsize=(18,6))\nsns.barplot(x=\"Deck\", y=\"Survived\", data=train, ax=axis1);\n\n# Cabin feature\n# Feature that tells whether a passenger had a cabin on the Titanic (O if no cabin number, 1 otherwise)\nfor dataset in full_data:\n    dataset['Cabin'] = dataset[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n\ntrain[[\"Cabin\", \"Survived\"]].groupby(['Cabin'], as_index=False).sum().sort_values(by='Survived', ascending=False)\ntrain[[\"Cabin\", \"Survived\"]].groupby(['Cabin'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n\nfig, (axis1) = plt.subplots(1,figsize=(18,6))\nsns.barplot(x=\"Cabin\", y=\"Survived\", data=train, ax=axis1);\n\n# FamilySize Feature\nfor dataset in full_data:\n# Create new feature FamilySize as a combination of SibSp and Parch\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch']+1\n      \nfig, (axis1) = plt.subplots(1,figsize=(18,6))\nsns.barplot(x=\"FamilySize\", y=\"Survived\", hue=\"Sex\", data=train, ax = axis1);\n\n# Features drop\nfeatures_drop = ['PassengerId', 'SibSp', 'Parch', 'Ticket']\ntrain = train.drop(features_drop, axis = 1)\ntest  = test.drop(features_drop, axis = 1)\n\ntrain_data = train.drop('Survived', axis=1)\ntarget = train['Survived']\n\n# Pearson Correlation Heatmap\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.01, size=15)\nsns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor='white',annot=True)\n\n#Pairplots\ng = sns.pairplot(train[[u'Survived', u'Pclass', u'Sex', u'Age', u'Fare',\n       u'FamilySize', u'Title']], hue='Survived', palette = 'seismic',size=1.2,diag_kind = 'kde',diag_kws=dict(shade=True),plot_kws=dict(s=10) )\ng.set(xticklabels=[])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ace3b01562f1da630d74f42a6515dffd202a4882"},"cell_type":"markdown","source":"**4. Data Model Selection**\n* Comparing the scores of each learning algorithm based on train data\n        kNN\n        Decision Tree\n        Random Forest\n        Naive Bayes\n        Support Vector Machines\n"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"c14962896ae364e235c457847fe7694e8f102739"},"cell_type":"code","source":"# Cross Validation (K-fold)\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nk_fold = KFold(n_splits=10, shuffle=True, random_state=0)\n\n# kNN\nclf = KNeighborsClassifier(n_neighbors = 13)\nscoring = 'accuracy'\nscore = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score)\nprint('kNN =',(round(np.mean(score)*100, 2)))\n\n# Decision Tree\nclf = DecisionTreeClassifier()\nscoring = 'accuracy'\nscore = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score)\nprint('Decision Tree =',(round(np.mean(score)*100, 2)))\n\n# Random Forest\nclf = RandomForestClassifier(n_estimators=13)\nscoring = 'accuracy'\nscore = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score)\nprint('Random Forest =',(round(np.mean(score)*100, 2)))\n\n# Naive Bayes\nclf = GaussianNB()\nscoring = 'accuracy'\nscore = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score)\nprint('Naive Bayes =',(round(np.mean(score)*100, 2)))\n\n# SVM\nclf = SVC()\nscoring = 'accuracy'\nscore = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(score)\nprint('SVC =',(round(np.mean(score)*100, 2)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1f03ab3a0613469d6f8e1ff964e0e220e454006d"},"cell_type":"markdown","source":"**5. Validate and implement data model**\n* Using the best algorithm to validate and implement it on test data.\n"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"38035dcd543b5f9f2a67a6a468aad5ea8c5bae1a"},"cell_type":"code","source":"# Testing\nclf = SVC()\nclf.fit(train_data, target)\nprediction = clf.predict(test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5fe961d1eeae6fa7a524fb49b7f4d902c5cd53b2"},"cell_type":"markdown","source":"**6. Submitting results**"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7d4405eee9d1500906b9eba653de2a1c2a3b25dc"},"cell_type":"code","source":"# Preparing data for Submission\ntest_Survived = pd.Series(prediction, name=\"Survived\")\nSubmission = pd.concat([PassengerId,test_Survived],axis=1)\nSubmission.head(15)\n\nfilename = 'Titanic Predictions 3K.csv'\nSubmission.to_csv(filename,index=False)\nprint('Saved file: ' + filename)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c4c0473738c333c1bc41574211e72b30cc8de3cc"},"cell_type":"markdown","source":"<font size=5>**Credits**</font>. \n\nIt is a fantastic self-learning journey especially with lot of knowledge sharings in this community.\nI will like to give big credits to following enthusiasts for most of the codes which I learnt and applied for my project submission. \n\n* [Titanic, a step-by-step intro to Machine Learning](https://www.kaggle.com/ydalat/titanic-a-step-by-step-intro-to-machine-learning) by Yvon Dalat\n* [Titanic: Machine Learning from Disaster](https://github.com/minsuk-heo/kaggle-titanic/blob/master/titanic-solution.ipynb) by Minsuk Heo"},{"metadata":{"_uuid":"5bb8a65e4cb4a8cf4d05428f0db32bb2080f4de1"},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}