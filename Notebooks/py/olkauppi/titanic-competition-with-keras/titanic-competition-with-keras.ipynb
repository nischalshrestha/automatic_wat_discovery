{"cells":[{"metadata":{"_uuid":"61f51e1633176efcb9ef33b2342b211f05e10808"},"cell_type":"markdown","source":"# Titanic prediction using Keras\n\n"},{"metadata":{"_uuid":"ccc658688a422e500c4de86dcb5c232422fcea0e"},"cell_type":"markdown","source":" I have used in my work the following references:\n\n1. [Kernel](http://https://www.kaggle.com/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy) by ldfreeman3 \n2. Udacity's course:  [Deep Learning A-Zâ„¢: Hands-On Artificial Neural Networks](https://www.udemy.com/deeplearning/)\n"},{"metadata":{"trusted":true,"_uuid":"b1165349825623deefc16d317333d89daffcbbac"},"cell_type":"code","source":"# Imports\nimport numpy as np\nimport pandas as pd\nfrom tensorflow import keras\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import model_selection\nfrom sklearn.model_selection import GridSearchCV\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Load the dataset\ndata_train = pd.read_csv('../input/train.csv')\ndata_test  = pd.read_csv('../input/test.csv')\n\n# Drop unnecessary columns\ndrop_column = ['PassengerId','Cabin', 'Ticket', 'Embarked']\ndata_train.drop(drop_column, axis=1, inplace = True)\n\ndata_cleaner = [data_train, data_test]\n\n# Fill missing values\nfor dataset in data_cleaner:    \n    #complete missing age with median\n    dataset['Age'].fillna(dataset['Age'].median(), inplace = True)\n\n    #complete missing fare with median\n    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)\n\n    #complete missing age with median\n    dataset['Age'].fillna(dataset['Age'].median(), inplace = True)\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"51f3383eae450f113710ce546db1447861e07163"},"cell_type":"code","source":"# Create data\nfor dataset in data_cleaner:    \n    #Discrete variables\n    dataset['FamilySize'] = dataset ['SibSp'] + dataset['Parch'] + 1\n\n    dataset['IsAlone'] = 1 #initialize to yes/1 is alone\n    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0 # now update to no/0 if family size is greater than 1\n\n    #quick and dirty code split title from name: http://www.pythonforbeginners.com/dictionary/python-split\n    dataset['Title'] = dataset['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n\n\n    #Continuous variable bins; qcut vs cut: https://stackoverflow.com/questions/30211923/what-is-the-difference-between-pandas-qcut-and-pandas-cut\n    #Fare Bins/Buckets using qcut or frequency bins: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.qcut.html\n    dataset['FareBin'] = pd.qcut(dataset['Fare'], 4)\n\n    #Age Bins/Buckets using cut or value bins: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html\n    dataset['AgeBin'] = pd.cut(dataset['Age'].astype(int), 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"737ad2cbfe0b54296cd4a2d1d0d058f7081c0ddd"},"cell_type":"code","source":"stat_min = 10 #while small is arbitrary, we'll use the common minimum in statistics: http://nicholasjjackson.com/2012/03/08/sample-size-is-10-a-magic-number/\n\nfor dataset in data_cleaner:    \n    title_names = (dataset['Title'].value_counts() < stat_min) #this will create a true false series with title name as index\n    #apply and lambda functions are quick and dirty code to find and replace with fewer lines of code: https://community.modeanalytics.com/python/tutorial/pandas-groupby-and-python-lambda-functions/\n    dataset['Title'] = dataset['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)\n\n\n#code categorical data\nlabel = LabelEncoder()\nfor dataset in data_cleaner:    \n    dataset['Sex_Code'] = label.fit_transform(dataset['Sex'])\n    dataset['Title_Code'] = label.fit_transform(dataset['Title'])\n    dataset['AgeBin_Code'] = label.fit_transform(dataset['AgeBin'])\n    dataset['FareBin_Code'] = label.fit_transform(dataset['FareBin'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8c235140d95a0d070ef26f199c7e8baa31fc6b8e"},"cell_type":"code","source":"# Y traget\nTarget = ['Survived']\n# X cols\ndata_x_cols = ['Sex','Pclass', 'Title','SibSp', 'Parch', 'Age', 'Fare', 'FamilySize', 'IsAlone'] #pretty name/values for charts\n\n# Convert categorical variable into dummy variables\ndata_x_train = pd.get_dummies(data_train[data_x_cols])\ndata_y_test = pd.get_dummies(data_test[data_x_cols])\n\ndata_x_cols_list = data_x_train.columns.tolist()\n\n# Split data    \ntrain1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data_x_train[data_x_cols_list], data_train[Target], random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"41820342a7083b7d598fa341ad293cf8a1a76d63"},"cell_type":"markdown","source":"## Method for building the model.\n Needed in GridSearch"},{"metadata":{"trusted":true,"_uuid":"d4a9f1b63d657e1eb5320015850ca8cec666b8e3","scrolled":false},"cell_type":"code","source":"def build_classifier(optimizer):\n    classifier = keras.Sequential()\n    classifier.add(keras.layers.Dense(units = 8, kernel_initializer = 'lecun_uniform', activation = 'relu', input_dim = 14))\n    classifier.add(keras.layers.Dropout(rate = 0.1))\n    classifier.add(keras.layers.Dense(units = 8, kernel_initializer = 'lecun_uniform', activation = 'relu'))\n    classifier.add(keras.layers.Dense(units = 1, kernel_initializer = 'lecun_uniform', activation = 'sigmoid'))\n    classifier.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics = ['accuracy'])\n    return classifier","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8b76bfe3942095d65b0740e951e060fbd71dea30"},"cell_type":"markdown","source":"## Tuning the hyper-parameters of an estimator \n\nUsing GridSearchCV\n\n*I have commented this out because it takes too long to run in this notebook.\nYou can run this locally in your own computer and try to tune the parameters.*\n\n```\n# Tuning the ANN\nclassifier = keras.wrappers.scikit_learn.KerasClassifier(build_fn = build_classifier)\n\nparameters = {'batch_size': [25, 32],\n              'epochs': [100, 200],\n              'optimizer': ['adam', 'rmsprop']}\ngrid_search = GridSearchCV(estimator = classifier,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10)\ngrid_search = grid_search.fit(test_x, test_y)\nbest_parameters = grid_search.best_params_\nbest_accuracy = grid_search.best_score_\n\nprint(best_parameters)\nprint(best_accuracy)\n```"},{"metadata":{"_uuid":"7888f103711bfdafea5383cdda1b3401f9ff3af7"},"cell_type":"markdown","source":"## Fitting the model using the best parameters from the part above"},{"metadata":{"trusted":true,"_uuid":"af7e6ea1f3ad22b44d6a2f3b0da55033c93281f4","scrolled":true},"cell_type":"code","source":"classifier = build_classifier(keras.optimizers.RMSprop(lr=0.03))\nclassifier.fit(train1_x, train1_y, batch_size = 15, nb_epoch = 150, validation_data=(test1_x, test1_y))\n\npredictions = classifier.predict_classes(data_y_test)\nids = data_test.PassengerId.copy()\noutput = ids.to_frame()\noutput[\"Survived\"]=predictions\noutput.columns = ['PassengerId', 'Survived']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4cac333992f01544703f750a55d81c3fcecdc648"},"cell_type":"code","source":"# Save output to file for the competition\noutput.to_csv(\"my_submission.csv\",index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}