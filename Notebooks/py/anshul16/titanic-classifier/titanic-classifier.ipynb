{"cells":[{"metadata":{"trusted":true,"_uuid":"08f145c942f88a055ad4cc8e91cf34c1eeeb8dcb"},"cell_type":"code","source":"\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set_style('whitegrid')\n\n#Read the data\ntrain_df=pd.read_csv('../input/train.csv')\ntest_df=pd.read_csv('../input/test.csv')\ndata_df=train_df.append(test_df)\n\n#explore the data\nprint(train_df.shape)\nprint(\"---------------\")\nprint(test_df.shape)\n\nprint(\"\\n Train Data Details\")\ntrain_df.info()\nprint(\"\\n Test Data Details\")\ntest_df.info()\nprint(\"--------------------------\\n\")\nprint(\"Null values correponding to each feature in train data\")\nprint(pd.isnull(train_df).sum())\nprint(\"\\n Null values correponding to each feature in test data\")\nprint(pd.isnull(test_df).sum())","execution_count":1,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"889ec7a407f9a2d1a2eb8f5591a4e59fcd58124c"},"cell_type":"code","source":"# visualize the data\ntrain_df.head(10)","execution_count":62,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"e013680e6b34c31bcacbb54c1cfbf470ce5ba40d"},"cell_type":"code","source":"# drop Ticket, PassengerId, Name\ntrain_df=train_df.drop(['PassengerId','Ticket'], axis=1)\ntest_df=test_df.drop(['Ticket'], axis=1)","execution_count":63,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"db8ca5183057362ba286988ad8b04b6bb54bc09b"},"cell_type":"code","source":"# visualise filtered train data\ntrain_df.head()","execution_count":64,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"88c38ed699b56634cef25f40ffd505850b5d2f64"},"cell_type":"code","source":"# visualize filtered test data\ntest_df.head()","execution_count":65,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"0e58c47334416c93b0bd4a87a5414ae70db8e52a"},"cell_type":"code","source":"# Dealing with missing values in embarked.\n# finding counts of each categorial value in embarked column'\ntrain_df['Embarked'].value_counts()\n\n#Thus filling missing embarked values with 'S' the most frequent values and then visualizing the effect on survival rates.\n\n#train_df['Embarked'].fillna('S')\n\n#Visualizing embarked and survival \nfig, axes = plt.subplots(1,3,figsize=(15,5))\n\n# Plot showing Distribution of Embarked\nsns.countplot(x='Embarked',data=train_df,ax=axes[0])\n\n#Plot Showing Survival rates corresponding to each Embarked category\nsns.countplot(x='Survived',hue='Embarked',data=train_df,ax=axes[1],order=[1,0])\n\n# Calculate Mean survival % based on each embarked category\nembarked_perc=train_df[['Embarked','Survived']].groupby('Embarked',as_index=False).mean()\nprint(embarked_perc)\nsns.barplot(x='Embarked',y='Survived',data=embarked_perc,ax=axes[2])\n\n# From the plots it can be interpreted that 'S' has a lower survival impact as compared to 'C' or 'Q'.\n# drop'S' and consider only 'C' or 'Q' as they have good survival rates.\n# ref-https://en.wikiversity.org/wiki/Dummy_variable_(statistics)\nembark_dummies_train  = pd.get_dummies(train_df['Embarked'])\nembark_dummies_train.drop(['S'], axis=1, inplace=True)\n\nembark_dummies_test = pd.get_dummies(test_df['Embarked'])\nembark_dummies_test.drop(['S'], axis=1, inplace=True)\n\ntrain_df=train_df.join(embark_dummies_train)\ntest_df=test_df.join(embark_dummies_test)\n\ntrain_df.drop(['Embarked'], axis=1,inplace=True)\ntest_df.drop(['Embarked'], axis=1,inplace=True)\n","execution_count":66,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"24f4426ad0fef06dcb8e739fc982299fa0f449d8","collapsed":true},"cell_type":"code","source":"print(train_df.head(10))\nprint(test_df.head(10))","execution_count":67,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"bfff393545349862d8657754ea4beb93409900f3","collapsed":true},"cell_type":"code","source":"# Examining Cabin\n# Cabin has lot of NULL values, thus we need to drop it. \n# Reference (https://analyticsindiamag.com/5-ways-handle-missing-values-machine-learning-datasets/)\n# The below figure indicates more than 77 % data in cabin section of train is missing, thus removing it\nprint(float(train_df['Cabin'].isnull().sum())/891.0)\n\ntrain_df.drop(['Cabin'],inplace=True,axis=1)\ntest_df.drop(['Cabin'],inplace=True,axis=1)","execution_count":68,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3774ad233f81d07edcd9e13fc7476756d7a79666","collapsed":true},"cell_type":"code","source":"print(train_df.head(10))\nprint(test_df.head(10))","execution_count":69,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8bba2f516e0e5d16e8cd4c2c3a70b5fdf4a35b89"},"cell_type":"code","source":"# Dealing with family size\n\n# instead of having two features SibSp and Parch we can have a feature with family of Alone.\n\ntrain_df['Family']=train_df['SibSp']+train_df['Parch']\ntrain_df['Family'].loc[train_df['Family'] > 0] = 1\ntrain_df['Family'].loc[train_df['Family'] == 0] = 0\n\ntest_df['Family']=test_df['SibSp']+test_df['Parch']\ntest_df['Family'].loc[test_df['Family']>0] = 1\ntest_df['Family'].loc[test_df['Family'] == 0] = 0\n\n\n# drop Parch and SibSp\ntrain_df.drop(['SibSp','Parch'],axis=1,inplace=True)\ntest_df.drop(['SibSp','Parch'],axis=1,inplace=True)\n\n# Visulaize Survival rate vs compared to family\nfig, axes = plt.subplots(1,2,figsize=(10,5))\n# Plot to show count of people with or without family\nsns.countplot(x='Family',data=train_df,ax=axes[0],order=[1,0])\n\n# Plot to show mean survival rate of people with or without family\nfamily_mean=train_df[['Family','Survived']].groupby('Family',as_index=False).mean()\nsns.barplot(x='Family',y='Survived',data=family_mean,ax=axes[1],order=[1,0])","execution_count":70,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"facb232d1a4d123bcfac42c15a74237e10237882","collapsed":true},"cell_type":"code","source":"\nprint(train_df.head(10))\nprint(test_df.head(10))","execution_count":71,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"30829ef61cfbf37f32ecffaa7800ede72eb462ff","collapsed":true},"cell_type":"code","source":"# Dealing with Age\ntrain_null_ages=(train_df['Age'].isnull().sum())/891.0\nprint(\"Null Age percentage %.10f\" %train_null_ages)\n# Thus not dropping ages since, filling ages with numbers in the range of mean+std_dev to mean-std_dev \n# This is chosen to keep the generated ages out of oulier section of data\n\n\n\n# for training data\ntrain_age_mean=train_df['Age'].mean()\ntrain_age_std_dev=train_df['Age'].std()\ntrain_null_ages=train_df['Age'].isnull().sum()\n\n# for test data\ntest_age_mean=test_df['Age'].mean()\ntest_age_std_dev=test_df['Age'].std()\ntest_null_ages=test_df['Age'].isnull().sum()\n\n# genetate random ages\ntrain_rand_ages=np.random.randint(train_age_mean-train_age_std_dev,train_age_mean+train_age_std_dev,size=train_null_ages)\ntest_rand_ages=np.random.randint(test_age_mean-test_age_std_dev,test_age_mean+test_age_std_dev,size=test_null_ages)\n\ntrain_df['Age'][np.isnan(train_df['Age'])]=train_rand_ages\ntest_df['Age'][np.isnan(test_df['Age'])]=test_rand_ages\n","execution_count":72,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"fb7114fa3b80a02b8b34d1d8e5830dee6cf9aee9","collapsed":true},"cell_type":"code","source":"print(train_df.head(10))\nprint(test_df.head(10))","execution_count":73,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"5515b2d0d0421d0817bb40bef48ce345ea5871c1","collapsed":true},"cell_type":"code","source":"# Fare\nprint(\"Missing Fare Values in train %d\" %train_df['Fare'].isnull().sum())\nprint(\"Missing Fare Values in test %d\" %test_df['Fare'].isnull().sum())\n\n#Only one value is missing in Fare section of test data. Fill it with median\ntest_df['Fare'].fillna(test_df['Fare'].median(),inplace=True)\n# converting fare values to intervals and assigning them Integer Labels \n# ref-https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/\n#https://stats.idre.ucla.edu/other/mult-pkg/whatstat/what-is-the-difference-between-categorical-ordinal-and-interval-variables/\ntrain_df['CategoricalFare'] = pd.qcut(train_df['Fare'], 4)\ntrain_fare_mean=train_df[['CategoricalFare','Survived']].groupby(['CategoricalFare'],as_index=False).mean()\nprint(train_fare_mean)\n\n# Mapping Fare in train data\ntrain_df.loc[ train_df['Fare'] <= 7.91, 'Fare']= 0\ntrain_df.loc[(train_df['Fare'] > 7.91) & (train_df['Fare'] <= 14.454), 'Fare'] = 1\ntrain_df.loc[(train_df['Fare'] > 14.454) & (train_df['Fare'] <= 31), 'Fare']   = 2\ntrain_df.loc[ train_df['Fare'] > 31, 'Fare'] = 3\ntrain_df['Fare'] = train_df['Fare'].astype(int)\n\n# Mapping Fare in test data\ntest_df.loc[ train_df['Fare'] <= 7.91, 'Fare']= 0\ntest_df.loc[(train_df['Fare'] > 7.91) & (test_df['Fare'] <= 14.454), 'Fare'] = 1\ntest_df.loc[(train_df['Fare'] > 14.454) & (test_df['Fare'] <= 31), 'Fare']   = 2\ntest_df.loc[ train_df['Fare'] > 31, 'Fare'] = 3\ntest_df['Fare'] = test_df['Fare'].astype(int)\n\ntrain_df.drop(['CategoricalFare'],axis=1,inplace=True)\n","execution_count":74,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a6e2720272bec6098f42a1c34e4924eca554d6b3","collapsed":true},"cell_type":"code","source":"print(train_df.head(10))\nprint(test_df.head(10))","execution_count":75,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"5867715c6430803ef996eb4dcbccd913441784d2"},"cell_type":"code","source":"# Pclass\nfig ,(ax1,ax2) = plt.subplots(1,2, figsize=(15,4))\n\nsns.countplot(x='Pclass',data=train_df,ax=ax1)\nPclass_survived_mean=train_df[['Pclass','Survived']].groupby('Pclass',as_index=False).mean()\nprint(Pclass_survived_mean)\nsns.barplot(x=\"Pclass\",y='Survived',data=Pclass_survived_mean,ax=ax2)\n\n# # Since 3rd class has lowest survival average we can drop it and retain remaining.\n\n# Pclass_train_dummies=pd.get_dummies(train_df['Pclass'])\n# Pclass_train_dummies.columns=['C1','C2','C3']\n# Pclass_train_dummies.drop(['C3'],inplace=True,axis=1)\n\n# Pclass_test_dummies=pd.get_dummies(test_df['Pclass'])\n# Pclass_test_dummies.columns=['C1','C2','C3']\n# Pclass_test_dummies.drop(['C3'],inplace=True,axis=1)\n\n# train_df.drop(['Pclass'],inplace=True,axis=1)\n# test_df.drop(['Pclass'],inplace=True,axis=1)\n\n# train_df=train_df.join(Pclass_train_dummies)\n# test_df=test_df.join(Pclass_test_dummies)","execution_count":76,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"683620cbddb5735796e691d75ad8bad2def08e79","collapsed":true},"cell_type":"code","source":"print(train_df.head(10))\nprint(test_df.head(10))","execution_count":77,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d66ab852a8c5d66b77ea21871b47e3fc7a68aa74"},"cell_type":"code","source":"\n\n# Sex\n\n# Sex category has close relation with Age and in all related to Survival.\ntrain_df['Age'] = train_df['Age'].astype(int)\ntest_df['Age']  = test_df['Age'].astype(int)\n\ntrain_age_mean=train_df[['Age','Survived']].groupby(['Age'],as_index=False).mean()\n\nfig, ax=plt.subplots(1,1,figsize=(18,4))\nsns.barplot(x='Age',y='Survived',data=train_age_mean)\n# Here we see that age < 16 have high survival rates, thus we can group sex into three sub categories,\n# We create a new column with 3 categorical values Male, Female and Children.\n\ndef get_person(person):\n    age,sex = person\n    return 'child' if age < 16 else sex\n\ntrain_df['Person']= train_df[['Age','Sex']].apply(get_person,axis=1)\ntest_df['Person']=test_df[['Age','Sex']].apply(get_person,axis=1)\n\nperson_mean_survival=train_df[['Person','Survived']].groupby(['Person'],as_index=False).mean()\nfig, ax=plt.subplots(1,1,figsize=(18,4))\nsns.barplot(x=\"Person\",y=\"Survived\",data=person_mean_survival)\n# We drop the male category as it has the lowest survival rate\n\ntrain_person_dummies=pd.get_dummies(train_df['Person'])\ntrain_person_dummies.drop(['male'],axis=1,inplace=True)\ntrain_df=train_df.join(train_person_dummies)\n\ntest_person_dummies=pd.get_dummies(test_df['Person'])\ntest_person_dummies.drop(['male'],axis=1,inplace=True)\ntest_df=test_df.join(test_person_dummies)\n","execution_count":78,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"00020a1165a015813323fef9f442d1a19d59d294","collapsed":true},"cell_type":"code","source":"train_df.drop(['Sex','Person'],axis=1,inplace=True)\ntest_df.drop(['Sex','Person'],axis=1,inplace=True)\nprint(train_df.head(10))\nprint(test_df.head(10))","execution_count":79,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"cf17a205c4cba60b35a8a4a5edf29b2b352db049","collapsed":true},"cell_type":"code","source":"# Converting Age into intervals and categorical data.\ntrain_df['CategoricalAge']=pd.qcut(train_df['Age'],4)\ntrain_age_mean=train_df[['CategoricalAge','Survived']].groupby(['CategoricalAge'],as_index=False).mean()\nprint(train_age_mean)\n\ntrain_df.loc[ train_df['Age'] <= 21, 'Age'] = 0\ntrain_df.loc[(train_df['Age'] > 21) & (train_df['Age'] <= 28), 'Age'] = 1\ntrain_df.loc[(train_df['Age'] > 28) & (train_df['Age'] <= 37), 'Age'] = 2\ntrain_df.loc[(train_df['Age'] > 37) & (train_df['Age'] <= 80), 'Age'] = 3 \n\ntest_df.loc[test_df['Age'] <= 21, 'Age'] = 0\ntest_df.loc[(test_df['Age'] > 21) & (test_df['Age'] <= 28), 'Age'] = 1\ntest_df.loc[(test_df['Age'] > 28) & (test_df['Age'] <= 37), 'Age'] = 2\ntest_df.loc[(test_df['Age'] > 37) & (test_df['Age'] <= 80), 'Age'] = 3\n\n","execution_count":80,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"d304d6e147e6aad63d9fc919c40e08ba0abc5fa2","collapsed":true},"cell_type":"code","source":"train_df.drop(['CategoricalAge'],inplace=True,axis=1)\nprint(train_df.head(10))\nprint(test_df.head(10))\n","execution_count":81,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e9b4904c2a182d3d7d6e9813698522328f9332ae","collapsed":true},"cell_type":"code","source":"# Processing Name\nimport re\ndef get_title(name):\n    # match characters from [a-zA-Z] followed by .\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    # If the title exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    return \"\"\n\ntrain_df['Title']=train_df['Name'].apply(get_title)\ntest_df['Title']=test_df['Name'].apply(get_title)\n\ntrain_title_survive=train_df[['Title','Survived']].groupby(['Title'],as_index=False).mean()\nprint(train_title_survive)\n\n# Replace titles other than Master,Miss,Mr,Mrs with 'Rare'\ntrain_df['Title'] = train_df['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n                                             'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ntest_df['Title'] = test_df['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n                                             'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\ntrain_df['Title'] = train_df['Title'].replace('Mlle', 'Miss')\ntrain_df['Title'] = train_df['Title'].replace('Ms', 'Miss')\ntrain_df['Title'] = train_df['Title'].replace('Mme', 'Mrs')\n\ntest_df['Title'] = test_df['Title'].replace('Mlle', 'Miss')\ntest_df['Title'] = test_df['Title'].replace('Ms', 'Miss')\ntest_df['Title'] = test_df['Title'].replace('Mme', 'Mrs')\n\n# title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n# train_df['Title'] = train_df['Title'].map(title_mapping)\n# train_df['Title'] = train_df['Title'].fillna(0)\n# test_df['Title'] = test_df['Title'].map(title_mapping)\n# test_df['Title'] = test_df['Title'].fillna(0)\n\ntrain_title_survive_2=train_df[['Title','Survived']].groupby(['Title'],as_index=False).mean()\nprint(train_title_survive_2)\n# dropping Mr. as it has less survival rate and assigning one hot encoding to others.\n\ntrain_title_dummies=pd.get_dummies(train_df['Title'])\ntrain_title_dummies.drop(['Mr'],inplace=True,axis=1)\ntrain_df=train_df.join(train_title_dummies)\n\ntest_title_dummies=pd.get_dummies(test_df['Title'])\ntest_title_dummies.drop(['Mr'],inplace=True,axis=1)\ntest_df=test_df.join(test_title_dummies)\n\ntrain_df.drop(['Name','Title'],axis=1,inplace=True)\ntest_df.drop(['Name','Title'],axis=1,inplace=True)","execution_count":82,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"7b3ab1462bf60f1bec8a8c47e7000eb5af2b74b1","collapsed":true},"cell_type":"code","source":"\nprint(train_df.head(10))\nprint(test_df.head(10))","execution_count":83,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"218eec46ef906e9ff3036d1a0b4e8a8354754d31"},"cell_type":"code","source":"# Split data sets into features and labels\nfeatures_train=train_df.drop(['Survived'],axis=1)\nlabels_train=train_df['Survived']\n\npids=test_df['PassengerId']\nfeatures_test=test_df.drop(['PassengerId'],axis=1)\nsns.countplot(x='Survived',data=train_df)\n\n# Random Forest with K fold cross validation\n# from sklearn.model_selection import cross_val_score,train_test_split\n# from sklearn.ensemble import RandomForestClassifier as rfc\n\n# # n_estimators has been set to 170 by trying out different values.\n# clf_rfc=rfc(n_estimators=170)\n\n# scores=cross_val_score(clf_rfc, features_train, labels_train, cv=7)\n# print(\"For K = %d cross_val_score = %.10f\" %(7,scores.mean()))","execution_count":84,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"0c8d7aba5c405d484f40bd78d38887bf04e6815e"},"cell_type":"code","source":"# Init models and use Stratified K-Fold Cross Val Score\n\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.model_selection import StratifiedShuffleSplit \nfrom sklearn.metrics import f1_score, accuracy_score\n\nmodels_list=[\n    RandomForestClassifier(n_estimators=250),\n    AdaBoostClassifier(RandomForestClassifier(n_estimators=250)),\n    GaussianNB(),\n    KNeighborsClassifier(n_neighbors=3),\n    SVC(),\n    LogisticRegression()\n]\n\nlog_cols = [\"Classifier\", \"F1-Score\"]\nlog = pd.DataFrame(columns=log_cols)\n\nf1_score_dict={}\nn_folds=10\nsss=StratifiedShuffleSplit(n_splits=n_folds,test_size=0.1,random_state=42)\n\nX=features_train.values\ny=labels_train.values\nfor train_index, test_index in sss.split(X, y):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    \n    for model in models_list:\n        # to extract only the name of the class excluding modules.\n        clf_name=model.__class__.__name__\n        model.fit(X_train,y_train)\n        predictions=model.predict(X_test)\n        f1=f1_score(y_test,predictions,\n                          average='binary'\n                         )\n        \n        if clf_name in f1_score_dict:\n            f1_score_dict[clf_name]+=f1\n        else:\n            f1_score_dict[clf_name]=f1\n\nfor clf in f1_score_dict:\n    f1_score_dict[clf] = f1_score_dict[clf] / float(n_folds)\n    log_entry = pd.DataFrame([[clf, f1_score_dict[clf]]], columns=log_cols)\n    log = log.append(log_entry)\n\n\nfig, axis = plt.subplots(1,1,figsize=(15,4))\nplt.xlabel('Classifiers')\nplt.ylabel('F1-Score(Mean)')\nplt.title('Classifiers vs F1-Score')\nlog=log.sort_values('F1-Score')\nsns.barplot(x='F1-Score',y='Classifier',data=log)","execution_count":108,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"f51f49bcc9ac7d5fb5fcbc42766ad8e918fafa09"},"cell_type":"code","source":"clf_final=AdaBoostClassifier(RandomForestClassifier(n_estimators=250))\nclf_final.fit(features_train,labels_train)\npredictions=clf_final.predict(features_test)\nsubmission = pd.DataFrame({\n        \"PassengerId\": pids,\n        \"Survived\": predictions\n    })\nsubmission.to_csv('submission_final.csv', index=False)","execution_count":109,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"4977b9b570e602ea415ffdeb81ef2d87880474eb"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"06cc6857d999ecf10abd49975513a72252ae00aa"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"collapsed":true,"_uuid":"adf5ff508c3ee8f2b48e31c01986b400ed966d4b"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}