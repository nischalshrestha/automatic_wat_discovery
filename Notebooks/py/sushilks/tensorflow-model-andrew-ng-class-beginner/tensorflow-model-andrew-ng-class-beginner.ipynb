{"nbformat_minor": 1, "nbformat": 4, "metadata": {"language_info": {"mimetype": "text/x-python", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "version": "3.6.1", "name": "python"}, "kernelspec": {"language": "python", "display_name": "Python 3", "name": "python3"}}, "cells": [{"execution_count": null, "outputs": [], "source": ["import pandas as pd\n", "import random\n", "from datetime import datetime\n", "import re\n", "import tensorflow as tf\n", "import numpy as np\n", "#from sklearn import metrics, cross_validation\n", "from sklearn.cross_validation import train_test_split\n", "from sklearn.preprocessing import LabelEncoder\n", "from sklearn.metrics import accuracy_score\n", "from tensorflow.contrib import layers\n", "from tensorflow.contrib import learn\n", "random.seed(42)\n", "\n", "test_orig = pd.read_csv(\"../input/test.csv\")\n", "train_orig = pd.read_csv(\"../input/train.csv\")\n", "train = train_orig.copy()\n", "test = test_orig.copy()"], "metadata": {"_uuid": "d37a5eba35d9c8e7d8a36aab174e47a2b964edae", "_cell_guid": "a67e3027-6fa1-42d8-8dfe-2fc5d944491a"}, "cell_type": "code"}, {"execution_count": null, "outputs": [], "source": ["def isNaN(x):\n", "    return x != x\n", "#fileds present in the data \n", "train.columns"], "metadata": {}, "cell_type": "code"}, {"source": ["Data inspection/cleanup\n", "Lets focus on converting all the column data to be numaric, We can see if missing data is going to cause problems after the first iteration is done\n", "In [6]:\n", "\n", "    "], "metadata": {}, "cell_type": "markdown"}, {"execution_count": null, "outputs": [], "source": ["\n", "for key in train.columns:\n", "    print(\"Number of Classes in \", key, \" = \", len(train[key].unique()), \n", "          \" Number of NAN = \" ,sum(isNaN(train[key])))\n"], "metadata": {}, "cell_type": "code"}, {"execution_count": null, "outputs": [], "source": ["# Age has  177 missing\n", "# Cabin has 687 missing\n", "# Embarked 2 missing\n", "#Replace Missing Embarked value, there are only 2\n", "for k in ['Embarked', 'Sex']:\n", "    for tr2 in [train, test]:\n", "        tr2[k] = tr2[k].fillna('N')"], "metadata": {"collapsed": true}, "cell_type": "code"}, {"execution_count": null, "outputs": [], "source": ["# lets check name next \n", "def getPName(st):\n", "    if isNaN(st):\n", "        return st\n", "    dt = st.split(',')\n", "    lastName = dt[0]\n", "    fullName = dt[1].split(' ')\n", "    fullName = list(filter(lambda f:f!='', fullName))\n", "    pn = fullName[0]\n", "    if pn == 'Mlle.':\n", "        pn = 'Miss.'\n", "    elif pn == 'Mme.':\n", "        pn = 'Mrs.'\n", "    elif pn == 'Ms.':\n", "        pn = 'Miss.'\n", "    return pn\n", "def getLName(st):\n", "    if isNaN(st):\n", "        return st\n", "    dt = st.split(',')\n", "    lastName = dt[0]\n", "    return lastName\n", "lnameMap = {}\n", "cnt = 0\n", "for tr2 in [train, test]:\n", "    lname = tr2.apply(lambda r:getLName(r['Name']), axis=1).unique()\n", "    for idx in range(0, len(lname)):\n", "        if not lname[idx] in lnameMap:\n", "            lnameMap[lname[idx]] = cnt\n", "            cnt +=1\n", "for tr2 in [train, test]:\n", "    tr2['LName'] = tr2.apply(lambda r:lnameMap[getLName(r['Name'])], axis=1)\n", "\n", "train['LName_t'] = learn.ops.categorical_variable(train['LName'], \n", "                                                  len(train['LName'].unique()),\n", "                                                 embedding_size=6, name='LName')"], "metadata": {}, "cell_type": "code"}, {"execution_count": null, "outputs": [], "source": ["titleMap = {}\n", "cnt =0\n", "for tr2 in [train, test]:\n", "    titles = tr2.apply(lambda r:getPName(r['Name']), axis=1).unique()\n", "    for idx in range(0, len(titles)):\n", "        if not titles[idx] in titleMap:\n", "            titleMap[titles[idx]] = cnt\n", "            cnt+=1\n", "for tr2 in [train, test]:\n", "    tr2['Titles'] = tr2.apply(lambda r:titleMap[getPName(r['Name'])], axis=1)\n", "print(train['Titles'].unique())\n", "print(titleMap)"], "metadata": {}, "cell_type": "code"}, {"execution_count": null, "outputs": [], "source": ["def single(dt):\n", "    if (dt['SibSp'] == 0) and (dt['Parch'] == 0):\n", "        return 0\n", "    return 1\n", "for tr2 in [train, test]:\n", "    tr2['Pclass_t'] = tf.one_hot(tr2['Pclass'], 3, 1.0, 0.0)\n", "    tr2['Single'] = tr2.apply(lambda r:single(r), axis=1)\n", "\n", "# Create a single column indicating people without kids. (Single) Maybe it will help\n", "# Seperated PClass into one hot encoding"], "metadata": {"collapsed": true}, "cell_type": "code"}, {"execution_count": null, "outputs": [], "source": ["# Age has lot of missing data  (177 entries)\n", "# Let's try to fill them based on average data based on different titles \n", "titleMapAvgAge = {}\n", "for k in titleMap:\n", "    num_valid = sum((train['Titles'] == titleMap[k]) & ~isNaN(train['Age'] ))\n", "    num_nan = sum((train['Titles'] == titleMap[k]) & isNaN(train['Age'] ))\n", "    sum_age   = sum(train.loc[(train['Titles'] == titleMap[k]) & ~isNaN(train['Age']),'Age'])\n", "    if (num_valid != 0):\n", "        avg = sum_age/num_valid\n", "    else:\n", "        avg = 0\n", "    titleMapAvgAge[titleMap[k]] = avg\n", "    print('title=',k, ' NumberOfItems:',num_valid,'/',num_nan, ' Average Age:',avg)\n", "# Plugin all the missing ages based on title \n", "\n", "def plugAge(r):\n", "    if not isNaN(r['Age']):\n", "        return r['Age']\n", "    return titleMapAvgAge[r['Titles']]\n", "train['Age'] = train.apply(lambda r:plugAge(r), axis=1)\n", "test['Age'] = test.apply(lambda r:plugAge(r), axis=1)\n"], "metadata": {}, "cell_type": "code"}, {"source": ["## Select the features to use for training"], "metadata": {}, "cell_type": "markdown"}, {"execution_count": null, "outputs": [], "source": ["#Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n", "#       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked', 'Embarked_t', 'Sex_t',\n", "#       'Titles', 'LName', 'Pclass_t', 'Single', 'Age_t', 'SibSp_t', 'Parch_t',\n", "#       'Fare_t', 'LName_t'],\n", "\n", "# Features to use \n", "\n", "\n", "featureListKey =[ 'Embarked', 'Sex', 'SibSp', 'Parch', 'Titles', 'LName', 'Age', 'Fare', \n", "                 'Pclass','Single']\n", "                 \n"], "metadata": {}, "cell_type": "code"}, {"source": ["## Prepeare the Train, Test, and Dev Sets "], "metadata": {}, "cell_type": "markdown"}, {"execution_count": null, "outputs": [], "source": ["fullDt = pd.concat([train, test])\n", "X = train[featureListKey].copy()\n", "y = train['Survived'].copy()\n", "\n", "#normalize Age\n", "for key in ['Age', 'Fare']:\n", "    X[key]=((X[key] - X[key].mean())/(X[key].max() - X[key].min()))\n", "X['Pclass'] = X['Pclass'] - 1\n", "\n", "XTrain, XTest_, yTrain, yTest_ = train_test_split(X, y, test_size=0.2, random_state=42)\n", "XTest, XDev, yTest, yDev = train_test_split(X, y, test_size=0.5, random_state=42)\n", "\n", "XEval=test[featureListKey].copy()\n", "for key in ['Age', 'Fare']:\n", "    XEval[key]=((XEval[key] - XEval[key].mean())/(XEval[key].max() - XEval[key].min()))\n", "XEval['Pclass'] = XEval['Pclass'] - 1"], "metadata": {"collapsed": true}, "cell_type": "code"}, {"source": ["## Tensorflow Data setup"], "metadata": {}, "cell_type": "markdown"}, {"execution_count": null, "outputs": [], "source": ["f_embarked = tf.feature_column.indicator_column(\n", "              tf.feature_column.categorical_column_with_vocabulary_list('Embarked',\n", "                        train['Embarked'].unique()))\n", "f_sex = tf.feature_column.indicator_column(\n", "              tf.feature_column.categorical_column_with_vocabulary_list('Sex',\n", "                        train['Sex'].unique()))\n", "#f_sex = tf.feature_column.numeric_column(key='Sex')\n", "f_sibsp = tf.feature_column.numeric_column(key='SibSp')\n", "f_parch = tf.feature_column.numeric_column(key='Parch')\n", "f_age = tf.feature_column.numeric_column(key='Age')\n", "f_titles = tf.feature_column.numeric_column(key='Titles')\n", "f_single = tf.feature_column.indicator_column(\n", "            tf.feature_column.categorical_column_with_identity('Single', 2))\n", "#f_fare = tf.feature_column.numeric_column(key='Fare')\n", "f_pclass = tf.feature_column.embedding_column(\n", "    tf.feature_column.categorical_column_with_identity(key='Pclass',num_buckets=3), 3)\n", "#f_titles = tf.feature_column.embedding_column(\n", "#    tf.feature_column.categorical_column_with_hash_bucket(key='Titles',hash_bucket_size=20), 10)\n", "f_columns= [ f_embarked, f_sex, f_sibsp, f_parch, f_pclass, f_age, f_titles, f_single]\n", "#f_columns= [ f_embarked, f_sex]\n", "train_input_fn = tf.estimator.inputs.pandas_input_fn(x=XTrain, \n", "                                                y=yTrain,\n", "                                                batch_size=100,\n", "                                                num_epochs=None,\n", "                                                shuffle=True)\n", "train_accuracy_input_fn = tf.estimator.inputs.pandas_input_fn(x=XTrain, \n", "                                                y=yTrain,\n", "                                                num_epochs=1,\n", "                                                shuffle=False)\n", "dev_input_fn = tf.estimator.inputs.pandas_input_fn(x=XDev, \n", "                                                y=yDev,\n", "                                                num_epochs=1,\n", "                                                shuffle=False)\n", "test_input_fn = tf.estimator.inputs.pandas_input_fn(x=XTest, \n", "                                                y=yTest,\n", "                                                num_epochs=1,\n", "                                                shuffle=False)\n"], "metadata": {"collapsed": true}, "cell_type": "code"}, {"execution_count": null, "outputs": [], "source": ["def model_fn(features, labels, mode, params):\n", "    is_training = True\n", "    if mode == tf.estimator.ModeKeys.PREDICT:\n", "        is_training = False\n", "    input_layer = tf.feature_column.input_layer(features=features, feature_columns=params['fc'])\n", "    first_hidden_layer_ = tf.layers.dense(input_layer,        40, activation=tf.nn.relu)       #40\n", "    first_hidden_layer  = tf.layers.dropout(first_hidden_layer_, rate=params['dropout'], training=is_training)\n", "    second_hidden_layer_= tf.layers.dense(first_hidden_layer, 20, activation=tf.nn.relu)#20\n", "    second_hidden_layer  = tf.layers.dropout(second_hidden_layer_, rate=params['dropout'], training=is_training)\n", "    # Connect the output layer to second hidden layer (no activation fn)\n", "    output_layer = tf.layers.dense(second_hidden_layer, 1) # , activation=tf.nn.sigmoid)\n", "    # Reshape output layer to 1-dim Tensor to return predictions\n", "    predictions = tf.reshape(output_layer, [-1])\n", "    predictions_threshold = tf.cast(tf.greater(predictions,0.5), tf.float32)\n", "    # Calculate loss using mean squared error\n", "    if not is_training:\n", "        return tf.estimator.EstimatorSpec(\n", "            mode = mode,\n", "            predictions={'Survived':predictions_threshold}\n", "        )\n", "    loss = tf.losses.mean_squared_error(labels, predictions)\n", "    #    loss = tf.losses.log_loss(labels, predictions)\n", "    #    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n", "    #                    labels=tf.cast(labels, tf.float32), \n", "    #                    logits=predictions))\n", "    optimizer = tf.train.FtrlOptimizer(\n", "                learning_rate=params['learning_rate'],\n", "                l2_regularization_strength=params['l2_regularization'])\n", "\n", "    train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())\n", "    eval_metric_ops = {\n", "      \"rmse\": tf.metrics.root_mean_squared_error(\n", "          tf.cast(labels, tf.float32), predictions),\n", "      \"accuracy\":tf.metrics.accuracy(labels, predictions_threshold)\n", "\n", "    }\n", "\n", "    # Provide an estimator spec for `ModeKeys.EVAL` and `ModeKeys.TRAIN` modes.\n", "    return tf.estimator.EstimatorSpec(\n", "      mode=mode,\n", "      loss=loss,\n", "      train_op=train_op,\n", "      eval_metric_ops=eval_metric_ops)"], "metadata": {"collapsed": true}, "cell_type": "code"}, {"execution_count": null, "outputs": [], "source": ["def do_training(lr, drp, l2r, fc, steps):\n", "    random.seed(42)\n", "    model_params={\"learning_rate\": lr, 'l2_regularization':l2r, 'dropout':drp, 'fc': f_columns}\n", "    my_nn = tf.estimator.Estimator(model_fn=model_fn, params=model_params)\n", "    r = my_nn.train(input_fn=train_input_fn, steps=steps )\n", "    evtrain=my_nn.evaluate(input_fn=train_accuracy_input_fn)\n", "    evdev=my_nn.evaluate(input_fn=dev_input_fn)\n", "    evtest=my_nn.evaluate(input_fn=test_input_fn)\n", "    print(r)\n", "    print(\"Loss:%s\" %evtest[\"loss\"], \" Accuracy Test:\", evtest['accuracy'],\" Accuracy Train:\", evtrain['accuracy'])\n", "    return {\n", "        'learningRate':lr,\n", "        'dropout': drp,\n", "        'l2Reg': l2r,\n", "        'AccuracyDev': evdev['accuracy'],\n", "        'AccuracyTest': evtest['accuracy'],\n", "        'AccuracyTrain':evtrain['accuracy'],\n", "        'Loss': evtest['loss'],\n", "        'Steps': steps\n", "    }"], "metadata": {"collapsed": true}, "cell_type": "code"}, {"source": ["## Find the value to use for Learning rate"], "metadata": {}, "cell_type": "markdown"}, {"execution_count": null, "outputs": [], "source": ["acc={}\n", "acnt = 0\n", "samp = [ # lr, l2, drp, steps\n", "    (0.01, 0.1, .1, 100)\n", "]\n", "l2r = 0.1 # 0.2\n", "drp = 0.1 # 0.2\n", "lr = 0.2\n", "\n", "# Pick learning rate\n", "acc={}\n", "for lr in [0.1,0.15,0.2, 0.25]:\n", "    acc[acnt] = do_training(lr, drp, l2r, f_columns, 500)\n", "    acnt +=1\n", "for idx in acc:\n", "    print('Learning Rate:', acc[idx]['learningRate'], 'Train Dev Accuracy:',acc[idx]['AccuracyDev'], ' TrainAccuracy:', acc[idx]['AccuracyTrain'] )\n", "#Learning Rate: 0.2 Train Dev Accuracy: 0.836323  TrainAccuracy: 0.825843\n", "#Learning Rate: 0.15 Train Dev Accuracy: 0.831839  TrainAccuracy: 0.831461\n"], "metadata": {}, "cell_type": "code"}, {"source": ["## Lets pick learning rate of 0.15, Find dropout next"], "metadata": {}, "cell_type": "markdown"}, {"execution_count": null, "outputs": [], "source": ["l2r = 0.15\n", "drp = 0.1 # 0.2\n", "lr = 0.2\n", "\n", "# Pick dropout\n", "acc={}\n", "for drp in [0.05,0.1,0.15, 0.2, 0.25]:\n", "    acc[acnt] = do_training(lr, drp, l2r, f_columns, 500)\n", "    acnt +=1\n", "for idx in acc:\n", "    print('Dropout:', acc[idx]['dropout'], 'Train Dev Accuracy:',acc[idx]['AccuracyDev'], ' TrainAccuracy:', acc[idx]['AccuracyTrain'] )\n", "#Dropout: 0.1 Train Dev Accuracy: 0.829596  TrainAccuracy: 0.831461   \n", "#Dropout: 0.05 Train Dev Accuracy: 0.831839  TrainAccuracy: 0.828652"], "metadata": {}, "cell_type": "code"}, {"source": ["## Lets use 0.15 for dropout, Find Regularization next ****"], "metadata": {}, "cell_type": "markdown"}, {"execution_count": null, "outputs": [], "source": ["l2r = 0.15\n", "drp = 0.15 # 0.2\n", "lr = 0.2\n", "\n", "# Pick dropout\n", "acc={}\n", "for l2r in [0.05,0.1,0.15, 0.2, 0.25]:\n", "    acc[acnt] = do_training(lr, drp, l2r, f_columns, 500)\n", "    acnt +=1\n", "for idx in acc:\n", "    print('l2Regularization:', acc[idx]['l2Reg'], 'Train Dev Accuracy:',acc[idx]['AccuracyDev'], ' TrainAccuracy:', acc[idx]['AccuracyTrain'] )\n", "#l2Regularization: 0.15 Train Dev Accuracy: 0.82287  TrainAccuracy: 0.825843  \n", "#l2Regularization: 0.15 Train Dev Accuracy: 0.820628  TrainAccuracy: 0.817416"], "metadata": {}, "cell_type": "code"}, {"source": ["## Lets use Regalurization value of 0.15, Find out how many steps to run for Next\n"], "metadata": {}, "cell_type": "markdown"}, {"execution_count": null, "outputs": [], "source": ["l2r = 0.15\n", "drp = 0.15 # 0.2\n", "lr = 0.15\n", "# Pick steps\n", "acc={}\n", "for stp in [ 2500, 3000, 3500, 4000]:\n", "    acc[acnt] = do_training(lr, drp, l2r, f_columns, stp)\n", "    acnt +=1\n", "\n", "for idx in acc:\n", "    print('idx:', idx, 'steps:', acc[idx]['Steps'], 'Train Dev Accuracy:',\n", "          acc[idx]['AccuracyDev'], ' TestAccuracy',acc[idx]['AccuracyTest'],' TrainAccuracy:', acc[idx]['AccuracyTrain'] )\n"], "metadata": {}, "cell_type": "code"}, {"source": ["## Let's pick 4000 for number of steps to train for , Now we have all the parameters"], "metadata": {}, "cell_type": "markdown"}, {"execution_count": null, "outputs": [], "source": ["#final parameter selection\n", "l2r = 0.15\n", "drp = 0.15 # 0.2\n", "lr = 0.15\n", "steps = 4000"], "metadata": {"collapsed": true}, "cell_type": "code"}, {"execution_count": null, "outputs": [], "source": ["t = datetime.now()\n", "print(\"Running with random Seed:\", t)\n", "random.seed(t)\n", "model_params={\"learning_rate\": lr, 'l2_regularization':l2r, 'dropout':drp, 'fc': f_columns}\n", "my_nn = tf.estimator.Estimator(model_fn=model_fn, params=model_params)\n", "r = my_nn.train(input_fn=train_input_fn, steps=steps )\n", "evtrain=my_nn.evaluate(input_fn=train_accuracy_input_fn)\n", "evdev=my_nn.evaluate(input_fn=dev_input_fn)\n", "evtest=my_nn.evaluate(input_fn=test_input_fn)\n"], "metadata": {}, "cell_type": "code"}, {"execution_count": null, "outputs": [], "source": ["print('Train Dev Accuracy:',evdev['accuracy'],\n", "      ' TestAccuracy',evtest['accuracy'],' TrainAccuracy:', evtrain['accuracy'] )\n", "# Train Dev Accuracy: 0.831839  TestAccuracy 0.85618  TrainAccuracy: 0.860955"], "metadata": {}, "cell_type": "code"}, {"execution_count": null, "outputs": [], "source": ["pred_input_fn = tf.estimator.inputs.pandas_input_fn(x=XEval, \n", "                                                num_epochs=1,\n", "                                                shuffle=False)\n", "pred = my_nn.predict(input_fn=pred_input_fn)\n", "v=[]\n", "for i, p in enumerate(pred):\n", "    v.append(int(p['Survived']))\n", "result = pd.DataFrame({'PassengerId':test['PassengerId'],'Survived':v})\n", "result.to_csv('result-d7.csv', index=False)"], "metadata": {}, "cell_type": "code"}, {"source": ["## This results should get close to .79 accuracy. "], "metadata": {}, "cell_type": "markdown"}, {"source": ["This implementation is based on some of the concepts form the andrew ng class,  The dataset in this case is too small to apply all the technicques so most likely a larget dataset will be better for practicing what was shown here. \n"], "metadata": {}, "cell_type": "markdown"}, {"execution_count": null, "outputs": [], "source": [], "metadata": {"collapsed": true}, "cell_type": "code"}]}