{"cells":[{"metadata":{"_uuid":"8a30f62bc02df1c50e45a60f739639528c6f4d39"},"cell_type":"markdown","source":"# Titanic: Machine Learning from Disaster (Kaggle)\nhttps://www.kaggle.com/c/titanic\n\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\nIn this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\n\n### Step 1 - Load Necessary Libraries"},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"9bda9f3d71810e3ed0a4db747c738b91fd35abf7"},"cell_type":"code","source":"# Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# import relevant modules\n%matplotlib inline\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport sklearn\n\n# Settings\nsns.set(style=\"darkgrid\")\nplt.rcParams['axes.labelsize'] = 14\nplt.rcParams['xtick.labelsize'] = 12\nplt.rcParams['ytick.labelsize'] = 12\nplt.rcParams['figure.figsize'] = (16, 6)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4f6a130b5034eb1170d5fa18d27f6397bb348483"},"cell_type":"markdown","source":"## Data Description: \n- survival: Survival (0 = No; 1 = Yes)\n- pclass: Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd)\n- name: Name\n- sex: Sex\n- age: Age\n- sibsp: Number of Siblings/Spouses Aboard\n- parch: Number of Parents/Children Aboard\n- ticket: Ticket Number\n- fare: Passenger Fare\n- cabin: Cabin\n- embarked: Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)\n- Special Notes:\n\nFeature Description:\n- Pclass is a proxy for socio-economic status (SES) 1st ~ Upper; 2nd ~ Middle; 3rd ~ Lower\n- Age is in Years; Fractional if Age less than One (1) If the Age is Estimated, it is in the form xx.5\n- With respect to the family relation variables (i.e. sibsp and parch) some relations were ignored. The following are the definitions used for sibsp and parch.\n    - Sibling: Brother, Sister, Stepbrother, or Stepsister of Passenger Aboard Titanic\n    - Spouse: Husband or Wife of Passenger Aboard Titanic (Mistresses and Fiances Ignored)\n    - Parent: Mother or Father of Passenger Aboard Titanic\n    - Child: Son, Daughter, Stepson, or Stepdaughter of Passenger Aboard Titanic\n    - Other family relatives excluded from this study include cousins, nephews/nieces, aunts/uncles, and in-laws. Some children travelled only with a nanny, therefore parch=0 for them. As well, some travelled with very close friends or neighbors in a village, however, the definitions do not support such relations.\n\n### Step 2 - Load Titanic Train & Test Dataset"},{"metadata":{"trusted":true,"_uuid":"1f0d792fe2552adb5c25f5c839a7e483966cde08"},"cell_type":"code","source":"# Load Titanic train dataset\n#train_url = 'https://raw.githubusercontent.com/anup-jana/Python-Machine-Learning/master/Datasets/kaggle_titanic_train.csv'\ntitanic_train_org = pd.read_csv('../input/train.csv')\n\n# Load Titanic test dataset\n#test_url = 'https://raw.githubusercontent.com/anup-jana/Python-Machine-Learning/master/Datasets/kaggle_titanic_test.csv'\ntitanic_test_org = pd.read_csv('../input/test.csv')\n\n# train set dimension\nprint('Train dataset dimension: {} rows, {} columns'.format(titanic_train_org.shape[0], titanic_train_org.shape[1]))\n\n# test set dimension\nprint('Test dataset dimension: {} rows, {} columns'.format(titanic_test_org.shape[0], titanic_test_org.shape[1]))\n\n# View train data\ntitanic_train_org.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ed6f139e3d12fc9bab317179970f059a9a43a7ef"},"cell_type":"code","source":"# Metadata of Titatnic dataset\ntitanic_train_org.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"486af49c698bfee8b26308d9ba231b6769ec8c4b"},"cell_type":"code","source":"# Descriptive statistics\ntitanic_train_org.describe(include='all')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"11bcfcaf6c0af2fa6e67404a711173ed6b9d832b"},"cell_type":"markdown","source":"### Step 3 - Feature Exploratory Data Analysis [EDA]\nThere are missing values in Age, Cabin and Embarked. We need to handle the null values and also let's see how various features are corelate to Survived response variable."},{"metadata":{"trusted":false,"_uuid":"f325601402a50fc7f3f4d8d99016a0ad0cfaf064"},"cell_type":"code","source":"titanic_data = titanic_train_org.copy() # Copy original titanic dataset for feature changes\n\n# Let's add another variable for opposite of Survived for counting purpose\ntitanic_data['Died'] = 1 - titanic_data['Survived']\ntitanic_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"26da8921216b48f5d7c978dadbbfd72b10059cb8"},"cell_type":"code","source":"# How many Survived & Died based Pclass feature\nprint(titanic_data.groupby('Pclass').agg('sum')[['Survived', 'Died']])\ntitanic_data.groupby('Pclass').agg('sum')[['Survived', 'Died']].plot(kind='bar', stacked=True, colors=['green', 'red']);","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"cb50b8ad21e2ee96a725bad312b1f5db309ccc16"},"cell_type":"code","source":"# How many Survived & Died based Sex feature\nprint(titanic_data.groupby('Sex').agg('sum')[['Survived', 'Died']])\ntitanic_data.groupby('Sex').agg('sum')[['Survived', 'Died']].plot(kind='bar', stacked=True, colors=['green', 'red']);","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"3a377ac8314875566ce4dfa06ec77bc1a098bf4e"},"cell_type":"code","source":"# How many Survived & Died based on Family size derived from SibSp & Parch feature\ntitanic_data['FamilySize'] = titanic_data['SibSp'] + titanic_data['Parch'] + 1\nprint(titanic_data.groupby('FamilySize').agg('sum')[['Survived', 'Died']])\ntitanic_data.groupby('FamilySize').agg('sum')[['Survived', 'Died']].plot(kind='bar', stacked=True, colors=['green', 'red']);","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"7f4f1a0c729076c19ff9b0624e58b1bbda210e90"},"cell_type":"code","source":"# How many Survived & Died based on whether passenger is Alone or not derived from Family size\ntitanic_data['IsAlone'] = np.where(titanic_data.FamilySize == 1, 1, 0)\nprint(titanic_data.groupby('IsAlone').agg('sum')[['Survived', 'Died']])\ntitanic_data.groupby('IsAlone').agg('sum')[['Survived', 'Died']].plot(kind='bar', stacked=True, colors=['green', 'red']);","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"38bfc08ceba94c3efce3cf5e657009742d597d77"},"cell_type":"code","source":"# find most frequent Embarked value and store in variable\nmost_embarked = titanic_data.Embarked.value_counts().index[0]\n\n# fill NaN with most_embarked value\ntitanic_data.Embarked = titanic_data.Embarked.fillna(most_embarked)\n\n# How many Survived & Died based on Embarked feature\nprint(titanic_data.groupby('Embarked').agg('sum')[['Survived', 'Died']])\ntitanic_data.groupby('Embarked').agg('sum')[['Survived', 'Died']].plot(kind='bar', stacked=True, colors=['green', 'red']);","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"7416a8782c53e016b48bdf7105c7614a987ec01e"},"cell_type":"code","source":"# How many Survived & Died based on Fare feature\nplt.hist([titanic_data[titanic_data['Survived'] == 1]['Fare'], titanic_data[titanic_data['Survived'] == 0]['Fare']], \n         stacked=True, color = ['green', 'red'],\n         bins = 50, label = ['Survived', 'Died']);\nplt.xlabel('Fare'); plt.ylabel('Number of Passengers'); plt.legend();","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"108af369c8e4f8834219f8faf745d00699d38547"},"cell_type":"code","source":"# How many Survived & Died based on Age feature\ntitanic_data['Age'] = titanic_data['Age'].fillna(titanic_data['Age'].median()) # imputing null values with median value temporarily\nplt.hist([titanic_data[titanic_data['Survived'] == 1]['Age'], titanic_data[titanic_data['Survived'] == 0]['Age']], \n         stacked=True, color = ['green', 'red'],\n         bins = 50, label = ['Survived', 'Died']);\nplt.xlabel('Age'); plt.ylabel('Number of Passengers'); plt.legend();","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"522742955c6de969ba53a7c82fd6abb72aefbc93"},"cell_type":"code","source":"ax = plt.subplot()\nax.scatter(titanic_data[titanic_data['Survived'] == 1]['Age'], titanic_data[titanic_data['Survived'] == 1]['Fare'], \n           c='green', s=titanic_data[titanic_data['Survived'] == 1]['Fare'])\nax.scatter(titanic_data[titanic_data['Survived'] == 0]['Age'], titanic_data[titanic_data['Survived'] == 0]['Fare'], \n           c='red', s=titanic_data[titanic_data['Survived'] == 0]['Fare']);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"31eef0636888d368d2157ff92709309b0876b9d5"},"cell_type":"markdown","source":"### Step 4 - Feature Engineering\n- Imputing null values\n- Transform categorical features\n- Identify new features based on existing features"},{"metadata":{"trusted":false,"_uuid":"0b2bae38d0050cb8fe86ebe38b81cadec5f870d4"},"cell_type":"code","source":"# Combine both train and test dataset for feature engineering\ncombined_data = pd.DataFrame()\ncombined_data = combined_data.append(titanic_train_org)\ncombined_data = combined_data.append(titanic_test_org)\ncombined_data.drop(['PassengerId'], axis=1, inplace=True)\ncombined_data.reset_index(drop=True, inplace=True)\n\n# create indexes to separate data later on\ntrain_idx = len(titanic_train_org)\ntest_idx = len(combined_data) - len(titanic_test_org)\n\nprint('Combined dataset dimension: {} rows, {} columns'.format(combined_data.shape[0], combined_data.shape[1]))\n\ncombined_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"682ea73e93ee9317a07923736c5fa8e69506db13"},"cell_type":"code","source":"titles = set()\nfor name in combined_data['Name']:\n    titles.add(name.split(',')[1].split('.')[0].strip())\n\nprint(titles)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"70dff16efd89685ca7b54dffbfcac76cdc415531"},"cell_type":"code","source":"# Let's categorize titles using below dic\nTitle_Dictionary = {\"Capt\": \"Special\", \"Col\": \"Special\", \"Major\": \"Special\", \"Jonkheer\": \"Special\", \"Don\": \"Special\",\n                    \"Sir\" : \"Special\", \"Dr\": \"Special\", \"Rev\": \"Special\", \"the Countess\":\"Special\", \"Mme\": \"Mrs\", \"Mr\" : \"Mr\",\n                    \"Mlle\": \"Miss\", \"Ms\": \"Mrs\", \"Mrs\" : \"Mrs\", \"Miss\" : \"Miss\", \"Master\" : \"Master\", \"Lady\" : \"Special\"}\n\n# Let's extract the title from each name\ncombined_data['Title'] = combined_data['Name'].map(lambda name:name.split(',')[1].split('.')[0].strip())\n    \n# a map of more aggregated title. Let's map each title\ncombined_data['Title'] = combined_data.Title.map(Title_Dictionary)\ncombined_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f7b1bfa99151143fdc8938ff9fada85f188ead04"},"cell_type":"code","source":"combined_data[combined_data['Title'].isnull()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"15e7d9c43240b002ea2d076196c00d7b21288163"},"cell_type":"code","source":"# Let's handle the null value of Title with Mrs as Name is Oliva & Age is 39\ncombined_data['Title'] = combined_data['Title'].fillna('Mrs')\ncombined_data.ix[1305]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"7ef7c34225fac6e184cb393cfee67e466cc5f8bf"},"cell_type":"code","source":"print('Number of missing ages in train dataset: ', combined_data.iloc[:train_idx].Age.isnull().sum())\nprint('Number of missing ages in test dataset:  ', combined_data.iloc[train_idx:].Age.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"c0b00e92f9e903f6b77dc711fa929497ae02832f"},"cell_type":"code","source":"grouped_age_train = combined_data.iloc[:train_idx].groupby(['Sex','Pclass','Title'])\ngrouped_median_age_train = grouped_age_train.median()\ngrouped_median_age_train = grouped_median_age_train.reset_index()[['Sex', 'Pclass', 'Title', 'Age']]\n\ngrouped_median_age_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"89ca0e28ac555b8060d006a24687e5ac3a7ca85f"},"cell_type":"code","source":"def fill_age(row):\n    condition = (\n        (grouped_median_age_train['Sex'] == row['Sex']) & \n        (grouped_median_age_train['Title'] == row['Title']) & \n        (grouped_median_age_train['Pclass'] == row['Pclass'])\n    ) \n    return grouped_median_age_train[condition]['Age'].values[0]\n\ncombined_data['Age'] = combined_data.apply(lambda row: fill_age(row) if np.isnan(row['Age']) else row['Age'], axis=1)\n\nprint('Number of missing ages in combined dataset: ', combined_data.Age.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"f4238b9e3ab0961ffa96ef477ad5283d4afdca54"},"cell_type":"code","source":"grouped_fare_train = combined_data.iloc[:891].groupby(['Pclass','Embarked'])\ngrouped_fare_median_train = grouped_fare_train.median()\ngrouped_fare_median_train = grouped_fare_median_train.reset_index()[['Pclass','Embarked', 'Fare']]\n\ngrouped_fare_median_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"98604ca13f62b23be3cfbc8d99b1e18d9df06f67"},"cell_type":"code","source":"def fill_fare(row):\n    condition = (\n        (grouped_fare_median_train['Embarked'] == row['Embarked']) & \n        (grouped_fare_median_train['Pclass'] == row['Pclass'])\n    ) \n    return grouped_fare_median_train[condition]['Fare'].values[0]\n\ncombined_data['Fare'] = combined_data.apply(lambda row: fill_fare(row) if np.isnan(row['Fare']) else row['Fare'], axis=1)\n\nprint('Number of missing fare in combined dataset: ', combined_data.Fare.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"fc2495ee25fc220ff967f3abdf518326b3bcabe3"},"cell_type":"code","source":"# Handling Cabin Data - # Did they have a Cabin?\ncombined_data['Has_Cabin'] = np.where(combined_data.Cabin.isnull(), 0, 1)\n\n# fill Cabin NaN with U for unknown\ncombined_data.Cabin = combined_data.Cabin.fillna('U')\n\n# map first letter of cabin to itself\ncombined_data['Deck'] = combined_data['Cabin'].map(lambda x: x[0])\n\ncombined_data['Deck'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"aef972c44054222c93be65e03b2ce2e25bf25144"},"cell_type":"code","source":"# a function that extracts each prefix of the ticket, returns 'XXX' if no prefix (i.e the ticket is a digit)\ndef cleanTicket(ticket):\n    ticket = ticket.replace('.', '')\n    ticket = ticket.replace('/', '')\n    ticket = ticket.split()\n    ticket = map(lambda t : t.strip(), ticket)\n    ticket = list(filter(lambda t : not t.isdigit(), ticket))\n    if len(ticket) > 0:\n        return ticket[0]\n    else: \n        return 'XXX'\n\ncombined_data['Ticket'] = combined_data['Ticket'].map(cleanTicket)\ncombined_data['Ticket'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"fe5a1b84370a52e319460fb583b770f9739a1b21"},"cell_type":"code","source":"# convert age to categories\ncut_points = [0,5,12,18,35,60,100]\nlabel_names = [\"Infant\",\"Child\",\"Teenager\",\"Young Adult\",\"Adult\",\"Senior\"]\ncombined_data['Age_Cat'] = pd.cut(combined_data['Age'], cut_points, labels=label_names)\ncombined_data['Age_Cat'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"e8f51905a1e375d60d422da2ec30f9de5b74722a"},"cell_type":"code","source":"# find most frequent Embarked value and store in variable\nmost_embarked = combined_data.Embarked.value_counts().index[0]\n\n# fill NaN with most_embarked value\ncombined_data.Embarked = combined_data.Embarked.fillna(most_embarked)\n\n# size of families (including the passenger)\ncombined_data['FamilySize'] = combined_data.Parch + combined_data.SibSp + 1\n\n# Convert the male and female groups to integer form\ncombined_data.Sex = combined_data.Sex.map({\"male\": 0, \"female\":1})\n#combined_data.Embarked = combined_data.Embarked.map({'S': 1, 'C':2, 'Q':3})\n\n# create dummy variables for categorical features\npclass_dummies = pd.get_dummies(combined_data.Pclass, prefix=\"Pclass\")\ncabin_dummies = pd.get_dummies(combined_data.Deck, prefix=\"Deck\")\ntitle_dummies = pd.get_dummies(combined_data.Title, prefix=\"Title\")\nembarked_dummies = pd.get_dummies(combined_data.Embarked, prefix=\"Embarked\")\nticket_dummies = pd.get_dummies(combined_data.Ticket, prefix=\"Ticket\")\nage_cat_dummies = pd.get_dummies(combined_data.Age_Cat, prefix=\"Age_Cat\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"153f3301d72382dd726243c6c3caf562ca2a8241"},"cell_type":"code","source":"# concatenate dummy columns with main dataset\ncombined_data = pd.concat([combined_data, pclass_dummies], axis=1)\ncombined_data = pd.concat([combined_data, cabin_dummies], axis=1)\ncombined_data = pd.concat([combined_data, title_dummies], axis=1)\ncombined_data = pd.concat([combined_data, embarked_dummies], axis=1)\ncombined_data = pd.concat([combined_data, ticket_dummies], axis=1)\ncombined_data = pd.concat([combined_data, age_cat_dummies], axis=1)\n\n# drop categorical fields\n#combined_data.drop(['Pclass', 'Title', 'Cabin', 'Embarked', 'Name', 'Ticket'], axis=1, inplace=True)\n\ncombined_data.drop(['Pclass', 'Title', 'Cabin', 'Deck', 'Name', 'Ticket', 'Parch', 'SibSp', 'Embarked', 'Age_Cat'], axis=1, inplace=True)\nprint('Combined dataset dimension: {} rows, {} columns'.format(combined_data.shape[0], combined_data.shape[1]))\ncombined_data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"50afd9b2af89cc185f3c572e46480ae1c7f4528c"},"cell_type":"markdown","source":"### Step 5 - Split into Train & Test Dataset\nWe will divide the dataset into train and test as it was provided originally. Later, we will split the train dataset into train and test for model buidling and tuning based on strata of Sex feature"},{"metadata":{"trusted":false,"_uuid":"15b66fb4f9401854333ee18cd2ca324e37139dbd"},"cell_type":"code","source":"# create train and test data\ntrain = combined_data[ :train_idx]\ntest = combined_data[test_idx: ]\n\n# Stratified sampling based on the service feature categorical values\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=50)\nfor train_index, test_index in split.split(train, train['Sex']):\n    strat_train_set = train.loc[train_index]\n    strat_test_set = train.loc[test_index]\n\n# Print number of instances for train and test dataset\nprint(\"Stratified Sampling: \", len(strat_train_set), \"train +\", len(strat_test_set), \"test\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"0cd974cd7a1eced0bd7d595a714ce8fb4e02cc53"},"cell_type":"code","source":"# separate target column from encoded data and remove categorical feature for whom dummy variables were created\nstrat_train_set_X = strat_train_set.drop(['Survived'], axis=1)\nstrat_test_set_X = strat_test_set.drop(['Survived'], axis=1)\n\nstrat_train_set_y = strat_train_set[['Survived']].copy()\nstrat_test_set_y = strat_test_set[['Survived']].copy()\n\n# Final test set\nfinal_test_X = test.drop('Survived', axis=1).values\n\nprint('Train dataset dimension: {} rows, {} columns'.format(strat_train_set_X.shape[0], strat_train_set_X.shape[1]))\nprint('Test dataset dimension: {} rows, {} columns'.format(strat_test_set_X.shape[0], strat_test_set_X.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"43a890ff6e5ff1352037d5a9b92a656834c6ddae"},"cell_type":"markdown","source":"### Step 6 - Feature Selection\nLet's build a basic random forest classifier and extract the features based on their importances"},{"metadata":{"trusted":false,"_uuid":"9c2ee1dd3493a8e70a7c022978f38ff4d83986d8"},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrfc_model = RandomForestClassifier();\n\nrefclasscol = strat_train_set_X.columns\n\n# fit random forest classifier on the training set\nrfc_model.fit(strat_train_set_X, strat_train_set_y);\n\n# extract important features\nscore = np.round(rfc_model.feature_importances_, 3)\nimportances = pd.DataFrame({'feature':refclasscol, 'importance':score})\nimportances = importances.sort_values('importance', ascending=False).set_index('feature')\n\n# plot importances\nimportances.plot.bar();","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b456656a32055652c6c6f1c399fe9fca6d96e9d0"},"cell_type":"markdown","source":"### Step 7 - Build Model based on Important Features\nLet's see how selection of features based on their importnace impact the model building and their cross validation score"},{"metadata":{"trusted":false,"_uuid":"72136706052ae0eef605b0e46f2e798a0d1af597"},"cell_type":"code","source":"feat_imp_threshold = importances.loc[importances['importance'] < np.max(importances['importance'])]\n\nfrom sklearn import metrics\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.model_selection import cross_val_score\n\n# fit model using each importance as a threshold\nthresholds = np.sort(np.unique(feat_imp_threshold.importance))\nfor thresh in thresholds:\n    # selecting features using threshold\n    selection = SelectFromModel(rfc_model, threshold=thresh, prefit=True)\n    select_train_x = selection.transform(strat_train_set_X)\n    \n    # training model\n    selection_model = RandomForestClassifier()\n    selection_model.fit(select_train_x, strat_train_set_y)\n    scores = cross_val_score(selection_model, select_train_x, strat_train_set_y, cv=10)\n    \n    # prediction through model\n    select_test_x = selection.transform(strat_test_set_X)\n    pred_y = selection_model.predict(select_test_x)\n    \n    # evaluating model\n    accuracy = metrics.accuracy_score(strat_test_set_y, pred_y)\n    confusion = metrics.confusion_matrix(strat_test_set_y, pred_y)\n    TP = confusion[1][1]; TN = confusion[0][0]; FP = confusion[0][1]; FN = confusion[1][0];\n    sensitivity = TP / float(TP + FN); specifivity = TN / float(TN + FP);\n    \n    print(\"Thresh=%.3f, n=%d, Test Accuracy: %.2f%%, Cross Val Mean Score=%.2f%%, Sensitivity=%.2f%%, Specifivity=%.2f%%\" % \n          (thresh, select_train_x.shape[1], accuracy*100.0, scores.mean()*100.0, sensitivity*100.0, specifivity*100.0))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2bd409d7bcae19d18b6ee40d5da5358da7c06122"},"cell_type":"markdown","source":"We can observe that usually when the threshold is 0.005 where number of features are 29, cross validation score and other scoring parameters are giving better results."},{"metadata":{"trusted":false,"_uuid":"38fabb7417ca0a39cfc9ace4520cf6295791d747"},"cell_type":"code","source":"thresh = 0.005\nfinal_selection = SelectFromModel(rfc_model, threshold=0.005, prefit=True)\nfinal_train_x = final_selection.transform(strat_train_set_X)\n    \n# training model\nfinal_modelRF = RandomForestClassifier(n_estimators=300, max_depth=10)\nfinal_modelRF.fit(final_train_x, strat_train_set_y)\nscores = cross_val_score(final_modelRF, final_train_x, strat_train_set_y, cv=10)\n    \n# prediction through model\nfinal_test_x = final_selection.transform(strat_test_set_X)\npred_y = final_modelRF.predict(final_test_x)\n    \n# evaluating model\naccuracy = metrics.accuracy_score(strat_test_set_y, pred_y)\nconfusion = metrics.confusion_matrix(strat_test_set_y, pred_y)\nTP = confusion[1][1]; TN = confusion[0][0]; FP = confusion[0][1]; FN = confusion[1][0];\nsensitivity = TP / float(TP + FN); specifivity = TN / float(TN + FP);\n\nprint(\"Thresh=%.3f, n=%d, Test Accuracy: %.2f%%, Cross Val Mean Score=%.2f%%, Sensitivity=%.2f%%, Specifivity=%.2f%%\" % \n    (thresh, final_train_x.shape[1], accuracy*100.0, scores.mean()*100.0, sensitivity*100.0, specifivity*100.0))\nprint(final_modelRF)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"7257b20479b2bb429c4713314fdec2b898043b29"},"cell_type":"markdown","source":"### Step 8 - Use Ensemble Model & Feature Selection\n#### Ensembling Model\nThere is a wide variety of models to use, from logistic regression to decision trees and more sophisticated ones such as random forests and gradient boosted trees. We'll be using Random Forests. Random Froests has proven a great efficiency in Kaggle competitions.\n\nFor more details about why ensemble methods perform well, you can refer to these posts:\n- http://mlwave.com/kaggle-ensembling-guide/\n- http://www.overkillanalytics.net/more-is-always-better-the-power-of-simple-ensembles/\n\n#### Feature selection\nWe've come up to more than 60 features so far. This number is quite large. When feature engineering is done, we usually tend to decrease the dimensionality by selecting the \"right\" number of features that capture the essential.\n\nIn fact, feature selection comes with many benefits:\n- It decreases redundancy among the data\n- It speeds up the training process\n- It reduces overfitting\n\nTree-based estimators can be used to compute feature importances, which in turn can be used to discard irrelevant features."},{"metadata":{"trusted":false,"_uuid":"8d21d1b13197836b96c195421362e870b4fc3f56"},"cell_type":"code","source":"from sklearn.pipeline import make_pipeline\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# create train and test data\nfinal_train = combined_data[ :train_idx]\nfinal_test = combined_data[test_idx: ]\n\n# separate target column from encoded data and remove categorical feature for whom dummy variables were created\nfinal_train_set_X = final_train.drop(['Survived'], axis=1)\nfinal_test_set_X = final_test.drop(['Survived'], axis=1)\n\nfinal_train_set_y = final_train[['Survived']].copy()\nfinal_test_set_y = final_test[['Survived']].copy()\n\n# Final test set\nfinal_test_set_X = test.drop('Survived', axis=1).values\n\nprint('Train dataset dimension: {} rows, {} columns'.format(final_train_set_X.shape[0], final_train_set_X.shape[1]))\nprint('Test dataset dimension: {} rows, {} columns'.format(final_test_set_X.shape[0], final_test_set_X.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"51e4790562949ff3f0dcb7018b062f326c7be724"},"cell_type":"code","source":"def compute_score(clf, X, y, scoring='accuracy'):\n    xval = cross_val_score(clf, X, y, cv = 5, scoring=scoring)\n    return np.mean(xval)\n\nclf = RandomForestClassifier(n_estimators=100, max_features='sqrt')\nclf = clf.fit(final_train_set_X, final_train_set_y)\n\nfeatures = pd.DataFrame()\nfeatures['feature'] = final_train_set_X.columns\nfeatures['importance'] = clf.feature_importances_\nfeatures.sort_values(by=['importance'], ascending=True, inplace=True)\nfeatures.set_index('feature', inplace=True)\nfeatures.plot(kind='barh', figsize=(25, 25));","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"0321049ef2675d0a145df9fc7fb2f5b04e61fbb5"},"cell_type":"code","source":"model = SelectFromModel(clf, threshold=0.005, prefit=True)\ntrain_reduced = model.transform(final_train_set_X)\ntest_reduced = model.transform(final_test_set_X)\n\nprint('Train dataset dimension: {} rows, {} columns'.format(final_train_set_X.shape[0], final_train_set_X.shape[1]))\nprint('Test dataset dimension: {} rows, {} columns'.format(final_test_set_X.shape[0], final_test_set_X.shape[1]))\n\nprint('Reduced Train dataset dimension: {} rows, {} columns'.format(train_reduced.shape[0], train_reduced.shape[1]))\nprint('Reduced Test dataset dimension: {} rows, {} columns'.format(test_reduced.shape[0], test_reduced.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"00da22f5a537f387fbb19b19ada699f286d08a01"},"cell_type":"code","source":"logreg_model = LogisticRegression()\nlogreg_cv_model = LogisticRegressionCV()\nrf_model = RandomForestClassifier()\ngboost_model = GradientBoostingClassifier()\nsvc_model = SVC(probability=True)\ndt_model = DecisionTreeClassifier()\nab_model = AdaBoostClassifier()\nknn_model = KNeighborsClassifier(5)\ngnb_model = GaussianNB()\n\nmodels = [logreg_model, logreg_cv_model, rf_model, gboost_model, svc_model, dt_model, ab_model, knn_model, gnb_model]\n\nfor model in models:\n    print('Cross-validation of : {0}'.format(model.__class__))\n    score = compute_score(clf=model, X=train_reduced, y=final_train_set_y, scoring='accuracy')\n    print('CV score = {0}'.format(score))\n    print('***********************************************************************')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ce5069ee867e5cf17ed727fab19130545e43fbbb"},"cell_type":"code","source":"# Hyperparameters Tuning for Decision Tree\ndt_param_grid = {\n                 'max_depth' : [4, 6, 8],\n                 'criterion': ['gini', 'entropy'],\n                 'max_features': ['sqrt', 'auto', 'log2'],\n                 'min_samples_split': [2, 3, 10],\n                 'min_samples_leaf': [1, 3, 10],\n                 }\ndecision_tree = DecisionTreeClassifier()\ndt_cross_val = StratifiedKFold(n_splits=5)\n\ndt_grid_search = GridSearchCV(decision_tree,\n                              scoring='accuracy',\n                              param_grid=dt_param_grid,\n                              cv=dt_cross_val,\n                              verbose=1\n                              )\n\ndt_grid_search.fit(train_reduced, final_train_set_y)\ndt_model = dt_grid_search\ndt_parameters = dt_grid_search.best_params_\n\nrf_model = RandomForestClassifier(**dt_parameters)\n\nscore = compute_score(clf=dt_model, X=train_reduced, y=final_train_set_y, scoring='accuracy')\nprint('Cross-validation of : {0}'.format(dt_model.__class__))\nprint('After Hyperparameters tuning CV score = {0}'.format(score))\nprint('Best score: {}'.format(dt_grid_search.best_score_))\nprint('Best parameters: {}'.format(dt_grid_search.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"ec80452fdae576fab9d313498497d556e87baa87"},"cell_type":"code","source":"# Hyperparameters Tuning for Random Forest\nrf_param_grid = {\n                 'max_depth' : [4, 6, 8],\n                 'n_estimators': [50, 10],\n                 'max_features': ['sqrt', 'auto', 'log2'],\n                 'min_samples_split': [2, 3, 10],\n                 'min_samples_leaf': [1, 3, 10],\n                 'bootstrap': [True, False],\n                 }\nforest = RandomForestClassifier()\nrf_cross_val = StratifiedKFold(n_splits=5)\n\nrf_grid_search = GridSearchCV(forest,\n                              scoring='accuracy',\n                              param_grid=rf_param_grid,\n                              cv=rf_cross_val,\n                              verbose=1\n                              )\n\nrf_grid_search.fit(train_reduced, final_train_set_y)\nrf_model = rf_grid_search\nrf_parameters = rf_grid_search.best_params_\n\nrf_model = RandomForestClassifier(**rf_parameters)\n\nscore = compute_score(clf=rf_model, X=train_reduced, y=final_train_set_y, scoring='accuracy')\nprint('Cross-validation of : {0}'.format(rf_model.__class__))\nprint('After Hyperparameters tuning CV score = {0}'.format(score))\nprint('Best score: {}'.format(rf_grid_search.best_score_))\nprint('Best parameters: {}'.format(rf_grid_search.best_params_))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b621b12055e26a2375be6ba75023feda86c0e0c1"},"cell_type":"markdown","source":"### Step10 - Selecting Final Model & Predicting Survival\nThis is my 5th trial of predicting Survival response and based on the 5 trail, i got my best accuracy score on kaggle site by using random forest algorithm by applying hyper-parameters tuning and feature selection. Below are the details of my 5 trials how the feature were selected and models were used.\n- <b>Trial 1</b> - I used few features and features such as ticket and Cabin were not really used. Total features used after dummy variables were 15 and Random Forest Classifier algorithm was applied using Feature Selection mechanism. <b>Kaggle Score - 0.76555</b>\n- <b>Trial 2</b> - Ticket and Cabin features were used including other hyper-parameters. Total 68 features were created after dummy mechanism on categorical features. After Feature Selection mechanism, 13 features were selcted for classification algorithm. Logistic Regression, Logistic Regression CV, Random Forest Classifier, Gradient Boosting Classifier, SVC, Decision Tree Classifier, Ada Boost Classifier were used as ensemble models to predict Survival response. <b>Kaggle Score - 0.77990</b>\n- <b>Trial 3</b> - Same as Trail 2 with addition of KNN Neighbors Classifier & Gaussian Naive Bayes. Hyper-parameters tuning on Random Forest Classifier using GridSearchCV to identify best parameters and score. Same ensemble model techniques were used except SVC, KNN Neighbors Classifier & Gaussian Naive Bayes as their cross validation score was below 80%. <b>Kaggle Score - 0.77990</b>\n- <b>Trial 4</b> - Same as Trail 3 but here Hyper-parameters tuning applied on both Random Forest Classifier & Decision Tree Classifier using GridSearchCV to identify best parameters and score. Again, ensemble model technique used as Trail 4. <b>Kaggle Score - 0.77033</b>\n- <b>Trial 5</b> - Total 68 features were derived and after Feature Selection using Random Forest Classifier 30 features were used for building classification model. Hyper-parameters tuning applied on Random Forest Classifier using GridSearchCV to identify best parameters and score. Finally, best parameters were used to predict the Survival response with only Random Forest Classifier. <b>Kaggle Score - 0.80382</b>"},{"metadata":{"trusted":false,"_uuid":"06263c9b3eba79bc67af2876653f41e4c27c1e9c"},"cell_type":"code","source":"# Using Ensemble model technique by considering all the models trained to predict Survival (Trial 4)\nmodels = [logreg_model, logreg_cv_model, rf_model, gboost_model, dt_model, ab_model]\n\ntrained_models = []\nfor model in models:\n    model.fit(train_reduced, final_train_set_y)\n    trained_models.append(model)\n\npredictions = []\nfor model in trained_models:\n    predictions.append(model.predict_proba(test_reduced)[:, 1])\n\n# Take the mean of probability identified by each model\nkaggle_df = pd.DataFrame(predictions).T\nkaggle_df['out'] = kaggle_df.mean(axis=1)\nkaggle_df['PassengerId'] = titanic_test_org['PassengerId']\nkaggle_df['out'] = kaggle_df['out'].map(lambda s: 1 if s >= 0.5 else 0)\n\n# dataframe with predictions\nkaggle_df = kaggle_df[['PassengerId', 'out']]\nkaggle_df.columns = ['PassengerId', 'Survived']\n\n# save to csv\nkaggle_df.to_csv('RFTunedsubmission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"fa88c61ab5a47bcb7653d5d09f61d17453fb1ed7"},"cell_type":"code","source":"# Predict Survival using only Random Forest Classifier after hyper-parameter tuning and feature selection (Trial 5)\nrf_model.fit(train_reduced, final_train_set_y)\n\npredictions = []\npredictions = rf_model.predict(test_reduced).astype(int)\n\nkaggle_df = pd.DataFrame()\nkaggle_df['PassengerId'] = titanic_test_org['PassengerId']\nkaggle_df['Survived'] = predictions\n\n# save to csv\nkaggle_df.to_csv('RFTunedsubmission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"5f5b8a9a245be430fafd688ad8b468cc492e0178"},"cell_type":"markdown","source":"### Step 11 - Improve the Model\nWe can improve the performance of model with more permutations and combinations. If you want my code for other trials then let me know. I will work on to improve this more later as and when I get time, but, as of 09-Aug, my score on kaggle is 0.80382 and rank is 904 which is top 10% :)"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}