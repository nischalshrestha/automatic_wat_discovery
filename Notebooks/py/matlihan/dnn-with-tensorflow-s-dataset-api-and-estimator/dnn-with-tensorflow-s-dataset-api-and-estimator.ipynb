{"nbformat_minor": 1, "nbformat": 4, "metadata": {"language_info": {"pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "codemirror_mode": {"name": "ipython", "version": 3}, "version": "3.6.3"}, "kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}}, "cells": [{"metadata": {"_uuid": "c28282248106a18bac23f9a2db61a3cb111d8ba3", "_cell_guid": "9efa25c5-9508-4281-8e9a-6ed99f6fa1f3"}, "cell_type": "markdown", "source": ["# DNN Approach using TensorFlow's Dataset API and Predefined Estimators"]}, {"metadata": {}, "cell_type": "markdown", "source": ["With TensorFlow 1.4, the Dataset API is [introduced](https://developers.googleblog.com/2017/09/introducing-tensorflow-datasets.html). The real advantage of the Dataset API is that a lot of memory management is done for the user when using large file-based datasets. And, in this work, we will be implementing a predefined DNN estimator and feed it with the Dataset API for Kaggle's Titanic dataset.\n", "\n", "---\n", "\n", "With Dataset API we can use file-based datasets or datasets in the memory. In this work we will read the data from a csv file. In order to have it, you should first do some  feature engineering and then split the train set into train and validation sets."]}, {"metadata": {}, "cell_type": "markdown", "source": ["## 1) Feature Engineering\n", "\n", "We will do the same feature engineering as Trevor Stephens did [here](http://trevorstephens.com/kaggle-titanic-tutorial/r-part-4-feature-engineering/) before. The difference is that Trevor Stephens uses R and we will use python pandas library."]}, {"metadata": {}, "cell_type": "markdown", "source": ["### Part 1: Label Encoding"]}, {"execution_count": null, "outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "source": ["import pandas as pd\n", "import numpy as np\n", "from sklearn import preprocessing as prep"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["# our dataset is here\n", "%ls ../input"]}, {"execution_count": null, "outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "source": ["# read csv files\n", "train_df = pd.read_csv('../input/train.csv')\n", "test_df = pd.read_csv('../input/test.csv')"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["test_df.head()"]}, {"execution_count": null, "outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "source": ["# add test df a survived cloumn\n", "test_df['Survived'] = 0"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["test_df.head()"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["# concat train and test sets\n", "concat = train_df.append(test_df, ignore_index=True)\n", "concat.head()"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["# check numbers\n", "print(train_df.shape)\n", "print(test_df.shape)\n", "print(concat.shape)"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["concat.Sex.unique()"]}, {"execution_count": null, "outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "source": ["# label encoder to transform categorical string data to integers\n", "le = prep.LabelEncoder()"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["le.fit(concat.Sex)\n", "le.classes_"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["# encode labels\n", "Sex_le = le.transform(concat.Sex)\n", "Sex_le[0:10]"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["concat_le = concat.copy()\n", "concat_le.head()"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["concat_le.Sex = Sex_le\n", "concat_le.head()"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["# check data types\n", "concat_le.dtypes"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["# print unique values\n", "print(concat.Survived.unique())\n", "print(concat.Pclass.unique())\n", "print(concat.Sex.unique())\n", "print(concat.SibSp.unique())\n", "print(concat.Parch.unique())\n", "print(concat.Embarked.unique())"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["# remove nans and fill with '0's for embarked\n", "embarked = concat['Embarked'].fillna('0')\n", "embarked.unique()"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["# label encode embarked\n", "le.fit(embarked)\n", "embarked = le.transform(embarked)\n", "embarked[:10]"]}, {"execution_count": null, "outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "source": ["concat_le.Embarked = embarked"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["# check\n", "concat_le.head(10)"]}, {"execution_count": null, "outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "source": ["# split train and test sets\n", "train_le = concat_le.iloc[:891].copy()\n", "test_le = concat_le.iloc[891:].copy()"]}, {"execution_count": null, "outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "source": ["# And save\n", "%mkdir -p data\n", "train_le.to_csv('./data/train_le.csv', index=False)\n", "test_le.to_csv('./data/test_le.csv', index=False)"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["%ls data"]}, {"metadata": {}, "cell_type": "markdown", "source": ["### Part 2: Further Feature Engineering"]}, {"execution_count": null, "outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "source": ["train = pd.read_csv('./data/train_le.csv')\n", "test = pd.read_csv('./data/test_le.csv')"]}, {"execution_count": null, "outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "source": ["# concat dfs again\n", "concat = train.append(test)"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["# check numbers\n", "concat.shape"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["train.shape[0] + test.shape[0]"]}, {"metadata": {}, "cell_type": "markdown", "source": ["Feature engineer names"]}, {"execution_count": null, "outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "source": ["NameSplit = concat.Name.str.split('[,.]')"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["NameSplit.head()"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["titles = [str.strip(name[1]) for name in NameSplit.values]\n", "titles[:10]"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["# New feature\n", "concat['Title'] = titles"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["concat.Title.unique()"]}, {"execution_count": null, "outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "source": ["# redundancy: combine Mademoiselle and Madame into a single type\n", "concat.Title.values[concat.Title.isin(['Mme', 'Mlle'])] = 'Mlle'"]}, {"execution_count": null, "outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "source": ["# keep reducing the number of factor levels\n", "concat.Title.values[concat.Title.isin(['Capt', 'Don', 'Major', 'Sir'])] = 'Sir'\n", "concat.Title.values[concat.Title.isin(['Dona', 'Lady', 'the Countess', 'Jonkheer'])] = 'Lady'"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["# label encode new feature too\n", "le.fit(concat.Title)\n", "le.classes_"]}, {"execution_count": null, "outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "source": ["concat.Title = le.transform(concat.Title)"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["concat.head(10)"]}, {"metadata": {}, "cell_type": "markdown", "source": ["### New features family size and family id"]}, {"execution_count": null, "outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "source": ["# new feature family size\n", "concat['FamilySize'] = concat.SibSp.values + concat.Parch.values + 1"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["concat.head(10)"]}, {"metadata": {}, "cell_type": "markdown", "source": ["New feature `FamilyID`, extract family information from surnames and family size information. Members of a family should have both the same surname and family size."]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["surnames = [str.strip(name[0]) for name in NameSplit.values]\n", "surnames[:10]"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["concat['Surname'] = surnames\n", "concat['FamilyID'] = concat.Surname.str.cat(concat.FamilySize.astype(str), sep='')\n", "concat.head(10)"]}, {"execution_count": null, "outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "source": ["# mark any family id as small if family size is less than or equal to 2\n", "concat.FamilyID.values[concat.FamilySize.values <= 2] = 'Small'"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["concat.head(10)"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["# check the frequency of family ids\n", "concat.FamilyID.value_counts()"]}, {"metadata": {}, "cell_type": "markdown", "source": ["Too many family ids with few family members, maybe some families had different last names or something else. Let's clean this too."]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["freq = list(dict(zip(concat.FamilyID.value_counts().index.tolist(), concat.FamilyID.value_counts().values)).items())\n", "type(freq)"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["freq = np.array(freq)\n", "freq[:10]"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["freq.shape"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["# select the family ids with frequency of 2 or less\n", "freq[freq[:,1].astype(int) <= 2].shape"]}, {"execution_count": null, "outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "source": ["freq = freq[freq[:,1].astype(int) <= 2]"]}, {"execution_count": null, "outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "source": ["# assign 'Small' for those\n", "concat.FamilyID.values[concat.FamilyID.isin(freq[:,0])] = 'Small'"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["concat.FamilyID.value_counts()"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["# label encoding for family id\n", "le.fit(concat.FamilyID)\n", "concat.FamilyID = le.transform(concat.FamilyID)\n", "concat.FamilyID.unique()"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["# choose usefull features\n", "concat_reduce = concat[[\n", "    'PassengerId', 'Pclass', 'Sex', 'Age', 'SibSp',\n", "    'Parch', 'Fare', 'Title', 'Embarked', 'FamilySize',\n", "    'FamilyID', 'Survived']]\n", "concat_reduce.head()"]}, {"execution_count": null, "outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "source": ["# split\n", "train_final = concat_reduce.iloc[:891].copy()\n", "test_final = concat_reduce.iloc[891:].copy()"]}, {"execution_count": null, "outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "source": ["# save\n", "train_final.to_csv('./data/train_final.csv', index=False)\n", "test_final.to_csv('./data/test_final.csv', index=False)"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["%ls data"]}, {"metadata": {}, "cell_type": "markdown", "source": ["## Part 3: Split train set  into train and validation sets"]}, {"execution_count": null, "outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "source": ["train = pd.read_csv('./data/train_final.csv')"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["train.shape"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["train.head(10)"]}, {"execution_count": null, "outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "source": ["# shuffle dataframe\n", "train = train.sample(frac=1).reset_index(drop=True)"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["train.head(10)"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["# define number of rows for validation set\n", "n_valid_rows = int(train.shape[0]*0.3)\n", "n_valid_rows"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["# split\n", "valid_split = train.iloc[:n_valid_rows].copy()\n", "train_split = train.iloc[n_valid_rows:].copy()\n", "print(train_split.shape)\n", "print(valid_split.shape)"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["# save\n", "train_split.to_csv('./data/train_split_final.csv', index=False)\n", "valid_split.to_csv('./data/valid_split_final.csv', index=False)\n", "\n", "%ls data"]}, {"metadata": {}, "cell_type": "markdown", "source": ["---"]}, {"metadata": {}, "cell_type": "markdown", "source": ["## 2) Deep Neural Networks with TensorFlow's Dataset API and Estimators"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["import tensorflow as tf"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["train = pd.read_csv('./data/train_split_final.csv')\n", "valid = pd.read_csv('./data/valid_split_final.csv')\n", "\n", "train.head()"]}, {"metadata": {}, "cell_type": "markdown", "source": ["`{Pclass, Sex, Age, SibSp, Parch, Fare, Title, Embarked, FamilySize, FamilyID}` we will be using as the features and the `Survived` column will be our labels."]}, {"metadata": {}, "cell_type": "markdown", "source": ["In order to use the Datasets API and feed the Estimator, we should write an input function like this:\n", "\n", "```python\n", "def input_fn():\n", "    ...<code>...\n", "    return ({ 'Pclass':[values], ..<etc>.., 'FamilyID':[values] },\n", "            [Survived])\n", "```"]}, {"metadata": {}, "cell_type": "markdown", "source": ["This function takes the `file_path` as input and outputs a two-element tuple. The first element of the tuple is a dictionary containing feature names as keys and features as values. And the second element is a list of labels for the training batch.\n", "\n", "Other two arguments for the input function are `perform_shuffle` and `repeat_count`. If `perform_shuffle` is `True` the order of the examples are shuffled. The `perform_shuffle` argument specifies the number of epochs during training, for instance, if `perform_shuffle=1` all the train set examples are passed only once."]}, {"metadata": {}, "cell_type": "markdown", "source": ["And the implementation is as follows, we will use this function to feed the estimator later."]}, {"execution_count": null, "outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "source": ["# define feature names first\n", "feature_names = [\n", "    'Pclass',\n", "    'Sex',\n", "    'Age',\n", "    'SibSp',\n", "    'Parch',\n", "    'Fare',\n", "    'Title',\n", "    'Embarked',\n", "    'FamilySize'\n", "    'FamilyID']"]}, {"execution_count": null, "outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "source": ["def titanic_input_fn(file_path, perform_shuffle=False, repeat_count=1):\n", "    def decode_csv(line):\n", "        # second argument of decode_csv defines the data types for each dataset column!\n", "        # the first argument is passenger ids thus integer\n", "        # the last column is survived or not labels thus integer\n", "        # and the rest are float.\n", "        parsed_line = tf.decode_csv(\n", "            line, [[0], [0.], [0.], [0.], [0.], [0.], [0.], [0.], [0.], [0.], [0.], [0]])\n", "        label = parsed_line[-1:] # Last element is the label\n", "        del parsed_line[-1] # Delete last element (it is the labels)\n", "        features = parsed_line[1:] # First element is excluded since it is the id column\n", "        d = dict(zip(feature_names, features)), label\n", "        return d\n", "    \n", "    dataset = (tf.data.TextLineDataset(file_path) # Read text file\n", "        .skip(1) # Skip header row\n", "        .map(decode_csv)) # Transform each elem by applying decode_csv fn\n", "    if perform_shuffle:\n", "        # Randomizes input using a window of 256 elements (read into memory)\n", "        dataset = dataset.shuffle(buffer_size=256)\n", "    dataset = dataset.repeat(repeat_count) # Repeats dataset this # times\n", "    dataset = dataset.batch(32)  # Batch size to use\n", "    iterator = dataset.make_one_shot_iterator()\n", "    batch_features, batch_labels = iterator.get_next()\n", "    return batch_features, batch_labels"]}, {"metadata": {}, "cell_type": "markdown", "source": ["Memory management is provided here with `TextLineDataset`."]}, {"metadata": {}, "cell_type": "markdown", "source": ["Let's print and check the first batch:"]}, {"execution_count": null, "outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "source": ["train_path = './data/train_final.csv'"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["next_batch = titanic_input_fn(train_path, False) # Will return first 32 elements\n", "\n", "with tf.Session() as sess:\n", "    first_batch = sess.run(next_batch)\n", "print(first_batch)"]}, {"metadata": {}, "cell_type": "markdown", "source": ["Ok, it is working!"]}, {"metadata": {}, "cell_type": "markdown", "source": ["---\n", "Now we will define our DNN estimator"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["# remove checkpoints directory if you will rebuild the estimator\n", "# by running this cell again\n", "#%rm -r ./checkpoints\n", "\n", "# path to save checkpoints\n", "save_dir = './checkpoints'\n", "\n", "# reset default graph if rebuilding the classifier\n", "tf.reset_default_graph()\n", "\n", "# Create the feature_columns, which specifies the input to our model.\n", "# All our input features are numeric, so use numeric_column for each one.\n", "feature_columns = [tf.feature_column.numeric_column(k) for k in feature_names]\n", "\n", "# define the classifier\n", "classifier = tf.estimator.DNNClassifier(\n", "    feature_columns=feature_columns, # The input features to our model\n", "    hidden_units=[2048, 1024, 512, 256, 128], # 5 layers\n", "    n_classes=2, # survived or not {1, 0}\n", "    model_dir=save_dir, # Path to where checkpoints etc are stored\n", "    optimizer=tf.train.RMSPropOptimizer(\n", "        learning_rate=0.00001),\n", "    dropout=0.1)"]}, {"metadata": {}, "cell_type": "markdown", "source": ["Now we will train the model using `titanic_input_fn` and our classifier."]}, {"execution_count": null, "outputs": [], "metadata": {"collapsed": true}, "cell_type": "code", "source": ["train_path = './data/train_split_final.csv'\n", "valid_path = './data/valid_split_final.csv'\n", "test_path = './data/test_final.csv'"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["# the classifier will run for 20 epochs below\n", "classifier.train(input_fn=lambda: titanic_input_fn(train_path, True, 20))"]}, {"metadata": {}, "cell_type": "markdown", "source": ["It is not tuned for a high accuracy, it is possible to get higher accuracies about %85 by tuning hyperparameters."]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["# evaluate\n", "# Return value will contain evaluation_metrics such as: loss & average_loss\n", "evaluate_result = classifier.evaluate(\n", "   input_fn=lambda: titanic_input_fn(valid_path, False, 1))\n", "print('')\n", "print(\"Evaluation results:\")\n", "for key in evaluate_result:\n", "    print(\"   {}, was: {}\".format(key, evaluate_result[key]))"]}, {"execution_count": null, "outputs": [], "metadata": {}, "cell_type": "code", "source": ["# and the prediction is like\n", "predict_results = classifier.predict(\n", "    input_fn=lambda: titanic_input_fn(test_path, False, 1))\n", "print(\"Predictions on test file\")\n", "i=892\n", "for prediction in predict_results:\n", "    # Will print the predicted class: 0 or 1.\n", "    print(prediction[\"class_ids\"][0])\n", "    i += 1\n", "print(i)"]}, {"metadata": {}, "cell_type": "markdown", "source": ["That's all folks!"]}]}