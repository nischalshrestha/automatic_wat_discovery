{"cells":[{"metadata":{"_uuid":"3877f99a6a5edd86cf5b695ed727206d964d8cfd","_cell_guid":"75fe77ee-7df7-4a99-bb77-6725cda41a23"},"cell_type":"markdown","source":"**Titanic Problem using Logistic Regression and Neural Net**"},{"metadata":{"_uuid":"245545b0898478aff4137a29208929a8525d842f","_cell_guid":"19e476f8-891b-4463-b248-b5d02b8a2b5b"},"cell_type":"markdown","source":"Welcome to my first problem on Kaggle. For now, I decided to approach it using two different methods (logistic regression and a shallow neural network) and find out how they perform. But, first, I wanted to have an idea about the data."},{"metadata":{"_kg_hide-output":false,"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here'sseveral helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nimport csv\n\n# Load training and test set and look at some of the data\n\ntrainData = pd.read_csv('../input/train.csv')\ntestData = pd.read_csv('../input/test.csv')\n\ntrainData.head(5)","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"2a81aabdf690caf136177bed5de312a298640874","_cell_guid":"4b185175-c468-4126-9418-9dd528a396c9"},"cell_type":"markdown","source":"I started making plots comparing how all the passengers with how the surviving passengers are distributed in relation to some variables, such as class, age and sex."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#First plots: find out how each parameter varies in relation to the survival of the passenger\n#Reason: understand which parameters are the most important for the problem\n\nsurvived = trainData[(trainData.Survived==1)]\n\nfig, axs = plt.subplots(1,2)\n\nprint(\"Economic class\")\ntrainData['Pclass'].value_counts().sort_index().plot.bar(ax=axs[0],figsize=(12, 6), fontsize=16)\nsurvived['Pclass'].value_counts().sort_index().plot.bar(ax=axs[1], fontsize=16)\n\naxs[0].set_title(\"People on the Titanic\", fontsize=20)\naxs[1].set_title(\"People who survived\", fontsize=20)","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"32fd188348c94735fe1e1556ccae79125a01ed1e","_cell_guid":"12f2915f-2dfe-4772-a95a-18a6b2fdcbf6","trusted":true},"cell_type":"code","source":"print(\"Sex\")\n\nfig2, axs2 = plt.subplots(1,2)\n\ntrainData['Sex'].value_counts().sort_index().plot.bar(ax=axs2[0], figsize=(12, 6), fontsize=16)\nsurvived['Sex'].value_counts().sort_index().plot.bar(ax=axs2[1], fontsize=16)\n\naxs2[0].set_title(\"People on the Titanic\", fontsize=20)\naxs2[1].set_title(\"People who survived\", fontsize=20)","execution_count":3,"outputs":[]},{"metadata":{"_uuid":"c73e3d7ee8b19e1c615d97087c1e24d87b583675","_cell_guid":"adf612ae-6b6b-42fc-9f2c-0c5321670cf6","trusted":true},"cell_type":"code","source":"print(\"Age\")\n\nfig3, axs3 = plt.subplots(1,2)\n\ntrainData['Age'].plot.hist(ax=axs3[0], figsize=(12, 6), fontsize=16)\nsurvived['Age'].plot.hist(ax=axs3[1], fontsize=16)\n\naxs3[0].set_title(\"People on the Titanic\", fontsize=20)\naxs3[1].set_title(\"People who survived\", fontsize=20)","execution_count":4,"outputs":[]},{"metadata":{"_uuid":"f73c6c64fbf92ca7872752ff1d8af433dc812ca0","_cell_guid":"aeea44f1-3552-4121-a0f8-76e7b16a1928","trusted":true},"cell_type":"code","source":"print(\"Number of siblings/spouses aboard\")\n\nfig4, axs4 = plt.subplots(1,2)\n\ntrainData['SibSp'].value_counts().sort_index().plot.bar(ax=axs4[0], figsize=(12, 6), fontsize=16)\nsurvived['SibSp'].value_counts().sort_index().plot.bar(ax=axs4[1], fontsize=16)\n\naxs4[0].set_title(\"People on the Titanic\", fontsize=20)\naxs4[1].set_title(\"People who survived\", fontsize=20)","execution_count":5,"outputs":[]},{"metadata":{"_uuid":"040ce7b2665908f1ad9fa37aada16fb551841b25","_cell_guid":"05643316-3542-4bc7-a976-301d3d1cfb8f","trusted":true},"cell_type":"code","source":"print(\"Number of parents/children aboard\")\n\nfig5, axs5 = plt.subplots(1,2)\n\ntrainData['Parch'].value_counts().sort_index().plot.bar(ax=axs5[0], figsize=(12, 6), fontsize=16)\nsurvived['Parch'].value_counts().sort_index().plot.bar(ax=axs5[1], fontsize=16)\n\naxs5[0].set_title(\"People on the Titanic\", fontsize=20)\naxs5[1].set_title(\"People who survived\", fontsize=20)","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"0ea142d88cd65443e15547cc8661f37091208289","_cell_guid":"75bb8ca3-9264-4684-a214-803f379320a9","trusted":true},"cell_type":"code","source":"print(\"Port of embarkation\")\n\nfig6, axs6 = plt.subplots(1,2)\n\ntrainData['Embarked'].value_counts().sort_index().plot.bar(ax=axs6[0], figsize=(12, 6),fontsize=16)\nsurvived['Embarked'].value_counts().sort_index().plot.bar(ax=axs6[1], fontsize=16)\n\naxs6[0].set_title(\"People on the Titanic\", fontsize=20)\naxs6[1].set_title(\"People who survived\", fontsize=20)","execution_count":7,"outputs":[]},{"metadata":{"_uuid":"726a1ece9d0396c5ec24f5ca7602f81c26cfaf8b","_cell_guid":"b9a82026-9f4e-44a1-b036-874d5b56bb82","trusted":true},"cell_type":"code","source":"print(\"Fare\")\n\nfig7, axs7 = plt.subplots(1,2)\n\ntrainData['Fare'].plot.hist(ax=axs7[0], figsize=(12, 6), fontsize=16)\nsurvived['Fare'].plot.hist(ax=axs7[1], fontsize=16)\n\naxs7[0].set_title(\"People on the Titanic\", fontsize=20)\naxs7[1].set_title(\"People who survived\", fontsize=20)","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"f9994534e7864dc779d42c5881aa8fd9fffa08b3","_cell_guid":"e824ed8d-aa38-43b7-8d34-acfc987a2ec3"},"cell_type":"markdown","source":"With that, we can see that age, sex and class are the variables that have the largest effect on how the passengers that survived the disaster are distributed (i.e., they are the ones that had the biggest change when compared to the whole population available in the dataset).\n\nNow, it is important to see if there are any problems with the dataset in terms of it not being complete for all the variables."},{"metadata":{"_uuid":"01a2e461cc4b3479f54c717cfca0eb057cf567b5","_cell_guid":"fc8cec3c-aaeb-45a0-9dd1-32fc8484bdd9","trusted":true},"cell_type":"code","source":"#counting how many entries are in the dataset for each of the variables\nprint(\"Number of non-null values in each column in dataset\")\nprint(\"Survived = \", trainData['Survived'].count())\nprint(\"Pclass = \", trainData['Pclass'].count())\nprint(\"Sex = \", trainData['Sex'].count())\nprint(\"Age = \", trainData['Age'].count())\nprint(\"SibSp = \", trainData['SibSp'].count())\nprint(\"Parch = \", trainData['Parch'].count())\nprint(\"Ticket = \", trainData['Ticket'].count())\nprint(\"Fare = \", trainData['Fare'].count())\nprint(\"Cabin = \", trainData['Cabin'].count())\nprint(\"Embarked = \", trainData['Embarked'].count())","execution_count":9,"outputs":[]},{"metadata":{"_uuid":"06db15eb354fcba14e779a6d263caa0fc34343d5","_cell_guid":"33cfcbeb-8d55-4314-b7a4-1ae8f51121f4"},"cell_type":"markdown","source":"That obviously show that some people in the list do not have a complete list of features. Since we only have data in the \"Cabin\" feature for less than a quarter of the people in the dataset, I will drop this feature. I will also drop \"Embarked\", because, although it is not missing a lot of information, the plots did not show a big influence, so, in this first simplest model, I will not consider it. Age, however, seems important, so it will have to kept and dealt with.\n\nNow, let's create a simple logistic regression model, using TensorFlow"},{"metadata":{"_uuid":"b691511c334172ae2861948c4602de409a2cb0a0","collapsed":true,"_cell_guid":"1ac06e72-b3e9-410d-ba1c-4268cd95d95d","trusted":true},"cell_type":"code","source":"def logisticRegression(X, Y, Xtest, alpha, nEpoch, sh, lamb=0):\n#Inputs: training data set, labels, data set to predict, learning rate, number of epochs, number of features in the data, regularization parameter\n    \n    x = tf.placeholder(tf.float32, [None, sh])\n    y = tf.placeholder(tf.float32, [None, 1])\n    \n    #threshold for logistic regression to decide between 1 and 0 options\n    p5 = tf.constant(0.5)\n\n    #create variables\n    W = tf.Variable(tf.random_normal([sh, 1], mean=0.0, stddev=0.05))\n    b = tf.Variable([0.])\n\n    #start logistic regression calculations, using a gradient descent optimizer\n    y_pred = tf.matmul(x, W) + b\n    y_pred_sigmoid = tf.sigmoid(y_pred)\n\n    x_entropy = tf.nn.sigmoid_cross_entropy_with_logits(logits=y_pred, labels=y)\n    loss = tf.reduce_mean(x_entropy)\n    \n    train_step = tf.train.GradientDescentOptimizer(alpha).minimize(loss)\n    delta = tf.abs((y - y_pred_sigmoid))\n    correct_prediction = tf.cast(tf.less(delta, p5), tf.int32)\n    prediction = tf.cast(tf.round(y_pred_sigmoid), tf.int32)\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n    # Train for the select number of epochs\n    init = tf.initialize_all_variables()\n    costs = []\n    accs = []\n\n    with tf.Session() as sess:\n        sess.run(init)\n\n        print('Training...')\n        for i in range(nEpoch):\n            fd_train = {x: X, y: Y.reshape((-1, 1))}\n            train_step.run(fd_train)\n            fd_test = {x: Xtest}\n            pred = prediction.eval(fd_test)\n        \n            if i % 5000 == 0:\n                loss_step = loss.eval(fd_train)\n                train_accuracy = accuracy.eval(fd_train)\n                costs.append(loss_step)\n                accs.append(train_accuracy)\n                print(\"cost at epoch \", i, \" is: \", loss_step)\n    \n    #return list of costs, the prediction results and the list of accuracies measured in the training set\n    return costs, pred, accs","execution_count":14,"outputs":[]},{"metadata":{"_uuid":"9a3e9132ad7d936c5aba4eb2ca03ce02365ca7de","_cell_guid":"a1eb1455-5dc1-4af1-98de-dd51af164eb5"},"cell_type":"markdown","source":"Now I will create a neural network with three hidden layers with 5, 2 and 4 units respectively, plus the output layer. The activations are calculated using ReLU functions, followed by a sigmoid in the last layer (binary classification). Here I am using the Xavier initializer for the variables and Adam optimization."},{"metadata":{"_uuid":"4513b5251583db6d120fd36f452618dd9f5dc3e8","collapsed":true,"_cell_guid":"d8f96fb3-4118-455a-95a5-460cb1732c2b","trusted":true},"cell_type":"code","source":"def neuralNetwork(X, Y, Xtest, alpha, nEpoch, sh):\n#Inputs: training data set, labels, data set to predict, learning rate, number of epochs, number of features in the data\n    \n    tf.reset_default_graph()\n    \n    x = tf.placeholder(tf.float32, [sh, None])\n    y = tf.placeholder(tf.float32, [1, None]) \n    \n    #threshold to decide between 1 and 0 options\n    p5 = tf.constant(0.5)\n\n    #create variables\n    W1 = tf.get_variable(\"W1\", [5, sh], initializer = tf.contrib.layers.xavier_initializer())\n    b1 = tf.get_variable(\"b1\", [5, 1], initializer = tf.zeros_initializer())\n    \n    W2 = tf.get_variable(\"W2\", [2, 5], initializer = tf.contrib.layers.xavier_initializer())\n    b2 = tf.get_variable(\"b2\", [2, 1], initializer = tf.zeros_initializer())\n    \n    W3 = tf.get_variable(\"W3\", [4, 2], initializer = tf.contrib.layers.xavier_initializer())\n    b3 = tf.get_variable(\"b3\", [4, 1], initializer = tf.zeros_initializer())\n\n    W4 = tf.get_variable(\"W4\", [1, 4], initializer = tf.contrib.layers.xavier_initializer())\n    b4 = tf.get_variable(\"b4\", [1, 1], initializer = tf.zeros_initializer())\n    \n    #start calculations for the neural network\n    Z1 = tf.add(tf.matmul(W1, x), b1)\n    A1 = tf.nn.relu(Z1)\n    \n    Z2 = tf.add(tf.matmul(W2, A1), b2)\n    A2 = tf.nn.relu(Z2)\n    \n    Z3 = tf.add(tf.matmul(W3, A2), b3)\n    A3 = tf.nn.relu(Z3)\n    \n    Z4 = tf.add(tf.matmul(W4, A3), b4)\n    \n    logits = tf.transpose(Z4)\n    labels = tf.transpose(y)\n    y_pred_sigmoid = tf.sigmoid(logits)\n\n    x_entropy = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=labels)\n    loss = tf.reduce_mean(x_entropy)\n    \n    train_step = tf.train.AdamOptimizer(alpha).minimize(loss)\n    delta = tf.abs((y - y_pred_sigmoid))\n    correct_prediction = tf.cast(tf.less(delta, p5), tf.int32)\n    prediction = tf.cast(tf.round(y_pred_sigmoid), tf.int32)\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n    \n    #Train for the selected number of epochs\n    init = tf.global_variables_initializer()\n    costs = []\n    accs = []\n\n    with tf.Session() as sess:\n        sess.run(init)\n\n        print('Training...')\n        for i in range(nEpoch):\n            fd_train = {x: X, y: Y.reshape((1, -1))}\n            train_step.run(fd_train)\n            fd_test = {x: Xtest}\n            pred = prediction.eval(fd_test)\n            \n            if i % 5000 == 0:\n                loss_step = loss.eval(fd_train)\n                train_accuracy = accuracy.eval(fd_train)\n                costs.append(loss_step)\n                accs.append(train_accuracy)\n                print(\"Cost at epoch \", i, \" is \", loss_step)\n                \n    #return list of costs, the prediction results and the list of accuracies measured in the training set       \n    return costs, pred, accs","execution_count":15,"outputs":[]},{"metadata":{"_uuid":"cd8e7aa25b04f4992beaf93d9d20df9002634e95","_cell_guid":"00e4add0-c44f-4ec2-b939-489ccf337ee4"},"cell_type":"markdown","source":"For the first attempt using the logistic regression model, I will split the data in two parts: one containing the age of the person, and use it, with four other features (class, sex, number of siblings/spouses and number of parents/children); and one without the age and train only for the other four features."},{"metadata":{"_uuid":"5aa231a3350d425f327df9320c146858a81f0d56","_cell_guid":"8c35012d-1594-427c-9c73-efdd1d146151","scrolled":true,"trusted":true},"cell_type":"code","source":"#number of training examples\nm = trainData['Name'].count()\n\n#create matrices X and Y with the desired features and substitute categorical features by numerical ones\nX = trainData[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch']]\nX['Sex'] = X['Sex'].replace(to_replace=['male', 'female'], value=[0, 1])\n\nY = trainData['Survived']\n\n#create matrices X and Y with the desired features and substitute categorical features by numerical ones\nXtest = testData[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch']]\nXidx = testData[['PassengerId']]\nXtest['Sex'] = Xtest['Sex'].replace(to_replace=['male', 'female'], value=[0, 1])","execution_count":16,"outputs":[]},{"metadata":{"_kg_hide-output":false,"_uuid":"98f3542514de3cbfc07a513bc304c8de916ad30f","_cell_guid":"7e5f76ef-5fc5-4512-9a2c-2072745d6ad3","scrolled":true,"trusted":true},"cell_type":"code","source":"#select only examples that contain information for the age\nX_age = X[np.isnan(X['Age'])!=True]\nY_age = Y[np.isnan(X['Age'])!=True]\n\nm = len(Y_age)\n\nY_age = np.reshape(Y_age, (m, 1))\n\nXtest_age = Xtest[np.isnan(Xtest['Age'])!=True]\nXidx_age = Xidx[np.isnan(Xtest['Age'])!=True]\n\n#train model with 5 features\ncosts, predictions, accuracy = logisticRegression(X_age, Y_age, Xtest_age, 0.01, 100000, 5)\n\nprint(\"Model trained!\")","execution_count":17,"outputs":[]},{"metadata":{"_uuid":"46a74ca9e65fcbd782a772689d41d949b6e386f1","_cell_guid":"aa6b775a-793d-4331-9f64-75ac5cc00a7e","trusted":true},"cell_type":"code","source":"#select only the used features from the data\nX_noage = X[['Pclass', 'Sex', 'SibSp', 'Parch']]\nY_noage = Y\n\nXtest_noage = Xtest[np.isnan(Xtest['Age'])]\nXtest_noage = Xtest_noage[['Pclass', 'Sex', 'SibSp', 'Parch']]\nXidx_noage = Xidx[np.isnan(Xtest['Age'])]\n\n#train model with 4 features\ncosts_na, predictions_na, accuracy_na = logisticRegression(X_noage, Y_noage, Xtest_noage, 0.01, 20000, 4)\n\nprint(\"Model trained!\")","execution_count":18,"outputs":[]},{"metadata":{"_uuid":"fa8c5452d8db04eee9466b0f8dd70e71100f7e88","_cell_guid":"abd43f98-a78b-4440-be7a-411e2eba633b"},"cell_type":"markdown","source":"The same thing that was done before, for the logistic regression, here is done for the neural network."},{"metadata":{"_uuid":"878fcec4b46f6453531acc4d599e1bc32642205a","_cell_guid":"a5bcac66-e4e3-42b1-be2c-72d6179aab25","trusted":true},"cell_type":"code","source":"#select only examples that contain information for the age\nX_age = X[np.isnan(X['Age'])!=True]\nY_age = Y[np.isnan(X['Age'])!=True]\n\nm = len(Y_age)\n\nY_age = np.reshape(Y_age, (m, 1))\n\nXtest_age = Xtest[np.isnan(Xtest['Age'])!=True]\nXidx_age = Xidx[np.isnan(Xtest['Age'])!=True]\n\n#train model with 5 features\ncosts_nn, predictions_nn, accuracy_nn = neuralNetwork(np.transpose(X_age), np.transpose(Y_age), np.transpose(Xtest_age), 0.003, 50000, 5)\n\nprint(\"Model trained!\")","execution_count":19,"outputs":[]},{"metadata":{"_uuid":"1eac18c90d4f5a8c95a50d1c0c517e50082a89d4","_cell_guid":"5d2daa7e-f52e-45c6-9a0a-ec5b36ac9401","trusted":true},"cell_type":"code","source":"#select only the used features from the data\nX_noage = X[['Pclass', 'Sex', 'SibSp', 'Parch']]\nY_noage = Y\n\nXtest_noage = Xtest[np.isnan(Xtest['Age'])]\nXtest_noage = Xtest_noage[['Pclass', 'Sex', 'SibSp', 'Parch']]\nXidx_noage = Xidx[np.isnan(Xtest['Age'])]\n\n#train model with 4 features\ncosts_na_nn, predictions_na_nn, accuracy_na_nn = neuralNetwork(np.transpose(X_noage), np.transpose(Y_noage), np.transpose(Xtest_noage), 0.003, 20000, 4)\n\nprint(\"Model trained!\")","execution_count":20,"outputs":[]},{"metadata":{"_uuid":"10466850625059b94095cdd3e05d8a02a09c1988","_cell_guid":"289cf11d-0325-4dac-a7e1-f414c1731557"},"cell_type":"markdown","source":"Now here I added some of the features I had ignored in the first models."},{"metadata":{"_uuid":"6375e2b627b4743d5c412bd7668e24294ba80915","_cell_guid":"7738c6e6-6014-44bf-b2b1-715d3be81ea5","trusted":true},"cell_type":"code","source":"pd.options.mode.chained_assignment = None\n\n#calculate the average fare and age of passengers\nfare_avg = np.sum(trainData['Fare'])/trainData['Fare'].count()\nage_avg = np.sum(trainData['Age'])/trainData['Age'].count()\n\n#Select the features from the data and substitute strings by integers\nX2 = trainData[['Pclass', \"Age\", \"Sex\", \"SibSp\", \"Parch\", 'Fare', 'Embarked']]\nX2['Sex'] = X2['Sex'].replace(to_replace=['male', 'female'], value=[0, 1])\nX2['Embarked'] = X2['Embarked'].replace(to_replace=['S', 'C', 'Q'], value=[0, 1, 2])\n\n#if the value for one of these features is not present, substitute by their average\nfor i in range(0, 891):\n    if np.isnan(X2['Fare'][i]):\n        X2['Fare'][i] = fare_avg\n    if np.isnan(X2['Age'][i]):\n        X2['Age'][i] = age_avg\n    if np.isnan(X2['Embarked'][i]):\n        X2['Embarked'][i] = 0\n\n#same thing done before, but now for the test data set\nX2test = testData[['Pclass', \"Age\", \"Sex\", \"SibSp\", \"Parch\", 'Fare', 'Embarked']]\nX2test['Sex'] = X2test['Sex'].replace(to_replace=['male', 'female'], value=[0, 1])\nX2test['Embarked'] = X2test['Embarked'].replace(to_replace=['S', 'C', 'Q'], value=[0, 1, 2])\n\nfor i in range(0, 418):\n    if np.isnan(X2test['Fare'][i]):\n        X2test['Fare'][i] = fare_avg\n    if np.isnan(X2test['Age'][i]):\n        X2test['Age'][i] = age_avg\n    if np.isnan(X2test['Embarked'][i]):\n        X2test['Embarked'][i] = 0\n\n#train the logistic regression model\ncosts_agelg, predictions_agelg, accuracy_agelg = logisticRegression(X2, Y, X2test, 0.003, 40000, 7)\n\nprint(\"Logistic regression trained\")\n\n#train the neural network model\ncosts_agenn, predictions_agenn, accuracy_agenn = neuralNetwork(np.transpose(X2), np.transpose(Y), np.transpose(X2test), 0.003, 100000, 7)\n\nprint(\"Neural network has been trained\")\n\n#use imputation to fix the data for NaN values\nX3 = trainData[['Pclass', \"Age\", \"Sex\", \"SibSp\", \"Parch\", 'Fare', 'Embarked']]\n\nX3['Sex'] = X3['Sex'].replace(to_replace=['male', 'female'], value=[0, 1])\nX3['Embarked'] = X3['Embarked'].replace(to_replace=['S', 'C', 'Q'], value=[0, 1, 2])\n\nfrom sklearn.preprocessing import Imputer\nimp = Imputer()\nX3 = imp.fit_transform(X3)\n\n#the same is done for the test set\nX3test = testData[['Pclass', \"Age\", \"Sex\", \"SibSp\", \"Parch\", 'Fare', 'Embarked']]\n\nX3test['Sex'] = X3test['Sex'].replace(to_replace=['male', 'female'], value=[0, 1])\nX3test['Embarked'] = X3test['Embarked'].replace(to_replace=['S', 'C', 'Q'], value=[0, 1, 2])\n\nX3test = imp.fit_transform(X3test)\n\n#train only neural network, because it seems to get better predictions\ncosts_imp, predictions_imp, accuracy_imp = neuralNetwork(np.transpose(X3), np.transpose(Y), np.transpose(X3test), 0.003, 100000, 7)\nprint(\"Neural network with imputation is trained!\")","execution_count":21,"outputs":[]},{"metadata":{"_uuid":"80c7f6ec76429ff11b02d45d7a12d9052089a3ed","_cell_guid":"ff5ac4c5-82dd-43c9-b353-33f756917e4c"},"cell_type":"markdown","source":"Once the models are trained, we can see how they did by looking at how the cost and accuracy progressed over the epochs. I am also printing the last value, so I have an idea how the model ended after training."},{"metadata":{"_uuid":"7af03f926d2fa78213a2a6b40e9dbaba1570ba93","_cell_guid":"80a7d4a9-6705-45d0-bd5b-7af2210a05cf","trusted":true},"cell_type":"code","source":"print(\"Logistic Regression with Age: \", costs[-1], \" and with no Age: \", costs_na[-1])\nprint(\"Neural Network with Age: \", costs_nn[-1], \" and with no Age: \", costs_na_nn[-1])\nprint(\"More features for Logistic Regression: \", costs_agelg[-1], \" and Neural Network: \", costs_agenn[-1])\nprint(\"Neural network with imputation: \", costs_imp[-1])\n\n#plot the loss in terms of number of epochs for each set of data\nplt.figure(1, figsize=(12, 6))\nplt.subplot(121)\nplt.plot(costs)\nplt.title('Cross Entropy Loss - LR, with age')\nplt.xlabel('epoch')\nplt.ylabel('loss')\n\nplt.subplot(122)\nplt.plot(costs_na)\nplt.title('Cross Entropy Loss - LR, without age')\nplt.xlabel('epoch')\nplt.show()\n\nplt.figure(2, figsize=(12, 6))\nplt.subplot(121)\nplt.plot(costs_nn)\nplt.title('Cross Entropy Loss - NN, with age')\nplt.xlabel('epoch')\nplt.ylabel('loss')\n\nplt.subplot(122)\nplt.plot(costs_na_nn)\nplt.title('Cross Entropy Loss - NN, without age')\nplt.xlabel('epoch')\nplt.ylabel('loss')\n\nplt.figure(3, figsize=(12, 6))\nplt.subplot(131)\nplt.plot(costs_agelg)\nplt.title('Cross Entropy Loss - LG, with more features')\nplt.xlabel('epoch')\nplt.ylabel('loss')\n\nplt.subplot(132)\nplt.plot(costs_agenn)\nplt.title('Cross Entropy Loss - NN, with more features')\nplt.xlabel('epoch')\nplt.ylabel('loss')\n\nplt.subplot(133)\nplt.plot(costs_imp)\nplt.title('Cross Entropy Loss - NN, imputation')\nplt.xlabel('epoch')\nplt.ylabel('loss')","execution_count":22,"outputs":[]},{"metadata":{"_uuid":"aa1e02b0415f08d3f7a9467f841f92de909a5798","_cell_guid":"1c698a5d-1d84-49f8-a325-cff3a3f6ef3b","trusted":true},"cell_type":"code","source":"#plot the accuracy in terms of number of epochs for each set of data\nprint(\"Logistic Regression with Age: \", accuracy[-1], \" and with no Age: \", accuracy_na[-1])\nprint(\"Neural Network with Age: \", accuracy_nn[-1], \" and with no Age: \", accuracy_na_nn[-1])\nprint(\"More Features for Logistic Regression: \", accuracy_agelg[-1], \" and Neural Network: \", accuracy_agenn[-1])\nprint(\"Neural Network with imputation: \", accuracy_imp[-1])\n\nplt.figure(4, figsize=(12, 6))\nplt.subplot(121)\nplt.plot(accuracy)\nplt.title('Model Accuracy - LR, with age')\nplt.xlabel('epoch')\nplt.ylabel('accuracy')\n\nplt.subplot(122)\nplt.plot(accuracy_na)\nplt.title('Model Accuracy - LR, without age')\nplt.xlabel('epoch')\n\nplt.figure(5, figsize=(12, 6))\nplt.subplot(121)\nplt.plot(accuracy_nn)\nplt.title('Model Accuracy - NN, with age')\nplt.xlabel('epoch')\nplt.ylabel('accuracy')\n\nplt.subplot(122)\nplt.plot(accuracy_na_nn)\nplt.title('Model Accuracy - NN, without age')\nplt.xlabel('epoch')\nplt.ylabel('accuracy')\n\nplt.figure(6, figsize=(12, 6))\nplt.subplot(131)\nplt.plot(accuracy_agelg)\nplt.title('Model Accuracy - LG, with more features')\nplt.xlabel('epoch')\nplt.ylabel('accuracy')\n\nplt.subplot(132)\nplt.plot(accuracy_agenn)\nplt.title('Model Accuracy - NN, with more features')\nplt.xlabel('epoch')\nplt.ylabel('accuracy')\n\nplt.subplot(133)\nplt.plot(accuracy_imp)\nplt.title('Model accuracy - NN, imputation')\nplt.xlabel('epoch')\nplt.ylabel('accuracy')\n\nplt.show()","execution_count":23,"outputs":[]},{"metadata":{"_uuid":"48ce3645a3505286922a97388539d839c351111d","collapsed":true,"_cell_guid":"234f0510-b6d5-4712-ae8d-a8d79c10f37b","trusted":true},"cell_type":"code","source":"#Build a matrix combining all the results obtained so far and output it to a csv file for each of the models tested\nnames = testData[['PassengerId']]\n\nresult_age = np.concatenate((np.reshape(Xidx_age, (332, 1)),predictions), axis=1)\nresult_noage = np.concatenate((np.reshape(Xidx_noage, (86, 1)),predictions_na), axis=1)\n\nresult = np.concatenate((result_age, result_noage), axis=0)\n\nmyfile = open('output_lg.csv','w')\n\ncolumn= ['PassengerId', 'Survived']\n\nwrtr = csv.writer(myfile, delimiter=',')\nwrtr.writerow(column)\nwrtr.writerows(result)\nmyfile.close()\n\nresult_age = np.concatenate((np.reshape(Xidx_age, (332, 1)),predictions_nn), axis=1)\nresult_noage = np.concatenate((np.reshape(Xidx_noage, (86, 1)),predictions_na_nn), axis=1)\n\nresult_nn = np.concatenate((result_age, result_noage), axis=0)\n\nmyfile = open('output_nn.csv','w')\n\ncolumn= ['PassengerId', 'Survived']\n\nwrtr = csv.writer(myfile, delimiter=',')\nwrtr.writerow(column)\nwrtr.writerows(result_nn)\nmyfile.close()\n\nresult_agelg = np.concatenate((np.reshape(Xidx, (418, 1)),predictions_agelg), axis=1)\n\nmyfile = open('output_agelg.csv','w')\n\ncolumn= ['PassengerId', 'Survived']\n\nwrtr = csv.writer(myfile, delimiter=',')\nwrtr.writerow(column)\nwrtr.writerows(result_agelg)\nmyfile.close()\n\nresult_agenn = np.concatenate((np.reshape(Xidx, (418, 1)),predictions_agenn), axis=1)\n\nmyfile = open('output_agenn.csv','w')\n\ncolumn= ['PassengerId', 'Survived']\n\nwrtr = csv.writer(myfile, delimiter=',')\nwrtr.writerow(column)\nwrtr.writerows(result_agenn)\nmyfile.close()\n\nresult_imp = np.concatenate((np.reshape(Xidx, (418, 1)),predictions_imp), axis=1)\n\nmyfile = open('output_imp.csv','w')\n\ncolumn= ['PassengerId', 'Survived']\n\nwrtr = csv.writer(myfile, delimiter=',')\nwrtr.writerow(column)\nwrtr.writerows(result_imp)\nmyfile.close()","execution_count":24,"outputs":[]},{"metadata":{"_uuid":"3c1862520520d50192cc93d1ad5ea00550309155","collapsed":true,"_cell_guid":"456f0258-f9fa-4963-a2a9-fe748953e59b"},"cell_type":"markdown","source":"The best result for my models, so far, has been 0.78947 in the test set, using the neural network trained at the end."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}