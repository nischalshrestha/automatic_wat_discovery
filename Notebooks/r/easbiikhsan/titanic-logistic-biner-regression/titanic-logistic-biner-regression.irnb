{"cells":[{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"_uuid":"f5ded49d8b1bdabd9630013ecdad81593932408e","trusted":true,"_cell_guid":"b87a53b5-fb8c-4bed-af74-0505a8a9ac6f"},"cell_type":"code","source":"# This is my first kernel using R.\n# Titanic Disaster is something so serious in sea transportation. \n# First of all, I begin the analysis this data set using EDA (Exploratory data Analysis) \n# I consist of Visualization and Descriptive analysis, and also data preprocessing like missing values handle.\n# I am going to use Logistic Biner Regression, with respons : Survival ==> 0 = No, 1 = Yes\n# And many predictors like : \n\n# 1) PClass\n# 2) embarked\n# 3) sibsp\n# 4) parch\n\n\nlibrary(ggplot2) # Data visualization\nlibrary(readr) # CSV file I/O, e.g. the read_csv function\ntrain <- read_csv(\"../input/train.csv\")\nhead(train,10)","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"b27df66e9f4098f9b0cc84061f9bde05af2c4643","trusted":true,"_cell_guid":"27a26c6a-df04-41a3-8a8e-30e2a26d9e97"},"cell_type":"code","source":"#I try to see the missing value in each column. Here, i am using sapply funcntion :\nattach(train)\n# we see how many missing value in each attributes, here:\nsapply(train, function(x) sum(is.na(x)))\n","execution_count":4,"outputs":[]},{"metadata":{"_kg_hide-output":true,"_uuid":"38b5e47e878bd1ca585b31a67a834c2c961fe5b0","trusted":true,"_cell_guid":"9706edc6-7926-4785-b4ca-ab7ac05716c3"},"cell_type":"code","source":"#And I try to plot the missing values VS observation with missmap from Amelia's package. \nlibrary(Amelia)\nmissmap(train)\n#from the summary we can see that cabin, age, and embarked variables have the missing data \n#but the cabin variable has much more missing value, so i drop it from the dataset. \n#and for the age variable, i use mean imputation. And for Embarket variable, i use mode statistics.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9b43b0595f9cebb24cbb63c4b4326595f0f5c3c3","trusted":true,"_cell_guid":"6b962953-ab11-4c2b-afa0-5ba7062a6020"},"cell_type":"code","source":"d_train = train[,-1]\n\n#This is below the process of imputation Age's attribute using mean\nd_train$Age[is.na(d_train$Age)] = mean(d_train$Age, na.rm = T) \n# This is below the process of imputation Embarked's attribute using mode, using getmode function\n# We use mode beacuse the data is discrete\ngetmode <- function(v) {\n   uniqv <- unique(v)\n   uniqv[which.max(tabulate(match(v, uniqv)))]\n}\nd_train$Embarked[is.na(d_train$Embarked)] = getmode(d_train$Embarked)","execution_count":7,"outputs":[]},{"metadata":{"_uuid":"f5869ec7d6e3076c2f69c50c5f6f97cabcf5cbfb","trusted":true,"_cell_guid":"7df12753-1467-4b25-89d3-788f610be71e"},"cell_type":"code","source":"## Modelling Data\n#After the EDA step, i try to modelling the data to model the survived variable depends on other variable. \n#here i am using logistic biner regression to classified survived = 1, or not survive = 0.\nmodel1 <- glm(Survived ~ Pclass + Age + Sex +SibSp + Parch + Fare + Embarked, family = binomial(link='logit'), data=d_train)\nsummary(model1)","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"245a74b5b64fb3aa8c11b3cc83f326305d2b5211","trusted":true,"_cell_guid":"6b8889cd-2009-4980-8579-5aebf778af21"},"cell_type":"code","source":"\n#After modelling, we see that Pclass, Age, Sex, SibSp have siginificant in model. \n#And for the goodness of the model we see AIC 802,78.\n#next i try to use the 2 other model like below : \nmodel2 <- glm(Survived ~ Pclass + Age + Sex +SibSp, family = binomial(link='logit'), data=d_train)\nsummary(model2)","execution_count":9,"outputs":[]},{"metadata":{"_uuid":"66ad8f7a854d70a521c80a462385dba01c873460","trusted":true,"_cell_guid":"bc0a4f47-b8e5-4fc0-aad0-5b00a9c3d8f9"},"cell_type":"code","source":"#and the model3 like below : \nmodel3 <- glm(Survived ~ Pclass + Age + Sex, family = binomial(link='logit'), data=d_train)\nsummary(model3)","execution_count":10,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"967efd598ea8dfb7ba623ff3310c6cbd7e577fd2"},"cell_type":"code","source":"# In this cases, i will use logistic regression with step function to find best subset wit AIC lower\n# Model 1 as full model, and use find AIC backward as below\nmodel.aic.backward <- step(model1, direction = \"backward\", trace = 1)","execution_count":11,"outputs":[]},{"metadata":{"_kg_hide-output":true,"_uuid":"64666b1ffdd2c685e71a298944610aaa53c9de5a","trusted":true,"_cell_guid":"26b32e1f-3a64-4e6c-b28d-ace7e1612b6e"},"cell_type":"code","source":"#we see that the AIC is lower at 799.91 using logistic biner regression, with variable X -> Pclass, Age, Sex SibSp, and Embarked\n\nmodel_final <- glm(Survived ~ Pclass + Age + Sex +SibSp + Embarked, family = binomial(link='logit'), data=d_train)\nsummary(model_final)\n##Conclusion\n#that's all about modelling the data. \n#Thanks.","execution_count":13,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2ac13ed0d2febab506574c8dfcb154ff4c14a0c4"},"cell_type":"code","source":"# Thats's ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"R","language":"R","name":"ir"},"language_info":{"mimetype":"text/x-r-source","name":"R","pygments_lexer":"r","version":"3.4.2","file_extension":".r","codemirror_mode":"r"}},"nbformat":4,"nbformat_minor":1}