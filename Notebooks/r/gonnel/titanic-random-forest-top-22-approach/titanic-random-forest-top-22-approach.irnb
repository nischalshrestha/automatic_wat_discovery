{"cells":[{"metadata":{"_uuid":"289cb685f16a526f3b83ee1d25a34282fc7173ed"},"cell_type":"markdown","source":"**Machine Learning from Disaster: Titanic. **\n\n\n** As Titanic 2 is set for sail, Kagglers should particularly be interested in knowing what was the most likely way for one to survive on the Titanic. \n    This kernel  is a very simple approach to the Titanic competition and is meant mainly as a step by step workflow on what Machine Learning entails.**\n    ****  Please share,upvote, provide comments.** **\n**It really keeps most of us going!**\n    \n    \n**** While I'm still a beginner myself, I believe I've learnt a few things and communication is an important part of this field just like in many other fields.****\n\n**** Let's get started by taking  a look at the general structure of the dataset.****\n    "},{"metadata":{"_uuid":"fb96e1aa8df872062673f8ce6dd3617621162d61"},"cell_type":"markdown","source":"**Loading packages and training data**"},{"metadata":{"trusted":true,"_uuid":"c2f04a500a3373db49c01804e549e948066d2ccf","_kg_hide-input":true},"cell_type":"code","source":" options(warn= -1)\n  nomsg<-suppressMessages\n   nomsg(library(tidyverse))\n   nomsg(library(caret))\n    nomsg(library(mice))\n      nomsg(library(RANN))\ntrain<-read.csv(\"../input/train.csv\")\ntrain<-as.tibble(train) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b9b5366a57fd87d57c58b2812b6eda3c76ddcb62"},"cell_type":"markdown","source":"**A Peek at the Data**"},{"metadata":{"trusted":true,"_uuid":"de4b696893bfdb2ea8bf5f077312374758f74fb1","_kg_hide-input":true},"cell_type":"code","source":"str(train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f36bd47acb637cc0976bc231f546a3ddc701df8b"},"cell_type":"markdown","source":"**From that first step, we can see a few things we need to do.**\n1. PassengerId is entered as numerical data. This would make our analysis erroneous. One can decide to either discard this column straight away or turn it into categorical data. I prefer the latter. We'll do the same with the passenger class(PClass).\n2.  We could do some feature engineering(on the Name column) to extract the title. I did this locally and found no real difference in acccuracy. \n3.  Eliminate some columns that may not be useful for predicting survival. You could do this now or do it after visualizing and observing some trends in the data and/or feature importance or correlation plots. \n\n All set up, let's implement what we've planned to do. \n"},{"metadata":{"_uuid":"cf55c1cc7c7425f36453fc48d4fcbb8128edff22"},"cell_type":"markdown","source":"**Data Cleaning step.**\n  I've introduced an AgeGroup column just to easen visualization and it may hold some information of the data especially the age information in which case we could choose to drop the Age column. "},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"3233333f0e659f337aaaaa3576fe2b1117929836"},"cell_type":"code","source":"#Remove cabin,name,Ticket for now as these may not predict survival \ntrain<-train %>% \n  mutate(PassengerId=as.factor(PassengerId),Pclass=as.factor(Pclass),\n         Survived=as.factor(Survived),AgeGroup=as.factor(findInterval(Age,c(0,18,35,100)))) %>% \n select(-PassengerId,-Name,-Ticket,-Cabin,-Embarked)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1d6ae3a10ad9ea2e18f2c92ffc8aacd006b33434"},"cell_type":"markdown","source":"**Missing Data.**\n  No matter how good you're at modeling, without the right data, your model is as good as useless. The classic adage goes \"Garbage In garbage out\". Therefore a key first step in any analysis is to check for any missing data and find out why it's missing. Missing values can mean one of the following:\n1.   Intentionally missing and labelled as \"NA\". This could for example be due to a participant lacking a response in which case this would be considered a binary \"0\".\n2. Missing by mistake: Errors are human and ultimately some data will contain unintentional missing values. These need to therefore be imputed carefully.\n3. Missing due to no data. This is tricky and what to do will depend on context and sample size. If the missing data is say 96%, discarding it would only leaving you with4% data. \"Low sample sizes, high standard errrors, useless conclusion.\" This is where imputation comes in handy.\nOn the other hand, let's say only one value is missing. Do you discard it? Impute it? This is a question left to the reader. \n\n**  Now that we know a few problems with missing data, let's check if we have any missing data in our dataset.**"},{"metadata":{"_uuid":"a53874fd2ab3e7dac14d572ba94f215c1b0b09a9"},"cell_type":"markdown","source":"**Check For Missing values.**"},{"metadata":{"trusted":true,"_uuid":"9fb5d3fe3fc7ccfa883cc4b46892ad24dbbd6d22"},"cell_type":"code","source":"#View and deal with nas\nanyNA(train$AgeGroup) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7e64a9a530a48938ec326409f3d8b14e64234e63"},"cell_type":"markdown","source":"**Who's missing aboard the Titanic?**\n\n**We're unlucky!!! Our dataset contains missing values. But where excatly are the missing values?!**"},{"metadata":{"trusted":true,"_uuid":"adbdf6f84a25ced53b9ba0859fb24e0e09eb0a13","_kg_hide-input":true},"cell_type":"code","source":"nomsg(require(Amelia))\nmissmap(train,col=c(\"snow\",\"indianred4\"),main=\"Who's missing aboard the Titanic?!\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b1f628e1cc68533a07d58d498802f1276b542f51"},"cell_type":"markdown","source":"**From the above plot, we can see that our missing values are in the Age and AgeGroup columns(essentially the same thing). There are many ways to impute missing values. \n  I tested different methods and found imputation with the median to be most effective. If you're interested you could use random forests or classification and regression trees to impute. There's a beautiful paper that compares these two latter methods.**\n   You can read it here: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3939843/ \n   "},{"metadata":{"trusted":true,"_uuid":"94811f120477a2b299a212fb8b5dc66228bc440c"},"cell_type":"markdown","source":"**Imputing Missing Values**"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"2cde4a026a733c3e979aee0791005a6fc2e8c148"},"cell_type":"code","source":"#Change levels\nlevels(train$AgeGroup)<-c(\"Young\",\"Mid Age\",\"Aged\")\nlevels(train$Sex)<-c(\"F\",\"M\")\n#Impute median\ntrain_1<-mice(train,m=3,method=\"cart\",printFlag = F)\ntrain_imp<-complete(train_1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6f5027159849d1e90a61d6f301d15bc9eff9fd5e"},"cell_type":"markdown","source":"**Do we still have missing values?**"},{"metadata":{"trusted":true,"_uuid":"b813cc80abe83ae6d68b6ebac4622e22c8a0f21b"},"cell_type":"code","source":"#checkNAs\nanyNA(train_imp) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"21e80b903783be06b32b73b214631d9f427497c8"},"cell_type":"markdown","source":"**Great! We've successfully replaced missing values. In the process of imputation however, our levels were lost and we thus need to redo them. **"},{"metadata":{"trusted":true,"_uuid":"c1c583bddcd6c155dcc641e35dbec92128a38220"},"cell_type":"code","source":"#redo levels\ntrain_imp<-train_imp %>% \n  mutate(AgeGroup=as.factor(findInterval(Age,c(0,18,35,100))))\nlevels(train_imp$AgeGroup)<-c(\"Young\",\"Mid Age\",\"Aged\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"63ee59697457a43073f7e8d050b944e9b1ac3029"},"cell_type":"markdown","source":"**Visualization.**\n  This should ideally be done early on during the EDA. Let's do a very simple visualization to see where our data is found and if we can observe any trends."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"1fefba37f26528d84cff62b7ce4234f8d6c6f89d"},"cell_type":"code","source":"#Let's visualise survival by Age Group\ntrain_imp %>% \n   ggplot(aes(Survived,fill=Sex))+geom_histogram(stat=\"count\")+facet_wrap(AgeGroup~Pclass)+\n  ggtitle(\"Survival by class,Agegroup and Gender\")+theme(plot.title=element_text(hjust=0.5))+\n scale_fill_manual(values=c(\"steelblue3\",\"orange\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4c1aff08896af877311cf3135ea56bac1242e7c2"},"cell_type":"markdown","source":"**Women and Children First!! It didn't work.**\n  As ambitious as it was, the women and children first policy failed to save those in the third class. Generally, those in the third class had lower chances of survival and especially so for men and youths. \n    As you get ready for Titanic 2, keep in mind that as a young man you were in an even more disadvantaged situation. Could this be due to more males than females? More youths? I"},{"metadata":{"_uuid":"0c3c125e6a34867ac5fb34dd522959e728bccc45"},"cell_type":"markdown","source":"**Machine Learning: How well can machines help us save future passengers?**"},{"metadata":{"_uuid":"4e4d1cce416345633276107136e85191e1aa3711"},"cell_type":"markdown","source":"**Partition Our Data into a Training and Validation Set.**\n  \n   A first step in model building is creating a set to train our models out of the train set and another to validate our train. This is important to give you a general first impression of how welll the model does on unseen data.\n    I've used a repeated cv as it did pretty well when I first built these models back then. "},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"17136fa6d4449d55dc3e23e41baa28ba0d5019f6"},"cell_type":"code","source":"#Create partition\ntrain1<-createDataPartition(train_imp$Survived,p=0.8,list=F)\nvalidate<-train_imp[-train1,]\ntrain1<-train_imp[train1,]\n#Set metric and control\ncontrol<-trainControl(method=\"repeatedcv\",number = 10,repeats = 3)\nmetric<-\"Accuracy\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"931005adc88398547af978e6fca327dcac456190"},"cell_type":"markdown","source":"**Building Our Models.**"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"f091ecd53312948c4f3338c555359591acf14c3c"},"cell_type":"code","source":"#Set up models\nset.seed(233)\nfit.knn<-train(Survived~.,data=train1,method=\"knn\",trControl=control,metric=metric)\nset.seed(233)\nfit.svm<-train(Survived~.,data=train1,method=\"svmRadial\",trControl=control,metric=metric)\nset.seed(233)\nfit.cart<-train(Survived~.,data=train1,method=\"rpart\",trControl=control,metric=metric)\nset.seed(233)\nfit.rf<-train(Survived~.,data=train1,method=\"rf\",trControl=control,metric=metric)\nset.seed(233)\nfit.nb<-train(Survived~.,data=train1,method=\"nb\",trControl=control,metric=metric,verbose=F)\n#Try Gradiet Boosting\nset.seed(233)\nfit.gbm<-train(Survived~.,data=train1,method=\"gbm\",trControl=control,metric=metric,verbose=F)\nresult<-resamples(list(knn=fit.knn,svm=fit.svm,cart=fit.cart,rf=fit.rf,nb=fit.nb,gbm=fit.gbm))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"38900de229e1077bf19728ba1a53f316e18d6672"},"cell_type":"markdown","source":"**Which Model performs best?**\n \n Here we visualize our model accuracies to see how well they do."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"879308297125c7dc1ee65851353ea2935abef368"},"cell_type":"code","source":"dotplot(result) ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d10b48824c4663cd56bc690851d49b72b72434bf"},"cell_type":"markdown","source":"We see that high up on the ladder are gbm,svm and random forest. Let's get the model details for only the \"winner\". **The Random Forest.**"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"605081d48c91c265d82db035fc4fb75cd237d13c"},"cell_type":"code","source":"getTrainPerf(fit.rf)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bcfad22b71844fe9c05ffd03e803bb906ed6a796"},"cell_type":"markdown","source":"**Validation.**\n\n Not too bad. Let's test this on our validation dataset.\n We take a look at the confusion matrix to see how \"confused\" our model is. "},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"d6c63d4af5a6e2f338157472e36e9d3c77194611"},"cell_type":"code","source":"#Validate \npredicted<-predict(fit.rf,validate)\ncmat<-confusionMatrix(predicted,validate$Survived)\ncmat$overall","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ba03bb5365a66269b196c6872b63d4d378c8d93"},"cell_type":"markdown","source":"**From the above table, we see that our model does a pretty good job on unseen data. We can therefore deploy it on our test dataset.**"},{"metadata":{"_uuid":"aa30266b8fef82521c7c151f24e6c9ad86c75015"},"cell_type":"markdown","source":"**Who's the King in The Forest?**\n  For those planning to sail aboard Titanic 2, you would be interested in knowing which were the most useful features for predicting survival on the Titanic. Let's see!"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"a83eb21436207e0d13346d1745b26ffb53c95d45"},"cell_type":"code","source":"plot(varImp(fit.rf,main=\"Who's running our forest?\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"601c4133440aca9e29a5e70b4db572218db3d78b"},"cell_type":"markdown","source":"**There we have it folks! The bigger the wallet, the better your survival chances. Your age was also important as was your gender. Take this into consideration!**"},{"metadata":{"_uuid":"ce23ad7518119238258edb9de9930bdeedb65f59"},"cell_type":"markdown","source":"**Getting our test data ready.**\n Here we basically just make our test dataset similar to our train data. "},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"9af14553e871c446f5f17b87f29e0066c933b270"},"cell_type":"code","source":"test<-read.csv(\"../input/test.csv\")\n#............\ntest<-test %>% \n  mutate(PassengerId=as.factor(PassengerId),Pclass=as.factor(Pclass),\n        AgeGroup=as.factor(findInterval(Age,c(0,18,35,100)))) %>% \n  select(-Ticket,-Cabin,-Name,-Embarked)\nlevels(test$AgeGroup)<-c(\"Young\",\"Mid Age\",\"Aged\")\nlevels(test$Sex)<-c(\"F\",\"M\")\n#Make as train data\n#Preprocess and remove NAs from age and Fare\n   test2<-preProcess(test,method=\"medianImpute\")\n   test2_imp<-predict(test2,test)\n   #map nas\n   test2_imp<-test2_imp %>% \n     mutate(AgeGroup=as.factor(findInterval(Age,c(0,18,35,100))))\n   #.....\n   levels(test2_imp$AgeGroup)=c(\"Young\",\"Mid Age\",\"Aged\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9d6a09683c392f43625c791e773ae9bce6da2238"},"cell_type":"markdown","source":"**The Moment of Truth.**"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"dca1cda3b9bcc534153a7b9597725a5c492689ee"},"cell_type":"code","source":"#Try on test data\npredictedtest<-predict(fit.rf,test2_imp,na.action=na.pass)\n#Set column\nSurvival<-test2_imp%>% \n  mutate(Survived=predictedtest) %>% \n  select(PassengerId,Survived)\n#find the confusion matrix\ncm<-confusionMatrix(predictedtest,Survival$Survived)\ncm$overall","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6c429297177d609df0d05b41f13aee61d9d8937d"},"cell_type":"markdown","source":"**We see that our model does a pretty good job. It did put me in the top 22% back then. I'm still going up the ladder but it was a nice model.**\n  Let's attempt to tune the model."},{"metadata":{"_uuid":"375aeb14788c61e94df53401aa7d46e0ebe38d01"},"cell_type":"markdown","source":"**Tuning the Model**"},{"metadata":{"trusted":true,"_uuid":"9b94870d7ecc786f6df801cad89aada246bd2a7f"},"cell_type":"code","source":"control12<-trainControl(method=\"repeatedcv\",number=10,repeats=3,search=\"grid\")\ntunegrid<-expand.grid(.mtry=c(1:15))\nset.seed(233)\nrf_tuned<-train(Survived~.,data=train1,method=\"rf\",trControl=control12,metric=metric,\n               tuneGrid=tunegrid)\ngetTrainPerf(fit.rf)\ngetTrainPerf(rf_tuned)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4075a5f850f8b15e5c0c5c264ed3567134e2c9d3"},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"e505be64ca44fac5fbb306d490fd29e32640b9d5"},"cell_type":"markdown","source":"We see that our tuned model does a slightly  better job on the train dataset. For purposes of this kernel version I'll not tune the model further. Let's test the tuned model our test data."},{"metadata":{"_uuid":"03a844a86a51307bb2fa6c43154f8cdf62fac7a3"},"cell_type":"markdown","source":"**Test Tuned Model on Test Data.**"},{"metadata":{"trusted":true,"_uuid":"db6e2dff1c982d85dd15cbc91e8fd0b20fef488a"},"cell_type":"code","source":"tunedprediction<-predict(rf_tuned,test2_imp,na.action=na.pass)\nSurvivalTuned<-test2_imp %>% \n              mutate(Survived=tunedprediction)\ncm2<-confusionMatrix(tunedprediction,SurvivalTuned$Survived)\ncm2$overall","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"12f4f8410575305461ab69786f6b496b661bf778"},"cell_type":"markdown","source":"This marks the end of this simple analysis.\n**Future Steps:**\n1. More Feature Engineering.\n2. Model Tuning.\n3. Improve visualization.\n4. Show Effectiveness of different imputation processes.\n\n****  Please share,upvote, provide comments.** **\n**It really keeps most of us going!**"}],"metadata":{"kernelspec":{"display_name":"R","language":"R","name":"ir"},"language_info":{"mimetype":"text/x-r-source","name":"R","pygments_lexer":"r","version":"3.4.2","file_extension":".r","codemirror_mode":"r"}},"nbformat":4,"nbformat_minor":1}