{"metadata": {"kernelspec": {"display_name": "R", "language": "R", "name": "ir"}, "language_info": {"mimetype": "text/x-r-source", "codemirror_mode": "r", "pygments_lexer": "r", "version": "3.4.2", "file_extension": ".r", "name": "R"}}, "nbformat": 4, "nbformat_minor": 1, "cells": [{"metadata": {"_cell_guid": "c81c6813-cfc9-46cf-b4c3-1b22ddd50a3f", "_uuid": "62f669029e53db764d8f0a11c710d3e9a3101263"}, "cell_type": "markdown", "source": ["# Exploring Survival on Titanic\n", "\n", "**Nishikant Sinha**\n", "\n", "**29 January 2018**\n", "\n", "Here goes the Titanic Survival End to End ML Pipeline\n", "\n", "1 ) **Introduction**\n", "    \n", "    1.1 Load Packages\n", "    1.2 Load Data\n", "    1.3 Run Statistical Summeries\n", "    \n", "2 ) **Missing Value Imputation**\n", "    \n", "    2.1 Figue out missing value\n", "    2.2 Sensible Imputation\n", "    2.3 Predictive Imputation\n", "    \n", "3 ) **Feature Engineering**\n", "    \n", "    3.1 Extract Title and Surname from Name\n", "    3.2 Calculate Family Size\n", "    3.3 Identify the real families\n", "    3.4 Visualizations\n", "    3.5 Treat few more vaiables\n", "    3.6 Factoring and Scaling Variable\n", "   \n", "4 ) **Prediction and Submission**\n", "    \n", "    4.1 Split data into train and test\n", "    4.2 Build First Classification Model ( i.e. Logistic Regression)\n", "    4.3 Naive Base Model\n", "    4.4 Support Vector Machine\n", "    4.5 K Nearest Neighbour\n", "    4.6 Decision Trees\n", "    4.7 Random Forest\n", "    4.8 Conditional Random Forest\n", "  \n", "5 ) **Conclusion**"]}, {"metadata": {"_cell_guid": "ab9917b0-fcf5-4188-887d-bd92d6d40759", "_uuid": "58f210ebc20579e6f5d87f1dd800e1fae3f8be5b"}, "cell_type": "markdown", "source": ["# 1 Introduction\n", "Hi Folks! The purpose of this notebook is to demonstrate the complete cycle of any EDA and predictive model implementation.\n", "\n", "Here we will be focusing on loading data, cleaning data, missing data imputation, basic visualization and predictive model building for predicting survivals on Titanic. I am new to machine learning, so please correct me if there are any mistakes, feedback and suggestion will be highly appreciated. Now let's begin our journey of exploaring Titanic Survival.\n"]}, {"metadata": {"_cell_guid": "764e3927-9953-4ea6-a19a-4b2b3a1935bd", "_uuid": "d6def4774cc8c0f6f5f68632b8fad3001e50e786"}, "cell_type": "markdown", "source": ["# 1.1 Load Packages"]}, {"source": ["# Disable warnings\n", "options(warn=-1)\n", "\n", "# Load Packages\n", "library(tidyverse)     # collection of libraries like readr, dplyr, ggplot\n", "library(rpart)         # classification algorithm\n", "library(mice)          # imputation\n", "library(scales)        # Visualization\n", "library(ggthemes)      # Visualization\n", "library(class)         # classification algorithm\n", "library(e1071)         # classification algorithm\n", "library(randomForest)  # classification algorithm\n", "library(party)         # classification algorithm"], "metadata": {"_cell_guid": "b24f088f-6e7d-4585-8410-bd80f331fbc7", "_uuid": "5c51af92a2f6b3971d2a2c924bd2b62412cf79be"}, "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "13aa263b-e9d3-42e4-9a6d-589cc1693837", "_uuid": "32bcc0356a97b23786adaafc99b7a34cd9c4cec8"}, "cell_type": "markdown", "source": ["# 1.2 Load Train and Test data"]}, {"source": ["# Read datasets\n", "train <- read_csv(\"../input/train.csv\")  # Load train data\n", "test <- read_csv(\"../input/test.csv\")    # Load test data\n", "full <- bind_rows(train,test)     # combine training and test dataset "], "metadata": {"_cell_guid": "73a503b2-f138-4ced-953f-186a859a47ba", "_uuid": "60e9160c3778a817ddff10d10e3da5f0ebfc8550"}, "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "012e844b-71c1-41cc-8729-1410821a7796", "_uuid": "b6e968975ad901284e562d8928e2f77143123bf6"}, "cell_type": "markdown", "source": ["# 1.3 Run Statistical Summaries"]}, {"metadata": {"_cell_guid": "be7a20e0-21b3-41a0-a4b0-c8f9b8c06ca9", "_uuid": "f9f549c5f3f4b307805385e3f2e1396201bc1caa"}, "cell_type": "markdown", "source": ["Let's have a quick look to our dataset."]}, {"source": ["head(full)  # View few records of loaded data"], "metadata": {"_cell_guid": "17ff2005-a8c7-4e8e-b21a-7f43a2d8ae92", "_uuid": "c6363fda831afaa05b4f6bcb447b672955c0cc91"}, "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "867e698b-3fc3-46b2-85b9-dc82f87cf0ff", "_uuid": "ca9ceeca9074bcb916c39b5b4a041ead89b7393e"}, "cell_type": "markdown", "source": ["It seems that everything worked well till here and our data file is loaded properly.\n", "\n", "Now let's see how many variables are there, what are there types and how many number of records are present. These can be easily known by R inbuilt function str()."]}, {"source": ["# Check Structure of data set\n", "str(full)"], "metadata": {"_cell_guid": "8f32309d-75ac-4437-916a-441c346cd847", "_uuid": "537565dad19f9da5f465ce625a3f1ecd898e414d"}, "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "33d1b5ce-e162-4256-8ec7-067629a9a2a7", "_uuid": "48c4a7f3c7753701dc07f0ed96606affc90f685e"}, "cell_type": "markdown", "source": ["We have got some sense of our variable, their class type, first few observations of each. Now we know that we are working with 12 variables and 1309 observations. Though these are very basic information about our dataset but these are of great use for further analysis."]}, {"metadata": {"_cell_guid": "b020cbc1-85d4-446a-9008-6a0bd3cb6459", "_uuid": "653229ddedb497216e737566916577bb304edd99"}, "cell_type": "markdown", "source": ["# 2 Missing Value Imputation\n", "The next and impotant part of any EDA is data cleaning. It's first and foremost step is to check for missing values in dataset. So let's see how many missing values each variable is have."]}, {"source": ["# Apply sum(is.na()) to each variable to check missing values\n", "sapply(full, function(x) sum(is.na(x)))\n", "# sapply() is used to apply a function to each element of list, dataframes or vectors."], "metadata": {"_cell_guid": "de76f01e-892c-4308-85a0-3f9dee8497d5", "_uuid": "b39e4c2d72416d4aa186adf6b183359c9a9da1b2"}, "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "78e804e3-77ad-489b-a034-2975ae331737", "_uuid": "4f9a156fda279642cc89448d09d57c2f49a056ac"}, "cell_type": "markdown", "source": ["There are 5 variables which have missing values i.e Survived, Age, Fare, Cabin and Embarked. \n", "\n", "Among these Survived variable doesn't need any treatment as we have to predict its value using machine learning predictive models, we will not impute the Cabin variable as it has too much sparseness. The variables which need to be treated are Age, Fare, and Embarked."]}, {"metadata": {"_cell_guid": "30bd2139-94b4-427f-81cb-791d5141060d", "_uuid": "06c9a27747baca0d1d213437f6709d69241f881c"}, "cell_type": "markdown", "source": ["# 2.1 Sensible Value Imputation\n", "So first let's begin `Sensible Value Imputation` process with Fare variable as it has only 1 missing value so sensible value imputation will be a good option for this."]}, {"source": ["# Find which observation has missing fare value\n", "which(is.na(full$Fare)==TRUE)   # which() will return indexes of the observation which has missing value for Fare\n", "# 1044 is the index of missing fare value\n", "\n", "full[1044,]   # View complete information of obbseravation which has missing Fare value"], "metadata": {"_cell_guid": "3d886c7c-8ca2-4a03-9860-298d240644a8", "_uuid": "6de56bcb663a7bbcd7dcb5923bdf9deb085b1575"}, "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "c62cab0d-4403-43a0-9508-92f9815a0b64", "_uuid": "3a0ffb88fa0264883f8946f11308ada776309824"}, "cell_type": "markdown", "source": ["The Passenger which has missing fare value belongs to class 3 and departed from Southampton(S), so we are going to visualize the fares distribution among the passengers sharing the same class and embarkment.\n", "\n", "We will use ggplot which is one of the most powerful visualization tool in R."]}, {"source": ["# Set the height and weight of the plot\n", "options(repr.plot.width=6, repr.plot.height=4)\n", "\n", "# Plot Fare distrfibution of class 3 passenger who embarked from port S\n", "ggplot(full[full$Pclass == '3' & full$Embarked == 'S',], aes(x = Fare)) +   \n", "geom_density(fill = 'royalblue', alpha ='0.7') +\n", "geom_vline(aes(xintercept=median(Fare, na.rm=T)), colour='red', linetype='dashed', lwd=1) +\n", "scale_x_continuous(labels=dollar) +\n", "theme_few()"], "metadata": {"_cell_guid": "1de350a1-5bb5-44e8-8ecd-437bdf201f30", "_uuid": "358e323617fbd3c20e5f4c99e9fc2af5958b8ee6"}, "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "0ad6a573-4bf4-4811-a1fa-dbb88112a0dc", "_uuid": "72b2bac98e9752e4b848619c83a3678d69995804"}, "cell_type": "markdown", "source": ["From this visualization it seems quite sensible to replace the missing fare value with median fare of class 3 passengers whose embarkment  port was 'S'."]}, {"source": ["# Replace missing fare with median fare for class and embarkment.\n", "full$Fare[1044] <- median(full[full$Pclass == 3 & full$Embarked == 'S', ]$Fare,na.rm = TRUE)"], "metadata": {"_cell_guid": "ff91f199-1186-4a89-a276-5cd3a7b010cc", "_uuid": "a7a628e5af234b9d0019f361acb6520264719f19"}, "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "d606a16f-41e4-49ce-a113-bd15eb7f934e", "_uuid": "043db1a06f034beeccf7cb7675c5ae6d1a43364f"}, "cell_type": "markdown", "source": ["Next we are going to impute the missing Embarkment port. First let's see the which 2 passenger has missing embarkment port, also check their complete information."]}, {"source": ["# Find which observation has missing Embarkation value\n", "which(is.na(full$Embarked)==TRUE)\n", "# Passengers with IDs 62 and 830 has missing embarkation value\n", "\n", "# View complete information of passengers with ID 62 and 830\n", "full[c(62,830),]"], "metadata": {"_cell_guid": "5a681481-c96e-4cb9-998f-09bb54a10bdc", "_uuid": "c00145de95a85412893a2f32de5f3fadb7e3fdfa"}, "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "5ef1fb1b-1652-47c3-8525-f92f00432f96", "_uuid": "0ff4fd3247376bfcea46d808d5ee786184e04399"}, "cell_type": "markdown", "source": ["From this information it seems clear that there are 2 significant similarities between these two passengers i.e. both belongs to class 1 and have paid $80 as fare. So we can visualize the port of embarkation sharing similar information."]}, {"source": ["# Before visualization get rid of passenger Ids 62 and 830 having missing embarkment port\n", "plot_data <- full[-c(62,830),]\n", "\n", "# Use ggplot2 to visualize embarkment, passenger class, & median fare\n", "ggplot(plot_data, aes(x = Embarked, y = Fare, fill = factor(Pclass))) +\n", "geom_boxplot() +\n", "geom_hline(aes(yintercept=80), colour='red', linetype='dashed', lwd=2) +\n", "scale_y_continuous(labels=dollar) +\n", "theme_few()"], "metadata": {"_cell_guid": "c5f637d4-3957-41a5-9fce-6cbc94ff7703", "_uuid": "fbfc4be104d6a4fdd05c8caa7a92eb1fa71b865c"}, "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "4ea09953-e001-4cde-b2b3-57ea0323e3e1", "_kg_hide-output": false, "_uuid": "9814d640819f0b389060aaee5b65c3f39db8b983", "_kg_hide-input": false}, "cell_type": "markdown", "source": ["This visualization gives a clear picture of the median fare for a first class passenger departing from Charbourg (\u2018C\u2019) which coincides nicely with the $80 paid by our embarkment-deficient passengers. So the missing embarkement values can be replaced by 'C'.\n"]}, {"source": ["# Replace NAs in Embarked with 'C'\n", "full$Embarked[c(62,830)] <- 'C'"], "metadata": {"_cell_guid": "7c32b802-a33c-41b1-b7fb-1a8704b98860", "_uuid": "935f02e14a1f6d4212cf6c0e2d9cf1e0bced7e73"}, "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "fb4c09b5-949e-4d25-87b6-1a0e54281b7b", "_uuid": "3e047d9d3f38a79f0ac86c9d524429276e85ec50"}, "cell_type": "markdown", "source": ["# 2.3 Predictive Value Imputation\n", "Now we will be dealing with Age variable imputation as this variable has 263 missing which will be quite inaccurate to impute sensibly, thus we will use `Predictive imputation`. \n", " For this there are several approach that we can follow but in this notebook we will see only two methods the first one is rpart (recursive partitioning for regression) and more fancy method which is mice (multivariate imputation by chained equation)"]}, {"source": ["# Convert categorical variable into factors\n", "factor_vars <- c('Pclass','Sex','Embarked')\n", "full[factor_vars] <- lapply(full[factor_vars], function(x) as.factor(x))"], "metadata": {"_cell_guid": "fe0be894-fbd9-4c8a-ad61-39b8d2ce49b4", "scrolled": true, "_uuid": "375be03885826e1f756fbfa1399738b4adcbf3e9"}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": ["# Set a random seed\n", "set.seed(129)\n", "\n", "# Build rpart model for age imputation \n", "age_pred <- rpart(Age ~ Pclass + Sex + SibSp + Parch + Fare + Embarked,\n", "                  data = full[!is.na(full$Age),], method = \"anova\") \n", "\n", "# Use the rpart model to predict the missing age values\n", "imputed_age <- predict(age_pred, full[is.na(full$Age),])\n", "rpart_imputation <- full\n", "missing_age_indexes <- which(is.na(full$Age)==TRUE)\n", "rpart_imputation$Age[missing_age_indexes] <- imputed_age"], "metadata": {"_cell_guid": "5580cd3e-1508-4b91-82c8-044311b3906d", "scrolled": false, "_uuid": "646ecbfbeedbab5141600322e141f866baa247e8"}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": ["# Perform mice imputation, excluding certain less-than-useful variables:\n", "mice_mod <- mice(full[, !names(full) %in% c('PassengerId','Name','Ticket','Cabin','Survived')],\n", "                 method='rf') \n", "\n", "# Save the complete output \n", "mice_output <- complete(mice_mod)"], "metadata": {"_cell_guid": "f6692f29-3ae4-41a1-8b66-f70652a763f5", "_uuid": "44f6825953537b957b85ef126ec6b3e940e65edf"}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": ["# Plot age distributions\n", "par(mfrow=c(1,3))\n", "hist(full$Age, freq=F, xlab ='Passengers Age',main='Age: Original Data', \n", "  col='turquoise4', ylim=c(0,0.06))\n", "lines(density(full$Age,na.rm = TRUE), col=\"red2\", lwd=1.5)\n", "\n", "hist(rpart_imputation$Age, freq=F, xlab ='Passengers Age', main='Age: Rpart Output', \n", "  col='turquoise3', ylim=c(0,0.06))\n", "lines(density(rpart_imputation$Age), col=\"red2\", lwd=1.5)\n", "\n", "hist(mice_output$Age, freq=F, xlab ='Passengers Age', main='Age: MICE Output', \n", "  col='turquoise1', ylim=c(0,0.06))\n", "lines(density(mice_output$Age), col=\"red2\", lwd=1.5)"], "metadata": {"_cell_guid": "2194f450-dcb2-44c6-81d4-bac7ac3695c3", "_uuid": "972810ef93b1a843f2ea9f9707dffeab9f3bedc8"}, "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "6e7780f7-811e-4754-adc0-7a393ee84545", "_uuid": "27dc1930c265b7179240cf479ca9a6abf46af4eb"}, "cell_type": "markdown", "source": ["The above visualization makes it clear that MICE imputation has performed better than rpart as it gives more similar distribution with respect to original distribution."]}, {"source": ["# Replace Age variable from the mice model.\n", "full$Age <- mice_output$Age\n", "\n", "# Check if any missing Age values got replaced \n", "sum(is.na(full$Age))"], "metadata": {"_cell_guid": "44971c01-b24b-4255-8d35-8170808a6bcc", "_uuid": "0f620e39319983fdbfde137ad854e6e26416cf36"}, "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "fad22d56-b306-4ae6-a38c-7c59808c3b5a", "_uuid": "b15f342134bd5d4623365b518940077b2fb5f5ce"}, "cell_type": "markdown", "source": ["# 3 Feature Engineering\n", "The next part of this EDA is about doing some **feature engineering** (Feature engineering is the process of transforming raw data into features that better represent the underlying problem to the predictive models, resulting in improved model accuracy on unseen data) with our dataset."]}, {"metadata": {"_cell_guid": "f37d002c-784a-4975-9ca6-09ada68e2c32", "_uuid": "82b12d280a7fcb7200cc7e1764282a42aee54ec6"}, "cell_type": "markdown", "source": ["# 3.1 Extract Title and Surname from Name\n", "Here the most obvious attention seeker of feature engineering is Name variable as it contains alot of things which can be useful for our machine learning  model building.  We can break it down into additional meaningful variables which can feed predictions or be used in the creation of additional new variables."]}, {"source": ["# Extract title from passengers name\n", "full$Title <- gsub('(.*, )|(\\\\..*)', '', full$Name) \n", "# gsub() function replaces all matches of a string, if the parameter is a string vector, returns a string \n", "# vector of the same length and with the same attributes\n", "\n", "# Show title counts by sex\n", "table(full$Sex, full$Title)"], "metadata": {"_cell_guid": "4c47803d-b824-4594-8402-cebbaf5e17b6", "_uuid": "f1738271207994a9d701ae6ddf0cde1175626818"}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": ["# Titles with very low cell counts to be combined to \"rare\" level\n", "rare_title <- c('Dona', 'Lady', 'the Countess','Capt', 'Col', 'Don', \n", "                'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer')\n", "\n", "# Also reassign mlle, ms, and mme accordingly\n", "full$Title[full$Title == 'Mlle']        <- 'Miss' \n", "full$Title[full$Title == 'Ms']          <- 'Miss'\n", "full$Title[full$Title == 'Mme']         <- 'Mrs' \n", "full$Title[full$Title %in% rare_title]  <- 'Rare Title'\n", "\n", "# Show title counts by sex again\n", "table(full$Sex, full$Title)"], "metadata": {"_cell_guid": "8e6f4b0e-1342-466a-8882-7c051db298ba", "_uuid": "4459d1440f3454be5912711c6e46d60f5ce1aa35"}, "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "522324e5-c082-4e7e-b607-8d3b75a0db77", "_uuid": "75301b4058f20256e09f5040a7b60d5060f50087"}, "cell_type": "markdown", "source": ["Alright. We are done with passenger's Tilte now. What else can we think up? Well we could try extracting the Surname of passengers and group them to find families. "]}, {"source": ["# Extract surname from name variable\n", "full$Surname <- sapply(full$Name, function(x) {strsplit(x, split = '[,.]')[[1]][1]})\n", "\n", "# Check the number of unique Surnames\n", "cat(paste('We have ', nlevels(factor(full$Surname)), ' unique surnames.'))"], "metadata": {"_cell_guid": "bc0e948b-0da0-4102-aae7-e476006cbb1f", "_uuid": "af3678591375cc1344072d2a4eb06579face4de0"}, "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "a96d63b1-ee95-4cb1-aee2-6665a780c3be", "_uuid": "a7a4eeef487818dc97a649bde6ba63705eaf6837"}, "cell_type": "markdown", "source": ["But a common last name such as Johnson might have a few extra non-related people aboard. In fact there are three Johnsons in a family with size 3, and another three probably unrelated Johnsons all travelling solo.\n", "\n", "Combining the Surname with the family size though should remedy this concern. So let\u2019s extract the passengers\u2019 family size."]}, {"metadata": {"_cell_guid": "0b1b2ff6-81d7-43fc-adde-a627d610953b", "_uuid": "f4af33ceba25b01c526cd45860b8abd50cd8a2c3"}, "cell_type": "markdown", "source": ["# 3.2 Calculate Family Size\n", "Pretty simple! We just add the number of siblings, spouses, parents and children the passenger had with them, and plus one for their own existence of course, and have a new variable indicating the size of the family they travelled with."]}, {"source": ["# Add familySize variable to our dataset\n", "full$familySize <- full$SibSp + full$Parch + 1"], "metadata": {"_cell_guid": "415fa987-890d-44d2-a835-27a201ecb035", "_uuid": "7658958789c936dbe650a1d923bdf1a0224634c6"}, "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "4ce4c6bd-1bcc-4b8c-88fd-d30f62e89c5e", "_uuid": "cd88fe9fad6890ef2ae99ec576064ca8f5745bdf"}, "cell_type": "markdown", "source": ["# 3.3 Identify the real family\n", "We then want to append the FamilySize variable to the front of Surname, but as we saw with factors, string operations need strings. So let\u2019s convert the FamilySize variable temporarily to a string and combine it with the Surname to get our new FamilyID variable:"]}, {"source": ["# Combine familySize and Surname to make familyID \n", "full$familyID <- paste(as.character(full$familySize), full$Surname, sep = \"\")\n", "\n", "#full$familyID[full$familySize == 1] <- 'Singleton'\n", "full$familyID[full$familySize <= 2] <- 'Small'\n", "\n", "# View the count of each category of familyID\n", "table(full$familyID)"], "metadata": {"_cell_guid": "bc674515-0d8c-4ca5-9ecf-e5eacbe5454f", "_uuid": "8ee669a4816b90dcf136f4717490862d96ae6b98"}, "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "783a177b-9f05-4c2a-94df-326edab0475d", "_uuid": "a60b4aec708b2298d096d3446b0c8f49a6e5d7e8"}, "cell_type": "markdown", "source": ["Hmm, a few seemed to have slipped through the cracks here. There\u2019s plenty of FamilyIDs with only one or two members, even though we wanted only family sizes of 3 or more. Perhaps some families had different last names, but whatever the case, all these one or two people groups is what we sought to avoid with the three person cut-off. Let\u2019s begin to clean this up:"]}, {"source": ["# Create a data frame having count of each category\n", "fmlyIDs <- data.frame(table(full$familyID))\n", "\n", "# Get the familyID which have count less than 3\n", "smallFamId <- fmlyIDs[fmlyIDs$Freq <=2,]\n", "\n", "# Replace the familyID which have count less than  3 with 'Small'\n", "full$familyID[full$familyID %in% smallFamId$Var1] <- 'Small'\n", "\n", "# Again check the count of each category of familyID\n", "table(full$familyID)"], "metadata": {"_cell_guid": "2615ec67-7123-4d7c-875d-13d640b1bc12", "_uuid": "7f785419ffa5cd27cecd0cf4aa8ce3fd8b9e403e"}, "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "91dd8feb-d65d-48de-afd6-606112925f3d", "_uuid": "ec98aeb436b5a902e1b244fbeca319ca415dee23"}, "cell_type": "markdown", "source": ["# 3.3 Visualization\n", "What does our family size variable look like? To help us understand how it may relate to survival, let\u2019s plot it among the training data."]}, {"source": ["# Use ggplot2 to visualize the relationship between family size & survival\n", "ggplot(full[1:891,], aes(x = familySize, fill = factor(Survived))) +\n", "  geom_bar(stat='count', position='dodge') +\n", "  scale_x_continuous(breaks=c(1:11)) +\n", "  labs(x = 'Family Size') +\n", "  theme_few()"], "metadata": {"_cell_guid": "60f8d707-365a-4793-b64e-d6add0606cf5", "_uuid": "82fc05fe3b6fea505407eafe995906cdd3bc6f53"}, "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "6777365c-9a00-482f-8f01-1634b4c56412", "_uuid": "68e5fa1885566b0f3a983334d35cee1ac4d42b78"}, "cell_type": "markdown", "source": ["Ah hah. We can see that there\u2019s a survival penalty to singletons and those with family sizes above 4. We can collapse this variable into three levels which will be helpful since there are comparatively fewer large families. Let\u2019s create a discretized family size variable."]}, {"source": ["# Discretize family size\n", "full$familySize[full$familySize > 4] <- 'large'\n", "full$familySize[full$familySize == 1] <- 'singleton'\n", "full$familySize[full$familySize < 5 & full$familySize > 1] <- 'small'\n", "\n", "# Show family size by survival using a mosaic plot\n", "mosaicplot(table(full$familySize, full$Survived), main='Family Size by Survival', shade=TRUE)"], "metadata": {"_cell_guid": "c76bc3cd-4b45-4661-8d7a-0d3258d67577", "_uuid": "42f2caeff96b80d4536ce1d52f78dbe8c1c7e34e"}, "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "56d6bebd-a1e4-445e-8bcb-5f8929dee267", "_uuid": "21b72b1e37ae7ea02aa1d35337d8ccacdade4e8b"}, "cell_type": "markdown", "source": ["The mosaic plot shows that we preserve our rule that there\u2019s a survival penalty among singletons and large families, but a benefit for passengers in small families.\n", "\n", "Now let's visualize the relationship between Age, Sex and Survival, to check which age and sex group is having highest death penalty\n"]}, {"source": ["# First we'll look at the relationship between age, sex & survival\n", "ggplot(full[1:891,], aes(Age, fill = factor(Survived))) + \n", "geom_histogram() + \n", "facet_grid(.~Sex) + \n", "theme_few()"], "metadata": {"_cell_guid": "5a2aff73-9ee4-401d-9310-05d2aa361887", "_uuid": "4ef296977e02dd67eb36644072a1b1ca40a10d86"}, "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "04e6595c-b996-46e9-b5a1-d7ea7ae44e33", "_uuid": "140c104b019c16bea974ccc39fd9221708e3b2cd"}, "cell_type": "markdown", "source": ["This visualization gives a clear idea that there is Survival penalty to men."]}, {"metadata": {"_cell_guid": "ecd6bdad-fa9b-413c-80c5-3459d53834c3", "_uuid": "05bafa86ee623324d408765ec544db14f4c7c0e7"}, "cell_type": "markdown", "source": ["# 3.4 Treat Few more variables"]}, {"metadata": {"_cell_guid": "3e007d78-c0ce-4dfa-86f2-9716004e2794", "_uuid": "8cb65d50488eff265b3f26e0d401686577476eb3"}, "cell_type": "markdown", "source": ["Since the Random Forests in R can only digest factors with up to 32 levels. Our FamilyID variable had almost double that. We could take two paths forward here, either change these levels to their underlying integers (using the unclass() function) and having the tree treat them as continuous variables, or manually reduce the number of levels to keep it under the threshold.\n", "\n", "Let\u2019s take the second approach. To do this we\u2019ll copy the familyID column to a new variable, familyID2, and then convert it from a factor back into a character string with as.character(). We can then increase our cut-off to be a \u201cSmall\u201d family from 2 to 3 people. Then we just convert it back to a factor and we\u2019re done:"]}, {"source": ["# Copy the familyID to new variable familyID2\n", "full$familyID2 <- full$familyID\n", "\n", "# Covert to string\n", "full$familyID2 <- as.character(full$familyID2)\n", "\n", "# Create data frame having count of each familyID\n", "fmlyIDs <- data.frame(table(full$familyID2))\n", "\n", "# find the family ID having count equal to 3\n", "smallFamId <- fmlyIDs[fmlyIDs$Freq ==3,]\n", "\n", "# Replace the familyID having count 3 with 'Small'\n", "full$familyID2[full$familyID %in% smallFamId$Var1] <- 'Small'\n", "\n", "# Check the familyID2 variable\n", "table(full$familyID2)"], "metadata": {"_cell_guid": "1a65746d-0524-43e9-8987-5a3124327807", "_uuid": "328c9ac1318a28a86b8b49d9d983a3617c36455d"}, "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "a992a47a-fae8-4030-9da2-0ec9b21d7d30", "_uuid": "35a72c774337767bebad7c550a50fbcb7d159a20"}, "cell_type": "markdown", "source": ["Now that we know everyone\u2019s age, we can create a couple of new age-dependent variables: Child and Mother. A child will simply be someone under 18 years of age and a mother is a passenger who is 1) female, 2) is over 18, 3) has more than 0 children (no kidding!), and 4) does not have the title \u2018Miss\u2019.\n", "Let's find out how many children were there on Titanic and what is there survival ratio."]}, {"source": ["# First create the child variable and indicate whether child or adult\n", "full$Child <- 'Adult'\n", "full$Child[full$Age < 18] <- 'Child'\n", "\n", "# Show the count \n", "table(full$Child, full$Survived)\n", "\n", "# Use ggplot2 to visualize the relationship between being a child & survival\n", "ggplot(as.data.frame(full[1:891,]), aes(x = Child, fill = factor(Survived))) +\n", "geom_bar(position = \"dodge\") +\n", "labs(x = 'Child or Adult') +\n", "theme_few()"], "metadata": {"_cell_guid": "8c7fc034-e68d-4917-9427-39d23fadf3d9", "_uuid": "64a3e41cc49164bb680236d67a90c84f0f91edea"}, "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "afc7d5df-791a-4eca-94d6-8262abc8fe1c", "_uuid": "f050990241ef5c1d52a2b38f84838ba831c61145"}, "cell_type": "markdown", "source": ["Looks like being a child doesn\u2019t hurt, but it\u2019s not going to necessarily save you either! We will finish off our feature engineering by creating the Mother variable. Maybe we can hope that mothers are more likely to have survived on the Titanic."]}, {"source": ["# Adding Mother variable \n", "full$Mother <- 'Not Mother'\n", "full$Mother[full$Sex == 'female'  & full$Parch >0 & full$Age >= 18 & full$Title != 'Miss'] <- 'Mother'\n", "\n", "# Show the count of mothers survival\n", "table(full$Mother, full$Survived)\n", "\n", "# Use ggplot2 to visualize the relationship between being a child & survival\n", "ggplot(as.data.frame(full[1:891,]), aes(x = Mother, fill = factor(Survived))) +\n", "  geom_bar(position = \"dodge\") +\n", "  labs(x = '') +\n", "  theme_few()"], "metadata": {"_cell_guid": "6dfeae0c-ee7e-45c8-a2e8-75c374c9a9f2", "_uuid": "72459f40cd5fbd37a10cd2a142b8cb2e8fb3fc4f"}, "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "dc65c3ca-ac57-4ee0-9cfd-07c08cd197b3", "_uuid": "a23dd656698c3ee7ff2a58f874bceef850e6f2de"}, "cell_type": "markdown", "source": ["Well this proves that our assumption was correct and Mothers are more likely to survive Titanic."]}, {"metadata": {"_cell_guid": "09e21847-5237-4ea5-932f-b77630c04a9f", "_uuid": "7a9c54561ceb6820451e161e1aaa6b6cd241f98a"}, "cell_type": "markdown", "source": ["# 3.5 Factoring and Scaling Variable\n", "Our final steps of feature engineering are to scale the continuos variable and convert categorical variables to factor that we have created."]}, {"source": ["# Scaling of continuos variable\n", "full[,c(6,10)] <- scale(full[,c(6,10)])"], "metadata": {"_cell_guid": "c0e32a1d-017f-40c9-ae7a-e0e07e069c83", "_uuid": "be0624c6e59bee3b317ef9c6550dc2db52e1ab7d"}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": ["# Finish by factorizing our categorical variables\n", "full$Title <- factor(full$Title)\n", "full$Surname <- factor(full$Surname)\n", "full$familySize <- factor(full$familySize)\n", "full$familyID <- factor(full$familyID)\n", "full$familyID2 <- factor(full$familyID2)\n", "full$Child  <- factor(full$Child)\n", "full$Mother <- factor(full$Mother)"], "metadata": {"_cell_guid": "720e81ea-261b-4b12-9667-7e457b59e7f2", "_uuid": "c4f9b514a32da8104e4162525a7a0454eb32b2d2"}, "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "6f5d5176-7503-441d-8439-9d1acd37880c", "_uuid": "03ddb0fe39137e1066c5f45e3ac787bf33d45ea8"}, "cell_type": "markdown", "source": ["# 4 Prediction and Submission"]}, {"metadata": {"_cell_guid": "2a135689-32af-4e72-8b93-211a22dead00", "_uuid": "5de76f5ec3528c2423145a37c06843ac9d0dfa19"}, "cell_type": "markdown", "source": ["# 4.1 Split data into Train and Test dateset\n", "We are now ready to split the test and training sets back into their original states, carrying our fancy new engineered variables with them.\n", "\n", "So let\u2019s break them apart and do some predictions on our new fancy engineered variables:"]}, {"source": ["train <- full[1:891,]\n", "test <- full[892:1309,]"], "metadata": {"_cell_guid": "e7fb7eab-869b-4302-b2f6-e89bbff54165", "_uuid": "03877976e724327b1400679b69517d73ad836d7b"}, "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "a723a6a4-2bd5-4c6e-bfb5-07e730267dd5", "_uuid": "e594b430caa952b63406e716dacfcf7c023d4310"}, "cell_type": "markdown", "source": ["# 4.2 Build First Predictive model (i.e. Logistic Regression)\n", "Time to do our predictions! The first binary classifier which will be using to predict survival is `Logistic Regression`. So let's begin the model bulding."]}, {"source": ["# build the model using all the important feature\n", "glm_model <- glm(data = train, formula = Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked \n", "                 + Title + familySize + familyID2 + Child + Mother,\n", "                 family = binomial(link = \"logit\"))\n", "\n", "# Print annova (analysis of variance) table, this will give you variance and df of each vriable\n", "anova(glm_model)\n", "\n", "# Check the summary of the model, it will have residual and null diviance by which we can calculate pseudo R^2 \n", "#summary(glm_model)\n", "\n", "# Calculate pseudo R^2\n", "psr <- 1 - (664.96/1186.7) # psr = 1 - (residual deviance / null deviance)\n", "\n", "# Print  the value of pseudo R-square \n", "print(psr)\n", "#This Logistic Regression model has Psuedo R-square of 0.4396562\n"], "metadata": {"_cell_guid": "e06a4ef8-00e3-4cca-80a4-88f82d28cd79", "_uuid": "7c281af66e8be90b7221c3fd1b86bb9569935075"}, "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "1ccb5cea-4dcd-49e3-bfbe-419fedd5b628", "_uuid": "f1b9c296bc2726357fa94bfef342b160d50ec240"}, "cell_type": "markdown", "source": ["Wow! we have successfully built our first binary classifier. Now its time to evaluate our model with both train and test dataset."]}, {"source": ["# Create confusion matrix with train data set to check its accuracy in trainig environment\n", "Prediction <- predict(glm_model, train[,-2], type = \"response\")\n", "confusion_matrix_train <- table(train$Survived, (Prediction > 0.5))  \n", "#  > 0.5 signifies that whatever comes greater than .5 will be considered 1 in confusion matrix\n", "\n", "print(confusion_matrix_train)\n", "\n", "#                    (Predicted Values)     \n", "#                       FALSE TRUE\n", "#  (Actual Values)   0   488   61\n", "#                    1    82  260\n", "\n", "# Calculate Precision and Recall based on the confusion matrix \n", "recall <- 260/(260+82)     # recall = True Positive(TP) / Actual Yes\n", "# 0.7602339\n", "\n", "precision <- 260/(260+61)   # precision = TP / Predicited Yes\n", "# 0.8099688\n", "\n", "# calculate the f1 score (measure of accuracy)\n", "f1 <- (2*recall*precision)/(recall+precision)\n", "print(paste0(\"Accuracy : \",f1))\n", "#  accuracy 0.784314"], "metadata": {"_cell_guid": "98ca1694-6579-48cb-a843-61758a9ed9bf", "_uuid": "e7b0af4f1066073cbef534e21945580118074b10"}, "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "b4df96b8-f69f-4506-b2a9-ff94f3d30cbf", "_uuid": "be530fc5b1a1c50add163fef73a3b84e991f89d1"}, "cell_type": "markdown", "source": ["It seems that our model is having pretty good accuracy of 0.784314 with train dataset, now let's check whether it has similar accuracy with our test dataset as well or is a case of overfitting.  "]}, {"source": ["# Do same prediction on test data set\n", "Prediction <- predict(glm_model, test[,-2], type = \"response\")\n", "Prediction <- as.integer(Prediction > 0.5)\n", "\n", "# Create data frame having 2 columns PassengerId and Survived as in sample submission of this competition\n", "submit <- data.frame(PassengerId = test$PassengerId, Survived = Prediction)\n", "\n", "# Write the data frame to a .csv file(in Rstudio)  and submit the file to kaggle \n", "# write.csv(submit, file = \"glm_model.csv\", row.names = FALSE)\n", "\n", "# Kaggle score 0.77990"], "metadata": {"_cell_guid": "029e593f-8db8-4020-aae5-837d439d9af8", "_uuid": "11cbe5aa972378c80106d8a9f3ddf2398cffbd47"}, "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "dbb6c8f0-b2a8-4035-842c-a089141acb05", "_uuid": "d8af1b7121cf16072f90ab3c1d6df032bbdb5d1b"}, "cell_type": "markdown", "source": ["On submission I have got accuracy as `0.77990` which is similar to our accuracy with train dataset, so the Logistic Regression model is performing quite well with test dataset also.\n"]}, {"metadata": {"_cell_guid": "9fc00add-503d-4d24-852d-8b311cfb622b", "_uuid": "45b45bccb5629637f8af257eb78aab0bd56c2783"}, "cell_type": "markdown", "source": ["# 4.3 Naive Bayes\n", "Well everything till now is perfect, next we have to build the remiaining binary classifier and repeat the same steps for evaluating our model.\n", "\n", "So now its time to build our second model i.e. `Naive Bayes` model"]}, {"source": ["# Create Naive Bayes model using naiveBayes function\n", "nb_model <- naiveBayes(as.factor(Survived) ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked \n", "                       + Title + familySize + familyID2 + Child + Mother,\n", "                       data = train)\n", "# Check summary of model\n", "summary(nb_model)"], "metadata": {"_cell_guid": "cea2f8af-d0ac-4761-bc3a-dc957555303d", "_uuid": "b232a45eacc721f07512d800c88041021ab77318"}, "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "eb0bbe33-92dd-459f-8285-a5d4b736835a", "_uuid": "7fc1de40f14b2272befa5ca9fa73a232d5c6e956"}, "cell_type": "markdown", "source": ["Great our model building is done now its evaluation time, repeat the same steps as done for Logistic Regression model"]}, {"source": ["#Creating confusion matrix for the model\n", "train_pred <- predict(nb_model, train)\n", "confusion_matrix_train <- table(train$Survived, train_pred)\n", "\n", "print(confusion_matrix_train)\n", "#                    (Predicted Values)     \n", "#                       FALSE TRUE\n", "#  (Actual Values)   0   470   79\n", "#                    1    86  256\n", "\n", "\n", "#Calculating Precision and recall\n", "recall <- 256/(256+86)     # recall = TP / actual yes\n", "# 0.748538\n", "\n", "precision <- 256/(256+79)   # precision = TP / predicted yes\n", "# 0.7641791\n", "\n", "f1 <- (2*recall*precision)/(recall+precision)\n", "print(paste0(\"Accuracy : \",f1))\n", "# 0.7562777"], "metadata": {"_cell_guid": "f9840b75-c858-4410-815d-27e92980296d", "_uuid": "7be6a4189bd2a496b5e96f857e3b87797cb1e2c0"}, "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "a97f57b7-a97f-40ff-a3ce-daf97d195eaf", "_uuid": "8d3ba56489c161d4b0660f17f7632c3e258ad7b7"}, "cell_type": "markdown", "source": ["It seems that our Logistic Regression model performs better with train dataset, let's check whether this apllies to test dataset or not."]}, {"source": ["#Do same prediction on test data set\n", "test_pred <- predict(nb_model, test)\n", "submit <- data.frame(PassengerId = test$PassengerId, Survived = test_pred)\n", "\n", "# Kaggle score 0.76076"], "metadata": {"_cell_guid": "da3b3275-1cde-4728-937f-32e591986e66", "_uuid": "288677aac6cc2fc846db47deaf21d1822a54e651"}, "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "74bdbbb2-46cb-4e73-8bc9-eac52415778c", "_uuid": "d11506dfdd57fe346c132a14ba333328ea5975d3"}, "cell_type": "markdown", "source": ["On submitting our result of Naive Bayes model I got accuracy of 0.76076, so till now Logistic Regression is leading and there is no case of overfitting in both the models.\n"]}, {"metadata": {"_cell_guid": "30aceb24-33d6-4c03-9a6b-5d8f3263f4ae", "_uuid": "c724207a62c6312381ebdde99e9d854855deed51"}, "cell_type": "markdown", "source": ["# 4.4 Support Vector Machine\n", "Now let's move to our third binary classifier i.e. `Support Vector Machine`"]}, {"source": ["# Build model using function svm()\n", "sv_model <- svm(as.factor(Survived) ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + Title \n", "                + familySize + familyID2 + Child + Mother,\n", "                train[,-c(11)])\n", "# Note always take care of the variable which have missing values for svm because if any row is having \n", "# missing feature then it will not give output for that particular observation\n", "# That's why I have excluded the Cabin variable which is having missing values\n", "\n", "# Check the information like gamma value, number of support vectors of the model built \n", "print(sv_model)"], "metadata": {"_cell_guid": "2ee41ccf-5c98-4906-8030-deaec9bd85c9", "_uuid": "a696341303792438b92f0badd84739fa53c74e0c"}, "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "18d017d9-670d-4b75-8763-2a14d57454fd", "_uuid": "ad37af863774b593f0ad04086ce3d3dc950010d4"}, "cell_type": "markdown", "source": ["Let's begin evaluating our Support Vector Machine model with train dataset."]}, {"source": ["#Creating confusion matrix for the model\n", "train_pred <- predict(sv_model, train[,-c(11)])\n", "confusion_matrix_train <- table(train$Survived, train_pred)\n", "\n", "print(confusion_matrix_train)\n", "#                    (Predicted Values)     \n", "#                       FALSE TRUE\n", "#  (Actual Values)   0   491   58\n", "#                    1    90  252\n", "\n", "#Calculating Precision and recall\n", "recall <- 252/(252+90)     # recall = TP / actual yes\n", "# 0.7368421\n", "\n", "precision <- 252/(252+58)   # precision = TP / predicted yes\n", "# 0.8129032\n", "\n", "f1 <- (2*recall*precision)/(recall+precision)\n", "print(paste0(\"Accuracy : \",f1))\n", "# 0.7730061"], "metadata": {"_cell_guid": "cbeb3cef-0374-4a75-ab8c-d32b1fdac731", "_uuid": "d2828ed5334d165a36b56247a027367a34eab9b3"}, "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "43adc2ba-e887-462b-adf4-95e91892ce24", "_uuid": "bb00fe4a67c9ddbcaa58a5c142d753b1e10d9d5f"}, "cell_type": "markdown", "source": ["Now it's time to submit result of our third binary classifier and check result."]}, {"source": ["#Do same prediction on test data set\n", "test_pred <- predict(sv_model, test[,-c(2,11)])\n", "submit <- data.frame(PassengerId = test$PassengerId, Survived = test_pred)\n", "\n", "# Kaggle score 0.78947"], "metadata": {"_cell_guid": "692055ba-b119-4980-b9ff-b6217393ab60", "_uuid": "218c7e84ffe99e8cb791bec28435daf0defdc39f"}, "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "1a57cfee-7dae-46c9-99e7-542a481fee37", "_uuid": "fba7413a4d38e899d419d11ddcf1214b736e5460"}, "cell_type": "markdown", "source": ["Woohoo! This time we have got 0.78947 accuracy score which highest among all."]}, {"metadata": {"_cell_guid": "3c5c93c9-acb5-46b4-8586-d6bdafa56473", "_uuid": "e1cece2f48cd231dc4fd6550ad9acd571ccc54ce"}, "cell_type": "markdown", "source": ["# 4.5 K Nearest Neighbour (K-NN)\n", "Our next binary classifier in this queue is `K-NN (K- Nearest Neighbour)`, let's gets our hands dirty with this model to check whether this model can do better than previous model or not."]}, {"source": ["# Copy the survived label to a new variable\n", "train_survived_labels <- train$Survived\n", "\n", "# Convert labels of all categorical variable from characters to numeric codes\n", "# make a new dataset and copy only the required features and then proceed with conversion\n", "knn_train <- train[,-c(1,2,4,9,11,14,16)]\n", "knn_train$Sex <- factor(knn_train$Sex, labels = c('0','1'))\n", "knn_train$Embarked <- factor(knn_train$Embarked, labels = c('1','2','3'))\n", "knn_train$Title <- factor(knn_train$Title, labels = c(1:5))\n", "knn_train$familySize <- factor(knn_train$familySize, labels = c(1:3))\n", "knn_train$familyID2 <- factor(knn_train$familyID2, labels = c(1:24))\n", "knn_train$Child <- factor(knn_train$Child, labels = c('0','1'))\n", "knn_train$Mother <- factor(knn_train$Mother, labels = c('0','1'))\n", "\n", "# Repeat the above steps for test dataset as well \n", "knn_test <- test[,-c(1,2,4,9,11,14,16)]\n", "knn_test$Sex <- factor(knn_test$Sex, labels = c('0','1'))\n", "knn_test$Embarked <- factor(knn_test$Embarked, labels = c('1','2','3'))\n", "knn_test$Title <- factor(knn_test$Title, labels = c(1:5))\n", "knn_test$familySize <- factor(knn_test$familySize, labels = c(1:3))\n", "knn_test$familyID2 <- factor(knn_test$familyID2, labels = c(1:20))\n", "knn_test$Child <- factor(knn_test$Child, labels = c('0','1'))\n", "knn_test$Mother <- factor(knn_test$Mother, labels = c('0','1'))"], "metadata": {"_cell_guid": "68d2e051-207a-4790-8d51-fc1294bf157c", "_uuid": "8b90ffa94a9480e87b83108d0a1529da25e026aa"}, "execution_count": null, "cell_type": "code", "outputs": []}, {"source": ["# Create the model using knn() function\n", "knn_model <- knn(train = knn_train, test = knn_test, cl = train_survived_labels, k = 37)\n", "\n", "# Write the result to .csv file \n", "submit <- data.frame(PassengerId = test$PassengerId, Survived = knn_model)\n", "\n", "# Kaggle score 0.71291"], "metadata": {"_cell_guid": "000d907d-c856-40d9-8e18-f245fd8e8ed6", "_uuid": "8cdb92cfdc6dd6cbbe8d3ea30d36c361e9e47340"}, "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "f787aeb6-b4fc-4960-8289-428c23e2d6dc", "_uuid": "23ad4ad6987f0c814ceb2799ee2a076d64fe3e63"}, "cell_type": "markdown", "source": ["Unfortunately k-NN model is the worst performer of this show, so we have to move to our next binary classifier for finding best model."]}, {"metadata": {"_cell_guid": "3ffb9abd-6fa3-4662-b2e7-f84ef36d2591", "_uuid": "6f85426f4f7c5c7c1bce54156efe356dd2a7e7d0"}, "cell_type": "markdown", "source": ["# 4.6 Decision Trees\n", "Our next binary classifier is `Decision Trees` which we have already used for imputing Age variable, so let's implement Decision Tree model for predicting survival on Titanic."]}, {"source": ["# set seed\n", "set.seed(415)\n", "\n", "# Create model using rpart() function\n", "dt_model <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + Title + familySize \n", "             + familyID + Child + Mother,\n", "      data = train,\n", "     method = \"class\")"], "metadata": {"_cell_guid": "dbaa848b-d994-4f9f-9475-8e90258e1e2f", "_uuid": "a995a440f4364dd9894091a2ac28f12e11c4f753"}, "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "326f20f9-ab45-47d8-930f-2246c5d1e8c0", "_uuid": "8c3a499480a1be0ea895dedb52c13b3eb0260163"}, "cell_type": "markdown", "source": ["Now it's time to evaluate our Decision Tree binary classifier."]}, {"source": ["#Creating confusion matrix for the model\n", "train_pred <- predict(dt_model, train, type = \"class\")\n", "confusion_matrix_train <- table(train$Survived, train_pred)\n", "\n", "print(confusion_matrix_train)\n", "#                    (Predicted Values)     \n", "#                       FALSE TRUE\n", "#  (Actual Values)   0   502   47\n", "#                    1    91  251\n", "\n", "#Calculating Precision and recall\n", "recall <- 251/(251+91)     # recall = TP / actual yes\n", "# 0.7426901\n", "\n", "precision <- 251/(251+47)   # precision = TP / predicted yes\n", "# 0.8141026\n", "\n", "f1 <- (2*recall*precision)/(recall+precision)\n", "print(paste0(\"Accuracy : \",f1))\n", "# 0.784375"], "metadata": {"_cell_guid": "0f3fc355-720f-462b-bcd0-f50c926faf73", "_uuid": "c521455493ce477016d923dd4bd2afc853797d2d"}, "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "8c66af57-e7ec-40b6-a7ca-508a10393ceb", "_uuid": "3b4a51b8f69fac64c123999db4b745759f653b98"}, "cell_type": "markdown", "source": ["Great! Our Decision tree model turned into best model till now in terms of accuracy with training dataset, let's check whether it can give similar result with test dataset or not."]}, {"source": ["Prediction <- predict(dt_model, test, type = \"class\")\n", "submit <- data.frame(PassengerId = test$PassengerId, Survived = Prediction)\n", "\n", "# Kaggle Score 0.79904"], "metadata": {"_cell_guid": "b93a4539-852a-49e0-b555-66435c0f99cc", "_uuid": "e615751d087bebdb1b7745a08eecd6023f02f78a"}, "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "07ec6322-72a4-43de-a6fa-8f1232d774a5", "_uuid": "59203bf60db7286866a9c88aaf89744509534d50"}, "cell_type": "markdown", "source": ["Wow! it seems that our Decision tree model has defeated Support Vector machine model in terms of accuracy with both train and test dataset. "]}, {"metadata": {"_cell_guid": "7ccb74e8-f7d2-4f3f-9c27-21e91c04d8dd", "_uuid": "7850939259f565784e16b6c8d6b25d183fc6f965"}, "cell_type": "markdown", "source": ["# 4.7 Random Forest\n", "Now let's move to our next binary classifier which is `Random Forest` and it is considered as extention of decision  trees as it has more than one tree. Time to begin our next hunt for best model.."]}, {"source": ["# set seed\n", "set.seed(415)\n", "\n", "# Build the model (note: not all possible variables are used)\n", "rf_model <- randomForest(as.factor(Survived) ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked \n", "                         + Title + familySize + familyID2 + Child + Mother,\n", "                    data=train,\n", "                    importance=TRUE,\n", "                    ntree=2000)\n", "\n", "# So let\u2019s look at what variables were important:\n", "varImpPlot(rf_model)"], "metadata": {"_cell_guid": "3167f884-3862-4d1a-a186-5d4403a0fa46", "_uuid": "1ab14b1874113ad6397fe0897362f7866d0e3fc1"}, "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "31f77244-9eba-4686-8e72-d6cf6a59567b", "_uuid": "6b7cf6e174cd0d27ca4f628043d41265d5118634"}, "cell_type": "markdown", "source": ["There\u2019s two types of importance measures shown above i.e.  MeanDecreaseAccuracy and MeanDecreaseGini. \n", "\n", "The accuracy one tests to see how worse the model performs without each variable, so a high decrease in accuracy would be expected for very predictive variables. \n", "\n", "The Gini one digs into the mathematics behind decision trees, but essentially measures how pure the nodes are at the end of the tree. Again it tests to see the result if each variable is taken out and a high score means the variable was important.\n", "\n", "Unsurprisingly, our Title variable was at the top for Gini measures. We should be pretty happy to see that the remaining engineered variables are doing quite nicely too.\n", "\n", "Now Let's perform the evaluation on train dataset using Random Forest model."]}, {"source": ["#Creating confusion matrix for the model\n", "train_pred <- predict(rf_model, train)\n", "confusion_matrix_train <- table(train$Survived, train_pred)\n", "\n", "print(confusion_matrix_train)\n", "#                    (Predicted Values)     \n", "#                       FALSE TRUE\n", "#  (Actual Values)   0   532   17\n", "#                    1    62  280\n", "\n", "#Calculating Precision and recall\n", "recall <- 280/(280+62)     # recall = TP / actual yes\n", "# 0.8187134\n", "\n", "precision <- 280/(280+17)   # precision = TP / predicted yes\n", "# 0.9427609\n", "\n", "f1 <- (2*recall*precision)/(recall+precision)\n", "print(paste0(\"Accuracy : \",f1))\n", "# 0.876369327073553"], "metadata": {"_cell_guid": "dd2df41a-7664-4d61-8b76-0df62717f1fe", "_uuid": "8ada488cd35ba59537a3c63f599b85d7aabef580"}, "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "59cd3e65-3621-4426-a12c-980815b3949a", "_uuid": "31bd82e1f715b2f98395c996073dadf8fdb545dd"}, "cell_type": "markdown", "source": ["Amazing! We have got a very nice accuracy score of 0.8763693 with train dataset using Random Forest, now let's see it's performance with train dataset."]}, {"source": ["Prediction <- predict(rf_model, test, type = \"class\")\n", "submit <- data.frame(PassengerId = test$PassengerId, Survived = Prediction)\n", "\n", "# Kaggle Score 0.77511"], "metadata": {"_cell_guid": "227827c6-f8f9-40ba-b718-c3cb5d24d15e", "_kg_hide-output": false, "_uuid": "e5c92fe28cdff47082dfc9959d0e2bd1c1dc4de4", "_kg_hide-input": false}, "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "3a1727e7-320f-4bea-bf2c-a7d6778e73c5", "_uuid": "6553aabff26b4b09ef5ea3197a9180f55d1d2a78"}, "cell_type": "markdown", "source": ["Unfortunately on submission we have got very less accuracy score i.e. 0.77033 which makes it clear that it is a case of overfitting.\n", "\n", "But let\u2019s not give up yet. There\u2019s more than one ensemble model. Let\u2019s try a forest of conditional inference trees. They make their decisions in slightly different ways, using a statistical test rather than a purity measure, but the basic construction of each tree is fairly similar."]}, {"metadata": {"_cell_guid": "7d70586c-1dcb-437f-8e14-30039de1a25c", "_uuid": "7233e9671ff4e20c9ee5a68ec255940010c364bc"}, "cell_type": "markdown", "source": ["# 4.8 Conditional Random Forest\n", "Conditional inference trees are able to handle factors with more levels than Random Forests can, so let\u2019s go back to out original version of FamilyID.\n", "\n", "So go ahead and build the `Conditional Random Forest` model."]}, {"source": ["# We again set the seed for consistent results and build a model in a similar way to our Random Forest\n", "set.seed(415)\n", "\n", "cf_model <- cforest(as.factor(Survived) ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + Title \n", "                    + familySize + familyID + Child + Mother,\n", "                    data = train,\n", "                    controls=cforest_unbiased(ntree=2000, mtry=3))"], "metadata": {"_cell_guid": "2584911e-07ce-440f-8ef3-83c78b07d148", "_uuid": "979b1b2a5a812e9e2ac3f69f1ed968012c7e90b0"}, "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "2849ed28-0648-4e45-8fce-30565425aa43", "_uuid": "ce4a415aaa78cc5ab8831216116fa6a8d6ee8a48"}, "cell_type": "markdown", "source": ["Now let's evaluate our last model of our binary classifier list."]}, {"source": ["#Creating confusion matrix for the model\n", "train_pred <- predict(cf_model, train, OOB=TRUE, type = \"response\")\n", "confusion_matrix_train <- table(train$Survived, train_pred)\n", "\n", "print(confusion_matrix_train)\n", "#                    (Predicted Values)     \n", "#                       FALSE TRUE\n", "#  (Actual Values)   0   508   41\n", "#                    1    93  249\n", "\n", "#Calculating Precision and recall\n", "recall <- 249/(249+93)     # recall = TP / actual yes\n", "# 0.7192982\n", "\n", "precision <- 249/(249+41)   # precision = TP / predicted yes\n", "# 0.8692579\n", "\n", "f1 <- (2*recall*precision)/(recall+precision)\n", "print(paste0(\"Accuracy : \",f1))\n", "# 0.7879746"], "metadata": {"_cell_guid": "df4b34a5-0c52-44bc-8a27-47a6f7d13f68", "_uuid": "6496040b50578806f8299a8bfc122b56e14803bc"}, "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "b6af8ac0-b1b2-4be8-8a33-37c3fff05379", "_uuid": "9a2cd9df60bae80b934ecb00d45a600948a7579c"}, "cell_type": "markdown", "source": ["Well this model performed similar to previous model with train dataset, let's see whether it can do better with test dataset or not.\n", "Time to do our final submission and check score."]}, {"source": ["Prediction <- predict(cf_model, test, OOB=TRUE, type = \"response\")\n", "submit <- data.frame(PassengerId = test$PassengerId, Survived = Prediction)\n", "\n", "#Kaggle score 0.80861"], "metadata": {"_cell_guid": "16f27b43-51b1-413d-bfee-e08ba5946618", "_uuid": "35b8466b021fb6e829c8eab2a17674bfbb037fde"}, "execution_count": null, "cell_type": "code", "outputs": []}, {"metadata": {"_cell_guid": "9b881efe-8419-447e-bf69-aa265557eee7", "_uuid": "948a7b3964ffba834452ff22b1eb1cbb52acdf4d"}, "cell_type": "markdown", "source": ["Whoa, glad we have got highest score till now i.e. 0.80861, so lastly Conditional Random Forest wins the race of prediciting survival."]}, {"metadata": {"_cell_guid": "c07ded0e-f743-42da-bcbd-af97781f954e", "_uuid": "1d73052d20373b64fc5642041868e20814d841a4"}, "cell_type": "markdown", "source": ["# 5 Conclusion\n", "Thank you for your time and attention. Please upvote and give suggestion as this will help me to learn more.\n", "\n", "**Hope you find it useful. Please upvote**\n", "\n", "**Keep Kaggling :)**"]}]}