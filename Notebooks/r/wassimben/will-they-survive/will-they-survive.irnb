{"metadata": {"language_info": {"codemirror_mode": "r", "file_extension": ".r", "version": "3.4.1", "name": "R", "pygments_lexer": "r", "mimetype": "text/x-r-source"}, "kernelspec": {"language": "R", "name": "ir", "display_name": "R"}}, "nbformat": 4, "cells": [{"metadata": {"_uuid": "6178f5b4c24d965a47f33d0620ee7693df5e832e", "collapsed": true, "_cell_guid": "d698f23d-483a-492e-aca0-01e9f3478c1d"}, "outputs": [], "cell_type": "code", "execution_count": null, "source": ["---\n", "title: \"Will they survive ?\"\n", "date: \"15 September 2017\"\n", "author: 'Wassim Ben Ismail'\n", "output:\n", "  html_document:\n", "    number_sections: true\n", "    toc: true\n", "    fig_width: 7\n", "    fig_height: 4.5\n", "    theme: readable\n", "    highlight: tango\n", "---\n", "# Introduction\n", "\n", "Welcome to my first Kaggle entry where I develop how I scored 81 percent in Titanic competition. This report is\n", "going to be kicked off by setting some expectations of our prediction accuracy. \n", "Then, I will walk you through all the features not only to clean them but also to understand how each one\n", "of them impact the survival or death of passengers. \n", "Finally, we\u2019re going to train our machine learning algorithm before generate a submission.\n", "\n", "**Acknowledgments**\n", "\n", "The current work has been greatly influenced by the below Kernels of our fellow Kagglers which I really recommend having a look at :\n", "\n", "[Titanic: Getting Started With R](http://trevorstephens.com/kaggle-titanic-tutorial/getting-started-with-r/)\n", "by Trevor Stephens.\n", "\n", "[Exploring Survival on the Titanic](https://www.kaggle.com/mrisdal/exploring-survival-on-the-titanic)\n", "by Megan Risdal.\n", "\n", "[Tidy TitaRnic](https://www.kaggle.com/headsortails/tidy-titarnic)\n", "by Heads or Tails.\n", "\n", "[Divide and Conquer [0.82297]](https://www.kaggle.com/pliptor/divide-and-conquer-0-82297) by Oscar TakeshitaDivide\n", "\n", "\n", "And many others.\n", "\n", "\n", "#Preparing the study ground\n", "\n", "##Loading packages\n", "We are  going to start first by loading the packages we will need through this project\n", "\n", "```{r,message=F,warning=F,error=FALSE}\n", "#Loading packages\n", "library(\"tidyverse\")\n", "library(\"VIM\")\n", "library(\"mice\")\n", "library(\"randomForest\")\n", "library(\"gmodels\")\n", "library(\"lattice\")\n", "library(\"caret\")\n", "library(\"irr\")\n", "library(\"corrplot\")\n", "library(\"party\")\n", "```\n", "\n", "##Loading the data\n", "\n", "To load our data we re going to use `read.csv()` function. Since it is quite tricky in the middle of the work to spot a space \" \", we re going to specify to `read.csv()` function that \"\", \" \" and NAs should be treated as NAs.\n", "\n", "```{r,message=FALSE, warning=FALSE}\n", "train<-read.csv(\"../input/train.csv\",stringsAsFactors = F,na.strings=c(\"\",\"NA\",\" \"))\n", "test<-read.csv(\"../input/test.csv\",stringsAsFactors = F,na.strings=c(\"\",\"NA\",\" \"))\n", "Full<-bind_rows(train,test)\n", "```\n", "\n", "It is quite important to combine the *Test* and  *Train* data so that we manipulate both of them simultaneously and then separate them just before training the Machine Learning algorithm.\n", "\n", "##Data Exploring\n", "\n", "To get a feel of the data we can look at its structure through the function `str()`\n", "\n", "```{r}\n", "str(Full)\n", "```\n", "\n", "we can see that our dataset is composed of :\n", "\n", "Variable Name | Description\n", "--------------|-------------\n", "Survived      | The Target that we want to predict\n", "Pclass        | Passenger's class\n", "Name          | Passenger's name\n", "Sex           | Passenger's sex\n", "Age           | Passenger's age\n", "SibSp         | Number of siblings/spouses aboard\n", "Parch         | Number of parents/children aboard\n", "Ticket        | Ticket number\n", "Fare          | Fare\n", "Cabin         | Cabin\n", "Embarked      | Port of embarkation\n", "\n", "#Calibrating our expectations\n", "\n", "Before diving in the realm of data science and try to predict whether a passenger is going to survive or not let\u2019s have a quick look on the proportion of survival and death.\n", "\n", "```{r}\n", "round (100 * prop.table(table(train$Survived)),2)\n", "```\n", "\n", "We can see that almost 62 % of passengers didn't make it. A lazy (And pessimist) predictor can submit to Kaggle a file full of zeros and will score 62 % ! In order to justify the investment we\u2019re going to put in this project we need to beat the 62 % accuracy rate and not the 50 % percent of the a flip of a coin ! \n", "\n", "I highly recommend going through [Titanic: Getting Started With R](http://trevorstephens.com/kaggle-titanic-tutorial/getting-started-with-r/) by *Trevor Stephens* where he nicely explains how to score 78 % without even touching the realm of machine learning or even try to clean or process that data ! \n", "\n", "Having said that, let's visit the features one by one and try to start building some understanding of the dataset.\n", "\n", "#Feature exploration and processing\n", "\n", "For each feature we are interested not only in understand how it correlates with the survival rate but also to identify anomalies such as outliers or missing data. \n", "\n", "##Pclass\n", "\n", "Pclass is the passenger ticket class and it  can be either 1,2 or 3. \n", "\n", "```{r}\n", "mosaicplot(table(Full$Pclass,Full$Survived),color = 2:3,main=\"Suvival vs Ticket class\",xlab = \"Pclass\", ylab = \"Survived\")\n", "\n", "```\n", "\n", "\n", "From this mosaic plot, we can conclude few things: \n", "*Most of the passengers are 3rd class passengers followed by first class then second class. \n", "*The better the class the higher the chances of survival! This makes sense because when we pay a premium for a ticket we get extra comfort and almost a secured seat on one of the safety boats.\n", "\n", "##Name\n", "\n", "This is obviously the name of the passenger and it is unique per passenger. This entails we cannot use it directly to predict what so ever because each passenger is unique at least according to this feature. Nevertheless let's have a look on one of the names and see its components!\n", "\n", "\n", "```{r}\n", "Full$Name[1]\n", "```\n", "\n", "We can see that the name has three components:\n", "\n", "*Family name\n", "*Title\n", "*Surname\n", "\n", "So let's explode the name column into three columns\n", "\n", "```{r,message=FALSE, warning=FALSE}\n", "Full<-Full %>% separate(Name,into=c(\"name\",\"surname\"),sep=\", \")%>% separate(surname,into=c(\"title\",\"surname\"),sep=\"\\\\. \")\n", "```\n", "\n", "Let's start first with with the **title**\n", "\n", "```{r}\n", "table(Full$Survived,Full$title)\n", "```\n", "\n", "We can see that we have some common titles and some pretty rare ones. The rare ones won\u2019t be of any help for us due to their low occurrence frequency which might lead the Machine Learning algorithm to fall in the trap of over-fitting. Solution? Let's aggregate some of them.\n", "\n", "```{r}\n", "Full$title[Full$title %in% c(\"Capt\",\"Col\",\"Major\",\"Dr\",\"Rev\",\"Don\")]<-\"Officer\" \n", "Full$title[Full$title %in% c(\"Jonkheer\",\"Sir\",\"the Countess\",\"Dona\",\"Lady\")]<-\"Royalty\" \n", "Full$title[Full$title %in% c(\"Mme\",\"Mrs\")] <- \"Mrs\" \n", "Full$title[Full$title %in% c(\"Mlle\",\"Miss\",\"Ms\")] <- \"Miss\"\n", "```\n", "\n", "So how the title impact a passenger survival ?\n", "\n", "```{r}\n", "ggplot(Full[1:891,],aes(x=title,fill=factor(Survived)))+geom_bar(position=\"dodge\")+ggtitle(\"Survival Vs Title\")\n", "```\n", "\n", "We can draw some conclusions here : \n", "*Royalty and Masters mostly survive.\n", "*Officers don't survive as often probably because it is their duty to help others survive.\n", "*Females (Miss and Mrs) survive quite well.\n", "*Men seem to mostly die.\n", "\n", "##Sex\n", "\n", "From the previous graphs, we can easily tell that females have a better chance of survival than males. To confirm this, let's just focus on the gender variable.\n", "\n", "\n", "```{r}\n", "mosaicplot(table(Full$Sex,Full$Survived),color = 2:3,main=\"Survival Vs Gender\")\n", "\n", "```\n", "\n", "The mosaic plot speaks for itself. Females survive and males die ! From the same graph we can also see that 2/3 of the Titanic population are males.\n", "## SibSp and Parch\n", "\n", "Numbers of Siblings/Spouses and number of Parents/kids can be distilled down to family size. A good way to visualize how these two feature impact the survival rate is :\n", " \n", "```{r}\n", "ggplot(train,aes(SibSp,Parch,color=factor(Survived)))+geom_count()+ggtitle(\"Survival as a function of Parent/Kids number and Spouse/Sibling number\")\n", "```\n", "\n", "The blue color circles are for those who survived. We can conclude from this graph that : \n", "*Most of passengers were traveling alone! And most of the solo travelers died (The red circle is bigger)\n", "*Families of a small size survive have a better survival chance! Notice the dominant blue circles in the lower left quadrant.\n", "\n", "All these conclusion motivate us to create two new variables that hopefully will help our Machine Learning algorithm in its learning task : \n", "*FSize: Family size\n", "*Ftype: Family type. Is it a \"Solo\" \"Small\" or \"Large \" family ?\n", "\n", "```{r}\n", "Full$Fsize<-Full$SibSp+Full$Parch+1\n", "ggplot(Full[1:891,],aes(x=Fsize,fill=factor(Survived)))+geom_bar(position=\"dodge\")+ggtitle(\"Survival VS family size\")\n", "```\n", "\n", "From the previous graph, solo travelers seem not to survive as often. Families with 4 or lower size seem to have a higher chance of survival. Bigger families seem to struggle to survive. \n", "\n", "```{r}\n", "Full$Ftype[Full$Fsize==1]<-\"Solo\"\n", "Full$Ftype[Full$Fsize>4]<-\"Large\"\n", "Full$Ftype[Full$Fsize>1&Full$Fsize<5]<-\"Small\"\n", "mosaicplot(table(Full$Ftype,Full$Survived),color =2:3, main=\"Survival Vs Family size\")\n", "```\n", "\n", "\n", "##Ticket\n", "\n", "This quite a tough feature to deal with and extract any useful information if we don't know the algorithm that was used to generate the numbers behind. For that reason we going to pass on it, but it is worth giving a try to see there is commonalties in the ticket numbers.\n", "\n", "##Fare\n", "\n", "A great way to start studying a numeric feature is through the `summary ()` function.\n", "\n", "```{r}\n", "summary(Full$Fare)\n", "```\n", "\n", "The median is half the mean so the fare data is skewed. We can also look at the max and min to have a feel of the range. From the summary result we can also see that we have 1 NA value .One solution would be to delete it  and pass on but since we\u2019re learning here it would be nice to have a guess of the missing fare.\n", "\n", "```{r}\n", "Full[is.na(Full$Fare),]\n", "```\n", "\n", "\n", "We can see that Mr.Thomas Storey (The passenger with a missing Fare value) boarded from Southampton with a ticket of a third class. Intuitively, ticket prices are usually dictated by the ticket class and from where we re boarding. So let s plot the fare density of the third class tickets that have boarded from Southampton.\n", "\n", "```{r,warning=False,error=False}\n", "ggplot(Full[Full$Pclass==3&Full$Embarked==\"S\",],aes(Fare))+geom_density()+geom_vline(aes(xintercept=median(Fare,na.rm = T)),color=\"red\",linetype=\"dashed\",lwd=2)+ggtitle(\"Fare density for Pclass=3 and Embarked = S\")\n", "```\n", "\n", "The fare density is uni-modal. Therefore, it makes sense to have the median as a guess of how much Mr thomas paid to get onboard.\n", "\n", "```{r}\n", "Full$Fare[is.na(Full$Fare)]<-median(Full$Fare[Full$Pclass==3&Full$Embarked==\"S\"],na.rm=T)\n", "```\n", "\n", "##Cabin\n", "\n", "Let's first check some Cabin values from our data set.\n", "\n", "```{r}\n", "head(Full$Cabin)\n", "```\n", "\n", "It looks like we have quite a lot of NAs. But how much ? \n", "\n", "```{r}\n", "round((100*sum(is.na(Full$Cabin))/length(Full$Cabin)),2)\n", "```\n", "\n", "77 % of the data is not there. Most of the Kernels, I have studied, decided to drop this feature due its scarcity. But we\u2019re going to proceed differently here. The cabin structure seems to always start with a letter, representing the deck, followed by a number. Could it be that some decks have a better chance of survival than others ? Maybe.\n", "What about the missing cabin information ? We can put all of them under the umbrella of a fictitious Unknown Deck.\n", "\n", "\n", "head(Full$Cabin)\n", "```{r}\n", "Full$Cabin[is.na(Full$Cabin)]<-\"U\"\n", "Full$Cabin<-substr(Full$Cabin,1,1)\n", "unique(Full$Cabin)\n", "```\n", "\n", "```{r}\n", "ggplot(Full[1:891,],aes(x=Cabin,fill=factor(Survived)))+geom_bar(position=\"dodge\")\n", "```\n", "\n", "\n", "Interesting!  All decks, except our fictitious unknown deck have better chances of survival than perishing.  \n", "\n", "##Embarked \n", "\n", "```{r,warning=F}\n", "summary(factor(Full$Embarked))\n", "```\n", "\n", "We can see that we have two passengers with no recorded embarking port! Once again we can just delete these cases and move on but since we\u2019re learning here let s have a guess! \n", "\n", "First things first, who are these two passengers?\n", "\n", "```{r}\n", "Full %>% filter(is.na(Embarked))\n", "```\n", "\n", "\n", "As we already did with the fare above, here we know the fare and Pcalss, and we need to guess of the Port. \n", "\n", "Both passengers have paid 80 $ and both have a first class ticket.\n", "\n", "\n", "```{r}\n", "ggplot(Full %>% filter(!is.na(Embarked)),aes(x=Embarked,y=Fare,fill=factor(Pclass)))+geom_boxplot()+geom_hline(aes(yintercept=80),color=\"purple\",lwd=2,linetype=\"dashed\")+ggtitle(\"Fare Vs Embarked port per Class\")\n", "```\n", "\n", "We can see that a first class ticket with 80$ falls exactly at the median of Cherbourg. So we have a strong evidence here that both passengers embarked from Cherbourg!\n", "\n", "```{r}\n", "Full$Embarked[is.na(Full$Embarked)]<-\"C\"\n", "```\n", "\n", "##Age\n", "\n", "```{r}\n", "summary(Full$Age)\n", "```\n", "\n", "We can see that the mean and median are close so apriori we don't have outliers issue here. On the otherhand, we notice that we have 263  NA values which is 20.1 % of the total data. Let s try to treat this.\n", "\n", "To do this we re going to use Multivariate Imputation via Chained Equations (MICE). A good tutorial on this technique can be found [here](https://www.r-bloggers.com/imputing-missing-data-with-r-mice-package/)\n", "\n", "First, let have a look on the structure of our dataset.\n", "\n", "```{r}\n", "str(Full)\n", "```\n", "\n", "We can see that some of the features are not yet coded in the good format. For instance, `Sex` is code as `chr` while in reality it is a categorical data with two possible values male or female. So we need to convert it to `factor`. This is a critical step before running MICE since it relies on the variable type to select it prediction algorithm.\n", "\n", "```{r}\n", "To_factor<-c(\"Survived\",\"Pclass\",\"title\",\"Sex\",\"Embarked\",\"Cabin\",\"Ftype\")\n", "Full[To_factor]<-lapply (Full[To_factor],function(x)as.factor(x))\n", "```\n", "\n", "Now let's run MICE and exclude `PassengerId`,`Survived`,`name`,`surname`,`Ticket` from  the age prediction.\n", "\n", "```{r,results='hide',message=FALSE}\n", "mice_mod<-mice(Full[,!names(Full)%in%c(\"PassengerId\",\"Survived\",\"name\",\"surname\",\"Ticket\")],seed=1991,m=20)\n", "```\n", "\n", "\n", "A first step toward confirming that MICE has converged is to plot the mean and SD evolution per iteration.\n", "\n", "```{r}\n", "plot(mice_mod)\n", "```\n", "\n", "In the technical description of the package a good convergence is achieved if the iteration lines are mingling and there is no clear trend ! So far so good ! \n", "\n", "Then we can check the density plot of the age values.\n", "\n", "```{r}\n", "densityplot(mice_mod)\n", "```\n", "\n", "The density plot of the actual non-missing values (the thick blue line) is  quite similar to the imputed values (the thin lines representing different iteration).\n", "\n", "So let's assign the imputed age values to our dataset.\n", "\n", "```{r}\n", "output<-complete(mice_mod)\n", "Full$Age<-output$Age\n", "```\n", "\n", "Since in crises times the protocol is to help women and children first, let s explore if children have a higher survival rate.\n", "\n", "```{r,warning=F,error=F}\n", "ggplot(Full[1:891,],aes(Age,fill=factor(Survived)))+geom_density(alpha=0.5)+ggtitle(\"Survival Vs Age\")+geom_vline(aes(xintercept=15),color=\"red\",linetype=\"dashed\",lwd=2)\n", "```\n", "\n", "Based on the density curves it having an age below 15 entails better chances to survive. So we can create a new feature called `Child` to help our algorithm in its learning process.\n", "\n", "```{r}\n", "Full$child<-factor(ifelse(Full$Age<15,1,0))\n", "```\n", "\n", "In this section, we had the chance to walk through all the features. We gain some understanding how each one of them affects passenger\u2019s survival. We have also cleaned our data by putting each feature in the right format (like factors) and more importantly, we treated the missing values issue. We even did what is called **Feature Engineering** by introducing new features in our dataset, hoping it makes the learning process of our algorithm easier.\n", "\n", "#Correlation analysis\n", "\n", "One of the challenges in machine learning is selecting the features that our ML algorithm going to use to make its prediction. One of the way to roughly guide our choice is the correlation matrix.\n", "\n", "```{r}\n", "str(Full)\n", "```\n", "\n", "```{r}\n", "Full[1:891,] %>% select(-PassengerId,-name,-surname,-Ticket) %>% mutate(\n", "  Survived=as.numeric  (factor(Survived)),\n", "  Ftype=as.numeric  (factor(Ftype)),\n", "  Pclass=as.numeric (Pclass),\n", "  title=as.numeric (title),\n", "  Sex=as.numeric (Sex),\n", "  child=as.numeric (child),\n", "  Cabin=as.numeric (Cabin),\n", "  Embarked=as.numeric (Embarked)) %>% cor(use=\"complete.obs\") %>% corrplot(type=\"lower\",diag=F)\n", "```\n", "\n", "From the correlation matrix, we can see that Sex and PClass followed by Fare and Cabin then Embarked are major contributors in predicting passenger\u2019s survival. Does this entail we need to discard the rest of the features? Probably not as we\u2019re going to discover later.\n", "\n", "#Prediction\n", "\n", "Before building our model let's make sure that everything is neat.\n", "\n", "```{r}\n", "str(Full)\n", "```\n", "\n", "Our features seems to be well coded.\n", "\n", "```{r}\n", "round(100*colSums(is.na(Full))/nrow(Full),2)\n", "```\n", "\n", "We don't have any missing values anymore (except for Survived which we're going to predict).\n", "Now that our data look fine, we need to split it back into a training and test set.\n", "\n", "```{r}\n", "train<-Full[1:891,]\n", "test<-Full[-(1:891),]\n", "```\n", "\n", "We're going to try here a randomforest algorithm to predict survival.\n", "\n", "```{r}\n", "rf_model <- randomForest(Survived ~ title + Age + Pclass + Sex + Fare  + Cabin + Embarked +child + Fsize, data = train, ntree=2000)\n", "\n", "rf_model\n", "```\n", "\n", "A forest of 2000 tree was used to predict the Survival of the passengers. An important parameter to look at is the `OBB estimate of error rate`. This error rate is a good indicator of future performance since it is the average of errors estimated on the un-seen samples by each tree while building the model which uses a bootstrap sampling method in the backend.\n", "We can also have a look on the confusion matrix on the bottom. We can see that our model is better at predicting death than survival.\n", "\n", "In order to study the weight of each variable on the prediction of passenger\u2019s survival.\n", "\n", "\n", "```{r}\n", "importance<-importance(rf_model)\n", "\n", "\n", "variable<-data.frame(name=row.names(importance),val=importance[,\"MeanDecreaseGini\"])\n", "\n", "\n", "ggplot(variable,aes(x=reorder(name,val),y=val,fill=val))+geom_bar(stat=\"identity\")+coord_flip()+ggtitle(\"Variables importance\")+xlab(\"Variable\")+ylab(\"Importance\")\n", "```\n", "\n", "\n", "`title`  has the highest weight on our survival prediction despite the fact that it didn\u2019t correlate well with Survived in the correlation matrix we plotted above.\n", "\n", "#Submission\n", "\n", "To generate our submission file we can run the following code.\n", "\n", "```{r}\n", "Survived<-predict(rf_model,test)\n", "Submit<-data.frame(PassengerID=test$PassengerId,Survived)\n", "write.csv(Submit,file=\"prediction.csv\",row.names =F)\n", "```\n", "\n", "Our submission yielded an accuracy rate of 78 %. To reach the 81 % level, we need to use conditional inference trees instead of randomforest algorithm. This was borrowed from Trevor Stephen [tutorial](http://trevorstephens.com/kaggle-titanic-tutorial/r-part-5-random-forests/).\n", "\n", "#Conclusion\n", "\n", "In this report, we have seen how to process the Titanic data to submit our prediction. We started by setting our expectations. Then we have visited all the features to clean them and gain further understanding on how they impact passengers survival. Then we have built our model to perform the prediction.\n", "There is definitely a lot to be done to improve the accuracy such as trying different algorithms and tweaking its parameters. Other engineered features might increase the accuracy as well.\n", "\n", "\n", "\n"]}], "nbformat_minor": 1}