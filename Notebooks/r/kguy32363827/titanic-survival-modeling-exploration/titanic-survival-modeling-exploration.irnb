{"metadata": {"kernelspec": {"language": "R", "display_name": "R", "name": "ir"}, "language_info": {"name": "R", "file_extension": ".r", "version": "3.4.2", "mimetype": "text/x-r-source", "codemirror_mode": "r", "pygments_lexer": "r"}}, "cells": [{"execution_count": null, "metadata": {"_uuid": "9e48ddf47d634dbd2547db3fb5a12f2a630a379c", "_cell_guid": "70723ecd-99f5-436a-b7b3-b7b60fc3a14c"}, "source": ["# This R environment comes with all of CRAN preinstalled, as well as many other helpful packages\n", "# The environment is defined by the kaggle/rstats docker image: https://github.com/kaggle/docker-rstats\n", "# For example, here's several helpful packages to load in \n", "\n", "library(ggplot2) # Data visualization\n", "library(readr) # CSV file I/O, e.g. the read_csv function\n", "\n", "# Input data files are available in the \"../input/\" directory.\n", "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n", "\n", "system(\"ls ../input\")\n", "\n", "# Any results you write to the current directory are saved as output."], "outputs": [], "cell_type": "code"}, {"source": ["Exploration of data & testing best features for predicting survival.\n", "\n", "## Read in training & test data\n", "Basic summary and setting aside holdout set for model validation"], "metadata": {}, "cell_type": "markdown"}, {"execution_count": null, "metadata": {}, "source": ["set.seed(1293847)\n", "trn_raw = read.csv('../input/train.csv')\n", "HO_idx <- sample(seq_len(nrow(trn_raw)), size = floor(0.2 * nrow(trn_raw)))\n", "trn_r = trn_raw[-HO_idx,]\n", "val_r = trn_raw[HO_idx,]\n", "tst_r = read.csv('../input/test.csv')\n", "str(trn_r)\n", "str(val_r)\n", "str(tst_r)"], "outputs": [], "cell_type": "code"}, {"source": ["Notes:\n", "\n", "Already seeing some variables that are unlikely to be useful in modeling due to high cardinality (Passenger ID, Name, Ticket, Cabin); might be worth looking into feature engineering for some of these\n", "\n", "Also already seeing some indications of missing data (factors labeled \"\" in cabin & embarked features); will need to investigate and fill or drop these variables or the records with missing data, and look for other cases of missing data\n", "\n", "Creating copys of the training sets to keep our modifications separate from the raw data import:"], "metadata": {}, "cell_type": "markdown"}, {"execution_count": null, "metadata": {}, "source": ["trn_m = trn_r\n", "val_m = val_r\n", "tst_m = tst_r"], "outputs": [], "cell_type": "code"}, {"source": ["## Addressing Missing Data\n", "### Embarked\n", "The Missing embarkment information appears to exist for only 2 records: 1 each in the training & validation datasets:"], "metadata": {}, "cell_type": "markdown"}, {"execution_count": null, "metadata": {}, "source": ["rbind(trn_m[trn_m$Embarked == \"\",],val_m[val_m$Embarked == \"\",])"], "outputs": [], "cell_type": "code"}, {"source": ["These two appear to be travelling together (same ticket # & cabin number) but are likely unrelated (different names, no parent/child or spouse/sibling relationship indicated). \n", "\n", "For minimal missing categorical data, it is generally best to assign the mode of the category.  A quick look at the \"embarked\" variable shows that passengers have a 72% chance of coming from location \"S\" (Southampton).  "], "metadata": {}, "cell_type": "markdown"}, {"execution_count": null, "metadata": {}, "source": ["table(trn_m$Embarked)/sum(table(trn_m$Embarked))"], "outputs": [], "cell_type": "code"}, {"source": ["Given the heavy prior probability and mode of Embarkment = S, it seems fairly safe to assign these two recrods to that embarkment category.  (There is not missing embarkement information in the test set, but if there were, we would assign the \"S\" value to those as well rather than reassessing the mode of the test population.)"], "metadata": {}, "cell_type": "markdown"}, {"execution_count": null, "metadata": {}, "source": ["trn_m[trn_m$Embarked == \"\",c('Embarked')] = 'S'\n", "val_m[val_m$Embarked == \"\",c('Embarked')] = 'S'"], "outputs": [], "cell_type": "code"}, {"source": ["#### Other categorical variables with missing data\n", "It looks like the only other categorical variable with missing data is \"Cabin\", which is an issue in both the train and test datasets.  We likely won't be using this variable as is (either going to be dropped or possibly used for some feature engineering), so we'll skip the missing data issue on it for now. \n", "\n", "#### Numeric variables with missing data\n", "Checking for any other missing data that has been flagged as \"NA\", which is a check that primarily works on numeric/integer vectors:"], "metadata": {}, "cell_type": "markdown"}, {"execution_count": null, "metadata": {}, "source": ["colSums(is.na(trn_m))\n", "colSums(is.na(val_m))\n", "colSums(is.na(tst_m))"], "outputs": [], "cell_type": "code"}, {"source": ["Looks like our training & validation sets are missing values under only Age, and the test data is missing values under Age & Fare. Next calculation is to get a sense of the percentage of observations that are missing data in these variables: "], "metadata": {}, "cell_type": "markdown"}, {"execution_count": null, "metadata": {}, "source": ["# Technique picked up from here: https://www.r-bloggers.com/imputing-missing-data-with-r-mice-package/:\n", "pMiss <- function(x){sum(is.na(x))/length(x)*100}\n", "apply(trn_m,2,pMiss)[apply(trn_m,2,pMiss)>0]\n", "apply(val_m,2,pMiss)[apply(val_m,2,pMiss)>0]\n", "apply(tst_m,2,pMiss)[apply(tst_m,2,pMiss)>0]"], "outputs": [], "cell_type": "code"}, {"source": ["One method for handling missing numeric data is to blanket-assign the mean value of the training data. There's virtually no harm in using this method for the missing Fare data in the test set; we'll take the mean from the training set and assign it to the single record with the missing value in the test set.\n", "\n", "Mean, standard deviation & median values for Fare in training data:"], "metadata": {}, "cell_type": "markdown"}, {"execution_count": null, "metadata": {}, "source": ["sd(trn_m$Fare)\n", "median(trn_m$Fare)\n", "fill_Fare = mean(trn_m$Fare)\n", "tst_m$Fare_orig = tst_m$Fare"], "outputs": [], "cell_type": "code"}, {"source": ["Comparison of Fare mean & standard deviation in test set before and after filling NA with training mean:\n"], "metadata": {}, "cell_type": "markdown"}, {"execution_count": null, "metadata": {}, "source": ["tst_m.origfare_noNA = tst_m[complete.cases(tst_m[,c('Fare')]),c('Fare')]\n", "mean(tst_m.origfare_noNA)\n", "sd(tst_m.origfare_noNA)\n", "\n", "tst_m[is.na(tst_m$Fare),c('Fare')] = fill_Fare\n", "mean(tst_m$Fare)\n", "sd(tst_m$Fare)"], "outputs": [], "cell_type": "code"}, {"source": ["Minimal changes with the data filling; Filled Fare data should be good to go. Dropping original Fare data vector from test set. "], "metadata": {}, "cell_type": "markdown"}, {"execution_count": null, "metadata": {}, "source": ["tst_m = subset(tst_m, select=-c(Fare_orig))"], "outputs": [], "cell_type": "code"}, {"source": ["For the age variable, the amount of missing data runs may run the risk of biasing the model if we assign a single value to such a large percentage of records. We'll still do it, and be sure to compare means, standard deviations, and visual distributions of the original vs. filled data to see how it looks.  \n"], "metadata": {}, "cell_type": "markdown"}, {"execution_count": null, "metadata": {}, "source": ["fill_Age_mean = mean(trn_m[complete.cases(trn_m[,c('Age')]),c('Age')])\n", "\n", "fill_Age_mean\n", "sd(trn_m[complete.cases(trn_m[,c('Age')]),c('Age')])\n", "median(trn_m[complete.cases(trn_m[,c('Age')]),c('Age')])"], "outputs": [], "cell_type": "code"}, {"execution_count": null, "metadata": {}, "source": ["trn_m$Age_comp_FM = trn_m$Age\n", "val_m$Age_comp_FM = val_m$Age\n", "tst_m$Age_comp_FM = tst_m$Age"], "outputs": [], "cell_type": "code"}, {"execution_count": null, "metadata": {}, "source": ["trn_m[is.na(trn_m$Age_comp_FM),c('Age_comp_FM')] = fill_Age_mean\n", "val_m[is.na(val_m$Age_comp_FM),c('Age_comp_FM')] = fill_Age_mean\n", "tst_m[is.na(tst_m$Age_comp_FM),c('Age_comp_FM')] = fill_Age_mean\n", "\n", "mean(trn_m$Age_comp_FM)\n", "sd(trn_m$Age_comp_FM)\n", "median(trn_m$Age_comp_FM)"], "outputs": [], "cell_type": "code"}, {"execution_count": null, "metadata": {}, "source": ["par(mfrow=c(1,2))\n", "hist(trn_m$Age, freq = FALSE, main = 'Age: Original', ylim = c(0,0.045))\n", "hist(trn_m$Age_comp_FM, freq = FALSE, main = 'Age: Filled with Mean', ylim = c(0,0.045))"], "outputs": [], "cell_type": "code"}, {"source": ["We can see that the mean and standard deviation isn't changing much using this method, but the overall distribution of the variable is pretty different. This makes sense, since we've essentially made 20% of the observations of this variable equal to the same value, which is not ideal and very likely to have introduced some level of bias in the data. \n", "\n", "While investigating other methods for addressing missing data, I ran across an interesting function from another Kaggler that I want to try: Using the MICE method to impute the values. \n", "\n", "Resources used for learning about the MICE method: \n", "\n", "    * https://www.r-bloggers.com/imputing-missing-data-with-r-mice-package/\n", "    * https://www.kaggle.com/mrisdal/exploring-survival-on-the-titanic\n", "\n", "Creating a cacluated column that estimates age for the observations missing that feature. "], "metadata": {}, "cell_type": "markdown"}, {"execution_count": null, "metadata": {}, "source": ["library(mice)\n", "cols_for_age_imp = c(\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\")\n", "trn_imp_ds = trn_m[names(trn_m) %in% cols_for_age_imp]\n", "trn_imp <- complete(mice(trn_imp_ds, seed = 123432))\n", "trn_m$Age_comp_MICE = trn_imp$Age\n", "\n", "val_imp_ds = val_m[names(val_m) %in% cols_for_age_imp]\n", "val_imp <- complete(mice(val_imp_ds, seed = 123412348))\n", "val_m$Age_comp_MICE = val_imp$Age\n", "\n", "tst_imp_ds = tst_m[names(tst_m) %in% cols_for_age_imp]\n", "tst_imp <- complete(mice(tst_imp_ds, seed = 123412348))\n", "tst_m$Age_comp_MICE = tst_imp$Age"], "outputs": [], "cell_type": "code"}, {"execution_count": null, "metadata": {}, "source": ["par(mfrow=c(3,3))\n", "hist(trn_m$Age, freq= FALSE, main = \"Training Data:  Original Age\", ylim = c(0,0.045))\n", "hist(trn_m$Age_comp_FM, freq= FALSE, main = \"Training Data: Age via Mean\", ylim = c(0,0.045))\n", "hist(trn_m$Age_comp_MICE, freq= FALSE, main = \"Training Data: Age via MICE\", ylim = c(0,0.045))\n", "\n", "hist(val_m$Age, freq= FALSE, main = \"Vdalidation Data:  Original Age\", ylim = c(0,0.045))\n", "hist(val_m$Age_comp_FM, freq= FALSE, main = \"Validation Data: Age via Mean\", ylim = c(0,0.045))\n", "hist(val_m$Age_comp_MICE, freq= FALSE, main = \"Validation Data: Age via MICE\", ylim = c(0,0.045))\n", "\n", "hist(tst_m$Age, freq= FALSE, main = \"Test Data:  Original Age\", ylim = c(0,0.045))\n", "hist(tst_m$Age_comp_FM, freq= FALSE, main = \"Test Data: Age via Mean\", ylim = c(0,0.045))\n", "hist(tst_m$Age_comp_MICE, freq= FALSE, main = \"Test Data: Age via MICE\", ylim = c(0,0.045))"], "outputs": [], "cell_type": "code"}, {"execution_count": null, "metadata": {}, "source": ["age_compare = data.frame( Dataset =character(), Calculation = character(),\n", "                   Mean = double(), StD = double(), \n", "                   stringsAsFactors=FALSE)\n", "\n", "age_compare[1,] = c('Train', 'Original', mean(trn_m[complete.cases(trn_m[,c('Age')]),c('Age')]), sd(trn_m[complete.cases(trn_m[,c('Age')]),c('Age')]))\n", "age_compare[2,] = c('Train', 'Filled by Mean', mean(trn_m$Age_comp_FM), sd(trn_m$Age_comp_FM))\n", "age_compare[3,] = c('Train', 'Filled by MICE', mean(trn_m$Age_comp_MICE), sd(trn_m$Age_comp_MICE))\n", "\n", "age_compare[4,] = c('Val', 'Original', mean(val_m[complete.cases(val_m[,c('Age')]),c('Age')]), sd(val_m[complete.cases(val_m[,c('Age')]),c('Age')]))\n", "age_compare[5,] = c('Val', 'Filled by Mean', mean(val_m$Age_comp_FM), sd(val_m$Age_comp_FM))\n", "age_compare[6,] = c('Val', 'Filled by MICE', mean(val_m$Age_comp_MICE), sd(val_m$Age_comp_MICE))\n", "\n", "age_compare[7,] = c('Test', 'Original', mean(tst_m[complete.cases(tst_m[,c('Age')]),c('Age')]), sd(tst_m[complete.cases(tst_m[,c('Age')]),c('Age')]))\n", "age_compare[8,] = c('Test', 'Filled by Mean', mean(tst_m$Age_comp_FM), sd(tst_m$Age_comp_FM))\n", "age_compare[9,] = c('Test', 'Filled by MICE', mean(tst_m$Age_comp_MICE), sd(tst_m$Age_comp_MICE))\n", "\n", "age_compare"], "outputs": [], "cell_type": "code"}, {"source": ["The means are slightly different from the original set using the MICE method, but the distributions are clearly more similar, as are the standard deviations-- so in the case of this amount of missing data, the MICE method seems to be the better approach. \n", "\n", "\n", "\n", "## Prior Distributions\n", "#### Learning more about the target variable\n", "Taking a look at the prior distribution in the training data for the target variable, and adding in a factorized version of it to assist with modeling later on."], "metadata": {}, "cell_type": "markdown"}, {"execution_count": null, "metadata": {}, "source": ["bp <- barplot(table(trn_m$Survived), main = 'Survival')\n", "text(bp,table(trn_m$Survived)*.9,labels=table(trn_m$Survived))\n", "text(bp,20,labels=round((table(trn_m$Survived)/sum(table(trn_m$Survived)))*100,2))\n", "trn_m$Survived_F = factor(trn_m$Survived)\n", "val_m$Survived_F = factor(val_m$Survived)"], "outputs": [], "cell_type": "code"}, {"source": ["We appear to have a somewhat imbalanced dataset for our binary classification (62% not survived vs. 38% survived), which can be tricky for machine learning.  Relying on accuracy as a meteric for model quality can be distorted because we're not operating off a 50-50 chance. To address this, we could plan on relying on alternative measures of model quality (precision, AUC, etc), or use alternative methods to balance the dataset (oversample, undersampling, creating synthetic data, etc.).  \n", "\n", "We'll plan on circling back ot this idea when we reach the modeling stage. For now, we'll get a look at our other variables and see what options we have for feature engineering. \n", "\n", "#### Learning more about the input/independent variables\n", "Factor variables as compared to target"], "metadata": {}, "cell_type": "markdown"}, {"execution_count": null, "metadata": {}, "source": ["trn_m$Pclass = factor(trn_m$Pclass)\n", "trn_m$Embarked = factor(trn_m$Embarked)\n", "trn_m$Sex = factor(trn_m$Sex)\n", "\n", "val_m$Pclass = factor(val_m$Pclass)\n", "val_m$Embarked = factor(val_m$Embarked)\n", "val_m$Sex = factor(val_m$Sex)\n", "\n", "tst_m$Pclass = factor(tst_m$Pclass)\n", "tst_m$Embarked = factor(tst_m$Embarked)\n", "tst_m$Sex = factor(tst_m$Sex)\n", "\n", "\n", "table(trn_m$Pclass, trn_m$Survived)\n", "print('Class')\n", "\n", "table(trn_m$Embarked,trn_m$Survived)\n", "print('Embarked')\n", "\n", "table(trn_m$Sex,trn_m$Survived)\n", "print('Sex')"], "outputs": [], "cell_type": "code"}, {"source": ["Numeric variables as compared to target\n"], "metadata": {}, "cell_type": "markdown"}, {"execution_count": null, "metadata": {}, "source": ["par(mfrow=c(2,2))\n", "boxplot(Fare~Survived, data=trn_m, main = \"Fare\")\n", "boxplot(Age_comp_MICE~Survived, data=trn_m, main = \"Age (filled via MICE)\")\n", "boxplot(SibSp~Survived, data=trn_m, main = \"Siblings/Spouses\")\n", "boxplot(Parch~Survived, data=trn_m, main = \"Parents/Children\")"], "outputs": [], "cell_type": "code"}, {"source": ["## Feature Engineering\n", "As stated earlier, the Ticket & Name features are not likely to be valuable predictors.  The Cabin feature might be, if records missing cabin data means \"did not have a cabin\", which could be indicative of socio-economic status. Adding in calculated feature of \"HasCabin\" (0 for no data, 1 for has data), and will check interaction with Pclass.\n", "\n", "The combined features of parent/child (aka has a parent or a child on board) and Sibling/spouse (has a sibling or a spouse on board) may be a little hard to use as-is, since the relationsihp could go either way.  One way to simplify those features is to create a combined feature of \"family size\" (self + counts from each of those categories) and test that new feature's significance level as compared with the original features. We may also want to look at creating more specific relationships (\"has child\", for example), but that will likely need to rely on a combination of age & last name in order to split out those relationsihp types. For now, just adding a feature of \"family size\", and a flag for those traveling \"solo\" vs. with family (feature name: \"HasFamily\")."], "metadata": {}, "cell_type": "markdown"}, {"execution_count": null, "metadata": {}, "source": ["trn_m$HasCabin <- factor(ifelse(trn_m$Cabin == \"\", c(0), c(1)))\n", "trn_m$FamilySize = trn_m$SibSp + trn_m$Parch +1\n", "trn_m$HasFamily <- factor(ifelse(trn_m$FamilySize == 1, c(0), c(1)))\n", "\n", "val_m$HasCabin <- factor(ifelse(val_m$Cabin == \"\", c(0), c(1)))\n", "val_m$FamilySize = val_m$SibSp + val_m$Parch +1\n", "val_m$HasFamily <- factor(ifelse(val_m$FamilySize == 1, c(0), c(1)))\n", "\n", "tst_m$HasCabin <- factor(ifelse(tst_m$Cabin == \"\", c(0), c(1)))\n", "tst_m$FamilySize = tst_m$SibSp + tst_m$Parch +1\n", "tst_m$HasFamily <- factor(ifelse(tst_m$FamilySize == 1, c(0), c(1)))"], "outputs": [], "cell_type": "code"}, {"source": ["### Checking distributions & signifigance testing of engineered features"], "metadata": {}, "cell_type": "markdown"}, {"execution_count": null, "metadata": {}, "source": ["table(trn_m$HasCabin,trn_m$Survived)\n", "print('HasCabin')\n", "\n", "table(trn_m$HasFamily,trn_m$Survived)\n", "print('HasFamily')"], "outputs": [], "cell_type": "code"}, {"source": ["#### Has Cabin vs. Not\n", "Taking a deeper look at our \"HasCabin\" variable: Is there an interaction between having cabin data recorded & the ticket class?"], "metadata": {}, "cell_type": "markdown"}, {"execution_count": null, "metadata": {}, "source": ["table(trn_m[,c(\"Pclass\", \"HasCabin\")])\n", "for (i in 1:3){\n", "  cat('Pclass', i,': ', ((table(trn_m[,c(\"Pclass\", \"HasCabin\")])[i,]/sum(table(trn_m[,c(\"Pclass\", \"HasCabin\")])[i,]))[2])*100, '%\\n')\n", "}"], "outputs": [], "cell_type": "code"}, {"source": ["For the people who have cabins in the lower classes, is there a difference in their probability for survival?\n"], "metadata": {}, "cell_type": "markdown"}, {"execution_count": null, "metadata": {}, "source": ["trn_m$NonFC_wCabin <- ifelse(trn_m$Pclass != 1 & trn_m$HasCabin == 1, c(1), c(0)) \n", "table(trn_m[,c(\"NonFC_wCabin\", \"Survived\")])\n", "for (i in 1:2){\n", "  cat('Survival rate of Non-First Class + Cabin Satus of ', i-1, ': ', ((table(trn_m[,c(\"NonFC_wCabin\", \"Survived\")])[i,]/sum(table(trn_m[,c(\"NonFC_wCabin\", \"Survived\")])[i,]))[2])*100, '%\\n')\n", "}"], "outputs": [], "cell_type": "code"}, {"source": ["It seems like you had a better chance of surviving as a non-first class passenger if you had a cabin on file.  However, only 80% of first class travelors have cabin data on file. Why is it not 100%?  In what case would someone purchase a first class ticket and not have a cabin?  Perhaps it gets recorded under only 1 person's name in a group?  \n", "\n", "How many of those **in first class with no cabin** appear to have been traveling alone (no siblings/spouse, & no parents/children)?"], "metadata": {}, "cell_type": "markdown"}, {"execution_count": null, "metadata": {}, "source": ["cat(dim(trn_m[trn_m$Pclass == 1 & trn_m$Cabin == \"\" & trn_m$SibSp==0 & trn_m$Parch==0,])[1], ' people traveling alone; (', \n", "    (dim(trn_m[trn_m$Pclass == 1 & trn_m$Cabin == \"\" & trn_m$SibSp==0 & trn_m$Parch==0,])[1]/table(trn_m[,c(\"Pclass\", \"HasCabin\")])[1,1])*100, '% of those in PClass 1 without cabins)')"], "outputs": [], "cell_type": "code"}, {"source": ["This seems to negate the idea that missing cabin infromation is meaningful; if people we would expect to have cabins do not have data recorded, it is reasonable to assume that missing data at other class levels is not a reliable source of information as well.  \n", "\n", "Another option is to assume that all first class passengers do have cabins and were sharing with non-family members, and not all occupants have the cabin information recorded on their passenger manifest.  That would then indicate that cabin information is not recorded on all parties shared accommodations with non-family members (perhaps only showing up under the name of the person who purchased it). However, a look into the types of people sharing cabins negates this idea:"], "metadata": {}, "cell_type": "markdown"}, {"execution_count": null, "metadata": {}, "source": ["cabintable = table(trn_m$Cabin)\n", "cab_occupancy = data.frame(cabintable)\n", "colnames(cab_occupancy) = c('Cabin', 'Cab_Occ')\n", "trn_m = merge(trn_m,cab_occupancy, by = 'Cabin')\n", "\n", "trn_m$SharedCabin = ifelse(trn_m$Cab_Occ >1, c(1), c(0)) \n", "cat(table(trn_m$FamilySize,trn_m$SharedCabin)[1,2], 'people traveling without a parental, spousal, sibling or child relationship were sharing cabins.')"], "outputs": [], "cell_type": "code"}, {"source": ["The prevalence of shared cabins between non-family members that are recorded negates the idea that missing cabin data for first class patrons is due to those cases; it seems, rather than missing cabin data is simply random missing data, and therefore assigning significance to a lack of a recorded cabin number seems unwise. However, we'll still test for significance and correlation before deciding whether to exclude this calculated feature."], "metadata": {}, "cell_type": "markdown"}, {"source": ["### Binning numeric non-continuous features\n", "Age:\n", "\n", "Splitting ages into 5 hypothetically significant categories:\n", "\n", "    * Infants  (Under 1)\n", "    * Children (1-12)\n", "    * Teens (12-17)\n", "    * Adults (18-59)\n", "    * Senior (60+)"], "metadata": {}, "cell_type": "markdown"}, {"execution_count": null, "metadata": {}, "source": ["trn_m$Age_Bin = cut(trn_m$Age_comp_MICE, breaks = c(0, 1, 12, 18, 60, 200))\n", "val_m$Age_Bin = cut(val_m$Age_comp_MICE, breaks = c(0, 1, 12, 18, 60, 200))\n", "tst_m$Age_Bin = cut(tst_m$Age_comp_MICE, breaks = c(0, 1, 12, 18, 60, 200))\n", "table(trn_m$Age_Bin, trn_m$Survived)"], "outputs": [], "cell_type": "code"}, {"source": ["## Significance Testing\n", "It seems likely that fare amount and ticket class would be highly correlated. Testing that theory:"], "metadata": {}, "cell_type": "markdown"}, {"execution_count": null, "metadata": {}, "source": ["boxplot(Fare~Pclass,data=trn_m, main=\"Fare vs Class\", \n", "  \txlab=\"pClass\", ylab=\"Fare\")"], "outputs": [], "cell_type": "code"}, {"source": ["It does seem like apart from a few outliers, fare follows class reasonably closely. It likely makes sense to keep only one of these features in the model to avoid overstating their shared influence on the outcome. The extreme distribution and outliers of the fare variable may result in that being the less desirable feature to keep.\n", "\n", "## Checking for significance & Collinearity\n", "### Chi-Squared test for discrete variables"], "metadata": {}, "cell_type": "markdown"}, {"execution_count": null, "metadata": {}, "source": ["## Create an empty dataframe which will house the Chi-Squared results\n", "Chi = data.frame( var_name =character(), val = character(),\n", "                   No_Survival = double(), Survived= double(), \n", "                   pvalue= double(),\n", "                   stringsAsFactors=FALSE)\n", "f=0\n", "chirow = 0\n", "disc_var = c(\"Pclass\", \"Sex\", \"Embarked\", \"HasCabin\", \"HasFamily\", \"Age_Bin\")\n", "cat('Running chi-squared tests on discrete independent variables...')\n", "for (i in 1:length(disc_var)){\n", "##Run the chi2 test on the next dependent variable\n", "  tbl <- table(trn_m[,c(disc_var[i])],trn_m$Survived_F)\n", "  t=chisq.test(tbl)\n", "  ## grab the test results to populate the next two rows of the Chi dataframe (one row for each value of the independent variable)\n", "  for (f in 1:dim(tbl)[1]){\n", "    chirow = chirow+1\n", "    Chi[chirow, c(1,2)] <- c(disc_var[i], rownames(tbl)[f])\n", "    Chi[chirow, c(3,4,5)]<-c(tbl[f,1], tbl[f,2],t$p.value)}\n", "}\n", "Chi = Chi[with(Chi, order(pvalue,var_name,val)), ]\n", "Chi"], "outputs": [], "cell_type": "code"}, {"source": ["All of the main variables we are considering are coming up with a p-value < 0.05, which is the threshold we are using for significance\n", "\n", "## Addressing dataset imbalance\n", "Creating an alternative training set that has been balanced via oversampling of the minority target class.\n", "\n", "Resources on oversampling:\n", "\n", "    * https://www.marcoaltini.com/blog/dealing-with-imbalanced-data-undersampling-oversampling-and-proper-cross-validation#comments\n", "    * https://www.rdocumentation.org/packages/mlr/versions/2.10/topics/oversample\n", "    * https://www.analyticsvidhya.com/blog/2016/03/practical-guide-deal-imbalanced-classification-problems/"], "metadata": {}, "cell_type": "markdown"}, {"execution_count": null, "metadata": {}, "source": ["train = trn_m[,append(disc_var,c(\"Survived_F\"))]\n", "val = val_m[,append(disc_var,c(\"Survived_F\"))]\n", "test = tst_m[,disc_var]\n", "library(ROSE)\n", "trn_cnt = as.numeric(table(train$Survived_F)[1]*2)\n", "train_os <- ovun.sample(Survived_F ~ ., data = train, method = \"over\",N = trn_cnt)$data\n", "table(train_os$Survived_F)"], "outputs": [], "cell_type": "code"}, {"source": ["## Predictive Models\n", "Resources:\n", "\n", "    * http://blog.revolutionanalytics.com/2016/11/calculating-auc.html\n", "\n", "Models:\n", "\n", "    * Naive Bayes\n", "    * Logistic Regression\n", "    * Random Forest"], "metadata": {}, "cell_type": "markdown"}, {"execution_count": null, "metadata": {}, "source": ["# creating training variables & empty dataframe to store model assessment results\n", "train_x = train[,-which(names(train) %in% c(\"Survived_F\"))]\n", "train_os_x = train_os[,-which(names(train_os) %in% c(\"Survived_F\"))]\n", "val_x = val[,-which(names(val) %in% c(\"Survived_F\"))]\n", "\n", "Model_compare = data.frame( Model =character(), Dataset = character(),\n", "                   AUC = double(), Accuracy= double(), \n", "                   Sensitivity= double(), Specificity = double(), Precision = double(), \n", "                   stringsAsFactors=FALSE)"], "outputs": [], "cell_type": "code"}, {"execution_count": null, "metadata": {}, "source": ["# Naive Bayes\n", "set.seed(12762)\n", "\n", "library(e1071)\n", "library(caret)\n", "library(pROC)\n", "library(ROCR)\n", "model_NB_ubal = naiveBayes(Survived_F~., data = train)\n", "p_NB_ubal = predict(model_NB_ubal,val, type=\"class\")\n", "roc_NB_ubal <- roc(val_m$Survived, as.numeric(p_NB_ubal)-1)\n", "cm_NB_ubal <- confusionMatrix(data=p_NB_ubal,reference=val$Survived_F, positive= '1')\n", "\n", "Model_compare[1,] = c('Naive Bayes', 'Unbalanced', auc(roc_NB_ubal), \n", "                     as.numeric(cm_NB_ubal$overall[1]), # Accuracy\n", "                     as.numeric(cm_NB_ubal$byClass[1]), # Sensitivity\n", "                     as.numeric(cm_NB_ubal$byClass[2]), # Specificity\n", "                     as.numeric(cm_NB_ubal$byClass[5])) # Precision\n", "\n", "\n", "model_NB_bal = naiveBayes(Survived_F~., data = train_os)\n", "p_NB_bal = predict(model_NB_bal,val, type=\"class\")\n", "roc_NB_bal <- roc(val_m$Survived, as.numeric(p_NB_bal)-1)\n", "cm_NB_bal <- confusionMatrix(data=p_NB_bal,reference=val$Survived_F, positive= '1')\n", "\n", "Model_compare[2,] = c('Naive Bayes', 'Balanced', auc(roc_NB_bal), \n", "                     as.numeric(cm_NB_bal$overall[1]), # Accuracy\n", "                     as.numeric(cm_NB_bal$byClass[1]), # Sensitivity\n", "                     as.numeric(cm_NB_bal$byClass[2]), # Specificity\n", "                     as.numeric(cm_NB_bal$byClass[5])) # Precision\n"], "outputs": [], "cell_type": "code"}, {"execution_count": null, "metadata": {}, "source": ["# Logistic Regression\n", "set.seed(23489234)\n", "model_LR_ubal <- glm(Survived_F ~.,family=binomial(link='logit'),   data=train)\n", "p_LR_ubal = predict(model_LR_ubal,val, type=\"response\")\n", "p_LR_ubal <- ifelse(p_LR_ubal > 0.5,1,0)\n", "roc_LR_ubal <- roc(val_m$Survived, as.numeric(p_LR_ubal)-1)\n", "cm_LR_ubal <- confusionMatrix(data=p_LR_ubal,reference=val$Survived_F, positive= '1')\n", "\n", "Model_compare[3,] = c('Logistic Regression', 'Unbalanced', auc(roc_LR_ubal), \n", "                     as.numeric(cm_LR_ubal$overall[1]), # Accuracy\n", "                     as.numeric(cm_LR_ubal$byClass[1]), # Sensitivity\n", "                     as.numeric(cm_LR_ubal$byClass[2]), # Specificity\n", "                     as.numeric(cm_LR_ubal$byClass[5])) # Precision\n", "\n", "model_LR_bal <- glm(Survived_F ~.,family=binomial(link='logit'),   data=train_os)\n", "p_LR_bal = predict(model_LR_bal,val, type=\"response\")\n", "p_LR_bal <- ifelse(p_LR_bal > 0.5,1,0)\n", "roc_LR_bal <- roc(val_m$Survived, as.numeric(p_LR_bal)-1)\n", "cm_LR_bal <- confusionMatrix(data=p_LR_bal,reference=val$Survived_F, positive= '1')\n", "\n", "Model_compare[4,] = c('Logistic Regression', 'Balanced', auc(roc_LR_bal), \n", "                     as.numeric(cm_LR_bal$overall[1]), # Accuracy\n", "                     as.numeric(cm_LR_bal$byClass[1]), # Sensitivity\n", "                     as.numeric(cm_LR_bal$byClass[2]), # Specificity\n", "                     as.numeric(cm_LR_bal$byClass[5])) # Precision"], "outputs": [], "cell_type": "code"}, {"execution_count": null, "metadata": {}, "source": ["## Random forest: initial comparison of balanced vs. imbalanced data\n", "## Resource: https://www.r-bloggers.com/random-forests-in-r/\n", "library(randomForest)\n", "set.seed(10981)\n", "model_RF_ubal = randomForest(Survived_F ~., data = train)\n", "model_RF_bal = randomForest(Survived_F ~., data = train_os)\n", "par(mfrow=c(1,2))\n", "plot(model_RF_ubal, main = 'Unbalanced', ylim = c(0,.4))\n", "plot(model_RF_bal, main = 'Balanced \\n(via oversampling)', ylim = c(0,.4))"], "outputs": [], "cell_type": "code"}, {"execution_count": null, "metadata": {}, "source": ["model_RF_bal"], "outputs": [], "cell_type": "code"}, {"execution_count": null, "metadata": {}, "source": ["rf_err = data.frame(model_RF_bal$err.rate)\n", "rf_err$combined_err = rowSums(rf_err)\n", "rf_err[which.min(rf_err$combined_err),]"], "outputs": [], "cell_type": "code"}, {"source": ["Balanced data set, and 116 trees looks like the winner. Next need to identify the optimal number of variables to sample for each tree."], "metadata": {}, "cell_type": "markdown"}, {"execution_count": null, "metadata": {}, "source": ["# calculating optimal number of variables per split\n", "set.seed(1239487)\n", "ntree = 116\n", "best_mtry <-tuneRF(x=train_os_x, y=train_os$Survived_F, ntreeTry=ntree, stepFactor=1.5, improve=0.01, trace=TRUE, plot=FALSE, dobest=TRUE)\n", "\n", "mtry = best_mtry[which(best_mtry == min(best_mtry), arr.ind=TRUE)[1],1]\n", "cat('Optimal Number of Trees: ', ntree)\n", "cat('\\nOptimal Number of Variables sampled per split: ', mtry)"], "outputs": [], "cell_type": "code"}, {"execution_count": null, "metadata": {}, "source": ["# Random Forest Model\n", "set.seed(23489234)\n", "model_RF_ubal = randomForest(Survived_F ~., data = train, mtry=mtry, ntree = ntree)\n", "p_RF_ubal = predict(model_RF_ubal,val, type=\"response\")\n", "roc_RF_ubal <- roc(val_m$Survived, as.numeric(p_RF_ubal)-1)\n", "cm_RF_ubal <- confusionMatrix(data=p_RF_ubal,reference=val$Survived_F, positive= '1')\n", "\n", "Model_compare[5,] = c('Random Forest', 'Unbalanced', auc(roc_RF_ubal), \n", "                     as.numeric(cm_RF_ubal$overall[1]), # Accuracy\n", "                     as.numeric(cm_RF_ubal$byClass[1]), # Sensitivity\n", "                     as.numeric(cm_RF_ubal$byClass[2]), # Specificity\n", "                     as.numeric(cm_RF_ubal$byClass[5])) # Precision\n", "\n", "model_RF_bal = randomForest(Survived_F ~., data = train_os, mtry=mtry, ntree = ntree)\n", "p_RF_bal = predict(model_RF_bal,val, type=\"response\")\n", "roc_RF_bal <- roc(val_m$Survived, as.numeric(p_RF_bal)-1)\n", "cm_RF_bal <- confusionMatrix(data=p_RF_bal,reference=val$Survived_F, positive= '1')\n", "\n", "Model_compare[6,] = c('Random Forest', 'Balanced', auc(roc_RF_bal), \n", "                     as.numeric(cm_RF_bal$overall[1]), # Accuracy\n", "                     as.numeric(cm_RF_bal$byClass[1]), # Sensitivity\n", "                     as.numeric(cm_RF_bal$byClass[2]), # Specificity\n", "                     as.numeric(cm_RF_bal$byClass[5])) # Precision"], "outputs": [], "cell_type": "code"}, {"execution_count": null, "metadata": {}, "source": ["Model_compare"], "outputs": [], "cell_type": "code"}, {"execution_count": null, "metadata": {}, "source": ["importance    <- importance(model_RF_bal)\n", "varImportance <- data.frame(Variables = row.names(importance), \n", "                            Importance = round(importance[ ,'MeanDecreaseGini'],2))\n", "plot(varImportance)"], "outputs": [], "cell_type": "code"}, {"execution_count": null, "metadata": {}, "source": ["final_predict_prob = predict(model_LR_bal,test, type=\"response\")\n", "final_prediction = ifelse(final_predict_prob > 0.5,1,0)\n", "solution_file = data.frame(\"PassengerId\" = tst_m$PassengerId, \"Survived\" = final_prediction)\n", "#write.csv(solution_file, file = 'titanic_survival_prediction__oversampledLR.csv', row.names = FALSE)"], "outputs": [], "cell_type": "code"}, {"source": ["***Prediction Accuracy for Submission: 73.684%***"], "metadata": {}, "cell_type": "markdown"}, {"source": ["## Playing with simple ensembling (class assignment voting)"], "metadata": {}, "cell_type": "markdown"}, {"execution_count": null, "metadata": {}, "source": ["LR = final_prediction\n", "ensemble_predict = as.data.frame(LR)\n", "ensemble_predict$NB = as.numeric(predict(model_NB_bal,test, type=\"class\"))-1\n", "ensemble_predict$RF = as.numeric(predict(model_RF_bal,test, type=\"response\"))-1\n", "ensemble_predict$LR = as.numeric(ensemble_predict$LR)\n", "ensemble_predict$NB = as.numeric(ensemble_predict$NB)\n", "ensemble_predict$RF = as.numeric(ensemble_predict$RF)\n", "ensembled = round(rowMeans(ensemble_predict),0)\n", "#write.csv(ensembled, file = 'titanic_survival_prediction__simpleEnsemble.csv', row.names = FALSE)"], "outputs": [], "cell_type": "code"}, {"source": ["% Matching between single model & simple ensemble:"], "metadata": {}, "cell_type": "markdown"}, {"execution_count": null, "metadata": {}, "source": ["sum(ensembled == final_prediction)/length(ensembled)*100"], "outputs": [], "cell_type": "code"}, {"source": ["No benefit to accuracy, in this case; results in same output as single model"], "metadata": {}, "cell_type": "markdown"}], "nbformat_minor": 1, "nbformat": 4}