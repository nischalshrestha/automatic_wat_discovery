{"cells":[{"metadata":{"_uuid":"c9807f6c7ae92f549a00bf07368b16f4470c2a6c","_execution_state":"idle","trusted":true},"cell_type":"markdown","source":"# Titanic Name \"Spam\" Filter:\n\n![spam](http://playagricola.com/Kaggle/spam2.jpg)\n\n# Introduction\n\nThank you [Oscar Takeshita][1] for postulating an interesting question [here][2]. Oscar asks, how well can we predict Titanic passengers' survival\nusing only the words in the Name column and ignoring semantic information (i.e. don't acknowledge and prioritize title, last name, etc). What a fun idea. We must detect which emails (passengers) are spam (stay alive) by analyzing words.\n\nThis problem particularly interests me because I previously made a kernel that only uses the Name column [here][3] that scored an impressive 82% leaderboard score and cross validated at 84%. Therefore we should be able to build a great model using the Words within the Names. It would be great if a purely machine learning algorithm can find unsupervised the patterns that I did by reasoning. Furthermore Oscar's formulation provides an opportunity to practice techniques that didn't come up when tackling Titanic in the usual way.  We must work in high dimensional space and use techniques involved in spam classification and dimension reduction. \n\nIn this notebook, we'll tackle this problem first using PCA plus kNN, then second we'll employ a variant of naive Bayes.\n\n# Build dictionary and vectorize passengers\nLet's begin. Our first task is to build a dictionary of all the words used in the Name column.\n\n[1]:https://www.kaggle.com/pliptor\n[2]:https://www.kaggle.com/pliptor/name-only-study-with-interactive-3d-plot\n[3]:https://www.kaggle.com/cdeotte/titanic-using-name-only-0-81818"},{"metadata":{"trusted":true,"_uuid":"8a4bf521d7258eb7d6b2f907b2c92157e9a45a71"},"cell_type":"code","source":"train <- read.csv(\"../input/train.csv\",stringsAsFactors=F)\ntest <- read.csv(\"../input/test.csv\",stringsAsFactors=F)\ntest$Survived <- NA\ndata <- rbind(train,test)\n\n# create dictionary from training names\nwords = paste(data$Name[1:891],collapse=' ')\nwords = gsub('[.,\"()/]','',words)\nwords = gsub(' . |  ',' ',words)\nwords = strsplit(words,' ')[[1]]\nfreq = ave(1:length(words),words,FUN=length)\ndictionary = data.frame(Words=words,Freq=freq,stringsAsFactors=F)\ndictionary = dictionary[!duplicated(dictionary$Words) & dictionary$Freq>1,]\ndictionary <- dictionary[order(-dictionary$Freq),]\n\n# create vectorization from dictionary\ndata2 <- data.frame(Mr=rep(0,1309))\nfor (i in dictionary$Words) data2[,i] <- 0\nfor (i in 1:nrow(data)){\n    n = gsub('[.,\"()/]','',data$Name[i])\n    n = gsub(' . |  ',' ',n)\n    n = strsplit(n,' ')[[1]]\n    for (j in n){\n        if (j %in% dictionary$Words){\n            data2[i,j] = 1\n        }\n    }\n}","execution_count":2,"outputs":[]},{"metadata":{"_uuid":"56742dc2c20041b78d7609f0743b310c981a21b9"},"cell_type":"markdown","source":"The dictionary Words are now in a data frame named _dictionary_. It contains all words found in the training dataset names 2 or more times. And we one-hot-encoded all the passengers using these 415 words. This vectorization of passengers is stored in the variable named _data2_. It has dimension 1309 rows and 415 columns. (Note the variable _data_, contains the usual Titanic spreadsheet of all training and test passengers.)\n## Calculate the probability of survival given word\nNext let's calculate what the probability of survival is given that a passenger's Name contains the specific word. We won't need this to implement PCA with kNN, but we'll use this when we implement a variant of naive Bayes below."},{"metadata":{"trusted":true,"_uuid":"27966b342d98c383352c385879406ad030015435","_kg_hide-input":true},"cell_type":"code","source":"# calculate survival probability\ns=1:891\ndictionary$Survival <- NA\nfor (i in 1:length(dictionary$Words)){\n    x = intersect(which(data2[,dictionary$Words[i]]>0),s)\n    dictionary$Survival[i] <- mean(data$Survive[x],na.rm=T)\n}\n# display words and survival\nrow.names(dictionary) <- 1:nrow(dictionary)\ncat(\"15 most frequent Words and their survival rate:\\n\")\nhead(data.frame(n=1:nrow(dictionary),dictionary[order(-dictionary$Freq),]),15)\ncat(sprintf('%d Words in dictionary\\n',nrow(dictionary)))\ndictionary$Words[order(dictionary$Words)]","execution_count":7,"outputs":[]},{"metadata":{"_uuid":"fa3bfdbd5c5be35e78ff6b4c568bd8b32d7e9324"},"cell_type":"markdown","source":"# Principal Component Analysis\n## Calculate the components\nNow let's use PCA plus kNN to classify passengers' survival. First, let's calcuate the principal components. In the process, let's determine which words are correlated with which other words. Our variable _data2_ is \\\\(X\\\\). It contains the 1309 passengers each as 1 row. The 413 columns indicate whether Word \\\\(k\\\\) is present in that passenger's Name. You can think of each passenger as a vector of zeros and ones of length 413. \n\n$$ \\Sigma= \\text{cov}(X)= \\frac{1}{n} X^TX + m^Tm  \\quad\\text{ where } m \\in R^{1 \\times d} \\text{ are } X \\text{ column means} \\quad \\text{and }X\\in R^{n\\times d}$$\n$$ \\text{Principal components}=P \\in R^{d \\times d} \\quad \\text{where } \\Sigma = PDP^T \\quad \\text{with } P \\text{ orthogonal} \\quad \\text{and } D \\text{ diagonal}$$\n$$ \\text{corr}(x_i,x_j) = \\frac {\\Sigma_{i,j}}{\\sigma_i \\sigma_j} \\quad \\text{where } \\sigma_i = \\text{standard deviation of }x_i$$"},{"metadata":{"trusted":true,"_uuid":"868a0a276e64fe52279487f0cf76d71a0cfefca2","scrolled":true,"_kg_hide-input":true},"cell_type":"code","source":"data3 <- as.matrix(data2)\n# calculate mean and sd\nm = rep(0,ncol(data3))\ns = rep(0,ncol(data3))\nfor (i in 1:ncol(data3)){\n    m[i] = mean(data3[,i])\n    s[i] = sd(data3[,i])\n}\n# calculate covariance\ndata4 = (1/1309)*t(data3) %*% data3 - m %*% t(m)\n# calculate correlation\ndata5 = data4 / (s %*% t(s))\n# calculate principal components\nev <- eigen(data4,symmetric=T)\nev2 <- data.frame(values=ev[[1]],vectors=ev[[2]])\ncat('Principal components and correlations have been calculated.\\n\\n')\n# find word pairs with frequency>=4 and high correlation\ncat(\"Correlations above 0.3 of Words with freq>=4 are:\\n\")\nfor (i in 1:length(which(dictionary$Freq>=4)))\nfor (j in (i+1):length(which(dictionary$Freq>=4))){\n    if (i!=j & abs(data5[i,j])>=0.3)\n        cat(sprintf(\"%s and %s have correlation r = %f\\n\",row.names(data5)[i],row.names(data5)[j],data5[i,j]))\t\n}\n#cat(\"\\nCorrelations above 0.5 of Words with freq<=3\\n\")\n#for (i in length(which(dictionary$Freq>=4)):nrow(data5)-1)\n#for (j in (i+1):nrow(data5)-1){\n#    if (i!=j & abs(data5[i,j])>=0.5)\n#        cat(sprintf(\"%s and %s have correlation r = %f\\n\",row.names(data5)[i],row.names(data5)[j],data5[i,j]))\n#}","execution_count":13,"outputs":[]},{"metadata":{"_uuid":"6770c55c0fe11d33728f67ebb59d1b56b791353a"},"cell_type":"markdown","source":"Wow, among the 115 Words whose frequency is 4 or larger, there isn't much correlation. This surprises me. I know that we're approaching this problem without semantic information. (We're pretending that we don't know what titles, first names, and last names are.) But let me add a comment here. I expected male first names to correlate with male titles and I expected PCA to cluster all these names and titles into only a few principal components. This didn't happen. After thinking about, it makes mathematical sense. Yes, male names are associated with male titles but the relationship isn't the mathematical one defined as correlation. Here is a plot of the relationship between Charles and Mr that shows why."},{"metadata":{"_kg_hide-output":true,"trusted":true,"scrolled":true,"_uuid":"9ffa2ae6cbdf0895debdc9210fa1b077e95f78c7","_kg_hide-input":true},"cell_type":"code","source":"library(ggplot2)\nlibrary(gridExtra)\n\n#ggplot(data=data2, aes(x=data2$Charles,y=data2$Mr)) + \n#    geom_jitter(width=0.1,height=0.1) +\n#    geom_smooth(method='lm') +\n#    labs(x='Charles',y='Mr',title='Corr(Charles,Mr) = 0.0828.\\nLinear regression R^2 = 0.00686')","execution_count":12,"outputs":[]},{"metadata":{"_uuid":"5490b451eb7d4436efcb6daa692b771a2bbb4c1c"},"cell_type":"markdown","source":"![Charles](http://playagricola.com/Kaggle/Charles.png)\n  \nIt is true that when Charles is present, Mr is more likely to be present but the reason that Charles and Mr don't correlate is because when Charles is absent, it isn't the case that Mr is more likely to be absent.\n\n## Transform passengers into principal component weights\nLet's convert all 1309 passengers into vectors of principal component weights."},{"metadata":{"trusted":true,"_uuid":"c1107f3c430e9f067ff2db8d0f4f5d4110590be2","_kg_hide-input":false},"cell_type":"code","source":"data3Transformed = (t(ev[[2]]))[,1:nrow(ev[[2]])] %*% t(data3)\ndataPC6 = data.frame(n=1:1309,Survived=data$Survived,t(data3Transformed)[,1:6])\ncolnames(dataPC6) <- c('PassengerId','Survived',paste('PC',1:6,' wgt',sep=''))\nrownames(dataPC6) <- 1:1309\nhead(data[,c('PassengerId','Name','Sex','Age')])\ncat('First 6 principal component weights for the first 6 passengers:\\n')\nhead(dataPC6[,-2])","execution_count":9,"outputs":[]},{"metadata":{"_uuid":"b38eb0007795918d26488e32b7a268a9d8289bfb"},"cell_type":"markdown","source":"Examining the first 6 principal components (which are not displayed above) we learn they reference the following words:  \n* PC1  is 82% Mr,   -48% Miss,   -28 %Mrs,   0% other words  \n* PC2 is -64% Miss,   72.9% Mrs,   0% other words  \n* PC3 is 68% William,   56% Master,   0% other words  \n* PC4 is 67% William,   -59% Master,   0% other words  \n* PC5 is 96% John,   0% other words   \n* PC6 is 90% Henry,   0% other words    \n  \nTherefore when Mr. Owen Braund, has principal components weights (0.8, -0.1, -0.2, 0.2, 0.0, 0.2) that means he has 0.8 of Mr and nothing positive for Mrs, Miss, William, John, or Master. The second passenger, Mrs. John Cumings has weights (-0.25, 0.8, -0.15, 0.3, 1.1, -0.1). This says that she is mainly John and Mrs. The third passenger, Miss Laina Heikkinen has weights (-0.5, -0.6, -0.1, 0.3, 0.1, 0.0). When multiplied by the principal components, she becomes mostly Miss. The other 3 make sense too.\n\n## Model and cross validate\nWe will use the passengers' principal components to classify unknown passengers using kNN. First let's cross validate and perform a grid search to find the optimal dimension _d_ of principal components and optimal _k_ for kNN"},{"metadata":{"trusted":true,"_uuid":"b6becb679ed0a20c71f13960eccc53b99767ac0d","_kg_hide-input":true},"cell_type":"code","source":"library(caret)\naccuracy <- matrix(nrow=5,ncol=5)\nrownames(accuracy) <- paste('d=',3:7,sep='')\ncolnames(accuracy) <- paste('k=',c(7,9,11,13,15),sep='')\nfor (d in 1:5){\n    xt = (t(ev[[2]]))[1:(d+2),1:nrow(ev[[2]])] %*% t(data3)\n    dataPC = data.frame(Survived=data$Survived,t(xt))\n    for (k in 1:5){\n        trials = 100\n        total = 0\n        for (i in 1:trials){\n            s = sample(1:891,802)\n            s2 = (1:891)[-s]\n            model <- knn3(factor(Survived) ~ .,dataPC[s,],k=2*k+5)\n            p <- predict(model,newdata=dataPC[s2,])\n            p <- ifelse(p[,2]>=0.5,1,0)\n            # calculate one minus misclassification rate\n            x = 1-sum(abs(dataPC$Survived[s2]-p))/length(s2)\n            #if (i%%10==0) cat(sprintf(\"Trial %d has CV accuracy %f\\n\",i,x))\n            total = total + x\n    }\n        #cat(sprintf(\"For d=%d, k=%d, average CV accuracy of %d trials is %f\\n\"\n        #    ,d+2,2*k+5,trials,total/trials))\n    accuracy[d,k] <- total/trials\n    }\n}\ncat('Cross validation accuracy using 10-fold CV:\\n')\naccuracy","execution_count":10,"outputs":[]},{"metadata":{"_uuid":"7f6c43f8d455d8dbb88beca38637ea870e5003be"},"cell_type":"markdown","source":"From our search, we see that d=5 and k=11 are the best combination achieving CV = 80.5% which is better than the gender model's cross validation of 78.6%. (Results may vary due to the random natural of cross validation k-folds.) So, PCA is detecting more than just gender, cool. Let's use those hyperparameters to make a submission to Kaggle\n## Submission to Kaggle"},{"metadata":{"trusted":true,"_uuid":"e2ca2255816ef7d3c2c1e5481e44f91b537ccda1","_kg_hide-input":false},"cell_type":"code","source":"d=5; k=11;\nxt = (t(ev[[2]]))[1:d,1:nrow(ev[[2]])] %*% t(data3)\ndataPC = data.frame(Survived=data$Survived,t(xt))\nmodel <- knn3(factor(Survived) ~ .,dataPC[1:891,],k=k)\np <- predict(model,newdata=dataPC[892:1309,])\np <- ifelse(p[,2]>=0.5,1,0)\nsubmit = data.frame(PassengerId=892:1309,Survived=p)\nwrite.csv(submit,'PCA5kNN11.csv',row.names=F)","execution_count":19,"outputs":[]},{"metadata":{"_uuid":"caefc934de5217ffecb3bb4ecfdef822bab28135"},"cell_type":"markdown","source":"![result](http://playagricola.com/Kaggle/resultPCA5kNN9.png)\nThis isn't a high score but we did achieve the accuracy of the gender model by implemented an interesting and complex model. Let's explore our PCA a bit and then implement a variant of naive Bayes."},{"metadata":{"_uuid":"406a667e01b47bda85f6390dab8003e27a8199d0"},"cell_type":"markdown","source":"## PCA Exploration\nThe overall lack of correlation among Words with frequency 4 or larger means that our principal components weren't able to compress these 115 Words into a dimension much less than 115. Let's plot the first 12 principal components. And then let's try to compress the first 115 Words. It appears the next 298 = 413 - 115 Words have some correlation and will be able to be compressed. However that won't help much because we'd prefer to use the first 115 frequent Words and discard the next 298 anyway. We expect each principal component to represent more or less a single word. So the principal components should be mostly flat with a single spike."},{"metadata":{"trusted":true,"_uuid":"0b474af2da930dcf98be97f13f672544f845ce2a","_kg_hide-input":true},"cell_type":"code","source":"pimage <- function(x){\n    return(ggplot(data=ev2) + \n           geom_line(aes(x=1:length(ev2[[x+1]]),y=ev2[[x+1]])) + \n           labs(x='',y='',title=paste('PC',x,sep='')))\n}\nx=0\ngrid.arrange(pimage(x+1),pimage(x+2),pimage(x+3),pimage(x+4),pimage(x+5),pimage(x+6),\n    pimage(x+7),pimage(x+8),pimage(x+9),pimage(x+10),pimage(x+11),pimage(x+12),as.table=F)","execution_count":13,"outputs":[]},{"metadata":{"_uuid":"884877feef0c3bee5aeec01b09718ad94670eb14"},"cell_type":"markdown","source":"As expected, these principal components only involve a single word more or less because the most frequent Words aren't correlated with other Words. The less frequent Words did correlate with other Words, so as we plot more principal components, we expect them to involve multiple words. For example, here are principal components 100 thru 112."},{"metadata":{"trusted":true,"_uuid":"63c00e3e3c0de72e95235ec9f859532d43bcf77a","_kg_hide-input":true},"cell_type":"code","source":"x=99\ngrid.arrange(pimage(x+1),pimage(x+2),pimage(x+3),pimage(x+4),pimage(x+5),pimage(x+6),\n    pimage(x+7),pimage(x+8),pimage(x+9),pimage(x+10),pimage(x+11),pimage(x+12),as.table=F)","execution_count":40,"outputs":[]},{"metadata":{"_uuid":"cf294924c056b6a5689522082485110b26f3b38b"},"cell_type":"markdown","source":"We can test PCA's ability to compress Words by one-hot-encoding our entire dictionary and transforming it into PC space only using k principal components. Then we can transform it back into regular Word space and see how many Words we get back. It would be fantastic if we could transform 413 Words into PC space using 20 principal components and then transform the result back and still have something like 100 Words. That would be a 5 to 1 compression."},{"metadata":{"trusted":true,"_uuid":"679522d076a164c5ee3a1ba20c5813c2d627115d","_kg_hide-input":true},"cell_type":"code","source":"prep <- function(data,s){\n    x=c(); y=c(); z=c()\n    for (i in 1:nrow(data))\n    for (j in 1:ncol(data)){\n        x=c(x,s*j)\n        y=c(y,s*i)\n        z=c(z,data[i,j])\n    }\n    return (data.frame(x=x,y=y,z=z))\n}\n\nx=diag(nrow(data5))\ng1 = ggplot(prep( x[4*1:100,4*1:100] ,4),aes(x,y)) +\n geom_raster(aes(fill=z)) +\n ylim(400,0) +\n labs(title='Words in dictionary',x='',y='')\nd = 5\nx=diag(nrow(data5))\nxt = (t(ev[[2]]))[1:d,1:nrow(ev[[2]])] %*% x\nx2 = ev[[2]][1:nrow(ev[[2]]),1:d] %*% xt\ng2 = ggplot(prep( x2[1:10,1:10] ,1),aes(x,y)) +\n geom_raster(aes(fill=z)) +\n ylim(10,0) +\n labs(title='Words recreated from 5 PC',x='',y='')\nd=10\nxt = (t(ev[[2]]))[1:d,1:nrow(ev[[2]])] %*% x\nx2 = ev[[2]][1:nrow(ev[[2]]),1:d] %*% xt\ng3 = ggplot(prep( x2[1:20,1:20] ,1),aes(x,y)) +\n geom_raster(aes(fill=z)) +\n ylim(20,0) +\n labs(title='Words recreated from 10 PC',x='',y='')\nd=25\nxt = (t(ev[[2]]))[1:d,1:nrow(ev[[2]])] %*% x\nx2 = ev[[2]][1:nrow(ev[[2]]),1:d] %*% xt\ng4 = ggplot(prep( x2[1:50,1:50] ,1),aes(x,y)) +\n geom_raster(aes(fill=z)) +\n ylim(50,0) +\n labs(title='Words recreated from 25 PC',x='',y='')\ngrid.arrange(g1,g2,g3,g4,nrow=2,ncol=2)","execution_count":41,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"197ecde74f124ada9ac53cf885468bd5bdf3f64f","_kg_hide-input":true},"cell_type":"code","source":"d=50\nxt = (t(ev[[2]]))[1:d,1:nrow(ev[[2]])] %*% x\nx2 = ev[[2]][1:nrow(ev[[2]]),1:d] %*% xt\ng5 = ggplot(prep( x2[1:100,1:100] ,1),aes(x,y)) +\n geom_raster(aes(fill=z)) +\n ylim(100,0) +\n labs(title='Words recreated from 50 PC',x='',y='')\nd=100\nxt = (t(ev[[2]]))[1:d,1:nrow(ev[[2]])] %*% x\nx2 = ev[[2]][1:nrow(ev[[2]]),1:d] %*% xt\ng6 = ggplot(prep( x2[2*1:100,2*1:100] ,2),aes(x,y)) +\n geom_raster(aes(fill=z)) +\n ylim(200,0) +\n labs(title='Words recreated from 100 PC',x='',y='')\nd=200\nxt = (t(ev[[2]]))[1:d,1:nrow(ev[[2]])] %*% x\nx2 = ev[[2]][1:nrow(ev[[2]]),1:d] %*% xt\ng7 = ggplot(prep( x2[4*1:100,4*1:100] ,4),aes(x,y)) +\n geom_raster(aes(fill=z)) +\n ylim(400,0) +\n labs(title='Words recreated from 200 PC',x='',y='')\nd=300\nxt = (t(ev[[2]]))[1:d,1:nrow(ev[[2]])] %*% x\nx2 = ev[[2]][1:nrow(ev[[2]]),1:d] %*% xt\ng8 = ggplot(prep( x2[4*1:100,4*1:100] ,4),aes(x,y)) +\n geom_raster(aes(fill=z)) +\n ylim(400,0) +\n labs(title='Words recreated from 300 PC',x='',y='')\ngrid.arrange(g5,g6,g7,g8,nrow=2,ncol=2)","execution_count":42,"outputs":[]},{"metadata":{"_uuid":"20b9f855ff159ffcde281ebe7b258de02f37816b"},"cell_type":"markdown","source":"As expected, we don't see much compression. When we use 5 principal components, we can only recreate 6 Words from the dictionary. Then 10 PC recreates 11 Words. Then 25 recreates 25. At 50, we begin to see some compression power. Using 50 PC, we can represent more than 50 Words. Next 100, 200, and 300 can represent more Words than 100, 200, 300 too. It appears that 200 and 300 PC can basically represent the entire dictionary of 413 Words.\n\n# Naive Bayes variant (Oscar's SLogL)\n\nSince we only really want to use Words with freq>=4 which are the first 115 Words and PCA isn't doing a great job compressing those 115, let's leave the Words in their original space. Eventually we can perform a dimension reduction by discarding less frequent Words thus mimicking PCA. Since the Words are so uncorrelated, the Words themselves when ordered by frequency are basically equivilent to principal components. Let's plot each Word's predictive power."},{"metadata":{"trusted":true,"_uuid":"0b8cf76a7de59077388197f46722e186161b98fe","scrolled":true,"_kg_hide-input":true},"cell_type":"code","source":"# returns mode of a set of numbers\ngetmode <- function(v) {\n   uniqv <- unique(v)\n   uniqv[which.max(tabulate(match(v, uniqv)))]\n}\n\n# calculate average (mode) position of each word\nPosition <- vector(\"list\",length=nrow(dictionary))\nfor (i in 1:nrow(data)){\n    n = gsub('[.,\"()/]','',data$Name[i])\n    n = gsub(' . |  ',' ',n)\n    n = strsplit(n,' ')[[1]]\n    for (j in 1:length(n)){\n        k = which(dictionary$Words==n[j])[1]\n        if (!is.na(k)) Position[[k]] = c(Position[[k]],j)\n    }\n}\ndictionary$Position <- NULL\nfor (i in 1:nrow(dictionary)) dictionary$Position[i] <- getmode(Position[[i]])\ndictionary$Position <- pmin(4,dictionary$Position)\n\nggplot(data=dictionary) +\ngeom_jitter(height=0.005,aes(x=1:nrow(dictionary),y=dictionary$Survival,color=factor(dictionary$Position))) +\nscale_colour_manual(values = c(\"red\", \"black\", \"blue\",\"white\"),labels=c('1','2','3','4+')) +\ngeom_hline(yintercept=0.384, linetype='dotted') +\ngeom_vline(xintercept=45) +\ngeom_vline(xintercept=57) +\ngeom_vline(xintercept=80) +\ngeom_vline(xintercept=115) +\ngeom_vline(xintercept=187) +\ngeom_vline(xintercept=6) +\nannotate('text',x=-15,y=0.87,label='n<=6\\nfreq\\n>=35') +\nannotate('text',x=98,y=0.87,label='freq\\n>=4',color='gray70') +\n#annotate('text',x=68,y=0.87,label='freq\\n>=5',color='gray70') +\nannotate('text',x=25,y=0.87,label='freq\\n>=7',color='gray70') +\nannotate('text',x=143,y=0.87,label='n>=116\\nfreq<=3') +\nannotate('text',x=215,y=0.87,label='n>=188\\nfreq<=2') +\nannotate('text',x=305,y=0.36,label='p = 0.384 = probability of survival') +\nlabs(x='Words order by frequency') +\nlabs(title='Each dot represents a Word in the dictionary.') +\nlabs(y='Probability Survival given Word',color='word\\'s\\nposition\\nin name')","execution_count":43,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"01a1ec4e8edd68aa1e64fef489fbeb188a067425"},"cell_type":"markdown","source":"This plot is very interesting. The Words whose frequencies are 35 or higher are mostly black dots. The 6 most frequent words are contained in the first 5 principal components we saw above. These Words are Mr (518 occurences), Miss (182), Mrs (129), William (63), John (44), and Master (40). Our key shows us that black dots are Words that are found in the second postion in a Name. (We're pretending this is a foreign language, so I can't say what these are, but we know these to be titles.) Next the Words with frequency between 7 and 34 are blue dots. These are Words found in position 3 (first names). Next we have a mixture of colors which include the red dots (last names).\n\n## Model and cross validate\n\nLet's use a variant of naive Bayes presented by [Oscar Takeshita][2] called [Divide and Conquer][1] to predict passenger survival.\n\n$$P(y|w_1,w_2,...,w_n) = \\text{logit}^{-1}\\Bigg(  \\sum_{k=1}^n{  \\log \\bigg( \\frac{ P(y|w_k)}{ 1-P(y|w_k) } \\bigg)} +  \\sum_{k=1}^{n-1}{  \\log \\bigg( \\frac{ 1-P(y)}{ P(y) } \\bigg)}  \\Bigg)$$\n\n$$ \\text{where} \\quad \\text{logit}^{-1}(z) = \\frac{1}{1 + e^{-z}} \\quad \\text{and} \\quad P(y|w_k) = \\text{probability of survival given word } k $$\n[1]: https://www.kaggle.com/pliptor/divide-and-conquer-0-82296\n[2]:https://www.kaggle.com/pliptor"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"5d8e8c0361dcb981fa011bff0949f385b943e2ce"},"cell_type":"code","source":"# log odds\nlogl <- function(p){\n    e = 0.001\n    q = 1 - p\n    return(log(max(p,e))-log(max(q,e)))\n}\n# inverse logit\nilogit <- function(z){\n    return (1/(1+exp(-z)))\n}\n# cross validation trials\ntrials = 100\ntotal = 0\nfor (k in 1:trials){\nif (k%%25==0) cat(sprintf(\"Begin trial %d\\n completed\",k))\ns = sample(1:891,802)\ns2 = (1:891)[-s]\n\n# calculate the Tfreq and Survival within our training subset\ndictionary$Survival <- NA\ndictionary$Tfreq <- 0\nfor (i in 1:length(dictionary$Words)){\n    x = intersect(which(data2[,dictionary$Words[i]]>0),s)\n    dictionary$Survival[i] <- mean(data$Survive[x],na.rm=T)\n    dictionary$Tfreq[i] <- length(x)\n}\ndictionary$Survival[is.na(dictionary$Survival)] <- NA\ndictionary$Tfreq[is.na(dictionary$Tfreq)] <- NA\ndictionary$Logl <- sapply(dictionary$Survival,FUN=logl)\n\n# calculate bias term for Oscar's naive Bayes\nps = sum(data$Survived[s])/length(s)\nbias = logl(1-ps)\n\n# the following line mimics PCA dimension reduction\ndictionary2 <- dictionary[dictionary$Tfreq>=4,]\np = rep(0,891-length(s))\nfor (j in 1:length(s2)){\n    c = 0\n    slogl = 0\n    # perform Oscar's naive Bayes\n    for (i in 1:nrow(dictionary2)){\n        if (data2[s2[j],dictionary2$Words[i]]>0 & !is.na(dictionary2$Survival[i])){\n            slogl = slogl + dictionary2$Logl[i]\n            if (c>0) slogl = slogl + bias\n            c = 1\n        }\n    }\n    if (c!=0) p[j] = ilogit(slogl)\n    else p[j] = ps\n    if (k%%25==0 & j%%10==0) cat(sprintf(\" j=%d \",j))\n}\np <- ifelse(p>=0.5,1,0)\n# calculate one minus misclassification rate\nx = 1-sum(abs(data$Survived[s2]-p))/length(s2)\nif (k%%25==0) cat(sprintf(\"\\n Trial %d has CV accuracy %f\\n\",k,x))\ntotal = total + x\n}\n\ncat(sprintf(\"Average CV accuracy of %d trials is %f\\n\",trials,total/trials))\n","execution_count":45,"outputs":[]},{"metadata":{"_uuid":"10688262e3e5beacea51061ec3fc1c73f9240fce"},"cell_type":"markdown","source":"Cross validation shows that when our model uses the first 115 dictionary Words which are all the Words with frequency 4 or more, then we can expect prediction accuracy of 80%! That's better than the gender model that cross validates at 78.6%, so we're extracting gender and something more from these words.\n\nI also ran cross validation using the first 6, 23, 42, 57, 80, 115, 187, 413 Words, (frequency >= 35, 12, 7, 6, 5, 4, 3, 2), which achieved results of 79%, 78%, 78%, 79%, 79%, 80%, 80%, 80% respectively. What's interesting is when we transistion from using the first 6 \"principal components\" to using the next 34, our CV drops. Then our CV improves when we add more \"principal components\". This experimental result can be explained by the plot of Words above. The first 6 \"principal components\" are black dots (titles), the next 38 are blue dots (first names), and after that are red dots (last names). If we use every Word (approximately 600 Words) including those with frequency equal 1 (that were removed from our dictionary), then the CV drops to 76%.\n## Submission to Kaggle\n![resultNB](http://playagricola.com/Kaggle/resultNaiveBayes.png)\nAgain, not a high score. But it did achieve the gender model accuracy which is nice. Also, this kernel serves as a template to build upon in the future.\n# Conclusion\nCross validation indicates that the black dots and red dots contain the most predictive power. And the blue dots hurt our CV. I checked and our CV increases to 80.5% by removing the blue and white dots. But we still aren't close to the Name only kernel posted [here][1] which has an 84% CV. Therefore there is still room to improve our model. In the future, I plan to use all the insights gained from the above investigations to improve PCA and naive Bayes operating on the words within the names.\n\nThank you everyone for reading my notebook. I look forward to your comments.\n\n# References\n\n* [Oscar Takeshita][2]: Postulates this interesting problem and provides a solution using PCA and kNN\n* [Chris Deotte][3]: Provides an example of PCA\n* [Chris Deotte][4]: Provides an example of SVD compression\n* [Cathy O'Neil & Rachel Schutt][5]: Doing Data Science explains naive Bayes\n* [Oscar Takeshita][6]: Presents a variant of naive Bayes called SLogL in his kernel Divide and Conquer\n\n[1]: https://www.kaggle.com/cdeotte/titanic-using-name-only-0-81818\n[2]: https://www.kaggle.com/pliptor/name-only-study-with-interactive-3d-plot\n[3]:http://www.ccom.ucsd.edu/~cdeotte/papers/PrincipleComponents.pdf\n[4]:http://www.ccom.ucsd.edu/~cdeotte/papers/SVD.pdf\n[5]: https://www.amazon.com/Doing-Data-Science-Straight-Frontline/dp/1449358659\n[6]:https://www.kaggle.com/pliptor/divide-and-conquer-0-82296\n"}],"metadata":{"kernelspec":{"display_name":"R","language":"R","name":"ir"},"language_info":{"mimetype":"text/x-r-source","name":"R","pygments_lexer":"r","version":"3.4.2","file_extension":".r","codemirror_mode":"r"}},"nbformat":4,"nbformat_minor":1}