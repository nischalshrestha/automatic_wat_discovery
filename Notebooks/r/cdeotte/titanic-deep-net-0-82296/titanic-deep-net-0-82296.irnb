{"cells":[{"metadata":{"_uuid":"c04a533ed6bed487ad2c95b8c77680abf9569386","_execution_state":"idle","trusted":false},"cell_type":"markdown","source":"# Titanic Deep Net\nI've been working on classifying MNIST hand written digits. My latest MNIST kernel is posted [here](https://www.kaggle.com/cdeotte/mnist-fully-connected-98-5), check it out. For that competition, I programmed a neural network in C. I thought it would be fun and a little silly to apply deep learning to the Titanic dataset. Let's give it a go!\n![deepnet](http://playagricola.com/Kaggle/deepNet.jpg)"},{"metadata":{"_uuid":"cffe87cab525cd1346e38bc4c19ed0d5aab2919b"},"cell_type":"markdown","source":"# Engineer Features\nFirst let's create some predictive scaled features from the Titanic training dataset in R and export a CSV for C. I believe the most important information to be Sex, Age (mainly to identify children), Family Size, FareAdj (mainly to identify Pclass), and family group survival information. We'll divide each by their max to get a feature value between 0 and 1."},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":false,"trusted":true,"_uuid":"d2f0aba413a82afc826136e1f3ae5ddc042d192a"},"cell_type":"code","source":"train <- read.csv('../input/train.csv',stringsAsFactors=F)\ntest <- read.csv('../input/test.csv',stringsAsFactors=F)\ntest$Survived <- NA; allData <- rbind(train,test)\nallData$TicketFreq <- ave(1:1309,allData$Ticket,FUN=length)\ntrain$Title <- substring(train$Name,regexpr(',',train$Name)+2,regexpr('[.]',train$Name)-1)\ntrain$Surname <- substring(train$Name,0,regexpr(',',train$Name)-1)\ntest$Title <- substring(test$Name,regexpr(',',test$Name)+2,regexpr('[.]',test$Name)-1)\ntest$Surname <- substring(test$Name,0,regexpr(',',test$Name)-1)\ntest$Fare[is.na(test$Fare)] = mean(allData$Fare[allData$Pclass==3],na.rm=T)\nlibrary(rpart)\nageFit <- rpart(Age ~ Title + Pclass + Sex + SibSp + Parch + Fare,train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"734a96a40d3e6262335059cf74d2928604321782"},"cell_type":"code","source":"# RESPONSE: SURVIVED\ntrainData <- data.frame(Survived=train$Survived)\n# FEATURE 1: GENDER\ntrainData$Sex <- 0\ntrainData$Sex[train$Sex=='female'] <- 1\n# FEATURE 2: AGE\ntrain$Age[is.na(train$Age)] <- predict(ageFit,train[is.na(train$Age),])\ntrainData$Age <- train$Age/80\n# FEATURE 3: FAMILY SIZE\ntrainData$FamilySize <- (train$SibSp + train$Parch + 1)/11\n# FEATURE 4: FARE ADJUSTED\ntrainData$FareAdj <- train$Fare/allData$TicketFreq[1:891]/222\n# FEATURE 5: DID ONE FAMILY MEMBER SURVIVE?\ntrainData$FamilyOneSurvived <- 0\n# FEATURE 6: DID ALL FAMILY MEMBERS DIE?\ntrainData$FamilyAllDied <- 0\nfor (i in 1:891){\n    x = which(train$Surname==train$Surname[i] & train$PassengerId!=i)\n    m <- mean(train$Survived[x])\n    if (!is.na(m) & m>0) trainData$FamilyOneSurvived[i] <- 1\n    if (!is.na(m) & m==0) trainData$FamilyAllDied[i] <- 1\n}\nwrite.csv(trainData,'trainData.csv',row.names=F)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cf024c7e63a518b8211380801fcb7ce7eb35551a"},"cell_type":"markdown","source":"Below are the first 10 training vectors. You will notice that all the feature values are between 0 and 1 since we divided each feature by its max value. This will help our deep network learn."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"1e3a3b7de304911a659c2ba8b6e3afacbe23c12e"},"cell_type":"code","source":"head(trainData,10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a88773f7b437d4e614c4dab9df75591e888b794e"},"cell_type":"markdown","source":"# Deep Neural Network in C\nNext we'll load these features into my neural network C program. (Learn more about my program [here](https://www.kaggle.com/cdeotte/mnist-fully-connected-98-5) )\n![load](http://playagricola.com/Kaggle/TitanicLoad.png)"},{"metadata":{"_uuid":"9f44be71f0d19951a94c9320edd430fa0f1fc445"},"cell_type":"markdown","source":"Let's create a deep neural network of 10 layers. We'll give each of the 8 hidden layers 10 neurons. For a deep net, ReLU activation is the best for the hidden layers. Since this is a classification problem, we'll use sigmoid for the output neuron. Let's use cost function equal to cross entropy. (Logistic regression) We'll apply a 10% dropout to all hidden neurons because this network is much too big, lol. We'll set the learning rate to 0.001. We'll use 75% (=681) of the training passengers to train with and we'll withhold 25% (=210) to validate our model's accuracy.\n## Training\n![EntAcc](http://playagricola.com/Kaggle/EntAcc.png)"},{"metadata":{"_uuid":"c39bdd41424ba73acbea9dd1c9ed673b49929c33"},"cell_type":"markdown","source":"The blue line is validation performance and the red line is training set performance. You can see that we have validation accuracy of 84.5%. It appears to have learned the training set to 83%. But that number is lower than what it really is. Neurons are being dropped out when it classifies a training passenger but not dropped out when classifying a validation set passenger. Therefore if we classified the training set without dropout, it should be around 84.5% also.\n## Prediction\nNext we'll export Kaggle's Titanic test dataset from R and import that into C. Then make predictions and submit them to Kaggle."},{"metadata":{"trusted":true,"_uuid":"5e1032b3b4f20a154d5712ee1ceecd9f0c5c3434","_kg_hide-input":true},"cell_type":"code","source":"# FEATURE 1: GENDER\ntestData <- data.frame(Sex=rep(0,418))\ntestData$Sex[test$Sex=='female'] <- 1\n# FEATURE 2: AGE\ntest$Age[is.na(test$Age)] <- predict(ageFit,test[is.na(test$Age),])\ntestData$Age <- test$Age/80\n# FEATURE 3: FAMILY SIZE\ntestData$FamilySize <- (test$SibSp + test$Parch + 1)/11\n# FEATURE 4: FARE ADJUSTED\ntest$FareAdj <- test$Fare/allData$TicketFreq[892:1309]\ntestData$FareAdj <- test$Fare/allData$TicketFreq[892:1309]/222\n# FEATURE 5: DID ONE FAMILY MEMBER SURVIVE?\ntestData$FamilyOneSurvived <- 0\n# FEATURE 6: DID ALL FAMILY MEMBERS DIE?\ntestData$FamilyAllDied <- 0\nfor (i in 1:418){\n    x = which(train$Surname==test$Surname[i])\n    m <- mean(train$Survived[x])\n    if (!is.na(m) & m>0) testData$FamilyOneSurvived[i] <- 1\n    if (!is.na(m) & m==0) testData$FamilyAllDied[i] <- 1\n}\ntest$OneSurvived <- testData$FamilyOneSurvived\ntest$AllDied <- testData$FamilyAllDied\nwrite.csv(testData,'testData.csv',row.names=F)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ab40bbaa4dad957970f8a627912af76ece155192"},"cell_type":"markdown","source":"![resultNN2](http://playagricola.com/Kaggle/NNresult.png)\nCool, this submission scores 82%. Not bad for an excessively deep neural network. The following male passengers were predicted to live: 913, 972, 981, 1053, 1086, 1088, 1093, 1173, 1199, 1223, 1236, 1284, 1309. The following female passengers were predicted to die: 910, 925, 929, 1017, 1024, 1032, 1045, 1051, 1080, 1114, 1155, 1165, 1172, 1176, 1196, 1251, 1257, 1268, 1274, 1275. All other males were predicted to die. All other females were predicted to live. Now let's use an R package and create a simple neural network.\n# R's Neural Net Package\nUsing R's neural net package, we'll create a 3 layer network. Our 6 input neurons will feed into 5 hidden neurons which will feed into 1 output neuron. (5 hidden neurons were chosen because that cross validates the best as shown below.) We'll use neuralnet's default settings which are sigmoid activation throughout and the sum of squared errors cost function.\n## Training"},{"metadata":{"trusted":true,"_uuid":"c1ae32c835bd2d9a83d8c05581fae593c1ff9fca"},"cell_type":"code","source":"library(neuralnet)\nf <- as.formula('Survived ~ Sex + Age + FamilySize + FareAdj + FamilyOneSurvived + FamilyAllDied')\nset.seed(8)\nfit <- neuralnet(f,trainData,hidden=5,linear.output=F)\nplot(fit)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7a92707557c5b05ba12e48b4e7e80ae22e5aad1a"},"cell_type":"markdown","source":"![NNpic](http://playagricola.com/Kaggle/NNpic.png)\nAbove is a plot of our simple neural network and associated weights and biases. Next we'll classify the test dataset and submit the results to Kaggle. We'll also display the names of all the males that are predicted to live and all the females that are predicted to die.\n## Prediction"},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"e65924eb708be113a6af0d20d025afdeeac512f5"},"cell_type":"code","source":"p <- compute(fit,testData); p <- ifelse(p[[2]]>0.5,1,0)\nsubmit <- data.frame(PassengerId=892:1309,Survived=p)\nwrite.csv(submit,'TitanicDeepNet.csv',row.names=F)\nx = which( (test$Sex=='male' & p==1) | (test$Sex=='female' & p==0) )\nrow.names(test) <- 892:1309; test[x,c('Name','Sex','Age','SibSp','Parch','FareAdj','OneSurvived','AllDied')]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"21428acf164347e00e1ebbcba057bb5c89cb9ca7"},"cell_type":"markdown","source":"![resultNN2](http://playagricola.com/Kaggle/NNresult2.png)\nWoohoo! Our Titanic neural network scored 82.3%. You'll notice that it predicted all adult males perish. This is a smart neural network."},{"metadata":{"_uuid":"ee6c65c88d7fde3fa7f0a38acdf93deb6eed5002"},"cell_type":"markdown","source":"## Cross Validation\nI used grid search to find the optimal network architecture. Using 2, 3, 4, 5, 6 hidden neurons has average cross validation of 82.3%, 82.3%, 81.9%, 82.7%, 81.6% respectively. Below is the average 5-fold cross validation for our network which uses 5 hidden neurons."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"73fc621fd8b040043ec05dfdf526857adf784e40"},"cell_type":"code","source":"set.seed(1)\nsum = 0\ntrials=10\ncat(sprintf(\"Beginning %d trials of CV\\n\",trials))\nfor (i in 1:trials){\n    s = sample(1:891,180)\n    fit <- neuralnet(f,trainData[-s,],hidden=5,linear.output=F)\n    p <- compute(fit,trainData[s,-1])\n    p <- ifelse(p[[2]]>0.5,1,0)\n    c = sum(abs(p-trainData$Survived[s]))\n    cat(sprintf(\"  i=%d acc=%f\\n\",i,1-c/180))\n    sum = sum + c\n}\ncat(sprintf(\"Average CV over %d trials = %f\",trials,1-sum/(180*trials)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fc00540cb2f1fe00440d31c30312caa67eb96c63"},"cell_type":"markdown","source":"Because a neural network's weights are randomly initialized, each time you train your net you get a different final result even with the same training data. If you change `set.seed(8)` in the code above before training then you'll get different network weights which make different predictions. What's interesting is that half the time, the network will predict that all adult males die and half the time it will predict a dozen survive. The females it predicts to die and boys predicted to live stay similar each time.\n```\n          adult             boy   public      cross\nset.seed   male   female   male    score   validation\n       1      0       26      9     81.8       82.7\n       2     13       24     10                82.7\n       3     17       23      9                82.7\n       4     12       23     12                82.7\n       5     25       18     12                82.7\n       6      0       23     12     80.8       82.7\n       7     16       25     12                82.7\n       8      0       21     13     82.3       82.7\n       9      0       23     11     80.3       82.7\n      10      4       17      8                82.7\n   ```\nThis seems like a lot of variability especially because the public scores when males are predicted to live will be lower than 80%. However when I cross validate, the accuracy always stays over 81% and averages 82.7%. This is because there are patterns of adult male survival in Kaggle's training dataset. However it was shown in [previous kernels](https://www.kaggle.com/cdeotte/titanic-wcg-xgboost-0-84688) that these patterns don't carry over to Kaggle's test dataset. Therefore as a final submission, I chose one of the above neural networks that predicted all adult males die, i.e. `set.seed = 1, 6, 8, or 9`.  Or if you prefer a more machine learning approach without human choice, you could create a majority vote ensemble from the 10 set.seed neural networks. The ensemble would predict all adult males die since no adult male was predicted to live with a majority (6 or more) of votes. If you create an ensemble using predictions with 9 or more votes, you'll score 81.8%. And 10 or more votes, you'll score 81.8%.\n![votes](http://playagricola.com/Kaggle/NNvotes.png). "},{"metadata":{"_uuid":"0e091ff22a72a15d48e4fbf872be47e5c93a34e2"},"cell_type":"markdown","source":"# Conclusion\nOur deep network with 10 layers and 70 hidden neurons was a bit silly and excessive. Most applications including predicting passenger survival do not require such a deep network. Our deep network performed well though. If we tune it and apply appropriate regularization, I'm sure we could get it to match our smaller network which scored 82.3%. Eventually I'll upload my C neural network program to GitHub so others can play with it. In the meanwhile, you can fork this R code and use R's neuralnet package. By setting the hyperparameter \"hidden\" to a numeric vector, you can add more than 1 hidden layer.\n\nMaking a smaller 3 layer network in R allowed us to use R's functionality to analyze it. It was interesting to observe how half the time the network predicted some adult males survive and half the time it didn't. I wonder why that is. I wonder how we can change the features so that it predicts all adult males die every time. However that may not be possible since there are patterns of adult males survival in the training dataset as shown in my [XGBoost kernel here](https://www.kaggle.com/cdeotte/titanic-wcg-xgboost-0-84688).\n\nNeural networks are fun to play with. I don't think it's the best model for Titanic prediction but it does well none-the-less. Nets excel at image classification. If image classfication interests you, check out my MNIST image recognition kernel [here](https://www.kaggle.com/cdeotte/mnist-fully-connected-98-5). Thanks for reading my kernel."},{"metadata":{"_uuid":"9165c99bf95b8cbd9a9b3e0b6c1d241e0f095c1a"},"cell_type":"markdown","source":"# More Neural Network Examples on Kaggle\nIf you wish to learn more about Titanic neural networks, below are more examples from Kaggle:\n* [Leonardo Ferreira][1] - [Titanic [EDA] + Keras Neural Networks - Guide][2] - 88 upvotes  \n* [Li-Yen Hsu][3] - [Titanic - Neural Network][4] - 41 upvotes - scores 81.3%\n* [sdufour][5] - [Trying neural network][6] - 17 upvotes - scores 80.9%  \n* [nmei-42][7] - [Titanic with Tensorflow][8] -  7 upvotes - scores 80.9%  \n* [Chris Deotte][23] - [Title and Surname only Neural Network][24] - 1 upvote - scores 80.4%\n* [Rafael Vicente Leite][9] - [Titanic - Artificial Neural Network][10] - 6 upvotes - scores 79.9%  \n* [DavidCoxon][11] - [Deeply Titanic][12] - 12 upvotes - scores 77%\n* [Alan Wong][15] - [Titanic: Neural Network using Keras][16] - 5 upvotes - scores 76.5%\n* [Lucas Silveira][13] - [Titanic: a simple beginner's approach][14] - 10 upvotes - scores 76%\n* [Khoo Chen Shiang][19] - [Deep Learning with Keras and TensorFlow][20] - 7 upvotes - scores 74.6%\n* [Siuming][21] = [Titanic with tensorflow][22] - 5 upvotes - scores 74%\n* [Damien Park][17] - [Artificial Neural Network using Keras][18] - 7 upvotes - scores 72.8%\n\nIf anyone knows of other helpful examples, let me know and I'll add them to the list. Thanks.\n\n[1]: https://www.kaggle.com/kabure\n[2]:https://www.kaggle.com/kabure/titanic-eda-keras-neural-networks-guide\n[3]:https://www.kaggle.com/liyenhsu\n[4]:https://www.kaggle.com/liyenhsu/titanic-neural-network\n[5]:https://www.kaggle.com/sdufour\n[6]:https://www.kaggle.com/sdufour/trying-neural-network\n[7]:https://www.kaggle.com/njmei42\n[8]:https://www.kaggle.com/njmei42/kaggle-titanic-with-tensorflow\n[9]:https://www.kaggle.com/rafaelvleite\n[10]:https://www.kaggle.com/rafaelvleite/titanic-artificial-neural-network-80-score\n[11]:https://www.kaggle.com/davidcoxon\n[12]:https://www.kaggle.com/davidcoxon/deeply-titanic\n[13]:https://www.kaggle.com/lssilveira11\n[14]:https://www.kaggle.com/lssilveira11/titanic-a-simple-beginner-s-approach\n[15]:https://www.kaggle.com/alan1229\n[16]:https://www.kaggle.com/alan1229/titanic-neural-network-using-keras\n[17]:https://www.kaggle.com/damienpark\n[18]:https://www.kaggle.com/damienpark/artificial-neural-network-using-keras\n[19]:https://www.kaggle.com/jameskhoo\n[20]:https://www.kaggle.com/jameskhoo/deep-learning-with-keras-and-tensorflow\n[21]:https://www.kaggle.com/wsm1992\n[22]:https://www.kaggle.com/wsm1992/titanic-with-tensorflow\n[23]:https://www.kaggle.com/cdeotte\n[24]:https://www.kaggle.com/c/titanic/discussion/60452"}],"metadata":{"kernelspec":{"display_name":"R","language":"R","name":"ir"},"language_info":{"mimetype":"text/x-r-source","name":"R","pygments_lexer":"r","version":"3.4.2","file_extension":".r","codemirror_mode":"r"}},"nbformat":4,"nbformat_minor":1}