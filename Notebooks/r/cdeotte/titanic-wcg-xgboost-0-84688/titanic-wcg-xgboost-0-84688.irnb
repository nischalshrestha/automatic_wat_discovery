{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"markdown","source":"# Do any adult males survive? XGBoost will tell us!\nThere are 245 adult males in the Titanic test dataset. By reviewing all the top models on Kaggle, we see that everyone predicts that all adult males die. Is it possible to find at least 1 adult male that lived? If we can correctly predict the survival of any adult males, we can combine those predictions with a previously top scoring model, such as the WCG model, and have the most accurate Titanic model ever!   \n![tree12](http://playagricola.com/Kaggle/tree12.jpg)  \n  \nThe WCG (woman-child-group) model (displayed in yellow above) was first described [here][1] and improved upon [here][2]. It was first implemented using only Name and scored 82%. Adding Ticket, Fare, and Pclass increased it's score to 83%. Then the [Mega Model][3] showed that there are patterns in the non-WCG passengers. This kernel finds those patterns and scores 85%.\n![tree12](http://playagricola.com/Kaggle/xgb.jpg)\n## Explore adult males survival with XGBoost\nFirst we'll load in the Titanic dataset and impute missing Age and Fare. Next we'll engineer two new variables, x1 = Fare / (Ticket Frequency * 10), and x2 = Parch + SibSp + 1 + (Age / 70). Reducing the feature space to 2 dimensions allows us to visualize everything.  \n  \n[1]:https://www.kaggle.com/cdeotte/titanic-using-name-only-0-81818\n[2]:https://www.kaggle.com/cdeotte/titantic-mega-model-0-84210\n[3]:https://www.kaggle.com/cdeotte/titantic-mega-model-0-84210"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"train <- read.csv('../input/titanic/train.csv',stringsAsFactors=F)\ntest <- read.csv('../input/titanic/test.csv',stringsAsFactors=F)\ntest$Survived <- NA; data <- rbind(train,test)\n# engineer person type\ndata$Title <- 'man'\ndata$Title[grep('Master',data$Name)] <- 'boy'\ndata$Title[data$Sex=='female'] <- 'woman'\n# impute missing Age and Fare\nlibrary(rpart)\nfit <- rpart(Age ~ Title + Pclass + SibSp + Parch,data=data)\ndata$Age[is.na(data$Age)] <- predict(fit,newdata=data[is.na(data$Age),])\nfit <- rpart(Fare ~ Title + Pclass + Embarked + Sex + Age,data=data)\ndata$Fare[is.na(data$Fare)] <- predict(fit,newdata=data[is.na(data$Fare),])\n# engineer features\ndata$TicketFreq <- ave(1:1309,data$Ticket,FUN=length)\ndata$FareAdj <- data$Fare / data$TicketFreq\ndata$FamilySize <- data$SibSp + data$Parch + 1\n# isolate training set males\ndata2 <- data[data$PassengerId<=891 & data$Title=='man',]\n# engineer 2 features from Fare, Ticket, SibSp, Parch, and Age\ndata3 <- data.frame(\n    y=data2$Survived,\n    x1=data2$Fare / (data2$TicketFreq * 10),\n    x2=(data2$SibSp + data2$Parch + 1) + (data2$Age / 70),\n    Pclass=data2$Pclass)\n# plot engineered features\nlibrary(ggplot2)\nlibrary(gridExtra)\ng1 = ggplot(data[data$FareAdj>0 & data$FareAdj<40,]) + \n    geom_density(aes(x=FareAdj,fill=factor(Pclass)),alpha=0.9,show.legend=F) +\n    labs(fill='Pclass') + geom_vline(xintercept=c(10,20),linetype='dotted') +\n    xlim(0,40) + labs(title='All Passengers',x='x1 = Fare / Ticket Frequency')\ng2 = ggplot(data[data$Fare>0 & data$Fare<40 ,]) +\n    geom_density(aes(x=Fare,fill=factor(Pclass)),alpha=0.9) +\n    xlim(0,40) + labs(title='All Passengers',fill='Pclass',y='')\ng3 = ggplot(data[!is.na(data$Survived) & data$Title=='man' & data$Pclass==1 & data$FamilySize<4,]) + \n    geom_density(aes(x=FamilySize+Age/70,fill=factor(Survived)),alpha=0.9,bw=0.04) +\n    labs(fill='Survived') +\n    annotate('text',x=1.4,y=2.4,label='Age 30\\nFS=1') +\n    annotate('text',x=2.4,y=2.4,label='Age 30\\nFS=2') +\n    geom_vline(xintercept=c(1.43,2.43),linetype='dotted') +\n    xlim(1,4) + labs(title='Adult Males Pclass=1',x='x2 = FamilySize + Age/70')\ngrid.arrange(g3,g1,g2,layout_matrix = rbind(c(1, 1), c(2, 3)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dc15771e73f98220774a5d29adc5604f251a5777"},"cell_type":"markdown","source":"By creating 'x2' = FamilySize + Age/70, we can view SibSp, Parch, and Age on 1 dimension. The plot above shows that adult males with FamilySize<=2 in Pclass=1 between the ages 20 and 36 have greater than 50% chance of survival. (To be exact, 56% = 18 / 32 survived in the training set). Also males in their 40's with FamilySize=2 have 63% (= 5 / 8 ) chance of survival. To create 'x1', we divide Fare by TicketFrequency because that is the actual price paid. The 11 Sage family members did not pay \\$69.55 for each of their 11 third class tickets. They must have paid \\$6.32 = 69.55 / 11. Notice above how the 3 Pclasses become separated once you adjust the Fare. Lastly, we divide 'x1' by 10 so that its scale matches the scale of 'x2' which helps certain machine learning algorithms. Let's apply XGBoost to the pair of variables (x1, x2) and see what patterns it discovers."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"0e98a9609bd97bcd7b0c856f4ddf1274a2192526"},"cell_type":"code","source":"# classify males with xgboost\nlibrary(xgboost)\nx1s <- seq(0, 5, length.out = 100)\nx2s <- seq(1, 3, length.out = 100)\ng <- data.frame(x1 = rep(x1s, each=100), x2 = rep(x2s, time = 100))\nparam <- list(objective   = \"binary:logistic\",\n            eval_metric = \"error\",\n            max_depth   = 5,\n            eta         = 0.1,\n            gammma      = 0.1,\n            colsample_bytree = 1,\n            min_child_weight = 1)\ncat('XGBoosting begun...\\n')\nxgb <- xgboost(params  = param,\n            data    = data.matrix(data3[,c('x1','x2')]),\n            label   = data3$y, \n            nrounds = 500,\n            print_every_n = 100,\n            verbose = 1)\np <- predict(xgb,newdata=data.matrix(g[,c('x1','x2')]))\ng$y <- ifelse(p>0.5,1,0)\n# plot classification regions\nggplot(data=data3[data3$x1<5 & data3$x2<3,]) +\n    xlim(0,5) + ylim(1,3) +\n    geom_tile(data=g,aes(x1,x2,fill=factor(y))) +\n    geom_density_2d(aes(x1,x2,color=factor(y))) +\n    geom_point(size=2,aes(x1,x2,color=factor(y),shape=factor(Pclass))) +\n    scale_color_manual(values=c('#AA0000','#00AA00'),\n        limits=c('0','1'),labels=c('0','1')) +\n    scale_fill_manual(values=c('#FF9999','#99FF99'),\n        limits=c('0','1'),labels=c('0','1')) +\n    labs(x='Fare / (10 x TicketFrequency)',y='FamilySize + (Age / 70)',shape='Pclass',fill='Classify',\n        title='XGBoost learns the training set\\'s\n        537 adult males. Green is P(live)>0.5',color='Survived') +\n    geom_vline(xintercept=2.8, linetype='dotted') +\n    geom_hline(yintercept=c(1.43,2.43), linetype='dotted') +\n    annotate('text',x=2.95,y=2.9,label='Fare = $28') +\n    annotate('text',x=4.7,y=2.35,label='Age = 30') +\n    annotate('text',x=4.7,y=1.35,label='Age = 30')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bd3684494c36610f3938364789630f447bc2e5aa"},"cell_type":"markdown","source":"Cool. It looks like XGBoost found patterns of survival among the training dataset's adult males! From the plot above, we see that the most probable surviving adult males have Pclass = 1, FareAdj = \\$28, SibSp + Parch <= 1, and Age = 30. The shading indicates what XGBoost's classifying decisions are. The green region corresponds with XGBoost predicting P(live)>0.5."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"3ee8eb7bae87a79ae0a2e5c4406682d8c45c987b"},"cell_type":"code","source":"set.seed(300)\ns = sample(1:537,100)\ns2 = (1:537)[-s]\nxgb <- xgboost(params  = param,\n            data    = data.matrix(data3[s2,c('x1','x2')]),\n            label   = data3$y[s2], \n            nrounds = 500,\n            print_every_n = 100,\n            silent = 1,\n            verbose = 0)\np <- predict(xgb,newdata=data.matrix(data3[s,c('x1','x2')]))\ndata3$y2 <- NA\nroc = data.frame(TN=rep(0,102),FP=rep(0,102),FN=rep(0,102)\n    ,TP=rep(0,102),FPR=rep(0,102),TPR=rep(0,102))\nrownames(roc) <- seq(0,1.01,0.01)\nfor (i in 1:102){\n    data3$y2[s] <- ifelse(p<(i-1)/100,0,1)\n    roc$TP[i] <- length(which(data3$y==1 & data3$y2==1))\n    roc$TN[i] <- length(which(data3$y==0 & data3$y2==0))\n    roc$FP[i] <- length(which(data3$y==0 & data3$y2==1))\n    roc$FN[i] <- length(which(data3$y==1 & data3$y2==0))\n    roc$FPR[i] <- roc$FP[i] / (roc$FP[i] + roc$TN[i])\n    roc$TPR[i] <- roc$TP[i] / (roc$TP[i] + roc$FN[i])\n}\ndiag <- data.frame(x=c(0,1),y=c(0,1))\nggplot(roc,aes(x=FPR,y=TPR)) + \n    geom_line() + labs(title='ROC curve where \"positive\" = male survives',\n        x='False positive rate',y='True positive rate') +\n    geom_point(data=roc[91,],aes(x=FPR,y=TPR),size=3,color='red') +\n    annotate('text',x=0.13,y=0.12,label='threshold p>=0.9',color='red') +\n    geom_point(data=roc[76,],aes(x=FPR,y=TPR),size=3,color='darkgreen') +\n    annotate('text',x=0.16,y=0.30,label='threshold p>=0.75',color='darkgreen') +\n    geom_point(data=roc[51,],aes(x=FPR,y=TPR),size=3,color='blue') +\n    annotate('text',x=0.20,y=0.5,label='threshold p>=0.5',color='blue') +\n    geom_line(data=diag,aes(x=x,y=y),linetype='dotted')\narea = 0\nfor(i in 1:101){\n    area = area + roc$TPR[i] * (roc$FPR[i]-roc$FPR[i+1])\n}\ncat(sprintf('Area under ROC = %f\\n',area))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3345019f7c301ae5ed7cfa7540310ecc524c69a3"},"cell_type":"markdown","source":"The AUC, area under the ROC, equals 0.82 > 0.50. (Running 100 trials has average AUC = 0.70) That means that XGBoost has found patterns of survival, hooray! The ROC plot also shows us that If we predict survive whenever XGBoost says P(live)>0.5, then we will make many Type 1 errors (i.e. we will incorrectly predict adult males survive when they don't). We need to carefully select our threshold to maximize our accuracy, so we will use grid search to find the best threshold among 0.50, 0.75, 0.90, and 0.92.\n## Grid search and cross validate\nUsing grid search, we find the optimal parameters for XGBoost to be treeDepth=5, p>=0.90. (If you wish to perform a grid search, uncomment the for loops in the code below and add hyperparamters that you wish to search. In this notebook, I'm not doing the grid search because it takes a while. I did it offline). Now let's perform cross validation to see to see if XGBoost is more accurate than the gender model which has CV = 78.6%."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"d8c6c46d68f88e47351ad0b64c2bdb8686625ba1"},"cell_type":"code","source":"#accuracy = matrix(nrow=4,ncol=4)\n#rownames(accuracy) <- c('d=3','d=4','d=5','d=6')\n#colnames(accuracy) <- c('0.50','0.75','0.90','0.92')\n#for (j in 1:4)\n#for (k in 1:4){\nset.seed(2)\ntrials=100; sum=0\nfor (i in 1:trials){\n    s = sample(1:891,800)\n    s2 = (1:891)[-s]\n    dataB <- data[data$PassengerId %in% s & data$Title=='man',]\n    dataC <- data[data$PassengerId %in% s2 & data$Title=='man',]\n    data$Predict <- 0\n    data$Predict[data$Sex=='female'] <- 1\n    dataTrain <- data.frame(y=dataB$Survived,x1=dataB$FareAdj/10,x2=dataB$FamilySize+dataB$Age/70)\n    dataTest <- data.frame(y=dataC$Survived,x1=dataC$FareAdj/10\n        ,x2=dataC$FamilySize+dataC$Age/70,PassengerId=dataC$PassengerId)\n    param <- list(objective   = \"binary:logistic\",\n              eval_metric = \"error\",\n              max_depth   = 5,\n              eta         = 0.1,\n              gammma      = 0.1,\n              colsample_bytree = 1,\n              min_child_weight = 1)\n    xgb <- xgboost(params  = param,\n              data    = data.matrix(dataTrain[,c('x1','x2')]),\n              label   = dataTrain$y, \n              nrounds = 500,\n              silent = 1,\n              print_every_n = 100,\n              verbose = 0)\n    p <- predict(xgb,newdata=data.matrix(dataTest[,c('x1','x2')]))\n    dataTest$p <- ifelse(p>=0.90,1,0)\n    data$Predict[dataTest$PassengerId] <- dataTest$p \n    sm = sum(abs(data$Predict[s2] - data$Survived[s2]))\n    cv = 1 - sm/length(s2)\n    #if (i %% 25==0) \n    #cat(sprintf('Trial %d has CV = %f\\n',i,cv))\n    sum = sum + cv\n}\ncat(sprintf('Average CV of %d trials = %f\\n',trials,sum/trials))\n#accuracy[j,k] <- sum/trials\n#}\n#accuracy","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"699fa387f05fb88dc228bc048f4c40b0450202b3"},"cell_type":"markdown","source":"Our CV accuracy is 79.0%. (Results shown above may vary due to the random nature of k-fold CV but I confirmed 79.0% with 10,000 trials offline). The gender model has a CV accuracy of 78.6%, therefore we have achieved a slight improvement. Hopefully, this will allow us to find at least 1 surviving adult male in the test dataset!\n## Submission to Kaggle"},{"metadata":{"_kg_hide-input":true,"trusted":true,"scrolled":false,"_uuid":"d5aa5fbf16ce8dfb18aa4a57af50a41d375ca373"},"cell_type":"code","source":"dataB <- data[data$PassengerId %in% 1:891 & data$Title=='man',]\ndataC <- data[data$PassengerId %in% 892:1309 & data$Title=='man',]\ndata$Predict <- 0\ndata$Predict[data$Sex=='female'] <- 1\ndataTrain <- data.frame(y=dataB$Survived,x1=dataB$FareAdj/10,x2=dataB$FamilySize+dataB$Age/70)\ndataTest <- data.frame(y=dataC$Survived,x1=dataC$FareAdj/10,Pclass=dataC$Pclass\n    ,x2=dataC$FamilySize+dataC$Age/70,PassengerId=dataC$PassengerId)\nparam <- list(objective   = \"binary:logistic\",\n              eval_metric = \"error\",\n              max_depth   = 5,\n              eta         = 0.1,\n              gammma      = 0.1,\n              colsample_bytree = 1,\n              min_child_weight = 1)\nxgb <- xgboost(params  = param,\n              data    = data.matrix(dataTrain[,c('x1','x2')]),\n              label   = dataTrain$y, \n              nrounds = 500,\n              silent = 1,\n              print_every_n = 100,\n              verbose = 0)\np <- predict(xgb,newdata=data.matrix(dataTest[,c('x1','x2')]))\ndataTest$p <- ifelse(p>=0.90,1,0)\ndata$Predict[dataTest$PassengerId] <- dataTest$p \nmaleLive = which(data$Title=='man' & data$Predict==1)\ncat(sprintf('The following %d adult males are predicted to live\\n',length(maleLive)))\ndata[maleLive,c('PassengerId','Pclass','Name','Age','SibSp','Parch','FareAdj')]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"972c374198100eaab09cab8ac1289f5cfed6714f"},"cell_type":"code","source":"x1s <- seq(2, 5, length.out = 100)\nx2s <- seq(1, 3, length.out = 100)\ng <- data.frame(x1 = rep(x1s, each=100), x2 = rep(x2s, time = 100))\np <- predict(xgb,newdata=data.matrix(g[,c('x1','x2')]))\ng$y <- ifelse(p>=0.90,1,0)\ng1 <- ggplot(data=dataTest[dataTest$x1>2 & dataTest$x1<5 & dataTest$x2>1 & dataTest$x2<3,]) +\n    xlim(2,5) + ylim(1,3) +\n    geom_tile(data=g,aes(x1,x2,fill=factor(y))) +\n    geom_point(size=2,aes(x1,x2,color=factor(p),shape=factor(Pclass))) +\n    scale_color_manual(values=c('#666666','#0000FF'),\n        limits=c('0','1'),labels=c('0','1')) +\n    scale_fill_manual(values=c('#FF9999','#99FF99'),\n        limits=c('0','1'),labels=c('0','1')) +\n    labs(x='Fare / (10 x TicketFrequency)',y='FamilySize + (Age / 70)',shape='Pclass',fill='Classifier',\n        title='XGBoost classifies the test set.\n        It predicts 4 adult males have P(live)>=0.9',color='Predict') +\n    geom_vline(xintercept=2.8, linetype='dotted') +\n    geom_hline(yintercept=c(1.43,2.43), linetype='dotted') +\n    annotate('text',x=2.95,y=2.9,label='Fare = $28') +\n    annotate('text',x=4.7,y=2.35,label='Age = 30') +\n    annotate('text',x=4.7,y=1.35,label='Age = 30')\n\nfor (i in which(dataTest$p==1)){\n    g1 <- g1 + annotate('text',x=dataTest$x1[i]-0.15,y=dataTest$x2[i],label=dataTest$PassengerId[i]\n        ,color='darkblue',size=4)\n}\ng1","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d51253343dbdd0e4cf55c6789e4629869e389ebd"},"cell_type":"markdown","source":"For the rest of the predictions, we'll use the following two simple rules (referred to as WCG model, woman-child-groups):\n* Predict survival for all boys in families where all females and boys live.  \n* Predict perish for all females in families where all females and boys die.  \n  \nThese two rules are explained and cross validated in the following two kernels, [Titanic using Name only][1] and [Titanic Mega Model][2]. Let's run the code from these kernels to find the 8 boys that live and 14 females that die.  \n  \n[1]:https://www.kaggle.com/cdeotte/titanic-using-name-only-0-81818\n[2]:https://www.kaggle.com/cdeotte/titantic-mega-model-0-84210"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"b52547199323effbc4ea8c844da7579d1f8854f0"},"cell_type":"code","source":"# engineer \"woman-child-groups\"\ndata$Surname = substring( data$Name,0,regexpr(',',data$Name)-1)\ndata$GroupId = paste( data$Surname, data$Pclass, sub('.$','X',data$Ticket), data$Fare, data$Embarked, sep='-')\ndata$GroupId[data$Title=='man'] <- 'noGroup'\n# Mrs Wilkes (Needs) is Mrs Hocking (Needs) sister\ndata$GroupId[893] <- data$GroupId[775]\ndata$GroupFreq <- ave(1:1309,data$GroupId,FUN=length)\ndata$GroupId[data$GroupFreq<=1] <- 'noGroup'\ncat(sprintf('We found %d woman-child-groups\\n',length(unique(data$GroupId))-1))\ndata$TicketId = paste( data$Pclass,sub('.$','X',data$Ticket),data$Fare,data$Embarked,sep='-')\n# add nannies and relatives to groups\nc = 0\nfor (i in which(data$Title!='man' & data$GroupId=='noGroup')){\n    data$GroupId[i] = data$GroupId[data$TicketId==data$TicketId[i]][1]\n    if (data$GroupId[i]!='noGroup') c = c + 1\n}\ncat(sprintf('We found %d nannies/relatives and added them to groups.\\n',c))\n# calculate group survival rates\ndata$GroupSurvival <- NA\ndata$Survived <- as.numeric(as.character(data$Survived))\ndata$GroupSurvival[1:891] <- ave(data$Survived[1:891],data$GroupId[1:891])\nfor (i in 892:1309) data$GroupSurvival[i] <- data$GroupSurvival[which(data$GroupId==data$GroupId[i])[1]]\n# classify unknown groups\ndata$GroupSurvival[is.na(data$GroupSurvival) & data$Pclass==3] <- 0\ndata$GroupSurvival[is.na(data$GroupSurvival) & data$Pclass!=3] <- 1\n# make predictions\ndata$Predict <- 0\ndata$Predict[data$Sex=='female'] <- 1\ndata$Predict[data$Title=='woman' & data$GroupSurvival==0] <- 0\ndata$Predict[data$Title=='boy' & data$GroupSurvival==1] <- 1\nx = which(data$Sex=='male' & data$Predict==1 & data$PassengerId>891)\ncat(sprintf('We found %d boys predicted to live\\n',length(x)))\nx = which(data$Sex=='female' & data$Predict==0 & data$PassengerId>891)\ncat(sprintf('We found %d females predicted to die\\n',length(x)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"610b70aadbd710768900a2133578c79149a4acb2"},"cell_type":"code","source":"submit <- data.frame(PassengerId=892:1309,Survived=data$Predict[892:1309])\nsubmit$Survived[maleLive-891] <- 1\nwrite.csv(submit,'WCG_XGBoost1.csv',row.names=F)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"58d691a040537ac7350ced4225aa191d59e03368"},"cell_type":"markdown","source":"Drum roll..... UGH! Our score stayed the same. It seems that half our predictions for which adult males survive were right and half were wrong. Submitting the WCG plus these additional 4 predictions didn't change the WCG's score even though cross validation suggested that it would :-( However, we did find 1 or 2 adult males that live, hip hip hooray!\n# Explore solo females survival with XGBoost\nLet's apply this XGBoost exploration technique to the solo females. If we can correctly predict the survival of any solo females, we can combine those predictions with the WCG model and have the most accurate Titanic model ever! The training dataset contains 304 females. Of those, 108 are in WCG, 126 are traveling solo, and 80 travel with a brother or husband and have no children. Here we will explore the solo females."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"a13427eb18ee37641580d400b129b310025f9aed"},"cell_type":"code","source":"# identify WCG females\nWCGtrain = which(data$Passenger<=891 & (data$GroupSurvival==0 | data$GroupSurvival==1))\nWCGtest = which(data$Passenger>891 & (data$GroupSurvival==0 | data$GroupSurvival==1))\n# identify solo females\ndata2 <- data[data$PassengerId<=891 & data$Title=='woman' & data$FamilySize==1,]\ndata3 <- data.frame(y=data2$Survived,x1=data2$FareAdj/10,x2=data2$Age/15,Pclass=data2$Pclass)\n# set zoom\nx1s <- seq(0.5, 1.5, length.out = 100)\nx2s <- seq(1, 3, length.out = 100)\ng <- data.frame(x1 = rep(x1s, each=100), x2 = rep(x2s, time = 100))\n# classify females with XGBoost\nparam <- list(objective   = \"binary:logistic\",\n              eval_metric = \"error\",\n              max_depth   = 5,\n              eta         = 0.1,\n              gammma      = 0.1,\n              colsample_bytree = 1,\n              min_child_weight = 1)\ncat('XGBoosting begun...\\n')\nxgb <- xgboost(params  = param,\n               data    = data.matrix(data3[,c('x1','x2')]),\n              label   = data3$y, \n              nrounds = 500,\n              print_every_n = 100,\n              verbose = 1)\np <- predict(xgb,newdata=data.matrix(g[,c('x1','x2')]))\ng$y <- ifelse(p<=0.25,0,1)\n# plot results\nggplot(data=data3[data3$x1>0.5 & data3$x1<1.5 & data3$x2>1 & data3$x2<3,]) +\n    xlim(0.5,1.5) + ylim(1,3) +\n    geom_tile(data=g,aes(x1,x2,fill=factor(y))) +\n    geom_density_2d(aes(x1,x2,color=factor(y))) +\n    geom_point(size=2,aes(x1,x2,color=factor(y),shape=factor(Pclass))) +\n    scale_color_manual(values=c('#AA0000','#00AA00'),\n        limits=c('0','1'),labels=c('0','1')) +\n    scale_fill_manual(values=c('#FF9999','#99FF99'),\n        limits=c('0','1'),labels=c('0','1')) +\n    labs(x='Fare / (10 x TicketFrequency)',y='Age / 15',shape='Pclass',fill='Classify',\n        title='XGBoost learns the training set\\'s\n        126 solo females. Red is P(die)>=0.75',color='Survived') +\n    geom_vline(xintercept=c(0.8,0.9), linetype='dotted') +\n    annotate('text',x=0.77,y=2.95,label='Fare = $8') +\n    annotate('text',x=0.93,y=2.95,label='Fare = $9') +\n    geom_hline(yintercept=c(1.33,1.67), linetype='dotted') +\n    annotate('text',x=1.35,y=1.61,label='Age = 25') +\n    annotate('text',x=1.35,y=1.28,label='Age = 20')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7fb48b4a812bcceff3148760ffdbc3d149e6425f"},"cell_type":"markdown","source":"It looks like XGBoost found patterns of survival among the training dataset's solo  females! From the plot above, we see that the most probable perishing solo females have Pclass = 3, FareAdj = \\$9, and Age = 22. Or they have Pclass=3, FareAdj = \\$8, and Age = 34. The shading indicates what XGBoost's classifying decisions are. The red region corresponds with XGBoost predicting P(die)>=0.75."},{"metadata":{"_uuid":"7eb6934e513bdb26148dc579531fc8f76bee9f1c"},"cell_type":"markdown","source":"## Grid search and cross validation\nUsing grid search, we find the optimal parameters for XGBoost to be treeDepth=5, p<=0.08. (If you wish to perform a grid search, uncomment the for loops in the code below and add hyperparamters that you wish to search. In this notebook, I'm not doing the grid search because it takes a while. I did it offline) Now let's perform cross validation to see to see if XGBoost is more accurate than the gender model which has CV = 78.6%."},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"4fab6ee6b21cf70b2b53ea6f0d3b756ba6739022"},"cell_type":"code","source":"#accuracy = matrix(nrow=4,ncol=4)\n#rownames(accuracy) <- c('d=3','d=4','d=5','d=6')\n#colnames(accuracy) <- c('0.08','0.10','0.25','0.50')\n#for (j in 1:4)\n#for (k in 1:4){\ntrials=100; sum=0\nfor (i in 1:trials){\n    s = sample(1:891,800)\n    s2 = (1:891)[-s]\n    dataB <- data[data$PassengerId %in% s & data$Title=='woman' & data$FamilySize==1,]\n    dataC <- data[data$PassengerId %in% s2 & data$Title=='woman'& data$FamilySize==1,]\n    data$Predict <- 0\n    data$Predict[data$Sex=='female'] <- 1\n    dataTrain <- data.frame(y=dataB$Survived,x1=dataB$FareAdj/10,x2=dataB$Age/15)\n    dataTest <- data.frame(y=dataC$Survived,x1=dataC$FareAdj/10\n        ,x2=dataC$Age/15,PassengerId=dataC$PassengerId)\n    param <- list(objective   = \"binary:logistic\",\n              eval_metric = \"error\",\n              max_depth   = 5,\n              eta         = 0.1,\n              gammma      = 0.1,\n              colsample_bytree = 1,\n              min_child_weight = 1)\n    xgb <- xgboost(params  = param,\n              data    = data.matrix(dataTrain[,c('x1','x2')]),\n              label   = dataTrain$y, \n              nrounds = 500,\n              silent = 1,\n              print_every_n = 100,\n              verbose = 0)\n    p <- predict(xgb,newdata=data.matrix(dataTest[,c('x1','x2')]))\n    dataTest$p <- ifelse(p<=0.08,0,1)\n    data$Predict[dataTest$PassengerId] <- dataTest$p \n    sm = sum(abs(data$Predict[s2] - data$Survived[s2]))\n    cv = 1 - sm/length(s2)\n    #if (i %% 25==0) \n    #cat(sprintf('Trial %d has CV = %f\\n',i,cv))\n    sum = sum + cv\n}\ncat(sprintf('Average CV of %d trials = %f\\n',trials,sum/trials))\n#accuracy[j,k] <- sum/trials\n#}\n#accuracy","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ccf503c30530b6fbc6bf74b0698e9ee91918205a"},"cell_type":"markdown","source":"Our CV accuracy is 79.0%. (Results shown above may vary due to the random nature of k-fold CV but I confirmed 79.0% with 10,000 trials offline). The gender model has a CV accuracy of 78.6%, therefore we have achieved a slight improvement. Hopefully, we can achieve a slight improvement to our public score too. Let's see.\n## Submission to Kaggle"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"111b732164fb2b525ed1b2788ab69438815ccce0"},"cell_type":"code","source":"dataB <- data[data$PassengerId %in% 1:891 & data$Title=='woman' & data$FamilySize==1,]\ndataC <- data[data$PassengerId %in% 892:1309 & data$Title=='woman' & data$FamilySize==1 \n    & !data$PassengerId %in% WCGtest,]\ndata$Predict <- 0\ndata$Predict[data$Sex=='female'] <- 1\ndataTrain <- data.frame(y=dataB$Survived,x1=dataB$FareAdj/10,x2=dataB$Age/15)\ndataTest <- data.frame(y=dataC$Survived,x1=dataC$FareAdj/10\n    ,x2=dataC$Age/15,PassengerId=dataC$PassengerId,Pclass=dataC$Pclass)\nparam <- list(objective   = \"binary:logistic\",\n              eval_metric = \"error\",\n              max_depth   = 5,\n              eta         = 0.1,\n              gammma      = 0.1,\n              colsample_bytree = 1,\n              min_child_weight = 1)\nxgb <- xgboost(params  = param,\n              data    = data.matrix(dataTrain[,c('x1','x2')]),\n              label   = dataTrain$y, \n              nrounds = 500,\n              silent = 1,\n              print_every_n = 100,\n              verbose = 0)\np <- predict(xgb,newdata=data.matrix(dataTest[,c('x1','x2')]))\ndataTest$p <- ifelse(p<=0.08,0,1)\ndata$Predict[dataTest$PassengerId] <- dataTest$p \nfemalePerish = which(data$Title=='woman' & data$Predict==0)\ncat(sprintf('The following %d females are predicted to die\\n',length(femalePerish)))\ndata[femalePerish,c('PassengerId','Pclass','Name','Age','SibSp','Parch','FareAdj')]","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"3508715bb323bac2a811f0e656b0e5d1972ff98a"},"cell_type":"code","source":"x1s <- seq(0.6, 1, length.out = 100)\nx2s <- seq(1, 3, length.out = 100)\ng <- data.frame(x1 = rep(x1s, each=100), x2 = rep(x2s, time = 100))\np <- predict(xgb,newdata=data.matrix(g[,c('x1','x2')]))\ng$y <- ifelse(p<=0.08,0,1)\ng1 <- ggplot(data=dataTest[dataTest$x1<1 & dataTest$x2<3,]) +\n    xlim(0.6,1) + ylim(1,3) +\n    geom_tile(data=g,aes(x1,x2,fill=factor(y))) +\n    geom_point(size=2,aes(x1,x2,color=factor(p),shape=factor(Pclass))) +\n    scale_color_manual(values=c('#0000FF','#666666'),\n        limits=c('0','1'),labels=c('0','1')) +\n    scale_fill_manual(values=c('#FF9999','#99FF99'),\n        limits=c('0','1'),labels=c('0','1')) +\n    labs(x='Fare / (10 x TicketFrequency)',y='Age / 15',shape='Pclass',fill='Classifier',\n        title='XGBoost classifies the test set.\n        It predicts 10 non-WCG females have P(die)>=0.92',color='Predict') +\n    geom_vline(xintercept=c(0.8,0.9), linetype='dotted') +\n    geom_hline(yintercept=c(1.33,1.67), linetype='dotted') +\n    annotate('text',x=0.8,y=2.95,label='Fare = $8') +\n    annotate('text',x=0.9,y=2.95,label='Fare = $9') +\n    annotate('text',x=0.65,y=1.61,label='Age = 25') +\n    annotate('text',x=0.65,y=1.28,label='Age = 20')\n# plot passenger numbers\nv = c(0.07,0.02,0.02,0.02,-0.04,0.02,0.02,0.02,0.02,0.02); c = 0\nfor (i in which(dataTest$p==0)){\n    c = c + 1;\n    g1 <- g1 + annotate('text',x=dataTest$x1[i]+0.015,y=dataTest$x2[i]+v[c],label=dataTest$PassengerId[i]\n        ,color='darkblue',size=3)\n}\ng1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9f3beb2366557e7e3f15a40529a98e016677da30"},"cell_type":"code","source":"submit$Survived[femalePerish-891] <- 0\nwrite.csv(submit,'WCG_XGBoost2.csv',row.names=F)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5b1e323c6ceadce25dbacfb4924eb4e6ab69ead3"},"cell_type":"markdown","source":"![result-female](http://playagricola.com/Kaggle/surnameModel5.png)\nAwesome! It looks like most of our female predictions are correct. Our score increased from 83.3% to 84.7%!! This implies that 80% of our 10 female predictions were accurate, pretty good. Wow, we now have the most accurate Titanic model ever!"},{"metadata":{"_uuid":"6cc5168b0a8df778124c01be637fbc9ef1542e6d"},"cell_type":"markdown","source":"## Comparison with Mega Model\nThe model we have constructed above is just the WCG plus XGBoost. Previously we combined the WCG with an ensemble of top Kaggle models and scored 84.2% [here][1]. How do XGBoost's predictions compare with the ensemble's predictions? Surprisingly they are different. XGBoost found females that the ensemble did not and the ensemble found females that XGBoost did not. This implies that no single person has found the complete model to classify non-WCG passengers yet. Below are XGBoost's predictions (of 4 adult males to live and 10 solo females to die) with columns indicating the ensemble's predictions:  \n  \n[1]:https://www.kaggle.com/cdeotte/titantic-mega-model-0-84210"},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"04466de1fc4edd87cf8daa09c165b99e2ae919cc"},"cell_type":"code","source":"top <- read.csv('../input/top-6-titanic-model/top6.csv')\nnew <- c(maleLive,femalePerish)\ntop$MegaModel <- 'NO'\nfor (i in new){\n    s = sum(top[i-891,c(2,3,4,5,7)])\n    if (s>2.5 & i %in% maleLive) top$MegaModel[i-891] <- 'YES'\n    if (s<2.5 & i %in% femalePerish) top$MegaModel[i-891] <- 'YES'\n}\ndata.frame(data[new,c('Name','Sex','Age','FareAdj')],inMegaModel=top[[9]][new-891],KM=top[[2]][new-891]\n    ,SCW=top[[3]][new-891],THW=top[[4]][new-891],FS=top[[5]][new-891],OT=top[[7]][new-891])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6684be1b423b4645dd2fc8cffd8ba3345677e23c"},"cell_type":"markdown","source":"The rows above are the 14 predictions made by XGBoost model. The column Mega Model states whether these predictions were made by Mega Model's ensemble. The last 5 columns are how each model within Mega Model's ensemble voted where 1 = live and 0 = die. From the table above, we see that XGBoost and Mega Model are quite different. Therefore combining the predictions of WCG, XGBoost, and the ensemble, will most likely increase our score. "},{"metadata":{"_kg_hide-input":true,"trusted":true,"scrolled":true,"_uuid":"69390f35235849a6d30325b2b309c7cb9760b621"},"cell_type":"code","source":"x = which(data$Passenger>891 & (data$GroupSurvival==0 | data$GroupSurvival==1))\ntop$Sum <- 0; top$Predict <- top$GenderModel\nfor (i in 1:418){\n    for (j in c(2,3,4,5,7)) top$Sum[i] = top$Sum[i] + top[i,j]\n    if (top$GenderModel[i]==0 & top$Sum[i]>2.5) top$Predict[i] <- 1\n    if (top$GenderModel[i]==1 & top$Sum[i]<2.5) top$Predict[i] <- 0\n}\nensemble <- top$PassengerId[!top$PassengerId %in% x & top$Predict!=top$GenderModel]\ncat(sprintf('The ensemble predicts that %d females die.\\n',length(ensemble)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c9b93272cd2fb2259351704211df56231b312a56"},"cell_type":"code","source":"submit$Survived[ensemble-891] <- 0\nwrite.csv(submit,'WCG_XGBoost_Ensemble.csv',row.names=F)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a111aa6e609139b63e3a6b3f547d5650d8842f7b"},"cell_type":"markdown","source":"![MegaBoost](http://playagricola.com/Kaggle/MegaBoost.png)\nWow, we scored 85.2%!! Woohoo!!"},{"metadata":{"_uuid":"813546fa6fef9686024c8faa82e4b594bffa30ca"},"cell_type":"markdown","source":"## Comparison with CART, kNN, SVM, Random Forest\nBelow shows how different methods classify the solo females limited to Pclass=3 (60 females). CART, kNN, and SVM cross validate at 78.9% and Random Forest 78.7%. The four models change 16, 16, 6, 9 female predictions from the WCG and obtain public scores of 83.8%, 83.3%, 83.3%, and 84.2% respectively. (WCG obtains 83.3% by itself.) The regions created by Random Forest are the most similar to XGBoost above. And it scores the most similar. CART is nice because it is very readable. CART says that solo females in Pclass=3 with either FareAdj > \\$8 or Age > 25 die. This pattern exists in the training dataset. Among the 22 females fullfilling these conditions, 64 percent = 14/22 die."},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true,"_uuid":"70fac2631ba3b778c9a2afabe87bcca92346f6bc"},"cell_type":"code","source":"library(rpart)\nlibrary(caret)\nlibrary(e1071)\nlibrary(randomForest)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true,"_uuid":"98c26b4f688106d323285ff411723ed1ee1e8ff2"},"cell_type":"code","source":"g = list(); gp = list()\ngt = c('CART learns the training set','kNN k=5 learns the training set'\n       ,'SVM with radial kernel','Random Forest with P(die)>0.75')\ndata2 <- data[data$PassengerId<=891 & data$Title=='woman' & data$FamilySize==1 & data$Pclass==3,]\ndata3 <- data.frame(y=data2$Survived,x1=data2$FareAdj/10,x2=data2$Age/15)\n# set zoom\nx1s <- seq(0.6, 1, length.out = 50)\nx2s <- seq(1, 3, length.out = 50)\nfor (i in 1:4){\n    g[[i]] <- data.frame(x1 = rep(x1s, each=100), x2 = rep(x2s, time = 100))\n    if (i==1){\n        fit <- rpart(factor(y) ~ x1 + x2,data3)\n        p <- predict(fit,newdata=g[[i]])[,2]\n        g[[i]]$y <- ifelse(p<=0.5,0,1)\n    } else if (i==2){\n        fit <- knn3(factor(y) ~ x1 + x2,data3,k=3)\n        p <- predict(fit,newdata=g[[i]])[,2]\n        g[[i]]$y <- ifelse(p<=0.5,0,1)\n    } else if (i==3){\n        fit <- svm(factor(y)~ x1 + x2,data3,kernel='radial')\n        p <- predict(fit,newdata=g[[i]])\n        g[[i]]$y <- as.numeric(as.character(p))\n    } else if (i==4){\n        fit <- randomForest(factor(y) ~ x1 + x2,data3)\n        p <- predict(fit,newdata=g[[i]],type='prob')[,2]\n        g[[i]]$y <- ifelse(p<=0.25,0,1)\n    }\n    # plot results\n    gp[[i]] <- ggplot(data=data3[data3$x1>0.6 & data3$x1<1 & data3$x2>1 & data3$x2<3,]) +\n        xlim(0.6,1) + ylim(1,3) +\n        geom_tile(data=g[[i]],aes(x1,x2,fill=factor(y)),show.legend=F) +\n        geom_point(size=1.5,aes(x1,x2,color=factor(y)),show.legend=F) +\n        scale_color_manual(values=c('#AA0000','#00AA00'),\n            limits=c('0','1'),labels=c('0','1')) +\n        scale_fill_manual(values=c('#FF9999','#99FF99'),\n            limits=c('0','1'),labels=c('0','1')) +\n        #labs(x='Fare / (10 x TicketFrequency)',y='Age / 15',fill='Classify',\n        labs(x='',y='',title=gt[i],color='Survived') +\n        geom_vline(xintercept=c(0.8,0.9), linetype='dotted') +\n        geom_hline(yintercept=c(1.33,1.67), linetype='dotted')\n}\ngrid.arrange(gp[[1]],gp[[2]],gp[[3]],gp[[4]],nrow=2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bc9d7c9a5aba8ee3931ac68dcc13322359bde0ca"},"cell_type":"markdown","source":"# Conclusion\nThis notebook demonstrated that it is very difficult to predict adult male survival. From the analysis above, it appears that there are patterns for adult male survival in the training set. However these patterns don't seem to carry over to the test dataset. I don't think that you can predict which adult males survive from Pclass, Age, Family Size, and Fare alone (even though the training set fools you into thinking that you can). If we want to predict male survival, we'll need to engineer features.  \n  \nRegarding females, prediction has more success. The WCG model by itself finds many females that perish, but this kernel showed that there are still patterns among the non-WCG females. For example, in the training set, there are 6 solo traveling females in Pclass = 3, with FareAdj between \\$8 and \\$9, and Age between 20 and 25. All 6 of these females died. That's a pattern. Using just Pclass, Age, Family Size, and Fare, we have shown that we can predict females to die with some success. We also found that females with FareAdj near 8 and Age near 34 were likely to perish.\n\nCan we do better than 85%? The WCG model can solidly achieve 84.4% CV and 83.3% public score by itself. This kernel showed there are more patterns in the data and the Mega Model's ensemble showed that there are more patterns in the data. It is encouraging to see that XGBoost here found different females than did the ensemble of Mega Model. This means that no single model has yet to classify all the non-WCG females. I encourage everyone to fork this kernel and improve it by building your own classifier for non-WCG passengers. (_NOTE: if you fork this kernel, turn on GPU under settings_)\n\nTo learn more about XGBoost, check out [Tae Hyon Whang's][1] great Python kernel called [Titanic Starter with XGBoost, 173/209 LB][2].  \n  \n[1]:https://www.kaggle.com/numbersareuseful\n[2]:https://www.kaggle.com/numbersareuseful/titanic-starter-with-xgboost-173-209-lb\n"},{"metadata":{"_uuid":"14d091833121a4d676b3d5d0ba42ab0eb2148eec"},"cell_type":"markdown","source":"# Summary of Titanic models\n Here's a review of this notebook and my previous two. First the population was divided into 5 groups:  \n* (A) Males adults - 61% of population  \n* (B) Males boys (Master title) - 5% of population  \n* (C) Females that have children, sisters, or mothers (brothers, husbands optional) - 12%\n* (D) Females that have brothers, or husbands (no children, no sisters, no mothers) - 8%  \n* (E) Females traveling alone - 14% of population  \n  \nNext we built 3 classifiers:    \n* (1) [WCG classifies B+C][1]  \n* (2) [Ensemble of top 5 Kaggle classifies A+D+E][1]  \n* (3) [XGBoost classifies E][2]  \n  \nFinally we assembled 4 models. Here are cross validations and public scores:  \n* WCG + GenderModel = 84.4% CV and 83.3% PS  \n* WCG + Ensemble = unknown CV and 84.2% PS  \n* WCG + XGBoost + GenderModel = 85.2% CV and 84.7% PS  \n* WCG + XGBoost + Ensemble = unknown CV and 85.2% PS  \n\nThank you for reading my notebook #3! I hope you enjoyed it. Check out notebooks [#1][3] and [#2][1]. Feel free to use my classifiers, improve them, add your own, and assemble new models. I believe it is possible to increase CV and PS above 85%. Good luck! (_NOTE: if you fork this kernel, turn on GPU under settings_)  \n  \n[1]:https://www.kaggle.com/cdeotte/titantic-mega-model-0-84210\n[2]:https://www.kaggle.com/cdeotte/titanic-wcg-xgboost-0-84688\n[3]:https://www.kaggle.com/cdeotte/titanic-using-name-only-0-81818"},{"metadata":{"_uuid":"6af53ed5b639b9e8c6ed594f34de1e78219d363b"},"cell_type":"markdown","source":"![hist](http://playagricola.com/Kaggle/histXGB.png)"}],"metadata":{"kernelspec":{"display_name":"R","language":"R","name":"ir"},"language_info":{"mimetype":"text/x-r-source","name":"R","pygments_lexer":"r","version":"3.4.2","file_extension":".r","codemirror_mode":"r"}},"nbformat":4,"nbformat_minor":1}