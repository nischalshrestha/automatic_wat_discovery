{"cells":[{"metadata":{"_uuid":"1363bb9da11eb41865ed33ed5d6876d38bd91051","_cell_guid":"56d7a7ea-e802-4472-bcab-fe80fc830e18"},"cell_type":"markdown","source":"# Simple Titanic model using only Name scores 82%!\n\nI'm excited to share my Titanic solution and write my first Kaggle notebook. I've had a fun time building dozens of models. Below is my favorite. This simple model only uses one feature more than the gender model but it scores an impressive 82% and outperforms 97% of all 11,300 Kaggle submissions which are all more complicated! Using only passenger names, gender is deduced from title (Mr. Mrs. etc), children are determined from title (Master.), and family groups are identified by duplicate surnames (last names).  \n  \nUPDATE: By combining this model with another that classifies single passengers, 85% was achieved [here][1]. Also 83% and 84% were achieved [here][2].\n\n![model](http://playagricola.com/Kaggle/tree4.jpg)\n\nFirst I'll explain how it works. Then, I'll present my model, code, cross validation, and submission results.\n\n![rules](http://playagricola.com/Kaggle/rules2.jpg)\n\n## Let's begin with the gender model... \nIf we just predict females survive and males perish, we cross validate at 78.6% and receive a public leaderboard score of 76.6%.  \n \n[1]:https://www.kaggle.com/cdeotte/titanic-wcg-xgboost-0-84688\n[2]:https://www.kaggle.com/cdeotte/titantic-mega-model-0-84210"},{"metadata":{"_uuid":"1391a64bf2b9813a90e852aba684f6f2d86ef826","_cell_guid":"f1bcfbf6-de73-4e77-ae4c-9c15ae33a0d0","trusted":false},"cell_type":"code","source":"# The gender model achieves a 76.6% leaderboard score\ntest <- read.csv(\"../input/titanic/test.csv\",stringsAsFactors=F)\ntest$Survived[test$Sex=='male'] <- 0\ntest$Survived[test$Sex=='female'] <- 1\nsubmit <- data.frame(PassengerId = test$PassengerId, Survived = test$Survived)\nwrite.csv(submit,\"genderModel.csv\",row.names=F)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fa64990b6b77bab6e0f899adfe9372ff50fefd92","_cell_guid":"2da6c2e4-4ba4-4bdc-96dd-f49d10b2df3d"},"cell_type":"markdown","source":"To improve upon the gender model, we need to determine which males survive and which females perish. By analyzing the Titanic data, we find that many male survivors are among the youth, namely 52.5% = 21/40 of males under 16 years old survive. And most females who perish are among Pclass==3 passengers, namely 50% = 72/144 of Pclass==3 females perish."},{"metadata":{"_uuid":"91cc81adc0fdd5570008dc826e694768569704d1","_cell_guid":"652bb627-2d8e-4418-a30c-2aa760431768","scrolled":false,"trusted":false},"cell_type":"code","source":"train <- read.csv(\"../input/titanic/train.csv\",stringsAsFactors=F)\ntable(train$Survived[train$Sex=='male' & train$Age<16])\ntable(train$Survived[train$Sex=='female' & train$Pclass==3])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"47badc5d112bde1c4254815d3a7dae40d1364f83","_cell_guid":"f29c2699-ae64-463c-9cc2-0fbbff218a79"},"cell_type":"markdown","source":"So let's focus on identifying surviving males and perishing females among these 184 passengers. The data indicates that woman and children were prioritized in rescue. Furthermore it appears that woman and children from the same family survived or perished together. There are 142 females and boys in the training dataset who have a relative (indicated by surname) in the training set that is a female or boy. Let's refer to these 142 passengers as members of \"woman-child-groups\".\n\n## Let's engineer a new feature to identify \"woman-child-groups\" and their survival rate."},{"metadata":{"_uuid":"5a403bf98c0562e1dc3a727ffd31c322b179cf27","_cell_guid":"c0204366-ca42-44a5-bb7a-7418820fa14a","scrolled":true,"trusted":false},"cell_type":"code","source":"# engineer titles in training dataset\ntrain$Title <- substring(train$Name,regexpr(\",\",train$Name)+2,regexpr(\"\\\\.\",train$Name)-1)\ntrain$Title[train$Title %in% c(\"Capt\",\"Don\",\"Major\",\"Col\",\"Rev\",\"Dr\",\"Sir\",\"Mr\",\"Jonkheer\")] <- \"man\"\ntrain$Title[train$Title %in% c(\"Dona\",\"the Countess\",\"Mme\",\"Mlle\",\"Ms\",\"Miss\",\"Lady\",\"Mrs\")] <- \"woman\"\ntrain$Title[train$Title %in% c(\"Master\")] <- \"boy\"\n# engineer \"woman-child-groups\" for training dataset\ntrain$Surname <- substring(train$Name,0,regexpr(\",\",train$Name)-1)\ntrain$Surname[train$Title=='man'] <- 'noGroup'\ntrain$SurnameFreq <- ave(1:891,train$Surname,FUN=length)\ntrain$Surname[train$SurnameFreq<=1] <- 'noGroup'\n# calculate \"woman-child-group\" survival rates\ntrain$SurnameSurvival <- ave(train$Survived,train$Surname)\ntable(train$SurnameSurvival[train$Surname!='noGroup'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cd73b78a1750bcdbbb407d081a588111cffeecc1","_cell_guid":"707476ff-c780-40a1-8afc-258a358eed20","trusted":false},"cell_type":"code","source":"# the following \"woman-child-groups\" all perish\nx=train[train$SurnameSurvival==0,c(\"Surname\")]; unique(x[order(x)])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"22e95ed9da6b5e60de59c3716df7e49d382d17d4","_cell_guid":"a0bcd12e-57b3-4ebd-bda8-2f16745349d8","scrolled":false,"trusted":false},"cell_type":"code","source":"# the following \"woman-child-groups\" all survive\nx=train[train$SurnameSurvival==1,c(\"Surname\")]; unique(x[order(x)])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4a93bb568f017a5f6a0da4998ea8e86034eef96c","_cell_guid":"e35207ec-19bf-4aaa-bd69-a815a3086e03","scrolled":false,"trusted":false},"cell_type":"code","source":"# the following \"woman-child-groups\" have mixed survival\ntrain[train$SurnameSurvival==1/7,c(\"Surname\",\"Title\",\"Survived\")]\ntrain[train$SurnameSurvival==1/3,c(\"Surname\",\"Title\",\"Survived\")]\ntrain[train$SurnameSurvival==3/4,c(\"Surname\",\"Title\",\"Survived\")]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"76f90d3cdde899fce5701718f71dca11dd211124","_cell_guid":"331e175c-3216-435d-97bb-395082602c1a"},"cell_type":"markdown","source":"The above code shows that 124 out of 142 \"woman-child-group\" passengers were part of \"woman-child-groups\" that either entirely lived or died. And 18 out of the 142 were part of \"woman-child-groups\" with mixed survival. Among these 18 passengers, 14 lived or died according to their \"woman-child-groups'\" average fate and 4 did not. Therefore among all 142 \"woman-child-group\" passengers,  97.2% = 138/142 lived or died according to their \"woman-child-groups'\" average fate. Our new engineered feature is a near perfect predictor! Wow!\n\n## Cross validation\nLet's try to guess what our cross validation accuracy will be when we use this new engineered feature in addition to the gender model before cross validating."},{"metadata":{"_uuid":"23bf960cfa7cf8e9b410b76e02dfdfb95021b7fa","_cell_guid":"60f6f1a3-2d6d-44dd-a22b-adfea04f6e84","trusted":true},"cell_type":"code","source":"library(ggplot2)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3eed1cdbf466c9c6a3b3f362edf0e67364c33ce8","_cell_guid":"779a1125-76bc-4aa9-88ec-7fbfc166aa8e","trusted":false},"cell_type":"code","source":"# adjust survival rates for use on training set\ntrain$AdjustedSurvival <- (train$SurnameSurvival * train$SurnameFreq - train$Survived) / (train$SurnameFreq-1)\n# apply gender model plus new predictor to training set\ntrain$predict <- 0\ntrain$predict[train$Title=='woman'] <- 1\ntrain$predict[train$Title=='boy' & train$AdjustedSurvival==1] <- 1\ntrain$predict[train$Title=='woman' & train$AdjustedSurvival==0] <- 0\n# plot how new predictor changes gender model\nggplot(train[train$Title=='woman',]) +\n    geom_jitter(aes(x=Pclass,y=predict,color=factor(Survived))) + \n    labs(title=\"36 female predictions change from gender model on training set\") +\n    labs(x=\"Pclass\",y=\"New Predictor\") +\n    geom_rect(alpha=0,color=\"black\",aes(xmin=2.5,xmax=3.5,ymin=-0.45,ymax=0.45))\ntable(train$Survived[train$Title=='woman' & train$predict==0])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fe9f969ed7ca758bbe14b87d0f2c4e3852f7d886","_cell_guid":"dcd3c3e9-cf6c-48da-8e38-e9fa544a3a54","scrolled":false,"trusted":false},"cell_type":"code","source":"# plot how new predictor changes gender model\nggplot(train[train$Title!='woman',]) +\n    geom_jitter(aes(x=Title,y=predict,color=factor(Survived))) +\n    labs(title=\"16 male predictions change from gender model on training set\") +\n    labs(x=\"Title\",y=\"New Predictor\") +\n    geom_rect(alpha=0,color=\"black\",aes(xmin=0.5,xmax=1.5,ymin=0.55,ymax=1.45))\ntable(train$Survived[train$Title!='woman' & train$predict==1])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9301165a53d5d5c8e601cf7b604c488a69e0817e","_cell_guid":"018a1ef6-d07b-414b-a56a-ea3b16ec651d"},"cell_type":"markdown","source":"The two plots above show that our new predictor changes 52 = 36 + 16 predictions from the gender model when applied to the training set. And amazingly, nearly all of the changes are now correct predictions! Wow! Therefore our cross validation accuracy should be 0.786 + (48/891) =  0.840. Let's test it."},{"metadata":{"_uuid":"aec0503dab66379d1bbc24404397eb96481675fb","_cell_guid":"027a3933-9de7-4452-90f6-7b25b837465a","scrolled":false,"trusted":false},"cell_type":"code","source":"# Perform 25 trials of 10-fold cross validation\ntrials = 25; sum = 0\nfor (j in 1:trials){\nx = sample(1:890); s = 0\nfor (i in 0:9){\n    # Engineer \"woman-child-groups\" from training subset\n    train$Surname <- substring(train$Name,0,regexpr(\",\",train$Name)-1)\n    train$Surname[train$Title=='man'] <- 'noGroup'\n    train$SurnameFreq <- ave(1:891,train$Surname,FUN=length)\n    train$Surname[train$SurnameFreq<=1] <- 'noGroup'\n    train$SurnameSurvival <- NA\n    # calculate training subset's surname survival rate\n    train$SurnameSurvival[-x[1:89+i*89]] <- ave(train$Survived[-x[1:89+i*89]],train$Surname[-x[1:89+i*89]])\n    # calculate testing subset's surname survival rate from training set's rate\n    for (k in x[1:89+i*89]) \n    train$SurnameSurvival[k] <- train$SurnameSurvival[which(!is.na(train$SurnameSurvival) & train$Surname==train$Surname[k])[1]]\n    # apply gender model plus new predictor\n    train$predict <- 0\n    train$predict[train$Title=='woman'] <- 1\n    train$predict[train$Title=='boy' & train$SurnameSurvival==1] <- 1\n    train$predict[train$Title=='woman' & train$SurnameSurvival==0] <- 0\n    c = sum(abs(train$predict[x[1:89+i*89]] - train$Survived[x[1:89+i*89]]))\n    s = s + c\n}\ncat( sprintf(\"Trial %d has 10-fold CV accuracy = %f\\n\",j,1-s/890))\nsum = sum + 1-s/890\n}\ncat(sprintf(\"Average 10-fold CV accuracy from %d trials = %f\\n\",trials,sum/trials))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b9407ad1f274cb0cb05553e2e6e4c80dfe87ef0c","_cell_guid":"8d024711-6d89-4d60-9630-8cf5b082e49c"},"cell_type":"markdown","source":"Our cross validation accuracy is 83.6% which is near our guess of 84.0%. \n\n## Submission to Kaggle and evaluation\nLet's try to guess what our Kaggle leaderboard score will be using this new engineered feature in addition to the gender model before submission."},{"metadata":{"_uuid":"9766fd8414cc3526678b5432cb8e8e0e243b71ce","_cell_guid":"97ed249d-8935-493c-85ae-d018d9c59b19","scrolled":false,"trusted":false},"cell_type":"code","source":"# engineer titles in test dataset\ntest$Title <- substring(test$Name,regexpr(\",\",test$Name)+2,regexpr(\"\\\\.\",test$Name)-1)\ntest$Title[test$Title %in% c(\"Capt\",\"Don\",\"Major\",\"Col\",\"Rev\",\"Dr\",\"Sir\",\"Mr\",\"Jonkheer\")] <- \"man\"\ntest$Title[test$Title %in% c(\"Dona\",\"the Countess\",\"Mme\",\"Mlle\",\"Ms\",\"Miss\",\"Lady\",\"Mrs\")] <- \"woman\"\ntest$Title[test$Title %in% c(\"Master\")] <- \"boy\"\n# engineer \"woman-child-groups\" for entire dataset\ntest$Survived <- NA; test$predict <- NA; train$AdjustedSurvival <- NULL\ntrain$Surname <- NULL; train$SurnameFreq <- NULL; train$SurnameSurvival <- NULL\nallData <- rbind(train,test)\nallData$Surname <- substring(allData$Name,0,regexpr(\",\",allData$Name)-1)\nallData$Surname[allData$Title=='man'] <- 'noGroup'\nallData$SurnameFreq <- ave(1:1309,allData$Surname,FUN=length)\nallData$Surname[allData$SurnameFreq<=1] <- 'noGroup'\n# using only \"Name\" scores 0.81818, correcting surname groups with \"Ticket\" scores 0.82296\n# search single woman and children and correct surname groups using Ticket\nfor (i in which(allData$Title!='man' & allData$Surname=='noGroup'))\n    allData$Surname[i] = allData$Surname[allData$Ticket==allData$Ticket[i]][1]\nallData$Surname[is.na(allData$Surname)] <- 'noGroup'\n# calculate \"woman-child-group\" survival rates\nallData$SurnameSurvival <- NA\nallData$SurnameSurvival[1:891] <- ave(allData$Survived[1:891],allData$Surname[1:891])\nfor (i in 892:1309) allData$SurnameSurvival[i] <- allData$SurnameSurvival[which(allData$Surname==allData$Surname[i])[1]]\n# apply gender model plus new predictor to test dataset\nallData$predict <- 0\nallData$predict[allData$Title=='woman'] <- 1\nallData$predict[allData$Title=='boy' & allData$SurnameSurvival==1] <- 1\nallData$predict[allData$Title=='woman' & allData$SurnameSurvival==0] <- 0\n# plot predictions\nggplot(allData[892:1309,]) +\n    geom_jitter(aes(x=Title,y=predict)) +\n    labs(title=\"18 predictions change from gender model on test set\") +\n    labs(x=\"Title\",y=\"Prediction\") +\n    geom_rect(alpha=0,color=\"black\",aes(xmin=0.5,xmax=1.5,ymin=0.55,ymax=1.45)) +\n    geom_rect(alpha=0,color=\"black\",aes(xmin=2.5,xmax=3.5,ymin=-0.45,ymax=0.45))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2e983f1408350f9bba03702c73d744f04b6e3117","_cell_guid":"8ad1b75f-2d9f-4837-8a20-275adf936d97"},"cell_type":"markdown","source":"The plot above shows that our new predictor changes 18 = 10 + 8 predictions from the gender model when applied to the test set. We suspect that all 18 of these chages are correct because nearly all the changes that our predictor made to the training dataset's gender model were correct. Therefore, we would expect that our Kaggle public leaderboard score will be 0.766 + (18/418) =  0.809."},{"metadata":{"_uuid":"ea52ae6563c0611485b7270731f1292dd9607c6d","_cell_guid":"fe114035-4fcc-4160-a718-7de498af6be3","trusted":false},"cell_type":"code","source":"# create CSV file to submit\nsubmit <- data.frame(PassengerId = allData$PassengerId[892:1309], Survived = allData$predict[892:1309])\nwrite.csv(submit,\"genderSurnameModel.csv\",row.names=F)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6f306661157edeaedf4057c9b38830ae488533f5","_cell_guid":"1f8dba8f-5641-45e0-85fd-560a2846b7cf"},"cell_type":"markdown","source":"![result](http://playagricola.com/Kaggle/surnameModel.png)\n\nWow awesome! Our Kaggle public leaderboard score result is 82.3% which is better than our guess of 80.9%. Luck was in our favor! The test dataset contains 418 observations. The public leaderboard only scores 209 of them. Remember that we changed 18 predictions from the gender model and suspect that all those changes are correct. The gender model receives a score of 0.766 = 160/209, therefore it appears that 12 of our 18 changes were included in our public leaderboard score, (160+12)/209 = 0.823. (Note that only using \"Name\" to engineer surname groups scores 0.818. Additionally correcting surname groups with \"Ticket\" scores 0.823.)\n\n## Reflection\n\nHow much more predictive information is hiding in the training dataset besides gender and family association? Based on comparison to other submitted models, it seems not much. Since November 2012,  11300 teams have scrutinized the Titanic dataset and searched for predictive features but only 2% of those models outperform this simple gender surname model. Therefore it seems that gender and family association capture most of the prediction power of the training dataset.\n\nI would love to discover ways to improve this model. I welcome everyone to fork this kernel and try to improve its predictive accuracy. Currently it only makes 18 wise predictions and then uses the gender model. Therefore if you can build a model that can predict the other 400 remaining passengers better than the gender model, you will have a Titanic solution scoring over 82%! [More info here][1]. Good luck and have fun!  \n\n![model](http://playagricola.com/Kaggle/tree3.jpg)  \n  \n[1]:https://www.kaggle.com/c/titanic/discussion/57447"},{"metadata":{"_uuid":"f85ef1283c35fae8ff5c3f7240dbf3f06e234e93","_cell_guid":"4c2dc348-e3cb-4c8c-b484-bce674a0d727","scrolled":true,"trusted":true},"cell_type":"code","source":"leader = read.csv(\"../input/kaggle-titanic-leaderboard-may-15-2018/leaderboard.csv\")\nc1 = \"gray30\"; c2 = \"black\"\ncolors <- c(rep(c1,6),c2,rep(c1,28),c2,rep(c1,11),c2,rep(c1,16))\nggplot(data=leader[leader$Score>=0.6 & leader$Score<=0.9,]) +\n    geom_histogram (aes(x=Score),binwidth=1/209,fill=colors,na.rm=TRUE) +\n    xlim(c(0.6,0.9)) + ylim(c(0,1500)) +\n    labs(title='Histogram of Kaggle titanic public leaderboard scores, May 15 2018') +\n    annotate('text', x=0.65, y=300, label='0.627 - All dead model') +\n    annotate('text', x=0.662, y=240, label='(=131/209) top 96%') +\n    annotate('text', x=0.79, y=1450, label='0.766 - Gender model') +\n    annotate('text', x=0.805, y=1390, label='(=160/209) top 64%') +\n    annotate('text', x=0.86, y=300, label='0.823 - Surname model') +\n    annotate('text', x=0.86, y=240, label='(=172/209) top 2%')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ccb44e1cf3bdda6c64487884cd06d99fefabcd8e","_cell_guid":"8bda1e22-14ae-4b5c-83ae-61e413d6e7e9"},"cell_type":"markdown","source":"Thank you everyone for reading my first notebook. I enjoyed writing it and look forward to your comments.  \n\nUPDATE: By combining this model with another that classifies single passengers, 85% was achieved [here][13]. Also 83% and 84% were achieved [here][14].\n\n## References\n* [Trevor Stephens][1]: Great tutorial for learning both R and Titanic modeling.\n* [Itai Blitzer][2]: Titanic code providing hints for engineered features.\n* [Megan Risdal][3]: Kaggle’s Titanic tutorial with great ggplot R visualizations.\n* [Jason][4]: Kaggle’s Titanic tutorial with great ggplot R visualizations.\n* [Michael Hahsler][5]: Shows R syntax for applying different machine learning models.\n* [Chris Deotte][6]: Interactive visualizations showing different machine learning models.\n* [Chris Deotte][7]: Interactive visualization showing neural network model.\n* [Oscar Takeshita][8]: Provides Titanic benchmarks of possible scores\n* [bhb2572][9]: Are Titanic models scoring 80% or better luck or science?\n* [Cathy O'Neil & Rachel Schutt][10]: Doing Data Science\n* [James, Witten, Hastie, & Tibshirani][11]: An Introduction to Statisical Learning\n* [Aurelien Geron][12]: Hands-On Machine Learning with Scikit-Learn & TensorFlow\n\n[1]:http://trevorstephens.com/kaggle-titanic-tutorial/getting-started-with-r/\n[2]:https://www.kaggle.com/itaibl/kaggle-titanic-tutorial-passing-82-part-1/code\n[3]:https://www.kaggle.com/mrisdal/exploring-survival-on-the-titanic\n[4]:https://www.kaggle.com/jasonm/large-families-not-good-for-survival\n[5]:http://michael.hahsler.net/SMU/EMIS7332/R/viz_classifier.html\n[6]:http://www.ccom.ucsd.edu/~cdeotte/programs/classify.html\n[7]:http://www.ccom.ucsd.edu/~cdeotte/programs/neuralnetwork.html\n[8]:https://www.kaggle.com/pliptor/how-am-i-doing-with-my-score/\n[9]:https://www.kaggle.com/c/titanic/discussion/56254\n[10]:https://www.amazon.com/Doing-Data-Science-Straight-Frontline/dp/1449358659\n[11]:https://www.amazon.com/Introduction-Statistical-Learning-Applications-Statistics/dp/1461471370/\n[12]:https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1491962291/\n[13]:https://www.kaggle.com/cdeotte/titanic-wcg-xgboost-0-84688\n[14]:https://www.kaggle.com/cdeotte/titantic-mega-model-0-84210"}],"metadata":{"language_info":{"mimetype":"text/x-r-source","name":"R","pygments_lexer":"r","version":"3.4.2","file_extension":".r","codemirror_mode":"r"},"kernelspec":{"display_name":"R","language":"R","name":"ir"}},"nbformat":4,"nbformat_minor":1}