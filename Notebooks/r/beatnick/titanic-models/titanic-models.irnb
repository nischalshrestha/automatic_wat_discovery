{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "57e42cc6-f33a-b78c-2b43-b61718964e83"
      },
      "source": [
        "The following notebook includes my procedure for building binary classification predictive models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "8a4345a6-f57c-e286-c658-3136ba669604"
      },
      "outputs": [],
      "source": [
        "# -----------------------------------------------------------------------------------\n",
        "# ---- Cross Validation Parameters -------------------------------------------------------------------\n",
        "# -----------------------------------------------------------------------------------\n",
        "\n",
        "# folds\n",
        "K = 2\n",
        "\n",
        "# replications per fold\n",
        "B = 5\n",
        "\n",
        "# -----------------------------------------------------------------------------------\n",
        "# ---- Packages ---------------------------------------------------------------------\n",
        "# -----------------------------------------------------------------------------------\n",
        "\n",
        "{\n",
        "\n",
        "# these are the packages needed to run this script file\n",
        "\n",
        "# data handling\n",
        "require(data.table)\n",
        "require(stringr)\n",
        "require(mice)\n",
        "\n",
        "# plotting\n",
        "require(ggplot2)\n",
        "require(gridExtra)\n",
        "require(GGally)\n",
        "require(VIM)\n",
        "require(lattice)\n",
        "require(scales)\n",
        "require(corrplot)\n",
        "\n",
        "# modeling\n",
        "require(MASS)\n",
        "require(neuralnet)\n",
        "require(randomForest)\n",
        "require(e1071)\n",
        "require(glmnet)\n",
        "require(pROC)\n",
        "require(caret)\n",
        "require(crossval)\n",
        "require(SuperLearner)\n",
        "require(xgboost)\n",
        "\n",
        "# parallel computing\n",
        "require(foreach)\n",
        "require(doParallel)\n",
        "\n",
        "}\n",
        "\n",
        "# -----------------------------------------------------------------------------------\n",
        "# ---- Functions --------------------------------------------------------------------\n",
        "# -----------------------------------------------------------------------------------\n",
        "\n",
        "{\n",
        "\n",
        "# ---- prints the data types of each column in a data frame -------------------------\n",
        "\n",
        "types = function(dat)\n",
        "{\n",
        "\tdat = data.frame(dat)\n",
        "  \n",
        "\tcolumn = sapply(1:ncol(dat), function(i) colnames(dat)[i])\n",
        "\tdata.type = sapply(1:ncol(dat), function(i) class(dat[,i]))\n",
        "\t\n",
        "\treturn(data.frame(cbind(column, data.type)))\t\n",
        "}\n",
        "\n",
        "# ---- emulates the default ggplot2 color scheme ------------------------------------\n",
        "\n",
        "ggcolor = function(n)\n",
        "{\n",
        "\thues = seq(15, 375, length = n + 1)\n",
        "\thcl(h = hues, l = 65, c = 100)[1:n]\n",
        "}\n",
        "\n",
        "# ---- summarizes diagnostic errors of a crossval() object --------------------------\n",
        "\n",
        "diag.cv = function(cv)\n",
        "{\n",
        "\trequire(data.table)\n",
        "\t\n",
        "\t# extract table of confusion matrix results\n",
        "\tdat = data.table(cv$stat.cv)\n",
        "\t\n",
        "\t# compute performance metrics for each iteration\n",
        "\toutput = lapply(1:nrow(dat), function(i) \n",
        "\t\t\t\tdata.table(acc = (dat$TP[i] + dat$TN[i]) / (dat$FP[i] + dat$TN[i] + dat$TP[i] + dat$FN[i]),\n",
        "\t\t\t\t\t\t\tsens = dat$TP[i] / (dat$TP[i] + dat$FN[i]),\n",
        "\t\t\t\t\t\t\tspec = dat$TN[i] / (dat$FP[i] + dat$TN[i]),\n",
        "\t\t\t\t\t\t\tauc = dat$AUC,\n",
        "\t\t\t\t\t\t\tor = (dat$TP[i] * dat$TN[i]) / (dat$FN[i] * dat$FP[i])))\n",
        "\t\n",
        "\t# merge the list into one table\n",
        "\toutput = rbindlist(output)\n",
        "\t\n",
        "\t# replace any NaN with NA\n",
        "\toutput = as.matrix(output)\n",
        "\toutput[is.nan(output)] = NA\n",
        "\toutput = data.table(output)\n",
        "\t\n",
        "\t# compute summary stats\n",
        "\toutput = output[, lapply(.SD, function(x) summary(na.omit(x)))]\n",
        "\toutput[, stat := c(\"Min\", \"Q1\", \"Median\", \"Mean\", \"Q3\", \"Max\")]\n",
        "\t\n",
        "\t# change data types\n",
        "\toutput[, acc := as.numeric(acc)]\n",
        "\toutput[, sens := as.numeric(sens)]\n",
        "\toutput[, spec := as.numeric(spec)]\n",
        "\toutput[, auc := as.numeric(auc)]\n",
        "\toutput[, or := as.numeric(or)]\n",
        "\toutput[, stat := factor(stat, levels = c(\"Min\", \"Q1\", \"Median\", \"Mean\", \"Q3\", \"Max\"))]\n",
        "\t\n",
        "\treturn(output)\n",
        "}\n",
        "\n",
        "}\n",
        "\n",
        "# -----------------------------------------------------------------------------------\n",
        "# ---- Prepare Data -----------------------------------------------------------------\n",
        "# -----------------------------------------------------------------------------------\n",
        "\n",
        "{\n",
        "\n",
        "# set work directory\n",
        "\n",
        "getwd()\n",
        "# setwd(\"F:/Documents/Working/Kaggle/Titanic\")\n",
        "\n",
        "# import the data\n",
        "\n",
        "train = data.table(read.csv(\"train.csv\", na.strings = \"\"))\n",
        "test = data.table(read.csv(\"test.csv\", na.strings = \"\"))\n",
        "\n",
        "# for column descriptions see: https://www.kaggle.com/c/titanic/data\n",
        "\n",
        "# ---- preparing train --------------------------------------------------------------\n",
        "\n",
        "{\n",
        "\n",
        "# lets check out train and update column data types\n",
        "\n",
        "train\n",
        "types(train)\n",
        "\n",
        "# update columns that should be treated as factors, not numbers\n",
        "\n",
        "train[, Survived := factor(Survived)]\n",
        "train[, Pclass := factor(Pclass)]\n",
        "\n",
        "# lets look at the levels of our factor columns\n",
        "\n",
        "levels(train$Survived)\n",
        "levels(train$Pclass)\n",
        "levels(train$Name)\n",
        "levels(train$Sex)\n",
        "levels(train$Ticket)\n",
        "levels(train$Cabin)\n",
        "levels(train$Embarked)\n",
        "\n",
        "# every PassengerId has their own Cabin\n",
        "# lets aggregating Cabin by its letter (ie. drop the number) so we can actually consider using it as a factor variable\n",
        "\n",
        "train[, Cabin := factor(gsub(\"[0-9]\", \"\", Cabin))]\n",
        "\n",
        "# lets also keep just one letter for each passenger\n",
        "\t# some passengers have mutliple cabins assigned to them\n",
        "\t# we'll just use the first cabin assignment\n",
        "\n",
        "train[, Cabin := factor(gsub(\" \", \"\", Cabin))]\n",
        "train[, Cabin := factor(substring(Cabin, 1, 1))]\n",
        "\n",
        "# verify that the levels of Cabin are just the letters\n",
        "\n",
        "levels(train$Cabin)\n",
        "\n",
        "# the level T looks weird, lets see how many times it appeared\n",
        "\t# its weird because it breaks the cabin lettering pattern by jumping from G to T\n",
        "\n",
        "train[Cabin == \"T\"]\n",
        "\n",
        "# it only appears once, so perhaps its a mistake\n",
        "# lets set it to NA and impute a value later\n",
        "\n",
        "train[Cabin == \"T\", Cabin := NA]\n",
        "train[, Cabin := droplevels(Cabin)]\n",
        "\n",
        "# lets check out if there are any missing values (NA's) in train\n",
        "\n",
        "aggr(train, numbers = TRUE, sortVars = TRUE, gap = 3, cex.axis = 0.8)\n",
        "\n",
        "# Cabin, Age, and Embarked have missing values\n",
        "# let impute the missing values, but before perform feature selection for Cabin, Age, and Embarked to find out which combination of variable are best for imputing missing values\n",
        "\n",
        "# ---- Cabin ------------------------------------------------------------------------\n",
        "\n",
        "{\n",
        "\n",
        "# create a copy of train\n",
        "\n",
        "mod.dat = data.table(train)\n",
        "\n",
        "# lets not include Name and Ticket becuase we cannot afford that many degrees of freedom (these are factors with a large number of levels)\n",
        "# lets not include PassengerId because that is an ID variable for each row, so it won't be helpful\n",
        "\n",
        "mod.dat[, c(\"Name\", \"Ticket\", \"PassengerId\") := NULL]\n",
        "\n",
        "# lets only keep the observations without missing values\n",
        "\n",
        "mod.dat = na.omit(mod.dat)\n",
        "\n",
        "# let look at the summary of mod.dat\n",
        "\n",
        "summary(mod.dat)\n",
        "\n",
        "# ---- removing highly correlated variables ----\n",
        "\n",
        "# extract all potential regressors\n",
        "cor.dat = data.table(mod.dat[,!\"Cabin\"])\n",
        "\n",
        "# reformat all columns to be numeric by creating dummy variables for factor columns\n",
        "cor.dat = data.table(model.matrix(~., cor.dat)[,-1])\n",
        "\n",
        "# compute correlations and plot them\n",
        "cors = cor(cor.dat)\n",
        "corrplot(cors, type = \"upper\", diag = FALSE, mar = c(2, 2, 2, 2), addshade = \"all\")\n",
        "\n",
        "# find out which regressors are highly correlated (>= 0.75) and remove them\n",
        "find.dat = findCorrelation(cors, cutoff = 0.75, names = TRUE)\n",
        "find.dat\n",
        "\n",
        "# no need to remove any columns from mod.dat according to find.dat\n",
        "\n",
        "# ---- ranking variables by importance ----\n",
        "\n",
        "# lets use cross validation to train the random forest model that will determine the importance of each variable\n",
        "\n",
        "# set up the cross validation mechanism\n",
        "cv = trainControl(method = \"repeatedcv\", number = K, repeats = B)\n",
        "\n",
        "# train our random forest model\n",
        "mod = train(Cabin ~ ., data = mod.dat, method = \"rf\", trControl = cv, importance = TRUE)\n",
        "\n",
        "# compute variable importance on a scale of 0 to 100\n",
        "imp = varImp(mod, scale = TRUE)$importance\n",
        "\n",
        "# transform imp into long format\n",
        "imp = data.table(regressor = rownames(imp), imp / 100)\n",
        "imp = melt(imp, id.vars = \"regressor\")\n",
        "\n",
        "# make regressor a factor for plotting purposes\n",
        "imp[, regressor := factor(regressor, levels = unique(regressor))]\n",
        "\n",
        "# plot a barplot of regressor importance\n",
        "\n",
        "ggplot(imp, aes(x = regressor, y = value, fill = value)) +\n",
        "geom_bar(stat = \"identity\", position = \"dodge\") +\n",
        "labs(x = \"Regressor\", y = \"Importance\") +\n",
        "scale_y_continuous(labels = percent) +\n",
        "scale_fill_gradient(low = \"yellow\", high = \"red\") +\n",
        "facet_wrap(~variable) +\n",
        "theme_dark(15) +\n",
        "theme(legend.position = \"none\", axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))\n",
        "\n",
        "# plot a barplot of mean regressor importance\n",
        "\n",
        "imp.avg = imp[, .(value = mean(value)), by = regressor]\n",
        "\n",
        "ggplot(imp.avg, aes(x = regressor, y = value, fill = value)) +\n",
        "geom_bar(stat = \"identity\", position = \"dodge\") +\n",
        "labs(x = \"Regressor\", y = \"Importance\") +\n",
        "scale_y_continuous(labels = percent) +\n",
        "scale_fill_gradient(low = \"yellow\", high = \"red\") +\n",
        "theme_dark(25) +\n",
        "theme(legend.position = \"none\")\n",
        "\n",
        "# there is no clear indication that any variables should be removed\n",
        "\n",
        "# ---- variable selection by recursive feature elimination ----\n",
        "\n",
        "# lets use cross validation to train the random forest model that will select our variables\n",
        "\n",
        "# set up the cross validation mechanism\n",
        "cv = rfeControl(functions = rfFuncs, method = \"repeatedcv\", number = K, repeats = B)\n",
        "\n",
        "# run the RFE algorithm\n",
        "vars = rfe(Cabin ~ ., data = mod.dat, sizes = 1:10, metric = \"Accuracy\", maximize = TRUE, rfeControl = cv)\n",
        "vars\n",
        "vars$optVariables\n",
        "\n",
        "# heres our regressors for cabin\n",
        "\n",
        "cabin.regressors = c(\"Fare\")\n",
        "\n",
        "# remove objects we no longer need\n",
        "\n",
        "rm(mod.dat, cors, cor.dat, find.dat, cv, imp, imp.avg, mod, vars)\n",
        "\n",
        "}\n",
        "\n",
        "# ---- Age --------------------------------------------------------------------------\n",
        "\n",
        "{\n",
        "\n",
        "# create a copy of train\n",
        "\n",
        "mod.dat = data.table(train)\n",
        "\n",
        "# lets not include Name and Ticket becuase we cannot afford that many degrees of freedom (these are factors with a large number of levels)\n",
        "# lets not include PassengerId because that is an ID variable for each row, so it won't be helpful\n",
        "# lets not include Cabin becuase of the large amount of missing values\n",
        "\n",
        "mod.dat[, c(\"Name\", \"Ticket\", \"Cabin\", \"PassengerId\") := NULL]\n",
        "\n",
        "# lets only keep the observations without missing values\n",
        "\n",
        "mod.dat = na.omit(mod.dat)\n",
        "\n",
        "# let look at the summary of mod.dat\n",
        "\n",
        "summary(mod.dat)\n",
        "\n",
        "# lets take the log of Age because it is a positive variable and the log function only accepts positive values\n",
        "# so this means we can impute the log(Age) and then transform the imputations back into the original Age units with exp()\n",
        "# this will ensure our final imputations are positive\n",
        "\n",
        "mod.dat[, log.Age := log(Age)]\n",
        "mod.dat[, Age := NULL]\n",
        "\n",
        "# ---- removing highly correlated variables ----\n",
        "\n",
        "# extract all potential regressors\n",
        "cor.dat = data.table(mod.dat[,!\"log.Age\"])\n",
        "\n",
        "# reformat all columns to be numeric by creating dummy variables for factor columns\n",
        "cor.dat = data.table(model.matrix(~., cor.dat)[,-1])\n",
        "\n",
        "# compute correlations and plot them\n",
        "cors = cor(cor.dat)\n",
        "corrplot(cors, type = \"upper\", diag = FALSE, mar = c(2, 2, 2, 2), addshade = \"all\")\n",
        "\n",
        "# find out which regressors are highly correlated (>= 0.75) and remove them\n",
        "find.dat = findCorrelation(cors, cutoff = 0.75, names = TRUE)\n",
        "find.dat\n",
        "\n",
        "# no need to remove any columns from mod.dat according to find.dat\n",
        "\n",
        "# ---- ranking variables by importance ----\n",
        "\n",
        "# lets use cross validation to train the random forest model that will determine the importance of each variable\n",
        "\n",
        "# set up the cross validation mechanism\n",
        "cv = trainControl(method = \"repeatedcv\", number = K, repeats = B)\n",
        "\n",
        "# train our random forest model\n",
        "mod = train(log.Age ~ ., data = mod.dat, method = \"rf\", trControl = cv, importance = TRUE)\n",
        "\n",
        "# compute variable importance on a scale of 0 to 100\n",
        "imp = varImp(mod, scale = TRUE)$importance\n",
        "\n",
        "# transform imp into a table\n",
        "imp = data.table(regressor = rownames(imp), imp / 100)\n",
        "\n",
        "# make regressor a factor for plotting purposes\n",
        "imp[, regressor := factor(regressor, levels = unique(regressor))]\n",
        "\n",
        "# plot a barplot of regressor importance\n",
        "\n",
        "ggplot(imp, aes(x = regressor, y = Overall, fill = Overall)) +\n",
        "geom_bar(stat = \"identity\", position = \"dodge\") +\n",
        "labs(x = \"Regressor\", y = \"Importance\") +\n",
        "scale_y_continuous(labels = percent) +\n",
        "scale_fill_gradient(low = \"yellow\", high = \"red\") +\n",
        "theme_dark(25) +\n",
        "theme(legend.position = \"none\")\n",
        "\n",
        "# based on this plots we can see that Embarked is the regressor that does not have enough importance\n",
        "# lets remove Embarked\n",
        "\n",
        "mod.dat[, Embarked := NULL]\n",
        "\n",
        "# ---- variable selection by recursive feature elimination ----\n",
        "\n",
        "# lets use cross validation to train the random forest model that will select our variables\n",
        "\n",
        "# set up the cross validation mechanism\n",
        "cv = rfeControl(functions = rfFuncs, method = \"repeatedcv\", number = K, repeats = B)\n",
        "\n",
        "# run the RFE algorithm\n",
        "vars = rfe(log.Age ~ ., data = mod.dat, sizes = 1:7, metric = \"RMSE\", maximize = FALSE, rfeControl = cv)\n",
        "vars\n",
        "vars$optVariables\n",
        "\n",
        "# heres our regressors for log.age\n",
        "\n",
        "log.age.regressors = c(\"Parch\", \"SibSp\", \"Survived\", \"Pclass\", \"Fare\", \"Sex\")\n",
        "\n",
        "# remove objects we no longer need\n",
        "\n",
        "rm(mod.dat, cors, cor.dat, find.dat, cv, imp, mod, vars)\n",
        "\n",
        "}\n",
        "\n",
        "# ---- Embarked ---------------------------------------------------------------------\n",
        "\n",
        "{\n",
        "\n",
        "# create a copy of train\n",
        "\n",
        "mod.dat = data.table(train)\n",
        "\n",
        "# lets not include Name and Ticket becuase we cannot afford that many degrees of freedom (these are factors with a large number of levels)\n",
        "# lets not include PassengerId because that is an ID variable for each row, so it won't be helpful\n",
        "# lets not include Cabin becuase of the large amount of missing values\n",
        "\n",
        "mod.dat[, c(\"Name\", \"Ticket\", \"Cabin\", \"PassengerId\") := NULL]\n",
        "\n",
        "# lets only keep the observations without missing values\n",
        "\n",
        "mod.dat = na.omit(mod.dat)\n",
        "\n",
        "# let look at the summary of mod.dat\n",
        "\n",
        "summary(mod.dat)\n",
        "\n",
        "# ---- removing highly correlated variables ----\n",
        "\n",
        "# extract all potential regressors\n",
        "cor.dat = data.table(mod.dat[,!\"Embarked\"])\n",
        "\n",
        "# reformat all columns to be numeric by creating dummy variables for factor columns\n",
        "cor.dat = data.table(model.matrix(~., cor.dat)[,-1])\n",
        "\n",
        "# compute correlations and plot them\n",
        "cors = cor(cor.dat)\n",
        "corrplot(cors, type = \"upper\", diag = FALSE, mar = c(2, 2, 2, 2), addshade = \"all\")\n",
        "\n",
        "# find out which regressors are highly correlated (>= 0.75) and remove them\n",
        "find.dat = findCorrelation(cors, cutoff = 0.75, names = TRUE)\n",
        "find.dat\n",
        "\n",
        "# no need to remove any columns from mod.dat according to find.dat\n",
        "\n",
        "# ---- ranking variables by importance ----\n",
        "\n",
        "# lets use cross validation to train the random forest model that will determine the importance of each variable\n",
        "\n",
        "# set up the cross validation mechanism\n",
        "cv = trainControl(method = \"repeatedcv\", number = K, repeats = B)\n",
        "\n",
        "# train our random forest model\n",
        "mod = train(Embarked ~ ., data = mod.dat, method = \"rf\", trControl = cv, importance = TRUE)\n",
        "\n",
        "# compute variable importance on a scale of 0 to 100\n",
        "imp = varImp(mod, scale = TRUE)$importance\n",
        "\n",
        "# transform imp into long format\n",
        "imp = data.table(regressor = rownames(imp), imp / 100)\n",
        "imp = melt(imp, id.vars = \"regressor\")\n",
        "\n",
        "# make regressor a factor for plotting purposes\n",
        "imp[, regressor := factor(regressor, levels = unique(regressor))]\n",
        "\n",
        "# plot a barplot of regressor importance\n",
        "\n",
        "ggplot(imp, aes(x = regressor, y = value, fill = value)) +\n",
        "geom_bar(stat = \"identity\", position = \"dodge\") +\n",
        "labs(x = \"Regressor\", y = \"Importance\") +\n",
        "scale_y_continuous(labels = percent) +\n",
        "scale_fill_gradient(low = \"yellow\", high = \"red\") +\n",
        "facet_wrap(~variable) +\n",
        "theme_dark(15) +\n",
        "theme(legend.position = \"none\", axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))\n",
        "\n",
        "# plot a barplot of mean regressor importance\n",
        "\n",
        "imp.avg = imp[, .(value = mean(value)), by = regressor]\n",
        "\n",
        "ggplot(imp.avg, aes(x = regressor, y = value, fill = value)) +\n",
        "geom_bar(stat = \"identity\", position = \"dodge\") +\n",
        "labs(x = \"Regressor\", y = \"Importance\") +\n",
        "scale_y_continuous(labels = percent) +\n",
        "scale_fill_gradient(low = \"yellow\", high = \"red\") +\n",
        "theme_dark(25) +\n",
        "theme(legend.position = \"none\")\n",
        "\n",
        "# there is no clear indication that any variables should be removed\n",
        "\n",
        "# ---- variable selection by recursive feature elimination ----\n",
        "\n",
        "# lets use cross validation to train the random forest model that will select our variables\n",
        "\n",
        "# set up the cross validation mechanism\n",
        "cv = rfeControl(functions = rfFuncs, method = \"repeatedcv\", number = K, repeats = B)\n",
        "\n",
        "# run the RFE algorithm\n",
        "vars = rfe(Embarked ~ ., data = mod.dat, sizes = 1:8, metric = \"Accuracy\", maximize = TRUE, rfeControl = cv)\n",
        "vars\n",
        "vars$optVariables\n",
        "\n",
        "# heres our regressors for embarked\n",
        "\n",
        "embark.regressors = c(\"Fare\")\n",
        "\n",
        "# remove objects we no longer need\n",
        "\n",
        "rm(mod.dat, cors, cor.dat, find.dat, cv, imp, imp.avg, mod, vars)\n",
        "\n",
        "}\n",
        "\n",
        "# ---- Imputations ------------------------------------------------------------------\n",
        "\n",
        "{\n",
        "\n",
        "# lets create a copy of train\n",
        "\n",
        "train.mice = data.table(train)\n",
        "\n",
        "# create log.Age\n",
        "\n",
        "train.mice[, log.Age := log(Age)]\n",
        "\n",
        "# lets create our predictor matrix that indicates what regressors to use\n",
        "# quickpred() sets up the default predictor matrix, then we can update it to what we want\n",
        "\n",
        "my.pred = quickpred(train.mice)\n",
        "my.pred\n",
        "\n",
        "# make adjustments\n",
        "\n",
        "my.pred[\"Cabin\",] = as.numeric(colnames(my.pred) %in% cabin.regressors)\n",
        "my.pred[\"Age\",] = 0\n",
        "my.pred[\"log.Age\",] = as.numeric(colnames(my.pred) %in% log.age.regressors)\n",
        "my.pred[\"Embarked\",] = as.numeric(colnames(my.pred) %in% embark.regressors)\n",
        "\n",
        "# verify adjustments\n",
        "\n",
        "my.pred\n",
        "\n",
        "# compute imputations\n",
        "\t# method = \"fastpmm\" uses predictive means matching to impute missing values (it's \"fast\" becuase it uses C++)\n",
        "\n",
        "imputations = mice(data = train.mice, predictorMatrix = my.pred, method = \"fastpmm\", seed = 42)\n",
        "\n",
        "# lets verify that the imputed cabin values follow the same relationships as the known cabin values\n",
        "\t# imputed values are orange\n",
        "\t# known values are blue\n",
        "\n",
        "xyplot(x = imputations, \n",
        "\t\t\tdata = Cabin ~ Fare, \n",
        "\t\t\talpha = 1/3, pch = 20, cex = 1, col = c(\"cornflowerblue\", \"tomato\"),\n",
        "\t\t\tjitter.x = TRUE, jitter.y = TRUE, factor = 1,\n",
        "\t\t\tscales = list(x = list(relation = \"free\")))\n",
        "\n",
        "# lets verify that the imputed log.age values follow the same relationships as the known age values\n",
        "\t# imputed values are orange\n",
        "\t# known values are blue\n",
        "\n",
        "xyplot(x = imputations, \n",
        "\t\t\tdata = log.Age ~ Parch + SibSp + Survived + Pclass + Fare + Sex, \n",
        "\t\t\talpha = 1/3, pch = 20, cex = 1, col = c(\"cornflowerblue\", \"tomato\"),\n",
        "\t\t\tjitter.x = TRUE, jitter.y = TRUE, factor = 1,\n",
        "\t\t\tscales = list(x = list(relation = \"free\")))\n",
        "\n",
        "# lets verify that the imputed embarked values follow the same relationships as the known embarked values\n",
        "\t# imputed values are orange\n",
        "\t# known values are blue\n",
        "\n",
        "xyplot(x = imputations, \n",
        "\t\t\tdata = Embarked ~ Fare, \n",
        "\t\t\talpha = 1/3, pch = 20, cex = 1, col = c(\"cornflowerblue\", \"tomato\"),\n",
        "\t\t\tjitter.x = TRUE, jitter.y = TRUE, factor = 2,\n",
        "\t\t\tscales = list(x = list(relation = \"free\")))\n",
        "\t\t\t\n",
        "# the spread for emabarked imputed values seems to be centered right at the mean of Fare which makes sense given the pmm method\n",
        "# normally this would be concerning but given that there are very few values of Embarked to impute, this relationship will suffice without much impact\n",
        "\n",
        "# lets extract all of the imputed data sets\n",
        "\n",
        "train.mice = data.table(complete(imputations, action = \"long\"))\n",
        "\n",
        "# lets only keep Cabin, log.Age, Embarked, and PassengerId \n",
        "# we are keeping PassengerId so we can aggregate Cabin, log.Age and Embarked across all imputations\n",
        "\n",
        "train.mice = train.mice[, .(PassengerId, Cabin, log.Age, Embarked)]\n",
        "\n",
        "# lets transform log.Age back into its original units\n",
        "\n",
        "train.mice[, Age := round(exp(log.Age), 2)]\n",
        "\n",
        "# lets take the most frequent Cabin value across all imputations\n",
        "# lets average Age across all imputations\n",
        "# lets take the most frequent Embarked value across all imputations\n",
        "\n",
        "train.mice = train.mice[, .(Cabin = names(sort(table(Cabin), decreasing = TRUE)[1]),\n",
        "\t\t\t\t\t\t\tAge = round(mean(Age), 2),\n",
        "\t\t\t\t\t\t\tEmbarked = names(sort(table(Embarked), decreasing = TRUE)[1])), \n",
        "\t\t\t\t\t\tby = PassengerId]\n",
        "\n",
        "# lets convert Cabin and Embarked back into factor data types\n",
        "\n",
        "train.mice[, Cabin := factor(Cabin)]\n",
        "train.mice[, Embarked := factor(Embarked)]\n",
        "\n",
        "# replace Cabin, Age, and Embarked in train with Cabin, Age, and Embarked in train.mice\n",
        "\n",
        "train[, Cabin := train.mice$Cabin]\n",
        "train[, Age := train.mice$Age]\n",
        "train[, Embarked := train.mice$Embarked]\n",
        "\n",
        "# lets remove objects we no longer need\n",
        "\n",
        "rm(imputations, my.pred, cabin.regressors, log.age.regressors, embark.regressors, train.mice)\n",
        "\n",
        "# lets verify that there are no missing values (NA's) in train\n",
        "\n",
        "aggr(train, numbers = TRUE, sortVars = TRUE, gap = 3, cex.axis = 0.8)\n",
        "\n",
        "}\n",
        "\n",
        "}\n",
        "\n",
        "# ---- preparing test ---------------------------------------------------------------\n",
        "\n",
        "{\n",
        "\n",
        "# lets check out test and update column data types\n",
        "\n",
        "test\n",
        "types(test)\n",
        "\n",
        "# update columns that should be treated as factors, not numbers\n",
        "\n",
        "test[, Pclass := factor(Pclass)]\n",
        "\n",
        "# lets look at the levels of our factor columns\n",
        "\n",
        "levels(test$Pclass)\n",
        "levels(test$Name)\n",
        "levels(test$Sex)\n",
        "levels(test$Ticket)\n",
        "levels(test$Cabin)\n",
        "levels(test$Embarked)\n",
        "\n",
        "# every PassengerId has their own Cabin\n",
        "# lets see if Cabin is useful by aggregating it by on its letter (ie. drop the number)\n",
        "\n",
        "test[, Cabin := factor(gsub(\"[0-9]\", \"\", Cabin))]\n",
        "\n",
        "# lets also keep just one letter for each passenger\n",
        "\n",
        "test[, Cabin := factor(gsub(\" \", \"\", Cabin))]\n",
        "test[, Cabin := factor(substring(Cabin, 1, 1))]\n",
        "\n",
        "# verify that the levels of Cabin are just the letters\n",
        "\n",
        "levels(test$Cabin)\n",
        "\n",
        "# lets check out if there are any missing values (NA's) in test\n",
        "\n",
        "aggr(test, numbers = TRUE, sortVars = TRUE, gap = 3, cex.axis = 0.8)\n",
        "\n",
        "# Cabin, Age, and Fare have missing values\n",
        "# let impute the missing data, but before doing that lets find a good combination of regressors that explain Cabin, Age, and Fare\n",
        "\n",
        "# ---- Cabin ------------------------------------------------------------------------\n",
        "\n",
        "{\n",
        "\n",
        "# create a copy of test\n",
        "\n",
        "mod.dat = data.table(test)\n",
        "\n",
        "# lets not include Name and Ticket becuase we cannot afford that many degrees of freedom (these are factors with a large number of levels)\n",
        "# lets not include PassengerId because that is an ID variable for each row, so it won't be helpful\n",
        "\n",
        "mod.dat[, c(\"Name\", \"Ticket\", \"PassengerId\") := NULL]\n",
        "\n",
        "# lets only keep the observations without missing values\n",
        "\n",
        "mod.dat = na.omit(mod.dat)\n",
        "\n",
        "# let look at the summary of mod.dat\n",
        "\n",
        "summary(mod.dat)\n",
        "\n",
        "# theres only 1 instance of Cabin == G and 1 instance of Embarked == Q\n",
        "# given these infrequencies lets remove them as levels for the sake of choosing regressors\n",
        "\n",
        "mod.dat = mod.dat[!(Cabin == \"G\" | Embarked == \"Q\")]\n",
        "mod.dat[, Cabin := droplevels(Cabin)]\n",
        "mod.dat[, Embarked := droplevels(Embarked)]\n",
        "\n",
        "# ---- removing highly correlated variables ----\n",
        "\n",
        "# extract all potential regressors\n",
        "cor.dat = data.table(mod.dat[,!\"Cabin\"])\n",
        "\n",
        "# reformat all columns to be numeric by creating dummy variables for factor columns\n",
        "cor.dat = data.table(model.matrix(~., cor.dat)[,-1])\n",
        "\n",
        "# compute correlations and plot them\n",
        "cors = cor(cor.dat)\n",
        "corrplot(cors, type = \"upper\", diag = FALSE, mar = c(2, 2, 2, 2), addshade = \"all\")\n",
        "\n",
        "# find out which regressors are highly correlated (>= 0.75) and remove them\n",
        "find.dat = findCorrelation(cors, cutoff = 0.75, names = TRUE)\n",
        "find.dat\n",
        "\n",
        "# no need to remove any columns from mod.dat according to find.dat\n",
        "\n",
        "# ---- ranking variables by importance ----\n",
        "\n",
        "# lets use cross validation to train the random forest model that will determine the importance of each variable\n",
        "\n",
        "# set up the cross validation mechanism\n",
        "cv = trainControl(method = \"repeatedcv\", number = K, repeats = B)\n",
        "\n",
        "# train our random forest model\n",
        "mod = train(Cabin ~ ., data = mod.dat, method = \"rf\", trControl = cv, importance = TRUE)\n",
        "\n",
        "# compute variable importance on a scale of 0 to 100\n",
        "imp = varImp(mod, scale = TRUE)$importance\n",
        "\n",
        "# transform imp into long format\n",
        "imp = data.table(regressor = rownames(imp), imp / 100)\n",
        "imp = melt(imp, id.vars = \"regressor\")\n",
        "\n",
        "# make regressor a factor for plotting purposes\n",
        "imp[, regressor := factor(regressor, levels = unique(regressor))]\n",
        "\n",
        "# plot a barplot of regressor importance\n",
        "\n",
        "ggplot(imp, aes(x = regressor, y = value, fill = value)) +\n",
        "geom_bar(stat = \"identity\", position = \"dodge\") +\n",
        "labs(x = \"Regressor\", y = \"Importance\") +\n",
        "scale_y_continuous(labels = percent) +\n",
        "scale_fill_gradient(low = \"yellow\", high = \"red\") +\n",
        "facet_wrap(~variable) +\n",
        "theme_dark(15) +\n",
        "theme(legend.position = \"none\", axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))\n",
        "\n",
        "# plot a barplot of mean regressor importance\n",
        "\n",
        "imp.avg = imp[, .(value = mean(value)), by = regressor]\n",
        "\n",
        "ggplot(imp.avg, aes(x = regressor, y = value, fill = value)) +\n",
        "geom_bar(stat = \"identity\", position = \"dodge\") +\n",
        "labs(x = \"Regressor\", y = \"Importance\") +\n",
        "scale_y_continuous(labels = percent) +\n",
        "scale_fill_gradient(low = \"yellow\", high = \"red\") +\n",
        "theme_dark(25) +\n",
        "theme(legend.position = \"none\")\n",
        "\n",
        "# based on these plots we can see that Sex doesn't seem important enough\n",
        "# lets remove Sex\n",
        "\n",
        "mod.dat[, Sex := NULL]\n",
        "\n",
        "# ---- variable selection by recursive feature elimination ----\n",
        "\n",
        "# lets use cross validation to train the random forest model that will select our variables\n",
        "\n",
        "# set up the cross validation mechanism\n",
        "cv = rfeControl(functions = rfFuncs, method = \"repeatedcv\", number = K, repeats = B)\n",
        "\n",
        "# run the RFE algorithm\n",
        "vars = rfe(Cabin ~ ., data = mod.dat, sizes = 1:7, metric = \"Accuracy\", maximize = TRUE, rfeControl = cv)\n",
        "vars\n",
        "vars$optVariables\n",
        "\n",
        "# heres our regressors for cabin\n",
        "\n",
        "cabin.regressors = c(\"Fare\")\n",
        "\n",
        "# remove objects we no longer need\n",
        "\n",
        "rm(mod.dat, cors, cor.dat, find.dat, cv, imp, imp.avg, mod, vars)\n",
        "\n",
        "}\n",
        "\n",
        "# ---- Age --------------------------------------------------------------------------\n",
        "\n",
        "{\n",
        "\n",
        "# create a copy of test\n",
        "\n",
        "mod.dat = data.table(test)\n",
        "\n",
        "# lets not include Name and Ticket becuase we cannot afford that many degrees of freedom (these are factors with a large number of levels)\n",
        "# lets not include PassengerId because that is an ID variable for each row, so it won't be helpful\n",
        "# lets not include Cabin becuase of the large amount of missing values\n",
        "\n",
        "mod.dat[, c(\"Name\", \"Ticket\", \"Cabin\", \"PassengerId\") := NULL]\n",
        "\n",
        "# lets only keep the observations without missing values\n",
        "\n",
        "mod.dat = na.omit(mod.dat)\n",
        "\n",
        "# let look at the summary of mod.dat\n",
        "\n",
        "summary(mod.dat)\n",
        "\n",
        "# lets take the log of Age because it is a positive variable and the log function only accepts positive values\n",
        "# so this means we can impute the log(Age) and then transform the imputations back into the original Age units with exp()\n",
        "# this will ensure our final imputations are positive\n",
        "\n",
        "mod.dat[, log.Age := log(Age)]\n",
        "mod.dat[, Age := NULL]\n",
        "\n",
        "# ---- removing highly correlated variables ----\n",
        "\n",
        "# extract all potential regressors\n",
        "cor.dat = data.table(mod.dat[,!\"log.Age\"])\n",
        "\n",
        "# reformat all columns to be numeric by creating dummy variables for factor columns\n",
        "cor.dat = data.table(model.matrix(~., cor.dat)[,-1])\n",
        "\n",
        "# compute correlations and plot them\n",
        "cors = cor(cor.dat)\n",
        "corrplot(cors, type = \"upper\", diag = FALSE, mar = c(2, 2, 2, 2), addshade = \"all\")\n",
        "\n",
        "# find out which regressors are highly correlated (>= 0.75) and remove them\n",
        "find.dat = findCorrelation(cors, cutoff = 0.75, names = TRUE)\n",
        "find.dat\n",
        "\n",
        "# no need to remove any columns from mod.dat according to find.dat\n",
        "\n",
        "# ---- ranking variables by importance ----\n",
        "\n",
        "# lets use cross validation to train the random forest model that will determine the importance of each variable\n",
        "\n",
        "# set up the cross validation mechanism\n",
        "cv = trainControl(method = \"repeatedcv\", number = K, repeats = B)\n",
        "\n",
        "# train our random forest model\n",
        "mod = train(log.Age ~ ., data = mod.dat, method = \"rf\", trControl = cv, importance = TRUE)\n",
        "\n",
        "# compute variable importance on a scale of 0 to 100\n",
        "imp = varImp(mod, scale = TRUE)$importance\n",
        "\n",
        "# transform imp into a table\n",
        "imp = data.table(regressor = rownames(imp), imp / 100)\n",
        "\n",
        "# make regressor a factor for plotting purposes\n",
        "imp[, regressor := factor(regressor, levels = unique(regressor))]\n",
        "\n",
        "# plot a barplot of regressor importance\n",
        "\n",
        "ggplot(imp, aes(x = regressor, y = Overall, fill = Overall)) +\n",
        "geom_bar(stat = \"identity\", position = \"dodge\") +\n",
        "labs(x = \"Regressor\", y = \"Importance\") +\n",
        "scale_y_continuous(labels = percent) +\n",
        "scale_fill_gradient(low = \"yellow\", high = \"red\") +\n",
        "theme_dark(25) +\n",
        "theme(legend.position = \"none\")\n",
        "\n",
        "# based on this plots we can see that Sex and Embarked are the regressor that does not have enough importance\n",
        "# SibSp doesn't look that good ethier but lets leave it up to recursive feature elimination to decide\n",
        "# lets remove Embarked\n",
        "\n",
        "mod.dat[, c(\"Sex\", \"Embarked\") := NULL]\n",
        "\n",
        "# ---- variable selection by recursive feature elimination ----\n",
        "\n",
        "# lets use cross validation to train the random forest model that will select our variables\n",
        "\n",
        "# set up the cross validation mechanism\n",
        "cv = rfeControl(functions = rfFuncs, method = \"repeatedcv\", number = K, repeats = B)\n",
        "\n",
        "# run the RFE algorithm\n",
        "vars = rfe(log.Age ~ ., data = mod.dat, sizes = 1:5, metric = \"RMSE\", maximize = FALSE, rfeControl = cv)\n",
        "vars\n",
        "vars$optVariables\n",
        "\n",
        "# heres our regressors for log.age\n",
        "\n",
        "log.age.regressors = c(\"Pclass\", \"Parch\", \"Fare\", \"SibSp\")\n",
        "\n",
        "# remove objects we no longer need\n",
        "\n",
        "rm(mod.dat, cors, cor.dat, find.dat, cv, imp, mod, vars)\n",
        "\n",
        "}\n",
        "\n",
        "# ---- Fare --------------------------------------------------------------------------\n",
        "\n",
        "{\n",
        "\n",
        "# create a copy of test\n",
        "\n",
        "mod.dat = data.table(test)\n",
        "\n",
        "# lets not include Name and Ticket becuase we cannot afford that many degrees of freedom (these are factors with a large number of levels)\n",
        "# lets not include PassengerId because that is an ID variable for each row, so it won't be helpful\n",
        "# lets not include Cabin becuase of the large amount of missing values\n",
        "\n",
        "mod.dat[, c(\"Name\", \"Ticket\", \"Cabin\", \"PassengerId\") := NULL]\n",
        "\n",
        "# lets only keep the observations without missing values\n",
        "\n",
        "mod.dat = na.omit(mod.dat)\n",
        "\n",
        "# let look at the summary of mod.dat\n",
        "\n",
        "summary(mod.dat)\n",
        "\n",
        "# lets take the log of (Fare + 1) because it is a positive variable and the log function only accepts positive values\n",
        "# so this means we can impute the log(Fare + 1) and then transform the imputations back into the original Fare units with exp()\n",
        "# this will ensure our final imputations are positive\n",
        "# the reason we are adding 1 to Fare is becuase it has a minimum value of 0, and log can only accept positive values\n",
        "\n",
        "mod.dat[, log.Fare.1 := log(Fare + 1)]\n",
        "mod.dat[, Fare := NULL]\n",
        "\n",
        "# ---- removing highly correlated variables ----\n",
        "\n",
        "# extract all potential regressors\n",
        "cor.dat = data.table(mod.dat[,!\"log.Fare.1\"])\n",
        "\n",
        "# reformat all columns to be numeric by creating dummy variables for factor columns\n",
        "cor.dat = data.table(model.matrix(~., cor.dat)[,-1])\n",
        "\n",
        "# compute correlations and plot them\n",
        "cors = cor(cor.dat)\n",
        "corrplot(cors, type = \"upper\", diag = FALSE, mar = c(2, 2, 2, 2), addshade = \"all\")\n",
        "\n",
        "# find out which regressors are highly correlated (>= 0.75) and remove them\n",
        "find.dat = findCorrelation(cors, cutoff = 0.75, names = TRUE)\n",
        "find.dat\n",
        "\n",
        "# no need to remove any columns from mod.dat according to find.dat\n",
        "\n",
        "# ---- ranking variables by importance ----\n",
        "\n",
        "# lets use cross validation to train the random forest model that will determine the importance of each variable\n",
        "\n",
        "# set up the cross validation mechanism\n",
        "cv = trainControl(method = \"repeatedcv\", number = K, repeats = B)\n",
        "\n",
        "# train our random forest model\n",
        "mod = train(log.Fare.1 ~ ., data = mod.dat, method = \"rf\", trControl = cv, importance = TRUE)\n",
        "\n",
        "# compute variable importance on a scale of 0 to 100\n",
        "imp = varImp(mod, scale = TRUE)$importance\n",
        "\n",
        "# transform imp into a table\n",
        "imp = data.table(regressor = rownames(imp), imp / 100)\n",
        "\n",
        "# make regressor a factor for plotting purposes\n",
        "imp[, regressor := factor(regressor, levels = unique(regressor))]\n",
        "\n",
        "# plot a barplot of regressor importance\n",
        "\n",
        "ggplot(imp, aes(x = regressor, y = Overall, fill = Overall)) +\n",
        "geom_bar(stat = \"identity\", position = \"dodge\") +\n",
        "labs(x = \"Regressor\", y = \"Importance\") +\n",
        "scale_y_continuous(labels = percent) +\n",
        "scale_fill_gradient(low = \"yellow\", high = \"red\") +\n",
        "theme_dark(25) +\n",
        "theme(legend.position = \"none\")\n",
        "\n",
        "# based on this plots we can see that Sex, Age, and Embarked are the regressors that do not have enough importance\n",
        "# lets remove Sex, Age, and Embarked\n",
        "\n",
        "mod.dat[, c(\"Sex\", \"Age\", \"Embarked\") := NULL]\n",
        "\n",
        "# ---- variable selection by recursive feature elimination ----\n",
        "\n",
        "# lets use cross validation to train the random forest model that will select our variables\n",
        "\n",
        "# set up the cross validation mechanism\n",
        "cv = rfeControl(functions = rfFuncs, method = \"repeatedcv\", number = K, repeats = B)\n",
        "\n",
        "# run the RFE algorithm\n",
        "vars = rfe(log.Fare.1 ~ ., data = mod.dat, sizes = 1:4, metric = \"RMSE\", maximize = FALSE, rfeControl = cv)\n",
        "vars\n",
        "vars$optVariables\n",
        "\n",
        "# heres our regressors for log.fare.1\n",
        "\n",
        "log.fare.1.regressors = c(\"Pclass\", \"SibSp\", \"Parch\")\n",
        "\n",
        "# remove objects we no longer need\n",
        "\n",
        "rm(mod.dat, cors, cor.dat, find.dat, cv, imp, mod, vars)\n",
        "\n",
        "}\n",
        "\n",
        "# ---- Imputations ------------------------------------------------------------------\n",
        "\n",
        "{\n",
        "\n",
        "# lets create a copy of test\n",
        "\n",
        "test.mice = data.table(test)\n",
        "\n",
        "# we will need to do log transformations on Age, and Fare\n",
        "\n",
        "test.mice[, log.Age := log(Age)]\n",
        "test.mice[, log.Fare.1 := log(Fare + 1)]\n",
        "\n",
        "# lets create our predictor matrix that indicates what regressors to use\n",
        "# quickpred() sets up the default predictor matrix, then we can update it to what we want\n",
        "\n",
        "my.pred = quickpred(test.mice)\n",
        "my.pred\n",
        "\n",
        "# make adjustments\n",
        "\n",
        "my.pred[\"Cabin\",] = as.numeric(colnames(my.pred) %in% cabin.regressors)\n",
        "my.pred[\"log.Age\",] = as.numeric(colnames(my.pred) %in% log.age.regressors)\n",
        "my.pred[\"Age\",] = 0\n",
        "my.pred[\"log.Fare.1\",] = as.numeric(colnames(my.pred) %in% log.fare.1.regressors)\n",
        "my.pred[\"Fare\",] = 0\n",
        "\n",
        "# verify adjustments\n",
        "\n",
        "my.pred\n",
        "\n",
        "# compute imputations\n",
        "\t# m denotes the number of imputations run\n",
        "\t# method = \"fastpmm\" is the predictive mean matching method which works for any data type\n",
        "\n",
        "imputations = mice(data = test.mice, predictorMatrix = my.pred, method = \"fastpmm\", seed = 42)\n",
        "\n",
        "# lets verify that the imputed cabin values follow the same relationships as the known cabin values\n",
        "\t# imputed values are orange\n",
        "\t# known values are blue\n",
        "\n",
        "xyplot(x = imputations, \n",
        "\t\t\tdata = Cabin ~ Fare, \n",
        "\t\t\talpha = 1/3, pch = 20, cex = 1, col = c(\"cornflowerblue\", \"tomato\"),\n",
        "\t\t\tjitter.x = TRUE, jitter.y = TRUE, factor = 1,\n",
        "\t\t\tscales = list(x = list(relation = \"free\")))\n",
        "\n",
        "# lets verify that the imputed age values follow the same relationships as the known age values\n",
        "\t# imputed values are orange\n",
        "\t# known values are blue\n",
        "\n",
        "xyplot(x = imputations, \n",
        "\t\t\tdata = log.Age ~ Pclass + Parch + Fare + SibSp, \n",
        "\t\t\talpha = 1/3, pch = 20, cex = 1, col = c(\"cornflowerblue\", \"tomato\"),\n",
        "\t\t\tjitter.x = TRUE, jitter.y = TRUE, factor = 1,\n",
        "\t\t\tscales = list(x = list(relation = \"free\")))\n",
        "\n",
        "# lets verify that the imputed Fare values follow the same relationships as the known Fare values\n",
        "\t# imputed values are orange\n",
        "\t# known values are blue\n",
        "\n",
        "xyplot(x = imputations, \n",
        "\t\t\tdata = log.Fare.1 ~ Pclass + SibSp + Parch, \n",
        "\t\t\talpha = 1/3, pch = 20, cex = 1, col = c(\"cornflowerblue\", \"tomato\"),\n",
        "\t\t\tjitter.x = TRUE, jitter.y = TRUE, factor = 2,\n",
        "\t\t\tscales = list(x = list(relation = \"free\")))\n",
        "\n",
        "# lets extract all of the imputed data sets\n",
        "\n",
        "test.mice = data.table(complete(imputations, action = \"long\"))\n",
        "\n",
        "# lets only keep Cabin, log.Age, log.Fare.1, and PassengerId \n",
        "# we are keeping PassengerId so we can aggregate Cabin, Age and Fare across all imputations\n",
        "\n",
        "test.mice = test.mice[, .(PassengerId, Cabin, log.Age, log.Fare.1)]\n",
        "\n",
        "# lets transform Age and Fare back into its original units\n",
        "\n",
        "test.mice[, Age := round(exp(log.Age), 2)]\n",
        "test.mice[, Fare := round(exp(log.Fare.1), 4) - 1]\n",
        "\n",
        "# lets take the most frequent Cabin value across all imputations\n",
        "# lets average Age across all imputations\n",
        "# lets average Fare across all imputations\n",
        "\n",
        "test.mice = test.mice[, .(Cabin = names(sort(table(Cabin), decreasing = TRUE)[1]),\n",
        "\t\t\t\t\t\t\tAge = round(mean(Age), 2),\n",
        "\t\t\t\t\t\t\tFare = round(mean(Fare), 4)), \n",
        "\t\t\t\t\t\tby = PassengerId]\n",
        "\n",
        "# lets convert Cabin and Fare back into factor data types\n",
        "\n",
        "test.mice[, Cabin := factor(Cabin)]\n",
        "\n",
        "# replace Cabin, Age, and Fare in test with Cabin, Age, and Fare in test.mice\n",
        "\n",
        "test[, Cabin := test.mice$Cabin]\n",
        "test[, Age := test.mice$Age]\n",
        "test[, Fare := test.mice$Fare]\n",
        "\n",
        "# lets remove objects we no longer need\n",
        "\n",
        "rm(imputations, my.pred, cabin.regressors, log.age.regressors, log.fare.1.regressors, test.mice)\n",
        "\n",
        "# lets verify that there are no missing values (NA's) in test\n",
        "\n",
        "aggr(test, numbers = TRUE, sortVars = TRUE, gap = 3, cex.axis = 0.8)\n",
        "\n",
        "}\n",
        "\n",
        "}\n",
        "\n",
        "}\n",
        "\n",
        "# -----------------------------------------------------------------------------------\n",
        "# ---- Plot Data --------------------------------------------------------------------\n",
        "# -----------------------------------------------------------------------------------\n",
        "\n",
        "{\n",
        "\n",
        "# ---- Transformations --------------------------------------------------------------\n",
        "\n",
        "{\n",
        "\n",
        "# lets produce boxplots of the numeric variables across the reponse variable Survived\n",
        "# lets also run t.tests to see if there is a significant difference in means of the numeric variables across the reponse variable Survived\n",
        "# we are looking to see if transformations on the numeric variables will create better seperation across Survived\n",
        "\n",
        "# ---- Age --------------------------------------------------------------------------\n",
        "\n",
        "p1 = ggplot(train, aes(x = Survived, y = Age, fill = Survived)) + \n",
        "\t\tgeom_boxplot(width = 0.5) +\n",
        "\t\ttheme_bw(base_size = 30) +\n",
        "\t\ttheme(legend.position = \"none\", plot.title = element_text(hjust = 0.5))\n",
        "\n",
        "p2 = ggplot(train, aes(x = Survived, y = log(Age), fill = Survived)) + \n",
        "\t\tgeom_boxplot(width = 0.5) +\n",
        "\t\ttheme_bw(base_size = 30) +\n",
        "\t\ttheme(legend.position = \"none\", plot.title = element_text(hjust = 0.5))\n",
        "\n",
        "# plot\n",
        "grid.arrange(p1, p2, nrow = 1)\n",
        "\n",
        "# test\n",
        "t.test(Age ~ Survived, data = train)\n",
        "t.test(log(Age) ~ Survived, data = train)\n",
        "\n",
        "# Age without transformation already provides significant seperation\n",
        "\n",
        "# ---- SibSp ------------------------------------------------------------------------\n",
        "\n",
        "p1 = ggplot(train, aes(x = Survived, y = SibSp, fill = Survived)) + \n",
        "\t\tgeom_boxplot(width = 0.5) +\n",
        "\t\ttheme_bw(base_size = 30) +\n",
        "\t\ttheme(legend.position = \"none\", plot.title = element_text(hjust = 0.5))\n",
        "\n",
        "p2 = ggplot(train, aes(x = Survived, y = SibSp^2, fill = Survived)) + \n",
        "\t\tgeom_boxplot(width = 0.5) +\n",
        "\t\ttheme_bw(base_size = 30) +\n",
        "\t\ttheme(legend.position = \"none\", plot.title = element_text(hjust = 0.5))\n",
        "\n",
        "# plot\n",
        "grid.arrange(p1, p2, nrow = 1)\n",
        "\n",
        "# test\n",
        "t.test(SibSp ~ Survived, data = train)\n",
        "t.test(SibSp^2 ~ Survived, data = train)\n",
        "\n",
        "# apply squared transformation\n",
        "train[, SibSp.2 := SibSp^2]\n",
        "\n",
        "# ---- Parch ------------------------------------------------------------------------\n",
        "\n",
        "p1 = ggplot(train, aes(x = Survived, y = Parch, fill = Survived)) + \n",
        "\t\tgeom_boxplot(width = 0.5) +\n",
        "\t\ttheme_bw(base_size = 30) +\n",
        "\t\ttheme(legend.position = \"none\", plot.title = element_text(hjust = 0.5))\n",
        "\n",
        "p2 = ggplot(train, aes(x = Survived, y = log(Parch + 1), fill = Survived)) + \n",
        "\t\tgeom_boxplot(width = 0.5) +\n",
        "\t\ttheme_bw(base_size = 30) +\n",
        "\t\ttheme(legend.position = \"none\", plot.title = element_text(hjust = 0.5))\n",
        "\n",
        "# plot\n",
        "grid.arrange(p1, p2, nrow = 1)\n",
        "\n",
        "# test\n",
        "t.test(Parch ~ Survived, data = train)\n",
        "t.test(log(Parch + 1) ~ Survived, data = train)\n",
        "\n",
        "# Parch without a transformation already provides significant seperation\n",
        "\n",
        "# ---- Fare -------------------------------------------------------------------------\n",
        "\n",
        "p1 = ggplot(train, aes(x = Survived, y = Fare, fill = Survived)) + \n",
        "\t\tgeom_boxplot(width = 0.5) +\n",
        "\t\ttheme_bw(base_size = 30) +\n",
        "\t\ttheme(legend.position = \"none\", plot.title = element_text(hjust = 0.5))\n",
        "\n",
        "p2 = ggplot(train, aes(x = Survived, y = log(Fare + 1), fill = Survived)) + \n",
        "\t\tgeom_boxplot(width = 0.5) +\n",
        "\t\ttheme_bw(base_size = 30) +\n",
        "\t\ttheme(legend.position = \"none\", plot.title = element_text(hjust = 0.5))\n",
        "\n",
        "# plot\n",
        "grid.arrange(p1, p2, nrow = 1)\n",
        "\n",
        "# test\n",
        "t.test(Fare ~ Survived, data = train)\n",
        "t.test(log(Fare + 1) ~ Survived, data = train)\n",
        "\n",
        "# Fare without a transformation already provides significant seperation\n",
        "\n",
        "# remove objects we no longer need\n",
        "\n",
        "rm(p1, p2)\n",
        "\n",
        "}\n",
        "\n",
        "# ---- Interactions -----------------------------------------------------------------\n",
        "\n",
        "{\n",
        "\n",
        "# ---- Importance -------------------------------------------------------------------\n",
        "\n",
        "{\n",
        "\n",
        "# create a copy of train\n",
        "\n",
        "mod.dat = data.table(train)\n",
        "\n",
        "# lets not include Name and Ticket becuase we cannot afford that many degrees of freedom (these are factors with a large number of levels)\n",
        "# lets not include PassengerId because that is an ID variable for each row, so it won't be helpful\n",
        "\n",
        "mod.dat[, c(\"Name\", \"Ticket\", \"PassengerId\") := NULL]\n",
        "\n",
        "# ---- removing highly correlated variables ----\n",
        "\n",
        "# extract all potential regressors\n",
        "cor.dat = data.table(mod.dat[,!\"Survived\"])\n",
        "\n",
        "# reformat all columns to be numeric by creating dummy variables for factor columns\n",
        "cor.dat = data.table(model.matrix(~.^2, cor.dat)[,-c(1:17)])\n",
        "# redifine mod.dat with just the two way interactions\n",
        "mod.dat = cbind(Survived = mod.dat$Survived, cor.dat)\n",
        "\n",
        "# compute correlations\n",
        "cors = cor(cor.dat)\n",
        "# make all NA correlations equal to 1\n",
        "cors[is.na(cors)] = 1\n",
        "# plot correlations\n",
        "corrplot(cors, type = \"upper\", diag = FALSE, mar = c(2, 2, 2, 2), addshade = \"all\")\n",
        "\n",
        "# find out which regressors are highly correlated (>= 0.75) and remove them\n",
        "find.dat = findCorrelation(cors, cutoff = 0.75, names = TRUE)\n",
        "find.dat\n",
        "\n",
        "# remove columns from mod.dat according to find.dat\n",
        "mod.dat = mod.dat[, !find.dat, with = FALSE]\n",
        "\n",
        "# ---- ranking variables by importance ----\n",
        "\n",
        "# lets use cross validation to train the random forest model that will determine the importance of each variable\n",
        "\n",
        "# set up the cross validation mechanism\n",
        "cv = trainControl(method = \"repeatedcv\", number = K, repeats = B)\n",
        "\n",
        "# train our random forest model\n",
        "mod = train(Survived ~ ., data = mod.dat, method = \"rf\", trControl = cv, importance = TRUE)\n",
        "\n",
        "# compute variable importance on a scale of 0 to 100\n",
        "imp = varImp(mod, scale = TRUE)$importance\n",
        "\n",
        "# transform imp into long format\n",
        "imp = data.table(regressor = rownames(imp), imp / 100)\n",
        "imp = melt(imp, id.vars = \"regressor\")\n",
        "\n",
        "# make regressor a factor for plotting purposes\n",
        "imp[, regressor := factor(regressor, levels = unique(regressor))]\n",
        "\n",
        "# plot a barplot of regressor importance\n",
        "\n",
        "ggplot(imp, aes(x = regressor, y = value, fill = value)) +\n",
        "geom_bar(stat = \"identity\", position = \"dodge\") +\n",
        "labs(x = \"Regressor\", y = \"Importance\") +\n",
        "scale_y_continuous(labels = percent) +\n",
        "scale_fill_gradient(low = \"yellow\", high = \"red\") +\n",
        "facet_wrap(~variable) +\n",
        "theme_dark(15) +\n",
        "theme(legend.position = \"none\", axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))\n",
        "\n",
        "# plot a barplot of mean regressor importance\n",
        "\n",
        "imp.avg = imp[, .(value = mean(value)), by = regressor]\n",
        "\n",
        "ggplot(imp.avg, aes(x = regressor, y = value, fill = value)) +\n",
        "geom_bar(stat = \"identity\", position = \"dodge\") +\n",
        "labs(x = \"Regressor\", y = \"Importance\") +\n",
        "scale_y_continuous(labels = percent) +\n",
        "scale_fill_gradient(low = \"yellow\", high = \"red\") +\n",
        "theme_dark(15) +\n",
        "theme(legend.position = \"none\", axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))\n",
        "\n",
        "# only keep variables with an importance greater than 40%\n",
        "\n",
        "keep.dat = c(\"Survived\", gsub(\"`\", \"\", imp.avg[value >= 0.4, regressor]))\n",
        "mod.dat = mod.dat[, keep.dat, with = FALSE]\n",
        "\n",
        "# ---- variable selection by recursive feature elimination ----\n",
        "\n",
        "# lets use cross validation to train the random forest model that will select our variables\n",
        "\n",
        "# set up the cross validation mechanism\n",
        "cv = rfeControl(functions = rfFuncs, method = \"repeatedcv\", number = K, repeats = B)\n",
        "\n",
        "# run the RFE algorithm\n",
        "vars = rfe(Survived ~ ., data = mod.dat, sizes = 1:(ncol(mod.dat) - 1), metric = \"Accuracy\", maximize = TRUE, rfeControl = cv)\n",
        "vars\n",
        "vars$optVariables\n",
        "\n",
        "# lets go with the 8 variable model becuase it has significantly less regressors with similar performance\n",
        "\n",
        "selectedVars = vars$variables\n",
        "bestVar = vars$control$functions$selectVar(selectedVars, 8)\n",
        "bestVar\n",
        "\n",
        "# here's our interactions\n",
        "\n",
        "survived.interactions = c(\"Sexmale:Age\", \"Pclass3:Age\", \"Sexmale:Fare\", \"Pclass3:Fare\", \"Age:Fare\", \"Pclass3:EmbarkedS\", \"Sexmale:EmbarkedS\", \"Fare:EmbarkedS\")\n",
        "\n",
        "# remove objects we no longer need\n",
        "\n",
        "rm(mod.dat, cors, cor.dat, find.dat, cv, imp, imp.avg, mod, vars, bestVar, keep.dat, selectedVars)\n",
        "\n",
        "}\n",
        "\n",
        "# ---- Plots ------------------------------------------------------------------------\n",
        "\n",
        "{\n",
        "\n",
        "# lets produce jitter plots of interactions of interest\n",
        "\n",
        "survived.interactions\n",
        "\n",
        "# Sexmale:Age\n",
        "\n",
        "ggplot(train, aes(x = Sex, y = cut(Age, breaks = hist(Age)$breaks), color = Survived)) + \n",
        "geom_jitter(alpha = 1/2, width = 1/4) +\n",
        "facet_wrap(~Survived) +\n",
        "theme_bw(base_size = 30) +\n",
        "theme(legend.position = \"none\", plot.title = element_text(hjust = 0.5)) +\n",
        "ylab(\"Age\")\n",
        "\n",
        "train[, Sexmale.Age := model.matrix(~Sex:Age, data = train)[,3]]\n",
        "\n",
        "# Pclass3:Age\n",
        "\n",
        "ggplot(train, aes(x = Pclass, y = cut(Age, breaks = hist(Age)$breaks), color = Survived)) + \n",
        "geom_jitter(alpha = 1/2, width = 1/4) +\n",
        "facet_wrap(~Survived) +\n",
        "theme_bw(base_size = 30) +\n",
        "theme(legend.position = \"none\", plot.title = element_text(hjust = 0.5)) +\n",
        "ylab(\"Age\")\n",
        "\n",
        "train[, Pclass3.Age := model.matrix(~Pclass:Age, data = train)[,4]]\n",
        "\n",
        "# Sexmale:Fare\n",
        "\n",
        "ggplot(train, aes(x = Sex, y = cut(Fare, breaks = c(-1e-10, hist(Fare)$breaks[-1])), color = Survived)) + \n",
        "geom_jitter(alpha = 1/2, width = 1/4) +\n",
        "facet_wrap(~Survived) +\n",
        "theme_bw(base_size = 30) +\n",
        "theme(legend.position = \"none\", plot.title = element_text(hjust = 0.5)) +\n",
        "ylab(\"Fare\")\n",
        "\n",
        "train[, Sexmale.Fare := model.matrix(~Sex:Fare, data = train)[,3]]\n",
        "\n",
        "# Pclass3:Fare\n",
        "\n",
        "ggplot(train, aes(x = Pclass, y = cut(Fare, breaks = c(-1e-10, hist(Fare)$breaks[-1])), color = Survived)) + \n",
        "geom_jitter(alpha = 1/2, width = 1/4) +\n",
        "facet_wrap(~Survived) +\n",
        "theme_bw(base_size = 30) +\n",
        "theme(legend.position = \"none\", plot.title = element_text(hjust = 0.5)) +\n",
        "ylab(\"Fare\")\n",
        "\n",
        "train[, Pclass3.Fare := model.matrix(~Pclass:Fare, data = train)[,4]]\n",
        "\n",
        "# Age:Fare\n",
        "\n",
        "ggplot(train, aes(x = cut(Age, breaks = hist(Age)$breaks), y = cut(Fare, breaks = c(-1e-10, hist(Fare)$breaks[-1])), color = Survived)) + \n",
        "geom_jitter(alpha = 1/2, width = 1/4) +\n",
        "facet_wrap(~Survived) +\n",
        "theme_bw(base_size = 20) +\n",
        "theme(legend.position = \"none\", plot.title = element_text(hjust = 0.5)) +\n",
        "labs(x = \"Age\", y = \"Fare\")\n",
        "\n",
        "# this doesn't appear as helpful\n",
        "# we won't include this interaction\n",
        "\n",
        "# Pclass3:EmbarkedS\n",
        "\n",
        "ggplot(train, aes(x = Pclass, y = Embarked, color = Survived)) + \n",
        "geom_jitter(alpha = 1/2, width = 1/4) +\n",
        "facet_wrap(~Survived) +\n",
        "theme_bw(base_size = 30) +\n",
        "theme(legend.position = \"none\", plot.title = element_text(hjust = 0.5))\n",
        "\n",
        "train[, Pclass3.EmbarkedS := model.matrix(~Pclass:Embarked, data = train)[,10]]\n",
        "\n",
        "# Sexmale:EmbarkedS\n",
        "\n",
        "ggplot(train, aes(x = Sex, y = Embarked, color = Survived)) + \n",
        "geom_jitter(alpha = 1/2, width = 1/4) +\n",
        "facet_wrap(~Survived) +\n",
        "theme_bw(base_size = 30) +\n",
        "theme(legend.position = \"none\", plot.title = element_text(hjust = 0.5))\n",
        "\n",
        "train[, Sexmale.EmbarkedS := model.matrix(~Sex:Embarked, data = train)[,7]]\n",
        "\n",
        "# Fare:EmbarkedS\n",
        "\n",
        "ggplot(train, aes(x = cut(Fare, breaks = c(-1e-10, hist(Fare)$breaks[-1])), y = Embarked, color = Survived)) + \n",
        "geom_jitter(alpha = 1/2, width = 1/4) +\n",
        "facet_wrap(~Survived) +\n",
        "theme_bw(base_size = 20) +\n",
        "theme(legend.position = \"none\", plot.title = element_text(hjust = 0.5)) +\n",
        "xlab(\"Fare\")\n",
        "\n",
        "# this doesn't appear as helpful\n",
        "# we won't include this interaction\n",
        "\n",
        "# remove interaction object\n",
        "\n",
        "rm(survived.interactions)\n",
        "\n",
        "}\n",
        "\n",
        "}\n",
        "\n",
        "}\n",
        "\n",
        "# -----------------------------------------------------------------------------------\n",
        "# ---- Variable Selection -----------------------------------------------------------\n",
        "# -----------------------------------------------------------------------------------\n",
        "\n",
        "{\n",
        "\n",
        "# build a copy of train for modeling\n",
        "\n",
        "mod.dat = data.table(train)\n",
        "\n",
        "# lets not include Name and Ticket becuase we cannot afford that many degrees of freedom (these are factors with a large number of levels)\n",
        "# lets not include PassengerId because that is an ID variable for each row, so it won't be helpful\n",
        "\n",
        "mod.dat[, c(\"Name\", \"Ticket\", \"PassengerId\") := NULL]\n",
        "\n",
        "# ---- removing highly correlated variables ----\n",
        "\n",
        "# extract all potential regressors\n",
        "cor.dat = data.table(mod.dat[,!\"Survived\"])\n",
        "\n",
        "# reformat all columns to be numeric by creating dummy variables for factor columns\n",
        "cor.dat = data.table(model.matrix(~., cor.dat)[,-1])\n",
        "\n",
        "# compute correlations and plot them\n",
        "cors = cor(cor.dat)\n",
        "corrplot(cors, type = \"upper\", diag = FALSE, mar = c(2, 2, 2, 2), addshade = \"all\")\n",
        "\n",
        "# find out which regressors are highly correlated (>= 0.75) and remove them\n",
        "find.dat = findCorrelation(cors, cutoff = 0.75, names = TRUE)\n",
        "find.dat\n",
        "\n",
        "# remove columns from mod.dat according to find.dat\n",
        "\n",
        "mod.dat = cbind(Survived = mod.dat$Survived, cor.dat)\n",
        "mod.dat[, c(\"Pclass3\", \"Sexmale.Age\", \"SibSp\") := NULL]\n",
        "\n",
        "# ---- ranking variables by importance ----\n",
        "\n",
        "# lets use cross validation to train the random forest model that will determine the importance of each variable\n",
        "\n",
        "# set up the cross validation mechanism\n",
        "cv = trainControl(method = \"repeatedcv\", number = K, repeats = B)\n",
        "\n",
        "# train our random forest model\n",
        "mod = train(Survived ~ ., data = mod.dat, method = \"rf\", trControl = cv, importance = TRUE)\n",
        "\n",
        "# compute variable importance on a scale of 0 to 100\n",
        "imp = varImp(mod, scale = TRUE)$importance\n",
        "\n",
        "# transform imp into long format\n",
        "imp = data.table(regressor = rownames(imp), imp / 100)\n",
        "imp = melt(imp, id.vars = \"regressor\")\n",
        "\n",
        "# make regressor a factor for plotting purposes\n",
        "imp[, regressor := factor(regressor, levels = unique(regressor))]\n",
        "\n",
        "# plot a barplot of regressor importance\n",
        "\n",
        "ggplot(imp, aes(x = regressor, y = value, fill = value)) +\n",
        "geom_bar(stat = \"identity\", position = \"dodge\") +\n",
        "labs(x = \"Regressor\", y = \"Importance\") +\n",
        "scale_y_continuous(labels = percent) +\n",
        "scale_fill_gradient(low = \"yellow\", high = \"red\") +\n",
        "facet_wrap(~variable) +\n",
        "theme_dark(15) +\n",
        "theme(legend.position = \"none\", axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))\n",
        "\n",
        "# plot a barplot of mean regressor importance\n",
        "\n",
        "imp.avg = imp[, .(value = mean(value)), by = regressor]\n",
        "\n",
        "ggplot(imp.avg, aes(x = regressor, y = value, fill = value)) +\n",
        "geom_bar(stat = \"identity\", position = \"dodge\") +\n",
        "labs(x = \"Regressor\", y = \"Importance\") +\n",
        "scale_y_continuous(labels = percent) +\n",
        "scale_fill_gradient(low = \"yellow\", high = \"red\") +\n",
        "theme_dark(20) +\n",
        "theme(legend.position = \"none\", axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))\n",
        "\n",
        "# only keep variables with an importance greater than 40%\n",
        "\n",
        "keep.dat = c(\"Survived\", gsub(\"`\", \"\", imp.avg[value >= 0.4, regressor]))\n",
        "mod.dat = mod.dat[, keep.dat, with = FALSE]\n",
        "\n",
        "# ---- variable selection by recursive feature elimination ----\n",
        "\n",
        "# lets use cross validation to train the random forest model that will select our variables\n",
        "\n",
        "# set up the cross validation mechanism\n",
        "cv = rfeControl(functions = rfFuncs, method = \"repeatedcv\", number = K, repeats = B)\n",
        "\n",
        "# run the RFE algorithm\n",
        "vars = rfe(Survived ~ ., data = mod.dat, sizes = 1:(ncol(mod.dat) - 1), metric = \"Accuracy\", maximize = TRUE, rfeControl = cv)\n",
        "vars\n",
        "vars$optVariables\n",
        "\n",
        "# heres our regressors for Survived\n",
        "\n",
        "survived.regressors = c(\"Sexmale\", \"Sexmale.Fare\", \"Pclass3.Age\", \"Age\", \"Fare\", \"Pclass3.Fare\", \"Sexmale.EmbarkedS\", \"Pclass3.EmbarkedS\", \"SibSp.2\")\n",
        "\n",
        "# lets remove all columns from train not in survived.regressors\n",
        "\n",
        "train = mod.dat[, c(\"Survived\", survived.regressors), with = FALSE]\n",
        "\n",
        "# lets update test so it has the same regressors as train\n",
        "\n",
        "mod.dat = data.table(test)\n",
        "\n",
        "# lets not include Name and Ticket becuase we cannot afford that many degrees of freedom (these are factors with a large number of levels)\n",
        "# lets not include PassengerId because that is an ID variable for each row, so it won't be helpful\n",
        "\n",
        "mod.dat[, c(\"Name\", \"Ticket\", \"PassengerId\") := NULL]\n",
        "\n",
        "# create all terms up to two-way interactions\n",
        "\n",
        "mod.dat = data.table(model.matrix(~ .^2, mod.dat)[,-1])\n",
        "\n",
        "# rename column names to have \".\" instead of \":\"\n",
        "\n",
        "name = gsub(\":\", \".\", names(mod.dat))\n",
        "setnames(mod.dat, name)\n",
        "\n",
        "# add transformation term to test\n",
        "\n",
        "mod.dat[, SibSp.2 := SibSp^2]\n",
        "\n",
        "# lets remove all columns from test not in survived.regressors\n",
        "\n",
        "test = mod.dat[, survived.regressors, with = FALSE]\n",
        "\n",
        "# remove objects we no longer need\n",
        "\n",
        "rm(keep.dat, name, mod.dat, cors, cor.dat, find.dat, cv, imp, imp.avg, mod, vars, survived.regressors)\n",
        "\n",
        "}\n",
        "\n",
        "# -----------------------------------------------------------------------------------\n",
        "# ---- Logistic Regression Model ----------------------------------------------------\n",
        "# -----------------------------------------------------------------------------------\n",
        "\n",
        "{\n",
        "\n",
        "# ---- Set Up -----------------------------------------------------------------------\n",
        "\n",
        "# lets build a logistic regression model with all terms\n",
        "# we are trying to see if we can remove any non-significant regressors\n",
        "\n",
        "log.mod.all = glm(Survived ~ ., data = train, \n",
        "\t\t\t\tfamily = binomial(link = \"logit\"), \n",
        "\t\t\t\tcontrol = list(maxit = 100))\n",
        "\n",
        "summary(log.mod.all)\n",
        "\n",
        "# remove Sexmale.EmbarkedS\n",
        "\n",
        "log.mod.trim = glm(Survived ~ . - Sexmale.EmbarkedS, data = train, \n",
        "\t\t\t\t\tfamily = binomial(link = \"logit\"), \n",
        "\t\t\t\t\tcontrol = list(maxit = 100))\n",
        "\n",
        "summary(log.mod.trim)\n",
        "\n",
        "# remove Pclass3.Fare\n",
        "\n",
        "log.mod.trim = glm(Survived ~ . - Sexmale.EmbarkedS - Pclass3.Fare, data = train, \n",
        "\t\t\t\t\tfamily = binomial(link = \"logit\"), \n",
        "\t\t\t\t\tcontrol = list(maxit = 100))\n",
        "\n",
        "summary(log.mod.trim)\n",
        "\n",
        "# ---- ROC --------------------------------------------------------------------------\n",
        "\n",
        "# create names for each model\n",
        "mod.name = c(\"all\", \"trim\")\n",
        "\n",
        "# lets compare the ROC curves of log.mod.all and log.mod.trim to determine the right cutoff point for each model\n",
        "\t# here are some useful definitions:\n",
        "\t\t# threshold - the point at which to choose 0 or 1 for the prediction (ie. cutoff point)\n",
        "\t\t\t# say threshold = .2, then any prediction >= 0.2 is accepted as 1 otherwise the prediction is accepted as 0\n",
        "\t\t# sensitivities - the proportion of observations that were correctly labeled as 1 (ie. True Positive Rate)\n",
        "\t\t# specificities - the proportion of observations that were correctly labeled as 0 (ie. True Negative Rate)\n",
        "\t\t# cases - the total amount of 1's in the data\n",
        "\t\t# controls - the total amount of 0's in the data\n",
        "\t\t# AUC - area under the curve - average value of sensitivity for all possible values of specificity, and vis versa\n",
        "\n",
        "# extract actual values\n",
        "actuals = as.numeric(as.character(train$Survived))\n",
        "\n",
        "# compute fitted values of each model\n",
        "fits = list(fitted(log.mod.all), fitted(log.mod.trim))\n",
        "\n",
        "# compute an roc object for each model\n",
        "rocs = lapply(1:length(fits), function(i) roc(actuals ~ fits[[i]]))\n",
        "\n",
        "# compute cutoffs to evaluate\n",
        "cutoffs = c(-Inf, seq(0.01, 0.99, 0.01), Inf)\n",
        "\n",
        "# compute roc curves for each model\n",
        "rocs.cutoffs = lapply(1:length(fits), function(i) data.table(t(coords(rocs[[i]], x = cutoffs, input = \"threshold\", ret = c(\"threshold\", \"specificity\", \"sensitivity\", \"accuracy\")))))\n",
        "\n",
        "# aggregate results into one table\n",
        "DT = rbindlist(lapply(1:length(fits), function(i) data.table(mod = mod.name[i], \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tcutoff = rocs.cutoffs[[i]]$threshold, \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tTPR = rocs.cutoffs[[i]]$sensitivity, \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tFPR = 1 - rocs.cutoffs[[i]]$specificity, \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tACC = rocs.cutoffs[[i]]$accuracy)))\n",
        "\n",
        "# make mod into a factor data type to facilitate plotting\n",
        "DT[, mod := factor(mod)]\n",
        "\n",
        "# compute cut-offs points that maximize accuracy\n",
        "\t# accuracy is the total number of correct predictions\n",
        "# these cuts are based on two weights (ie. two entries in the vector 'best.weights')\n",
        "\t# entry 1: the relative cost of of a false negative classification (as compared with a false positive classification)\n",
        "\t\t# default value = 1 (ie. both types of false classifications are equally bad)\n",
        "\t# entry 2: the prevalence, or the proportion of cases in the population (n.cases/(n.controls + n.cases))\n",
        "\t\t# default value = 0.5 (ie. there are expeted to be an equal number of 0's and 1's)\n",
        "\t\t\n",
        "cuts = as.numeric(sapply(1:length(fits), function(i) \n",
        "\t\t\t\t\t\t\tcoords(rocs[[i]], \n",
        "\t\t\t\t\t\t\t\t\tx = \"best\", \n",
        "\t\t\t\t\t\t\t\t\tbest.weights = c(1, length(rocs[[i]]$cases) / length(rocs[[i]]$response)))[1]))\n",
        "\n",
        "# plot ROC curves\n",
        "\n",
        "roc.plot = ggplot(DT, aes(x = FPR, y = TPR, group = mod, color = mod)) +\n",
        "\t\t\tgeom_line(size = 1) +\n",
        "\t\t\tgeom_abline(slope = 1, size = 1) +\n",
        "\t\t\tscale_y_continuous(labels = percent) +\n",
        "\t\t\tscale_x_continuous(labels = percent) +\n",
        "\t\t\tlabs(x = \"False Positive\", y = \"True Positive\", color = \"Model\") + \n",
        "\t\t\ttheme_bw(base_size = 25) +\n",
        "\t\t\ttheme(legend.position = \"top\", legend.key.size = unit(.25, \"in\"), plot.title = element_text(hjust = 0.5)) +\n",
        "\t\t\tguides(color = guide_legend(override.aes = list(size = 10, linetype = 1), nrow = 1))\n",
        "\n",
        "roc.plot\n",
        "\n",
        "# plot the accuracy curves\n",
        "\t# the dashed lines indicate the cut-off for each model that maximizes accuracy\n",
        "\t\n",
        "acc.plot = ggplot(DT, aes(x = cutoff, y = ACC, group = mod, color = mod)) +\n",
        "\t\t\tgeom_line(size = 1) +\n",
        "\t\t\tgeom_vline(xintercept = cuts, color = ggcolor(length(cuts)), size = 1, linetype = \"dashed\") +\n",
        "\t\t\tscale_y_continuous(labels = percent) +\n",
        "\t\t\tscale_x_continuous(labels = percent) +\n",
        "\t\t\tlabs(x = \"Cut-Off\", y = \"Accuracy\", color = \"Model\") + \n",
        "\t\t\ttheme_bw(base_size = 25) +\n",
        "\t\t\ttheme(legend.position = \"top\", legend.key.size = unit(.25, \"in\"), plot.title = element_text(hjust = 0.5)) +\n",
        "\t\t\tguides(color = guide_legend(override.aes = list(size = 10, linetype = 1), nrow = 1))\n",
        "\n",
        "acc.plot\n",
        "\n",
        "# ---- CV ---------------------------------------------------------------------------\n",
        "\n",
        "# lets use cross validation to choose which model is best\n",
        "\n",
        "# two definitions:\n",
        "\t# positive ~ a passenger survives\n",
        "\t# negative ~ a passenger doesn't survive\n",
        "\n",
        "# for each iteration in the K-fold validation we will compute the following metrics\n",
        "\t# acc ~ percent of correct predictions\n",
        "\t# sens ~ percent of correct positive predictions out of all observations that are truely positive\n",
        "\t# spec ~ percent of correct negative predictions out of all observations that are truely negative\n",
        "\t# ppv ~ percent of correct positive predictions out of all observations that were predicted to be positive\n",
        "\t# npv ~ percent of correct negative predictions out of all observations that were predicted to be negative\n",
        "\t# or ~ likelihood of being correct over the likelihood of being incorrect\n",
        "\t\t# (ie. we are 'or' times more likely to be correct than incorrect)\n",
        "\n",
        "# build a function that will report prediction results of our models\n",
        "\n",
        "log.pred = function(Xtrain, Ytrain, Xtest, Ytest, negative, cutoff)\n",
        "{\n",
        "\t# build the table for training the model\n",
        "\tdat = data.table(Xtrain)\n",
        "\tdat[, y := Ytrain]\n",
        "\t\n",
        "\t# build the training model\n",
        "\tmod = glm(y ~ ., data = dat, \n",
        "\t\t\t\tfamily = binomial(link = \"logit\"), \n",
        "\t\t\t\tcontrol = list(maxit = 100))\n",
        "\t\n",
        "\t# make predictions with the training model using the test set\n",
        "\tynew = predict(mod, data.table(Xtest), type = \"response\")\n",
        "\tynew = as.numeric(ynew >= cutoff)\n",
        "\tYtest = as.numeric(as.character(Ytest))\n",
        "\t\n",
        "\t# build a confusion matrix to summarize the performance of our training model\n",
        "\toutput = confusionMatrix(Ytest, ynew, negative = negative)\n",
        "\toutput = c(output, \"AUC\" = as.numeric(auc(roc(Ytest ~ ynew))))\n",
        "\t\n",
        "\t# the confusion matrix is the output\n",
        "\treturn(output)\n",
        "}\n",
        "\n",
        "# compute diagnostic errors for each model\n",
        "\n",
        "log.diag = foreach(i = 1:2) %do%\n",
        "{\n",
        "\t# extract our predictors (X)\n",
        "\t\n",
        "\tif(i == 1)\n",
        "\t{\n",
        "\t\tX = data.table(train)\n",
        "\t\t\n",
        "\t} else\n",
        "\t{\n",
        "\t\tX = data.table(train[,!c(\"Sexmale.EmbarkedS\", \"Pclass3.Fare\")])\n",
        "\t} \n",
        "\t\n",
        "\t# extract our response (Y)\n",
        "\t\n",
        "\tY = X$Survived\n",
        "\tX[, Survived := NULL]\n",
        "\t\n",
        "\t# perform cross validation\n",
        "\t\n",
        "\tlog.cv = crossval(predfun = log.pred, X = X, Y = Y, K = K, B = B, \n",
        "\t\t\t\t\t\tverbose = FALSE, cutoff = cuts[i], negative = 0)\n",
        "\n",
        "\t# compute diagnostic errors of cross validation\n",
        "\n",
        "\toutput = diag.cv(log.cv)\n",
        "\toutput[, mod := mod.name[i]]\n",
        "\t\n",
        "\treturn(output)\n",
        "}\n",
        "\n",
        "# combine the list of tables into one table\n",
        "\n",
        "log.diag = rbindlist(log.diag)\n",
        "\n",
        "# convert log.diag into long format for plotting purposes\n",
        "\n",
        "DT = data.table(melt(log.diag, id.vars = c(\"stat\", \"mod\")))\n",
        "\n",
        "# convert mod into a factor for plotting purposes\n",
        "\n",
        "DT[, mod := factor(mod)]\n",
        "\n",
        "# remove Inf values as these don't help\n",
        "\n",
        "DT = data.table(DT[value < Inf])\n",
        "\n",
        "# plot barplots of each diagnostic metric\n",
        "\n",
        "ggplot(DT[stat == \"Q1\" | stat == \"Median\" | stat == \"Q3\"], aes(x = stat, y = value, group = mod, fill = mod)) +\n",
        "geom_bar(stat = \"identity\", position = \"dodge\") +\n",
        "labs(x = \"Summary Statistic\", y = \"Value\", fill = \"Model\") + \n",
        "facet_wrap(~variable, scales = \"free_y\") +\n",
        "theme_bw(base_size = 15) +\n",
        "theme(legend.position = \"top\", legend.key.size = unit(.25, \"in\"), plot.title = element_text(hjust = 0.5)) +\n",
        "guides(fill = guide_legend(override.aes = list(size = 10, linetype = 1), nrow = 1))\n",
        "\n",
        "# ---- Results ----------------------------------------------------------------------\n",
        "\n",
        "# in the roc and accuracy plots both models are very similar\n",
        "# in the diagnostics barplot, log.mod.trim is slightly higher across most metrics\n",
        "\n",
        "names(log.mod.trim$coefficients)\n",
        "\n",
        "log.mod = glm(formula = Survived ~ Sexmale + Sexmale.Fare + Pclass3.Age + Age + Fare + Pclass3.EmbarkedS + SibSp.2, \n",
        "\t\t\t\tdata = train, family = binomial(link = \"logit\"), control = list(maxit = 100))\n",
        "\n",
        "log.cutoff = cuts[2]\n",
        "\n",
        "# only keep mod == trim and change it to log as this is our chosen logistic regression model\n",
        "\n",
        "log.diag = log.diag[mod == \"trim\"]\n",
        "log.diag[, mod := rep(\"log\", nrow(log.diag))]\n",
        "\n",
        "# store model diagnostic results\n",
        "\n",
        "mods.diag = data.table(log.diag)\n",
        "\n",
        "# store the model\n",
        "\n",
        "log.list = list(\"mod\" = log.mod, \"cutoff\" = log.cutoff)\n",
        "mods.list = list(\"log\" = log.list)\n",
        "\n",
        "# remove objects we no longer need\n",
        "\n",
        "rm(actuals, cuts, DT, fits, i, roc.plot, acc.plot,\n",
        "\tlog.cutoff, log.cv, log.diag, log.list, log.mod, log.mod.all, log.mod.trim, \n",
        "\tlog.pred, mod.name, output, rocs, X, Y, rocs.cutoffs, cutoffs)\n",
        "\n",
        "}\n",
        "\n",
        "# -----------------------------------------------------------------------------------\n",
        "# ---- Penalty Regression Model -----------------------------------------------------\n",
        "# -----------------------------------------------------------------------------------\n",
        "\n",
        "{\n",
        "\n",
        "# Ridge Regression differs from least squares regression by penalizing the objective function (Min: RSS) by adding the product of a constant (lambda2) and the sum of the squared coefficients (the L-2 norm)\n",
        "# so the objective function for Ridge Regression is (Min: RSS + lambda2 * L-2)\n",
        "# this penalty factor is used to discourage overfitting by incentivizing the model to:\n",
        "\t# choose coefficients that aren't too large\n",
        "\t# shrink coefficients that would be small, close to zero\n",
        "# glmnet offers ridge regression by setting the parameter alpha = 0\n",
        "\n",
        "# Lasso Regression differs from least squares regression by penalizing the objective function (Min: RSS) by adding the product of a constant (lambda1) and the sum of the absolute coefficients (the L-1 norm)\n",
        "# so the objective function for Ridge Regression is (Min: RSS + lambda1 * L-1)\n",
        "# this penalty factor is used to discourage overfitting by incentivizing the model to:\n",
        "\t# choose coefficients that aren't too large\n",
        "\t# shrink coefficients that would be small, to zero\n",
        "# glmnet offers lasso regression by setting the parameter alpha = 1\n",
        "\n",
        "# Elastic Net Regression differs from least squares regression by penalizing the objective function (Min: RSS) by adding the Ridge Regression and Lasso Regression penalty factors\n",
        "# so the objective function for Ridge Regression is (Min: RSS + lambda2 * L-2 + lambda1 * L-1)\n",
        "# the glmnet package uses the parameters alpha and lambda, where:\n",
        "\t# lambda = lambda1 + lambda2\n",
        "\t# alpha = lambda1 / lambda\n",
        "# the glmnet package offers cross validation to optimize lambda for a given alpha\n",
        "# alpha takes on values between 0 and 1 so we will try various alpha values and use cross validation to see which alpha yeilds the best model\n",
        "\n",
        "# ---- Set Up -----------------------------------------------------------------------\n",
        "\n",
        "# extract predictors (X) and response (Y)\n",
        "# lets use the regressors from the log.mod.all and log.mod.trim models and see which set of variables performs better\n",
        "\n",
        "X.all = data.table(train[,!\"Survived\"])\n",
        "X.trim = data.table(X.all[, !c(\"Sexmale.EmbarkedS\", \"Pclass3.Fare\")])\n",
        "X.all = as.matrix(X.all)\n",
        "X.trim = as.matrix(X.trim)\n",
        "Y = train$Survived\n",
        "\n",
        "# build a sequence of alpha values to test\n",
        "\n",
        "doe = data.table(expand.grid(name = c(\"all\", \"trim\"), alpha = seq(0, 1, 0.1)))\n",
        "\n",
        "# build each model in doe\n",
        "\n",
        "mods = foreach(i = 1:nrow(doe)) %do%\n",
        "{\n",
        "\t# extract our predictors (X)\n",
        "\t\n",
        "\tif(doe$name[i] == \"all\")\n",
        "\t{\n",
        "\t\tX = X.all\n",
        "\t\t\n",
        "\t} else\n",
        "\t{\n",
        "\t\tX = X.trim\n",
        "\t}\n",
        "\t\n",
        "\toutput = cv.glmnet(x = X, y = Y, family = \"binomial\", alpha = doe$alpha[i])\n",
        "\treturn(output)\n",
        "}\n",
        "\n",
        "# ---- ROC --------------------------------------------------------------------------\n",
        "\n",
        "# create names for each model\n",
        "mod.name = 1:nrow(doe)\n",
        "\n",
        "# extract actual values\n",
        "actuals = as.numeric(as.character(Y))\n",
        "\n",
        "# compute fitted values of each model\n",
        "fits = foreach(i = 1:length(mods)) %do% \n",
        "{\n",
        "\t# extract our predictors (X)\n",
        "\t\n",
        "\tif(doe$name[i] == \"all\")\n",
        "\t{\n",
        "\t\tX = X.all\n",
        "\t\t\n",
        "\t} else\n",
        "\t{\n",
        "\t\tX = X.trim\n",
        "\t}\n",
        "\t\n",
        "\toutput = as.numeric(predict(mods[[i]], s = mods[[i]]$lambda.min, X, type = \"response\"))\n",
        "\treturn(output)\n",
        "}\n",
        "\n",
        "# compute an roc object for each model\n",
        "rocs = lapply(1:length(fits), function(i) roc(actuals ~ fits[[i]]))\n",
        "\n",
        "# compute cutoffs to evaluate\n",
        "cutoffs = c(-Inf, seq(0.01, 0.99, 0.01), Inf)\n",
        "\n",
        "# compute roc curves for each model\n",
        "rocs.cutoffs = lapply(1:length(fits), function(i) data.table(t(coords(rocs[[i]], x = cutoffs, input = \"threshold\", ret = c(\"threshold\", \"specificity\", \"sensitivity\", \"accuracy\")))))\n",
        "\n",
        "# aggregate results into one table\n",
        "DT = rbindlist(lapply(1:length(fits), function(i) data.table(mod = mod.name[i], \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tcutoff = rocs.cutoffs[[i]]$threshold, \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tTPR = rocs.cutoffs[[i]]$sensitivity, \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tFPR = 1 - rocs.cutoffs[[i]]$specificity, \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tACC = rocs.cutoffs[[i]]$accuracy)))\n",
        "\n",
        "# make mod into a factor data type to facilitate plotting\n",
        "DT[, mod := factor(mod)]\n",
        "\n",
        "# compute cut-offs points that maximize accuracy\n",
        "cuts = as.numeric(sapply(1:length(fits), function(i) \n",
        "\t\t\t\t\t\t\tcoords(rocs[[i]], \n",
        "\t\t\t\t\t\t\t\t\tx = \"best\", \n",
        "\t\t\t\t\t\t\t\t\tbest.weights = c(1, length(rocs[[i]]$cases) / length(rocs[[i]]$response)))[1]))\n",
        "\n",
        "# plot ROC curves\n",
        "\n",
        "roc.plot = ggplot(DT, aes(x = FPR, y = TPR, group = mod, color = mod)) +\n",
        "\t\t\tgeom_line(size = 1) +\n",
        "\t\t\tgeom_abline(slope = 1, size = 1) +\n",
        "\t\t\tscale_y_continuous(labels = percent) +\n",
        "\t\t\tscale_x_continuous(labels = percent) +\n",
        "\t\t\tlabs(x = \"False Positive\", y = \"True Positive\", color = \"Model\") + \n",
        "\t\t\ttheme_bw(base_size = 25) +\n",
        "\t\t\ttheme(legend.position = \"top\", legend.key.size = unit(.25, \"in\"), plot.title = element_text(hjust = 0.5)) +\n",
        "\t\t\tguides(color = guide_legend(override.aes = list(size = 10, linetype = 1), nrow = 2))\n",
        "\n",
        "roc.plot\n",
        "\n",
        "# plot the accuracy curves\n",
        "\t# the dashed lines indicate the cut-off for each model that maximizes accuracy\n",
        "\t\n",
        "acc.plot = ggplot(DT, aes(x = cutoff, y = ACC, group = mod, color = mod)) +\n",
        "\t\t\tgeom_line(size = 1) +\n",
        "\t\t\tgeom_vline(xintercept = cuts, color = ggcolor(length(cuts)), size = 1, linetype = \"dashed\") +\n",
        "\t\t\tscale_y_continuous(labels = percent) +\n",
        "\t\t\tscale_x_continuous(labels = percent) +\n",
        "\t\t\tlabs(x = \"Cut-Off\", y = \"Accuracy\", color = \"Model\") + \n",
        "\t\t\ttheme_bw(base_size = 25) +\n",
        "\t\t\ttheme(legend.position = \"top\", legend.key.size = unit(.25, \"in\"), plot.title = element_text(hjust = 0.5)) +\n",
        "\t\t\tguides(color = guide_legend(override.aes = list(size = 10, linetype = 1), nrow = 2))\n",
        "\n",
        "acc.plot\n",
        "\n",
        "# ---- CV ---------------------------------------------------------------------------\n",
        "\n",
        "# build a function that will report prediction results of our models\n",
        "\n",
        "pen.pred = function(Xtrain, Ytrain, Xtest, Ytest, negative, cutoff, alpha)\n",
        "{\n",
        "\t# build the training model\n",
        "\tmod = cv.glmnet(x = Xtrain, y = Ytrain, family = \"binomial\", alpha = alpha)\n",
        "\t\n",
        "\t# make predictions with the training model using the test set\n",
        "\tynew = as.numeric(predict(mod, s = mod$lambda.min, Xtest, type = \"response\"))\n",
        "\tynew = as.numeric(ynew >= cutoff)\n",
        "\tYtest = as.numeric(as.character(Ytest))\n",
        "\t\n",
        "\t# build a confusion matrix to summarize the performance of our training model\n",
        "\toutput = confusionMatrix(Ytest, ynew, negative = negative)\n",
        "\toutput = c(output, \"AUC\" = as.numeric(auc(roc(Ytest ~ ynew))))\n",
        "\t\n",
        "\t# the confusion matrix is the output\n",
        "\treturn(output)\n",
        "}\n",
        "\n",
        "# compute diagonistic errors for each of the models in doe\n",
        "\n",
        "pen.diag = foreach(i = 1:nrow(doe)) %do%\n",
        "{\n",
        "\t# extract our predictors (X)\n",
        "\t\n",
        "\tif(doe$name[i] == \"all\")\n",
        "\t{\n",
        "\t\tX = X.all\n",
        "\t\t\n",
        "\t} else\n",
        "\t{\n",
        "\t\tX = X.trim\n",
        "\t}\n",
        "\t\n",
        "\t# perform cross validation\n",
        "\t\n",
        "\tpen.cv = crossval(predfun = pen.pred, X = X, Y = Y, K = K, B = B, \n",
        "\t\t\t\t\t\tverbose = FALSE, negative = 0, alpha = doe$alpha[i], cutoff = cuts[i])\n",
        "\n",
        "\t# compute diagnostic errors of cross validation\n",
        "\n",
        "\toutput = diag.cv(pen.cv)\n",
        "\n",
        "\t# add columns of parameter values that define model i\n",
        "\n",
        "\toutput[, name := doe$name[i]]\n",
        "\toutput[, alpha := doe$alpha[i]]\n",
        "\toutput[, mod := i]\n",
        "\t\n",
        "\treturn(output)\n",
        "}\n",
        "\n",
        "# combine the list of tables into one table\n",
        "pen.diag = rbindlist(pen.diag)\n",
        "\n",
        "# convert pen.diag into long format for plotting purposes\n",
        "DT = data.table(melt(pen.diag, id.vars = c(\"stat\", \"name\", \"alpha\", \"mod\")))\n",
        "\n",
        "# convert mod into a factor for plotting purposes\n",
        "DT[, mod := factor(mod)]\n",
        "\n",
        "# remove Inf values as these don't help\n",
        "DT = data.table(DT[value < Inf])\n",
        "\n",
        "# plot barplots of each diagnostic metric\n",
        "\n",
        "diag.plot = ggplot(DT[stat == \"Q1\" | stat == \"Median\" | stat == \"Q3\"], aes(x = stat, y = value, group = mod, fill = mod)) +\n",
        "\t\t\tgeom_bar(stat = \"identity\", position = \"dodge\", color = \"white\") +\n",
        "\t\t\tlabs(x = \"Summary Statistic\", y = \"Value\", fill = \"Model\") + \n",
        "\t\t\tfacet_wrap(~variable, scales = \"free_y\") +\n",
        "\t\t\ttheme_bw(base_size = 15) +\n",
        "\t\t\ttheme(legend.position = \"top\", legend.key.size = unit(.25, \"in\"), plot.title = element_text(hjust = 0.5)) +\n",
        "\t\t\tguides(fill = guide_legend(override.aes = list(size = 10, linetype = 1), nrow = 1))\n",
        "\n",
        "diag.plot\n",
        "\n",
        "# ---- Results ----------------------------------------------------------------------\n",
        "\n",
        "# the roc and accuracy plots look very similar\n",
        "# the odds ratio in the diagnostics plot stands out across models\n",
        "# lets filter out models\n",
        "\n",
        "pen.diag[stat == \"Median\" & or >= 20 & sens >= 0.60 & name == \"trim\"]\n",
        "\n",
        "# model 14 looks good in this group\n",
        "# build our model using all terms and let glmnet's cross validation determine which terms to keep\n",
        "\n",
        "pen.mod = cv.glmnet(x = X.trim, y = Y, family = \"binomial\", alpha = 0.6)\n",
        "\n",
        "# extract coefficients of the chosen terms for the lambda that minimizes mean cross-validated error\n",
        "\n",
        "pen.coef = coef(pen.mod, s = \"lambda.min\")\n",
        "pen.coef = data.table(term = rownames(pen.coef), coefficient = as.numeric(pen.coef))\n",
        "\n",
        "# store model diagnostic results\n",
        "\n",
        "pen.diag = pen.diag[mod == 14]\n",
        "pen.diag[, c(\"name\", \"alpha\") := NULL]\n",
        "\n",
        "pen.diag[, mod := rep(\"pen\", nrow(pen.diag))]\n",
        "mods.diag = rbind(mods.diag, pen.diag)\n",
        "\n",
        "# store the model\n",
        "\n",
        "pen.list = list(\"mod\" = pen.mod, \"coef\" = pen.coef, \"cutoff\" = cuts[14])\n",
        "mods.list$pen = pen.list\n",
        "\n",
        "# remove objects we no longer need\n",
        "\n",
        "rm(DT, pen.pred, pen.cv, X, Y, pen.diag, pen.list, pen.mod, pen.coef, mods, doe, i, diag.plot,\n",
        "\toutput, acc.plot, actuals, cutoffs, cuts, fits, mod.name, roc.plot, rocs, rocs.cutoffs,\n",
        "\tX.all, X.trim)\n",
        "\n",
        "}\n",
        "\n",
        "# -----------------------------------------------------------------------------------\n",
        "# ---- Linear Discriminant Analysis -------------------------------------------------\n",
        "# -----------------------------------------------------------------------------------\n",
        "\n",
        "{\n",
        "\n",
        "# ---- Set Up -----------------------------------------------------------------------\n",
        "\n",
        "# extract predictors (X) and response (Y)\n",
        "# lets use the regressors from the log.mod.all and log.mod.trim models and see which set of variables performs better\n",
        "\n",
        "X.all = data.table(train[,!\"Survived\"])\n",
        "X.trim = data.table(X.all[, !c(\"Sexmale.EmbarkedS\", \"Pclass3.Fare\")])\n",
        "Y = train$Survived\n",
        "\n",
        "# build a sequence of alpha values to test\n",
        "\n",
        "doe = data.table(name = c(\"all\", \"trim\"))\n",
        "\n",
        "# build each model in doe\n",
        "\n",
        "mods = foreach(i = 1:nrow(doe)) %do%\n",
        "{\n",
        "\t# extract our predictors (X)\n",
        "\t\n",
        "\tif(doe$name[i] == \"all\")\n",
        "\t{\n",
        "\t\tX = X.all\n",
        "\t\t\n",
        "\t} else\n",
        "\t{\n",
        "\t\tX = X.trim\n",
        "\t}\n",
        "\t\n",
        "\t# build table for modeling\n",
        "\t\n",
        "\tdat = cbind(Y, X)\n",
        "\t\n",
        "\t# build model\n",
        "\t\n",
        "\toutput = lda(Y ~ ., data = dat)\n",
        "\treturn(output)\n",
        "}\n",
        "\n",
        "# ---- ROC --------------------------------------------------------------------------\n",
        "\n",
        "# create names for each model\n",
        "mod.name = 1:nrow(doe)\n",
        "\n",
        "# extract actual values\n",
        "actuals = as.numeric(as.character(Y))\n",
        "\n",
        "# compute fitted values of each model\n",
        "fits = foreach(i = 1:length(mods)) %do% \n",
        "{\n",
        "\t# extract our predictors (X)\n",
        "\t\n",
        "\tif(doe$name[i] == \"all\")\n",
        "\t{\n",
        "\t\tX = X.all\n",
        "\t\t\n",
        "\t} else\n",
        "\t{\n",
        "\t\tX = X.trim\n",
        "\t}\n",
        "\t\n",
        "\toutput = as.numeric(predict(mods[[i]], newdata = X)$posterior[,2])\n",
        "\treturn(output)\n",
        "}\n",
        "\n",
        "# compute an roc object for each model\n",
        "rocs = lapply(1:length(fits), function(i) roc(actuals ~ fits[[i]]))\n",
        "\n",
        "# compute cutoffs to evaluate\n",
        "cutoffs = c(-Inf, seq(0.01, 0.99, 0.01), Inf)\n",
        "\n",
        "# compute roc curves for each model\n",
        "rocs.cutoffs = lapply(1:length(fits), function(i) data.table(t(coords(rocs[[i]], x = cutoffs, input = \"threshold\", ret = c(\"threshold\", \"specificity\", \"sensitivity\", \"accuracy\")))))\n",
        "\n",
        "# aggregate results into one table\n",
        "DT = rbindlist(lapply(1:length(fits), function(i) data.table(mod = mod.name[i], \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tcutoff = rocs.cutoffs[[i]]$threshold, \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tTPR = rocs.cutoffs[[i]]$sensitivity, \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tFPR = 1 - rocs.cutoffs[[i]]$specificity, \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tACC = rocs.cutoffs[[i]]$accuracy)))\n",
        "\n",
        "# make mod into a factor data type to facilitate plotting\n",
        "DT[, mod := factor(mod)]\n",
        "\n",
        "# compute cut-offs points that maximize accuracy\n",
        "cuts = as.numeric(sapply(1:length(fits), function(i) \n",
        "\t\t\t\t\t\t\tcoords(rocs[[i]], \n",
        "\t\t\t\t\t\t\t\t\tx = \"best\", \n",
        "\t\t\t\t\t\t\t\t\tbest.weights = c(1, length(rocs[[i]]$cases) / length(rocs[[i]]$response)))[1]))\n",
        "\n",
        "# plot ROC curves\n",
        "\n",
        "roc.plot = ggplot(DT, aes(x = FPR, y = TPR, group = mod, color = mod)) +\n",
        "\t\t\tgeom_line(size = 1) +\n",
        "\t\t\tgeom_abline(slope = 1, size = 1) +\n",
        "\t\t\tscale_y_continuous(labels = percent) +\n",
        "\t\t\tscale_x_continuous(labels = percent) +\n",
        "\t\t\tlabs(x = \"False Positive\", y = \"True Positive\", color = \"Model\") + \n",
        "\t\t\ttheme_bw(base_size = 25) +\n",
        "\t\t\ttheme(legend.position = \"top\", legend.key.size = unit(.25, \"in\"), plot.title = element_text(hjust = 0.5)) +\n",
        "\t\t\tguides(color = guide_legend(override.aes = list(size = 10, linetype = 1), nrow = 1))\n",
        "\n",
        "roc.plot\n",
        "\n",
        "# plot the accuracy curves\n",
        "\t# the dashed lines indicate the cut-off for each model that maximizes accuracy\n",
        "\t\n",
        "acc.plot = ggplot(DT, aes(x = cutoff, y = ACC, group = mod, color = mod)) +\n",
        "\t\t\tgeom_line(size = 1) +\n",
        "\t\t\tgeom_vline(xintercept = cuts, color = ggcolor(length(cuts)), size = 1, linetype = \"dashed\") +\n",
        "\t\t\tscale_y_continuous(labels = percent) +\n",
        "\t\t\tscale_x_continuous(labels = percent) +\n",
        "\t\t\tlabs(x = \"Cut-Off\", y = \"Accuracy\", color = \"Model\") + \n",
        "\t\t\ttheme_bw(base_size = 25) +\n",
        "\t\t\ttheme(legend.position = \"top\", legend.key.size = unit(.25, \"in\"), plot.title = element_text(hjust = 0.5)) +\n",
        "\t\t\tguides(color = guide_legend(override.aes = list(size = 10, linetype = 1), nrow = 1))\n",
        "\n",
        "acc.plot\n",
        "\n",
        "# ---- CV ---------------------------------------------------------------------------\n",
        "\n",
        "# build a function that will report prediction results of our models\n",
        "\n",
        "lda.pred = function(Xtrain, Ytrain, Xtest, Ytest, negative, cutoff)\n",
        "{\n",
        "\t# build table for modeling\n",
        "\t\n",
        "\tdat = cbind(\"Y\" = Ytrain, Xtrain)\n",
        "\t\n",
        "\t# build the training model\n",
        "\tmod = lda(Y ~ ., data = dat)\n",
        "\t\n",
        "\t# make predictions with the training model using the test set\n",
        "\tynew = as.numeric(predict(mod, newdata = Xtest)$posterior[,2])\n",
        "\tynew = as.numeric(ynew >= cutoff)\n",
        "\tYtest = as.numeric(as.character(Ytest))\n",
        "\t\n",
        "\t# build a confusion matrix to summarize the performance of our training model\n",
        "\toutput = confusionMatrix(Ytest, ynew, negative = negative)\n",
        "\toutput = c(output, \"AUC\" = as.numeric(auc(roc(Ytest ~ ynew))))\n",
        "\t\n",
        "\t# the confusion matrix is the output\n",
        "\treturn(output)\n",
        "}\n",
        "\n",
        "# compute diagonistic errors for each of the models in doe\n",
        "\n",
        "lda.diag = foreach(i = 1:nrow(doe)) %do%\n",
        "{\n",
        "\t# extract our predictors (X)\n",
        "\t\n",
        "\tif(doe$name[i] == \"all\")\n",
        "\t{\n",
        "\t\tX = X.all\n",
        "\t\t\n",
        "\t} else\n",
        "\t{\n",
        "\t\tX = X.trim\n",
        "\t}\n",
        "\t\n",
        "\t# perform cross validation\n",
        "\t\n",
        "\tlda.cv = crossval(predfun = lda.pred, X = X, Y = Y, K = K, B = B, \n",
        "\t\t\t\t\t\tverbose = FALSE, negative = 0, cutoff = cuts[i])\n",
        "\n",
        "\t# compute diagnostic errors of cross validation\n",
        "\n",
        "\toutput = diag.cv(lda.cv)\n",
        "\n",
        "\t# add columns of parameter values that define model i\n",
        "\n",
        "\toutput[, name := doe$name[i]]\n",
        "\toutput[, mod := i]\n",
        "\t\n",
        "\treturn(output)\n",
        "}\n",
        "\n",
        "# combine the list of tables into one table\n",
        "lda.diag = rbindlist(lda.diag)\n",
        "\n",
        "# convert lda.diag into long format for plotting purposes\n",
        "DT = data.table(melt(lda.diag, id.vars = c(\"stat\", \"name\", \"mod\")))\n",
        "\n",
        "# convert mod into a factor for plotting purposes\n",
        "DT[, mod := factor(mod)]\n",
        "\n",
        "# remove Inf values as these don't help\n",
        "DT = data.table(DT[value < Inf])\n",
        "\n",
        "# plot barplots of each diagnostic metric\n",
        "\n",
        "diag.plot = ggplot(DT[stat == \"Q1\" | stat == \"Median\" | stat == \"Q3\"], aes(x = stat, y = value, group = mod, fill = mod)) +\n",
        "\t\t\tgeom_bar(stat = \"identity\", position = \"dodge\", color = \"white\") +\n",
        "\t\t\tlabs(x = \"Summary Statistic\", y = \"Value\", fill = \"Model\") + \n",
        "\t\t\tfacet_wrap(~variable, scales = \"free_y\") +\n",
        "\t\t\ttheme_bw(base_size = 15) +\n",
        "\t\t\ttheme(legend.position = \"top\", legend.key.size = unit(.25, \"in\"), plot.title = element_text(hjust = 0.5)) +\n",
        "\t\t\tguides(fill = guide_legend(override.aes = list(size = 10, linetype = 1), nrow = 1))\n",
        "\n",
        "diag.plot\n",
        "\n",
        "# ---- Results ----------------------------------------------------------------------\n",
        "\n",
        "# the roc, accuracy, and diagnostic plots look very similar\n",
        "# lets go with the trim model becuase it requires less terms fo similar performance\n",
        "\n",
        "lda.mod = lda(Survived ~ Sexmale + Sexmale.Fare + Pclass3.Age + Age + Fare + Pclass3.EmbarkedS + SibSp.2, data = train)\n",
        "\n",
        "# store model diagnostic results\n",
        "\n",
        "lda.diag = lda.diag[mod == 2]\n",
        "lda.diag[, c(\"name\") := NULL]\n",
        "\n",
        "lda.diag[, mod := rep(\"lda\", nrow(lda.diag))]\n",
        "mods.diag = rbind(mods.diag, lda.diag)\n",
        "\n",
        "# store the model\n",
        "\n",
        "lda.list = list(\"mod\" = lda.mod, \"cutoff\" = cuts[2])\n",
        "mods.list$lda = lda.list\n",
        "\n",
        "# remove objects we no longer need\n",
        "\n",
        "rm(DT, dat, lda.pred, lda.cv, X, Y, lda.diag, lda.list, lda.mod, mods, doe, i, diag.plot,\n",
        "\toutput, acc.plot, actuals, cutoffs, cuts, fits, mod.name, roc.plot, rocs, rocs.cutoffs,\n",
        "\tX.all, X.trim)\n",
        "\n",
        "}\n",
        "\n",
        "# -----------------------------------------------------------------------------------\n",
        "# ---- Quadratic Discriminant Analysis ----------------------------------------------\n",
        "# -----------------------------------------------------------------------------------\n",
        "\n",
        "{\n",
        "\n",
        "# ---- Set Up -----------------------------------------------------------------------\n",
        "\n",
        "# extract predictors (X) and response (Y)\n",
        "# lets use the regressors from the log.mod.all and log.mod.trim models and see which set of variables performs better\n",
        "\n",
        "X.all = data.table(train[,!\"Survived\"])\n",
        "X.trim = data.table(X.all[, !c(\"Sexmale.EmbarkedS\", \"Pclass3.Fare\")])\n",
        "Y = train$Survived\n",
        "\n",
        "# build a sequence of alpha values to test\n",
        "\n",
        "doe = data.table(name = c(\"all\", \"trim\"))\n",
        "\n",
        "# build each model in doe\n",
        "\n",
        "mods = foreach(i = 1:nrow(doe)) %do%\n",
        "{\n",
        "\t# extract our predictors (X)\n",
        "\t\n",
        "\tif(doe$name[i] == \"all\")\n",
        "\t{\n",
        "\t\tX = X.all\n",
        "\t\t\n",
        "\t} else\n",
        "\t{\n",
        "\t\tX = X.trim\n",
        "\t}\n",
        "\t\n",
        "\t# build table for modeling\n",
        "\t\n",
        "\tdat = cbind(Y, X)\n",
        "\t\n",
        "\t# build model\n",
        "\t\n",
        "\toutput = qda(Y ~ ., data = dat)\n",
        "\treturn(output)\n",
        "}\n",
        "\n",
        "# ---- ROC --------------------------------------------------------------------------\n",
        "\n",
        "# create names for each model\n",
        "mod.name = 1:nrow(doe)\n",
        "\n",
        "# extract actual values\n",
        "actuals = as.numeric(as.character(Y))\n",
        "\n",
        "# compute fitted values of each model\n",
        "fits = foreach(i = 1:length(mods)) %do% \n",
        "{\n",
        "\t# extract our predictors (X)\n",
        "\t\n",
        "\tif(doe$name[i] == \"all\")\n",
        "\t{\n",
        "\t\tX = X.all\n",
        "\t\t\n",
        "\t} else\n",
        "\t{\n",
        "\t\tX = X.trim\n",
        "\t}\n",
        "\t\n",
        "\toutput = as.numeric(predict(mods[[i]], newdata = X)$posterior[,2])\n",
        "\treturn(output)\n",
        "}\n",
        "\n",
        "# compute an roc object for each model\n",
        "rocs = lapply(1:length(fits), function(i) roc(actuals ~ fits[[i]]))\n",
        "\n",
        "# compute cutoffs to evaluate\n",
        "cutoffs = c(-Inf, seq(0.01, 0.99, 0.01), Inf)\n",
        "\n",
        "# compute roc curves for each model\n",
        "rocs.cutoffs = lapply(1:length(fits), function(i) data.table(t(coords(rocs[[i]], x = cutoffs, input = \"threshold\", ret = c(\"threshold\", \"specificity\", \"sensitivity\", \"accuracy\")))))\n",
        "\n",
        "# aggregate results into one table\n",
        "DT = rbindlist(lapply(1:length(fits), function(i) data.table(mod = mod.name[i], \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tcutoff = rocs.cutoffs[[i]]$threshold, \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tTPR = rocs.cutoffs[[i]]$sensitivity, \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tFPR = 1 - rocs.cutoffs[[i]]$specificity, \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tACC = rocs.cutoffs[[i]]$accuracy)))\n",
        "\n",
        "# make mod into a factor data type to facilitate plotting\n",
        "DT[, mod := factor(mod)]\n",
        "\n",
        "# compute cut-offs points that maximize accuracy\n",
        "cuts = as.numeric(sapply(1:length(fits), function(i) \n",
        "\t\t\t\t\t\t\tcoords(rocs[[i]], \n",
        "\t\t\t\t\t\t\t\t\tx = \"best\", \n",
        "\t\t\t\t\t\t\t\t\tbest.weights = c(1, length(rocs[[i]]$cases) / length(rocs[[i]]$response)))[1]))\n",
        "\n",
        "# plot ROC curves\n",
        "\n",
        "roc.plot = ggplot(DT, aes(x = FPR, y = TPR, group = mod, color = mod)) +\n",
        "\t\t\tgeom_line(size = 1) +\n",
        "\t\t\tgeom_abline(slope = 1, size = 1) +\n",
        "\t\t\tscale_y_continuous(labels = percent) +\n",
        "\t\t\tscale_x_continuous(labels = percent) +\n",
        "\t\t\tlabs(x = \"False Positive\", y = \"True Positive\", color = \"Model\") + \n",
        "\t\t\ttheme_bw(base_size = 25) +\n",
        "\t\t\ttheme(legend.position = \"top\", legend.key.size = unit(.25, \"in\"), plot.title = element_text(hjust = 0.5)) +\n",
        "\t\t\tguides(color = guide_legend(override.aes = list(size = 10, linetype = 1), nrow = 1))\n",
        "\n",
        "roc.plot\n",
        "\n",
        "# plot the accuracy curves\n",
        "\t# the dashed lines indicate the cut-off for each model that maximizes accuracy\n",
        "\t\n",
        "acc.plot = ggplot(DT, aes(x = cutoff, y = ACC, group = mod, color = mod)) +\n",
        "\t\t\tgeom_line(size = 1) +\n",
        "\t\t\tgeom_vline(xintercept = cuts, color = ggcolor(length(cuts)), size = 1, linetype = \"dashed\") +\n",
        "\t\t\tscale_y_continuous(labels = percent) +\n",
        "\t\t\tscale_x_continuous(labels = percent) +\n",
        "\t\t\tlabs(x = \"Cut-Off\", y = \"Accuracy\", color = \"Model\") + \n",
        "\t\t\ttheme_bw(base_size = 25) +\n",
        "\t\t\ttheme(legend.position = \"top\", legend.key.size = unit(.25, \"in\"), plot.title = element_text(hjust = 0.5)) +\n",
        "\t\t\tguides(color = guide_legend(override.aes = list(size = 10, linetype = 1), nrow = 1))\n",
        "\n",
        "acc.plot\n",
        "\n",
        "# ---- CV ---------------------------------------------------------------------------\n",
        "\n",
        "# build a function that will report prediction results of our models\n",
        "\n",
        "qda.pred = function(Xtrain, Ytrain, Xtest, Ytest, negative, cutoff)\n",
        "{\n",
        "\t# build table for modeling\n",
        "\t\n",
        "\tdat = cbind(\"Y\" = Ytrain, Xtrain)\n",
        "\t\n",
        "\t# build the training model\n",
        "\tmod = qda(Y ~ ., data = dat)\n",
        "\t\n",
        "\t# make predictions with the training model using the test set\n",
        "\tynew = as.numeric(predict(mod, newdata = Xtest)$posterior[,2])\n",
        "\tynew = as.numeric(ynew >= cutoff)\n",
        "\tYtest = as.numeric(as.character(Ytest))\n",
        "\t\n",
        "\t# build a confusion matrix to summarize the performance of our training model\n",
        "\toutput = confusionMatrix(Ytest, ynew, negative = negative)\n",
        "\toutput = c(output, \"AUC\" = as.numeric(auc(roc(Ytest ~ ynew))))\n",
        "\t\n",
        "\t# the confusion matrix is the output\n",
        "\treturn(output)\n",
        "}\n",
        "\n",
        "# compute diagonistic errors for each of the models in doe\n",
        "\n",
        "qda.diag = foreach(i = 1:nrow(doe)) %do%\n",
        "{\n",
        "\t# extract our predictors (X)\n",
        "\t\n",
        "\tif(doe$name[i] == \"all\")\n",
        "\t{\n",
        "\t\tX = X.all\n",
        "\t\t\n",
        "\t} else\n",
        "\t{\n",
        "\t\tX = X.trim\n",
        "\t}\n",
        "\t\n",
        "\t# perform cross validation\n",
        "\t\n",
        "\tqda.cv = crossval(predfun = qda.pred, X = X, Y = Y, K = K, B = B, \n",
        "\t\t\t\t\t\tverbose = FALSE, negative = 0, cutoff = cuts[i])\n",
        "\n",
        "\t# compute diagnostic errors of cross validation\n",
        "\n",
        "\toutput = diag.cv(qda.cv)\n",
        "\n",
        "\t# add columns of parameter values that define model i\n",
        "\n",
        "\toutput[, name := doe$name[i]]\n",
        "\toutput[, mod := i]\n",
        "\t\n",
        "\treturn(output)\n",
        "}\n",
        "\n",
        "# combine the list of tables into one table\n",
        "qda.diag = rbindlist(qda.diag)\n",
        "\n",
        "# convert qda.diag into long format for plotting purposes\n",
        "DT = data.table(melt(qda.diag, id.vars = c(\"stat\", \"name\", \"mod\")))\n",
        "\n",
        "# convert mod into a factor for plotting purposes\n",
        "DT[, mod := factor(mod)]\n",
        "\n",
        "# remove Inf values as these don't help\n",
        "DT = data.table(DT[value < Inf])\n",
        "\n",
        "# plot barplots of each diagnostic metric\n",
        "\n",
        "diag.plot = ggplot(DT[stat == \"Q1\" | stat == \"Median\" | stat == \"Q3\"], aes(x = stat, y = value, group = mod, fill = mod)) +\n",
        "\t\t\tgeom_bar(stat = \"identity\", position = \"dodge\", color = \"white\") +\n",
        "\t\t\tlabs(x = \"Summary Statistic\", y = \"Value\", fill = \"Model\") + \n",
        "\t\t\tfacet_wrap(~variable, scales = \"free_y\") +\n",
        "\t\t\ttheme_bw(base_size = 15) +\n",
        "\t\t\ttheme(legend.position = \"top\", legend.key.size = unit(.25, \"in\"), plot.title = element_text(hjust = 0.5)) +\n",
        "\t\t\tguides(fill = guide_legend(override.aes = list(size = 10, linetype = 1), nrow = 1))\n",
        "\n",
        "diag.plot\n",
        "\n",
        "# ---- Results ----------------------------------------------------------------------\n",
        "\n",
        "# the roc and accuracy plots look very similar\n",
        "# the diagnostic plot shows the all model to perform better\n",
        "\n",
        "qda.mod = qda(Survived ~ ., data = train)\n",
        "\n",
        "# store model diagnostic results\n",
        "\n",
        "qda.diag = qda.diag[mod == 1]\n",
        "qda.diag[, c(\"name\") := NULL]\n",
        "\n",
        "qda.diag[, mod := rep(\"qda\", nrow(qda.diag))]\n",
        "mods.diag = rbind(mods.diag, qda.diag)\n",
        "\n",
        "# store the model\n",
        "\n",
        "qda.list = list(\"mod\" = qda.mod, \"cutoff\" = cuts[1])\n",
        "mods.list$qda = qda.list\n",
        "\n",
        "# remove objects we no longer need\n",
        "\n",
        "rm(DT, dat, qda.pred, qda.cv, X, Y, qda.diag, qda.list, qda.mod, mods, doe, i, diag.plot,\n",
        "\toutput, acc.plot, actuals, cutoffs, cuts, fits, mod.name, roc.plot, rocs, rocs.cutoffs,\n",
        "\tX.all, X.trim)\n",
        "\n",
        "}\n",
        "\n",
        "# -----------------------------------------------------------------------------------\n",
        "# ---- Gradient Boosting Model ------------------------------------------------------\n",
        "# -----------------------------------------------------------------------------------\n",
        "\n",
        "{\n",
        "\n",
        "# ---- Set Up -----------------------------------------------------------------------\n",
        "\n",
        "# we have 5 hyperparameters of interest:\n",
        "\t# eta  ~ the learning rate\n",
        "\t\t# default value: 0.3\n",
        "\t# max_depth  ~ maximum depth of a tree\n",
        "\t\t# default value: 6\n",
        "\t# min_child_weight ~ minimum sum of instance weight needed in a child\n",
        "\t\t# default value: 1\n",
        "\t# nrounds ~ max number of iterations\n",
        "\t# gamma ~ minimum loss reduction required to make a further partition on a leaf node\n",
        "\t\t\n",
        "# check out this link for help on tuning:\n",
        "# # http://machinelearningmastery.com/configure-gradient-boosting-algorithm/\n",
        "\t\n",
        "# extract our predictors (X) and response (Y)\n",
        "\n",
        "X = data.table(train)\n",
        "Y = as.numeric(as.character(X$Survived))\n",
        "X[, Survived := NULL]\n",
        "X = as.matrix(X)\n",
        "\n",
        "# compute the proportion of events in our response (ie. Survived == 1)\n",
        "\n",
        "event.rate = nrow(train[Survived == 1]) / nrow(train)\n",
        "\n",
        "# create parameter combinations to test\n",
        "\n",
        "doe = data.table(expand.grid(nrounds = 500,\n",
        "\t\t\t\t\t\t\t\teta = 0.1,\n",
        "\t\t\t\t\t\t\t\tmax_depth = c(4, 7, 10), \n",
        "\t\t\t\t\t\t\t\tmin_child_weight = seq(1/sqrt(event.rate), 3/event.rate, length.out = 3),\n",
        "\t\t\t\t\t\t\t\tgamma = 0))\n",
        "\n",
        "# build models\n",
        "mods = lapply(1:nrow(doe), function(i) \n",
        "\t\t\t\txgboost(label = Y, data = X,\n",
        "\t\t\t\t\t\tobjective = \"binary:logistic\", eval_metric = \"auc\",\n",
        "\t\t\t\t\t\teta = doe$eta[i],\n",
        "\t\t\t\t\t\tmax_depth = doe$max_depth[i],\n",
        "\t\t\t\t\t\tnrounds = doe$nrounds[i],\n",
        "\t\t\t\t\t\tmin_child_weight = doe$min_child_weight[i],\n",
        "\t\t\t\t\t\tgamma = doe$gamma[i], verbose = 0))\n",
        "\n",
        "# ---- ROC --------------------------------------------------------------------------\n",
        "\n",
        "# create names for each model\n",
        "mod.name = 1:nrow(doe)\n",
        "\n",
        "# extract actual values\n",
        "actuals = Y\n",
        "\n",
        "# compute fitted values of each model\n",
        "fits = lapply(1:length(mods), function(i) as.numeric(predict(mods[[i]], newdata = X)))\n",
        "\n",
        "# compute an roc object for each model\n",
        "rocs = lapply(1:length(fits), function(i) roc(actuals ~ fits[[i]]))\n",
        "\n",
        "# compute cutoffs to evaluate\n",
        "cutoffs = c(-Inf, seq(0.01, 0.99, 0.01), Inf)\n",
        "\n",
        "# compute roc curves for each model\n",
        "rocs.cutoffs = lapply(1:length(fits), function(i) data.table(t(coords(rocs[[i]], x = cutoffs, input = \"threshold\", ret = c(\"threshold\", \"specificity\", \"sensitivity\", \"accuracy\")))))\n",
        "\n",
        "# aggregate results into one table\n",
        "DT = rbindlist(lapply(1:length(fits), function(i) data.table(mod = mod.name[i], \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tcutoff = rocs.cutoffs[[i]]$threshold, \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tTPR = rocs.cutoffs[[i]]$sensitivity, \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tFPR = 1 - rocs.cutoffs[[i]]$specificity, \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tACC = rocs.cutoffs[[i]]$accuracy)))\n",
        "\n",
        "# make mod into a factor data type to facilitate plotting\n",
        "DT[, mod := factor(mod)]\n",
        "\n",
        "# compute cut-offs points that maximize accuracy\n",
        "cuts = as.numeric(sapply(1:length(fits), function(i) \n",
        "\t\t\t\t\t\t\tcoords(rocs[[i]], \n",
        "\t\t\t\t\t\t\t\t\tx = \"best\", \n",
        "\t\t\t\t\t\t\t\t\tbest.weights = c(1, length(rocs[[i]]$cases) / length(rocs[[i]]$response)))[1]))\n",
        "\n",
        "# plot ROC curves\n",
        "\n",
        "roc.plot = ggplot(DT, aes(x = FPR, y = TPR, group = mod, color = mod)) +\n",
        "\t\t\tgeom_line(size = 1) +\n",
        "\t\t\tgeom_abline(slope = 1, size = 1) +\n",
        "\t\t\tscale_y_continuous(labels = percent) +\n",
        "\t\t\tscale_x_continuous(labels = percent) +\n",
        "\t\t\tlabs(x = \"False Positive\", y = \"True Positive\", color = \"Model\") + \n",
        "\t\t\ttheme_bw(base_size = 25) +\n",
        "\t\t\ttheme(legend.position = \"top\", legend.key.size = unit(.25, \"in\"), plot.title = element_text(hjust = 0.5)) +\n",
        "\t\t\tguides(color = guide_legend(override.aes = list(size = 10, linetype = 1), nrow = 1))\n",
        "\n",
        "roc.plot\n",
        "\n",
        "# plot the accuracy curves\n",
        "\t# the dashed lines indicate the cut-off for each model that maximizes accuracy\n",
        "\t\n",
        "acc.plot = ggplot(DT, aes(x = cutoff, y = ACC, group = mod, color = mod)) +\n",
        "\t\t\tgeom_line(size = 1) +\n",
        "\t\t\tgeom_vline(xintercept = cuts, color = ggcolor(length(cuts)), size = 1, linetype = \"dashed\") +\n",
        "\t\t\tscale_y_continuous(labels = percent) +\n",
        "\t\t\tscale_x_continuous(labels = percent) +\n",
        "\t\t\tlabs(x = \"Cut-Off\", y = \"Accuracy\", color = \"Model\") + \n",
        "\t\t\ttheme_bw(base_size = 25) +\n",
        "\t\t\ttheme(legend.position = \"top\", legend.key.size = unit(.25, \"in\"), plot.title = element_text(hjust = 0.5)) +\n",
        "\t\t\tguides(color = guide_legend(override.aes = list(size = 10, linetype = 1), nrow = 1))\n",
        "\n",
        "acc.plot\n",
        "\n",
        "# ---- CV ---------------------------------------------------------------------------\n",
        "\n",
        "# build a function that will report prediction results of our model\n",
        "\n",
        "gbm.pred = function(Xtrain, Ytrain, Xtest, Ytest, negative, cutoff, objective, eval_metric, eta, max_depth, nrounds, min_child_weight, gamma)\n",
        "{\n",
        "\t# build the training model\n",
        "\tmod = xgboost(label = Ytrain, data = Xtrain,\n",
        "\t\t\t\t\tobjective = objective, eval_metric = eval_metric,\n",
        "\t\t\t\t\teta = eta,\n",
        "\t\t\t\t\tmax_depth = max_depth,\n",
        "\t\t\t\t\tnrounds = nrounds,\n",
        "\t\t\t\t\tmin_child_weight = min_child_weight,\n",
        "\t\t\t\t\tgamma = gamma, verbose = 0)\n",
        "\t\n",
        "\t# make predictions with the training model using the test set\n",
        "\tynew = as.numeric(predict(mod, newdata = Xtest))\n",
        "\tynew = as.numeric(ynew >= cutoff)\n",
        "\t\n",
        "\t# build a confusion matrix to summarize the performance of our training model\n",
        "\toutput = confusionMatrix(Ytest, ynew, negative = negative)\n",
        "\toutput = c(output, \"AUC\" = as.numeric(auc(roc(Ytest ~ ynew))))\n",
        "\t\n",
        "\t# the confusion matrix is the output\n",
        "\treturn(output)\n",
        "}\n",
        "\n",
        "# compute diagonistic errors for each of the models in doe\n",
        "\n",
        "gbm.diag = foreach(i = 1:nrow(doe)) %do%\n",
        "{\n",
        "\t# perform cross validation\n",
        "\t\n",
        "\tgbm.cv = crossval(predfun = gbm.pred, X = X, Y = Y, K = K, B = B, \n",
        "\t\t\t\t\t\tverbose = FALSE, negative = 0, cutoff = cuts[i],\n",
        "\t\t\t\t\t\tobjective = \"binary:logistic\", eval_metric = \"auc\",\n",
        "\t\t\t\t\t\teta = doe$eta[i], max_depth = doe$max_depth[i],\n",
        "\t\t\t\t\t\tnrounds = doe$nrounds[i], min_child_weight = doe$min_child_weight[i],\n",
        "\t\t\t\t\t\tgamma = doe$gamma[i])\n",
        "\t\n",
        "\t# compute diagnostic errors of cross validation\n",
        "\t\n",
        "\toutput = diag.cv(gbm.cv)\n",
        "\t\n",
        "\t# add columns of parameter values that define model i\n",
        "\t\n",
        "\toutput[, eta := doe$eta[i]]\n",
        "\toutput[, max_depth := doe$max_depth[i]]\n",
        "\toutput[, nrounds := doe$nrounds[i]]\n",
        "\toutput[, min_child_weight := doe$min_child_weight[i]]\n",
        "\toutput[, gamma := doe$gamma[i]]\n",
        "\toutput[, mod := i]\n",
        "\t\n",
        "\treturn(output)\n",
        "}\n",
        "\n",
        "# combine the list of tables into one table\n",
        "gbm.diag = rbindlist(gbm.diag)\n",
        "\n",
        "# convert gbm.diag into long format for plotting purposes\n",
        "DT = data.table(melt(gbm.diag, id.vars = c(\"stat\", \"eta\", \"max_depth\", \"nrounds\", \"min_child_weight\", \"gamma\", \"mod\")))\n",
        "\n",
        "# convert mod into a factor for plotting purposes\n",
        "DT[, mod := factor(mod)]\n",
        "\n",
        "# remove Inf values as these don't help\n",
        "DT = data.table(DT[value < Inf])\n",
        "\n",
        "# plot barplots of each diagnostic metric\n",
        "\n",
        "diag.plot = ggplot(DT[stat == \"Q1\" | stat == \"Median\" | stat == \"Q3\"], aes(x = stat, y = value, group = mod, fill = mod)) +\n",
        "\t\t\tgeom_bar(stat = \"identity\", position = \"dodge\", color = \"white\") +\n",
        "\t\t\tlabs(x = \"Summary Statistic\", y = \"Value\", fill = \"Model\") + \n",
        "\t\t\tfacet_wrap(~variable, scales = \"free_y\") +\n",
        "\t\t\ttheme_bw(base_size = 15) +\n",
        "\t\t\ttheme(legend.position = \"top\", legend.key.size = unit(.25, \"in\"), plot.title = element_text(hjust = 0.5)) +\n",
        "\t\t\tguides(fill = guide_legend(override.aes = list(size = 10, linetype = 1), nrow = 1))\n",
        "\n",
        "diag.plot\n",
        "\n",
        "# ---- Results ----------------------------------------------------------------------\n",
        "\n",
        "# the roc and accuracy plots show similar performance across models\n",
        "# in the diagnostics plot model 4 looks to be better than most models across each metric\n",
        "# lets choose model 4\n",
        "\n",
        "gbm.diag = gbm.diag[mod == 4]\n",
        "\n",
        "# rename model to gbm as this is our chosen random forest model\n",
        "\n",
        "gbm.diag[, mod := rep(\"gbm\", nrow(gbm.diag))]\n",
        "\n",
        "# build our model\n",
        "\n",
        "gbm.mod = xgboost(label = Y, data = X,\n",
        "\t\t\t\t\tobjective = \"binary:logistic\", eval_metric = \"auc\",\n",
        "\t\t\t\t\teta = 0.1, max_depth = 4,\n",
        "\t\t\t\t\tnrounds = 500, min_child_weight = 4.714936,\n",
        "\t\t\t\t\tgamma = 0, verbose = 0)\n",
        "\n",
        "# store model diagnostic results\n",
        "\n",
        "gbm.diag[, c(\"eta\", \"max_depth\", \"nrounds\", \"min_child_weight\", \"gamma\") := NULL]\n",
        "mods.diag = rbind(mods.diag, gbm.diag)\n",
        "\n",
        "# store the model\n",
        "\n",
        "gbm.list = list(\"mod\" = gbm.mod, \"cutoff\" = cuts[4])\n",
        "mods.list$gbm = gbm.list\n",
        "\n",
        "# remove objects we no longer need\n",
        "\n",
        "rm(doe, DT, i, output, gbm.pred, gbm.cv, X, Y, gbm.diag, gbm.list, gbm.mod, event.rate,\n",
        "\tacc.plot, actuals, cutoffs, cuts, diag.plot, fits, mods, mod.name, roc.plot, rocs, rocs.cutoffs)\n",
        "\n",
        "}\n",
        "\n",
        "# -----------------------------------------------------------------------------------\n",
        "# ---- Random Forest Model ----------------------------------------------------------\n",
        "# -----------------------------------------------------------------------------------\n",
        "\n",
        "{\n",
        "\n",
        "# ---- Set Up -----------------------------------------------------------------------\n",
        "\n",
        "# we have 3 hyperparameters of interest:\n",
        "\t# ntree ~ number of decision trees to create\n",
        "\t\t# default value: 500\n",
        "\t# mtry ~ the number of regressors to randomly choose at each parent node of each tree, from which the best is chosen (based on minimal OOB error) to split the parent node into two child nodes\n",
        "\t\t# default value: floor(sqrt(p)) (where p is number of regressors)\n",
        "\t# nodesize ~ minimum size of terminal nodes (ie. the minimum number of data points that can be grouped together in any node of a tree)\n",
        "\t\t# default value: 1\n",
        "\n",
        "# default value for mtry in our case:\n",
        "floor(sqrt(ncol(train) - 1))\n",
        "\n",
        "# extract our predictors (X) and response (Y)\n",
        "\n",
        "X = data.table(train)\n",
        "Y = X$Survived\n",
        "X[, Survived := NULL]\n",
        "\n",
        "# create parameter combinations to test\n",
        "\n",
        "doe = data.table(expand.grid(ntree = 500,\n",
        "\t\t\t\t\t\t\t\tmtry = c(3, 5, 7), \n",
        "\t\t\t\t\t\t\t\tnodesize = c(1, 5, 9)))\n",
        "\n",
        "# build models\n",
        "mods = lapply(1:nrow(doe), function(i) \n",
        "\t\t\t\trandomForest(Survived ~ .,\n",
        "\t\t\t\t\t\t\t\tdata = train,\n",
        "\t\t\t\t\t\t\t\tntree = doe$ntree[i],\n",
        "\t\t\t\t\t\t\t\tmtry = doe$mtry[i],\n",
        "\t\t\t\t\t\t\t\tnodesize = doe$nodesize[i]))\n",
        "\n",
        "# ---- ROC --------------------------------------------------------------------------\n",
        "\n",
        "# create names for each model\n",
        "mod.name = 1:nrow(doe)\n",
        "\n",
        "# extract actual values\n",
        "actuals = as.numeric(as.character(train$Survived))\n",
        "\n",
        "# compute fitted values of each model\n",
        "fits = lapply(1:length(mods), function(i) as.numeric(predict(mods[[i]], type = \"prob\")[,2]))\n",
        "\n",
        "# compute an roc object for each model\n",
        "rocs = lapply(1:length(fits), function(i) roc(actuals ~ fits[[i]]))\n",
        "\n",
        "# compute cutoffs to evaluate\n",
        "cutoffs = c(-Inf, seq(0.01, 0.99, 0.01), Inf)\n",
        "\n",
        "# compute roc curves for each model\n",
        "rocs.cutoffs = lapply(1:length(fits), function(i) data.table(t(coords(rocs[[i]], x = cutoffs, input = \"threshold\", ret = c(\"threshold\", \"specificity\", \"sensitivity\", \"accuracy\")))))\n",
        "\n",
        "# aggregate results into one table\n",
        "DT = rbindlist(lapply(1:length(fits), function(i) data.table(mod = mod.name[i], \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tcutoff = rocs.cutoffs[[i]]$threshold, \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tTPR = rocs.cutoffs[[i]]$sensitivity, \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tFPR = 1 - rocs.cutoffs[[i]]$specificity, \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tACC = rocs.cutoffs[[i]]$accuracy)))\n",
        "\n",
        "# make mod into a factor data type to facilitate plotting\n",
        "DT[, mod := factor(mod)]\n",
        "\n",
        "# compute cut-offs points that maximize accuracy\n",
        "cuts = as.numeric(sapply(1:length(fits), function(i) \n",
        "\t\t\t\t\t\t\tcoords(rocs[[i]], \n",
        "\t\t\t\t\t\t\t\t\tx = \"best\", \n",
        "\t\t\t\t\t\t\t\t\tbest.weights = c(1, length(rocs[[i]]$cases) / length(rocs[[i]]$response)))[1]))\n",
        "\n",
        "# plot ROC curves\n",
        "\n",
        "roc.plot = ggplot(DT, aes(x = FPR, y = TPR, group = mod, color = mod)) +\n",
        "\t\t\tgeom_line(size = 1) +\n",
        "\t\t\tgeom_abline(slope = 1, size = 1) +\n",
        "\t\t\tscale_y_continuous(labels = percent) +\n",
        "\t\t\tscale_x_continuous(labels = percent) +\n",
        "\t\t\tlabs(x = \"False Positive\", y = \"True Positive\", color = \"Model\") + \n",
        "\t\t\ttheme_bw(base_size = 25) +\n",
        "\t\t\ttheme(legend.position = \"top\", legend.key.size = unit(.25, \"in\"), plot.title = element_text(hjust = 0.5)) +\n",
        "\t\t\tguides(color = guide_legend(override.aes = list(size = 10, linetype = 1), nrow = 1))\n",
        "\n",
        "roc.plot\n",
        "\n",
        "# plot the accuracy curves\n",
        "\t# the dashed lines indicate the cut-off for each model that maximizes accuracy\n",
        "\t\n",
        "acc.plot = ggplot(DT, aes(x = cutoff, y = ACC, group = mod, color = mod)) +\n",
        "\t\t\tgeom_line(size = 1) +\n",
        "\t\t\tgeom_vline(xintercept = cuts, color = ggcolor(length(cuts)), size = 1, linetype = \"dashed\") +\n",
        "\t\t\tscale_y_continuous(labels = percent) +\n",
        "\t\t\tscale_x_continuous(labels = percent) +\n",
        "\t\t\tlabs(x = \"Cut-Off\", y = \"Accuracy\", color = \"Model\") + \n",
        "\t\t\ttheme_bw(base_size = 25) +\n",
        "\t\t\ttheme(legend.position = \"top\", legend.key.size = unit(.25, \"in\"), plot.title = element_text(hjust = 0.5)) +\n",
        "\t\t\tguides(color = guide_legend(override.aes = list(size = 10, linetype = 1), nrow = 1))\n",
        "\n",
        "acc.plot\n",
        "\n",
        "# ---- CV ---------------------------------------------------------------------------\n",
        "\n",
        "# build a function that will report prediction results of our model\n",
        "\n",
        "rf.pred = function(Xtrain, Ytrain, Xtest, Ytest, negative, ntree, mtry, nodesize, cutoff)\n",
        "{\n",
        "\t# build the table for training the model\n",
        "\tdat = data.table(Xtrain)\n",
        "\tdat[, y := Ytrain]\n",
        "\t\n",
        "\t# build the training model\n",
        "\tmod = randomForest(y ~ .,\n",
        "\t\t\t\t\t\tdata = dat,\n",
        "\t\t\t\t\t\tntree = ntree,\n",
        "\t\t\t\t\t\tmtry = mtry,\n",
        "\t\t\t\t\t\tnodesize = nodesize)\n",
        "\t\n",
        "\t# make predictions with the training model using the test set\n",
        "\tynew = as.numeric(predict(mod, newdata = data.table(Xtest), type = \"prob\")[,2])\n",
        "\tynew = as.numeric(ynew >= cutoff)\n",
        "\tYtest = as.numeric(as.character(Ytest))\n",
        "\t\n",
        "\t# build a confusion matrix to summarize the performance of our training model\n",
        "\toutput = confusionMatrix(Ytest, ynew, negative = negative)\n",
        "\toutput = c(output, \"AUC\" = as.numeric(auc(roc(Ytest ~ ynew))))\n",
        "\t\n",
        "\t# the confusion matrix is the output\n",
        "\treturn(output)\n",
        "}\n",
        "\n",
        "# compute diagonistic errors for each of the models in doe\n",
        "\n",
        "rf.diag = foreach(i = 1:nrow(doe)) %do%\n",
        "{\n",
        "\t# perform cross validation\n",
        "\t\n",
        "\trf.cv = crossval(predfun = rf.pred, X = X, Y = Y, K = K, B = B, \n",
        "\t\t\t\t\t\tverbose = FALSE, negative = 0, \n",
        "\t\t\t\t\t\tntree = doe$ntree[i], mtry = doe$mtry[i], nodesize = doe$nodesize[i], cutoff = cuts[i])\n",
        "\n",
        "\t# compute diagnostic errors of cross validation\n",
        "\n",
        "\toutput = diag.cv(rf.cv)\n",
        "\n",
        "\t# add columns of parameter values that define model i\n",
        "\n",
        "\toutput[, ntree := doe$ntree[i]] \n",
        "\toutput[, mtry := doe$mtry[i]]\n",
        "\toutput[, nodesize := doe$nodesize[i]]\n",
        "\toutput[, mod := i]\n",
        "\t\n",
        "\treturn(output)\n",
        "}\n",
        "\n",
        "# combine the list of tables into one table\n",
        "rf.diag = rbindlist(rf.diag)\n",
        "\n",
        "# convert rf.diag into long format for plotting purposes\n",
        "DT = data.table(melt(rf.diag, id.vars = c(\"stat\", \"ntree\", \"mtry\", \"nodesize\", \"mod\")))\n",
        "\n",
        "# convert mod into a factor for plotting purposes\n",
        "DT[, mod := factor(mod)]\n",
        "\n",
        "# remove Inf values as these don't help\n",
        "DT = data.table(DT[value < Inf])\n",
        "\n",
        "# plot barplots of each diagnostic metric\n",
        "\n",
        "diag.plot = ggplot(DT[stat == \"Q1\" | stat == \"Median\" | stat == \"Q3\"], aes(x = stat, y = value, group = mod, fill = mod)) +\n",
        "\t\t\tgeom_bar(stat = \"identity\", position = \"dodge\", color = \"white\") +\n",
        "\t\t\tlabs(x = \"Summary Statistic\", y = \"Value\", fill = \"Model\") + \n",
        "\t\t\tfacet_wrap(~variable, scales = \"free_y\") +\n",
        "\t\t\ttheme_bw(base_size = 15) +\n",
        "\t\t\ttheme(legend.position = \"top\", legend.key.size = unit(.25, \"in\"), plot.title = element_text(hjust = 0.5)) +\n",
        "\t\t\tguides(fill = guide_legend(override.aes = list(size = 10, linetype = 1), nrow = 1))\n",
        "\n",
        "diag.plot\n",
        "\n",
        "# ---- Results ----------------------------------------------------------------------\n",
        "\n",
        "# the roc and accuracy plots show similar performance across models\n",
        "# in the diagnostics plot, model 4 seems to do better than most models across all metrics\n",
        "# lets go with model 4\n",
        "\n",
        "rf.diag = rf.diag[mod == 4]\n",
        "\n",
        "# rename model to rf as this is our chosen random forest model\n",
        "\n",
        "rf.diag[, mod := rep(\"rf\", nrow(rf.diag))]\n",
        "\n",
        "# build our random forest model\n",
        "\n",
        "rf.mod = randomForest(Survived ~ ., data = train,\n",
        "\t\t\t\t\t\tntree = 500, mtry = 3, nodesize = 5)\n",
        "\n",
        "# store model diagnostic results\n",
        "\n",
        "rf.diag[, c(\"ntree\", \"mtry\", \"nodesize\") := NULL]\n",
        "mods.diag = rbind(mods.diag, rf.diag)\n",
        "\n",
        "# store the model\n",
        "\n",
        "rf.list = list(\"mod\" = rf.mod, \"cutoff\" = cuts[4])\n",
        "mods.list$rf = rf.list\n",
        "\n",
        "# remove objects we no longer need\n",
        "\n",
        "rm(doe, DT, i, output, rf.pred, rf.cv, X, Y, rf.diag, rf.list, rf.mod,\n",
        "\tacc.plot, actuals, cutoffs, cuts, diag.plot, fits, mods, mod.name, roc.plot, rocs, rocs.cutoffs)\n",
        "\n",
        "}\n",
        "\n",
        "# -----------------------------------------------------------------------------------\n",
        "# ---- Nueral Network Model ---------------------------------------------------------\n",
        "# -----------------------------------------------------------------------------------\n",
        "\n",
        "{\n",
        "\n",
        "# ---- Set Up -----------------------------------------------------------------------\n",
        "\n",
        "# adjust the structure of train such that each regressor is numeric\n",
        "\n",
        "# create a copy of train\n",
        "train.nn = data.table(train)\n",
        "# make Survived a numeric value\n",
        "train.nn[, Survived := as.numeric(as.character(Survived))]\n",
        "\n",
        "# check out the min and max values of each column in train.nn\n",
        "min.max = melt(train.nn[, !\"Survived\"], measure.vars = 1:(ncol(train.nn) - 1))\n",
        "min.max = min.max[, .(min = min(value), max = max(value)), by = variable]\n",
        "min.max = min.max[min < 0 | max > 1]\n",
        "min.max\n",
        "\n",
        "# rescale columns that have a max value above 1 and/or a min value below 0\n",
        "\t# a 0 to 1 scaling is required for neural networks\n",
        "train.nn[, rescale.Age := rescale(Age, to = c(0, 1))]\n",
        "train.nn[, rescale.Fare := rescale(Fare, to = c(0, 1))]\n",
        "train.nn[, rescale.SibSp.2 := rescale(SibSp.2, to = c(0, 1))]\n",
        "train.nn[, rescale.Sexmale.Fare := rescale(Sexmale.Fare, to = c(0, 1))]\n",
        "train.nn[, rescale.Pclass3.Age := rescale(Pclass3.Age, to = c(0, 1))]\n",
        "train.nn[, rescale.Pclass3.Fare := rescale(Pclass3.Fare, to = c(0, 1))]\n",
        "\n",
        "# remove columns that violate the scale\n",
        "train.nn[, as.character(min.max$variable) := NULL]\n",
        "\n",
        "# repeat this adjustment for test incase we end up using the chosen neuralnet on test\n",
        "\n",
        "# create a copy of train\n",
        "test.nn = data.table(test)\n",
        "\n",
        "# check out the min and max values of each column in test.nn\n",
        "min.max = melt(test.nn, measure.vars = 1:ncol(test.nn))\n",
        "min.max = min.max[, .(min = min(value), max = max(value)), by = variable]\n",
        "min.max = min.max[min < 0 | max > 1]\n",
        "min.max\n",
        "\n",
        "# rescale columns that have a max value above 1 and/or a min value below 0\n",
        "\t# a 0 to 1 scaling is required for neural networks\n",
        "test.nn[, rescale.Age := rescale(Age, to = c(0, 1), from = range(train$Age))]\n",
        "test.nn[, rescale.Fare := rescale(Fare, to = c(0, 1), from = range(train$Fare))]\n",
        "test.nn[, rescale.SibSp.2 := rescale(SibSp.2, to = c(0, 1), from = range(train$SibSp.2))]\n",
        "test.nn[, rescale.Sexmale.Fare := rescale(Sexmale.Fare, to = c(0, 1), from = range(train$Sexmale.Fare))]\n",
        "test.nn[, rescale.Pclass3.Age := rescale(Pclass3.Age, to = c(0, 1), from = range(train$Pclass3.Age))]\n",
        "test.nn[, rescale.Pclass3.Fare := rescale(Pclass3.Fare, to = c(0, 1), from = range(train$Pclass3.Fare))]\n",
        "\n",
        "# remove columns that violate the scale\n",
        "test.nn[, as.character(min.max$variable) := NULL]\n",
        "\n",
        "# extract our predictors (X) and response (Y)\n",
        "X = data.table(train.nn)\n",
        "Y = X$Survived\n",
        "X[, Survived := NULL]\n",
        "\n",
        "# we have 3 hyperparameters of interest:\n",
        "\t# hidden ~ a vector of integers specifying the number of hidden neurons (vertices) in each layer\n",
        "\t# learningrate ~ a numeric value specifying the threshold for the partial derivatives of the error function as stopping criteria\n",
        "\t# threshold ~ a numeric value specifying the learning rate used by traditional backpropagation. Used only for traditional backpropagation.\n",
        "\t\n",
        "# lets use cross validation to tune our model\n",
        "\n",
        "# we are going to use rprop+ as our algorithm so learningrate does not need to be specified and we will stick to the defualt values for learningrate.factor\n",
        "# we are going to fix threshold to a value larger than the default to speed up the computation process\n",
        "# we are going to vary the structure of our hidden layers as this is what defines the nueral network structure\n",
        "\n",
        "# one rule of thumb for choosing the number of hidden nodes is 2/3 the size of the input layer plus the size of the output layer\n",
        "\n",
        "input.size = ncol(X)\n",
        "output.size = length(levels(train$Survived)) - 1\n",
        "\n",
        "hidden.size = round(((2/3) * input.size) + output.size, 0)\n",
        "hidden.size\n",
        "\n",
        "rm(input.size, output.size, hidden.size)\n",
        "\n",
        "# the hidden layer size seems small so we will try higher values as well and let cross validation distinuish if the higher node sizes yeild overfitting\n",
        "\n",
        "# create parameter combinations to test\n",
        "\t# layers ~ indicate how many layers we will try\n",
        "\t\t# we will only try 1 and 2 becuase 1 is expected to work for the vast majority of practical applications and 2 allows us to potentially capture some nonlinearlity in the data\n",
        "\t# nodes ~ indicates the total number of hidden layer nodes in our network\n",
        "\t# prop ~ indicates what proportion of nodes to give to the first hidden layer when we are building a 2 hidden layer network\n",
        "\n",
        "doe = data.table(expand.grid(layers = c(1, 2),\n",
        "\t\t\t\t\t\t\tnodes = c(10, 30, 50),\n",
        "\t\t\t\t\t\t\tprop = c(1/3, 1/2, 2/3)))\n",
        "\n",
        "# make prop = 1 when layers = 1, and then remove duplicated scenarios\n",
        "\n",
        "doe = doe[layers == 1, prop := 1]\t\t\t\n",
        "doe = doe[!duplicated(doe)]\n",
        "\n",
        "# build the table for training the model\n",
        "dat = data.table(X)\n",
        "dat[, y := Y]\n",
        "\n",
        "# build a formula string for the training model\n",
        "rhs = paste(names(X), collapse = \" + \")\n",
        "form = paste(\"y ~\", rhs)\n",
        "\n",
        "# build models\n",
        "mods = foreach(i = 1:nrow(doe)) %do%\n",
        "{\n",
        "\t# compute the hidden layer structure for model i\n",
        "\t\n",
        "\thidden = vector(length = doe$layers[i])\n",
        "\thidden[1] = round(doe$nodes[i] * doe$prop[i], 0)\n",
        "\tif(doe$layers[i] == 2) hidden[2] = doe$nodes[i] - hidden[1]\n",
        "\t\n",
        "\t# build model i\n",
        "\t\n",
        "\toutput = neuralnet(form, data = dat, \n",
        "\t\t\t\t\t\thidden = hidden, threshold = 0.25, \n",
        "\t\t\t\t\t\tlinear.output = FALSE)\n",
        "\t\n",
        "\treturn(output)\n",
        "}\n",
        "\n",
        "# ---- ROC --------------------------------------------------------------------------\n",
        "\n",
        "# create names for each model\n",
        "mod.name = 1:nrow(doe)\n",
        "\n",
        "# extract actual values\n",
        "actuals = dat$y\n",
        "\n",
        "# compute fitted values of each model\n",
        "fits = lapply(1:length(mods), function(i) as.numeric(mods[[i]]$net.result[[1]]))\n",
        "\n",
        "# compute an roc object for each model\n",
        "rocs = lapply(1:length(fits), function(i) roc(actuals ~ fits[[i]]))\n",
        "\n",
        "# compute cutoffs to evaluate\n",
        "cutoffs = c(-Inf, seq(0.01, 0.99, 0.01), Inf)\n",
        "\n",
        "# compute roc curves for each model\n",
        "rocs.cutoffs = lapply(1:length(fits), function(i) data.table(t(coords(rocs[[i]], x = cutoffs, input = \"threshold\", ret = c(\"threshold\", \"specificity\", \"sensitivity\", \"accuracy\")))))\n",
        "\n",
        "# aggregate results into one table\n",
        "DT = rbindlist(lapply(1:length(fits), function(i) data.table(mod = mod.name[i], \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tcutoff = rocs.cutoffs[[i]]$threshold, \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tTPR = rocs.cutoffs[[i]]$sensitivity, \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tFPR = 1 - rocs.cutoffs[[i]]$specificity, \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tACC = rocs.cutoffs[[i]]$accuracy)))\n",
        "\n",
        "# make mod into a factor data type to facilitate plotting\n",
        "DT[, mod := factor(mod)]\n",
        "\n",
        "# compute cut-offs points that maximize accuracy\n",
        "cuts = as.numeric(sapply(1:length(fits), function(i) \n",
        "\t\t\t\t\t\t\tcoords(rocs[[i]], \n",
        "\t\t\t\t\t\t\t\t\tx = \"best\", \n",
        "\t\t\t\t\t\t\t\t\tbest.weights = c(1, length(rocs[[i]]$cases) / length(rocs[[i]]$response)))[1]))\n",
        "\n",
        "# plot ROC curves\n",
        "\n",
        "roc.plot = ggplot(DT, aes(x = FPR, y = TPR, group = mod, color = mod)) +\n",
        "\t\t\tgeom_line(size = 1) +\n",
        "\t\t\tgeom_abline(slope = 1, size = 1) +\n",
        "\t\t\tscale_y_continuous(labels = percent) +\n",
        "\t\t\tscale_x_continuous(labels = percent) +\n",
        "\t\t\tlabs(x = \"False Positive\", y = \"True Positive\", color = \"Model\") + \n",
        "\t\t\ttheme_bw(base_size = 25) +\n",
        "\t\t\ttheme(legend.position = \"top\", legend.key.size = unit(.25, \"in\"), plot.title = element_text(hjust = 0.5)) +\n",
        "\t\t\tguides(color = guide_legend(override.aes = list(size = 10, linetype = 1), nrow = 1))\n",
        "\n",
        "roc.plot\n",
        "\n",
        "# plot the accuracy curves\n",
        "\t# the dashed lines indicate the cut-off for each model that maximizes accuracy\n",
        "\t\n",
        "acc.plot = ggplot(DT, aes(x = cutoff, y = ACC, group = mod, color = mod)) +\n",
        "\t\t\tgeom_line(size = 1) +\n",
        "\t\t\tgeom_vline(xintercept = cuts, color = ggcolor(length(cuts)), size = 1, linetype = \"dashed\") +\n",
        "\t\t\tscale_y_continuous(labels = percent) +\n",
        "\t\t\tscale_x_continuous(labels = percent) +\n",
        "\t\t\tlabs(x = \"Cut-Off\", y = \"Accuracy\", color = \"Model\") + \n",
        "\t\t\ttheme_bw(base_size = 25) +\n",
        "\t\t\ttheme(legend.position = \"top\", legend.key.size = unit(.25, \"in\"), plot.title = element_text(hjust = 0.5)) +\n",
        "\t\t\tguides(color = guide_legend(override.aes = list(size = 10, linetype = 1), nrow = 1))\n",
        "\n",
        "acc.plot\n",
        "\n",
        "# ---- CV ---------------------------------------------------------------------------\n",
        "\n",
        "# build a function that will report prediction results of our model\n",
        "\n",
        "nn.pred = function(Xtrain, Ytrain, Xtest, Ytest, negative, hidden, threshold, linear.output, cutoff)\n",
        "{\n",
        "\t# build the table for training the model\n",
        "\tdat = data.table(Xtrain)\n",
        "\tdat[, y := Ytrain]\n",
        "\t\n",
        "\t# build a formula string for the training model\n",
        "\trhs = paste(names(Xtrain), collapse = \" + \")\n",
        "\tform = paste(\"y ~\", rhs)\n",
        "\t\n",
        "\t# build the training model\n",
        "\tmod = neuralnet(form,\n",
        "\t\t\t\t\tdata = dat,\n",
        "\t\t\t\t\thidden = hidden,  \n",
        "\t\t\t\t\tthreshold = threshold, \n",
        "\t\t\t\t\tlinear.output = linear.output)\n",
        "\t\n",
        "\t# make predictions with the training model using the test set\n",
        "\tynew = as.numeric(compute(mod, data.table(Xtest))$net.result)\n",
        "\tynew = as.numeric(ynew >= cutoff)\t\n",
        "\n",
        "\t# build a confusion matrix to summarize the performance of our training model\n",
        "\toutput = confusionMatrix(Ytest, ynew, negative = negative)\n",
        "\toutput = c(output, \"AUC\" = as.numeric(auc(roc(Ytest ~ ynew))))\n",
        "\t\n",
        "\t# the confusion matrix is the output\n",
        "\treturn(output)\n",
        "}\n",
        "\n",
        "# compute diagonistic errors for each of the models in doe\n",
        "\n",
        "nn.diag = foreach(i = 1:nrow(doe)) %do%\n",
        "{\n",
        "\t# compute the hidden layer structure for model i\n",
        "\t\n",
        "\thidden = vector(length = doe$layers[i])\n",
        "\thidden[1] = round(doe$nodes[i] * doe$prop[i], 0)\n",
        "\tif(doe$layers[i] == 2) hidden[2] = doe$nodes[i] - hidden[1]\n",
        "\t\n",
        "\t# perform cross validation\n",
        "\t\n",
        "\tnn.cv = crossval(predfun = nn.pred, X = X, Y = Y, K = K, B = B, verbose = FALSE, negative = 0, \n",
        "\t\t\t\t\t\thidden = hidden, threshold = 0.25, linear.output = FALSE, cutoff = cuts[i])\n",
        "\n",
        "\t# compute diagnostic errors of cross validation\n",
        "\n",
        "\toutput = diag.cv(nn.cv)\n",
        "\n",
        "\t# add columns of parameter values that define model i\n",
        "\n",
        "\toutput[, layers := doe$layers[i]] \n",
        "\toutput[, nodes := doe$nodes[i]]\n",
        "\toutput[, prop := doe$prop[i]]\n",
        "\toutput[, mod := i]\n",
        "\t\n",
        "\treturn(output)\n",
        "}\n",
        "\n",
        "# combine the list of tables into one table\n",
        "nn.diag = rbindlist(nn.diag)\n",
        "\n",
        "# convert nn.diag into long format for plotting purposes\n",
        "DT = data.table(melt(nn.diag, id.vars = c(\"stat\", \"layers\", \"nodes\", \"prop\", \"mod\")))\n",
        "\n",
        "# convert mod into a factor for plotting purposes\n",
        "DT[, mod := factor(mod)]\n",
        "\n",
        "# remove Inf values as these don't help\n",
        "DT = data.table(DT[value < Inf])\n",
        "\n",
        "# plot barplots of each diagnostic metric\n",
        "\n",
        "diag.plot = ggplot(DT[stat == \"Q1\" | stat == \"Median\" | stat == \"Q3\"], aes(x = stat, y = value, group = mod, fill = mod)) +\n",
        "\t\t\tgeom_bar(stat = \"identity\", position = \"dodge\", color = \"white\") +\n",
        "\t\t\tlabs(x = \"Summary Statistic\", y = \"Value\", fill = \"Model\") + \n",
        "\t\t\tfacet_wrap(~variable, scales = \"free_y\") +\n",
        "\t\t\ttheme_bw(base_size = 15) +\n",
        "\t\t\ttheme(legend.position = \"top\", legend.key.size = unit(.25, \"in\"), plot.title = element_text(hjust = 0.5)) +\n",
        "\t\t\tguides(fill = guide_legend(override.aes = list(size = 10, linetype = 1), nrow = 1))\n",
        "\n",
        "diag.plot\n",
        "\n",
        "# ---- Results ----------------------------------------------------------------------\n",
        "\n",
        "# the roc and accuracy plots show similar performance \n",
        "# the diagnostics plot shows that model 8 has better odds ratio while being better tha nmost models across the other metrics\n",
        "\n",
        "# lets pick model 8\n",
        "\n",
        "nn.diag = nn.diag[mod == 8]\n",
        "\n",
        "# rename the model to nn as this is our chosen model\n",
        "\n",
        "nn.diag[, mod := rep(\"nn\", nrow(nn.diag))]\n",
        "\n",
        "# get the cutoff value and auc\n",
        "\n",
        "nn.cutoff = cuts[8]\n",
        "\n",
        "# build our model\n",
        "\n",
        "nn.mod = neuralnet(Survived ~ Sexmale + Sexmale.EmbarkedS + Pclass3.EmbarkedS + rescale.Age + rescale.Fare + rescale.SibSp.2 + rescale.Sexmale.Fare + rescale.Pclass3.Age + rescale.Pclass3.Fare, \n",
        "\t\t\t\t\tdata = train.nn, hidden = c(15, 15), threshold = 0.25, linear.output = FALSE)\n",
        "\n",
        "# store model diagnostic results\n",
        "\n",
        "nn.diag[, c(\"layers\", \"nodes\", \"prop\") := NULL]\n",
        "mods.diag = rbind(mods.diag, nn.diag)\n",
        "\n",
        "# store the model\n",
        "\n",
        "nn.list = list(\"mod\" = nn.mod, \"cutoff\" = nn.cutoff, \"train\" = train.nn, \"test\" = test.nn)\n",
        "mods.list$nn = nn.list\n",
        "\n",
        "# remove objects we no longer need\n",
        "\n",
        "rm(doe, DT, i, output, nn.pred, nn.cv, X, Y, nn.diag, nn.list, nn.mod,\n",
        "\tacc.plot, actuals, cutoffs, cuts, diag.plot, fits, mods, mod.name, roc.plot, rocs, rocs.cutoffs,\n",
        "\tdat, form, hidden, min.max, nn.cutoff, rhs, train.nn, test.nn)\n",
        "\n",
        "}\n",
        "\n",
        "# -----------------------------------------------------------------------------------\n",
        "# ---- Support Vector Machine Model -------------------------------------------------\n",
        "# -----------------------------------------------------------------------------------\n",
        "\n",
        "{\n",
        "\n",
        "# ---- Set Up -----------------------------------------------------------------------\n",
        "\n",
        "# we will focus on nu-classification\n",
        "# this is becuase the parameter 'nu' takes on values within the interval [0, 1] so it makes tuning simplier\n",
        "\n",
        "# we will focus on the radial basis as our kernel function for projecting our data into a higher demensional feature space becuase:\n",
        "\t# this seems to be the popular method\n",
        "\t# this only requires one parameter: 'gamma'\n",
        "\t# this may end up being less computationally intensive \n",
        "\t\t# becuase we are estimating how the data would behave in higher demsional space without directly computing this projection as other kernels do\n",
        "\n",
        "# hyperparameters of interest:\n",
        "\t# nu:\n",
        "\t\t# It is used to determine the proportion of support vectors you desire to keep in your solution with respect to the total number of samples in the dataset.\n",
        "\t\t# If this takes on a large value, then all of your data points could become support vectors, which is overfitting\n",
        "\t# gamma:\n",
        "\t\t# If gamma is too large, the radius of the area of influence of the support vectors only includes the support vector itself and no amount of regularization with 'cost' will be able to prevent overfitting.\n",
        "\t\t# When gamma is very small, the model is too constrained and cannot capture the complexity or \u201cshape\u201d of the data. The region of influence of any selected support vector would include the whole training set.\n",
        "\t\t# grid.py tool from the libSVM package checks values of gamma from 2^-15 to 2^3 \n",
        "\t\t\n",
        "# the default values of our parameters are:\n",
        "\t# nu = 0.5\n",
        "\t# gamma = 1 / (number of regressors)\n",
        "\n",
        "# gamma in our case\n",
        "1 / (ncol(train) - 1)\n",
        "\n",
        "# extract our predictors (X) and response (Y)\n",
        "\n",
        "X = data.table(train)\n",
        "Y = X$Survived\n",
        "X[, Survived := NULL]\n",
        "\n",
        "# lets use cross validation to tune our model\n",
        "\n",
        "# create parameter combinations to test\n",
        "\n",
        "doe = expand.grid(gamma = seq(from = 2^(-15), to = 2^3, length.out = 7),\n",
        "\t\t\t\t  nu = seq(from = 0.3, to = 0.7, by = 0.1))\n",
        "\n",
        "doe = rbind(c(1 / ncol(X), 0.5), doe)\n",
        "doe = doe[!duplicated(doe),]\n",
        "doe = data.table(doe)\n",
        "\n",
        "# build models\n",
        "mods = lapply(1:nrow(doe), function(i) \n",
        "\t\t\t\tsvm(Survived ~ ., data = train, type = \"nu-classification\",\n",
        "\t\t\t\t\tgamma = doe$gamma[i], nu = doe$nu[i], probability = TRUE))\n",
        "\n",
        "# ---- ROC --------------------------------------------------------------------------\n",
        "\n",
        "# create names for each model\n",
        "mod.name = 1:nrow(doe)\n",
        "\n",
        "# extract actual values\n",
        "actuals = as.numeric(as.character(train$Survived))\n",
        "\n",
        "# compute fitted values of each model\n",
        "fits = lapply(1:length(mods), function(i) as.numeric(attr(predict(mods[[i]], newdata = X, probability = TRUE), \"probabilities\")[,2]))\n",
        "\n",
        "# compute an roc object for each model\n",
        "rocs = lapply(1:length(fits), function(i) roc(actuals ~ fits[[i]]))\n",
        "\n",
        "# compute cutoffs to evaluate\n",
        "cutoffs = c(-Inf, seq(0.01, 0.99, 0.01), Inf)\n",
        "\n",
        "# compute roc curves for each model\n",
        "rocs.cutoffs = lapply(1:length(fits), function(i) data.table(t(coords(rocs[[i]], x = cutoffs, input = \"threshold\", ret = c(\"threshold\", \"specificity\", \"sensitivity\", \"accuracy\")))))\n",
        "\n",
        "# aggregate results into one table\n",
        "DT = rbindlist(lapply(1:length(fits), function(i) data.table(mod = mod.name[i], \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tcutoff = rocs.cutoffs[[i]]$threshold, \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tTPR = rocs.cutoffs[[i]]$sensitivity, \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tFPR = 1 - rocs.cutoffs[[i]]$specificity, \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tACC = rocs.cutoffs[[i]]$accuracy)))\n",
        "\n",
        "# make mod into a factor data type to facilitate plotting\n",
        "DT[, mod := factor(mod)]\n",
        "\n",
        "# compute cut-offs points that maximize accuracy\n",
        "cuts = as.numeric(sapply(1:length(fits), function(i) \n",
        "\t\t\t\t\t\t\tcoords(rocs[[i]], \n",
        "\t\t\t\t\t\t\t\t\tx = \"best\", \n",
        "\t\t\t\t\t\t\t\t\tbest.weights = c(1, length(rocs[[i]]$cases) / length(rocs[[i]]$response)))[1]))\n",
        "\n",
        "# plot ROC curves\n",
        "\n",
        "roc.plot = ggplot(DT, aes(x = FPR, y = TPR, group = mod, color = mod)) +\n",
        "\t\t\tgeom_line(size = 1) +\n",
        "\t\t\tgeom_abline(slope = 1, size = 1) +\n",
        "\t\t\tscale_y_continuous(labels = percent) +\n",
        "\t\t\tscale_x_continuous(labels = percent) +\n",
        "\t\t\tlabs(x = \"False Positive\", y = \"True Positive\", color = \"Model\") + \n",
        "\t\t\ttheme_bw(base_size = 25) +\n",
        "\t\t\ttheme(legend.position = \"top\", legend.key.size = unit(.25, \"in\"), plot.title = element_text(hjust = 0.5)) +\n",
        "\t\t\tguides(color = guide_legend(override.aes = list(size = 10, linetype = 1), nrow = 2))\n",
        "\n",
        "roc.plot\n",
        "\n",
        "# plot the accuracy curves\n",
        "\t# the dashed lines indicate the cut-off for each model that maximizes accuracy\n",
        "\t\n",
        "acc.plot = ggplot(DT, aes(x = cutoff, y = ACC, group = mod, color = mod)) +\n",
        "\t\t\tgeom_line(size = 1) +\n",
        "\t\t\tgeom_vline(xintercept = cuts, color = ggcolor(length(cuts)), size = 1, linetype = \"dashed\") +\n",
        "\t\t\tscale_y_continuous(labels = percent) +\n",
        "\t\t\tscale_x_continuous(labels = percent) +\n",
        "\t\t\tlabs(x = \"Cut-Off\", y = \"Accuracy\", color = \"Model\") + \n",
        "\t\t\ttheme_bw(base_size = 25) +\n",
        "\t\t\ttheme(legend.position = \"top\", legend.key.size = unit(.25, \"in\"), plot.title = element_text(hjust = 0.5)) +\n",
        "\t\t\tguides(color = guide_legend(override.aes = list(size = 10, linetype = 1), nrow = 2))\n",
        "\n",
        "acc.plot\n",
        "\n",
        "# ---- CV ---------------------------------------------------------------------------\n",
        "\n",
        "# build a function that will report prediction results of our model\n",
        "\n",
        "svm.pred = function(Xtrain, Ytrain, Xtest, Ytest, negative, gamma, nu, cutoff)\n",
        "{\n",
        "\t# build the table for training the model\n",
        "\tdat = data.table(Xtrain)\n",
        "\tdat[, y := Ytrain]\n",
        "\n",
        "\t# build the training model\n",
        "\tmod = svm(y ~ .,\n",
        "\t\t\t\tdata = dat,\n",
        "\t\t\t\ttype = \"nu-classification\",\n",
        "\t\t\t\tgamma = gamma,\n",
        "\t\t\t\tnu = nu,\n",
        "\t\t\t\tprobability = TRUE)\n",
        "\t\n",
        "\t# make predictions with the training model using the test set\n",
        "\tynew = as.numeric(attr(predict(mod, newdata = data.table(Xtest), probability = TRUE), \"probabilities\")[,2])\n",
        "\tynew = as.numeric(ynew >= cutoff)\n",
        "\tYtest = as.numeric(as.character(Ytest))\n",
        "\t\n",
        "\t# build a confusion matrix to summarize the performance of our training model\n",
        "\toutput = confusionMatrix(Ytest, ynew, negative = negative)\n",
        "\toutput = c(output, \"AUC\" = as.numeric(auc(roc(Ytest ~ ynew))))\n",
        "\t\n",
        "\t# the confusion matrix is the output\n",
        "\treturn(output)\n",
        "}\n",
        "\n",
        "# compute diagonistic errors for each of the models in doe\n",
        "\n",
        "svm.diag = foreach(i = 1:nrow(doe)) %do%\n",
        "{\n",
        "\t# perform cross validation\n",
        "\t\n",
        "\tsvm.cv = crossval(predfun = svm.pred, X = X, Y = Y, K = K, B = B, \n",
        "\t\t\t\t\t\tverbose = FALSE, negative = 0, gamma = doe$gamma[i], nu = doe$nu[i], cutoff = cuts[i])\n",
        "\t\n",
        "\t# compute diagnostic errors of cross validation\n",
        "\t\n",
        "\toutput = diag.cv(svm.cv)\n",
        "\t\n",
        "\t# add columns of parameter values that define model i\n",
        "\t\n",
        "\toutput[, gamma := doe$gamma[i]] \n",
        "\toutput[, nu := doe$nu[i]]\n",
        "\toutput[, mod := i]\n",
        "\t\n",
        "\treturn(output)\n",
        "}\n",
        "\n",
        "# combine the list of tables into one table\n",
        "svm.diag = rbindlist(svm.diag)\n",
        "\n",
        "# convert svm.diag into long format for plotting purposes\n",
        "DT = data.table(melt(svm.diag, id.vars = c(\"stat\", \"gamma\", \"nu\", \"mod\")))\n",
        "\n",
        "# convert mod into a factor for plotting purposes\n",
        "DT[, mod := factor(mod)]\n",
        "\n",
        "# remove Inf values as these don't help\n",
        "DT = data.table(DT[value < Inf])\n",
        "\n",
        "# plot barplots of each diagnostic metric\n",
        "\n",
        "diag.plot = ggplot(DT[stat == \"Q1\" | stat == \"Median\" | stat == \"Q3\"], aes(x = stat, y = value, group = mod, fill = mod)) +\n",
        "\t\t\tgeom_bar(stat = \"identity\", position = \"dodge\", color = \"white\") +\n",
        "\t\t\tlabs(x = \"Summary Statistic\", y = \"Value\", fill = \"Model\") + \n",
        "\t\t\tfacet_wrap(~variable, scales = \"free_y\") +\n",
        "\t\t\ttheme_bw(base_size = 15) +\n",
        "\t\t\ttheme(legend.position = \"top\", legend.key.size = unit(.25, \"in\"), plot.title = element_text(hjust = 0.5)) +\n",
        "\t\t\tguides(fill = guide_legend(override.aes = list(size = 10, linetype = 1), nrow = 2))\n",
        "\n",
        "diag.plot\n",
        "\n",
        "# some models have poor sensitivity relative to other models\n",
        "# and some models have significantly better odds ratio\n",
        "\n",
        "# ---- Results ----------------------------------------------------------------------\n",
        "\n",
        "# lets filter out models based on sensitivity and odds ratio\n",
        "\n",
        "svm.diag[stat == \"Median\" & or >= 10 & acc >= 0.7 & auc >= 0.77]\n",
        "\n",
        "# lets go with model 32\n",
        "\n",
        "svm.diag = svm.diag[mod == 32]\n",
        "\n",
        "# rename model to svm as this is our chosen model\n",
        "\n",
        "svm.diag[, mod := rep(\"svm\", nrow(svm.diag))]\n",
        "\n",
        "# build our model\n",
        "\n",
        "svm.mod = svm(Survived ~ ., data = train, type = \"nu-classification\",\n",
        "\t\t\t\tgamma = 2.66668701171875, nu = 0.7, probability = TRUE)\n",
        "\n",
        "# store model diagnostic results\n",
        "\n",
        "svm.diag[, c(\"gamma\", \"nu\") := NULL]\n",
        "mods.diag = rbind(mods.diag, svm.diag)\n",
        "\n",
        "# store the model\n",
        "\n",
        "svm.list = list(\"mod\" = svm.mod, \"cutoff\" = cuts[32])\n",
        "mods.list$svm = svm.list\n",
        "\n",
        "# remove objects we no longer need\n",
        "\n",
        "rm(doe, DT, i, output, svm.pred, svm.cv, X, Y, svm.diag, svm.list, svm.mod,\n",
        "\tacc.plot, actuals, cutoffs, cuts, diag.plot, fits, mods, mod.name, roc.plot, rocs, rocs.cutoffs)\n",
        "\n",
        "}\n",
        "\n",
        "# -----------------------------------------------------------------------------------\n",
        "# ---- Super Learner Model ----------------------------------------------------------\n",
        "# -----------------------------------------------------------------------------------\n",
        "\n",
        "{\n",
        "\n",
        "# ---- Set Up -----------------------------------------------------------------------\n",
        "\n",
        "# extract our predictors (X) and response (Y)\n",
        "\n",
        "X = data.table(train)\n",
        "Y = as.numeric(as.character(X$Survived))\n",
        "X[, Survived := NULL]\n",
        "X = as.matrix(X)\n",
        "\n",
        "# create Super Learner wrappers for our models of interest\n",
        "\n",
        "# logistic regression wrapper\n",
        "my.log = function(Y, X, newX, ...)\n",
        "{\n",
        "\t# build the table for training the model\n",
        "\tdat = data.table(X)\n",
        "\tdat[, y := factor(Y, levels = 0:1)]\n",
        "\t\n",
        "\t# build the training model\n",
        "\tmod = glm(y ~ ., data = dat,\n",
        "\t\t\t\tfamily = binomial(link = \"logit\"), \n",
        "\t\t\t\tcontrol = list(maxit = 100))\n",
        "\t\n",
        "\t# make predictions with the training model using the test set\n",
        "\tynew = predict(mod, data.table(newX), type = \"response\")\n",
        "\tynew = as.numeric(ynew >= mods.list$log$cutoff)\n",
        "\t\n",
        "\t# return a list of the model (must label as fit) and predictions (must label as pred)\n",
        "\toutput = list(pred = ynew, fit = mod)\n",
        "\treturn(output)\n",
        "}\n",
        "\n",
        "# linear discriminant analysis wrapper\n",
        "my.lda = function(Y, X, newX, ...)\n",
        "{\n",
        "\t# build the table for training the model\n",
        "\tdat = data.table(X)\n",
        "\tdat[, y := factor(Y, levels = 0:1)]\n",
        "\t\n",
        "\t# build the training model\n",
        "\tmod = lda(y ~ ., data = dat)\n",
        "\t\n",
        "\t# make predictions with the training model using the test set\n",
        "\tynew = as.numeric(predict(mod, newdata = data.table(newX))$posterior[,2])\n",
        "\tynew = as.numeric(ynew >= mods.list$lda$cutoff)\n",
        "\t\n",
        "\t# return a list of the model (must label as fit) and predictions (must label as pred)\n",
        "\toutput = list(pred = ynew, fit = mod)\n",
        "\treturn(output)\n",
        "}\n",
        "\n",
        "# quadratic discriminant analysis wrapper\n",
        "my.qda = function(Y, X, newX, ...)\n",
        "{\n",
        "\t# build the table for training the model\n",
        "\tdat = data.table(X)\n",
        "\tdat[, y := factor(Y, levels = 0:1)]\n",
        "\t\n",
        "\t# build the training model\n",
        "\tmod = qda(y ~ ., data = dat)\n",
        "\t\n",
        "\t# make predictions with the training model using the test set\n",
        "\tynew = as.numeric(predict(mod, newdata = data.table(newX))$posterior[,2])\n",
        "\tynew = as.numeric(ynew >= mods.list$qda$cutoff)\n",
        "\t\n",
        "\t# return a list of the model (must label as fit) and predictions (must label as pred)\n",
        "\toutput = list(pred = ynew, fit = mod)\n",
        "\treturn(output)\n",
        "}\n",
        "\n",
        "# penalty regression wrapper\n",
        "my.pen = function(Y, X, newX, ...)\n",
        "{\n",
        "\t# make Y into a factor data type\n",
        "\tY = factor(Y, levels = 0:1)\n",
        "\t\n",
        "\t# build the training model\n",
        "\tmod = cv.glmnet(x = X, y = Y, family = \"binomial\", alpha = 0.1)\n",
        "\t\n",
        "\t# make predictions with the training model using the test set\n",
        "\tynew = as.numeric(predict(mod, s = mod$lambda.min, newX, type = \"response\"))\n",
        "\tynew = as.numeric(ynew >= mods.list$pen$cutoff)\n",
        "\t\n",
        "\t# return a list of the model (must label as fit) and predictions (must label as pred)\n",
        "\toutput = list(pred = ynew, fit = mod)\n",
        "\treturn(output)\n",
        "}\n",
        "\n",
        "# gradient boosting wrapper\n",
        "my.gbm = function(Y, X, newX, ...)\n",
        "{\n",
        "\t# build the training model\n",
        "\tmod = xgboost(label = Y, data = X,\n",
        "\t\t\t\t\tobjective = \"binary:logistic\", eval_metric = \"auc\",\n",
        "\t\t\t\t\teta = 0.1, max_depth = 4,\n",
        "\t\t\t\t\tnrounds = 500, min_child_weight = 4.714936,\n",
        "\t\t\t\t\tgamma = 0, verbose = 0)\n",
        "\t\n",
        "\t# make predictions with the training model using the test set\n",
        "\tynew = as.numeric(predict(mod, newdata = newX))\n",
        "\tynew = as.numeric(ynew >= mods.list$gbm$cutoff)\n",
        "\t\n",
        "\t# return a list of the model (must label as fit) and predictions (must label as pred)\n",
        "\toutput = list(pred = ynew, fit = mod)\n",
        "\treturn(output)\n",
        "}\n",
        "\n",
        "# random forest wrapper\n",
        "my.rf = function(Y, X, newX, ...)\n",
        "{\n",
        "\t# build the table for training the model\n",
        "\tdat = data.table(X)\n",
        "\tdat[, y := factor(Y, levels = 0:1)]\n",
        "\t\n",
        "\t# build the training model\n",
        "\tmod = randomForest(y ~ .,\n",
        "\t\t\t\t\t\tdata = dat,\n",
        "\t\t\t\t\t\tntree = 500,\n",
        "\t\t\t\t\t\tmtry = 3,\n",
        "\t\t\t\t\t\tnodesize = 5)\n",
        "\t\n",
        "\t# make predictions with the training model using the test set\n",
        "\tynew = as.numeric(predict(mod, newdata = data.table(newX), type = \"prob\")[,2])\n",
        "\tynew = as.numeric(ynew >= mods.list$rf$cutoff)\n",
        "\t\n",
        "\t# return a list of the model (must label as fit) and predictions (must label as pred)\n",
        "\toutput = list(pred = ynew, fit = mod)\n",
        "\treturn(output)\n",
        "}\n",
        "\n",
        "# neural network wrapper\n",
        "my.nn = function(Y, X, newX, ...)\n",
        "{\n",
        "\t# build the table for training the model\n",
        "\tdat = data.table(X)\n",
        "\tdat[, y := Y]\n",
        "\t\n",
        "\t# build the table for the predictions\n",
        "\tnew.dat = data.table(newX)\n",
        "\t\n",
        "\t# rescale columns in dat that have a max value above 1 and/or a min value below 0\n",
        "\t\t# a 0 to 1 scaling is required for neural networks\n",
        "\tdat[, rescale.Age := rescale(Age, to = c(0, 1), from = range(train$Age))]\n",
        "\tdat[, rescale.Fare := rescale(Fare, to = c(0, 1), from = range(train$Fare))]\n",
        "\tdat[, rescale.SibSp.2 := rescale(SibSp.2, to = c(0, 1), from = range(train$SibSp.2))]\n",
        "\tdat[, rescale.Sexmale.Fare := rescale(Sexmale.Fare, to = c(0, 1), from = range(train$Sexmale.Fare))]\n",
        "\tdat[, rescale.Pclass3.Age := rescale(Pclass3.Age, to = c(0, 1), from = range(train$Pclass3.Age))]\n",
        "\tdat[, rescale.Pclass3.Fare := rescale(Pclass3.Fare, to = c(0, 1), from = range(train$Pclass3.Fare))]\n",
        "\t\n",
        "\t# remove columns that violate the scale\n",
        "\tdat[, c(\"Age\", \"Fare\", \"SibSp.2\", \"Sexmale.Fare\", \"Pclass3.Age\", \"Pclass3.Fare\") := NULL]\n",
        "\t\n",
        "\t# rescale columns in new.dat that have a max value above 1 and/or a min value below 0\n",
        "\t\t# a 0 to 1 scaling is required for neural networks\n",
        "\tnew.dat[, rescale.Age := rescale(Age, to = c(0, 1), from = range(train$Age))]\n",
        "\tnew.dat[, rescale.Fare := rescale(Fare, to = c(0, 1), from = range(train$Fare))]\n",
        "\tnew.dat[, rescale.SibSp.2 := rescale(SibSp.2, to = c(0, 1), from = range(train$SibSp.2))]\n",
        "\tnew.dat[, rescale.Sexmale.Fare := rescale(Sexmale.Fare, to = c(0, 1), from = range(train$Sexmale.Fare))]\n",
        "\tnew.dat[, rescale.Pclass3.Age := rescale(Pclass3.Age, to = c(0, 1), from = range(train$Pclass3.Age))]\n",
        "\tnew.dat[, rescale.Pclass3.Fare := rescale(Pclass3.Fare, to = c(0, 1), from = range(train$Pclass3.Fare))]\n",
        "\t\n",
        "\t# remove columns that violate the scale\n",
        "\tnew.dat[, c(\"Age\", \"Fare\", \"SibSp.2\", \"Sexmale.Fare\", \"Pclass3.Age\", \"Pclass3.Fare\") := NULL]\n",
        "\t\n",
        "\t# build a formula string for the training model\n",
        "\trhs = paste(names(new.dat), collapse = \" + \")\n",
        "\tform = paste(\"y ~\", rhs)\n",
        "\t\n",
        "\t# build the training model\n",
        "\tmod = neuralnet(form,\n",
        "\t\t\t\t\tdata = dat,\n",
        "\t\t\t\t\thidden = c(15, 15), \n",
        "\t\t\t\t\tthreshold = 0.25, \n",
        "\t\t\t\t\tlinear.output = FALSE)\n",
        "\t\n",
        "\t# make predictions with the training model using the test set\n",
        "\tynew = as.numeric(compute(mod, new.dat)$net.result)\n",
        "\tynew = as.numeric(ynew >= mods.list$nn$cutoff)\t\n",
        "\t\n",
        "\t# return a list of the model (must label as fit) and predictions (must label as pred)\n",
        "\toutput = list(pred = ynew, fit = mod)\n",
        "\treturn(output)\n",
        "}\n",
        "\n",
        "# support vector machine wrapper\n",
        "my.svm = function(Y, X, newX, ...)\n",
        "{\n",
        "\t# build the table for training the model\n",
        "\tdat = data.table(X)\n",
        "\tdat[, y := factor(Y, levels = 0:1)]\n",
        "\t\n",
        "\t# build the training model\n",
        "\tmod = svm(y ~ .,\n",
        "\t\t\t\tdata = dat,\n",
        "\t\t\t\ttype = \"nu-classification\",\n",
        "\t\t\t\tgamma = 2.66668701171875, \n",
        "\t\t\t\tnu = 0.7,\n",
        "\t\t\t\tprobability = TRUE)\n",
        "\t\n",
        "\t# make predictions with the training model using the test set\n",
        "\tynew = as.numeric(attr(predict(mod, newdata = data.table(newX), probability = TRUE), \"probabilities\")[,2])\n",
        "\tynew = as.numeric(ynew >= mods.list$svm$cutoff)\n",
        "\t\n",
        "\t# return a list of the model (must label as fit) and predictions (must label as pred)\n",
        "\toutput = list(pred = ynew, fit = mod)\n",
        "\treturn(output)\n",
        "}\n",
        "\n",
        "# create a library of the above wrappers\n",
        "my.library = list(\"my.log\", \"my.pen\", \"my.lda\", \"my.qda\", \"my.gbm\", \"my.rf\", \"my.nn\", \"my.svm\")\n",
        "\n",
        "# build the super learner model\n",
        "sl.mod = SuperLearner(Y = Y, X = X, family = binomial(), \n",
        "\t\t\t\t\t\tSL.library = my.library, method = \"method.AUC\")\n",
        "\n",
        "# ---- ROC --------------------------------------------------------------------------\n",
        "\n",
        "# create names for each model\n",
        "mod.name = c(\"Super Learner\")\n",
        "\n",
        "# extract actual values\n",
        "actuals = Y\n",
        "\n",
        "# compute fitted values of each model\n",
        "fits = list(as.numeric(sl.mod$SL.predict))\n",
        "\n",
        "# compute an roc object for each model\n",
        "rocs = lapply(1:length(fits), function(i) roc(actuals ~ fits[[i]]))\n",
        "\n",
        "# compute cutoffs to evaluate\n",
        "cutoffs = c(-Inf, seq(0.01, 0.99, 0.01), Inf)\n",
        "\n",
        "# compute roc curves for each model\n",
        "rocs.cutoffs = lapply(1:length(fits), function(i) data.table(t(coords(rocs[[i]], x = cutoffs, input = \"threshold\", ret = c(\"threshold\", \"specificity\", \"sensitivity\", \"accuracy\")))))\n",
        "\n",
        "# aggregate results into one table\n",
        "DT = rbindlist(lapply(1:length(fits), function(i) data.table(mod = mod.name[i], \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tcutoff = rocs.cutoffs[[i]]$threshold, \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tTPR = rocs.cutoffs[[i]]$sensitivity, \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tFPR = 1 - rocs.cutoffs[[i]]$specificity, \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tACC = rocs.cutoffs[[i]]$accuracy)))\n",
        "\n",
        "# make mod into a factor data type to facilitate plotting\n",
        "DT[, mod := factor(mod)]\n",
        "\n",
        "# compute cut-offs points that maximize accuracy\n",
        "cuts = as.numeric(sapply(1:length(fits), function(i) \n",
        "\t\t\t\t\t\t\tcoords(rocs[[i]], \n",
        "\t\t\t\t\t\t\t\t\tx = \"best\", \n",
        "\t\t\t\t\t\t\t\t\tbest.weights = c(1, length(rocs[[i]]$cases) / length(rocs[[i]]$response)))[1]))\n",
        "\n",
        "# plot ROC curves\n",
        "\n",
        "roc.plot = ggplot(DT, aes(x = FPR, y = TPR, group = mod, color = mod)) +\n",
        "\t\t\tgeom_line(size = 1) +\n",
        "\t\t\tgeom_abline(slope = 1, size = 1) +\n",
        "\t\t\tscale_y_continuous(labels = percent) +\n",
        "\t\t\tscale_x_continuous(labels = percent) +\n",
        "\t\t\tlabs(x = \"False Positive\", y = \"True Positive\", color = \"Model\") + \n",
        "\t\t\ttheme_bw(base_size = 25) +\n",
        "\t\t\ttheme(legend.position = \"top\", legend.key.size = unit(.25, \"in\"), plot.title = element_text(hjust = 0.5)) +\n",
        "\t\t\tguides(color = guide_legend(override.aes = list(size = 10, linetype = 1), nrow = 1))\n",
        "\n",
        "roc.plot\n",
        "\n",
        "# plot the accuracy curves\n",
        "\t# the dashed lines indicate the cut-off for each model that maximizes accuracy\n",
        "\t\n",
        "acc.plot = ggplot(DT, aes(x = cutoff, y = ACC, group = mod, color = mod)) +\n",
        "\t\t\tgeom_line(size = 1) +\n",
        "\t\t\tgeom_vline(xintercept = cuts, color = ggcolor(length(cuts)), size = 1, linetype = \"dashed\") +\n",
        "\t\t\tscale_y_continuous(labels = percent) +\n",
        "\t\t\tscale_x_continuous(labels = percent) +\n",
        "\t\t\tlabs(x = \"Cut-Off\", y = \"Accuracy\", color = \"Model\") + \n",
        "\t\t\ttheme_bw(base_size = 25) +\n",
        "\t\t\ttheme(legend.position = \"top\", legend.key.size = unit(.25, \"in\"), plot.title = element_text(hjust = 0.5)) +\n",
        "\t\t\tguides(color = guide_legend(override.aes = list(size = 10, linetype = 1), nrow = 1))\n",
        "\n",
        "acc.plot\n",
        "\n",
        "# ---- CV ---------------------------------------------------------------------------\n",
        "\n",
        "# build a function that will report prediction results of our models\n",
        "\n",
        "sl.pred = function(Xtrain, Ytrain, Xtest, Ytest, negative, cutoff)\n",
        "{\n",
        "\t# build the training model\n",
        "\tmod = SuperLearner(Y = Ytrain, X = Xtrain, newX = Xtest, family = binomial(), \n",
        "\t\t\t\t\t\tSL.library = my.library, method = \"method.AUC\")\n",
        "\t\n",
        "\t# make predictions with the training model using the test set\n",
        "\tynew = as.numeric(mod$SL.predict)\n",
        "\tynew = as.numeric(ynew >= cutoff)\n",
        "\t\n",
        "\t# build a confusion matrix to summarize the performance of our training model\n",
        "\toutput = confusionMatrix(Ytest, ynew, negative = negative)\n",
        "\toutput = c(output, \"AUC\" = as.numeric(auc(roc(Ytest ~ ynew))))\n",
        "\t\n",
        "\t# the confusion matrix is the output\n",
        "\treturn(output)\n",
        "}\n",
        "\n",
        "# perform cross validation\n",
        "\n",
        "sl.cv = crossval(predfun = sl.pred, X = X, Y = Y, K = K, B = B, \n",
        "\t\t\t\t\tverbose = FALSE, cutoff = cuts, negative = 0)\n",
        "\n",
        "# compute diagnostic errors of cross validation\n",
        "\n",
        "sl.diag = diag.cv(sl.cv)\n",
        "\n",
        "# ---- Results ----------------------------------------------------------------------\n",
        "\n",
        "# store model diagnostic results\n",
        "\n",
        "sl.diag[, mod := rep(\"sl\", nrow(sl.diag))]\n",
        "mods.diag = rbind(mods.diag, sl.diag)\n",
        "\n",
        "# store the model\n",
        "\n",
        "sl.list = list(\"mod\" = sl.mod, \"cutoff\" = cuts)\n",
        "mods.list$sl = sl.list\n",
        "\n",
        "# remove objects we no longer need\n",
        "\n",
        "rm(DT, sl.pred, sl.cv, X, Y, sl.diag, sl.list, sl.mod, my.gbm, my.lda, my.qda, my.log,\n",
        "\tacc.plot, actuals, cutoffs, cuts, fits, mod.name, roc.plot, rocs, rocs.cutoffs,\n",
        "\tmy.pen, my.rf, my.nn, my.svm, my.library)\n",
        "\n",
        "}\n",
        "\n",
        "# -----------------------------------------------------------------------------------\n",
        "# ---- Model Selection --------------------------------------------------------------\n",
        "# -----------------------------------------------------------------------------------\n",
        "\n",
        "{\n",
        "\n",
        "# ---- Diagnostics ------------------------------------------------------------------\n",
        "\n",
        "# convert mods.diag into long format for plotting purposes\n",
        "DT = data.table(melt(mods.diag, id.vars = c(\"stat\", \"mod\")))\n",
        "\n",
        "# convert mod into a factor for plotting purposes\n",
        "DT[, mod := factor(mod)]\n",
        "\n",
        "# remove Inf values as these don't help\n",
        "DT = data.table(DT[value < Inf])\n",
        "\n",
        "# plot barplots of each diagnostic metric\n",
        "\n",
        "diag.plot = ggplot(DT[stat == \"Q1\" | stat == \"Median\" | stat == \"Q3\"], aes(x = stat, y = value, group = mod, fill = mod)) +\n",
        "\t\t\tgeom_bar(stat = \"identity\", position = \"dodge\", color = \"white\") +\n",
        "\t\t\tlabs(x = \"Summary Statistic\", y = \"Value\", fill = \"Model\") + \n",
        "\t\t\tfacet_wrap(~variable, scales = \"free_y\") +\n",
        "\t\t\ttheme_bw(base_size = 15) +\n",
        "\t\t\ttheme(legend.position = \"top\", legend.key.size = unit(.25, \"in\"), plot.title = element_text(hjust = 0.5)) +\n",
        "\t\t\tguides(fill = guide_legend(override.aes = list(size = 10, linetype = 1), nrow = 2))\n",
        "\n",
        "diag.plot\n",
        "\n",
        "# lets go with the super learner\n",
        "\n",
        "# ---- predict test -----------------------------------------------------------------\n",
        "\n",
        "# extract our predictors (X) and response (Y)\n",
        "\n",
        "X = data.table(train)\n",
        "Y = as.numeric(as.character(X$Survived))\n",
        "X[, Survived := NULL]\n",
        "X = as.matrix(X)\n",
        "newX = as.matrix(test)\n",
        "\n",
        "# create Super Learner wrappers\n",
        "\n",
        "# logistic regression wrapper\n",
        "my.log = function(Y, X, newX, ...)\n",
        "{\n",
        "\t# build the table for training the model\n",
        "\tdat = data.table(X)\n",
        "\tdat[, y := factor(Y, levels = 0:1)]\n",
        "\t\n",
        "\t# build the training model\n",
        "\tmod = glm(y ~ ., data = dat,\n",
        "\t\t\t\tfamily = binomial(link = \"logit\"), \n",
        "\t\t\t\tcontrol = list(maxit = 100))\n",
        "\t\n",
        "\t# make predictions with the training model using the test set\n",
        "\tynew = predict(mod, data.table(newX), type = \"response\")\n",
        "\tynew = as.numeric(ynew >= mods.list$log$cutoff)\n",
        "\t\n",
        "\t# return a list of the model (must label as fit) and predictions (must label as pred)\n",
        "\toutput = list(pred = ynew, fit = mod)\n",
        "\treturn(output)\n",
        "}\n",
        "\n",
        "# linear discriminant analysis wrapper\n",
        "my.lda = function(Y, X, newX, ...)\n",
        "{\n",
        "\t# build the table for training the model\n",
        "\tdat = data.table(X)\n",
        "\tdat[, y := factor(Y, levels = 0:1)]\n",
        "\t\n",
        "\t# build the training model\n",
        "\tmod = lda(y ~ ., data = dat)\n",
        "\t\n",
        "\t# make predictions with the training model using the test set\n",
        "\tynew = as.numeric(predict(mod, newdata = data.table(newX))$posterior[,2])\n",
        "\tynew = as.numeric(ynew >= mods.list$lda$cutoff)\n",
        "\t\n",
        "\t# return a list of the model (must label as fit) and predictions (must label as pred)\n",
        "\toutput = list(pred = ynew, fit = mod)\n",
        "\treturn(output)\n",
        "}\n",
        "\n",
        "# quadratic discriminant analysis wrapper\n",
        "my.qda = function(Y, X, newX, ...)\n",
        "{\n",
        "\t# build the table for training the model\n",
        "\tdat = data.table(X)\n",
        "\tdat[, y := factor(Y, levels = 0:1)]\n",
        "\t\n",
        "\t# build the training model\n",
        "\tmod = qda(y ~ ., data = dat)\n",
        "\t\n",
        "\t# make predictions with the training model using the test set\n",
        "\tynew = as.numeric(predict(mod, newdata = data.table(newX))$posterior[,2])\n",
        "\tynew = as.numeric(ynew >= mods.list$qda$cutoff)\n",
        "\t\n",
        "\t# return a list of the model (must label as fit) and predictions (must label as pred)\n",
        "\toutput = list(pred = ynew, fit = mod)\n",
        "\treturn(output)\n",
        "}\n",
        "\n",
        "# penalty regression wrapper\n",
        "my.pen = function(Y, X, newX, ...)\n",
        "{\n",
        "\t# make Y into a factor data type\n",
        "\tY = factor(Y, levels = 0:1)\n",
        "\t\n",
        "\t# build the training model\n",
        "\tmod = cv.glmnet(x = X, y = Y, family = \"binomial\", alpha = 0.1)\n",
        "\t\n",
        "\t# make predictions with the training model using the test set\n",
        "\tynew = as.numeric(predict(mod, s = mod$lambda.min, newX, type = \"response\"))\n",
        "\tynew = as.numeric(ynew >= mods.list$pen$cutoff)\n",
        "\t\n",
        "\t# return a list of the model (must label as fit) and predictions (must label as pred)\n",
        "\toutput = list(pred = ynew, fit = mod)\n",
        "\treturn(output)\n",
        "}\n",
        "\n",
        "# gradient boosting wrapper\n",
        "my.gbm = function(Y, X, newX, ...)\n",
        "{\n",
        "\t# build the training model\n",
        "\tmod = xgboost(label = Y, data = X,\n",
        "\t\t\t\t\tobjective = \"binary:logistic\", eval_metric = \"auc\",\n",
        "\t\t\t\t\teta = 0.1, max_depth = 4,\n",
        "\t\t\t\t\tnrounds = 500, min_child_weight = 4.714936,\n",
        "\t\t\t\t\tgamma = 0, verbose = 0)\n",
        "\t\n",
        "\t# make predictions with the training model using the test set\n",
        "\tynew = as.numeric(predict(mod, newdata = newX))\n",
        "\tynew = as.numeric(ynew >= mods.list$gbm$cutoff)\n",
        "\t\n",
        "\t# return a list of the model (must label as fit) and predictions (must label as pred)\n",
        "\toutput = list(pred = ynew, fit = mod)\n",
        "\treturn(output)\n",
        "}\n",
        "\n",
        "# random forest wrapper\n",
        "my.rf = function(Y, X, newX, ...)\n",
        "{\n",
        "\t# build the table for training the model\n",
        "\tdat = data.table(X)\n",
        "\tdat[, y := factor(Y, levels = 0:1)]\n",
        "\t\n",
        "\t# build the training model\n",
        "\tmod = randomForest(y ~ .,\n",
        "\t\t\t\t\t\tdata = dat,\n",
        "\t\t\t\t\t\tntree = 500,\n",
        "\t\t\t\t\t\tmtry = 3,\n",
        "\t\t\t\t\t\tnodesize = 5)\n",
        "\t\n",
        "\t# make predictions with the training model using the test set\n",
        "\tynew = as.numeric(predict(mod, newdata = data.table(newX), type = \"prob\")[,2])\n",
        "\tynew = as.numeric(ynew >= mods.list$rf$cutoff)\n",
        "\t\n",
        "\t# return a list of the model (must label as fit) and predictions (must label as pred)\n",
        "\toutput = list(pred = ynew, fit = mod)\n",
        "\treturn(output)\n",
        "}\n",
        "\n",
        "# neural network wrapper\n",
        "my.nn = function(Y, X, newX, ...)\n",
        "{\n",
        "\t# build the table for training the model\n",
        "\tdat = data.table(X)\n",
        "\tdat[, y := Y]\n",
        "\t\n",
        "\t# build the table for the predictions\n",
        "\tnew.dat = data.table(newX)\n",
        "\t\n",
        "\t# rescale columns in dat that have a max value above 1 and/or a min value below 0\n",
        "\t\t# a 0 to 1 scaling is required for neural networks\n",
        "\tdat[, rescale.Age := rescale(Age, to = c(0, 1), from = range(train$Age))]\n",
        "\tdat[, rescale.Fare := rescale(Fare, to = c(0, 1), from = range(train$Fare))]\n",
        "\tdat[, rescale.SibSp.2 := rescale(SibSp.2, to = c(0, 1), from = range(train$SibSp.2))]\n",
        "\tdat[, rescale.Sexmale.Fare := rescale(Sexmale.Fare, to = c(0, 1), from = range(train$Sexmale.Fare))]\n",
        "\tdat[, rescale.Pclass3.Age := rescale(Pclass3.Age, to = c(0, 1), from = range(train$Pclass3.Age))]\n",
        "\tdat[, rescale.Pclass3.Fare := rescale(Pclass3.Fare, to = c(0, 1), from = range(train$Pclass3.Fare))]\n",
        "\t\n",
        "\t# remove columns that violate the scale\n",
        "\tdat[, c(\"Age\", \"Fare\", \"SibSp.2\", \"Sexmale.Fare\", \"Pclass3.Age\", \"Pclass3.Fare\") := NULL]\n",
        "\t\n",
        "\t# rescale columns in new.dat that have a max value above 1 and/or a min value below 0\n",
        "\t\t# a 0 to 1 scaling is required for neural networks\n",
        "\tnew.dat[, rescale.Age := rescale(Age, to = c(0, 1), from = range(train$Age))]\n",
        "\tnew.dat[, rescale.Fare := rescale(Fare, to = c(0, 1), from = range(train$Fare))]\n",
        "\tnew.dat[, rescale.SibSp.2 := rescale(SibSp.2, to = c(0, 1), from = range(train$SibSp.2))]\n",
        "\tnew.dat[, rescale.Sexmale.Fare := rescale(Sexmale.Fare, to = c(0, 1), from = range(train$Sexmale.Fare))]\n",
        "\tnew.dat[, rescale.Pclass3.Age := rescale(Pclass3.Age, to = c(0, 1), from = range(train$Pclass3.Age))]\n",
        "\tnew.dat[, rescale.Pclass3.Fare := rescale(Pclass3.Fare, to = c(0, 1), from = range(train$Pclass3.Fare))]\n",
        "\t\n",
        "\t# remove columns that violate the scale\n",
        "\tnew.dat[, c(\"Age\", \"Fare\", \"SibSp.2\", \"Sexmale.Fare\", \"Pclass3.Age\", \"Pclass3.Fare\") := NULL]\n",
        "\t\n",
        "\t# build a formula string for the training model\n",
        "\trhs = paste(names(new.dat), collapse = \" + \")\n",
        "\tform = paste(\"y ~\", rhs)\n",
        "\t\n",
        "\t# build the training model\n",
        "\tmod = neuralnet(form,\n",
        "\t\t\t\t\tdata = dat,\n",
        "\t\t\t\t\thidden = c(15, 15), \n",
        "\t\t\t\t\tthreshold = 0.25, \n",
        "\t\t\t\t\tlinear.output = FALSE)\n",
        "\t\n",
        "\t# make predictions with the training model using the test set\n",
        "\tynew = as.numeric(compute(mod, new.dat)$net.result)\n",
        "\tynew = as.numeric(ynew >= mods.list$nn$cutoff)\t\n",
        "\t\n",
        "\t# return a list of the model (must label as fit) and predictions (must label as pred)\n",
        "\toutput = list(pred = ynew, fit = mod)\n",
        "\treturn(output)\n",
        "}\n",
        "\n",
        "# support vector machine wrapper\n",
        "my.svm = function(Y, X, newX, ...)\n",
        "{\n",
        "\t# build the table for training the model\n",
        "\tdat = data.table(X)\n",
        "\tdat[, y := factor(Y, levels = 0:1)]\n",
        "\t\n",
        "\t# build the training model\n",
        "\tmod = svm(y ~ .,\n",
        "\t\t\t\tdata = dat,\n",
        "\t\t\t\ttype = \"nu-classification\",\n",
        "\t\t\t\tgamma = 2.66668701171875, \n",
        "\t\t\t\tnu = 0.7,\n",
        "\t\t\t\tprobability = TRUE)\n",
        "\t\n",
        "\t# make predictions with the training model using the test set\n",
        "\tynew = as.numeric(attr(predict(mod, newdata = data.table(newX), probability = TRUE), \"probabilities\")[,2])\n",
        "\tynew = as.numeric(ynew >= mods.list$svm$cutoff)\n",
        "\t\n",
        "\t# return a list of the model (must label as fit) and predictions (must label as pred)\n",
        "\toutput = list(pred = ynew, fit = mod)\n",
        "\treturn(output)\n",
        "}\n",
        "\n",
        "# create a library of the above wrappers\n",
        "my.library = list(\"my.log\", \"my.pen\", \"my.lda\", \"my.qda\", \"my.gbm\", \"my.rf\", \"my.nn\", \"my.svm\")\n",
        "\n",
        "# build the model\n",
        "my.mod = SuperLearner(Y = Y, X = X, newX = newX, family = binomial(), \n",
        "\t\t\t\t\t\tSL.library = my.library, method = \"method.AUC\")\n",
        "\n",
        "my.mod\n",
        "\n",
        "# extract predictions\n",
        "my.pred = my.mod$SL.predict\n",
        "my.pred = as.numeric(my.pred >= mods.list$sl$cutoff)\n",
        "\n",
        "# build submission\n",
        "my.pred = data.table(PassengerId = (1:nrow(test)) + nrow(train),\n",
        "\t\t\t\t\t\tSurvived = my.pred)\n",
        "\n",
        "write.csv(my.pred, file = \"submission-nick-morris.csv\", row.names = FALSE)\n",
        "\n",
        "}                        "
      ]
    }
  ],
  "metadata": {
    "_change_revision": 0,
    "_is_fork": false,
    "kernelspec": {
      "display_name": "R",
      "language": "R",
      "name": "ir"
    },
    "language_info": {
      "codemirror_mode": "r",
      "file_extension": ".r",
      "mimetype": "text/x-r-source",
      "name": "R",
      "pygments_lexer": "r",
      "version": "3.4.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}